 # Paper Summary The paper presents a new method for embedding visual images into a state space suitable for effective control by an actor-critic style RL algorithm. They show how a previously explored idea of using a bisimulation between state abstractions and reward sequences to group states that are similar from a decision theoretic perspective can be extended to a continuous deep embedded representation using twin network style learning through standard gradient descent optimization. They call the approach Deep Bisimulation for Control (DBC).  The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains. The approach contrasts directly with algorithms that use an autoencoder to find a compact representation by reconstructing input frames or predicting future input frames. These approaches necessarily represent enough information to reconstruct both task relevant and incidental details. Experimentally, the paper shows that performance of reconstruction-based approaches degrades by a large and significant amount when extraneous background detail is present in image frames, while the proposed method is immune. The evaluation is done on widely respected benchmark of Deep Mind MuJoCo based articulated figure simulations and a CARLA realistic image simulation of car driving which shows that focusing on task specific detail is important on realistic tasks. # Pros and Cons The work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images.  The ability to find compact, task relevant representations of high-dimensional inputs is central to the ICLR community. The paper covers relevant background in state abstraction in RL and it is clear how it relates to the paper's contribution.  The paper clearly explains the background of bisimulation and how the idea of grouping states according to their probable reward sequences can be practically realized by grouping states by their immediate reward and future state distributions. It is also clear in principle that a twin network can be used to induce a latent space with these properties.  The paper shows that the policy will converge and that the error will be bounded and that the representation will generalize to any problem domain in which causal dependencies are a subset of the problem domain the representation was trained on. Which is nice.  There is a nice evaluation on both MuJoCo articulated figure problems from the Deep Mind Control suite as well as the CARLA simulated driving application that uses large realistically rendered images. Comparison against state-of-the-art RL algorthims makes results convincing. I think the CARLA example is nice as it shows that practical problems without artificial augmentations have the property that the input details can be distracting if fully reconstructed. The superiority of DBC in this case really makes this point well. The paper does not report computational load associated with their approach. Presumably due to the closed form Wasserstein approximations, we are probably looking at only a fractional increase in time for the encoder network above the policy, dynamics and reward models. So roughly 30% more?? Key figures for central results are too small to get meaningful interpretations unless enlarged by a factor of 4 or 5. Somehow this seems to go against the spirit of page limits to me. Figure 6 and the description don't seem to be aligned, but I can imagine that it can be readily fixed. # Recommendation  I recommend acceptance. The paper shows how to extend bisimulation principle for grouping states to continuous deep actor critic methods and provides convincing evidence on standard benchmarks that it is effective in extracting a task-relevant abstraction that is robust to noise in observations and focuses on task specific detail.  # Questions  Figure 6: it seems that the simple_distractors environment is different than setting 2 natural video setting. This caption could use some reworking to get parallelism clear. The ideal gas is the same as simple_distractors?  Also the graphs seems to be about different experimental types. Does the first experiment also use frozen encoder?  The graphs do not seem to relate to the text which talks about walker_stand, walker_run reward functions. I am confused. Can the paper have any insight into the relative performance of their algorithm versus benchmarks algorithms across different tasks (finger spin, cheetah, walker) given that they are all stick figures? # Feedback  I was able to work through Definition 1 and assure myself it made sense. In the end I made a small picture that helped. There seems to be some ambiguity in notation between observations, underlying state and latent variable spaces.  For instance, in section 3, script S is defined as a state space. In section 4, the function d is defined on script S x script S which is described as an observation space.  Admittedly, the work seems to be situated in an approximately fully observable world in which states and observations are somewhat equivalent. I suspect that is why the 5-camera 300-degree suround view was necessary in the CARLA experiments. Interesting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function. This is to enforce the separateness of the optmizations in algorithm 1? Definition 2, the bisimulation metric contains a max & so this is worst case discrepancy between the futures between state space actions and empirical actions. Was average discrepancy considered? This might be relevant later in discussion about intractability in previous approaches (section 4 paragraph 3). Equation 4 specifies an L1 metric for the distance in abstraction space ||z_i  Z_j ||. Is there a motivation for this choice? Again trying to bound the worst error? Theorem 4 references Theorem 5 which is not in the main text. In section 5, ideally epsilon would be briefly described before it appears in a bound. Figure 3 is very small... the labels  particularly the subscripts are unreadable. It may not be making a point important enough to include it. The causal variables section could be shorted in general. It is a good point but not completely surprising. Figure 4 is a key figure to support the paper's hypothesis that deep bisimulation is effective for state abstraction in noisy images. This figure really needs to be bigger. In particular, I had to strain to read the legends to understand if the axes were different between the uncluttered video of articulated figures and the cluttered video of articulated figures with a background movie in them.  It was also hard to make out which line was which.  In particular, the cheetah column does not show DBC improving on cluttered video scenario --- it tops out at around 250 in both top and bottom graphs, but the scales are very different. It confuses the message a bit.  There is a statement a single loss function would be less stable and require balancing the components. Is this speculation or based on experience? Separate optimizations implicitly define a balance between these terms wouldnt they? Namely equal balance? To some degree, figure 9 is making the same argument as figure 5. Could leave this out if you were tight for space. Figure 5: What is the first column of figures to the left? It doesnt seem to be relevant? Otherwise, I think the figure is very effective in conveying the structured embedding space does a better job of grouping similar states. It is also very small. Drawing a white border between the figure pairs would underscore visually that there are two states in each image  Figure 9: I could not make sense of the images on the sides of the figure. Particularly, the images on the left side.  They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was. Section 6.4 reward highway progression an penalizes collisions  an => and Future work  another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds. For instance, can the agent drive a car with only a forward view if given memory? Does this break down if it does not have memory? This could either be an explicit neural memory or implicit memory such as an LSTM &  Estimating uncertainty could also be important to produce agents that can work in the real world and assess when they know what they are doing and when they do not. Another future work area is in modeling of transition distributions as something more complex than Gaussians & what if there are distinct possible futures that are equally valid: it is ok for the robot to turn left or right as long as it avoids the object straight ahead?  The paper presents a method called SaliencyMix. They improve a method that augments images by adding random patches from other images. The innovation is that they select these patches using a saliency map.This paper has an excellent discussion and critique of previous work. They discuss the existing work with a nice summary and then discuss reasons why selecting random patches can have issues. There is a clear argument for their method over selecting patches randomly. They make clear claims that this approach improves performance over randomly selecting patches. The experiments support this with sufficient related work (Cutout, Cutmix) and exploration of other design decisions and aspects about the idea.The idea is relatively straightforward and is inline with existing literature. The paper is well executed so there is not much to complain about.The availability of the source code is not clear from the text.A potentially interesting analysis (but not required) is an analysis of the increased runtime in practice. This paper is in defense of simple semi-supervised learning (SSL) with pseudo-labeling (PL): authors demonstrate with experiments on 4 vision datasets (CIFAR-10, CIFAR-100, Pascal VOC and UCF-101) that pseudo-labeling can perform on par with consistency regularization methods. Authors argue that PL doesn't work well because of poor network calibration: because of that the high confident predictions are wrong leading to noisy training and poor generalization. The main contribution of the paper is the usage of prediction uncertainty selection in addition to the confidence-based selection which provides high accuracy of PL used in the further training. Besides this PL is generalized to create negative labels: with this authors perform and show effectiveness of negative learning and multi-label classification. Proposed approach performs in the same ballpark as state-of-the-art methods on CIFAR-10 and CIFAR-100, while it achieves the new state-of-the-art results on video dataset and multi-label task. It is worth to notice that proposed approach is independent from the domain while consistency regularization methods extensively are based on the specific augmentation techniques for the vision/datasets.Pros:- Cool idea on predictions uncertainty based selection for pseudo-labeling, no any dependence on domain-specific augmentations for the method- Analysis of correlation between model calibration and prediction uncertainty- Analysis of UPS compared to the conventional PL and confidence-based PL- Ablation study on each component of the method, and dependence on the network architecture- Well-designed (fair comparison) extensive experiments and comparisons on 4 dataset for multiclass and multi-label classification with better performance than other methodsCons:- Absent of large-scale experiments with ImageNetComments:- typo page 3 "would lead to binary psuedo-labels" -> "would lead to binary pseudo-labels"- "For the multi-label case, $\gamma = 0.5$ would lead to binary pseudo-labels, in which multiple classes can be present in one sample." - this sentence is not clear. If $\gamma = 0.5$ it could be only one or two classes presented in the pseudo-label vector.- Eq. (2), it is obvious but still please specify that $\tau_p >= \tau_n$- Figure 1 (b) and (c) - on which data is this analysis done?- Do authors use the same network $f_{\theta, k}$ on each PL iteration $k$ and just randomly reinitialize it?- What is $\gamma$ value in experiments?- For Table 1 would be good to have a clarification on baselines: which are consistency regularization based, which are PL. - typo in footnote 2 on page 7: add dot at the end of sentence.- typo page 7 "experimental set-ups." -> "experimental setups.", "labeled samples Both" -> "labeled samples. Both"- Did authors try experiments on ImageNet too?- On which data is study in Fig.2 performed?- typo page 8 "unique in that it can be easily" -> "unique in that: it can be easily" (or any punctuation here)- For ECE computation is percentile binning used?- some possible relevant works: https://arxiv.org/pdf/2003.03773.pdf, https://arxiv.org/abs/2006.07733, simCLR v2, https://arxiv.org/abs/2006.09882It is very well written paper with extensive experiments and ablations (except large-scale experiment), which prove the method efficiency and generalization. Hope, this will push the study of simple SSL approach, pseudo-labeling, with the new competitive results not only in vision but in other domains too. In this paper, the authors propose MSR, a parametrization of convolutional kernels that allows for meta-learning symmetries shared between several tasks. Each kernel is represented as a product of a structure matrix and a vector of the kernel weights. The kernel weights are updated during the inner loop.  The structure matrix is updated during the outer loop.  Strengths1. The paper is interesting and is easy to read. The figures help a lot in understanding the discussed ideas. 2. The authors demonstrate that the proposed method outperforms the baseline meta-learning models. They empirically prove that MSR indeed learns valuable symmetries from the set of tasks and the provided data.3. The related work as well as the experimental part allow for a clear positioning of the proposed approach. It demonstrates a valuable connection between meta-learning and building equivariant models.I did not find any major weaknesses in the presented paper.Questions1. A matrix $W$ of size $8\times8$ can be reparametrized in several ways. The corresponding vector $v$ can be of size $1, 2, 4, \dots 64$. Do you consider the size of the vector as a hyperparameter? If so, how to choose it? 2. If we consider the case of the exact flip symmetry, then the length of $\text{vec}(W)$ must be even. One half encodes the original weight and the other half encodes the flipped weight. So we end up with a constraint between the shape of the matrix and the structure of the symmetry group. The same argument applies to all other symmetry groups. What happens if the constraint is not satisfied? Can we learn a flip symmetry for $W$ of size $7 \times 2$? How $U$ will look in this case?I enjoyed reading the paper. It is insightful, well-written, and demonstrates several valuable results both theoretical and experimental.  This paper addresses probabilistic data driven model calibration, i.e. aligning predicted target probabilities with actual ones. This problem has been extensively studied for classification tasks but solutions for regression tasks have limitations as illustrated by Fig.1. The authors intend to fill this gap and introduce a general kernel-based calibration framework that subsumes other ones previously defined for classification. The contribution of the authors is thus clearly stated and positioned w.r.t. prior arts. The authors start by proposing an alternative definition of calibration (Def.2) in order to cast the problem into integral probability metrics. It is this re-definition of the problem that allow them to encompass prior arts as special cases. For a number of practical and theoretical reasons, the authors focus on a special case of this framework which involves the computation of the MMD as a metric. Based on the MMD literature but also relying on the structure of their problem (where the auxiliary variable can be marginalized out), the authors provide several consistent estimator with known rates in dataset size.The validity of the proposed estimates is assessed through convincing numerical experiments that involve a calibration test.I honestly do not have much critic to address to this work which seems to have reached a level of maturity perfectly adapted for publication in ICLR. The only damper is that the proposed methodology allows to detect miscalibration not yet to cure it. However, the authors seem to have some ideas on that too as mentioned in their conclusion. This is a paper in theoretical computer science which considers what many would consider a very hard problem -- learning a latent simplex lurking 'underneath' a cloud of data.It is extremely well organized and tightly put together. It patiently shows that this problem is lurking behind many different high-dimensional data analysis challenges, and then carefully lays the groundworkfor discussing a proposed algorithm and  its performance characteristics.What really surprised me about the paper was that the assumptionsgoing to make up the 'math result' seemed to be quite stylized to me,in such a way that many datasets could never really satisfying them(although perhaps some could). Nevertheless, the authors present results onreal data which seem to suggest that the algorithm really can be usedin empirical representation learning. This really surprises me,while putting forward positive results based on the authors' owninvented assumptions impresses me not so much.  So it would be interestingto hear from the authors, for example in a talk or in an eventual journal paper,to what extent the data actually obey the stated assumptions. If theydon'y obey the stated assumptions of this paper, then some less restrictiveassumptions would be the 'sharp conditions'. In that case, what are the true conditions needed for the algorithm to work? Obviously the significance this work will have in the eyes of potential users depends a lot on the just-discussed  Q and A. The paper proposes an approach to develop new Reinforcement Learning (RL) algorithms through a population-based method very reminiscent of Genetic Programming (GP). The authors "evolve" loss functions that can be used across different RL scenarios and providing good generalization.Pros:- The paper is well written, well structured and clear at all times. There are few typos here and there but nothing that affects the readability of the paper.- The results are relevant: the fact that the method re-discovers recently proposed learning models is remarkable, as well as it finds new models indicates that it is a line of research worthy of further exploration.- The other minor contributions are also noteworthy, specifically the Functional equivalence check, which IMO is a brilliant idea, as well as the early hurdles approach.- The interpretation of the approach is also correct: the fact that authors make a clear distinction between learning from scratch and bootstrapping tells me that they truly understand the overall framework they based their method on.Cons:- The main idea behind this type of meta-learning is always very interesting to revisit. Nevertheless, and truth to be told, the idea of using GP to find new loss (or "objective/fitness", as known in evolutionary computation (EC)) functions is quite old [1,2]. At some points the work presented here feels somewhat "old-fashioned", or a mere re-edition or adaptation of those early seminal works.- The results can be easily rebutted; if I understand correctly, the authors are presenting the results of a SINGLE run for every scenario they tested their method on (different no. of environments) , which tell us nothing about the average behavior of their proposed approach. Although this fact is understandable given such long training times (3 days with 300 cpus for a single run), many people will not accept the presented results arguing that they could have been the result of lucky runs. On the bright side, such practice is somewhat usual in the deep learning community, so many people may overlook it for now. I'd suggest the authors make a statement arguing why they feel confident that they approach may present a low variance, or why they still consider these results relevant, even if they require twenty or 30 runs to find results this good. They could argue that even if their method is currently statistically unreliable, it could probably be stabilized with many methods proposed in the EC community (such as spatially distributed populations [3], etc.)- The thing that bugs me the most of this work is that the proposed method is clearly a flavor of GP; however the authors never really acknowledge it by its name; instead they claim it to be something called "Regularized Evolution" which is supposedly introduced in a previous paper; nevertheless, it is really just GP: they are evolving tree-shaped programs, which is the hallmark of GP (there exist variants of GP that are not even EC-based [4]), so I don't see a reason for not calling it for what it is. I wish the authors clearly state why their approach cannot be considered part of the GP framework, or if it indeed is, then make such statement clearly.- In this vein, it is also interesting to note that they do not use a crossover operation. In the GP framework, crossover is generally regarded as a more powerful operator than mutation, and the combination of both operators can give the best results. My guess is that crossover is difficult to implement given the "search language" (or 'primitives', as known in GP), and then crossover would result in many invalid programs. Still, it would be nice if authors could explain a little bit on this issue.Other comments:It draw to my attention that you use populations of size 300 as well as a 300 cpu system. It would seem that you intend to have one cpu for each individual evaluation; however, given the advances presented by the functional equivalence check and the early hurdles technique, I wonder what happens when a individual is no longer being evaluated.. is that cpu left idle? or is it reassigned to contribute to the evaluation of another individual (which sound complex to do, given the RL nature of the problems being treated)?References:1. Bengio, S., Bengio, Y., & Cloutier, J. (1994, June). Use of genetic programming for the search of a new learning rule for neural networks. In Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence (pp. 324-327). IEEE.2. Trujillo, L., & Olague, G. (2006, July). Synthesis of interest point detectors through genetic programming. In Proceedings of the 8th annual conference on Genetic and evolutionary computation (pp. 887-894).3. Tomassini, M. (2006). Spatially structured evolutionary algorithms: Artificial evolution in space and time. Springer Science & Business Media.4. Hooper, D. C., Flann, N. S., & Fuller, S. R. (1997). Recombinative hill-climbing: A stronger search method for genetic programming. Genetic Programming, 174-179. Summary: This paper investigates lookahead dynamics of smooth games. By this the authors mean discrete-time dynamical systems generating from a given algorithm by adding a relaxation step in the updates. The main aim of the paper is to solve smooth games. Under sufficient convexity assumptions Nash equilibria for such games can be identified as solutions to a Variational Inequality with a monotone and operator. This is in particular the case for convex-concave min-max problems. The main conclusion of this paper is that a combination of relaxation and lookahead effects stabilizes the learning dynamics and can lead to acceleration over the base algorithm.Evaluation: This is a very strong paper with an extremely large number of interesting results. In my opinion it makes an extremely good contribution to the flourishing literature  on game dynamics. I only have some small technical remarks which can easily be fixed. Specific Remarks:  .  Interchange the order of eqs. (5) and (6).. Define $F^{k}$ in eq. (7) . Check for consistency of notation: Sometimes $M_{m\times m}$ is used for the matrix space, then $\mathbb{R}^{m\times m}$. If the former is used, explain which field of numbers is used. . Define $\rho$ in Theorem 3 ##########################################################################Summary: The paper proposes a new task in Graph Learning. Basically, the idea is the following: suppose we have a node classification model trained on a Graph G, suppose we have a new node (not present in G) and we want to classify it. Given that the new node has no connections with Gs other nodes we cannot leverage any structural information to run the classifier. This is an issue that authors present it as cold-start in semi-supervised graph learning. The solution, even if simple, is very effective and for this reason even more interesting. Basically, they start with a retrieval step (they call it link prediction) that is trained using a link reconstruction loss and its based on dot-product to make the link prediction phase scalable. After those links are reconstructed they run a node classification step (based on GCN, GraphSAGE, or GAT) on G plus the new node and the predicted links.((The results obtained in the experiments are really encouraging with improvements ranging from 15 to 25% over a baseline that does not consider the link prediction phase.##########################################################################Reasons for score:  Overall, I vote for accepting. I consider this as a non-trivial step forward towards using Graph Learning on recommendation problems and node classification problems. There are many applications of Graph Learning where the technique presented in this paper can be of help. Consider, for instance, all the words relying on GNNs to do fake news detection based on a semi-supervised technique. All of those methods fail in the case a new document (node) has no explicit connections to the graph. This method would solve that issue.  ##########################################################################Pros:  1. The paper tackles a problem that is, in my opinion, very important and, so far, overlooked: cold-start node prediction using graph learning.2. The technique presented in the paper is simple, which I consider a plus. It works and its simple.3. The experiments show a great improvement over the baseline ##########################################################################Cons:  1. One big limitation of this work, that is in my opinion under explored is that it is based on the assumption that links depend on the content/features of the nodes. In some cases this assumption might not hold true. I would like authors to discuss on this point.2. I am not sure how the link prediction phase could be made scalable. As it is defined now it is an O(n^2) step. Or better, I have some ideas but Id like authors to discuss this.3. Why you pick a threshold of .5 in equation (8) Shouldnt this be an hyperparam? ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################Some minor issues (1) There is an inconsistency in the notation for M in equation (4) and M in equation (5). In equation (4) M takes 3 parameters, while in (5) it takes 2. Please clarify.(2) Why you speak about cold nodes X_c instead of  cold node? Up until now it looked like you only used one node at a time. Please clarify.(3) What is q() in equations (5) and (6)?(4) In Figure 3, what is the black line under the blue curve? Its not written anywhere. In this paper, the authors propose a generic system for performing one-shot audiovisual synthesis from only one small sample. The results are impressive for in-the-wild speech synthesis and their approach could have a broader impact in the community. Strengths: + One shot audiovisual synthesis for a target speaker. + The publication of a new dataset for AV synthesis evaluation. + Comprehensive analysisWeaknesses:  - No theoretical novelty. It seems much of the benefits of the approach comes from the extra data and training procedure. Other comments:In Sec 3.2. Autoencoders as projection operators, the authors here make it sound that they are the first ones that noted that autoencoders can capture the data-generating distribution.  ## SummaryThe paper presents a class of RL algorithm based on analytic gradients of the objective, coming directly from a fully differentiable simulator and differentiable rewards.The analytic gradients are used to optimize trajectories.The known states and actions from the optimized trajectories are then used to improve the deterministic policy.Overall, this is an interesting paper that maximally leverages analytic gradient informationthat come directly from a differentiable simulator, rather than a learned differentiable dynamics model.The methods prove to be highly effective for the given class of problems they are tested on.## Strengths- This work provides strong insights into the benefits of differentiable simulators for  trajectory-guided reinforcement learning.  The paper provides a solid discussion and evaluation of  the use of first and second-order methods.- It demonstrates effective and efficient results for a difficult class of problem that has direct application,  i.e., cable-suspended loads## Weaknesses- The foundations of the trajectory-based optimization method exist in prior art [Zimmermann 2019]- There is perhaps a restricted class of tasks can benefit from the approach:  contact collisions, infinite-horizons,  and non-differentiable rewards may be off limits?## RecommendationThe RL community can benefit from this work in several ways, as outlined with the various strengths given above.## Questions- Is the method restricted to fixed-length episodic tasks?  Fine if it is, but readers will want to know.- Do local minima pose a problem, given that the method effectively does no true exploration?- Can PODS be viewed as a form of fast analytic-derivative trajectory optimization followed by behavior cloning?- Can hard constraints be handled, e.g., obstacles or joint limits that are to be avoided?- What are the tradeoffs between working in handle-space vs robot joint space?## Additional FeedbackWhat would be the impact of using many fewer rollouts per epoch?Why not also compare the results for some tasks which are commonly used for RL, e.g., pendulum and acrobot swing-up?Appendix A.2 only provides very minimal details on the environments.What are the dimensions and masses of the various pendulum systems?The following is an early paper (1998) that uses back-prop-through-time to directly optimize a control objective:"NeuroAnimator:Fast Neural Network Emulation and Control of Physics-Based Models"The proposed work has connections with Guided Policy Search (Levine and Koltun, IMCL 2013),which also uses optimized trajectories as training data, only in their case for a stochastic policyand using differential dynamic programming, instead of the deterministic policy setting used heretogether with a Hessian-dependent Newton's method. Explaining these connections could be helpfulto many. ## SummaryThe paper presents a method of regularising the latent space of an Autoencoder in a way that pressures the data manifold to be convex. This allows interpolation within the latent space which does not leave the data manifold and results in realistic reconstructions as one moves from one point to another.This is done through the introduction of adversarial and cycle-consistency losses, over and above the usual reconstruction loss and a smoothness loss. The adversarial loss ensures that the interpolated reconstructions are realistic, while cycle-consistency encourages a bijective mapping.## Quality & ClarityThe paper is clearly written and without typographical or grammatical errors. It is structured logically and the authors' arguments are easily followed.The results are compelling and the proposed AEAI technique clearly outperforms other methods in the qualitative experiments, with quantitative results to substantiate it.## Originally & SignificanceThe contribution of the paper is clear in that it imposes convexity regularisation to the latex space. The approach is compared with modern competing techniques and the work is well positioned among recent literature in the field.## OutcomeThis is a clear, high-quality paper with compelling results. The paper is clearly written and easy to understand. The authors give a clear introduction to Newton and quasi-Newton methods, and summarize three main drawbacks of quasi-Newton methods for nonconvex stochastic optimization which is the real case for many practical problems. The three main challenges are stochastic variance, nonconvexity and inefficiency.As far as I known, there is no quasi-Newton methods that could solve the above challenges simultaneously, and this paper give a solution that considering all the three aspects.For efficiency, the computational and memory efficiency are both considered. Instead of depending on first order informations from m previous iterations, the proposed method approximates the Hessian matrix by  considering  the diagonal parameters of B_t which is more memory and computational efficient. To solve the stochastic variance, the authors propose stepsize bias correction to work better in stochastic gradient descent framework. And exponential moving average to g_t is adopted to make the gradient more stable.And to better support nonconvexity, a rectified trick is used in the constraint B_t. Whats more, in order to produce a better Hessian approximation, a parameter-wise weak secant condition is used.All the above ideas look reasonable, and this give a full workable solution to train networks in variance domains.The authors conduct several experiments in Image Classification, Language Modeling and Neural Machine Translation, and experiments show the effectiveness of the proposed method. The proposed method has advantage in all tasks, and for Language Modeling, the improvements seem to be very large.Questions:(1) What are the effects of different factors such as stepsize bias correction, rectified trick and a parameter-wise weak secant.(2) Why the improvements for Language Modeling are larger than for Neural Machine Translation and Image Classification?The authors are strongly suggested to test more network structures on challenging tasks and try to achieve new state of the art results which could make the method more convincing. And more theoretical analysis is suggested to better understand the proposed method. This works motivates the use of a factorized critic for multi-agent policy gradient.   The technique is well-motivated, and the exposition anticipates and answers readers' likely concerns.   The experiment section is well-organized, supports the paper's major claims, and is empirically compelling.The policy improvement claims in section 4.1.2 are initially unintuitive, but ultimately are intelligible as an agent-block-coordinate local optimality statement.  However this reviewer is not clear on the quality of these local optima (i.e., when do we get "trapped"?).  For example, is it possible to design a task where the local optima are all very poor?   Of course, the experiment section indicates many benchmark tasks are amenable to this decomposition; but perhaps reasoning about this would help in (re)defining multi-agent problems to encourage success, e.g., it would be interesting if adding actions that communicate information directly between agents mitigates the local optima problem.  This paper proposes a simple but highly effective approach to improve differentially private deep learning, by projecting gradients to a small subspace spanned by public data.This idea has been proposed before, but this paper seems to have the unique (and important) insight to release noisy residual gradients, so as to ensure that the final gradient estimation remains unbiased.Experiments on MNIST, SVHN and CIFAR-10 show that the proposed approach significantly improves the privacy-utility tradeoff of DP-SGD.This paper is excellent. It is well motivated, as the assumption of access to a small amount of public data seems very reasonable in practice. The proposed algorithm is quite natural. The key insight is to release private noisy estimates of both the low-dimensional gradient embedding *and* the residual (low-norm) gradient. This is easily shown to produce an unbiased estimate of the gradient, which is crucial for performance.The experiments convincingly show that this approach leads to strong performance improvements, and I predict that this simple technique will be applied in many subsequent works.I have a number of (mostly minor) comments and suggestions for improvement: - In Algorithm 1, and in the main text, it would be good to clarify the clipping applied to both the embedded and residual gradients. It is also worth noting that in practice, the estimated gradient is probably *not* quite unbiased because of this clipping (the same holds for standard DP-SGD of course).- You mention that you use "privacy amplification by subsampling" to compute the DP guarantees. It would be good to clarify this, since some of these theorems apply specifically to a "subsample + gaussian mechanism" approach, which your algorithm doesn't quite match (GEP applies two independent Gaussian mechanisms to functions of a subsampled gradient).This seems like it could be resolved as follows:- you have embedded gradients Wi clipped to norm C1- you have residual gradients Ri clipped to norm C2- You can "stack" these as gradients gi = [Wi/C1 ri/C2] of norm S=sqrt(2), and then apply a single Gaussian mechanism with sensitivity S to this stacked gradient. After that, you can rescale the two components by C1 and C2 respectively. This seems identical to your proposed approach of applying two Gaussian mechanisms with sensitivities scaled by a factor \sqrt(2). - Only in Section 4 do you mention that you split the gradient into different groups to fit them in memory. It would be nice to see this discussed earlier on in the paper. E.g., the abstract claims that GEP has "low computational cost", which then seems strange when Section 3.1 says that the cost is O(m*k*p), which would be huge even for moderate DNNs. Section 3.2. mentions using a ResNet-20 with k=2000 basis vectors, which wouldn't fit in any GPU memory if it weren't for the grouping. While reading these sections, I was confused about how this could possibly be implemented at a low cost, as described.Following on this, it would be nice to have some results on the additional computational and memory costs of GEP in the evaluation. - Papernot et al. show that the choice of architecture can have a large influence on DP-SGD (https://arxiv.org/abs/2007.14191). The results they obtain are sometimes better than your GP baseline. It would be interesting to see if you can get even better results with GEP with such architectures.- This is somewhat orthogonal to your approach, but do you have a sense of whether 2000 public ImageNet samples could be leveraged in other ways than as gradient basis vectors? E.g., on CIFAR-10, it is well-known that unsupervised dictionaries of data patches can achieve >80% accuracy. So one could also consider using the public data to learn a patch dictionary, and then simply train a small private classifier on top of these features. Maybe this would perform better than GEP? ## SummaryThe paper presents a new, more complex, dataset for the use of disentangled representation learning. The dataset is based on real and simulated images of the trifinger robot platform. There are 7 factors of variation with high-resolution measurements of these factors. The dataset contains over 1 million simulated images and another ~1000 annotated images of a real trifinger robot arm.The authors also present a new neural architecture to scale disentanglement on more complex datasets and present a large empirical study on the performance of various techniques on out-of-distribution downstream task performance.## Quality & ClarityThe paper itself is well written, with a structure that effectively guides the reader through the work and results.The dataset, significance and experiments are clearly outlined.## Originally & SignificanceThe novelty of the dataset is clear. Providing a complex disentanglement dataset where the underlying factors of variation are inherently correlated. The in- and out-of-distribution experiments are possible because of the presence of both synthetic and real-world data.The experiments run are repeated multiple times and the results are convincing. They use both unsupervised and weakly supervised approaches and the results are both intuitive and supported by the literature. The experiments on out-of-distribution representation transfer are interesting and show that disentangled representations can lead to better transfer to out-of-distribution tasks.## Outcome RationaleThis dataset is likely to be extremely useful to the community going forward and work disentangled representation learning is likely to benefit from it. The experimental setups are sensible and the largescale benchmarks support the use of disentangled representations when transferring from simulated to real-world scenarios. Summary: The paper introduces a framework for lifelong learning of compositional structures. The algorithm is loosely inspired by biological learning and consists of two main steps. The step of component selection relies on existing methods that can learn task-specific structure. In the next step (adaptation), the algorithm adapts the knowledge from the previous tasks to the current task and if that is insufficient to solve the task, new components are added. Adaptation step relies on existing methods for adapting the knowledge state given a new task in continual learning (component parameters are updated). Knowledge expansion (adding new components) uses component dropout, a method proposed by the authors which combines pruning and alternating backpropagation steps with and without the potential new component. The proposed method is beneficial in terms of computational complexity in comparison with the standard lifelong learning methods. The authors evaluate the method on three compositional structures and show that it outperforms the baselines. The paper includes visualisation of the learned components, extensive appendix with additional experiments and ablation studies, and a systematic overview of the prior work in learning compositional structures and lifelong learning.Score justification:The paper is exceptionally well-written and rich in terms of experimental results and ablation studies. The proposed algorithm combines existing methods in a novel way and extends them with component dropout. The topics of learning compositional structures and lifelong/continual learning are of high interest to the community. The new procedure (Algorithm 1) balances out the key problems of retaining existing knowledge and expanding it without frequent expensive data revisiting steps. The only concern would be the lack of conclusion and sporadic vague phrasing. Overall the paper is very interesting to read and it should have a strong impact on the research in lifelong learning. Pros: The justified parallel between the algorithm and children development researchThe contributions are well-positioned in comparison to the existing work (in the Related work section and throughout the paper)Computational complexity is discussedExtensive experiments which show strong results in learning components of an increasing complexity Code is released for reproducibilityCons:The main paper combined with the appendix is quite long for a conference paperConclusion is missingFrom the sentence Our framework separates the learning process into two broad stages:learning how to best combine existing components in order to assimilate a novel problem, and learning how to adapt the set of existing components to accommodate the new problem. it is unclear how the two stages differIn Table 1, you could mention the meaning of the symbols which parametrize the complexities in the caption so that it is possible to better understand the computational complexity analysis without the appendixQuestions during rebuttal period: Please address and clarify the cons above  Typos: None were found This is in my view a strong contribution to the field of policy gradient methods for RL in the context of continuous control. The method the authors proposed is dedicated to solving the premature convergence issue in PPO through the learning of variance control policy. The authors employ CMA-ES which is usually employed for adaptive Gaussian exploration. The method is simple and yet provides good results on a several benchmarks when compared to PPO.One key insight developed in the paper consists in employing the advantage function as a means of filtering out samples that are associated with poorer rewards. Namely, negative advantage values imply that the corresponding samples are filtered out. Although with standard PPO such a filtering of samples leads to a premature shrinkage of the variance of the policy, CMA-ES increases the variance to enable exploration.A key technical point is concerned with the learning of the policy variance which is cleverly done, BEFORE updating the policy mean, by exploiting a window of historical rewards over H iterations. This enables an elegant and computationally cheap means of changing the variance for a specific state.Several experiments confirm that this method may be effective on different task when compared to PPO. Before concluding the authors carefully relate their work to prior research and delineate some limitations.Strengths:   o) The paper is well written.   o) The method introduced in the paper to learn how to explore is elegant, simple and seems robust.   o) The paper combines educational analysis through a trivial example with more realistic examples which helps the reader understand the phenomenon helping the learning as well as its practical impact.Weaknesses:   o) The experiments focus a lot on MuJuCo-1M. Although this task is compelling and difficult, more variety in experiments could help single out other applications where PPO-CMA helps find better control policies. SummaryThe paper focuses on pruning neural networks. They propose to identify the nodes to be pruned even before training the whole network (conventionally, it is done as a separate step after the nn was trained and involves a number of iterations of retraining pruned nn). This initial step that identifies the connections to be pruned works off a mini-batch of data.Authors introduce  a criterion to be used for identifying important parts of the network (connection sensitivity), that does not depend on the magnitude of the weights for neurons: they start by introducing a set of binary weights (one per a weight from a neuron) that indicate whether the connection is on or off and can be removed. Reformulating the optimization problem and relaxing the constraints on the binary weights, they approximate the sensitivity of the loss with respect to these indicator variables via the gradient. Then the normalized magnitude of these gradients is used to chose the connections to keep (keeping top k connections)Clarity:Well written, easy to followDetailed commentsOverall, very interesting. Seemingly very simple idea that seem to work well. Table 2 does look impressive and it seems that it also reduces the overfiting, and the experiment with random labels on mnist seem to demonstrate that the method indeed preserves only connections relevant to the real labels, simplifying the architecture to a point when it cant just memorize the dataSeveral questions/critiques:- When you relax the binary constraints, it becomes an approximation to an optimization problem, any indication of how far you are off solving it this way? - For the initialization method of the weights, you seem to state that VS-H is the one to use. I wonder if it actually task dependent and architecture dependent. If yes, then the propose method still has a hyperparameter - how to initialize the weights initially- How does it compare with just randomly dropping the connections or dropping them based on the magnitude of the initial weights.  It seems that the meat comes from the fact that you are able to use the label and good initial values, i wonder if just doing a couple of iterations of forward-backprop and then dropping the weights based on their magnitude can give you comparable results - How does it compare to a distillation - it does not involve many cycles of retraining and can speed up inference time too-Can it replace the architecture search - initialize a large architecture, use the method to prune the connections and here you go. Did you try that instead of using already pre-tuned architectures like AlexNet. The paper provides a number of novel interesting theoretical results on "vanilla" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called "2 stage VAEs" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. Main theoretical contributions:1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1).In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered.2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5).In case when r &lt; d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r "informative" dimensions to produce the outputs perfectly landing on the true data manifold. Main algorithmic contributions:(0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. Review summary: I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. ****************** Couple of comments and typos:***************(0) Is the code / checkpoints going to be available anytime soon?(1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning.(2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. (3) It would be nice to specify the dimensionality of the Sz matrix in definition 1.(4) Line ater Eq. 3: I think it should be $\int p_gt(x) \log p_\theta(x) dx$ ?(5) Eq 4: p_\theta(x|x)(6) Page 4: "... mass to most all measurable...".(7) Eq 34. Is it sqrt(\gamma_t) or just \gamma_t?(8) Line after Eq 40. Why exactly D(u^*) is finite?I only checked proofs of Theorems 1 and 2 in details and those looked correct. [1] Lucic et al., 2018.[2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html[3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642 This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.Specifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of "depth bases".These values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.The paper is clear, well organized, well written and easy to follow.Even if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.In particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.The experiments are quite convincing and seem to clearly support the efficacy of the proposed method.I don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:1) In page 5, the authors write "learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations".I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.The word "guarantee" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences. The seminal work of Olshausen and Field on sparse coding is widely accepted as one of the main sources of inspiration for dictionary learning. This contribution makes the connection from dictionary learning back to a neuronal approach. Building on the Local Competitive Algorithm (LCA) of Rozell et al. and the theoretical analysis of Tang et al., this submission revisits the dictionary learning under two constraints that the gradient is learned locally and that the neural assemblies maintain consistent weight in the network. These constraints are relevant for a better understanding of the underlying principles in neuroscience and for applicative development on neuromorphic chipsets.The proposed theorems extend the previous work of sparse coding with spiking neurons and address the update of dictionary using only information available from local neurons. The submission cares as well for the possible implementation on parallel architectures. The numerical experiments are conducted on three datasets and show the influence of weight initialization and the convergence on each dataset. An example of image denoising is provided in appendix. Quality - Pro:   o This paper was in general a quality effort. It had a thorough bibliography of both older and recent relevant research contributions   o Providing useful, well done experimental results on four tasks was also a sign of this good thoroughness - Con: none observedClarity - Pro:   o The paper was generally well-written and clear. Results were clearly presented. - Con:   o Notwithstanding the half page of explanation of the intuition behind the new ON-LSTM update rules (top of p.5), it wasn't really enough for my old brain to get a good sense of what was going on  though I'm sure younger, smarter people will have made more sense of it. :) It would really help to try to provide more intuition and understanding here. Things that would probably really help include a worked example and diagrams.   o There were minor English/copyediting problems, but nothing that interfered with understanding. E.g., "monotonously" on p.4 should be "monotonically" (twice).Originality - Pro   o This was REALLY NEAT! This paper had a real, clear, different idea that appeared interesting and promising. That puts it into the top half of accepted papers right there.   o The basic idea of the different update time scales, done flexibly, controlled by the master forget/input gates seemed original, flexible, and good. - Con: Nothing really observed; there are clearly a bunch of slightly related ideas, well referenced in this paper.Significance - Pro   o If this idea pans out well, it would be a really interesting new structural prior to add to the somewhat impoverished vocabulary of successful techniques for building deep learning systems.   o Has an original, promising approach. That has the opportunity for impact and significance. - Con:   o The results so far are interesting, and in places promising, but not so clearly good that this idea doesn't need further evaluation of its usefulness.   o All the results presented are on small datasets (Penn Treebank WSJ (1 million words) size or smaller). What are the prospects on bigger datasets?  It looks like in principle this shouldn't be a big obstacle  except for not having a highly tuned CuDNN implementation, it looks like this should basically be fairly efficient like an LSTM and not hard to scale like, e.g., an RNNG.Other comments: - Some of the wording on page 1 seemed strange to me. Natural language has a linear overt form as spoken and (hence) written. It's really not that the sequential form is just how people conventionally "present" it. That is, it's not akin to a chemical compound which is really 3 dimensional but commonly "presented" by chemists in a convenient sequential notation. - p.2 2nd paragraph: Don't RNNs "explicitly impose a chain structure" not "implicitly"?!? - I wasn't sure I was sold on the name "Ordered Neurons". I'm not sure I have the perfect answer here, but it feels more like "multi-timescale units" is what is going on. - The LM results look good. - Because of all the different datasets, etc. it was a little hard to call the grammar induction results, but they at least look competently strong. - The stronger results on long dependencies in targeted syntactic evaluation look promising, but maybe you need a bigger hidden size so you can also do as well on short dependencies? - The logical inference results were promising  they seem to suggest that you capture some but not all of the value of explicit tree structure (a TreeLSTM) on a task like this. - The tree structures in Appendix A look promisingly good. The authors propose a modification of existing adaptive variants of SGD to avoid problems with generalization. It is known that adaptive gradient algorithms such as Adam tend to find good parameter values more quickly initially, but in the later phases of training they stop making good progress due to necessarily low learning rates so SGD often outperforms them past a certain point. The suggested algorithm Padam achieves the best of both worlds, quick initial improvements and good performance in the later stages.This is potentially a very significant contribution which could become the next state-of-the-art optimization method for deep learning. The paper is very clear and well-written, providing a good overview of existing approaches and explaining the specific issue it addresses. The authors have included the right amount of equations so that they provide the required details but do not obfuscate the explanations. The experiments consist of a comprehensive evaluation of Padam against the popular alternatives and show clear improvements over them.I have not evaluated the convergence theorem or its proof since this is not my area of expertise. One thing that stood out to me is that I don't see why theta* should be unique.Some minor suggestions for improving the paper:Towards the end of section 2 you mention a non-convergence issue of Adam. It would be useful to add a few sentences to explain exactly what the issue is.I would suggest moving the details of the grid search for p to the main text since many readers would be interested to know what's typically a good value for this parameter.Would it make sense to try to adapt the value of p, increasing it as the training progresses? Since that's an obvious extension some comment about it would be useful.On the bottom of page 6: "Figures 1 plots" -&gt; "Figure 1 plots".Make sure to protect the proper names in the bibliography so that they are typeset starting with uppercase letters. This is an excellent analysis paper of a very interesting phenomenon in deep neural networks.Quality, Clarity, Originality:As far as I know, the paper explores a very relevant and original question -- studying how the learning process of different examples in the dataset varies. In particular, the authors study whether some examples are harder to learn than others (examples that are forgotten and relearned multiple times through learning.) We can imagine that such examples are "support vectors" for neural networks, helping define the decision boundary.The paper is very clear and the experiments are of very high quality. I particularly appreciated the effort of the authors to use architectures that achieve close to SOTA on all datasets to ensure conclusions are valid in this setting. I also thought the multiple repetitions and analysing rank correlation over different random seeds was a good additional test.SignificanceThis paper has some very interesting and significant takeaways.Some of the other experiments I thought were particularly insightful were the effect  on test error of removing examples that aren't forgotten to examples that are forgotten more. In summary, the "harder" examples are more crucial to define the right decision boundaries. I also liked the experiment with noisy labels, showing that this results in networks forgetting faster.My one suggestion would be to try this experiment with noisy *data* instead of noisy labels, as we are especially curious about the effect of the data (as opposed to a different labelling task.)I encourage the authors to followup with a larger scaled version of their experiments. It's possible that for a harder task like Imagenet, a combination of "easy" and "hard" examples might be needed to enable learning and define good decision boundaries.I argue strongly for this paper to be accepted to ICLR, I think it will be of great interest to the community. Authors investigate the possibility to learn a generalized embedding function that captures salient and compositional features of sketches by directly imitating human sketches. The manuscript is written clearly and concisely. Methods have been presented with enough detail and seem accurate. Particularly, the results from the Quickdraw and Omniglot datasets showing generated sketches are rather impressive, and the ones for the natural images seem promising. Overall, I very much enjoyed reading the paper and suggest it for publication without any major changes.In my view the results presented in Figure 5, and especially 5C, are the most impressive and interesting ones. These results deserve more space in the manuscript. I was curious to know whether there were also many unsuccessful conceptual composition examples. Are the examples shown in Figure 5C the best ones, or are they representative of performance in general? Does this approach also work with natural images to any extent? Could the authors elaborate on why or why not this may be the case? Let's be frank: I have never been a fan of comparing real brains with back-prop trained multilayer neural networks that have little to do with real neurons.  For instance, I am unmoved when Figure 1 compares multilayer network simulations with experimental data on actual kitten. More precisely, I see such comparisons as cheap shots.However, after forgetting about the kitten,  I can see lots of good things in this paper.  The artificial neural network experiments designed by the authors show interesting phenomena in a manner that is amenable to replication. The experiments about the varied effects of different kinds of deficits are particularly interesting and could inspire other researchers in creating mathematical models for these striking differences.  The authors also correlate these effects with the two phases they observe in the variations of the trace of the Fisher information matrix.  This is reminiscent of Tishby's bottleneck view on neural networks, but different in interesting ways. To start with, the trace of the Fisher information matrix is much easier to estimate than Tishby's mutual information between patterns, labels, and layer activation. It also might represent something of a different nature, in ways that I do not understand at this point.In addition the paper is very well written, the comments are well though, and the experiments seem easy to replicate.Given all these qualities, I'll gladly take the kitten as well.. Originality: Existing attention models do not statistically express interactions among multiple attentions. The authors of this manuscript reformulate p(y|x) and define prior attention distribution (a_t depends on previous outputs y_&lt;t) and posterior attention distribution (a_t depends on current output y_t as well), and essentially compute the prior attention at current position using posterior attention at the previous position. The hypothesis and derivations make statistical sense, and a couple of assumptions/approximations seem to be mild. Quality: The overall quality of this paper is technically sound. It push forward the development of attention models in sequence to sequence mapping.Clarity: The ideas are presented well, if the readers go through it slowly or twice. However, the authors need to clarify the following issues: x_a is not well defined. In Section 2.2, P(y) as a short form of Pr(y|x_1:m) could be problematic and confusing in interpretation of dependency over which variables.  Page 3: line 19 of Section 2.2.1, should s_{n-1} be s_{t-1}?In Postr-Joint, Eq. (5) and others, I believe a'_{t-1} is better than a', because the former indicate it is attention for position t-1.I am a bit lost in the description of coupling energies. The two formulas for proximity biased coupling and monotonicity biased coupling are not well explained. In addition to the above major issues, I also identified a few minors: significant find -&gt; significant findingLast line of page 2: should P(y_t|y_&lt;t, a_&lt;n, a_n) be P(y_t|y_&lt;t, a_&lt;t, a_t)?top-k -&gt; top-Ka equally weighted combination -&gt; an equally weighted combinationSome citations are not used properly, such as last 3rd line of page 4, and brackets are forgotten in some places, etc.End of Section 3, x should be in boldface.non-differentiability , -&gt; non-differentiability,Full stop "." is missing in some places.Luong attention is not defined.Significance: comparisons with an existing soft-attention model and an sparse-attention model on five machine translation datasets show that the performance of using posterior attention indeed are better than benchmark models. The paper presents in interesting and new analysis of stability of scattering transforms on graphs, when the domain (graph) is perturbed by deformations. It combines key ingredients of scattering transforms (extended here to graphs through graph diffusion wavelets), deformation of graphs (based on graph diffusion distances) and a theoretical stability analysis. Similarly to the Euclidian domain, it is shown that linear filters cannot provide representations that are simultaneously rich and stable. Generally, the paper is pretty complete, interesting and sufficiently well presented. One might wonder if the choice of the diffusion framework for both the representation construction, and the deformation analysis, is a simplistic choice, and how similar ideas could extend to different domain deformation for example. The experiments and comparisons are also very minimal, and hard to interpret. Comparing things only with GFT and SVM is probability a 'easy' choice, with the advent of a plethora of new graph convnets architectures (GFT is probably not a 'graph baseline', as mentioned in the conclusion). This is however an interesting work, that will likely generate exciting discussions at ICLR. This paper presents and empirical and theoretical study of the convergence of asynchronous stochastic gradient descent training if there are delays due to the asynchronous part of it. The paper can be neatly split in two parts: a simulation study and a theoretical analysis.The simulation study compares, under fixed hyperparameters, the behavior of distributed training under different simulated levels of delay on different problems and different model architectures. Overall the results are very interesting, but the simulation could have been more thorough. Specifically, the same hyperparameter values were used across batch sizes and across different values of the distributed delay. Some algorithms failed to converge under some settings and others experienced dramatic slowdowns, but without careful study of hyperparameters it's hard to tell whether these behaviors are normal or outliers. Also it would have been interesting to see a recurrent architecture there, as I've heard much anecdotal evidence about the robustness of RNNs and LSTMs to asynchronous training. I strongly advise the authors to redo the experiments with some hyperparameter tuning for different levels of staleness to make these results more believable.The theoretical analysis identifies a quantity called gradient coherence and proves that a learning rate based on the coherence can lead to an optimal convergence rate even under asynchronous training. The proof is correct (I checked the major steps but not all details), and it's sufficiently different from the analysis of hogwild algorithms to be of independent interest. The paper also shows the empirical behavior of the gradient coherence statistic during model training; interestingly this seems to also explain the heuristic commonly believed that to make asynchronous training work one needs to slowly anneal the number of workers (coherence is much worse in the earlier than later phases of training). This quantity is interesting also because it's somewhat independent of the variance of the stochastic gradient across minibatches (it's the time variance, in a way), and further analysis might also show interesting results. 1/ Summary of the paperThis paper proposes a novel spatio-temporal graph scattering transform (ST-GST) as an intermediate representation of time-varying signals on graphs, which can then be fed into a simple ML model. Building on previously introduced Graph Scattering Transform (GST), the authors investigate the technical question of how to use both the spatial and temporal dimensions, proposing either a joint construction (GST applied on product graph) or a separate one (keeping both dimensions separate conceptually, while mixing). Theoretical results valid in both cases guarantee stability to noise in the input signal or the spatial graph.Numerical experiments show that the proposed method outperforms graph convolutional networks in the small data regime, and get a performance close to the state-of-the-art in the large data regime.2/ Acceptance decisionAccept. This is a very good paper with novel contributions on the algorithmic side, theoretical guarantees and convincing numerical experiments.3/ Supporting arguments- In terms of novelty, this paper is the first to tackle the investigate the construction of a fixed representation of spatio-temporal graphs based on priors, using the graph scattering transform approach.- In terms of technical contributions, the paper delves into the interesting technical question of how to build such a transform, a proposes convincing arguments to support its final approach, the separable ST-GST. Further, the authors provide theoretical guarantees of the stability of the ST-GST representation with respect to input and spatial graph variations, based on assumptions on the pre-defined filters used in the transform.- Experimental results show that in the very low data regime (MSR Action3D dataset with only 288 training samples), the proposed approach improves upon the graph convolutional networks SOTA by ~5% of accuracy (87 vs 82.2). In the large data regime, the proposed method reaches a performance close to SOTA (73.1 vs 75.8% accuracy). The authors also show that the gap between GCN and ST-GST widens as the number of samples decreases.- This paper is well motivated and well written. Although it is quite dense and with involved mathematical operations, it is quite easy to follow.4/ Additional comments- The authors do a very nice job of putting the joint and separable graph filters under similar notations (Eq. 1 and 2). When it comes to comparing both approaches at the end of Sec 3.2, I am not sure to understand correctly the statement that the separable approach is more « flexible » than the joint one. While I understand that separability brings an easier interpretability of filter responses in the spatial and temporal domain, it seems also that in terms of polynomials with atoms of the form $\lambda_s^p \otimes \Lambda_t^q$ for various $p$, $q$, the family spanned by separable transforms might be smaller than e.g. the strong product (although Im not sure). Do you have any more insights in this direction?- In the numerical results of Table 1, the separable approach gets the better results with much more scales in the temporal dim (20) than in the spatial one (5). In the joint approaches, a single number of scales (15) is used, and it spans both dimensions. Beyond the computational requirements of the joint approach, do you think this forced equality is an issue for the joint approach?- In order to compare the significance of results in the low-data regime, standard deviations would have been appreciated, especially to understand the differences between the different joint approaches.- Minor details: which graph shift is used in the experiments reported in the last page of the main paper? In Section 3.2, the authors state $S = A$ (for both spatial, temporal and joint cases), but appendix A defines $S$ as a lazy random walk for geometric graph wavelets and Appendix C states that geometric graph wavelets are used in the main paper. Similarly, which non linear activation is used eventually? This paper presents a very strong combined attack method, where infectedtraining examples are crafted such the the trojan backdoor becomes verydifficult to detect.  I feel their approach to be relevant, informativeand presents a significant advance.The paper focuses on the mechanisms of cleverly disguising the Trojan trainingdata, and does an excellent evaluation.  The attack is particularly importantin some online training scenarios, where one might wish to use non-trustedtraining data. The evaluation is extensive, covering many existing and potential defenses,with appendices covering different trojan types, trojan intensities, and attackdetectability, etc.  Readbility, and organization between main paper and appendix material was good.Given the strength of the attack, what practical implications does this have?Appendix A addresses this for 2 cases that *fail* to defend, but a question Ifound important was what steps would make the attack harder?  For example, could App. A case (2) make the adversarial component more difficult if somelayers have parameters unknown, perhaps on a secure compute platform?Does this have implications for how future compute platforms are set up?p.7: Anomaly Index was actually *not* defined in appendix F, but in Wang et al.This is a well-presented paper, with extensive experimental investigationand should be published.  1. Summary of this paper    - The topic of this paper is multi-hop QA, which studies answering complex natural language questions. Complex questions require an information aggregation process across multiple documents and recent multi-hop QA models design this process by sequentially retrieving relevant documents (Asai 2020 et al.). This paper alleviates two problems in recent multi-hop QA models. One is that recent multi-hop QA models require external knowledge such as Wikipedia hyperlinks. This problem results in the models' low generalization ability on new domains that the external knowledge is no longer available. The other problem is computational efficiency. The authors propose a novel multi-hop QA model named MDR that does not require external knowledge and is ten times faster than the recent models. MDR uses question reformulation and MIPS. Question reformulation design the information aggregation process by iteratively generating a query vector related to the documents that should be accompanied to answer the original question. MDR generates such query vectors by comparing the given question and previously retrieved documents. MDR encodes passages in a large corpus(indexing) with the same encoder used in the question reformulation process and uses MIPS to find relevant documents with the generated query vectors. In experiments, the authors show that MDR outperforms recent multi-hop QA models, and also they show the computational efficiency of MDR.2. Strong and weak points of this paper    - Strong points        - This paper provides a detailed analysis of their method. Experimental results show the validity of the proposed method, and some strong findings described below.            - Table 2 confirms that MDR outperforms "Graph Rec Retriever (Asai et al.)". This result shows the feasibility of a more accurate multi-hop QA model without external knowledge such as Wikipedia hyperlinks.            - Table 3 shows a detailed analysis of each component in MDR. This table indicates several vital features for multi-hop QA models that can be easily ignored in the model design process. The experimental results on "w/o order" and "w/o linked negatives" show significant findings in multi-hop QA.            - Table 4 shows that the question reformulation method (MDR) has similar performance to the question decomposition method with human-annotated sub-questions.            - Table 5 shows the end-to-end performance of multi-hop QA models. MDR outperforms existing state-of-the-art multi-hop QA models.        - The proposed method is computationally efficient.        - The proposed method is simple. Many follow-up studies based on the proposed method are expected.        - The experimental results support their claim.    - Weak points        - This paper does not mention the publicly available code of their method. It would be nice if the authors provide implementations after the decision process.        - In the section "Question Decomposition for Retrieval," the authors conclude that question decomposition is unnecessary in the context of dense retrieval with a strong pretrained encoder. However, Table 4 shows that question decomposition with a simple open-domain QA model has a similar performance to MDR. These results indicate that question decomposition is an effective method to make simple single-hop open-domain QA models used in multi-hop QA. Please provide more evidence for the conclusion, "unnecessity of question decomposition."3. Recommendation    - Accept    - This paper provides several significant findings that are expected to be referred to by many other studies. Their method is simple and outperforms other multi-hop QA models. Also, it is computationally efficient.4. Questions    - In Table 4, the Decomp method is based on DPR (dense passage retriever). What will be the results if MDR uses the gold sub-questions? Does using sub-questions in MDR increase retriever performance?    - Please provide the number of hard negative samples for a question.    - In section 2.2, what is the start token in the sentence " Specifically, we apply layer normalization over the start tokens representations from RoBERTa to get the final dense query/passage vectors." Is the start token pooled_output of the CLS token or hidden representation of the CLS token? Summary: This paper focuses on better learning of video-text representations. To this end, the paper introduces a new generative task of cross-captioning which alleviates the typical issue of contrastive learning by learning to reconstruct a samples text representation as a weighted combination of a video support set. The proposed approach performs better than previous work on various datasets. Pros: 1) The paper is well written and easy to follow. 2) The proposed method sounds novel and interesting. The empirical evaluations on various datasets suggest that the proposed method is better. 3) The ablations on various modules of the proposed method is very interesting and thorough. Cons:1) The current approach limits to using only the videos in the current batch for the support set. One could also try retrieving the support set from the full dataset in an online way. It would be interesting to see this ablation. Overall:The proposed method of cross-captioning is novel and the thorough empirical evaluations/ablations further show the superiority of the proposed method as well as the usefulness of each component.  Questions: 1) Please provide statistical significance scores wherever necessary, e.g., Table-4 ours vs ours-pretrained difference is statistically significant? 2) Is it possible to ablate on the choice of support set from within a batch vs. full dataset? The authors propose a deep network approach using the principle of rate reduction as a loss under a gradient ascent approach, avoiding traditional backpropagation. Besides, the work attempts to interpret the proposed framework from both geometrical and statistical views. Then, shift-invariant properties are discussed. The innovative method allows the inclusion of a new layer structure named ReduNet, which could benefit the deep learning community. Though the experiments are not challenging concerning the studied databases, the authors aim to probe the concept without a complete implementation tuning. Overall, the paper is illustrative enough regarding the mathematical foundation. The authors tackle the problem of self-supervised representation learning, and validate their approach on downstream Reinforcement Learning tasks. Building on the insight that predictability in local patches is a good inductive bias for salient regions that characterize objects, the authors propose a well-reasoned, well-engineered and thoroughly validated pipeline for learning object keypoints without supervision. The authors present a wide range of ablative studies to validate their design choices, and demonstrate the superiority of their method both illustratively as well as quantitatively on a number of standard Atari benchmarks. The paper is very well written and clearly explained, with the assumptions clearly stated, and design choices thoroughly validated through ablative studies. Overall the authors make a very compelling argument for using local predictability as an intrinsic property of objects, leading me to recommend accepting this paper for publication.Pros:+ The intro motivates the problem well, contrasting the proposed method with a number of key recent methods. The implementation details are well recorded in the supplementary, with the added mention of releasing the source code + The keypoint detection pipeline is well reasoned and well explained: using the error map obtained through the spatial prediction task to recover keypoints via a bottleneck with limited capacity is a neat idea. The authors ablate a number of design choices (number of keypoints, which encoder layers to use); Figure 1 and 2 are great at showing the high-level components of the method as well as (intermediate) outputs+ The comparison against Transporter is thorough and well analyzed. Fig2.b. provides a very clear insight into the limitations of Transporter, showing that the method proposed by the authors is able to achieve some robustness to visual distractors. PKey-CNN uses a similar method as Transporter for encoding keypoint features for downstream tasks, and thus serves to show that the keypoints identified are indeed superior. PKey-GNN further increases performance on a number of Atari games. + Very good ablative analysis and qualitative examples.Some questions:+ Do the authors have any further insights regarding why PKey-GNN would perform worse than PKey-CNN? While the authors reasoning makes sense, in my understanding a GNN based approach should be able to model any kind of interaction. + The authors demonstrate impressive results on a number of Atari games. I am wondering how this method would perform on a slightly more complex environment, i.e. CarRacing in OpenAIs gym environment, or maybe even going as far as CARLA?+ As I understand, PermaKey is first trained on Atari game rollouts, with the policy trained afterwards. Would it be possible to optimize both the keypoints and the policy together, end-to-end? The authors provide a compelling theoretical explanation for a large class of adversarial examples.  While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work.I can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling.  More data here would be great!)As much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)? This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. The regularisation enforces smoothness on a graph built on the different features at different layers of the NN system. The proposed ideas are quite interesting, and integrates nicely into NN architectures. A few paths for improvements:- the 'optimal' choice of the power of the Laplacian, in 3.5, is eluded- the figures are not presented ideally, nor in a very readable form - for example, their are 90-degree rotated compared to classical presentations, and the plots are hardly readable- the might exist a tradeoff between robustness, and performance (accuracy), that seem to be explaining the proposed results (see Fawzi - Machine Learning 2018, for example)- in 4.2, what is a mean case of adversarial noise? Also, it would be good to see the effect of the regularizer of both the 'original' network, and on the network trained with data augmentation. It is not clear which one is considered here, but it would be interesting to study both, actually. - the second paragraph of the conclusion (transfer of perturbations) opens interesting perspective, but the problem might not be as trivial as the authors seem to hint in the text. Overall, very interesting and nice work, which might be better positioned (especially in terms of experiments) wrt to other recent methods that propose to improve robustness in NNs. The problem setting considered in this paper is that of the recent wave of "goal-conditioned" formulations for hierarchical control in Reinforcement Learning. In this problem, a low-level controller is incentivized to reach a goal state designated by a higher-level controller. This goal is represented in an abstract (embedding) multi-dimensional vector space. Establishing "closeness to goal" entails the existence of some distance metric (assumed to be given an fixed) and a function $f$ which can project states to their corresponding goal representation. The "representation learning" problem referred to by the authors pertains to this function. The paper is built around the question: how does the choice of $f$ affects the expressivity of the class of policies induced in the lower level controller, which in turn affects the optimality of the overall system. The authors answer this question by first providing a bound on the loss of optimality due to the potential mismatch between the distribution over next states under the choice of primitive actions produced by a locally optimal low-level controller. The structure of the argument mimics that of model compression methods based on bisimulation metrics. The model compression here is with respect to the actions (or behaviors) rather than states (as in aggregation/bismulation methods). In that sense, this paper is a valuable contribution to the more general problem of understanding the nature of the interaction between state abstraction and temporal abstraction and where the two may blend (as discussed by Dietterich and MAXQ or Konidaris for example). Using the proposed bounds as an objective, the authors then derive a gradient-based algorithm for learning a better $f$. While restricted to a specific kind of temporal abstraction model, this paper offers the first (to my knowledge) clear formulation  of "goal-conditioned" (which I believe is an expression proposed by the authors) HRL fleshed out of architectural and algorithmic considerations. The template of analysis is also novel and may even be useful in the more general SMDP/options perspective. I recommend this paper for acceptance mostly based on this: I believe that these two aspects will be lasting contributions (much more than the specifics of the proposed algorithms). # Comments and QuestionsIt's certainly good to pitch the paper as a "representation learning" paper at a Representation Learning conference, but I would be careful in using this expression too broadly. The term "representation" can mean different things depending on what part of the system is considered. Representation learning of the policies, value functions etc. I don't have specific recommendations for how to phrase things differently, but please make sure to define upfront which represention you are referring to. Representation learning in the sense of let's say Baxter (1995) or Minsky (1961) is more about "ease of learning" (computation, number of samples etc) than "accuracy". In the same way, one could argue that options are more about learning more easily (faster) than for getting more reward (primitive options achieve the optimal). Rather than quantifying the loss of optimality, it would be interesting to also understand how much one gains in terms of convergence speed for a given $f$ versus another. I would like to see (it's up to you) this question being discussed in your paper. In other words, I think that you need to provide some more motivation as to why think the representation learning of $f$ should be equated with the problem of maximizing the return. One reason why I think that is stems from the model formulation in the first place: the low-level controller is a local one and maximizes its own pseudo-reward (vs one that knows about other goals and what the higher level controller may do). It's both a feature, and limitation of this model formulation; the "full information" counterpart also has its drawbacks.A limitation of this work is also that the analysis for the temporally extended version of the low-level controller is restricted to open-loop policies. The extension to closed-loop policies is important. There is also some arbitrariness in the choice of distance function which would be important to study. Relevant work (it's up to you to include or not): - Philip Thomas and Andrew Barto in "Motor Primitive Discovery" (2012) also talk about options-like abstraction in terms of compression of action. You may want to have a look. - Still and Precup (2011) in "An information-theoretic approach to curiosity-driven reinforcement learning" also talk about viewing actions as "summary of the state" (in their own words). In particular, they look at minimizing the mutual information between state-action pairs. - More generally, I think that the idea of finding "lossless" subgoal representations is also related to ideas of "empowerment" (the line of work of Polani). Graph Neural Networks(GNN) are gaining traction and generating a lot of interest. In this work, the authors apply them to the community detection problem, and in particular to graphs generated from the stochastic block model. The main new contribution here is called "line graph neural network" that operate directly over the edges of the graph, using efficiently the power of the "non backtracking operator" as a spectral method for such problems.Training such GNN on data generated from the stochastic block model and other graph generating models, the authors shows that the resulting method can be competitive on both artificial and real datasets.This is definitely an interesting idea, and a nice contribution to GNN, that should be of interest to ICML folks.References and citations are fine for the most part, except for one very odd exception concerning one of the main object of the paper: the non-backtracking operator itself! While discussed in many places, no references whatsoever are given for its origin in detection problems. I believe this is due to (Krzakala et al, 2013) ---a paper cited for other reasons--- and given the importance of the non-backtracking operator for this paper, this should be acknowledged explicitly.Pro: Interesting new idea for GNN, that lead to more powerful method and open exciting direction of research. A nice theoretical analysis of the landscape of the graph. Con:The evidence provided in Table 1 is rather weak. The hard phase is defined in terms of computational complexity (polynomial vs exponential) and therefore require tests on many different sizes. This paper presents a straightforward looking approach for creating a neural networks that can run under different resource constraints, e.g. less computation but lower quality solution and expensive high quality solution, while all the networks are having the same filters. The idea is to share the filters of the cheapest network with those of the larger more expensive networksa and train all those networks jointly with weight sharing. One important practical observation is that the batch-normalization parameters should not be shared between those filters in order to get good results. However, the most interesting surprising observation, that is the main novelty of the work that even the highest quality vision network get substantially better by this training methodology as compared to be training alone without any weight sharing with the smaller networks, when trained for object detection and segmentation purposes (but not for recognition). This is a highly unexpected result and provides a new unanticipated way of training better segmentation models. It is especially nice that the paper does not pretend that this phenomenon is well understood but leaves its proper explanation for future work. I think a lot of interesting work is to be expected along these lines. Mode collapse in the context of GANs occurs when the generator only learns one of the multiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.This paper is positioned in the context of Bayesian GANs (Saatsci &amp; Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.The authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. For the inference, they adapt the Stochastic Gradient HMC used by Saatsci &amp; Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.Overall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.Some typos:- The authors argue that compare to point mass...+ The authors argue that, compared to point mass...- Theorem 1 states that any the ideal generator+ Theorem 1 states that any ideal generator- Assume the GAN objective and the discriminator space are symmetry+ Assume the GAN objective and the discriminator space have symmetry- Eqn. 8 will degenerated as a Gibbs sampling+ Eqn. 8 will degenerate as a Gibbs sampling This paper presents an approach to exploration in RL via random network distillation.The agent generates a random neural network, and adds an "intrinsic reward" based on the regression error of this random function.The main evidence for its efficacy comes from evaluation on Atari games, particularly Montezuma's revenge, where it attains state of the art results.There are several things to like about this paper:- The writing is clear and well thought out. - The actual algorithm is sensible, simple, intuitive and clearly effective.- The results are significant: this is really a "step change" compared to previous Montezuma results.- This work takes the well-known "exploration bonus" approach, combines it with some of the observations of (Osband et al) and simplifies the treatment... so in some ways it's quite standard... but there are several new insights:  + Focus on normalization schemes for "randomized prior function"  + Bootstrapping "intrinsic reward" over episode boundaries  + Incorporating large-scale policy-based algorithmsTo help improve the paper, I will highlight some potential issues:- For a paper on exploration, it does not make sense to present results in terms of "parameter updates". This should instead be presented in terms of actor/environment steps. This is something that happens consistently across the paper. If you want to show that "many actors makes it better" then you can divide this by #actors... so that the curves still functionally look the same. This is an easy thing to change... but I think it's important to do this!- Like other "count-based" methods, this exploration bonus is not linked to the task. As such, you have to get "lucky" that you do the right kind of generalization from the "random network". I think that you should mention this issue, potentially in your section 2. That is not to say that this is therefore a bad method, but particularly with reference to (Osband et al 2018) this approach does not address their observation from Section 2.4 of that paper... you don't necessarily get the "right" type of generalization from this random network (that has nothing to do with the task). You could then point out that, empirically, using a random convnet seems to do just fine in Atari! ;D- The whole section about "pure exploration" is somewhat interesting, but you shouldn't assess that performance in terms of "reward"... because that is just a peculiarity of these games... we could easily imagine a game where "pure exploration" gives a huge negative reward... but that wouldn't mean that it was bad at pure exploration! Therefore, how can you justify the quality of pure exploration by reference to the "best return".- Although the paper is definitely good, and I've already outlined several truly novel additions from this paper, on another level the actual intellectual contribution of this paper is perhaps not *as* large as it may seem from the Abstract or associated OpenAI publicity/blog posts https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/  + This paper is about adding an "exploration bonus" to RL rewards (this goes back at least to Kearns+Singh 2002)  + The form of this bonus comes from prediction error on a random function  + I have some concerns on the process of "anonymous" reviews in this "blog+tweet" settingOverall, I like the paper a lot, I think it must be accepted and also it's right at the top of ICLR best papers!The writing is good, the results are good, the algorithm is good and I think it will have impact.The main missing piece is a clear discussion of any of the algorithms potential weaknesses - is this the final solution to exploration? What do you think about the issues of generalization? How would this perform in a linear system? What if the basis functions are not aligned?It's not that the algorithm needs to address all of these things to be a good algorithm, but the paper should try to do a better job about highlighting any potential missing pieces - particularly when the results are so impressive. The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.The authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.I find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  More broadly, there's an additional axis to the optimization problem which is "How much does the training scheme know about the particulars of the problem?", ranging from "Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)" to "knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)" to "knows a little bit about the problem structure, and uses hyperparameter tuned ADAM" to "knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD".Model completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.An excellent contribution, and I'm excited to see follow-up work.* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point. This paper aims to distinguish between networks which memorize and those with generalize by introducing a new detection method based on NMF. They evaluate this method across a number of datasets and provide comparisons to both PCA and random ablations (as in Morcos et al., 2018), finding that NMF outperforms both. Finally, they show that NMF is well-correlated with generalization error and can be used for early stopping. This is an overall excellent paper. The writing is clear and and focused, and the experiments are careful and rigorous. The discussion of prior work is thorough. The question of how to detect memorization in DNNs is one of great interest, and this makes nice steps towards this goal. As such, it will likely have significant impact.  Major comments:1) The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.Minor comments: 1) While the comparisons to random ablations are mostly fair, it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA. 2) The error bars on the plots are often hard to see. Increasing the transparency somewhat would be helpful.Typos: 1) Section 1, third paragraph: We show that networks that networks that generalize& should be We show that networks that generalize...2) Section 3.1, third paragraph: Because threshold is the& should be Because thresholding is the&3) Section 3.2, third paragraph: In the most non-linear case we would& should be In the most non-linear case, we would&4) Figure 2 caption: ...with increasing level of& should be ...with increasing levels of&5) Section 4.1.1, second to last line of last paragraph: missing space before final sentence6) Figure 4a label: Fahsion-MNIST should be Fashion-MNIST In this work the authors prove several claims regarding the inductive bias of gradient descent and gradient flow trained on deep linear networks with linearly separable data. They show that asymptotically gradient descent minimizes the risk, each weight matrix converges to its rank one approximation and the top singular vectors of two adjacent weight matrices align. Furthermore, for the logistic and exponential loss the induced linear predictor converges to the max margin solution. This work is very interesting and novel. It provides a comprehensive and exact characterization of the dynamics of gradient descent for linear networks. Such strong guarantees are essential for understanding neural networks and extremely rare in the realm of non-convex optimization results. The work is a major contribution over the paper of Gunasekar et al. (2018) which assume that the risk is minimized. The proof techniques are interesting and I believe that they will be useful in analyzing neural networks in other settings.Regarding Lemma 3, the proof is not clear. Lemma 8 does not exist in the paper of Soudry et al. (2017). It is also claimed that with probability 1 there are at most d support vectors. How does this relate with assumption 3, which implies that there are at least d support vectors? This paper does not even try to propose yet another "vacuous" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some "prescriptive" insights (per Sanjeev Arora) towards understanding generalization in deep neural nets.I have no major complaints but for a few questions regarding clarifications,1. From Eq.(5), such distances are defined for only one out of the many possible pairs of labels. So when forming the so-called "margin signature", how exactly do you compose it from all such pair-wise distances? Do you pool all the distances together before computing the statistics, or do you aggregate individual statistics from pair-wise distances? And how do you select which pairs to include or exclude? Are you assuming "i" is always the ground-truth label class for $x_k$ here?2. In Eq.(3), the way you define the distance (that flipping i and j would change the sign of the distance) is implying that {i, j} should not be viewed as an unordered pair, in which case a better notation might be (i, j) (i.e. replacing sets "{}" with tuples "()" to signal that order matters).And why do you "only consider distances with positive sign"? I can understand doing this for when neither i nor j corresponds to the ground-truth label of x, because you really can't tell which score should be higher. But when i happens to be the ground-truth label, wouldn't a positive distance and a negative distance be meaningful different and therefore it should only be beneficial to include both of them in the margin samples?And a minor typo: In Eq.(4), $\bar{x}_k$ should have been $\bar{x}^l$? Building on the recent progresses in the analysis of random high-dimensional statistics problem and in particular of message passing algorithm, this paper analyses the performances of weighted tied auto-encoder.  Technically, the paper is using the state evolution formalism. In particular the main theorem uses the analysis of the multi-layer version of these algorithm, the so-called state evolution technics, in order to analyse the behaviour of optimal decoding in weight-tied decoder. It is based on a clever trick that the behaviour of the decoding is similar to the one of the reconstruction on a multilayer estimation problem. This is a very orginal use of these technics.The results are 3-folds: (i) a deep analysis of the limitation of weight-tied DAE, in the random setting, (ii) the demonstration of the sensitivity to perturbations and (iii) a clever method for initialisation that  to train a DAE.Pro: a rigorous work, a clever use of the recent progresses in rigorous analysis of random neural net, and a very deep answer to interesting questions, and Con: I do not see much against the paper. A minor comment: the fact that the DAE is "weight-tied" is fundamental in this analysis. It actually should be mentioned in the title! This is a good paper. First of all, it presents a large-scale corpus for visual speech recognition. Second, it demonstrates a visual speech recognition system based on open-vocabulary that gives the state-of-the-art recognition accuracy.  The paper is very well written and all the technical details are clearly laid out.  I, for one, would like to thank the authors for this meticulous work to the community.   This is by far the largest dataset and the most impressive performance for VSR I have even seen in the ASR/VSR community.  I enjoyed reading this paper.   This paper introduces new benchmarks for measuring the robustness of computer vision models to various image corruptions. In contrast with the popular notion of adversarial robustness, instead of measuring robustness to small, worst-case perturbations this benchmark measures robustness in the average case, where the corruptions are larger and more likely to be encountered at deployment time. The first benchmark Imagenet-C consists of 15 commonly occurring image corruptions, ranging from additive noise, simulated weather corruptions, to digital corruptions arising from compression artifacts. Each corruption type has several levels of severity and overall corruption score is measured by improved robustness over a baseline model (in this case AlexNet). The second benchmark Imagenet-P measures the consistency of model predictions in a sequence of slightly perturbed image frames. These image sequences are produced by gradually varying an image corruption (e.g. gradually blurring an image). The stability of model predictions is measured by changes in the order of the top-5 predictions of the model. More stable models should not change their prediction to minute distortions in the image. Extensive experiments are run to benchmark recent architecture developments on this new benchmark. Its found that more recent architectures are more robust on this benchmark, although this gained robustness is largely due to the architectures being more accurate overall. Some techniques for increasing model robustness are explored, including a recent adversarial defense Adversarial Logit Pairing, this method was shown to greatly increase robustness on the proposed benchmark. The authors recommend future work benchmark performance on this suite of common corruptions without training on this corruptions directly, and cite prior work which has found that training on one corruption type typically does not generalize to other corruption types. Thus the benchmark is a method for measuring model performance to unknown corruptions which should be expected during test time.In my opinion this is an important contribution which could change how we measure the robustness of our models. Adversarial robustness is a closely related and popular metric but it is extremely difficult to measure and reported values of adversarial robustness are continuously being falsified [1,2,3]. In contrast, this benchmark provides a standardized and computationally tractable benchmark for measuring the robustness of neural networks to image corruptions. The proposed image corruptions are also more realistic, and better model the types of corruptions computer vision models are likely to encounter during deployment. I hope that future papers will consider this benchmark when measuring and improving neural network robustness. It remains to be seen how difficult the proposed benchmark will be, but the authors perform experiments on a number of baselines and show that it is non-trivial and interesting. At a minimum, solving this benchmark is a necessary step towards robust vision classifiers. Although I agree with the authors recommendation that future works not train on all of the Imagenet-C corruptions, I think it might be more realistic to allow training on a subset of the corruptions. The reason why I mention this is its unclear whether or not adversarial training should be considered as performing data augmentation on some of these corruptions, it certainly is doing some form of data augmentation. Concurrent work [4] has run experiments on a resnet-50 for Imagenet and found that Gaussian data augmentation with large enough sigma (e.g. sigma = .4 when image pixels are on a [0,1] scale) does improve robustness to pepper noise and Gaussian blurring, with improvements comparable to that of adversarial training. Have the authors tried Gaussian data augmentation to see if it improves robustness to the other corruptions? I think this is an important baseline to compare with adversarial training or ALP.Few specific comments/typos:Page 2 l infinity perturbations on small imagesThe (Stone, 1982) reference is interesting, but its not clear to me that their main result has implications for adversarial robustness. Can the authors clarify how to map the L_p norm in function space of ||T_n - T(theta) || to the traditional notion of adversarial robustness?1. https://arxiv.org/pdf/1705.07263.pdf2. https://arxiv.org/pdf/1802.00420.pdf3. https://arxiv.org/pdf/1607.04311.pdf4. https://openreview.net/forum?id=S1xoy3CcYX&amp;noteId=BklKxJBF57 Summary: This paper observes that a major flaw in common image-classification networks is their lack of robustness to common corruptions and perturbations. The authors develop and publish two variants of the ImageNet validation dataset, one for corruptions and one for perturbations. They then propose metrics for evaluating several common networks on their new datasets and find that robustness has not improved much from AlexNet to ResNet. They do, however, find several ways to improve performance including using larger networks, using ResNeXt, and using adversarial logit pairing.Quality: The datasets and metrics are very thoroughly treated, and are the key contribution of the paper. Some questions: What happens if you combine ResNeXt with ALP or histogram equalization? Or any other combinations? Is ALP equally beneficial across all networks? Are there other useful adversarial defenses?Clarity: The novel validation sets and reasoning for them are well-explained, as are the evaluation metrics. Some explanation of adversarial logit pairing would be welcome, and some intuition (or speculation) as to why it is so effective at improving robustness.Originality: Although adversarial robustness is a relatively popular subject, I am not aware of any other work presenting datasets of corrupted/perturbed images.Significance: The paper highlights a significant weakness in many image-classification networks, provides a benchmark, and identifies ways to improve robustness. It would be improved by more thorough testing, but that is less important than the dataset, metrics and basic benchmarking provided.Question: Why do authors do not recommend training on the new datasets? This paper presented a novel kernel decomposition for nonsymmetric determinantal point processes, which enables linear time of inference and learning w.r.t. the cardinality of the ground set M. This is a significant improvement over previous arts and makes NDPP practical in relatively large datasets. The theoretical of the paper is solid and supportive to the main claim of the paper. This paper is well written and easy to follow. Even for readers without much theoretical background, this paper is still moderately friendly since the logic and insight are clear. I vote this paper a strong acceptance, except for some minor concerns as follows:1) The authors employed a way to simplify the kernel decomposition below Theorem 1. However, it is unclear what the impact of this simplification is to the exactness of learning or inference. It would me a plus if the authors can give some theoretical analysis on the gap between such simplification and $P_0^+$.2) I notice the authors provide both learning and inference procedures for NDPP. It seems these two procedures can formulate a way to learn latent NDPP in an Expectation-Maximization fashion. I would suggest the authors give some discussion about the feasibility of such integration, and this can be an interesting direction.In general, I enjoy reading this paper and think this paper is insightful. This manuscript contributes a new online gradient descent algorithm with adaptation to local curvature, in the style of the Adam optimizer, ie with a diagonal reweighting of the gradient that serves as an adaptive step size. First the authors identify a limitation of Adam: the adaptive step size decreases with the gradient magnitude. The paper is well written.The strengths of the paper are a interesting theoretical analysis of convergence difficulties in ADAM, a proposal for an improvement, and nice empirical results that shows good benefits. In my eyes, the limitations of the paper are that the example studied is a bit contrived and as a results, I am not sure how general the improvements.# Specific comments and suggestionsUnder the ambitious term "theorem", the results of theorem 2 and 3 limited to the example of failure given in eq 6. I would have been more humble, and called such analyses "lemma". Similarly, theorem 4 is an extension of this example to stochastic online settings. More generally, I am worried that the theoretical results and the intuitions backing the improvements are built only on one pathological example. Are there arguments to claim that this example is a prototype for a more general behavior?Ali Rahimi presented a very simple example of poor perform of the Adam optimizer in his test-of-time award speech at NIPS this year (https://www.youtube.com/watch?v=Qi1Yry33TQE): a very ill-conditioned factorized linear model (product of two matrices that correspond to two different layers) with a square loss. It seems like an excellent test for any optimizer that tries to be robust to ill-conditioning (as with Adam), though I suspect that the problem solved here is a different one than the problem raised by Rahimi's example.With regards to the solution proposed, temporal decorrelation, I wonder how it interacts with mini-batch side. With only a light understanding of the problem, it seems to me that large mini-batches will decrease the variance of the gradient estimates and hence increase the correlation of successive samples, breaking the assumptions of the method.Using a shared scalar across the multiple dimensions implies that the direction of the step is now the same as that of the gradient. This is a strong departure compared to ADAM. It would be interesting to illustrate the two behaviors to optimize an ill-conditioned quadratic function, for which the gradient direction is not a very good choice.The performance gain compared to ADAM seems consistent. It would have been interesting to see Nadam in the comparisons.I would like to congratulate the authors for sharing code.There is a typo on the y label of figure 4 right. This paper proposes some modifications to established procedures for neural speech synthesis and investigates their effect experimentally. The proposed modifications are mostly fairly straightforward conceptually, but appear to work well, and this reviewer feels the paper has huge value in its experimental contributions extending and clarifying certain aspects of WaveNet training and distillation. The paper is well-written and fairly concise, with a short-and-sweet experimental results section.Major comments:The conceptual novelty seems a little overstated in the abstract. For example, the value seems to not really be in the "proposing" a text-to-wave neural architecture for speech synthesis (which, aside from important experimental tweaks, is essentially Tacotron 2 training all parameters from scratch) but in showing that it works well experimentally. Conceptually the paper is extremely close to the parallel wavenet paper, the main differences being slightly different component distributions (Gaussian instead of logistic), a different set of loss terms in addition to the reverse KL, and joint training of the spectral synthesis and waveform synthesis parts of the model.It would be super insightful to include log probabilities on the test set (everywhere MOS results have been reported) in the experimental results. This would help tease apart the effects of architecture inductive bias, different divergences, distillation, etc. One of the really nice things about flow-based models is the ability to compute the log probability tractably.Minor comments:Perhaps mention that teacher forcing is maximum likelihood in the introduction? Currently it almost sounds like the paper is contrasting teacher forcing for WaveNet (paragraph 2) and MLE (list item 1).At the end of paragraph 3 in the introduction, it would be helpful to mention that the intractable KL divergence being referred to is the frame-level one-step-ahead predictions, not the entire sequence-level prediction. Also, for 1D distributions isn't taking a large number of samples quite effective in practice?In introduction list item 3, suggest mentioning Tacotron 2 (Shen et al) and contrasting with the present work for clarity.In section 3.1, it surprises me slightly that clipping at -7 is essential. It would be helpful to state what exactly goes wrong if this is not done. Does it lead to overfitting and so bad test log likelihoods? What effect is noticeable in the generated samples?Equation (6) is incorrect. It should be conditioned on &lt; t, not &lt;= t. Conditioning on z &lt;= t would make x_t deterministic.Equation (7) is technically true as written, but only because all the distributions involved are deterministic. If &lt;= t is replaced with &lt; t (which based on the mistake in (6) is what I suspect the authors intended) then it is no longer true. This equation is not used anywhere as far as I can tell. It seems to me like the property that enables non-recursive-over-time ("parallel") sampling is (5), not (7). Incidentally, when multiple one-step-ahead samples are taken per frame for parallel wavenet, the samples viewed at the sequence level are highly correlated, and do not obey anything like (7), but it doesn't affect the correctness of the expected value.The IAF doesn't really "infers its output x at all time steps". Maybe "models" instead of "infers"?Learning an "IAF directly through maximum likelihood" doesn't seem all that impractical. People train networks with recursive dependence such as RNNs (which is essentially what would be required to train certain forms of IAF with MLE) as opposed to non-recursive dependence such as CNNs all the time, after all. It seems like this claim depends on the details of the transform $f$.Out of interest, did the authors consider reversing the sequence being generated in time between successive IAF blocks? This would limit the ability to do low latency synthesis but might improve performance considerably.The first paragraph in section 3.3 seems like it should probably be part of section 3.3.1 (it's not related to other losses such as spectrogram frame loss, for example). It would be helpful to state explicitly that: (a) the goal is to minimize the sequence-level reverse KL; (b) this can be approximated by taking a single sample z, but this may have high variance; (c) the variance of this estimate can be reduced by marginalizing over the one-step-ahead predictions for each frame; (d) parallel wavenet's mixture of logistics means it has to use a separate Monte Carlo sampling at the frame-level, whereas the proposed Gaussian allows this one-step-ahead marginalization to be performed analytically. This one-step-ahead marginalization is an example of Rao-Blackwellization.It didn't seem clear from section 3.3 and 3.3.1 that parallel wavenet also uses the one-step-ahead marginalization trick to reduce the variance.It might be helpful to mention that using the reverse KL would be expected to have mode-fitting behavior, making samples sound better but log probability on the test set worse.It was not clear to me what difference or similarity was being demonstrated in Figure 1.Small point, but "Oord et al" should be "van Oord et al" throughout (it's a surname).In section 3.3.2, can the authors give any insight as to why training with reverse KL alone leads to whispering, and why adding the STFT term fixes this? (If it's only something that's been noticed empirically, "will lead" -&gt; "empirically we found"?)I noticed quite a large qualitative perceptual difference between the student and teacher samples, particularly in the speech synthesis case (experiment 3), even though I think I'd rate the quality on a linear scale as fairly similar (in line with the MOS results). The teacher sounds noticeably "harsher" but "clearer" Do the authors have any insight as to why this perceptual difference occurs (if they also perceive a qualitative difference)? Is it probably a difference in inductive bias between an AF (which WaveNet can be seen as) and IAF?I found it fascinating that reverse KL and forward KL lead to roughly the same MOS for spectrum-to-waveform. I assumed reverse KL would be better due to its preference for high-quality samples due to mode fitting.Out of curiosity, what is responsible for the pops at the start of the spectrogram-conditioned distilled models? Also why are the synthesized samples shorter than the ground truth (less initial silence)? Summary:  This paper makes the following four contributions. Firstly, it is shown (Theorem 1) that \Theta(N^{2/3}) parameters are sufficient for neural networks --with sigmoidal activation functions -- to memorize N input-label pairs, under an admittedly mild \Delta-separated assumption on the input points (Definition 1). This is an improvement over existing results which show that \Theta(N) parameters suffice, albeit typically for arbitrary N input-label pairs. As discussed in the paper, this result implies that depth is crucial for memorization with sub-linear parameters.  Secondly, the authors show (Theorem 2) that for fully connected networks of width 3, with sigmoidal activations,  \Theta(N^{2/3} + log(\Delta)) parameters suffice for memorizing any \Delta-separated set of N input-label pairs. This implies that the network width does not necessarily have to increase with N for memorization with sub-linear number of parameters. Thirdly, the authors study the question of identifying the maximum number of input-label pairs a given network can memorize and provide general criteria (Theorem 3) for the same. Existing results for this problem show that the number of arbitrary input-label pairs that can be memorized is at most linear in the number of parameters. The result in this paper shows that memorization of \Delta separated points can be done with the number of pairs super linear in the number of parameters.Finally, empirical evidence is provided which support their theoretical results, namely that deep networks memorize better than shallow networks (With the same number of parameters).----------------------------------------------------------------------------Reasons for score:  Overall, I vote for accepting this paper. It provides several novel theoretical results for the problem of memorization of neural networks, which together constitute a valuable contribution towards developing a fundamental understanding of deep networks.------------------------------------------------------------------------------Pros-Very well written and clearly structured paper. The problem has been motivated nicely in the introduction.-The related work section is rigorous and clearly details the current results for this problem.-The theoretical results in this paper are very interesting and also strong in my opinion. I like the idea of \Delta-separatedness for input points and it is also justified from a practical perspective. -The empirical results are convincing and support the theoretical claims in the paper. -------------------------------------------------------------------------------Cons: I could not find any major issues with the paper barring the fact that no proof sketch is provided within the main text for any of the theorems. Some minor remarks (typos etc.) are listed below, I am hoping that the authors will provide the clarifications stated therein in the rebuttal phase.------------------------------------------------------------------------------Minor comments:-Typo on pg. 3 (Section 2.1): d_{max} and d_{max} --> d_{max} and d_{min}-Typo on pg. 5: Definition 3 define the memorizability & --> Definition 3 defines memorizability &-On pg. 2, 3rd para after Theorem 1: I think the opening sentence is true provided  log(\Delta) is less than N^{w}, isnt it?-Towards the bottom of pg. 2, after the Theorem of Bartlett et al., it is mentioned that Theorem 1 together with Bartletts theorem imply that depth necessarily has to increase with N for memorization with sublinear number of parameters (this is stated on pg. 5 as well). This implication was not clear to me, some additional explanation will be helpful.-The discussion after theorem 2 (starting on pg. 5) involving the comparison with the function approximation problem is a bit unclear, especially the part on pg. 6 which discusses transforming the d_x dimensional inputs to distinct scalar values. The paper introduces and develops the notion of input-dependent baselines for Policy Gradient Methods in RL.The insight developed in the paper is clear: in environments such as data centers or outside settings external factors (traffic load or wind) constitute high magnitude perturbations that ultimately strongly change rewards.Learning an input-dependent baseline function helps clear out the variance created by such perturbations in a way that does not bias the policy gradient estimate (the authors provide a theoretical proof of that fact).The authors propose different methods to train the input dependent baseline function:   o) a multi-value network based approach   o) a meta-learning approachThe performance of these two methods is compared on simulated robotic locomotion tasks as well as a load balancing and video bitrate adaptation task.The input dependent baseline strongly outperforms the state dependent baseline in both cases.Strengths:   o) The paper is well written   o) The method is novel and simple while strongly reducing variance in Monte Carlo policy gradient estimates without inducing bias.   o) The experiment evidence is strong.Weaknesses:   o) Vehicular traffic has been the subject of recent development through deep reinforcement learning (e.g. https://arxiv.org/pdf/1701.08832.pdf and https://arxiv.org/pdf/1710.05465.pdf). In this particular setting exogenous noise (demand for throughput and accidents) could strongly benefit from input dependent baselines. I believe the authors should mention such potential applications of the method which may have major societal impact.   o) There is a lot of space dedicated to well know facts about policy gradient methods. I believe it could be more impactful to put the proof of Theorem 1 in the main body of the paper as it is clearly a key theoretical property. Summary: This paper addresses an important problem in density estimation which is to scale the generation to high fidelity images. Till now, there have been no good density modeling results on large images when taken into account large datasets like Imagenet (there have been encouraging results like with Glow, but on 5-bit color intensities and simpler datasets like CelebA). This paper is the first to successfully show convincing Imagenet samples with 128x128 resolution for a likelihood density model, which is hard even for a GAN (only one GAN paper (SAGAN) prior to this conference has managed to show unconditional 128x128 Imagenet samples). The ideas in this paper to pick an ordering scheme at subsampled slices uniformly interleaved in the image and condition slice generation in an autoregressive way is very likely to be adopted/adapted to more high fidelity density modeling like videos. Another important idea in this paper is to do depth upscaling, focusing on salient color intensity bits first (first 3 bits per color channel) before generating the remaining bits. The color intensity dependency structure is also neat: The non-salient bits per channel are conditioned on all previously generated color bits (for all spatial locations). Overall, I think this paper is a huge advance in density modeling, deserves an oral presentation and deserves as much credit as BigGAN, probably more, given that it is doing unconditional generation. Details:Major:-1. Can you point out the total number of parameters in the models? Also would be good to know what hardware accelerators were used. The batch sizes mentioned in the Appendix (2048 for 256x256 Imagenet) are too big and needs TPUs? If TPU pods, which version (how many cores)? If not, I am curious to know how many GPUs were used.0. I would really like to know the sampling times. The model still generates the image pixel by pixel. Would be good to have a number for future papers to reference this.1. Any reason why 256x256 Imagenet samples are not included in the paper? Given that you did show 256x256 CelebA samples, sampling time can't be an issue for you to not show Imagenet 256x256. So, it would be nice to include them. I don't think any paper so far has shown good 256x256 unconditional samples. So showing this will make the paper even stronger.2. Until now I have seen no good 64x64 Imagenet samples from a density model. PixelRNN samples are funky (colorful but no global structure). So I am curious if this model can get that. It may be the case that it doesn't, given that subscale ordering didn't really help on 32x32.  It would be nice to see both 5-bit and 8-bit, and for 8-bit, both the versions: with and without depth upscaling.3. I didn't quite understand the architecture in slice encoding (Sec 3.2).  Especially the part about using a residual block convnet to encode the previous slices with padding, and to preserve relative meta-position of the slices. The part I get is that you concatenate the 32x32 slices along the channel dimension, with padded slices. I also get that padding is necessary to have the same channel dimension for any intermediate slice. Not sure if I see the whole point of preserving ordering. Isn't it just normal padding -&gt; space to depth in a structured block-wise fashion? 4. Can you clarify how you condition the self-attention + Gated PixelCNN block on the previous slice embedding you get out of the above convnet? There are two embeddings passed in if I understand correctly: (1) All previous slices, (2) Tiled meta-position of current slice.  It is not clear to me how the conditioning is done for the transformer pixelcnn on this auxiliary embedding. The way you condition matters a lot for good performance, so it would be helpful for people to replicate your results if you provide all details. 5. I also don't understand the depth upscaling architecture completely. Could you provide a diagram clarifying how the conditioning is done there given that you have access to all pixels' salient bits now and not just meta-positions prior to this slice? 6. It is really cool that you don't lose out in bits/dim after depth upscaling that much. If you take Grayscale PixelCNN (pointed out in the anonymous comment), the bits/dim isn't as good as PixelCNN though samples are more structured. There is 0.04 b.p.d  difference in 256x256, but no difference in 128x128. Would be nice to explain this when you add the citation.7. The architecture in the Appendix can be improved. It is hard to understand the notations. What are residual channels, attention channels, attention ffn layer, "parameter attention", conv channels? Minor: Typo: unpredented --&gt; unprecedented This contribution describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift. A latent representation is extracted from SEEG signals and is the input of a LTSM trained to predict muscle activity. To mitigate the variation of neural activities across days, the authors compare a CCA approach, a Kullback-Leibler divergence minimization and a novel adversarial approach called ADAN.The authors evaluate their approach on 16-days recording of neurons from the motor cortex of rhesus monkey, along with EMG recording of corresponding the arm and hand. The results show that the domain adaptation from the first recording is best handled with the proposed adversarial scheme. Compared to CCA-based and KL-based approaches, the ADAN scheme is able to significantly improve the EMG prediction, requiring a relatively small calibration dataset.The individual variability in day-to-day brain signal is difficult to harness and this work offers an interesting approach to address this problem. The contributions are well described, the limitation of CCA and KL are convincing and are supported by the experimental results. The important work on the figure help to provide a good understanding of the benefit of this approach.Some parts could be improved. The results of Fig. 2B to investigate the role of latent variables extracted from the trained autoencoder are not clear, the simultaneous training could be better explained. As the authors claimed that their method allows to make an unsupervised alignment neural recording, independently of the task, an experiment on another dataset could enforce this claim. I'm going to keep this review short because I thought this was a really well-motivated and executed paper. The paper builds on the 'synthetic control' approach that is popular in econometrics which uses a weighted sum of the outcomes of individuals in the control group as an estimate of the control potential outcome, which can then be compared to a given treatment outcome to estimate the individual treatment effect. The step is the choice of weights: Sync twin uses the same prediction approach, but constructs the weights for the weighted sum using hidden representations from a recurrent network. This lets them work with irregularly spaced observations & missing covariates in a natural manner. My only substantive complaint is in the framing of the paper: the introduction describes this temporal setting as more challenging than the static setting, but given that the paper is not about dynamic treatments (which are indeed far harder to deal with), the fact that you see multiple observations for any given individual makes the problem easier not harder... To be clear - this is not bad thing, but rather than saying, "there are many methods for the static setting, but few are able to address temporal confounding" (which isn't really true - these approaches could easily be adapted to the single treatment temporal setting using the appropriate recurrent architectures) - instead emphasize the fact that the static approaches don't take advantage of multiple observations from the same individuals. Multiple observations is a *stronger* assumption than the static setting---albeit a very natural one in this domain that you leverage for trustworthiness, etc.---so it shouldn't be presented as though its a weaker assumption / harder setting...  A recent trend of 'pruning at initialization' in neural network pruning has left me baffled. It's counter-intuitive that neural networks can be pruned at initialisation, improving results for the training done thereafter. Nitpicking semantics, one could hardly even call this a pruning technique, since there is no a-priori knowledge of the dataset distilled in the network. Perhaps it's more aptly referred to as a method of sparse initialisation methods.The authors of this paper seem to have had the same doubts when it comes to the pruning at initialization literature, and put the magnifying glass on these methods that have recently been published. In an extensive and comprehensive study, they show that pruning at initialization methods do naught but set per-layer sparsity rates, where the sparse initialization might as well have been random.Writing these types of papers is important. It's essentially a survey paper, reproducing results from other papers, and testing their claims. In moving forward in this field, this type of work is crucial in filtering out the sense from the nonsense. Although the paper does not provide any new shiny optimization method, or stellar new GAN with a funny name, I highly laude the authors for working on this survey, and I believe the impact on the model efficiency community of this work is significant.Now on to the nitty-gritty of the paper. The key argument that shuffling the weights within a layer, essentially functioning like a re-initialization with a given per-layer sparsity ratio, is solid, convincing and damning. The inversion arguments equally so. The only qualm I have with the results are that there are not more on different architectures... but I hardly think that's necessary to make the point. If these methods were to work as intended, it should show on the common computer vision architectures we work with.The paper is well-written and well-structured. Clear and concise, with an extensive appendix highlighting more background information, and showing that the authors know the field very well. They covered every angle I could think of to put a crowbar in the paper and pry open some problems. I would have liked to see a discussion on why we would even want to prune at initialization, because the arguments in the paper only really come to light if we consider the utility of the methods. Sure, pruning at initialization is worse than pruning, but perhaps there are reasons to prune at initialization anyway. Are we looking at this as a research exercise? A quest to improve our understanding of networks? Is it done for sparse training? If it's the latter, a discussion on this, and perhaps a comparison to other sparse training methods would be in order. + This paper studies the single-path one-shot super-network predictions and ranking correlation throughout an entire search space, as all stand-alone model results are known in advance. This is a crucial step in NAS. As we know, inaccurate architecture rating is the cause of ineffective NAS in almost all existing NAS methods. It makes nearly all previous NAS methods not better the random architecture selection (suggested by two ICLR 2020 papers and many ICLR 2021 submissions). Therefore, analyzing the architecture rating problem is of most importance in NAS. This paper takes a deep insight into the architecture rating problem, which provides a timely metric for evaluating NAS's effectiveness. (+)- In the following text, another paper entitled "Block-wisely Supervised Neural Architecture Search with Knowledge Distillation" should be discussed: "Recent efforts have shown improvements by strictly fair operation sampling in the super-network training phase (Chu et al. (2019b)), by adding a linear 1×1 convolution to skip connections, improving training stability (Chu et al. (2019a)), or by dividing the search space (Zhao et al. (2020))," (-)- Kendall's Tau is a good metric. As shown in EagleEye, Spearman Correlation Coefficient(SCC) and Pearson Correlation Coefficient (PCC) are also good metrics. Could the authors also provide a comparison using these two metrics? (-)EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning- I think NAS-Bench-201 is not enough. As we know, CIFAR-10 is sometimes considered a toy benchmark, and the sole result on CIFAR-10 is not convincing. Could the authors provide more results in addition to CIFAR-10? (-)- As we know, there may be a gap between the small-channel supernet and the large-channel finally-adopted architecture. We are quite interested in the ranking correlations between a subnet obtained from the small-channel supernet and a channel-expanded version of the subnet trained from scratch. Could the authors provide such a ranking correlation analysis? (-)- Could the authors provide more details in Figure 3. Figure 3 shows that the lines on the top mean the operation is used more frequently. But I am not sure what the value of the y-axis means. (-)- Could the authors present some comments on "Perhaps the most surprising is the low importance of Average Pooling, even lower than Zero, an operation that does absolutely nothing"? (-)+ The following observation is believed to be crucial in NAS: "The baseline for small networks (top left, red) has the same averaged prediction accuracy for the top 10 as for the top 500 networks". This validates the inefficiency of SPOS in architecture search. (+)+ The following observation is also important in NAS: "Masking Skip (blue, left) is the most harmful to Äa (=1). As seen in Figure 4, the top-N networks have a worse average predicted accuracy than the top-M (for N < M) networks, and sometimes even below the random sample, which is terrible. Interestingly, \tau may improve within the predictions for the top-N architectures." Especially, the phenomenon that masking skip connection reduces the ranking correlations is interesting. As is shown in SCARLET-NAS, the supernet training with skip connection is not fair. But in this paper, we can see that skip connection benefits the ranking correlation. We are interested in this opposite opinion. Specifically, it is fascinating to see that "Although the additional transformers seem to stabilize training, as seen by the lower standard deviation, they also worsen the Äa problem." Besides, the phenomenon of "\tao may improve within the predictions for the top-N architectures" indicates that the metric for ranking correlations maybe not perfect. A more reasonable metric may be desirable. (+-)+ The following observation is important: "medium-sized super-networks require additional care." As shown by Figure 4, the averaged predicted accuracy of top-N networks in several subsets is lower than that of a random subset of networks. This is consistent with previous work like DNA, which shows a large search space may be harmful to the architecture rating. Even if a medium-sized supernet has a bad architecture rating, the ranking correlation should be worse in a large-sized supernet. (+)DNA: Block-wisely Supervised Neural Architecture Search with Knowledge Distillation- The following description is questionable: "After the architecture search, all Linear Transformers can safely be removed, as they do not impact the network capacity". Actually, stacking many fully connected layers without non-linear activations could lead to only one fully connected layer. It is an open question of whether optimizing loss(ABCx, y) is as difficult as optimizing loss(Dx, y) using stochastic gradient descent. (-)+ The results providing evidence against disabling cell topology sharing during the training phase are exciting and new to the public. (+)+ The following observation is fascinating: "The absolute validation accuracy value is increased by uniform sampling. However, this is not relevant, as only the correct ranking matters". This is against FairNAS. (+)+ It is interesting and convincing that many tricks such as learning rate warm-up, gradient clipping, and regularization do not work to improve the ranking correlation. We are pleased that the authors provide so many experiments to point out some misleading approaches in NAS. I think this paper is very important in the context of AutoML. (+)- The analysis is based on medium-sized and small-sized search space. It would be good to see some analysis of large-sized search space. (-)Overall, this paper provides a timely analysis of the current NAS's ineffectiveness caused by the inaccurate architecture rating problem. As there are many NAS papers published every year and their ineffectiveness may still be not widely recognized by the reviewers and the public, I recommend a strong acceptance for this paper to promote the analysis of the NAS's architecture rating problem. Graph convolution has been defined to be permutation equivariant to the neighborhood vertices. If one were to define an anisotropic kernel then a reference edge would have to be defined corresponding to edge angle 0. Figure 1 is very illuminating.Equivariance with respect to this reference can be achieved only with a special mechanism. The authors here proposed message passing via edge transporters.The crux of the approach is in equation (2) and in particular in $\rho(g_{p \rightarrow q} \in [0,2\pi)$. What happens is that the feature vectors of the adjacent vertices have to be transported to the center node, namely a transformation from the local coordinates in p to the local coordinates in q correcting, thus, the underlying gauge difference. This step includes also an alignment of the tangent planes. I strongly believe that appendix A belongs to the main paper.The second crucial point is that equivariance imposes a linear constraint on the kernels $K_{Self}$ and $K_{Neigh}$. This allows a kernel to be written as a linear combination of 20 kernels for $K_{Neigh}$ and $K_{Self}$ resulting in 24 only unknowns for one layer.The related work section is comprehensive. The experiment on shape correspondence achieves performance comparable to the state of the art spiralNet++. The paper would benefit from other experiments on manifold like performing mesh convolutions for human or object reconstruction from images. Faust is pretty standard in the GML community but it is an easy task in terms of feature learning.The paper contribution is elegant and significant: Gauge equivariance  is a necessity if you want an anisotropic diffusion.The paper is unreadable without the appendix and somehow it would be better to make it self-contained and move the experiments in the appendix. Summary:The paper explores the impact of different types of data augmentations for protein sequence data, and does a thorough benchmark analysis on them. The authors used a pre-trained transformer model, fine tuned the model on augmented data using two approaches, namely, contrastive learning and masked token prediction. This finetuned model was evaluated with an added linear layer on a range of tasks.Overall, I vote for strong acceptance as I am genuinely excited about the paper. My reasons are below-Pros:1. There are no known good augmentations for protein sequences. This is probably the first paper that tries a wide range of data augmentations for protein sequences, and tests these augmentations on a wide range of benchmarked tasks.2. The design of the data augmentations is creative. Specially, random dictionary and random alanine replacement augmentations seem to improve the downstream performances quite consistently. 3. The paper has done comprehensive experiments showing most possible configurations of the augmentations and their combination with contrastive learning and mask token prediction. The paper is so comprehensive in its experiments in the scope of a conference that I cant really think about any strong cons. I would just ask the authors if there are any plans to bring these augmentations to a library so that the community can start trying them rather than implementing the augmentations themselves? Thanks for this wonderful paper. Summary:The paper proposes deep encoder and shallow decoder models for auto-regressive NMT. They compare rigorously to NAR models. They also study three factors: layer allocation, speed measurement and knowledge distillation. They include that with a 12E-D1 model they obtain significant speed-up and can outperform the standard 6-6 AR model and almost always beat the NAR model in terms of quality. They also show that NAR models need deep decoders because they need to handle reordering.Reasons for score:I scored this paper a 9. I think this is an important paper which establishes very strong AR baselines for future NAR work in the field. They correctly point out the three issues with the comparisons that many NAR papers make. They conduct various meaningful ablation studies and validate their various hypothesis properly. They also show that certain factors like knowledge distillation need to be applied to both AR and NAR systems. Finally, they advocate for reporting both S_1 and S_max when comparing speed gains.Cons:- One issue I had with the presentation of the results was the selection of different formats and language pairs for different experiments. For example, table 2, 3 and 4 report on different subsets of language-pairs. Same with the figures. This might raise questions of whether the authors are randomly subselecting or selecting favorable subsets. I would have liked to see all experiments done on all LPs.Minor comments: - Section 2.1: S_max - "This is closer to practical scenarios where one wants to translate a large amount of text." - this is a very subjective statement and I would tone this down. - Section 2.2.2: "Denote respectively by E and D the numbers of encoder and decoder layers." -- please fix grammarMissing citations:- Section 1: Along with Sutskever, Bahdanau and Vaswani. please also cite https://www.aclweb.org/anthology/D13-1176.pdf and Wu et al. 2016 (https://arxiv.org/abs/1609.08144) when you mention state-of-the-art NMT. This article is concerned with convergence guarantees of online stochastic gradient descent for a rather generic class of three layers neural networks (instead of similar analyses that treated two layers). The main results state that in a proper limit of infinite width + vanishing learning rate, the dynamics of online SGD is proven to be tracked thanks a mean-field description in the form of coupled ordinary differential equations. Once this mean-field description at disposal, the main result is obtained: in the infinite width + vanishing learning rate + infinite time (= number of training samples), the generalization error tends to it minimal value for a broad class of models and losses (not necessarily convex, which is a novelty of the work) as well as generic data distribution.Overall this paper is very well written, enjoyable to read despite the technicality of the results, and understandable even for non-specialists of this line of works (like myself). I did not check the appendices and proofs. In the main part there are no typos, and I have no main concerns to bring about. Yet: I would find interesting to know more details about the differences with the refs Nguyen (2019); Araujo et al. (2019); Sirignano & Spiliopoulos (2019); that is not clear.  Also I would find useful to have some hints about the meaning of the (trained third layer) hypothesis in Theorem 8. Finally I find a bit surprising that there are nor restrictions whatsoever on the data distribution (or I missed that). The authors may comment on that in the final version.I recommend publication. Even if I'm not a specialist, it is obvious that the authors made a big effort of redaction, that the results are very solid, the proof technique seems original and requires less assumptions than previous works (I liked very much the "idea of proof" part). I have very few doubts about the quality of the paper despite I did not read the proof details, and the fact that I'm not aware of the literature in this specific field. Pros:- The paper presents an RL approach to train a model which may control a power grid. In that effort another actor-critic method coined as SMAAC is suggested which may become useful in other applications as well.- The authors used the concept of afterstates to reduce the huge state-space offered by L2RPN problem. They used Graph Neural networks along with the transformer based attention mechanism to achieve state of the art performance. - Formulating the hierarchical  decision model is very beneficial as it allows the exploration without violating the grid boundaries.- It is shown using the case study that the concept of afterstate has played crucial rule in the success of the model proposed. The same technique does not show good results with SMAAC\AS, which is  SMAAC without afterstate. Cons:- In section 3.1 it is said "Thus, for line switch actions, we simply follow the rule always reconnecting the power lines whenever they get disconnected due to the overflow" but this can also cause severe damage to the appliance. There is not enough detail provided on this, but I believe that re-connection must be done AFTER the grid state is improved so the appliances are not damaged.- According to Kundur 1994, a system goes through "Alert state" and "Emergency state" from normal state to get into "In extremis state".  The desired behavior would be to do restorative action in the "Alert state" to avoid any damage, if not possible then in "Emergency state". But authors have chosen to act only in hazardous situations (section 3.1). A little more discussion would be beneficial in this regard. Apparently, it does not seem difficult to act sooner, as one only has to reduce the width of allowed boundaries, but does this have any effect on exploration? that is not clearly discussed in the article.- There is a small spelling mistake in section 5.1 "largest" is written as "largeest" Authors propose an approach to perform authentication based on users' behaviour (behavioral biometrics) on two different domains (web and mobile) by means of a siamese NN. The proposal is evaluated on three different dataset, collected ad-hoc for this work, showing a performance up to >0.9 acc. In the experimentation setup authors include different data splits, sequences lengths and training methods (pairs and triplets). The work shows the feasibility of using this type of approach for few-shot classification under real industry constraints.------------------ContributionsMy recommendation for this manuscript is a strong accept. I believe this paper makes a relevant contribution to the field since it addresses a topic which is quite interesting for the audience, it is technically sound, covers different hypotheses and provides very useful insight to practitioners about the use of these approaches in real industry applications. The last point is the most interesting in my opinion, honestly. The paper is well structured, with a clean format, very well written and easy to read.Probably its weakest point is the novelty, since it addresses a well-known problem using a long-established NN architecture. However the combination of one-shot learning, siamese architecture and behavioral biometrics is something I personally have not seen before, so I consider it robust enough.------------------RecommendationsAlthough it is covered in the section 3.1, given the scope of the conference and its relevance for the reproducibility of the results, Id ask authors to include a clear and detailed explanation of the feature vector employed. Some chart or graph where the reader can see easily how many features come from which source (sensor, keypad, gesture), how the derived features are computed and how movements are translated to features. Even better if the authors can include some examples.  Maybe as an appendix, but I really believe its important and a clear improvement for the manuscript.I think itd be interesting to add further details about the data splitting mechanism. I believe the paper needs at least to state clearly whether the train/test split is done by users (where a group of users are employed to perform inference and the rest as control test) or by sequences (where the data from each user is split into train and test). This is a relevant topic since the later has been demonstrated in previous HAR works to offer less robust and generalizable models. Please expand this point.Figure 4 - Please specify clearly which sequence length is the one used for this experiment. It seems taken for granted it is 10 sec , since its the configuration offering best results, but please make it more clear to the reader.From a personal perspective I think itd be interesting to expand the work adding feature extraction mechanism using the state-of-art in time series modelling, this is, recurrent neural units or attention based mechanism. Im really curious to see how that compares with the current engineered features.------------------MinorFigure 2b - Id recommend to scale the Accuracy axis since the series are hard to analyse (they are too close together)Typo:  Figure 2 shows the ROC curve and the FAR and FRR for the best setup in web environment -> Figure 3 shows the ROC curve and the FAR and FRR for the best setup in web environmentTypo: Figure 2 shows the ROC curve and the FAR and FRR for the best setup in mobile environment. ->  Figure 3 shows the ROC curve and the FAR and FRR for the best setup in mobile environment. This paper addressed an essential task for large-scale application of object detection -- semi-supervised learning. It introduced a simple but effective Unbiased Teacher to solve the traditionally problematic data imbalance issue. Pros(1) Provides strong evidences and analysis on the class-imbalance problem inherited in pseudo-labeling methods;(2) Proposed       (a) an interesting learning paradigm where the teacher is the temporal ensemble of student networks;      (b) focal loss in place of cross entropy;(3) The experiment and ablation suggested that the resulting teacher model is not prone to class-imbalance-induced overfitting and the improvement from SOTA is significant.ConsCurious to know why there is a drop in AP_50 comparing to STAC.  The paper proposes Continuous Query Decomposition (CQD), an approach for answering Existential Positive First-Order (EPFO)queries over incomplete knowledge graphs exploiting a neural link predictor for 1-hop-only queries.Entities are embedded in a low dimensional space and entity vectors are used to compute the score of query atoms thatare then combined using a t-norm for conjunction and t-conorm for disjunction.Answers to queries are found either with continuous optimisation by gradient descent to find embeddings for query variablesor combinatorial optimisation where top-k entities for query variables are looked for yielding a beam search.CQD is compared with Graph Query Embedding (GQE) and Query2Box over three datasets on a large number of queries.The result show that CQD outperforms the baselines on Hit@3 on average.CQD also offers the possibility of explaining the results of queries by showing the top scoring entities for query variables and the score of atoms.CQD tackles the difficult problem of answering queries that are beyond simple 1-hop completion queries. It improvesover previous work which need to train the model over a large number of queries (Hamilton et al., 2018;Daza & Cochez, 2020;Ren et al., 2020) and do not consider disjunctive queries (Hamilton et al., 2018; Daza & Cochez, 2020).These advantages are obtained by not embedding the query into a low dimensional space but using continuous or combinatorialoptimization to answer queries, considering the query as a formula in fuzzy logic and applying t-norms and t-conorms.While the use of fuzzy logic in query answering is not new, they way in which it is combined with entity embeddings andneural link predictors is original to the best of my knowledge.The fact that queries are not embedded (and so learning does not need large numbers of queries) is a strong point of CQD,with competing methods (Hamilton et al., 2018;Daza & Cochez, 2020; Ren et al., 2020) requiring many queries for tuning the query embeddings.Since queries are not embedded, the results of CQD are also easier to explain. The experiments are sufficiently extensive to support the claim of the paper that CQD is also outperforming competitors in terms of the quality of solutions. However, the authors should justify why they used embedding size 500 for their methodsand 400 for the baselines.From a technical point of view the article seems sound but the authors say that "Then, after we identified the optimal representation for variables $A, V_1, \ldots  V_m$, we replace the query target embedding $e_A$ with the embedding representations $e_c \in R^k$ of all entities $c \in E$, and use the resulting complex query score to compute the likelihood that such entities answer the query."In this way the authors throw away vector $e_A$ that may have information about the problems, isn't there a method toexploit the information in $e_A$?I have a few remarks about the presentation:Citation Raedt, 2008 should be De Raedt, 2008.In Figure 1 the edges of the graphs have the opposite direction with respect to the caption and main text.Page 6: "we only make use of type 1-chain queries to train the neural linkpredictor": do the authors mean 1-hop queries? 1-chain appears here for the first time."In all cases, we use a rank of 500.": for rank do the authors mean the embedding size? This should be clarifiedPage 7: "Sincea query can have multiple answers, we implement a filtered setting, whereby for a given answer, wefilter out other correct answers from the ranking before computing H@3.": this sentence is not clear. Does it meanthat answers that follow from the KG without completion are removed from the ranking? Summary: This paper proposes Continuous Query Decomposition (CQD) a novel method for evaluating complex queries over incomplete KGs. Each variable of a logical query (involving existential quantifiers, conjunctions and disjunctions) is mapped to an embedding. A link predictor, trained on single edge prediction, is used to score the atomic query involving the variable. The full query is evaluated using continuous versions of the logical operators and gradient-based or combinatorial optimization.Evaluating complex logical queries on (necessarily incomplete) KGs and other graph-structured data is an important problem for data mining purposes. The paper proposes an elegant and effective method. Strong pointsElegant, efficient solution.SOTA results.Provides aspects of explainability, although this could be discussed and illustrated better.Detailed comments- What is particularly important/challenging about EPFO queries, beyond existential and conjunctive ones? Obviously it is an extension that covers more FOL, but a qualitative discussion would help the reader, particularly with respect to applications to KGs.- Could you talk more, give more insights about the 8 complex queries types? Why are they important?- The query What international organisations contain the country of nationality of Thomas Aquinas? sounds really artificial. Maybe there is a better example involving entities and relations, similar to the drugs one?- Could you say a bit more with respect to how the KG incompleteness is accounted for in the evaluation?- The paper mentions .. in many complex domains, an open challenge is developing techniques for answering complex queries involving multiple and potentially unobserved edges, entities, and variables, rather than just single edges. It would be great to articulate this more for sake of providing context and motivation. The authors consider the problem of providing optimal scheduling schemes for adiabatic quantum computing (AQC), i.e. with a way of interpolating / evolving a Hamiltonian from its initial form to its final form such that it is nwither too slow nor too rapid and thus can harness the speedup that AQC offers for solving combinatorial optimization problems (or, to be specific: QUBOs).Indeed, optimal scheduling of is a problem of theoretical as well of practtical concern as it makes or breaks the success of AQC but general, closed form solutions are hard to come by (or simply unknown at this point in time). Addressing drawback of previouslt proposed reinforcement learning approaches to this problem, the authors propose to train deep neural networks. To this end, they introduce a novel loss function loss that maximizes the fidelity based on an approximation of the success probability. In practical experiments, they find that schedules learned this way outperform those from previous approaches and, in addition, generalize from one type of problem to different instances of problems. Their experiments also reveal the approach to outperform previous.This paper presents, original, convincing, and interesting work on a problem of considerable practical importance in adiabatic quantum computing. The idea of using neural networks in order to learn good (optimal) schedules for AQC is elegant and apparanetly leads to very good results. Moreover, to those with a background in AQC and deep learning, the paper is easy to read and clearly describes basic ideas and required technical details.  Summary:   This paper complements a class of recent work on function approximation for image function representation that leverages random fourier functions and sinusiodal activation functions (SIREN).  The main insight in the paper an alternative scheme that just repeatedly applies nonlinear filters (sinusoids, gabor wavelet functions) to the networks input and multiplies together linear functions of these features.  The authors show that due to multiplicative properties of fourier/ gabor filters the end resulting mapping is also represented by a linear combination of fourier/gabor filters. Essentially the entire function is a linear function approximator over an exponential number of fourier/gabor basis functions with a low-rank, polynomial number of coefficients of the MFN network.  Experiments are done to compare the proposed FourierNet, GaborNets against past work (FF, FF with positional encoding, FF Gaussian SIREN).  The results are competitive with other methods and in certain cases GaborNets outperform in PSNR benchmarks for various problems.Pros:  I like the paper very much, it is very well written and clear. The idea is novel.  Cons: The empirical benchmarks are the same as used in the SIREN paper.  I would liked to see a discussion or analysis on tradeoffs between using SIREN or GaborNet (i.e. in what settings they are appropriate and perform perturbation analysis).   The paper investigates a new training objective, contrastive tension (CT), for obtaining unsupervised sentence embeddings. The objective operates by initializing two models with identical weights and then training the models to produce similar sentence embeddings to each other for identical sentences and dissimilar representations for different sentences. This objective encourages the paired models to agree on positive examples, but at the same time encourages divergence in their models weights by providing different sentences to each encoders for the negative pairs containing different sentences. The new objective is applied as an unsupervised finetuning tasks for BERT, Sentence-BERT, Distill BERT, multilingual BERT, XLNet and XLMR.The paper demonstrates consistently strong results empirical results on the unsupervised semantic textual similarity (STS) task. The results on supervised STS are mixed with more modest gains and losses over the baseline for some configurations. The paper provides reasonably good analysis on demonstrating the impact of the proposed technique within different layers of a pre-trained model as well as on the effect on the model scores vs. human labels. I found the analysis particularly interesting that showed CT helped the model to better discriminate/score pairs with lower similarity scores (Figure 3). I also like the breadth of the experiments that included a number of different models, finetuning corpora and STS datasets includes multilingual STS.In terms of potential improvements, I found the results in Figure 1 somewhat surprising in that they show that without further fine-tuning on either unsupervised or supervised data the default representations for many pre-training models are poorly suited for STS. It is well established that the final layers are not useful, but I haven't yet seen an analysis looking at all the layers of so many models. The presentation could possible be improved by including for contrast one existing transformer model that performs well on the STS task (e.g., S-BERT or maybe USE). While presenting the worst-performing results across runs is a refreshing change from other papers that might be cherry picking their results, I found the inclusion of just the worst-performing results makes it a little hard to full understand the performance of the model. If possible update all of the results with the worst-performance across runs with average and worst, maybe using something like AvgScore[WorseScore]. If this makes the tables too crowded, consider including more comprehensive results in the appendix of the paper. 1) Summary- The authors proposed the negative data augmentation technique which is useful for generative adversarial networks, anomaly detection, self-supervised learning frameworks.- The idea is simple, and the technique was proven that it is powerful for several tasks.- They performed several experiments, and I think the experiments were enough to show the technique's superiority.2) Strong points- Good idea- Strong experimental results- Simple to use- Easy to understand3) Weak points- In Figure 3, they claim that in the absence of NDA. the support of a generative model learned from samples may "over-generalize"...- I am not sure that the sentence is true.This paper is well written, and concrete, I recommend that this paper should be presented in ICLR 2021.  Summary: This paper presents a generative model based on stochastic differential equations (SDEs), which generalizes two other score-based generative models score matching with Langevin dynamics (SMLD) and denoising diffusion probabilistic modeling (DDPM). The recipe to sample from the data distribution is based on (i) the observation that both SMLD and DDPM can be formulated as the discretization of an SDE, (ii) the finding from Anderson (1982) about the reverse of an Ito process, (iii) a score model. A novel aspect of the presented technique is the use of the score model as "predictor", which gives the initial sample from the MCMC sampler that serves as "corrector". Finally, the Ito process induced by the reverse SDE is formulated in a deterministic manner, leading to a neural-ODE based generative model.Pros:- The authors did a good job at showing connections between the previous score-based generative models and their model. I believe on its own this is a nice contribution.- The method is thoroughly analyzed. I went through the derivations and didn't find any errors.- Experiments show that combining the predictor and corrector routines leads to better performance, a nice validation of the theoretical claims.- As promised, the model achieves SOTA on several tasks.Cons:- I'm having difficulty seeing the transformation of the reverse SDE into an ODE (from eq.10 to eq.12). Is it as simple as multiplying the second term with 1/2 and discarding the Brownian motion? Also, eq.12 is a simple ODE system, which has nothing to with a process as far as I understand. Maybe more explanation or pointers in Maoutsa et al., 2020 would be nice.- The paper lacks the discussion on the benefits/downsides of different SDE solvers, discretization time steps, etc.- As such, the paper lacks a "toy example" experiment, for example, on a simple 2D dataset like half-moons. A visual demonstration of the SDEs and probability flow (maybe corresponding vector fields and Brownian motion over time) would be interesting.Additional comments:- A typo (intead) right below eq.9- Best performing rows can be bold in Table 1. ##########################################################################Summary:This paper proposes a benchmark for high-level mathematical reasoning and study the reasoning capabilities of neural sequence-to-sequence models. This is a non-synthetic dataset from the largest repository of proofs written by human experts in a theorem prover, which has a broad coverage of undergraduate and research-level mathematical and computer science theorems. Based on this dataset, the model need to fill in a missing intermediate proposition given surrounding proofs, named as IsarStep. It's a very interesting task. This task provides a starting point for the long-term goal of having machines generate human-readable proofs automatically. The experiments and analysis also reveal that neural models can capture non-trivial mathematical reasoning.##########################################################################Reasons for score:  Overall, I strongly vote for accepting. I think this is a very important work and this benchwork would benefit to other fresh ideas and new approaches for mathematical reasoning related research. My only concern is that as a benchmark, do authors need to conduct more experiments and baseline models on their data set to be more convincing? ##########################################################################Pros: Pros:+ 1. The paper mined a large corpus of formal proofs and defined a proposition generation task as a benchmark for testing machine learning models mathematical reasoning capabilities. Such beckmark is important and beneficial to the development of the artificial intelligence community. + 2. The proposed HAT model is novel for better capturing reasoning between source and target propositions. The design of two types of layers is reasonable and interesting. The local layers model the correlation between tokens within a proposition, and the global layers model the correlation between propositions. + 3. This paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed model. Experiments and analysis reveal that while the IsarStep task is challenging, neural models can capture non-trivial mathematical reasoning.+ 4. The paper is well-written and the design decisions are clearly explained. The comparison of benchmark methods is also interesting to read. In general, I think this is a worthy publication.  ##########################################################################Cons: My only concern is that as a benchmark, do authors need to conduct more experiments and baseline models on their data set to be more convincing? The authors only use two baseline models: RNNSearch and transformer, it seems to be insufficient. At Bert era, do those improved models based on Generative Learning tasks could also be applied to IsarStep as baseline, like MASS(Masked Sequence to Sequence Pre-training for Language Generation)/UNILM(Unied Language Model Pre-training for Natural Language Understanding and Generation)? #### General CommentsA proper initialization plays an important role in the success of over-parameterized models such as deep neural networks and high dimensional models.  However, the explicit role of initialization in theoretical results of an algorithm has not been stated well to my knowledge.  The main task of this paper is to  present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. Specially,  it is shown that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization.With respect to linear networks, the paper makes the following three main contributions:1	 The role of initialization of the gradient flow on the convergence is characterized explicitly.    (2) The stationary point of the gradient flow is sufficiently close to the min-norm solution in the linear case.3	 Random initialization for large wide  linear networks ensures that the dynamics of the network parameters       are constrained to a low-dimensional manifold. Overall, this is a written- well paper with significant novelty.  The results seem interesting in the deep learning theory literature. #### Specific Comments(1) For Theorem 2,  the network width is required to be a polynomial of the input dimension D, which may be loose in some practical network structures.  I wonder whether such constrain can be relaxed further? it will be better that some quantitative comparison with those related work is made. (2) When noisy gradient descent is considered,   is the current analysis  still applicable to the case and similar results can be derived?  (3) If an activation function is added such that the hypothesis class is nonlinear,  is the adopted analysis still valid? if not, what is the additional challenges?  This is a well written paper with some interesting results.  This paper is to propose a distributional sliced-Wasserstein distance to address the limitations of standard SW and Max-SW. The proposed method finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections. Some theoretical results are presented in both the main paper and its supplementary document. This reviewer personally enjoys reading this paper. Here are a few additional comments. 1. How to select $\lambda_C$ in practice? The authors need to discuss it in details.  The main aim of the paper is to make use of human interaction/motion to learn a visual  representation that can be re-used for classic visual tasks such as depth estimation. The authors claim that by encoding interaction and attention cues in the self-supervised representation, the method can outperform visual-only state-of-the-art methods. To study the interaction element, the authors attach sensors like Inertial Movement Units (IMUs) to the limbs of subjects and monitor their reaction to visual events in daily life. The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants which include synchronized streams of images, body part movements, and gaze information.Paper Strengths:This is a simply awesome paper. Idea is novel, well-validated, and well-written. The result is strong.+ Novel intuition: The idea of the paper is intuitive, where it proposes to incorporate body part movements and gaze information in learning visual representations. Attention does play an impact in many tasks like action recognition and scene classification, which might benefit from the proposed representation learning. Also, in case of tasks like depth estimation and future prediction of dynamics, it is insightful to use body movement since it encodes temporal changes.+ Experimental setup and ablation studies: The intuition of the authors to incorporate body part movements and gaze information in their representation has been well justified by the experimental setups and ablation studies. The importance of using each objective in the representation learning has been effectively demonstrated by showing its impact on various target tasks.+ Performance: The authors have shown that the representation, trained using movement and attention supervision, outperforms the visual-only representations in all the tasks mentioned in the paper by a range of 1.3% to 7%+ Dataset: The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants. This is a novel dataset with synchronized streams of images, body part movements and gaze information.Paper Weaknesses:- Objective function selection: In the ablation studies of body parts, we see how the removal of a body part can affect the performance on the target tasks. However, it is still not clear how each individual part may fare since the performances do not vary greatly from each other. The exclusion of the torso results in a lower error for the depth estimation, but an explanation for why exactly would that be the case may be beneficial, since intuitively it may seem that the complete movement of the body should result in better performance. Apologies for the late review!## Summary of the PaperThe paper provides a potential theoretical explanation of the known empirical observation that cold (or tempered) posteriors improve predictive performance of deep Bayesian neural networks. The provided explanation is simple and it leads to additional predictions, which the authors check empirically as far as possible with existing data sets. The empirical results agree with the predictions.## Main StrengthsI believe the main message of the paper is so relevant and seems so simple (at least in hindsight) that it has the potential of becoming a kind of "common knowledge" in Bayesian neural networks community (caveat: I can't judge if these findings had already been informally known to a larger group of researchers, but unless someone has explicitly written them down somewhere I wouldn't hold this against the paper).Researchers have been trying to increase predictive performance of deep neural networks by applying scalable Bayesian methods to deep learning for a while, but even replicating the performance of point estimated models with Bayesian neural networks has proven surprisingly difficult. To my knowledge, it was found out only recently that Bayesian neural networks systematically outperform their point estimated counterparts if the prior is made artificially sharper than what probability theory would predict (i.e., by "lowering the temperature"). This empirical result has been puzzling from a theoretical perspective, but the present paper provides a simple potential explanation for this effect.I believe the findings in this paper go beyond a theoretical justification of an empirically known fact. The findings may also have implications on model robustness: the authors argue that the "tempering effect" is a result of curation of the training set. Validations and tests sets are typically curated in the same way as the training set in the machine learning community. However, when models are deployed in the field, they typically see uncurated data points. I would be curious to know if explicitly modeling the curation process, as the authors do in this paper, would also address this issue.## Potential WeaknessesThere's one caveat to my review: I am not an expert on Bayesian neural networks and, as stated above, the argument made in this paper seems so simple in hindsight that I cannot say with absolute certainty that it hasn't been made before. I personally haven't heard this argument before, but if other reviewers can point to a reference that already made this argument, then that would probably be the only thing that could convince me to lower my rating. Otherwise, I would consider the simplicity of the authors' argument a strength of the paper.## Questions to the Authors- What do the authors mean with the phrase "finite networks" in the first paragraph? Is it the same as networks with point estimated parameters (as opposed to Bayesian neural networks)?- As mentioned above, the paper models the curation of the training set, but I didn't understand how or if curation of the test set is modeled. Could the authors clarify this? Specifically, what changes for a model trained on curated data when it is either (a) tested on an equally curated test set or (b) applied to uncurated data in the wild? Would the optimal $\lambda$ during training differ between cases (a) and (b) or would the posterior have to be changed after training?## Minor Issues- I like the short Section 2 (which compares "cold" and "tempered" posteriors) very much! I think it could even be improved by adding one reference each for "cold" and "tempered" posteriors, respectively. More importantly, as far as I understand, the two are really essentially the same if one uses, e.g., a Gaussian prior. Unless I'm mistaken, the missing factor of $\frac{1}{T}$ in Eq. 2 could then be absorbed into a rescaling of the prior covariance (unless the prior covariance itself is learned with expectation maximization). If this is correct then I'd add a corresponding statement at the end of Section 2 (it would make the paper's claims more widely applicable).- I think Figure 2 is never discussed in the paper (but there should be enough space left to discuss it). The figure caption says "Schematic diagram". Could the authors clarify what this means? Do the points come from some toy model with a 2-d parameter space (or does the figure show a 2-d PCA of the parameter space) or were the points really just drawn manually to visualize the idea? I think both would be fine but I would be very curious to know how the figure looks with real data.- In Figure 4, the last panel is labelled "F" but referred to as "E". Also, in the last two panels, the left dashed vertical bar is not discussed. Is it the theoretically expected optimal value of $\lambda$ (i.e., $\lambda=\frac{1}{4}$) or the empirically found optimal value? The paper proposes a novel SPRT-TANDEM algorithm minimizing the divergence between estimated and true Log-Likelihood Ratios of SPRT and making it thereby Bayes optimal for various real-world applications. The paper is very well written, clear and scientifically sound and provides  extensive contributions, e.g. a database in addition to the algorithm. Performance of the algorithm is demonstrated via three experiments.  Previous research is given sufficient credit. The only thing I would still like to see more is the discussion at the conclusions. Why does this seemingly simple modification to the existing SPRT method provide so superior performance.The appendices are referred a lot in the text but they are missing from the paper?A very minor comment: The following sentence is a bit vaguely written:Long short-term memory (LSTM)-s/LSTM-m impose monotonicity on classification ...I guess it should be : Long short-term memory (LSTM) variants LSTM-S and LSTM-M impose monotonicity on classification ... This problem is well-motivated --- estimating dose-response is a challenging and practically important problem.The paper is extremely well written.  It explained complex (poor written) ideas in the semiparametric literature clearly. The comparison against existing works is clear.The theory, as far as I can tell, is sound. It improves the existing results in targeted regularization and can be adapted to analyze one step TMLE.The experiment shows that the model outperforms existing benchmarks on the task it set out to do. My main suggestion to improve the paper is to use some datasets that actually have continuous treatment in evaluating the method.  In particular, I would be interested in seeing an application of the methods on real-world datasets. That being said, while it will improve the paper, it's probably asking too much for an 8-page conference submission. I believe the paper at its current state is sufficient for acceptance.I want to thank the authors for writing such an elegant paper. I really enjoyed reading it.  This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks.Pros:1.A novel algorithm that promotes the interpolation ability of AE2.A new synthesized line benchmark to verify the interpolation ability of different AE variants3.Strong results on downstream classification and clustering tasksCons: 1.The interplay of the adversarial network (between AE and critic) isnt very clear and can be improved2.Eq. 1, should x be x_1 or a new data other than x1 and x2?3.The paper states that the 2nd term of Eq. 1 isnt crucial. If x is a new data (other than x1 or x2), how can the critic infer \alpha without a reference to x1 or x2?4.The paper states that encouraging this behavior also produce semantically smooth interpolation &. Besides the empirical evidences from data, it would be better to any some theoretical justifications. The authors propose TD-VAE to solve an important problem in agent learning, simulating the future by doing jumpy-rollouts in abstract states with uncertainty. The authors first formulate the sequential TD-VAE and then generalize it for jumpy rollouts. The proposed method is well evaluated for four tasks including high dimensional complex task.Pros.- Advancing a significant problem- Principled and quite original modeling based on variational inference- Rigorous experiments including complex high dimensional experiments- Clear and intuitive explanation (but can be improved further)Cons. - Some details on the experiments are missing (due to page limit). It would be great to include these in the Appendix. - It is a complex model. For reproducibility, detail specification on the hyperparameters and architecture will be helpful.Minor comments- Why q(z_{t-1}|z_t, b_{t-1}, b_t) depends both  b_{t-1}, b_t, not only b_t?- The original model does not take the jump interval as input. Then, it is not clear how the jump interval is determined in p(z|z)? This paper proposes graph-convolutional GANs for irregular 3D point clouds that learn domain (the graph structure) and features at the same time. In addition, a method for upsampling at the GAN generator is introduced. The paper is very well written, addresses a relevant problem (classification of 3D point clouds with arbitrary, a priori unknown graph structure) in an original way, and supports the presented ideas with convincing experiments. It aggregates the latest developments in the field, the Wasserstein GAN, edge-conditional convolutions into a concise framework and designs a novel GAN generator. I have only some minor concerns:1)My only serious concern is the degree of novelty with respect to (Achlioptas et al., 2017). The discriminator is the same and although the generator is a fully connected network in that paper, it would be good to highlight conceptual improvements as well as quantitative advantages of the paper at hand more thoroughly. Similarly, expanding a bit more on the differences and improvements over (Grover et al., 2018) would improve the paper. 2)P3, second to last line of 2.1: reference needs to be fixed "&Grover et al. (Grover et al., 2018)"3)It would be helpful to highlight the usefulness of artificially generating irregular 3D point clouds from an application perspective, too. While GANs have various applications if applied to images it is not obvious how artificially created irregular 3D point clouds can be useful. Although the theoretical insights presented in the paper are exciting, a more high-level motivation would further improve its quality.4)A discussion of shortcomings of the presented method seems missing. While it is understandable that emphasis is put on novelty and its advantages, it would be interesting to see where the authors see room for improvement. This paper proposes a method for the detection of adversarial examples via what the authors term "neural fingerprinting" (NeuralFP). Essentially, a reference collection of perturbations are applied to the training data so as to learn the effects on the classification decision. The premise here is that on average, normal examples from a common class would have similar changes in the classification decision when reference perturbations are applied, whereas adversarial examples (particularly those off the local submanifold) may have a markedly different set of changes from what was expected for the targeted class. These reference perturbations as well as the anticipated output perturbations together form the "fingerprints".To measure the difference between observed outputs and fingerprints, the average (squared?) Euclidean distance is used. Given a fixed set of input fingerprints (presumably chosen so as to provide coverage of the range of possible perturbation directions), the authors use the distance formula as a regression loss ("fingerprint loss") to train the choice of output fingerprints. Although the authors do not explicitly state it this way, this secondary training objective encourages a K-Means style clustering of output perturbations where the output fingerprints serve as cluster representatives. This learning formulation is to my mind is both very innovative and extremely effective, as demonstrated by the authors' experimental results. Their experiments show superlative performance (near perfect detection!) against essentially the full range of state-of-the-art attacks. They give careful attention to the mode of attack, and show excellent performance even for adaptive white-box attacks, in which existing attack methods are given the opportunity to minimize the fingerprint loss.The presentation of the paper is excellent - clear, well-motivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.Overall, the reported results are so good, and the approach so convincing, that one wonders what the weaknesses of the approach might be (if any). Questions that do come to mind are:* Can an adversarial strategy can be developed that could execute a successful attack while minimizing the fingerprint loss. * Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades? * What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.)Overall, this is a very strong and important result, fully deserving of acceptance.P.S. Two sets of typos that need attention:* In Equation 3, the Euclidean norm is taken. In Equation 5, the squared Euclidean norm is taken. Presumably, one of these is a typo. Which?* In the definition of delta-min and delta-max in the first paragraph of Section 2.2, y-hat should be w-hat. The paper proposes to perform a constraint optimization of an approximation of the expected reward function for unparameterized policy with subsequent projection of the solution to the nearest parameterized one. This approach allows fast ("nearly closed form") solutions for nonparametric policies and leads to an increase in sample efficiency.The proposed approach is interesting and the results are promising. In this work, the authors propose a novel method of estimating conditional distributions over arbitrary partitions of variables $x = [x_1,x_2]$ using an existing pre-trained flow model for $p(x)$. Their method fits a new *pre-generator* flow $\hat{f}$ for each observation which maps from a base distribution $\epsilon \sim N(0,\mathbb{I})$ to the latent variables $z \sim N(0,\mathbb{I})$ where the mapping $z \leftrightarrow x$ is learned by the pre-trained "base" flow $f$. The result is a mapping $\hat{f}$ which learns to shift probability mass to regions of the latent space which correspond to the conditional distribution of the given observation. The authors present comprehensive experimental results which show a clear improvement over existing methods for conditional inference but with the drawback of needing to re-train the pre-generator for each individual observation.Pros:- Very well written, clear, easy to understand- The proposed method is intuitive and well defined- Placement of the method relative to recent work is very clearly explained- Comprehensive theoretical analysis including an interesting hardness result, which is uncommon for the deep learning literature- Comprehensive and convincing empirical analysis with clear resultsCons:- There is very little discussion on how the construction of the base generator $f$ affects the results of the proposed method- The proof of hardness is somewhat opaque and feels contrived; but this is often the case with hardness proofs!- The method has a clear weakness in needing to be retrained for each observation. However, this is clearly stated by the authors and left open as a direction for future work.Overall, I think this is an exceptional paper which makes a significant contribution to the field. I think it is suitable to accept as-is with only a few minor adjustments which I will enumerate below.1. It would be nice (but not absolutely necessary) to see some discussion regarding the construction of the base generator, as I mentioned in the Cons above; e.g. does the performance of this method depend significantly on the user's choice of base model? Intuitively, I would think so.2. A few notes on the proofs:- The variable lower-case $m$ shows up in several spots in the hardness proof but is never defined. Perhaps these are typos and you meant to write $M$?- In the proof for equation 3, the notation for expectations (i.e. $\mathbb{E}$) is inconsistent in a few places. Presumably just typos.- I may be missing something, but it's not immediately clear why $y=T(x)$ can be substituted for the conditioner $x$ in $p_{\sigma}(\tilde{y}=y^*|x)$. My immediate intuition is that this would only be valid if $T$ is injective, otherwise this may change the underlying conditional density. Please correct me if I am wrong, and preferably add a clarification to the proof as to why this is justified.Congratulations to the authors on a job well done! The method works by substituting a hidden layer with a denoised version. Not only it enable to provide more robust classification results, but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution.Improvements in adversarial robustness on three datasets are significant.Bibliography is good, the text is clear, with interesting and complete experimentations. The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. Thought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC.Quality: strong, clarity:strong, originality:strong, significance: strong,Pros:- The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow.- The proposed method is able to improve in sample complexity, speed and convergence over past methods.- The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update.- Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory.  Cons:- It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour) This paper proposed a bio-inspired sparse coding algorithm where iterationsfor dictionary updates take into account the past updates. It is arguedthat time takes a crucial rule in learning.The paper is quite well written and contains an extensive literature reviewdemonstrating a good understanding of previous literature in both ML/DL and biologicalvision.The idea of using a "non-linear gain normalization" to adjust atom selectionin sparse coding is interesting and as far as I know novel, while providinginteresting empirical results: The system learns in an unsupervised way faster.Misc:- Using &lt; &gt; for latex brakets is not ideal. I would recommend: $\langle\,,\rangle$- "derivable" I guess you mean "differentiable"- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but thereference to Pedregosa et al. for sklearn is missing. Summary:This paper introduces a new supervised dimensionality reduction model. Supervision is provided in the form of class probabilities and the learning algorithm learns low-dimensional representations such that posterior cluster assignment probabilities given the representations match the observed class probabilities. The representations can be learned directly or the parameters of a neural network can be learned which maps inputs to the lower-dimensional space. The authors provide an extensive theoretical analysis of the proposed method and evaluate it on dimensionality reduction, visualization, and zero-shot learning tasks.Review:Overall, I thought this was an excellent paper. The idea is well-motivated, the presentation is clear, and the evaluations are both comprehensive and provide insight into the behavior of the proposed methods (I will not comment on the theoretical analysis, as it is entirely contained in the supplemental materials). I was honestly impressed by the shear volume of content in this paper, particularly since I found none of it to be superfluous. Frankly, this paper might be better served as two papers or a longer journal paper, but that is hardly a reason not to accept it. I strongly recommend acceptance and have only a couple of comments on presentation.Comments:- When trying to understand the proposed method, I found it useful to expand out the full objective function and derive the gradients w.r.t. to f_i. If my maths were correct, the gradient of the objective w.r.t. f_i can be written as the difference between the expected gradient of the divergence w.r.t Y and the expected gradient of the divergence w.r.t. the posterior cluster assignment probabilities. Though not surprising in and of itself, the authors might consider including this equation as it really helped me understand what the learning algorithm was doing. - The authors might consider adding a more complete description of the zero-shot learning task. My understanding of the task was that there are text descriptions of each category and at test time new text descriptions are added that were not in the training set. The goal is to map an unseen image to a class based on the text descriptions of the classes. A couple of sentences explaining this in the first paragraph of section 4.2 would help those who are not familiar with this zero-shot learning setup. Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively). In particular, they examine two methods for breaking the weight symmetry required in backpropagation: feedback alignment and sign-symmetry. They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance.The paper is clear, well motivated, and significant in that it advances our understanding of how recently proposed biologically plausible methods for getting around the weight symmetry problem work on large datasets.In particular, I appreciated: the clear introduction and explanation of the weight symmetry problem and how it arises in the context of backprop, the thorough experiments on two large scale problems, the clarity of the presented results, and the discussion about future directions of study.Minor comments:- s/there/therefore in the first paragraph on page 2- The authors claim that their conclusions "largely disagree with results from Bartunov et al 2018". I would suggest a slight rewording here: the authors' results *extend* our understanding of Bartunov et al 2018. They do not disagree in the sense that this paper also finds that feedback alignment alone is insufficient to train large models on ImageNet.- Figure 1: I was expecting to see a curve for performance of feedback alignment on AlexNet- Figure 1: The colors are hard to follow. For example, the two shades of purple represent the two FA models, which makes sense, but then there are two separate hues (black and blue) for the sign-symmetry models. Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models. This would make it easier to group the related models.- Figure 2: Would be nice if these colors (for backprop/FA/SS) matched the colors in Figure 1.- Figure 3: Why is there such a small change in the average alignment angle (2 degrees?) I found that surprising.- Figure 3: The right two panels would be clearer on the same panel. That is, instead of showing the std. dev. separately, show it as the spread (using error bars) on the plot with the mean. This makes it easier to get a sense if the distributions overlap or not.- Figure 3 (b/c): Could also use the same colors for BP/SS as Figs 1 and 2.- Figure 3 (caption): I think the blue/red labels in the caption are mixed up for panel (a). In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation.The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training.Although all the included ideas are not fully novel, the manuscript shows a relevant originality, paving the way for what can be a major breakthrough in deep learning theory and practice in the next few years. The paper is well written and organised, with the tackled problem well framed into the context. The suite of experiments is broad and diverse and overall convincing, even if the performances are not striking. Very interesting the biological interpretation and the proposal for the construction in the brain.A couple of remarks: I would be interested in understanding the robustness of the sign-symmetry algorithm w.r.t. for instance dropout and (mini)batch size, and to see the behaviour of the algorithm on datasets with small sample size; second, there is probably too much stress on comparing w/ (Bartunov , 2018), while the manuscript is robust enough not to need such motivation.Minor: refs are not homogeneous, first names citations are not consistent. This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model. I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data. The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below). So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.). I only have a couple of concerns: 1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from. The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges). Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used). Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.  The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, "here are some simple functions for which we would need the additional parameters that we define" makes sense; but arguing that Hartford et al. "fail approximating rather simple functions" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting). 2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work. I would have also liked to see a comparison to these methods in the the classification results.3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input. This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0]. So the two results coincide for the exchangeable case. Might be worth pointing this out. OVERVIEW:The authors tackle the problem of detecting small/low resolution objects in an image. Their key idea is that detecting bigger objects is an easier task and can be used to guide the detection of smaller objects. This is done using the "Feature Intertwiner"  which consists of two branches, one for the larger objects (more reliable set that is also easier to detect) and one for the smaller objects (less reliable set). The second branch contains a make-up layer learned during training (which acts as the guidance from the more reliable set) that helps compensate details needed for detection. The authors define a class buffer that contains representative elements of object features from the reliable set for every category &amp; scale and an intertwiner loss that computes the L2 loss between the features from the less reliable set &amp; the class buffer. They also use an Optimal Transport procedure with a Sinkhorn divergence loss between object features from both sets. The overall loss of the system is now a sum of the detection loss, the intertwiner loss and the optimal transport loss. They evaluate their model on the COCO Object detection challenge showing state-of-the-art performance. They also provide thorough ablation analysis of various design choices. The qualitative result in Fig.1 showing well clustered features for both high &amp; low resolution objects via t-SNE is a nice touch.COMMENTS:Clarity - The paper is well written and easy to follow.Originality &amp; Significance - The paper tackles an important problem and provides a novel solution. Quality - The paper is complete in that it tackles an important problem, provides a novel solution and demonstrates via thorough experiments the improvement achieved using their approach. QUESTIONS:1. The Class Buffer seems very restricted in having a single element per object category per scale to represent all features. The advantage of forcing such a representation is tight clustering in the feature space. But, wouldn't a dictionary approach with multiple elements give more flexibility to the model and learn a richer feature representation at the cost of not-so-good clustering ?2. Any comment on why you drop performance for couch ? (and baseball bat + bedroll)3. In Table 4 of Appendix where you compare with more object detection results, I find it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else. I was expecting you to be the best under that metric due to the explicit design for small objects. They (MaskRCNN, updated results) are also significantly better than the rest under AP_M but worse under AP_L. Can you explain this behavior ? Is the ResNeXt backbone that much better for small objects ? The paper introduces Variational Stochastic Differential Networks to filter and smooth sporadically observed time series.The authors adopt a Bayesian perceptive on the smoothing problem for time series living a latent space and irregularly observed.In particular, the random evolution of the process in latent space is clearly accounted for in the paper by embedding an SDE into an RNN.The authors derive a variational loss for their model and describe in fact multiple losses with different improvements such as importance sampling. In the end the authors do report issues caused by excess variance in training and settle for a convex combination of the two losses they propose.The paper becomes quite interesting in its experimental part as the authors show the superiority of their method as compared to previous approaches on data sets concerned with Mocap, synthetic OU process data and meteorological data. This paper investigates the extrapolation power of MLPs and GNNs (trained by gradient descent with mean squared loss) from a theoretical perspective. The authors show results of extensive experiments that back up their theoretical findings.  In particular, the authors study the question of what these neural networks learn outside the training distribution, and identify conditions when they extrapolate well. Their findings suggest that ReLU MLPs extrapolate well in linear tasks, with a fast convergence rate (O(1/\epsilon). GNNs (having MLP modules) extrapolate well when the non-linear operations are encoded in either network architecture or data representation, so as the inner MLP modules are aligned with only linear functions.The paper is well written, ideas and definitions clearly explained and experiments laid out in detail. The theoretical contributions of the work are important as they enhance our understanding of how these networks learn and how well they generalize. These findings help us design GNNs based on the data and problem at hand. As such, this work addresses a fundamental question in GNN understanding and must be published. Some comments/questions to the authors:- In Section 3.2, diversity of a distribution is informally defined in terms of training support and direction. A more thorough definition would be helpful. - The title of the paper is somewhat misleading: from feedforward to GNN insinuates that there are other network types that are discussed in the paper. This paper analyzes the extrapolate ability of MLPs and GNNs. In contrast to the existing theoretical works that focus on generalizability  and capacity of these models, this paper emphasizes the behavior of training algorithm using gradient descent. It takes analogy of kernel regression via the neural tangent kernel as an example to study the bias induced by the gradient descent algorithm. The presentation of this paper is clear and well-organized with the most significant result shown in the first section, raising interest of the readers, as opposed to leaving them behind a massive amount of proofs. The contribution of this paper is significant as well since it draws attention of the researcher to theoretical analysis on the bias induced from the implementations of the algorithms as compared to the theoretical analysis on the model structure itself. Model extrapolation is also closely connected to topics such as meta-learning, multi-task learning, domain adaptation and semi-supervised learning since the ability of model extrapolation will limit its performance when applied to other tasks. Pros:1. This paper has shown some interesting results: for instance, MLP with ReLU trained by GD will converge to linear functions along any direction from origin outside the support of the training data. This coincide with the idea that MLP are piecewise linear in different regions. The proof is complicated though and requires the analogy to the kernel regression as basis.  This result seems to suggest that the learning of MLP on data manifold supported by training data is also local linear and without support of training data, the induction follows the inertia of linearity.  It is curious to see if this is due to the piecewise linearity of ReLU function.  Maybe we will have better nonlinear extrapolation for MLP using tanh and other sigmoid functions. 2.  Comparison between GNN and Dynamic programming algorithm is very intuitive and inspiring. It suggests that max/min aggregate as opposed to more commonly used sum-aggregate in GNN is more suitable for extrapolation and the similarity between max/min aggregate GNN and DP is also very convincing. In general, this paper built up a good intuition before diving into the proof, which is well-appreciated. 3.   The suggestion to improve extrapolation is to put the nonlinearity into the architecture of the GNN or into the input representation is useful. For instance, replacing sum-aggregate to min/max aggregate helps to achieve good extrapolation. It also explains why the pre-trained embeddings such as BERT can be used in other tasks and still extrapolate well.Suggestions:1. Limitations of the study scope. This paper only discuss results of neural network using ReLU and GD. Although GD is widely used, the ReLU as the activation function plays a critical role in the study of extrapolation. It is necessary to provide analysis on the use of other common activation function to understand if the extrapolation ability is expanded. 2.  It is interesting to see more connection with domain adaptation and semi-supervised learning as well. The paper intends to contribute a novel task (Cache, as realized in AI2-THOR), the architecture of a strong Cache agent which learns reusable representations which allow significant transfer performance, and novel methods for evaluating the quality of dynamic image representations. The first and third contributions are directly related to the conference topics, and the second provides additional evidence in favor of the papers core idea: training on interactive gameplay allows learning flexible representations (in the sense of supporting many tasks via transfer) even when images are highly correlated and synthetic.The key strength of the paper is the very general core idea it advances and how this idea is explored via a novel task. The paper is easy to read and convincing. The partition into details needed for the papers core argument and details specific to the experiments in the appendix is well done. (If anything even more could have been pushed to the appendix.)One weakness of the paper is that a specific notion of flexible (which is mentioned in the title and twice in the abstract but nowhere else) is not advanced or integrated with the core idea. How does gameplay relate to flexibility? Why might flexibility be harder to achieve via passive learning or reinforcement learning with fixed reward functions? Because the authors place stress on the idea of how play and interaction contribute to representation learning (rather than a new method), slightly more space should be given to developing the general idea. The idea is not specific to vision, but only vision-related representations are considered. Sketching how the idea ought to work for text or audio would be useful if the focus really is on this very general idea.Recommendation: strong accept. The philosophical aims of the paper make it stand out amongst the mass of related work that is otherwise very engineering focused. The experiments are soundly executed in a way that ends up clearly demonstrating the core idea.Questions for the authors:- A step where the hider needs to retrieve the object they hid would seem appropriate. Are there certain limitations of the AI2-THOR environment that make adding this step (which would seem to expose more of the richness of the simulated world through fixed rules of the game) infeasible to add?- Inversely, do the authors feel that it was important that the hider manipulate the object into the desired location? How much of the richness of the simulated world comes through in the task feels relevant to the core ideas of the paper, but the paper currently does not address this kind of detail in the design of the Cache game within AI2-THOR.Section-by-section reactions: (to see how opinions change over time)Title+Abstract:- The notion of flexible seems to be at the heart of this papers intended contribution. Hopefully it will be defined in the body text. Uh oh, it looks like flex only ever appears on the first of the submissions 36 pages. Hopefully a synonym will get defined later.Introduction:- Excellent motivation.- Good that representations of interest (SIRs/DIRs) are named and distinguished. Many other papers, in the interest of highlighting end-to-end training, would forget to do this.- Good explicit list of contributions, excellent that two are specifically centered on representation learning.- Missed opportunity to highlight a distinct role for flexible representations. (I dont quite know what it should mean beyond supporting transfer well. A representation that could easily be scaled up or down in dimensionality by stripping channels in a well defined order might be considered flexible in another sense. Likewise, one that was defined in terms of pluggable input modules to work with novel combinations of familiar input types might be considered differently flexible. What kind of flexibility do you want?)Related Work:- Another take on learning visual representations via interactive gameplay is seen in https://arxiv.org/abs/1812.03125 where the authors learn a SIR (trained on a proxy task of predicting videogame memory state) that supports the use of low-continuous-space exploration strategies like rapidly-exploring random trees. The representations are learned offline/passively, but they are learned as to improve the efficiency of the very exploration process that builds that dataset for offline learning.Playing Cache in a Simulation:- It seems notable that the hiding agent is never asked to retrieve the object they have hidden. Without this step, the hiding agent may find ways of manipulating objects in a way that makes them simply unretrievable (e.g. the object is pushed into a corner in a way that causes it to glitch out of the room, etc.). A step like this would require the hider to learn a finer grained representation of the hiding location that gives itself a clue as to how it should be retrieved (e.g. under the couch in a place youll never be able to see but will be there if you actually reach for it).Learning to Play Cache:- Great!Experiments:- All well done.Discussion:- We believe that it is time for a paradigm shift via a move towards experiential,interactive, learning. -- something similar has been said by many other researchers in many different decades, so it would be good to say whats different about the situation in 2021. The difference now seems to be the availability of simulators with visual fidelity comparable enough to reality to demonstrate meaningful sim2real transfer. Are there other bullet points that could be added to a why-now argument? Summary and Contributions: This paper aims to find convex alternatives for deep learning based image reconstruction problems. This is well motivated by medical imaging, where there is risk of hallucinating unseen pixels, and there is high demand for robustness of training and interpretability of the prediction. For a two-layer convolutional neural network with ReLU activation, with weight-decay regularization, that is non-convex, this paper establishes strong convex duality, where the dual optimization is quite tractable and interpretable. It also shows empirical results for MNIST denoising and MRI reconstruction that support the claims.Overall, I enjoyed reading this paper. It is a solid work, very well written, and it has a balanced mix of theory and application, where the theory shows direct practical impact.Strong points:-   This paper is very well written and well organized. The motivation is very clear and sound.-   The proposed convex duality framework is solid and it fits nicely an important application with minimal assumptions.-   The interpretability offered by the dual network is very interesting. This is a novel and elegant view. In particular, the filters visualized in Fig. 4 are insightful to what a neural network learns. The clustering interpretation is also neat.-  The experiments with MNIST and fastMRI are convincing. In particular, the experiments with MRI are important and practically valuable, since in medical imaging deep learning has become the standard method nowadays for reconstruction, and, no or little is known about the interpretability of existing methods.Additional feedback and suggestions:- The appendix (Fig. 9) includes experiments with impulsive noise that show SGD for convex dual network converges to a better training/validation loss than the non-convex network. It might be better to move this to the main paper, as this is the message that the paper is trying to convey?- Typos and notation mismatch in the proof of Theorem 1 in the appendix: what is X in eq. 13? index j in eq. 12?- In the final version, it would be also useful to add the filter visualization for the MRI reconstruction as well. It would be interesting to see how dataset dependent the learned filters are.- I am wondering about the complexity of the dual optimization problem. Are there fast convex solvers for scaling up the computations? Also, about the required number of sign patterns, can the authors provide intuitions for the datasets used in the paper, say for MNIST? Correctness: The analyses are solid and the claims are correct. Relation to prior work: The paper has clearly discussed the connections with the previous works.Reproducibility: The details of the experiments and the public datasets are included in the paper. Thanks also for sharing the code. The paper presents a new approach to multivariate probabilistic time series forecasting. The authors propose to combine recurrent neural networks with conditioned normalizing flows to model the output distribution. They explain a few variations of the method and evaluate them against the baselines. *Quality*The choice of the output distribution is always a hard step in the probabilistic time series forecasting using RNNs. The authors propose an interesting workaround by trying to learn the shape of the output distribution directly with conditioned normalizing flows. In combination with a Transformer model, this also has the potential to reduce training/inference time for high-dimensional datasets. The authors explain and motivate the proposed method really well and I think the paper is a useful contribution to the field of probabilistic time series forecasting.*Clarity*The paper is very well-written. *Originality*To my best knowledge, the proposed approach is new.*Significance*Rather significant. The multivariate forecasting models suffer from the problems with scaling and the papers proposed combination of transformers with conditioned normalizing flows has the potential to overcome this problem.Pros* Conditioned normalizing flows can ease the design choice of the output distribution* The method scales well in terms of number of time series to forecast* Great empirical resultsCons* Since the scaling is a big issue for multivariate models, it would be useful to compare training/inference times between the evaluated methods. The work proposes a hybrid architecture that has: (1) language-specific (LS) components; (2) as well as the components that are shared across all the languages -- a trade-off between specificity and generality.  A key conclusion of the work is that the best architectures typically are. the ones that have ~10-30% language-specific capacity. In terms of experimental work, the work uses WMT-14 and OPUS-100 datasets to show the proposed trade-off. In terms of exposition of the ideas, it's a well-written paper for the most part. One issue that the authors could improve on is clarifying how "the amount of LS computation" is measured. You have mentioned it several times in the abstract/intro and it's neither clear nor referenced (it could be the number of parameters, it could be the number of basic computations, etc). For a new reader, it takes quite a while to find that $p$ is defined in eq. 6 and defined as a budget contains. One other quibble is that all the trade-off figures are shown based BLEU/automatic metrics, which are known to be inaccurate. It would be nice to repeat one of the included evaluation with human judgments. Overall, I view this as a good contribution to pave the way towards stronger, but reasonably-sized multilingual models. This is partially assuming that the authors will stay true to their promise that "Source code and models will be released." This paper describes a Deep Variational Bayes Filter (DBVF) for Deep-Learning based SLAM in 3D environments. It builds upon similar work for 2D environments in [Mirchev et. al. 19], and learns a full 3D RGBD occupancy map and a sequence of 6 DoF poses (localization) using raw stereoscopic camera data. Differentiable ray-casting and an attention model is described to access the learnt global map to give a local map and an expected observation - using an emission model from the current pose and local map. A transition model describing the evolution of the dynamics of the agent is also learnt. A variational approximation of the actual posterior (of the sequence of poses and the map, given the sequence of observations) is learnt by optimizing the standard ELBO equation from Variational Bayes. Such deep generative models, once learnt (in an unsupervised way) for an environment, allows one to hallucinate a sequence of poses and observations, given the learnt map and control inputs. This allows downstream robotic control tasks like environment exploration and path planning to be integrated into the model. Experiments on a simulated dataset with a flying drone in a subway and living room environments demonstrate good SLAM performance (that approach traditional methods): bird's eye view projections of the 6 DoF poses and the emitted maps closely match the ground truth poses and the occupancy grid. This is a well-written paper that represents a good step up from [Mirchev et. al. 19] to formulate a DVBF with realistic RGBD data streams. The authors mention that the computational times for this method is still far from conventional SLAM techniques - an actual quantification of the time taken during inference would be useful. #### SummaryThis paper examines the question of how to best estimate the bounding box distribution in probabilistic object detectors, specially regarding the use of non proper scoring rules and issues with the standard negative log-likelihood (NLL, where there is no ground truth for predictive variance).I believe this paper has many interesting findings that will be useful for the design and training of future probabilistic object detectors. Two main takeaways are that the NLL is not appropriate for training bounding box regressors since it produces high entropy distributions, penalizing overconfident predictions too much. The second message is that the way object detectors select bounding box regression targets with an IoU > 0.5 threshold affects uncertainty in the bounding boxes, since this prevents variations across all IoU ranges to be presented and learned by the detector.#### Reasons for ScoreThis paper provides a much needed evaluation of probabilistic object detection from a statistical point of view. Many Computer Vision researchers focus into chasing state of the art metrics (mAP, PDQ, etc) and do not put enough attention into statistical issues such as using non-proper scoring rules, which can hide details and hinder progress.I believe that this paper will bring a breath of fresh air to the probabilistic object detection field, from its takeaways there are clear directions to improve this kind of detectors and advance the state of the art into other than just chasing metrics. The quality of predictive uncertainty is very important for safe applications of object detection, for example in autonomous driving or robot perception.#### Pros- Very good evaluation, with multiple datasets (COCO, OpenImages, and corrupted variations of COCO), producing a robust results and conclusions.- Good selection of object detectors (DETR, Faster R-CNN, RetinaNet), including one stage and two stage detectors and the recent DETR that proposes a set-based approach to object detection.- Clear recommendations for future research: use the energy score for training, evaluate with multiple scoring rules, and put attention in the way how bounding box regression targets are selected, with a varied selection of targets being the best, instead of being limited to higher IoU values. We should also rethink the way probabilistic object detectors are evaluated, using proper scoring rules and multiple metrics, since the standard mAP and PDQ can hide details and differences between detectors.- Using the energy score even for training has a ~2% improvement in mAP over NLL which should also be noted.- This paper provides a more statistical view on object detection, focused on the bounding box regression problem of a probabilistic object detector, which is not that common in the literature, specially in Computer Vision.- The paper is well written and a pleasure to read.#### Cons- I believe that the energy score might be a bit problematic for object detection, since it requires sampling from a Gaussian, the loss then becomes stochastic (adding noise to the process) and sampling bounding boxes might become a bottleneck during training. This is a minor issue.- The paper focuses into predictive uncertainty, instead of separating epistemic and aleatoric uncertainty. This is explicitly mentioned in the paper and only a minor issue, since separating both kinds of uncertainty is difficult.#### Questions for Rebuttal PeriodCan you motivate the selection of object detectors? There are many probabilistic object detectors, including Gaussian YOLO for example, so a motivation for these specific detectors would be an improvement.Can you also motivate the selection of the scoring rules? Why is were the energy score and DMM selected, were other alternatives considered?#### Minor Issues- In Section 3.3 and Figure 1, please specify the values of p for DMM and M for the energy score (I assume it is 1000 as mentioned in the Appendix) that are used for evaluation.- In Figure 3, please add that these results are produced using DETR to the caption, it will be easier to read and interpret.- This paper has a very statistical view on the object detection problem, maybe it is worth to also take a look at per-class metrics and visualize the produced bounding boxes, some of this analysis could be added to the appendix. Aggregated metrics could also hide details in some classes.- I think the title could be more informative, since this work focuses into bounding regression uncertainty/variance, this could be part of the title. Summary:The paper presents VECO: a pre-trained encoder-decoder model which is capable of both cross-lingual understanding and generation. They present two losses for inner-sequence and cross-sequence understanding. These components are then used to either build a encoder-only model or an encoder-decoder model. They present results on XTREME for cross-lingual understanding and on MT for generation. The results on both tasks are quite competitive and clearly shows the benefit of using both monolingual and parallel data with IS_MLM and CS-MLM.Reasons for score:I vote for accepting the paper. The paper initializes from XLM-R and then fine-tunes it on monolingual and parallel data in 50 languages. The authors get +3 average gain over the previous best system on XTREME and were ranked #1 at the time of submission. They also get handy gains in the MT benchmarks. The two ablation experiments show that the new CS-MLM task is indeed beneficial and improves the performance. One of the concerns I have is that the authors are not upfront about their model being in just 50 languages and hence might not be comparable to other models like XLM-R which support 100+ languages.xCons:- As mentioned above, please be upfront about training only on 50 languages. It's only mentioned in the Appendix. This needs to be mentioned in the main text and maybe even in the results table.- I would like to see more information about the amount of data (both monolingual and parallel) listed clearly in the Appendix.- What would happen if VECO was trained on 100 languages from XLM-R? That result would be interesting to see in Table 1.- I would personally like to see more ablation experiments: experiments where the amount of pre-trained monolingual and parallel data were reduced independently to see the impact it has performance.Minor comments:In Section 4.1, kindly cite all the representative tasks in XTREME as suggested here:https://github.com/google-research/xtreme#paper- It would be great if the authors stated the number of monolingual and parallel examples explicitly in Section 3. Appendix A doesn't provide the number of examples but just the size (1TB) of parallel data used.- Please provide breakdown of amount of monolingual data and bilingual data per language/language-pair. How many languages do you get monolingual data in? It's not clear from the paper.- Also, please be explicit about the number of languages supported in the model (50). This is hidden in Appendix A. One can argue that this is not a fair comparison against XLM-R since it's trained on 100+ languages.- Change "Ours implementation" to "Our implementation" everywhere.Section 7:- Change "targeting at initializing both..." to "targeted at ..." This paper introduces PARROT, a novel approach for pretraining a reinforcement learning agent on near-optimal trajectories by learning a behavioral prior. Essentially, the authors learn a word2vec style embedding of actions for a simple virtual single-arm environment. This embedding will naturally place more common examples from its training data towards the center of the gaussian, making sampling them during training time more likely. The authors demonstrate that this approach outperforms existing pretraining methods in this domain. This paper presents an interesting novel approach and presents strong support for its value. In particular I appreciate that the basic intuition is relatively straightforward and therefore should be relatively easy to test in new domains. In addition, the evaluation results are impressive. The paper claims in two instances that human examples would be appropriate for the training data. However, this is not confirmed in the current evaluations to my understanding. The paper indicates that the training data should be near optimal multiple times but never explicitly defines what is meant by near optimal. Is it that actions must be useful (also vague)? More clarity on this would be appreciated. I have some concerns around certain claims or arguments made in the paper (more on that below). However, this paper still ticks all the boxes for me in terms of novelty and value, and so I would argue for its acceptance. Some questions for the authors: 1. What is meant by near optimal? Can this be more formally defined?2. The constraint that the action dimensionalities are fixed seems like a large one. I understand that its necessary for the invertible requirement, but I could imagine that a similar approach to this could still be helpful in sim2real problems without this constraint. Would the authors agree?3. The paper domain is still relatively simple compared to real world problem domains, to what extent do you expect this approach to generalize? Some smaller points/additional feedback: - The introduction and abstract are a bit vague, and I didnt grasp the approach intuition until figure 1. Id appreciate if some of this information could be moved up into paragraph 3 of the introduction. -Id replace large such datasets with such large datasets or just remove such-I recognize that theoretically that the approach should be able to represent every possible environment action, but it would seem to me still possible that good/useful actions could be placed so far from the center of the Gaussian as to make them very unlikely, depending on the training data. Some discussion about how to avoid this/whether it is a concern would be helpful.  Overall, I found this to be an excellent paper. The topic of generalizing to new environments is clearly important and the authors do a good job motivating this problem. I found the paper well written and clear, with many intuitive examples. I found the method compelling and the theory appears correct. The experiments were well designed and convincing. Insofar as I have concerns with the paper, they relate to clearly communicating the specific setting considered by the authors and contrasting their work with others (details below).--- Comments --- 1. One piece that was unclear to me was why if was necessary to assume that $\mathbb{F}$ remains fixed. Shifts in $\mathbb{F}$ are certainly possible in real applications (e.g., consider a change in a treatment policy relating observed lab measurements to observed treatments at a particular hospital). Is this a constraint on the method? That is, if the observed environments contain shifts in $\mathbb{F}$, would the proposed method fail to produce a model that is robust to those shifts?2. More generally, I didn't think the assumptions were clearly communicated. I think paragraph 2 of A.3.1 should probably be moved into the main paper.3. I found the example in the intro a bit unclear. I would try to communicate earlier what you are hoping to show with the example. Additionally, at this point in the paper, it is not really clear what a "causal solution" is or how it differs from the proposed solution. 4. I thought the remarks following Equation (5) were very helpful and would recommend adding a similar high-level discussion after Theorem 1. Something like: The first term on the RHS is the expected loss and the second term is zero if the constraints discussed above are satisfied, thus by minimizing the constrained expected loss, we are minimizing an upper bound on the LHS.5. By the time I got to Equation (6) I found myself wondering why *this* robust objective is better than all the others. This was then addressed, in part, by the last parts of 3.1 and 3.3, but I would consider including a more explicit contrast between these various objectives earlier in the paper. I think something like Section 2 of Kreuger et al. (2020) would help contextualize your contribution a bit better.6. Links to Equation (9) should be swapped for Equation (1) (e.g., page 1, par 2).7. I would recommend swapping $\alpha$ for another symbol since $\alpha_e$ is also used. BeBold: Exploration Beyond the boundaries of Explored RegionsThe authors address the problem of exploring efficiently for a reinforcement learning agent when the reward function is sparse. They propose to compute an intrinsic reward based on the inverse visitation count to reduce the visit imbalance generated by optimizing for the extrinsinc reward.This work present an interesting approach to progressively explore at the boundaries of the most visited states (a phenomenon that occurs when the agent focus on maximizing known rewards). They address the key problem of short-sightedness in this domain. The contribution is clearly stated by the authors. The results support the claim that their method achieved state-of-the-art performance of the Minigrid environments and Nethack. They also provide an analysis of the necessity of clipping and having an episodic restriction on the intrinsic reward (ERIR).A couple of remarks however: - the context should be clarified: you mention a quite classical RL setting, which assumes is done by interacting with the environment. In that case, one can explore via action selection and potentially discover new states (by trying new actions or experiencing an unexpected outcome). On the other side, you method tackle the problem of visiting properly a known space (as the effect of the method "marks" actions that transition from well-visited to poorly visited parts of the state space for the next episode). This should be made explicit.- How is this augmented reward information used? I didn't see a clear mention of the learning algorithm implemented in the agent, please make that explicit.As a main criticism, I's day that the underlying assumption of this work is that the environment is somewhat known and that "exploration" only happens within this known environement. It is critical to the method that the action selection process itself moves the agent towards fully-unknown parts of the state space.Section 6 should be clarified on how your methods and the related work actually relate ; it is only done for the last paragraph but should be clarified for the other approaches you cite.Typos:p4: "from MiniGird: ..."p6: "... heatmap of normalizd visitation counts..."p8: "... maximizng mutual information" This paper describes a neural vocoder based on a diffusion probabilistic model. The model utilizes a fixed-length markov chain to convert between a latent uncorrelated Gaussian vector and a full-length observation. The conversion from observation to latent is fixed and amounts to adding noise at each step. The conversion from latent to observation reveals slightly more of the observation from the latent at each step via a sort of cancellation. This process is derived theoretically based on maximizing the variational lower bound (ELBO) of the model and follows Ho et al. (2020) who derived it for image generation. Thorough experiments show that the model produces high quality speech syntheses on the LJ dataset (MOS comparable to WaveNet and real speech) when conditionally synthesizing from the true mel spectrogram, while generating much more quickly than WaveNet. Perhaps more interesting and surprising, however, is that it generates very high quality and intelligible short utterances with no conditioning, and also admits to global conditioning, e.g., with a digit label.The paper is very clearly written, with the description of the model going into sufficient detail in the main body of the paper for the reader to understand it without getting bogged down in the less immediately relevant details. The experiments are thorough and well executed, comparing with listening tests to many state of the art neural vocoders for the conditional task. It describes a thorough evaluation of the unconditional generation task, which is in general difficult, but in this case was constrained in such a way as to make it feasible and informative, using reasonable metrics that clearly show the advantages of the proposed approach. The literature review is thorough and comes at a point in the paper where the reader understands the proposed approach and can appreciate the nuances of the differences between the approaches. Audio samples are provided on a companion website and demonstrate the effectiveness of the approach along with some additional interesting properties of the model not even mentioned in the paper (denoising most impressively, interpolation between speakers is less convincing). The paper has two minor weaknesses. First is that it does not make a clearer distinction between the concurrent work from Chen et al (2020) along similar lines, although presumably this paper was not released prior to submission of the current paper. An extended comparison would be welcome in a camera ready version of this paper. Second, that it doesn't explicitly state the real-time factor of WaveNet generation in the results discussion on page 6, which is presumably much smaller than 1. This section compares to WaveNet in terms of quality and WaveFlow in terms of speed, slightly being slightly worse in both comparisons, but better in the opposite, partially missing, comparisons.Overall, this paper makes a strong contribution to the field of neural vocoding and to the field of representation learning more generally for long-duration intricately structured signals (i.e., speech). Recently, there are a large number of deep learning theory papers related to the property of neural tangent kernel. This paper shows that for ReLU, the kernels derived from deep fully-connected networks have the same approx. properties as their shallow two-layer counterpart. This highlights the limitation of the kernel framework for understanding the benefits of deep networks from such perspective.I really like the idea of this paper. This paper is very well-written. I verified the proofs, it looks correct. I think this paper should be clearly accepted. Here are some minor comments about references.In page 1, several references should be added into NTK literatureSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks (ICML)Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. (NeurIPS)Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks. (NeurIPS)Jan van den Brand, Binghui Peng, Zhao Song, Omri Weinstein. Training (Overparametrized) Neural Networks in Near-Linear Time. (ITCS)Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.In page 6, there is a section talking about ''finite neuron case''. It might be reason to slightly mention about ''infinite neuron case''. The work by Arora et al and Lee et al, shows a connection between infinite case and (ridge) kernel regression.Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. (NeurIPS)Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. (NeurIPS)  Summary\The key message of this paper is that input-gradients (gradient of the logit wrt to input) or loss-gradients are/might be unrelated to the discriminative capabilities of a DNN. The input-gradient is a key primitive in several interpretability and visualization methods. Until now, it has been taken as a given that these gradients reveal 'why' or what parts of the inputs the model is sensitive to. However, this paper questions this reasoning and says that if the input-gradients can be easily manipulated without changing the generalization ability of the model, then does the input-gradient really contain discriminative signals?  To test their hypothesis, the paper re-interprets the input-gradient as class-conditional generative model using the score matching view. The paper then develops a 'regularizer' that is called a taylor trace estimator that requires less backward passes than the hutchinson estimator, which when added to the model objective can modulate how 'generative' the model is. With this regularizer, the papers tests the hypothesis that improving the implicit density model also improves input-gradient interpretability. The paper tests this through experiments on an image dataset and finds that this is the case.Significance\This work has far reaching significance for the field of visual interpretability of DNNs. It suggests that reading into these input-gradients might be akin to reading tea leaves. The key insight in this work is simple and the demonstration is quite powerful in my opinion. The argument in this paper seems obvious in hindsight, but that is exactly why the paper is a significant one. I have several additional questions later in my review, but this work is important and suggests that insights based on input-gradients might be spurious. Clarity\Overall, the paper is relatively clear and easy to read. Several of the key experiments are well-justified.Originality\The insight in this paper via the score-matching perspective is new in the interpretability domain. The claim that input-gradients can be easily manipulated is not new, but the general insight in this work is new and important. Overall, this paper opens up several questions about what input-gradients really convey. Taylor Trace Estimator\I am confused about the derivation of this estimator and I am probably missing something, so can you walk me through this? Let's say the Taylor series expansion around a point x is: $f(y) = f(x) + \nabla f(x)^\top(y-x) + \frac{1}{2}(y-x)^\top\nabla^2f(x)(y-x) + \mathcal{O}(\left\Vert y-x\right\Vert^3)$now if solve for $(y-x)^\top\nabla^2f(x)(y-x) $, we get: $\frac{1}{2}(y-x)^\top\nabla^2f(x)(y-x) = f(y) - f(x) -  \nabla f(x)^\top(y-x) -  \mathcal{O}(\left\Vert y-x\right\Vert^3)$$v= y-x$ in your notation, and it is a zero mean gaussian, so we get: $\mathbb{E}[\nabla f(x)^\top v] = 0$, which leads to the approximation that you get. However, where did the $\frac{1}{\sigma^2}$ outside the expectation come from in the final form?The point on Adversarial Training\In section 4, this paper notes that recent work has shown that when a model is trained with explanation penalization, it results in more 'interpretable' gradients. This connection was made more formal in recent work (https://arxiv.org/abs/1810.06583.pdf)  that notes that training models while penalizing their integrated gradients explanations is equivalent (Thm 5.1 in that paper, for the right loss function and some other assumptions) to $\ell-\infty$ adversarial training. Other Feedback and Questions-  The current section 4 is really a discussion/implications section. I suggest the authors call it that. It should also likely come after section 5 since that is where the experimental results are. From reading section 4, am I right to conclude that the paper is also suggesting that activation maximization, pixel perturbation, and the results of adversarial training say more about the implicit generative model than discriminative information for a DNN? That is, I should also not take the results of activation maximization as explaining to me what a neuron that learned?- Do these results extend to methods that post-process or use the input-gradients as a primitive? For example, smoothgrad adds noise to the input and takes an average of the corresponding input gradients. Integrated gradients can be seen as as sum of interpolated input-gradients along an all-zeroes input to an input of interest. Should I also take it from these results that Smoothgrad and integrated gradients don't indicate discriminative behavior as well?- For example, consider grad-cam, which looks at the output of a convolutional layer in computing a sensitivity map as opposed to the logits, does your analysis apply to that case too? I think it probably doesn't, unless one can also view the output of convolutional filters are implicit density models as well. - Does the analysis in section 2 apply to the probability output from softmax? I.e., can the 'probability-gradients' also be arbitrarily manipulated? Overall, this work raises several important questions like why trained models have implicit density models that are aligned with the inputs in the first place. This question seems key to tying up the remaining loose ends in this work. This said, this paper is still a thought-provoking one and a useful one for the literature on DNN interpretability.  This paper examines gradient-based attribution methods that have been proposed in the explainability literature from a theoretical perspective motivated by a recent observation in energy-based generative models. First, the authors point out a general weakness of gradient-based attribution that derives from the fact that input-gradients do not provide well-defined explanations, since the shift-invariance of the softmax output makes them arbitrary.The authors then propose that the reason for the success of gradient-based attribution models can be explained by the fact that discriminative models "contain an implicit" class-conditional density model (the mentioned recent observation about energy-based generative models).They then go on to elaborate on this idea showing how aligning the implicit class-conditional generative model to the "true" generative model of the data would help provide relates to gradient-based attribution, how the alignment can be efficiently promoted with a novel implementation of score-matching, and how this mechanism can be practically realized as regularization costs.The authors then carry out empirical studies that convincingly confirm the prediction of their theoretical ideas. First, they show that samples generated with score-matching and the proposed gradient-norm regularization are better in the sense of being less noisy and in terms of their discriminative accuracy via a trained discriminative model as proposed by the "GAN-test approach".Finally, they show that the quality of gradient-based explanations are better according to a discriminative version of the pixel perturbation test, a method to evaluate gradient explanations by perturbing pixels ranked in increasing order of relevance.In conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy-based generative models, and gradient-based explanations, uses this theoretical framework to explain how gradient-based explanation are overcoming the softmax shift-invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations. The paper presents a disentanglement metric to measure the intrinsic properties of a generative model with respect to the factor of variation in the dataset. Toward this, the paper first assumes disentangled factors reside in different manifolds. These different manifolds are the sub-manifolds of some manifold M for a given disentangled generative model. The paper considers the fact that in an entangled model the sub-manifolds are not homeomorphic and thus similarity across submanifolds can be measured to evaluate a models disentanglement. As such, disentanglement is related to the topological similarity.  For measuring topological similarity, the paper then introduces Wasserstein Relative Living Times. The proposed metric is used to evaluate standard disentanglement methods and datasets demonstrating the importance. Pros:- The paper is well written and the proposed metric is well articulated.- The manifold interpretation of disentanglement (section 3) is clear and could be considered a stand-alone contribution to the disentanglement community. - The experiments covered depth in terms of dataset selection and the number of models considered for the evaluation of the metric. Few questions/suggestions:- Can the authors briefly describe why W. Distance defines a valid metric on barcode space and others dont, or point out to relevant literature in section 3.1?- In Li et. al., 2019, MIG-sup was proposed to remedy the weakness of the MIG metric. For the unsupervised portion, since MIG was considered a comparision, can authors also compare their method with MIG-sup? Or at least discuss it along with MIG? Li, Z., Murkute, J. V., Gyawali, P. K., & Wang, L. (2019, September). PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS. In International Conference on Learning Representations. This is a great investigation on how to scale the gain of the inhibitory weights to balance the impact that the changes that the excitatory and inhibitory connections have on the layers output. I think using the KL distance that naturally connects with the Fisher Information is neat. I appreciate the effort that the authors make to connect the manner neural circuits are designed and connect it with ANN. You never know when the breakthrough can arise.I love the experiments that the authors present illustrating with clarity the impact that having the proper gain modulation of the inhibitory changes have in the speed of convergence.My single constructive criticism is that the inspiration in cortical circuits do not prevent the authors to get inspiration from smaller neural circuits like in insects for example. The Mushroom Bodies of the insects are the equivalent of the cortex and present feedforward inhibition. The number of layers is much smaller but the neural principles that operate are fairly consistent across multiple animal species. Drawing from that experience, the mutual inhibition within layer may provide a natural mechanism to keep balance in the output distribution as shown for example in mean field models that investigate the regulation of activity in a dynamical neural layer (see for example https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003133). Other that this comment I learn and enjoy from reading this paper. I think it should be accepted. This paper presents a new architecture called BGNN (Boost-GNN), which combines the benefits of GNNs (Graph Neural Nets) with GBDTs (Gradient Boosted Decision Trees).Basic idea:* GBDTs work well with *heterogeneous* tabular data.* GNNs work well on *graphs* with *homogeneous* sparse features.* BGNNs work well on *graphs* where the nodes contain *heterogeneous* tabular data.* BGNNs is optimized end-to-end and seems to obtain great SOTA results!An example of the data BGNN works well on is in social networks. E.g. each node could be a person with heterogeneous characteristics such as age, gender, graduation date.The main trick is in how to train this end-to-end effectively. This is done by iteratively adding trees that fit the GNN gradient updates, allowing the GNN to *backpropagate* into the GBDT.More in detail:* GBDTs make a prediction for each node in the graph.* The GNN reads the output of the GBDTs and corrects the predictions based on the graph structure.* When doing gradient descent on the GNN the authors critically also optimize with respect to the input features. The difference between the optimized input features and the original input features becomes the new objective for next round of update on the decision trees.Results seem very strong across all tasks considered in the paper.Strengths:* The paper presents an interesting and convincing analysis of why GBDTs are so advantageous for tabular data, while GNNs are the best choice for graph data.* The combination of GNNs and GBDTs is not trivial. Several papers are cited attempting the combination of neural nets and gradient boosting, but they are reported to be computationally more expensive and not as powerful.* The detail about computing updated targets for GBDTs while doing gradient descent on the GNN seems particularly interesting and creative.* The results obtained by this paper are impressive, surpassing SOTA models by considerable margins.* The paper also finds that representations learned with BGNN have more discernible structure, suggesting that they are more interpretable. As a new framework for object detection, DETR is very important. However,  it suffers from slow convergence and limited feature spatial resolution. This paper proposes deformable attention, which attends to a small set of sampling locations rather than all the locations in the original DETR. Besides, the paper applies multi-scale deformable attention for better results.The paper is well-written and obtains very impressive results. Traning for only 50 epochs, deformable Detr obtain results similar to DETR which is trained for 500 epochs. By implementing a two-stage detector based on deformable Detr, the paper obtain state-of-the-art object detection results with a very high AP (52.3) on AP. A few suggestions for improving the paper are given as follows. (1) The training and testing times could be reported in the paper, which is useful for other researchers to implement and use this method. (2) Some related methods on sparse connected self-attention/transformer [a,b,c] should be cited and discussed. [a] Representative Graph Neural Network, CVPR 2020 [b] Dynamic Graph Message Passing Networks, CVPR 2020 [c] CCNet: Criss-Cross Attention for Semantic Segmentation in ICCV 19 & TPAMI 2020 This paper proposes an interesting application of Langevin sampling with the energy-based model (EBM) to defend against adversarial attacks. Compared to adversarial training (AT), the proposed adversarial preprocessing (AP) based method can be used to secure existing models without the need for retraining.As an EOT based defense, naturally, a larger number of replicates would lead to better adversarial robustness. The idea behind the proposed defense is to leverage the long-run sampling using EBM for the EOT based defense. However, training long-run EBM is challenging. To overcome this, the authors apply an interesting two-phase training method with Adam for the first phase to quickly train the short-run one and SGD for the second phase to gradually align the long-run one with the short-run one.The authors compared their approach to a large number of defenses as well as the state-of-the-art AT based defense. The reported results are very impressive, with an 84.12% benign accuracy and 78.9% adversarial robustness on CIFAR-10. In addition, the proposed method can still achieve 54.9% adversarial robustness even under an adaptive attack, which is close to the state-of-the-art AT based defense. However, compared to AT, the benefit of the proposed method is that it does not require retraining the model. Strength:- Achieve robustness performance similar to adversarial training without tampering with the classifier.- An interesting two-phase training method to address the non-convergent problem for long-run sampling.- Consider an EOT attack as the adaptive attacking method.Weakness:- Overall, it is hard to find flaws in this paper. The only concern I have is that the timing overhead is still large (~10 sec per image when K=1500, Fig. 10) and seems can hardly be parallelized since the long-run sampling is serial. This might greatly limit the applicability of the proposed defense when timing constraint is critical. The authors introduce and formalize the concept of Invariant Learning Consistency (ICL), which is motivated by the idea that "good explanations are hard to vary" in the context of deep learning. Instead of using the arithmetic mean to pool gradients (logical OR), the authors propose to use the element-wise geometric mean of gradients with a logical AND masking. Experimental results on both synthetic and real-world data sets are reported under the setup of supervised learning and reinforcement learning. This paper is well written and clearly presented. The exploration of using geometric mean with a logical AND masking to pool gradients in deep learning is very interesting and novel. The proposed method learns invariances in gradient-based optimizations and can help with the memorization issue. This work has made good efforts towards a better understanding of learning, memorization and generalization of OOD. Experimental results on both synthetic and real-world data sets have demonstrated the effectiveness of the proposed AND-mask, comparing with commonly used regularizers and several baselines. As the authors point out, the AND-mask is one of multiple possible ways to improve consistency, and it is unlikely to be a practical algorithm for all applications. It would be great if the authors can provide further insights on what kind of data distributions (or applications) could potentially benefit from the proposed method versus using the arithmetic mean. In addition, it would be interesting to explore hybrid methods that combine the advantages of pooling gradients using arithmetic mean and geometric mean (i.e., AND-mask). My overall feeling about this paper is that I really liked it. I felt it was clearly written, nice pacing, didn't bombard us with things we already know, nor did it skip over things we don't know about. I like the approach of using the sigmoids to be able to learn the thresholds and the weight pruning. I really like this approach. In practice, there are still several hyper-parameters to tune, which are apparently model-specific (table 1), and it seems unclear how to directly set the sparsity (we can only influence it using lambda, not set it directly, as far as I can see?). Nevertheless I do really like this approach and direction, ie of learning things as much as possible. The approach seems to me analagous to using eg Adam instead of SGD.I feel the experiments section could be written a bit more clearly so as to make the assertions in the conclusion and introduction stand out as really obvious. For example, the assertion that LTP can be used in the presence of batch normalization was not made apparent in the experiments section I felt, nor was it shown as a benefit compared to other baselines, I felt.Details:'computationally extensive' => 'computationally expensive'figure 1 really far from where it is first referenced. had to hunt for it....writing really clear. Good exposition of related papers. Pacing very nice. Very easy to understand.Good choice of which information to present. (cf papers that present lots of well-known knowledge, or skip over some key advanced concepts).Kind of a detail, but I like the use of $sigm$ to denote sigmoid, (cf many papers will write the sigmoid out in full, which makes the equations much harder to read)Personally, I would prefer that $\sigma_T(w_{kl}) = \frac{\partial sigm((w^2_{kl}-\tau_l)/T)}{\partial w_{kl}}$ is defined before equation 3. Otherwise I first read equation 3, wondering what is $\sigma_T$, and then realize it is defined underneath.To be honest I'm not a fan of the notation $\sigma_T(\cdot)$, since when I first read it I parsed it as $sigm_T(\cdot)$. I would prefer either using some other symbol, or perhaps writing out the full partial derivative, though that seems probably long, without some other symbol, or perhaps adding a $'$, like $\sigma_T'(\cdot)$.I'd also prefer that the definition of $\sigma_T(w_{kl})$ includes the derivative in this definition, ie:$$\sigma_T'(w_{kl}) = \frac{\partial}{\partial w_{kl}} sigm\left(\frac{w^2_{kl} - \tau_l}{T}\right) = ... etc ....$$I like the paragraph that describes the behavior of $\sigma_T'$I like the exposition of the various regularization methods. Concise and yet easy to understand.$\eta$, in equation 8 is not obviously defined anywhere. I went hunting for it, but couldn't find it. Please define it before using it.Equation 8 comes out of nowhere, with no explanation of what it means, or why it is. The rest of the paper explains things very well, but equation 8, I'd have to think a lot, wasn't immediately obvious to me where it comes from, what it means.I'm not sure what the syntax $\sim T$ means here. It normally means 'is distributed as', but that meaning doesn't seem to make sense here? It looks like it's being used to mean $\approx$?.Ok, I had to go all the way back to equation 2 and the paragraph after it. Looks like $\sim$ is being used here to mean $\approx$. I think that using $\approx$ would be more standard, and easier to understand? Ok, I googled $\sim$, and it turns out that it can often be used to mean 'is approximately the same order of magnitude as', eg https://math.stackexchange.com/a/2177014/45703  But I personally found it confusing because it is very often used to mean 'is distributed as', so personally I would prefer to have a short explanation like 'where $\sim$ means "is approximately the same order of magnitude as"'I guess the other reason I find it confusing though is this equation implies to me that $|w^2_{kl} - \tau_l| = 0$ would not be in the region, but in fact the region is I feel something like:$$|w^2_{kl} - \tau_l| \lesssim T$$Personally I think I would prefer the conditions for the transitional region written in this way; would be less confusing for me; I think.Similarly equation 5 would be:$$\sigma_T(w_{kl}) \approx \frac{1}{T}, \text{ for } |w^2_{kl}-\tau_l| \lesssim T$$and then equation 8 becomes:$$\eta \cdot \left| \frac{\partial \mathcal{L}^T}{\partial w_{kl}}\right| \ll T , \text{ for }  |w^2_{kl}-\tau_l| \lesssim T$$I'm not sure though why this derivative is in this constraint? Isnt the constraint simply that$$\sum_{l=1}^L \sum_{k=1}^K I\left[ |w^2_{kl} - \tau_l| \right] > m$$where $I[\cdot]$ is an indicator function, and $m$ is some positive integer?ie, at least some points need to be in the transitional region. I'm not sure I follow why we need a derivative in the constraint. Please can you add some description around equation 8 so I can follow what is going on :)And also trying to work through equation 8, it seems like it is saying that we want to make the derivatives as close to zero as possible, relative to T. But isnt this the opposite of what we want? Dont we want to have a reasonable number of derivatives that are not near zero?Ok, after equatino 9, we get some explanation for equation 8 :) But I think the explanation could be moved forward somewhat :)))Ok, based on this explanation, ie the one after equation 9, $\eta$ is probably learnin rate. But please define $\eta$, near equation 8 :)From the explanation, I'm not sure that equation 8 is a condition that actually *prevents* premature pruning, so much as a heuristic to minimize weights moving too quickly out of the transitional region. Preference to be clearer about this, since it would certainly have helped me to understand equation 8 more easily and quickly :)equation 11, the brackets could be nicer looking if use "\left(" and "\right)", I feel (so they are as large vertically as the derivative fractions they contain)$\lambda$ in equation 11 is not defined. From section 4.1, we can see it is a hyper-parameter to be tuned, but that is not stated in equation 11. Preference to state at equation 11 that $\lambda$ is a hyper-parameter, and $\eta_{\tau_l}$ is the learning rate for the threshold of layer l. Hmmm, does this mean that each layer has its own learning rate to tune for its threshold? If there is one single learning rate for the thresholds, I think it might be clearer to represent it as $\eta_\tau$? If there are per-layer learning rates, then this seems to be to contradict the implied promises in the introduction that we don't have per-layer hyper-parameters to set?Ok, looks like $\lambda$ was first used in equation 9. But still wasnt defined there I think?I'd also prefer that equation 9 was defined before presenting equation 8, on the whole. This way I can read sequentially, not have to skip forwards and backwards.The sentence just before and after equation 12 is very complex and hard to take in. Please consider breaking into smaller simpler sentences. eg "$\partial L_T/\partial w_{kl}$is given by 9, 10, 3 and 7 as:(equation goes here)This includes $\sigma_T(w_{kl})$, which from equation (5) $\approx 1/T$, and will become large for $T \ll 1$. This means that the gradient will become large, and constraint (8) will be violated."Figure 1 I feel needs a lot more explanation.- why are they so symmetrical about the y=x line? doesnt this imply that the number of weights less than theshold before pruning and the number of weights less than theshold after pruning is similar?- why is the y-axis labeled 'w'? I thought the pruned weights are 'v'?- why make the plot? What is the motivation of this plot?- why is the scale of the axes radically different between left and right (0-16 vs 0-3) ?- why is the left hand plot preferable to the right hand plot?- why is the gap around threshold in the right hand plot a bad thing?- why is the proportion of weights below threshold similar in both left and right?4. experimentsI like the table of hyper-parameters in table 1. (cf many papers skip over which hyper-parameter settings were used, making reproducing the work challenging)Appreciate the explanation of the significance of $\lambda$ as the primary hyper-paramter determining the sparsity levels. I feel that this explanation could be moved back to equation 9.Appreciate the observations on how to set $\lambda$, and $\eta_{\tau_l}$.I wouldnt really call figure 2 an 'ablation study'. It's more like a comparison study of various baselines and approaches I feel? An ablation study I feel would be more like:- no regularization- no drop second term in 12 (and simply not use any clamping etc in its place)I think the ablation study should go after the imagenet pruning results. (I mean, I think it's traditional to put ablation studies after the section of results vs other baselines/sota models)Table 2 is very unclear to me- why is your method not at the bottom of the table- I think your own method should be in bold and say "(ours)" after the name- looking at the table, it's not very clear why we should choose LTP?   - the highest rate and top1 looks to be Kusupati et al?- in the text description, it says that Kusupati uses a stronger baseline, ie STR   - why don't you use STR too?- in the text it says that Renda et al needs more training   - why not put the amount of training required as an additional column in the table?Jumping to table 3 mid-paragraph is I think jarring. I think first talk about table 2 in one paragrpah, then talk about table 3 in the next. Or at least don't mix and match across tables, at least without having first presented each table on its own first. I feel. Like the sentence 'In fact as table 3 shows' cannot I feel precede the clause 'Finally, figure 3 provides ...', which presents what is table 3. Oh, that's figure 3, not table 3. Anyway, I think table 3 needs some introduction, please.Yes, so, it seems to me that the resnet50 results from table 3 could be added to table 2 perhaps?Figure 3 should be in a separate paragraph, since it is not a comparison with baselines/other models. It's just an obseration about LTP itself. And really, without any comparison with how other models/pruning strategies are, I'm not sure it is very meaningful to me? Like, for all I know other models have smaller error bars?I think table 4 presentation should follow immedaitely table 2 presentation.Then table 3 presentation.Figure 3 might be best in a separate 'appendix'-y sub-section at the end of section 4, I feel. Since it's not comparing to other models, like table 2, 4 nad 3 are.Table 4. If torchvision gives worse results than caffenet, then why not use caffenet instead, or re-implement the caffenet version of alexnet in torch? Otherwise, we can see taht LTP results in table 4 dont match the baselines, and we cannot tell if this is because the torchvision baseline is weaker, and LTP is strong, or whether LTP is weaker than eg Ye et al.In fact, Ye et al only drops 0.1% top-5 accuracy compared to original, whereas LTP drops 0.4%, compared to torchnet original, so I feel that justifying the lower top-5 error rate on the weaker baseline model is not entirely sufficient?Table 5 looks like the strongest table to me. Might be worth putting it first? I feel that it could be useful to highlight the top results in each column in each scenario in bold? I think that ideally each column should be a single scenario (whereas here each column is multiple scenarios), then it is easy to highlight the top in each column. For example you could put the different rates as different columns, and use eg top-5 accuracy throughout the table. (or put top 1 and top 5 accuracies as tuples perhaps?)I kind of think that table 3 should be folded into the other tables.I think it's not clear from these tables why we should use eg LTP instead of Kusupati et al, or Ye et al. I think that either make it clear in the table somehow, or perhaps put in the text. Like eg "Our method needs considerably less hyper-parameter tuning than existing SoTA methods, whilst achieving nearly the same accuracies for similar levels of compression."In the conclusion, you mention batch normalization, but this was not brought to the fore in the experiments section. Like, I would expect to see some models that can only be pruned using LTP, and other approaches fail to prune, but I don't remember this being shown clearly in the experiment section?Basically, I think the assertions in the conclusion are exciting, but aren't made clearly obvious in the experiments section. I think for each assertion in the conclusion there should be a single table or graph that shows this assertion very clearly, in comparison to other possible baselines. The paper proposed a new method to prune a neural network. The method is interesting, innovative and effective. It makes it possible to learn tunning parameter via back propagation, hence learn together with network's weights. The work is well motivated. The paper is well structured, the writing is clear and easy to follow.The conducted experiments are thorough and clearly show the efficiency of the proposed method. The paper contains enough information to replicate the experiments.The work would be beneficial for others if the code is published open.A question for clarification: When hard prunning the network (section 4.1), we just replace sigmoid(x) by step(x)? Summary========This work proposed a BO based NAS method using Weisfeiler-Lehman kernel. The idea is novel and natural considering the neural network architectures as acyclic directed graphs. I am a bit surprised to see no one tried it before in the NAS field and it is great to know that using WL kernel leads to competitive NAS performance comparing to other NAS methods and at the same time improves interpretability. Pros====* The proposed idea is novel and natural, given the graph natures of network architecture.* The notes on the interpretability is very interesting and differ the work from other methods.* Extensive empirical studies and ablation studies.* Extensive detail for reproducibility.* The paper is very well written.Minor comments===============I think this is a really nice work and I only have some minor comments:* There is another line of work using BO for NAS: Ru, Binxin, Pedro Esperanca, and Fabio Carlucci. "Neural Architecture Generator Optimization." arXiv preprint arXiv:2004.01395 (2020). Would be nice to know how does the proposed method compared to it.* The appendix C mentioned about using MKL to combine WL and MLP kernels. But in the end the author used 0.7 and 0.3 as the weights for them. I am wondering whether some simple MKL algorithm such as ALIGNF  can improve the performance here. You can find more detail in this paper: Cortes C, Mohri M, Rostamizadeh A. Algorithms for learning kernels based on centered alignment[J]. The Journal of Machine Learning Research, 2012, 13(1): 795-828.Reason for score==============I liked this work a lot, it bridged NAS and BO through the usage of graph kernels (WL kernel). As a result, NAS becomes more sample efficient, which is empirically verified by extensive study in this work. The author did a very good job on the empirical evaluations, they are thorough, solid and contains many ablation studies to understand their methods. I really enjoyed this paper. It's one of my favorite papers from 2020. Authors: thank you for writing it!The paper proposes a framework for jointly fitting the parameters of a physical model (such as the parameters of an ODE or PDE) and learning a neural network to model the error or residual of this physical model. The idea is to find a physical dynamics model $F_p \in \mathcal{F}_p$ (e.g. where $\mathcal{F}_p$ is a set of PDEs with different parameter values) and a neural residual dynamics model $F_a \in \mathcal{F}_a$ (e.g. where $\mathcal{F}_a$ is a hypothesis class of neural networks) which minimize the norm of $F_a$ while constraining the composed dynamics $F = F_a + F_p$ to agree with observed data. Interestingly and importantly, the paper proves the minimum-norm decomposition of the observed dynamics into physical model dynamics and neural residual dynamics is unique, given a condition on the geometry of $F_p$. This condition is that $\mathcal{F}_p$ should be a Cheybshev set: a sufficient condition is that it is a closed convex set in a strict normed space. To me, this condition seems very mild.The paper goes on to show that this method, termed APHYNITY, produces significant gains in predictive accuracy over purely learned methods, purely physics-driven methods, and other forms of combining physical models and learned models. The gain is most significant when the physical model is incomplete. When it is complete there may be some small advantage due to the neural residual accounting for some discretization error; regardless there is no harm, and the learned neural residual is very small. The paper also shows that APHYNITY produces better parameter estimates for the physical model in the presence of incomplete physics. This seems important. Our physics models are always approximations and in many interesting applications (climate/atmosphere, bioengineering, mechanics of materials, etc) commonly used physics models may have an interestingly-sized gap with reality. When the identified parameters, not the predictions, are needed for some downstream task such as decision making, APHYNITY could help with better parameter ID.I thought this paper was clear and well written. The main paper presents an easy-to-follow story, the appendices contain plenty of detail, and when while reading the main paper I wanted more detail on specific points, it was usually easy to follow links to the correct appendices. (Should be true of all papers, but often isn't). ---Thoughts and feedback: - It would be useful to see some simple visual demonstration of the effect of APHYNITY on parameter estimation in incomplete models. (I should note the paper is already quite long and thorough, though).- The integral trajectory-based approach (section 3.2, 3.3, motivated in Appendix D) used to fit the parameters of both NN and physics model seems like the "right" way to do this to me. Nonetheless it would be interesting to see numerical comparison with the alternative (supervision over derivatives).- I wonder if the integral trajectory-based approach, vs the supervision-over-derivatives approach, can be related to traditional methods for parameter ID in ODE?- It seems to me that the authors do a good job explaining and relating to prior work on learning physical systems. However, with both the density of recent literature in this space and the decades of work combining ODE solvers and function approximators, if there are missing references (papers on similar work that the current submission does not cite), it's quite likely I would not have noticed.- It would be interesting to know if this could help figure out *in what way* a physical model is misspecified, and guide design of an approximate physical model that better captures reality. I suspect yes, although it might simply boil down to this method having a better estimate of the residual than if one just does a least-squares fit with the physical model.---Typos:- "bayesian" -> "Bayesian" (end page 2)- "si" -> "is" (page 3)---I think this is a well written paper likely to be of interest to a large number of ICLR attendees, with some important novel contributions, and that the method proposed has a good chance of being widely adopted in the subfield of ML+physical simulation.  __Summary__The authors describe a method to transform 3D protein structures for supervised machine learning. Their method introduces a convolution operation that considers both the intrinsic distances between atoms as defined by their bond structure and the extrinsic distances as defined by 3D proximity. They also introduce interpretable pooling operations developed using known biology of the amino acids. Overall, the method is effective and straightforward to follow due to having avoided unnecessary complexity. The figures greatly aid the reader.The authors method outperforms a variety of competitive alternatives on protein fold and function classification tasks. These are important problems for which the authors model has achieved a significant performance boost. I dont see why this model wouldnt work well for any 3D protein structure labels that can be collected. They also perform a through ablation analysis to establish the contribution of the various components of their method.__Major comments__* I wasnt able to understand what the neighborhood ablations represent and how they differ from convolution ablations. Are the neighbors used for anything other than the convolutions? For example, CovNeigh uses only the intrinsic distances, similarly to InConvC. What makes these different?__Minor comments__* On page 7, a Table 4 is mentioned that doesnt appear to exist. I think they mean Table 3. This paper describes a deep learning architecture for representing and performing classifications on protein structures.  The representation involves three different distances: Euclidean distance and the shorted path between two atoms, where edges are either along covalent bonds or also include hydrogen bonds.  Each atom has a vector of associated features, and convolution is accomplished by defining a kernel on all three distances and then summing the features of each neighboring atom, weighted by the kernel value.  The paper also proposes three protein-specific pooling operations to cope with the large input size when representing all atoms in a protein.Overall, this is an extremely clear paper, and the core ideas appear to be sound.  Furthermore, the experimental validation is quite extensive, and the results are impressively good.  Some positive points are that the authors consider several different tasks, and numerous state-of-the-art methods are included in the comparison.  I particularly appreciated the careful ablation study, demonstrating not just that the entire system works end-to-end but that the various pieces each contribute to its behavior.The experimental setup appears to be valid. There is always the chance that these results could be optimistic due to (presumably unintentional) model selection happening during development of the proposed method, or because of a mismatch between the training data used for the published models and the test set used here.  But I can't see how the authors could have done a better job to guard against such issues, other than the obvious step of making their code and trained models publicly available.  It is unfortunate that the manuscript makes no mention of this. One drawback to this work is its focus on recent literature.  I found it strange that the earliest citation in the related work section is from 2013.  The tasks being solved here have been the focus of extensive research going back 25 years or more.The manuscript is up front about the fact that a drawback of the method is its requirement that the input proteins have known 3D structure.  However, another potential drawback is that the input does not take into account homology information drawn from, e.g., a sequence similarity search over a large protein database.  This information is typically represented as a PSSM column for each observed amino acid.  I would like to have seen this acknowledged, since it seems like a potentially valuable source of additional information.A minor point: the introduction states that the model captures primary, seconary and tertiary structure, and then says that "As chain bindings affect the tertiary structure, the quaternary structure is captured implicitly."  But of course, this argument could apply to any of the other levels: amino acid sequence implicitly captures secondary and tertiary structure.Incidentally, the Murzin cite has an incorrect year (1955). In the manuscript, the authors introduce a parametric version of UMAP, replacing the original embedding optimization step with a deep learning solution detecting a parametric relationship between data and embedding. The novel approach compares favourably with the standard algorithm and, as a major contribution, defines a loss function that can be employed for other important applications such as constraining the latent distribution of autoencoders, and improving classifier accuracy for semi-supervised learning. The paper is well written, complete and thoroughly detailed, both in the theoretical and the experimental section. The introduced material represents a significant advancement in the field, becoming a valuable resource for researchers in several areas. A couple of notes:- An application to one or more large real world dataset (e.g. single-cell sequencing, or weather radar data) would strengthen even more the authors claims and the papers impact, so I would suggest to include it, at least in the Appendix.- Fig.3 in the Appendix is extremely useful to graphically explain the algorithm to a broader audience - I understand the page length limit, but I would strongly recommend to fit it in the main text.- I would also suggest to include (maybe in the Appendix) a kind of how-to fully worked example to help researchers in optimising the use of novel algorithm in a data exploration pipeline- I would point out (within the limitation of the anonimity requirement) the availability of the code for the algorithm This paper introduces a deep learning based digital contact tracing method to minimize the spread of COVID19. The proposed method is based on locally processed information collected on the mobile app. Unlike the most commonly used digital tracing approach that sends quarantine recommendations to all recent contacts of a newly diagnosed person, the developed method in this paper considers all the information related to the users and the ones who have been in contacts with them in order to make user specific recommendations. This is not an easy problem because of different conflicting factors involving in making recommendation decisions, i.e. user privacy, mobility restrictions, and public health. The proposed method, called proactive content tracing, is a set-based architecture (that uses attention) and perform distributed inference to preserve privacy.The authors utilized a simulator called COVIsim (not sure whether it is COVAsim or COVIsim) to train the proposed deep learning model and showed that their proposed method outperforms two baselines on multiple different metrics. The use of simulated data is well-justified given how difficult it is to obtain relevant COVID19 tracing data. This paper advances the current digital contact tracing significantly and is a great contribution to the field.  Summary: The paper studies the expressive power of several classes of recently suggested models: message passing neural networks, Linear GNNs, and Folklore GNNs. Understanding the expressivity of GNNs is a timely and important topic since unlike other neural architectures as fully connected networks, the most widely-used models today, message-passing networks, are not universal. The paper first formally defines the three model classes mentioned above. The paper then reviews previous results on the separation power of these models (can a model tell the difference between two non-isomorphic graphs) and summarizes them, as well as new results they obtain, in prop. 3. Theorem 4 then connects these separation results to the approximation power of the model classes by a novel adaptation of a recently introduced generalization of the Stone-Weierstrass theorem.  The authors conclude the paper with an experimental section showing that folklore GNNs perform significantly better than other models in solving the quadratic assignment problem, which supports the theoretical claimsStrong points:New strong results: the paper closes the following knowledge gaps: (1) Folklore GNNs: k Folklore GNNs are as strong as k+1 WL, (2) Equivariant separation: connecting the separation power of equivariant (rather than invariant) models to their equivariant WL counterpart (3) Link separation results to approximation results: the authors suggest a generalization of the stone Weierstrass theorem and use it to show new approximation results.    The paper succinctly summarizes the current results on the separation power of various GNN classes.The paper Introduces new tools from approximation theory. These tools seem important and will be useful for future papers targeting the expressive power of invariant and equivariant deep modelsInteresting experimental results. Showing for the first time that more expressive models  (2-FWL) substantially outperform previous (2-WL) models on an important task.Weak points:None. I have several comments on the exposition. See below. In addition, I would like to ask the authors to explicitly and succinctly state their generalization of the Stone-Weierstrass theorem for general symmetries and deep models so it would be easy for future work to use it. Please do so for both invariant and equivariant case, and consider stating it in the paper itself.Recommendation: This is a high-quality paper with several strong contributions as listed above. Most importantly, the paper proves several important results on the expressive power of GNNs and introduces useful mathematical tools that I am sure will be used by the community. Hence, I strongly encourage the acceptance of this paper (and would also recommend it as a spotlight/oral presentation).Minor comments:* Can the authors comment on Building powerful and equivariant graph neural networks with structural message-passing (Vignac et al. 2020). Specifically, Theorem 2 shows that their models can separate any pair of non-isomorphic graphs while using only second-order tensors* So in terms of separating power, when restricted to tensors of order k, k-FGNN is the most powerful architecture.  => Among the architectures considered in this paper.* For regular-grid graphs, they match classical convolutional networks which by design can only approximate translation-invariant functions and hence have limited expressive power. In this paper, we focus instead on more expressive architectures. => I believe that mentioning the relation between GNNs and 2-WL here is a better example of the limited expressive power of GNNS.* Note that 2-WL solves k-cliques for k <= 6 (Furer (2017)) so that these networks are probably not comparable to 2-WL. => As far as I know 2WL cannot detect triangles. Please explain.* k-FWL: maybe write 2 FGNN layers explicitly and discuss matrix multiplication connection?* Appendix D is written really nicely with good examples* Algebras are closed under multiplication but sets of neural networks are not (although products can be easily approximated in most cases). I believe this gap is filled in Appendix D but would like to ask the authors to explain it in the main text.* The paper has several typos. The authors frame the decomposition of the Laplacian equation as an unsupervised regression problem that is using a 5-level (and fully connected?) neural network as regression function.  A cost function to be used in the optimization is proposed that is expanded to eigendecomposition problems of increasing complexity. A comparison with a classical method indicates that the proposed approach is comparable to or better than the former for the given task.I like the overall approach and the idea of framing the solution of the eigenvalue decomposition as an ANN regression problem, and I would see that this is of interest to ICLR. From an application perspective, the authors refer to computer vision tasks, I would be interested in a somewhat deeper discussion of constraints of the approach, for example:As both methods are somewhat on par in terms of various performance indicators I was wondering whether the proposed approach is faster?In real world problem critical information is contained in the boundary conditions. Could the authors discuss pros and cons of their approach with respect to them? And, again, compare to existing methods? The authors introduce a novel method for inferring a simple algebraic expression for an output in terms of an input. The method outputs a sequence tokens of the algebraic expression as represented by the pre-order traversal of an expression tree. For each token of the sequence, a recurrent neural network outputs a probability distribution on possible tokens, from which the token is sampled. Some notable contributions:1. The authors introduce a novel reinforcement learning objective that optimizes the quality of the best examples instead of optimizing the average quality.2. At each point in the sequence, their recurrent neural network takes as input the sibling and parent nodes of the expression tree, instead of the previous token in the sequence.3. Simple constraints can be introduced in their approach by zeroing out the sampling probability for tokens that don't meet the constraints. (For example, they disallow redundant expressions like log(exp(x))).The authors do a comprehensive comparison with other methods (in which their work performs very favorably) as well as an ablation analysis to confirm that each of their innovations is helpful (which I was very pleased to see).Well done. The authors propose a novel regularization approach aimed at addressing issues of class imbalance and heteroskedasticity. This adaptive approach uses a Lipschitz regularizer with varying strength in different parts of the input space, regularizing harder in cases of rare and noisy examples. The authors derive the optional regularization strength in the one-dimensional setting, to set ground for the proposed approach and its application in higher-dimensional settings. The approach is evaluated on multiple image datasets, and a textual dataset - and compared to a number of baselines, including those involving noise-cleaning, reweigthing-based methods, meta learning, robust loss functions, as well as tuned uniform regularization. The improvements seem quite strong, and clearly demonstrate the utility of the proposed approach. The paper is well structured, clearly written - and was a pleasure to read.I only have brief comments:- Unless I am mistaken (I could have missed it), I didnt see a concrete mention of a statistical test that was used to determine statistical significance. The authors do use the word significant to qualify the observed differences, but that should only be phrased as such if appropriate statistical testing has been done and has been outlined when describing the evaluation. That being said, the improvements are quite stark and I dont doubt the validity of the claims - I am merely suggesting that the authors should include this information for clarity.- While the evaluation clearly shows the benefits of the proposed approach, and there is a detailed ablation study, I think that it would be interesting to additionally discuss and analyze in more detail the resulting regularization coming from the proposed approach, on WebVision (or one of the other datasets, wherever its easiest) - to show how the distribution of how many examples are being regularized by which factor - and contrast that with the optimal uniform regularization. There could also be a correspondence with Figure 1, if constraining the comparison to freq,rare ; noisy, clean input types. Visualizing some such distributions would help understand the method better - and add to the presented analysis. The authors propose to learn what they term recurrent independent mechanisms (RIMs), a new recurrent architecture with components with nearly independent transition dynamics.  RIMs exhibit excellent generalization on tasks in which the factors of variation differ systematically between the training and evaluation distributions.The paper is very well organized and well written, with relatively simple and consistently clear explanations of key concepts. The authors are precise in this language use, noting for example that they use the term "mechanism" in two slightly different ways (footnote 1).  The authors state that their central question is" how a gradient-based deep learning approach can discover a representation of thigh-level variable which favour forming independent but sparsely interacting recurrent mechanisms in order to benefit from the modularity and independent mechanisms assumption."  The paper could be improved by more clearly explaining the unique benefits of a gradient-based approach to this task.The experiments in section four are particularly thoughtful and well-designed.  Rather than merely comparing performance on some  set of benchmarks, the authors aim to elucidate key capabilities with specific experiments.  The authors provide a novel combination of known architectures to an important use case of reducing the density of required  measurements in sensor-fusion based temporal multi-class inference tasks. This has implications in energy consumptions of wearable sensors.  but could even generalise to measurement timings in clinical care to make the work of nurses more efficient, and reduce the stress caused by some medical procedures..The authors represent a way to train consistent policy that predicts the best combination of sensors to estimate the state of the subjects.  They have found that a smaller set of features.  is more explainable than the full set of features.  However, I think that this somewhat of an overpromise.  The trained model gives the optimal density of the measurements and can discern also if old values and features measured are till OK for the inference.  This does not mean that those measurements are not needed at all in the features.  One can only argue that the required features can be estimated from the older measurement. So, the current set of active sensors is not the full set of required measurement values and can not be exclusively used to explain the logic of the system. Even more, the logic of the policy deciding the new  measurement is not discussed in an explainability context. The authors provide no data on this. It may be just an  estimate the derivative of the signal and ignore a new measurement, if it's time  derivative is small enough. As a summary , I support publication of the manuscript, provided  the authors modify the message on the interpretable features. The paper presents a method for generating synthetic datasets from the large realworld datasets. The CNN trained on such synthetic dataset supposed to have similar accuracy on the realworld data, as trained on the original one. The benefit of a such procedure is reduced model training time and storage space (for data).The method is built on the idea that the gradients of the network being trained on the real images should be similar to gradients, which were obtained by the training on the synthetic images.   The method is validated on MNIST, SVHN, FashionMNIST and CIFAR-10 on several different architectures: MLP, AlexNet, VGG-like and ResNet architectures. Moreover, the paper compares the proposed method vs  many other baselines, e.g. methods, which select "representative" image from the dataset (coreset methods), as well as Dataset Distillation and cGAN. *****Overall I like the paper a lot. The method is well-motivated, shows good (sota) results and also often (for MNIST, SVHN, FashionMNIST)  produces human-recognizable examples, although there is no term/regularization directly encouraging this. The paper also studies how architectural choices like normalization, pooling, etc. influence the generated samples and how samples generated for the architecture A are suitable for training architecture B.I don't see any major weakness in the paper.Questions and comments.- In Figure 5, IMO it is bad practice to fit linear regression into blob-like data. (minor)- Have you tried to add some term, encouranding the diversity for the different synthetic samples belonging to the same class?  The paper proposes a new neural net architecture based partially on the previously proposed GeneralizedPageRank (GPR). The model adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction. The main advantage of  this approach is that -- unlike previously proposed GCNs -- this approach works well for both homophilic and heterophilic graphs (due to the use of GPR). This allows for e.g. node classification without a priori knowledge about the type of graph at hand. Additionally, this approach avoids feature over-smoothing, a known problem in GCNs. ############I recommend this paper for acceptance due to its novelty and its algorithmic contribution. I really enjoyed reading it. ############Pros+ A very interesting approach with useful practical implications for a node classification of both both homophilic and heterophilic graphs. + Very clearly written and well articulated. + Great literature review of related topics in the area of GCNs. ############Cons/Suggestions - I found the explanation of GPR in the beginning of Sec 3 a bit confusing. Please rewrite to add a bit more detail (e.g. what is gamma?), I had to consult the original  paper to understand the notation here.   ##########################################################################Summary: This paper conducted a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents##########################################################################Reasons for score:  Overall, I'd vote for acceptance to the paper. The paper is informative and practical; however, I'm not sure that the paper meets ICLR's requirement.Pros:1. Reproducibility is one of the main issues for various RL algorithms. This paper conducts a large-scale empirical study for popular on-policy algorithms.2. The paper is well-written, and the suggestion is useful to me.Sorry, but I didn't go through all the details in the appendix. The authors survey a wide variety of implementation-level and hyperparameter decisions in reinforcement learning for continuous control tasks. They train over 250.000 agents with different settings and suggest empirical guidelines.As the authors indicate, there's not too much related work so one could call this work pioneering: The sort of work conducted by the authors is crucially important for a field afloat with tricks and tweaks, many of which are typically not discussed in the scientific literature due to a misplaced conceit around this being "not research, just engineering" entirely absent from established fields of science such as experimental Physics. It is also typically not done as it is just plain hard to do. Combined with the often lackluster response from the community, the cost-benefits trade-off has just not been worth it, especially for junior researchers. It's all the more commendable that the authors have engaged with this formidable task of bringing some of the "secret sauce" out of the heads of senior engineers in the various labs and into published and peer-reviewed science.The authors compare various choices of configurations obtained from the Cartesian product of 8 factors which they call thematic groups: Policy Losses (Sec. 3.1), Networks architecture (Sec. 3.2), Normalization and clipping (Sec. 3.3), Advantage Estimation (Sec. 3.4), Training setup (Sec. 3.5), Timesteps handling (Sec. 3.6), Optimizers (Sec. 3.7), and Regularization (Sec. 3.8). The high-variance nature of training RL agents makes it such that the individual factors in these configurations often have surprising non-linear cross-relations such that the problem space cannot be evaluated incrementally (i.e., it's often not possible to establish "the best" architecture first and select the right learning rate afterwards). The authors propose a novel approach of considering for each choice the distribution of values among the top 5% configurations trained in that experiment. Their experimental design is such that the values for each choice are distributed uniformly at random and thus if certain values are over-represented in the top models this indicates that the specific choice is important in guaranteeing good performance.As for improvements on the paper, I have one major and only a few minor comments. My major comment is that the paper does not indicate anywhere that the research code is released, only that it's based on SEED RL. I believe an authoritative public implementation of the configurations considered would be extremely worthwhile, both for the community and the authors. If they haven't already done so (there's no supplements to this submission and I refrained from doing any web searches to preserve anonymity), I'd urge the authors to invest the time to release a (possibly cleaned-up) version of their code.As for minor comments, I'm not clear about the philosophical distinction of something being "due to the algorithms or due to their implementations" (in the Introduction). I very much see the point the authors are making, which is an important one -- what makes RL results work is often "nitty-gritty" details not mentioned in the main part of the relevant publications (and often just barely mentioned in appendices). However, in the strictest sense, the algorithm very much is the implementation -- that's what produces a given result. It's worthwhile to keep the distinction between an idea (say, PPO) and a given implementation of that idea (e.g., presumably the authors had to re-implement PPO in TF2 when using SEED RL and couldn't use OpenAI's original implementation). It's also fine to call the idea "the algorithm", but I'd have preferred to see this distinction more clearly defined.Somewhat related: The authors are very much correct about what they call "standard modus operandi of algorithms [...] such as PPO", namely iterating between generating experience using the current policy and using the experience to improve the policy. I'd add that strictly speaking no _iteration_ is necessary, as for instance IMPALA, coming from the A3C line of development, does both asynchronously in parallel, and I suspect so do the authors given their use of SEED RL. My suggestion would be to slightly rephrase this sentence and mention IMPALA along with PPO. Perhaps there could also be a comment somewhere about what constitutes "PPO" (or "IMPALA") -- e.g., IMPALA consists of (1) an asynchronous actor/learner split [with further choices of when/how the weights are copied from learner to actor, see e.g. [this comment](https://github.com/deepmind/scalable_agent/blob/master/experiment.py#L508)], (2) a specific type of policy-gradient loss, v-trace, (3) a specific neural network architecture, optionally including recurrence via an LSTM and potentially even (4) a specific type of preprocessing for environments such as Atari or DMLab [according to some papers, swapping out the implementation of the frame-downscaling algorithm in Atari has measurable impact on final performance -- this will matter a lot when evaluating re-implementations of an "algorithm"]. I'd like for the authors to take on this opportunity and propose a common language to discuss these distinctions, which in practise are often confusing to junior researchers (and some senior researchers, too).Further related to IMPALA and v-trace, I was surprised about the word "unsurprisingly" and the explanation in "Perhaps unsurprisingly, PG and V-trace perform worse on all tasks. This is likely caused by their inability to handle data that become off-policy in one iteration, either due to multiple passes over experience [...] or a large experience buffer in relation to the batch size." While the results speak for themselves, my understanding of v-trace was that it was specifically designed for the very goal of dealing with the "slight" off-policiness produced by asynchronous actor/learner splits in a PG setting. Perhaps the authors have an intuition I'm lacking at this point, but if so I'd appreciate further elaboration.As a final and perhaps trivial comment, I was slightly irritated by the notation/typography for the inverse cumulative density function of a binomial distribution. In $\rm\LaTeX$, the symbols $icdf$ read as the in-context nonsensical $i\cdot c\cdot d\cdot f$ while the authors would presumably want to use $\mathrm{icdf}$ (compare $exp(x)$ vs $\exp(x)$ or $sin(x)$ vs $\sin(x)$). I'd propose `\mathrm{icdf}`  as the correct syntax for this in $\rm\LaTeX$.In follow-up work, I'd like to see a similar paper for various "discrete RL" tasks (a subset of Atari, VizDoom, DMLab, MiniGrid, BabyAI, ProcGen, and perhaps even Obstacle Tower, Minecraft, StarCraft (I or II) or the recent NetHack environment) with similar factors of configurations. I assume this is a task yet more daunting, but no less useful to the overall community of researchers.Overall, this is a strong paper and I recommend it for publication. The article proposes a model for estimating physiological signals from videos. The novelty of the proposed approach is to use the low attention regions to estimate a model of the signal noise. Using this estimate can improve the performance of the denoising component. The model is well described and sensible and the experimental section demonstrates state-of-the-art results on what is a very relevant task.Overall, the article describes an interesting approach and contains valuable insights into the limitations of attention-based approaches in noisy signals. QualityThe paper is well written and mathematically precise. Statements are supported through helpful illustrations and the appendix provides the reader with sufficient information about the experimental setup and evaluations.ClarityThe approach is clearly motivated and is described in full mathematical detail. The authors also show a highly efficient way to integrate the actually computational very expensive loss into neural networks. The derivation is moderately easy to follow for readers with a background in linear algebra.OriginalityThe proposed is, while inspired by previous orthogonalization approaches, a novel idea and its relation to previous work is discussed appropriately.SignificanceUnsupervised pretraining in itself has a large significance for deep learning, even though it lost in popularity due to other approaches that achieved a similar result without the extra preparation phase of the neural network training. It is important and useful to keep the research in this area alive and the authors contributed very valuable knowledge with this paper.Pros:-- very promising approach for pretraining of neural networks-- well written paper with good illustrations-- large experimental evaluationCons-- derivation only for networks without activation function-- evaluation very much focused on image tasks-- Figure 5: legend hardly readable (kind of true for many figures)-- source code not referenced (might be due to remain anonymous during review) **Quality:**Overall the quality of this work is high.  The quantitative and qualitative results are impressive relative to the SoA. I would like to see the qualitative results for the Best model as opposed to just the Pretty model, and I'm curious why the best qualitative mode was not the same as the best quantitative model.  I would think analyzing this difference could give the authors insight into how to improve the model. **Clarity:**Overall the paper is written clearly, explaining and justifying the different components of the model clearly.  There are a few issues/questions I have:* Page 2: change "non-reflective" -> "reflective"* For depth estimation, I'm wondering why you changed the MVSNet loss function to use BerHu instead of L1 used in the original paper?* Could you define the terms in the BerHu criterion?  What are x and c? It would also be good to shed some intuition on why this criterion is the right one.* The mixing constants in your loss function ($\lambda$) vary across several orders of magnitude.  How were those selected?* On page 6 you state that two values of $\tau$ are used, but elsewhere in the paper $\tau$ is defined as $10^{-4}$ and you use $\tau$ and $2\tau$. **Originality:**The paper generally uses a mix of SoA techniques creatively woven together in a fairly sophisticated model. Oher novel aspects such as using the neural renderer to create the contrastive depth module was interesting. **Significance:**This work is significant based on the importance of the problem - this is one of the harder and most important problems in computer vision today,  in the quality of its results and in the creative way it combines SoA methods to provide multiple semi-supervised losses.  Summary-------------The paper extends soft actor-critic (SAC) to the batch RL setting, replacing the policy entropy in the objective function with the KL divergence from the behavioral policy. The temperature parameter tau weighting the reward agains the KL term is annealed towards zero during the optimization process, which corresponds to starting with behavioral cloning for high values of tau and ending up with the standard reward maximization RL objective for tau=0. Theoretical analysis and experiments confirm the advantages of the proposed method.Decision-----------I vote for accepting the paper. The idea of annealing the KL constraint is simple and elegant. Although it is very similar to other constrained policy update methods discussed in the Related Work section, the evaluation in the batch RL setting and demonstration of the improved convergence properties is novel. The execution is of high quality, with evaluations on tabular problems, MuJoCo, Atari, and a contextual bandit problem for movie recommendation.Questions--------------1. As pointed out in Sec. 3.3, when the policy deviates too much from the behavioral policy, the value estimate becomes erroneous. Therefore, a criterion based on the ensemble variance of the Q-function estimates is proposed. Is there a way to derive such a criterion from first principles?2. Can you relate your work to [1]?3. Does your method work when the behavior policy is not known but only the dataset is available?4. Can you quantify how far the optimal policy is allowed to be from the behavioral policy? For example, on a pendulum swing-up task, if the behavioral policy is taking random actions and the pendulum always jitters at the bottom, inferring the optimal policy from this data appears quite challenging. Can one give some criteria when the method is expected to work well?References---------------[1] Nachum, O., & Dai, B. (2020). Reinforcement learning via fenchel-rockafellar duality. arXiv preprint arXiv:2001.01866. This paper presents the traveling observer model (TOM), a general framework to learn multiple heterogenous supervized (input,output) tasks, which are indexed by a continuous "variable embedding" that is automatically learned by the system. The authors show on simple problems that the learned task embeddings can recover an intuitive organization of the problems' variables in space or time. They also show that the model simultaneously trained on 121 seemingly unrelated classification tasks can outperform state-of-the art supervized methods fine-tuned on single tasks.The proposed model is novel, technically sound, of broad interest and very promising. The paper is clearly written and easy to follow.  The presented experiments convincingly demonstrate the sensibility and usefulness of the approach. The topic perfectly fits the scope of ICLR.Minor suggestions for improvement:- Section 2 (first paragraph) the notations are a bit confusing here. First, the sample indices s=1...S_t are denoted as superscripts while task indices t=1...T are denoted as subscript, but then the sample indices are dropped and never used again, while task indices become superscripts and variable dimensions are denoted as subscript. The definitions of sets V_t^In and V_t^out is also strange. I think they should denote the union of all the spaces that variables are living in, but instead they are defined as finite sets of specific variables. The definition of "the universe" V in section 3 is also a bit sketchy. Is that a set of sets? a category?- I think that when using a pre-defined "oracle" variable embedding, the proposed model becomes very similar or even equivalent to conditional neural processes (Gamello et al. 2018). It would be interesting to comment on that.- There is an unfortunate double use of the letter h for two different things in equation (3) and (4)- Sec. 4.4 "after joint training the model is finetuned on each task with at least 5K samples" -> is the whole model fine-tuned or only the function g? or g and h? Please clarify. Paper is very well written and addresses an important topic; using Traveling Observer Model (TOM) in multi-task learning for tasks that do not have no spatial organization unlike, for example, images. Although the paper is said to be a first implementation of TOM, it does thorough experimenting and result analysis of its preformance from various aspects and by comparing it to many sophisticated models. Future research for improving and testing the algorithm is clearly detailed. Related scientific literature is sufficiently addressed,  mathematical background and the method are clearly presented, extensive and relevant experiments are done and result analyzed. I didn't even find any typos.  The paper develops techniques, systems and models to leverage conditional computation, greatly scaling model size while only requiring sublinear computation with respect to model size.   As a result, a sparsely-gated MoE model with 600B parameters for NMT tasks has been trained efficiently (4 days on 2048TPUs) with new state-of-art accuracy.Merits of the paper:- Provide an effective strategy to address the expensive computational cost of training giant models- Offer a comprehensive solution covering techniques, systems and models - Well-designed APIs and implementations to express a wide range of parallel computation patterns with  minimal changes to the existing mode code-  New state-of-art models have been trained using the strategy and systems, as a great demonstration on the effectiveness and efficiency of the approach.Places to improve-  It is helpful to further clarify the difference of this work comparing with the prior work Shazeer et al. (2017) beside transformer vs RNN.  For example, are the features like load balancing, efficiency at scale, auxiliary loss, random routing new contributions of this work, or similar as the prior work, or with certain incremental improvements?  It would also be helpful for readers to understand, which techniques are generic and which are model-type specific.- Both this work and the prior work Shazeer et al. (2017) apply conditional computation on neural machine translation tasks.  It would be helpful to comment on the generality and effectiveness of the solution on other types of tasks. Paper summary - This paper extends the differentiable plasticity framework of Miconi et al. (2018) by dynamically modulating the plasticity learning rate. This is accomplished via an output unit of the network which defines the plasticity learning rate for the next timestep. A variation on this dynamic learning rate related to eligibility traces is also proposed.Both dynamic modulation variations strikingly outperform non-plastic and plastic non-modulated recurrent networks on a cue-reward association task with high-dimensional cues. The methods marginally outperform plastic non-modulated recurrent networks on a 9x9 water maze task. Finally, the authors show that adding dynamic plasticity to a small LSTM without dropout improves performance on Penn Treebank.The paper motivates dynamic plasticity by analogy to the hypothesized role of dopamine in reward-driven learning in humans and animals.Clarity -  The paper is very clear and well written. The introduction provides useful insights, motivates the work convincingly, and provides interesting connections to past work.Originality - I don't know of any other work that models the role of dopamine in quite this way, or that applies dynamic plasticity modulation in settings like these.Quality - The experiments are well chosen and seem technically sound.Significance - The results show that meta-learning by gradient descent to modulate the plasticity learning rate is a promising direction -- a significant contribution in my view.Other Comments - The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017. I gather that Experiment 3 presents small LSTMs without recurrent dropout instead because combining plasticity and dropout proved challenging (or at least the authors haven't tried it yet). I think the paper is solid as-is; positive results in this comparison would take it to the next level.Questions:Why were zero-sequences necessary in Experiment 1? This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail. Perhaps the authors could clarify on what a confounding "time-locked scheduling strategy" would look like in this task?Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?Why is non-plastic rnn left out of Figure 2b?Typos"However, in Nature," -- no capsin appendix: "(see Figure A.4)" -- the figure is labeled "Figure 3" In general, it is not clear, at least theoretically, how, and when unsupervised data helps to generalization of nonlinear methods. In the literature there are important and elegant works exists that analyses the impact of usage of unlabelled data during training, however, (if I am not mistaken) all these analyses have been done for linear models. Authors analyse and shed some light to several aspects of using unlabelled data during training. They formalize their analyses based on expansion assumption. I think it can be restated as the similarity between members of the same classes is bounded by below. Intuitively such an assumption is quite reasonable. The authors use the term input consistency for defining a broad set of methods e.g. transformations of the image should be similar to each other, and they also couple their analysis using the expansion assumption with input consistency. In their view input consistency brings a local stability/generalization and expansion property brings global stability/generalization. This is again quite reasonable way of thinking because intuitively just forcing an input point to be close to transformed version of itself sounds a weak property for a good generalization performance. The authors supply quite a bit of theoretical novel material to support their intuition and analysis. Furthermore, they present some supportive experiments albeit not an extensive one. Strong and weak points:a)Strong points: Please see above.b)The paper is quite dense, and the reader needs to be familiar with learning theory concepts. I wonder if authors would have focused on only one aspect of the problem which they are dealing. In the current version semi-supervised learning methods, unsupervised domain adaptation and unsupervised learning are covered. Every of them is a field by itself. I understand the desire of a unified and generic framework however I can imagine that there is a risk of diluting the message.c)Another understandably weak point is the experiments. I personally think that conducting an experimental study in the scope of this quite challenging however it will be nice see expansion property on a real dataset.Recommendation:Overall, I would like this paper to get published because (if I am not mistaken) paper develops an initial understanding extremely important field e.g. self-training/self-supervised learning. Supporting arguments:a)I found the assumptions paper quite intuitive and necessary. Authors also supply population level guarantees for unsupervised learning. Moreover, they extend their work finite-sample guarantees by using margin concept and Lipschitz continuity. They extend their work to domain adaptation and semi-supervised learning. The novel material in the paper is extensive.Questions:a)As I mentioned before I would like to expansion property on some real-world datasets. For example, can authors present some evidence of expansion property for a chosen deep neural network on a dataset (or multiple datasets) and quantify the expansion property based on some metric.  Improvement Suggestions:a)The paper is quite dense, and the reader needs to be familiar with learning theory concepts. I would recommend authors to decrease density of the paper and may be move some parts to the supplementary material.Although I am quite positive about paper, I would like to see the discussion and comments. I am open to change my review to any direction if some new evidence/discussion/published work supplied. **Summary:**This paper provides a theoretical analysis of self-training for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. The authors propose a novel assumption that they dub _expansion_ to effect this analysis. The expansion assumption requires that the neighborhood of small sets have a class conditional distribution that is large. Under this assumption, the authors show population results for an algorithm that performs self-training under the objective that enforces input consistency. They also provide finite sample guarantees based on off-the-shelf generalization bounds for unsupervised learning. **+ves**+ The expansion property seems neat, and seems like a natural quantity for making progress in understanding self-training theoretically + The authors also provide support for this empirically in Section D. + The paper is very well written. In particular, the summary of the theoretical results in the main paper is very well done. I also appreciated the proof intuition for Theorem 4.3's proof**Concerns/Comments:**- I think it would be helpful for the reader to have a brief proof sketch of the theorems immediately after the statement. The results are already summarized (well!) informally in the introduction. - It is unclear to me how realistic Assumption 4.1 is. **Questions for the Authors:**- Can you expand on the optimization objective of (4.1)? How may one implement this in practice? Or, how is this analogous to what is being done in practice? - Can you comment on the realism of Assumption 4.1? Is it possible to verify this on toy examples as you have done with the assumptions in Section 3? - Can you comment on obtaining a finite sample version of Theorem 4.3?  The paper proposed a novel differentiable neural GLCM network which captures the high reference textural information and discard the lower-frequency semantic information so as to solve the domain generalisation challenge. The author also proposed an approach HEX to discard the superficial representations. Two synthetic datasets are created for demonstrating the methods advantages on scenarios where the domain-specific information is correlated with the semantic information. The proposal is well structured and written. The quality of the paper is excellent in terms of novelty and originality. The proposed methods are evaluated thoroughly through experiments with different types of dataset and has shown to achieve good performance. This paper studies the dynamics-based curiosity intrinsic reward where the agent is rewarded highly in states where the forward dynamic prediction errors are high in an embedding space (either due to complexity of the state or unfamiliarity).Overall I like the paper, it's systematic and follows a series of practical considerations and step-by-step experimentations.One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search. While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods. In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.Another area of improvement is the experiments around VAE. While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance. Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).An interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best. So it would be interesting to explore how much training is needed for the embedding model. RFs are never trained and IDFs are continuously trained. So maybe somewhere in between could be the sweet spot with training for a short while and then fixing the features. Given the growing interest in building trust worthy and privacy protecting AI systems, this paper demonstrates a novel approach to achieve these important goals by allowing a trusted, but slow, computation engine to leverage a fast but untrusted computation engine. For the sake of protecting privacy, this is done by establishing an additive secret share such that evaluation on one part of the share is performed offline and the computation on the other part of the share is performed on the untrusted engine. To verify the correctness of the computation on the untrusted server, a randomized algorithm is used to sample the correctness of the results. Using these techniques, the authors demonstrate an order of magnitude speedup compared to running only on the trusted engine and 3-4 orders of magnitude speedup compared to software-based solutions.Overall this is a strong paper which presents good ideas that have influence in ML and beyond. I appreciate the fact that the authors are planning to make their code publicly available which makes it more reproducible. Below are a few comments/questions/suggestions 1.This papers, and other papers too, propose mechanisms to protect the privacy of the data while outsourcing the computation on a prediction task. However, an alternative approach would be to bring the computation to the data, which means performing the prediction on the client side. In what sense is it better to outsource the computation? Note that outsourcing the computation requires both complexity on the server side and additional computation on the client side (encryption &amp; decryption).2.You present the limitations of the trust model of SGX only in the appendix while in the paper you compare to other techniques such as Gazzelle which have a different trust model and assumption. It makes sense to, at least, hint the reader on these differences. 3.In section 2.2: has to be processed with high throughput when available is it high throughput that is required or low latency?4.In Section 4.3: in one of the VGG experiment you computed only the convolution layers which, as you say, are commonly used to generate features. In this case, however, doesnt it make more sense that the feature generation will take place on the client side while only the upper layers (dense layers) will be outsourced?5.In section 4.3 Private Inference : do you include in the time reported also the offline preprocessing time? As far as I understand this should take the same amount of time as computing on the TEE. The paper is well written and flow well. The only thing I would like to see added is an elaboration of "run a semantic parsing module to translate a question into an executable program". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words? This paper describes an approach for unsupervised learning of node features on a graph (with known structure), so that learned local representations represent community information that has high mutual info with a graph-level summary. The general idea is they apply InfoMax to graphs via graph convolutional networks (GCN), and report impressive results, including rivaling supervised learning methods for node classification. The 3 experiments are on paper topic classification, social network modeling, and protein classification.The idea of using InfoMax with GCNs for unsupervised node learning is clever and timely, the technical contribution is solid, the experiments are executed well, and the paper is clear and easy to read. Paper summary: This paper presents a deep-learning based method for super-resolving low-resolution labels into high-resolution labels given the joint distribution between those low- and high- resolution labels. This is useful for many semantic segmentation tasks where high-resolution ground truth data is hard and expensive to collect. Its main contribution is a novel loss function that allows to minimize the distance between the distribution determined by a set of model outputs and the corresponding distribution given by low-resolution label over the same set of outputs. The paper also thoroughly evaluates the proposed method for two main tasks, the first being a land cover mapping task and the second being a medical imaging problem.For the land cover application, adding low-resolution data to high-resolution data worsens the results when evaluating on the geographic area from which the high-resolution data was taken. However, when testing the model on new geographic areas and only adding the low-resolution data from this new area in training makes significant improvements.Generally the paper is very well written, well structured, all explanations are clear, examples and figures are presented when needed and convey helpful information for the reader. The overall idea is fun, original, useful (especially in remote sensing) and is presented in a a convincing way. All major claims are supported by experimental evaluation. There are nevertheless a few concerns:Major Concerns:On a conceptual level, the main concern is that the paper assumes we are given a joint distribution of low and high resolution labels, where we are given the joint distribution P(Y,Z), which seems the main limitation of this method. In fact, to correctly estimat this joint distribution either requires additional knowledge about low-resolution data such as the example presented on the NCLD data : For instance, the Developed, Medium Intensity class [...] of the coarse classes, or it requires actual high-resolution labelled data to correctly estimate this joint distribution. I think the paper would greatly benefit from including a section that discusses the impact of this limitation.Another point is footnote 3 on page 5. This argument is valid but it would be more convincing to give a thorough explanation on why the choice of the presented loss function is better compared to the KL divergence based loss function or at least some evidence that the two perform similarly when evaluating the method.Minor Concerns: -such as CRFs or iterative evaluation I would include a citation on this type of work.-Format of some references in the text need to be corrected, e.g. into different land cover classes Demir et al. (2018); Kuo et al. (2018); Davydow et al. (2018); Tian et al. (2018). - SummaryThis paper presents a graph-network-based architecture for learning to perform mesh-based simulations, which can be run more efficiently than the full, "ground-truth" simulations. The experiments demonstrate that the proposed method is able to learn to simulate a wide range of different physical scenarios. Moreover, the presented results also demonstrate an ability to generalize to configurations different from the one seen in training. - ProsThe paper is clearly written and strikes a good balance of presenting the relevant information in its main body and including the necessary details in the supplementary material. The additional videos provided are helpful in demonstrating the model's capabilities. The experiments performed are comprehensive, with comparisons to a wide variety of significant baselines (e.g., other graph neural networks, grid based methods, etc.) and evaluations of the impact of diverse hyperparameters.Most importantly, the results presented are strong, containing both impressive qualitative demonstrations and good quantitative performance when compared to baselines.  The ability to operate directly on meshes, in contrast to previous work that operated on structured grids, makes this method more relevant to real-world applications.- ConsThe proposed method is, conceptually, a straightforward evolution of previously proposed graph-network-based methods for physics simulations, with most notable similarities to Sanchez-Gonzalez et al. (2020) ([33] in-paper reference).The ability to generalize to higher-dimensional meshes is only briefly (quantitatively) evaluated (more on this in the comments below).- Reasons for scoreOverall, given the "pros" described above, notably the strength of the results presented on a wide variety of tasks, I recomend this paper for acceptance. As mentioned above, the method proposed is a modification of a graph-network based method previous employed to simulate particle dynamics. In order to work on meshes, some modifications are proposed. such as adding mesh- and world-space connections to the graph, learning to predict the S matrix to remesh, etc. As such, this works stand on the strength of the practical results it achieves. The results demonstrate an impressive ability of the proposed method to learn to capture both Eulerian and Lagrangian mesh simulations. The wide diversity of tasks for which the model works well is unprecedented. The experimentation is extensive, with comparisons to significant baselines and evaluations of the relevance of different hyperparameter choices.Moreover, methods that learn to simulate physical processes more efficiently, and applications of graph neural networks are two research directions that have garnered a lot of interest in recent work. As an intersection of these two areas this paper should be of interest to a wide audience in the conference. - Additional commentsOne important drawback of these types of physical simulation methods, which require training on ground truth data, is that training itself requires a large amount of data from the very process we want to simulate. If the learned model only presents a limited ability to generalize, this can severely limit the applicability of the method. This does not seem to be the case here, as the experiments seem to demonstrate an good ability to generalize to unseed scenarios, provided in part by the ability of the graph neural network to learn local/scale-invariant interactions.In this direction, an important capability of the model, suggested in the paper in the "tassel sub-experiment" of the FlagDynamic experiment, is the ability to generalize to much larger meshes than the ones seen in training. As mentioned in the "cons" section above, however, this ability is only briefly (qualitatively) evaluated. It would be interesting to see a more thorough evaluation of this ability, due to its potential large importance to practical applicability, by allowing the model to be trained on smaller examples and then scaled up at inference time.Moreover, a thorough analysis of the error trade-offs when moving from smaller to larger meshes would also be interesting.In the section on "Key hyperparameters", within the experimental results, it is claimed that "the model performs best given the shortest possible history ..., with any extra history leading to overfitting. This differs from GNS [33], which used $h \in 2 \dots 5$ for best performance." Do the authors have any hypothesis as to why this might be the case here, versus the observation in [33]? Summary:The authors study proposed importance metrics for pruning neurons/channels in deep neural networks and analyze what properties of parameters are favored by each approach by studying the relationship between model parameters, gradients, 2nd order derivatives and loss. Through this analysis they develop a rich understanding of the consequences of different pruning criteria and use their understanding to propose modifications to existing techniques that produce higher quality models across different settings.Pros:The framework used by the authors is clear and easy to understand but also very general. The authors mix of empirical results and theoretical analysis makes a convincing case for the accuracy of their observations. The authors go beyond observation and analysis and use their insights to design new approaches to pruning that outperform existing techniques. The paper is well written and well organized.Cons:This paper has few limitations. The main limitation is that all experiments were conducted on relatively small datasets (CIFAR). Given that is has been shown that some techniques in model compression produce state-of-the-art results on small tasks but fail on larger models and datasets [1, 2], Id encourage their authors to further validate their insights on a larger dataset (i.e., ImageNet).Comments:I found that the authors waited a long time to explain the term gradient flow, which was important in sections 1-3 but not fully detailed until the start of section 4. On page 1 the authors say in parenthesis that gradient flow is gradient descent with infinitesimal learning rate, but I found this explanation was not clear. The second sentence of section 4 the evolution over time of model parameters, gradient, and loss was much more clear. Id encourage the authors to potentially work some of these details earlier into the text.References:1. https://arxiv.org/abs/1902.095742. https://arxiv.org/abs/2003.03033 This paper proposes a novel clustering technique that combines the self-organising map (SOM) (Kohonen, 1998) ideas with the differentiable quantized clustering ideas of VQ-VAE (van den Oord et al, 2017). The resulting algorithm is able to achieve better unsupervised clustering than either technique on its own. It also beats the k-means clustering approach. The authors also suggest augmenting their setup with a model of cluster transition dynamics for time-series data, which seems to improve the clustering further, as well as providing an interpretable 2D visualisation of the system's dynamics.This approach addresses an important problem of easy interpretable visualisation of complex dynamics of a multi-dimensional system. This solution can have immediate wide spread real life applications, for example in fields like medicine or finance. The paper is very well written and the model clearly outperforms its baselines. The authors also include very nice evaluation of the importance of the different parts of the model for the final performance.This is one of the best papers I have reviewed in a while. The only question I have is in terms of the medical data. The map learnt by SOM-VAE-prob presented in Fig. 4 appears to have 2 clusters with 'less healthy' patients (near the top left and top right edges). It would be good to have an analysis of what differences there are between these two clusters, and whether they are recovered consistently. The paper presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks. In the heart of the method is state proximity estimator, which measures the distance between states in the originator and destination tasks. This estimator is used in the reward for the transition policy. The method is evaluated on number of MojoCo tasks, including locomotion and manipulation.Strengths:+ Well motivated and relevant topic. One of the big downsides in the current state of the art is lack of understanding how to learn complex tasks. This papers tackles that problem.+ The paper is well written and the presentation is clear.+ The method is simple, yet original. Overall, an elegant approach that appears to be working well.+ Comprehensive evaluations over several tasks and several baselines.Questions:- In the metapolicy, what ensures consistency, i.e. it selects the same policy in the consecutive steps?- Can the authors comment on the weaknesses and the limits of the method? In distributed optimisation, it is well known that asynchronous methods outperform synchronous methods in many cases. However, the questions as to whether (and when) asynchronous methods can be shown to have any speed-up, as the number of nodes increases, has been open. The paper under review answers the question in the affirmative and does so very elegantly.I have only a few minor quibbles and a question. There are some recent papers that could be cited:http://proceedings.mlr.press/v80/zhou18b.htmlhttp://proceedings.mlr.press/v80/lian18a.htmlhttps://nips.cc/Conferences/2018/Schedule?showEvent=11368and the formatting of the bibliography needs to be improved. In the synchronous case, some of the analyses extend to partially separable functions, e.g.:https://arxiv.org/abs/1406.0238and citations thereof. Would it be possible to extend the present work in that direction? # Positive aspects of this submission- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.# Criticism- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem. ## SummaryThe authors pose sparsification as a subdifferential inclusion problem, a novel formulation that results in quite meaningful results on established benchmarks/tasks. The paper overall is very well-written with a detailed overview of current sparsification techniques and how the proposed method differs.## Pros* Very comprehensive analysis and proofs (which seem correct, although not thoroughly verified)* Empirical results justify this novel approach across the board## SuggestionsThe computational characteristics of using SIS has not been characterized in the manuscript; it is no very clear what the complexity of training a large model is using the proposed approach. The authors suggest their training approach is efficient, but do not provide any empirical results or further justification. For example, all of the results in Table 3 and Table 4 can have an additional column that characterizes the time to train.  Strengths:1. The paper is well written. It includes clear math notations and figures. Readers can easily follow the thought process of the authors. For example, Figure 2 shows the relation of l2 loss and phase loss with respect to target energy, indicating the importance of penalizing phase loss in the end to end system. The same observation is supported by Figure 3.2. Strong results. The proposed end2end model significantly outperforms previous SOTA in terms of objective measures and subject tests. The video demo is very convincing. The model improved spatialization and sound quality.3. High novelty. This paper proposes to impose monotonicity and causality to the learned warping function, which incorporates the physics of sound propagation. I am excited to another example of applying domain knowledge to an end-to-end model. The model includes two novel components: the neural warp network compensates the errors from geometry warp, the temporal convolution works as a post processing module to account for reverberation and other effects. Ablation study shows both components are critical. To be improved:1. The caption for Figure 4(a) seems to be incomplete.2. It would be good to include a table to compare the proposed model with baselines in terms of model size and inference speed. This paper proposes a H divergence that is a generalization of many popular f divergences and IPMs. The paper gives an empirical estimator with convergence rates for this divergence, where the rates are very fast when the two distributions are equal. The paper shows how the empirical estimator has practical use for two sample tests and measuring the corruption of a sample. The proposed H divergence is "useful" when the two distributions are close to each other, but as the authors acknowledge in the future work, it is an open question whether it could be "useful" in other cases.Overall I think this paper is very interesting and has a lot of novelty. I am not extremely on top of the most recent literature on measuring differences between probability distributions, so there may be literature that is not being reviewed and ignored, but from an "outsiders" perspective this seems to be a significant contribution to the area. There are some minor grammar issues (see minor comments for the ones I caught) and the paper could use a thorough re-read for grammar in general. Major Comments:The proof for Proposition 2 shows that if the intersection between the optimal action spaces of p and q is empty, then the divergence is greater than 0. However it is not seem obvious that the converse is true i.e. if the divergence is greater than 0 then the intersection is empty. If the converse is trivial, having some explanation for it would be helpful.The notation for the proof of Lemma 3 is rather confusing. The authors want to state that the two samples are equal except for at the points x_j and x'_j, but writing "i \neq j" seems to imply at first glance that the two samples are not equal where the indexes don't line up instead i.e. x_1 \neq x'_2. It would be clearer to just state x_i = x'_i except at one index j. Also remove the second "consider", it is not necessary.Is it computationally possible to run the experiments more than 10 times? The power looks good but the type I errors still look a little noisy. Granted the scale is very small, but 10 is not considered a large number of "simulations".Minor Comments:- Please make Figures 2 and 3 bigger. There seems to be some white space you can play with and there is some room before the 8 page limit.- Need an s here: "distance that work(s) well for distributions over high dimensional images."- Remove "among of" here: "We show that H-divergences generally monotonically increases with the among of corruption added to the samples "?- Use "or" instead of a slash here: "entropy / uncertainty"- Remove "in order" here: "measure how much more difficult it is in order to minimize loss on the mixture distribution"- Insert "that" here: "probability (that) an algorithm makes a type"- "test power" is a strange term. Normally it is referred to as just "power" or "statistical power" or "power of a test"- The caption in Figure 3 has (Left 2) and (Right 2) which are weirdly bolded and might be better written as (two on the left) and (two on the right)? Also missing an "s" here: "Our method (H-Div, dashed line) achieve(s)"- Missing "s" here: "Each permutation test use(s) 100 permutation(s)" The paper explores and compares several methods for parallel training of deep nets. It presents the results on multiple datasets for image classification and language modelling.# QualityThis work provides many experiments with neat visualizations. Pros:- The paper provides many rigorous experiments for 7 different methods of parallel training- The paper conducts experiments on various datasets, including image classification on CIFAR-10, language modelling on LM1B, image classification on ImageNet- The paper conducts experiments analyzing the properties of the various methods of parallel trainingCons:- Not many architectures explored: only Resnet for ImageNet, only Transformer for LM1B- The paper doesn't touch on the recurrent architectures- It is always great to see even more experiments on more datasets and tasks# ClarityThe paper is well written and easy to follow. Although I am not an expert in the parallel training, it was easy to understand.Pros:- The literature is reviewed well- The figures are compact, dense and informativeCons:- Sometimes, the figures are hard to read in print- The conclusion is not very informative, I expected to see clear "do"s and "don't"s of parallel training# OriginalityThe research presented in the paper is original.# SignificanceThe work presented is clearly of great significance to the community.# ConclusionThis is a thorough exploration of several methods for parallel training. The work provides a multitude of experiments comparing the given architectures. While the work doesn't introduce any groundbreaking ideas, it conducts rigorous experiments and presents them well.Suggestions for improvements:-  I expected to see a list of "do"s and "don't"s in the conclusion- It is hard to read the figures when printed In my opinion, this paper is a very good paper: novel in the approach, high impacts in both theoretic sense and practical sense, and well-written. The problem of universal multi-agent reinforcement learning for multiple tasks is very interesting and challenging, and the methodology proposed in this paper is inspiring and the demonstrated experimental results are very impressive. Main contributions of this paper are as follows, which are very significant and impressive: [1] The proposed UPDeT-based MARL framework outperforms RNN-based frameworks on state-of-the-art centralized functions by a large margin in terms of final performance.[2] The proposed model has strong transfer capability and can handle a number of different task at a time.[3] The proposed model accelerates the transfer learning speed so that it is about 10 times faster compared to RNN-based models in most scenarios.The paper presentation is in good quality, the concepts and the methodologies were explained clearly. The provided experiments section is also convincing and somewhat extensive, described and presented very clearly, which supports the main claimed contribution very well.The authors also point out some promising future research direction on top of this work, which is also helpful.Overall, I strongly support this paper to be accepted and published in ICLR. The experiment results are impressive, and the proposed methodologies are inspiring and novel. I believe that many people in the research community would find it valuable/inspiring and benefit from this paper. Summary: The paper studies compression of gradients in distributed training with the goal of minimizing communication costs.  The main contribution of the paper is the development of an *induced compressor* that takes as input, a possibly biased compressor, and outputs an unbiased compressor with similar error variance as compared with the original compressor.  As the output of the induced compressor is unbiased, it obviates the need for error feedback, which is needed when biased gradient estimates are used.Reason for Score:I vote for accepting the paper.  --> The idea of an induced compressor is simple (after the fact), and elegant, and advances state of the art in gradient compression. -->The paper has solid theoretical reasoning via a convergence analysis that does demonstrate that for certain (common) parameter choices the induced compressor is expected to have better convergence than the biased counter part-->   Satisfactory experiments to back up the theoretical insights. In particular, the output of the induced compressor requires more bits as compared with the underlying biased compressor. Empirical results show that, an induced compressor obtained from a class of biased compressors (e.g., top K gradients) has better performance than a biased compressor from that class with the same communication cost.Pros:--> This paper resolves a problem of current gradient compression techniques: the good compressors that require a small number of bits/values to represent the gradients are inevitably biased, and the bias needs to be compensated via error feedback techniques. It shows that there are good unbiased compressors and provides a recipe for constructing them.--> The paper is well-written and based on a sound theoretical reasoning.Cons:--> I would like a more detailed discussion on the effect of the unbiased compressor, that is used in the development of the induced compressor. How sensitive is the performance/compression to the choice of the unbiased compressor? --> I wonder how the theory would apply to weakly convex or non-convex settings.  PlasticineLabThe paper presents a new soft-body manipulation benchmark for RL and differentiable planning.The presented simulation suite is very interesting and the contribution is solid.Strength:- new simulation benchmark with features that are not yet well explored- differentiable physics to open up possibilities for planning methods- tasks are difficult enough to be challenging for a while- baseline results are providedWeaknesses:- only the computation times would be good to addPresentation:The paper is clearly written and easy to follow.Ways to improve the paper:- wall-clock times would really be very useful. Both for the forward pass as well as a backward pass through the entire horizon with Adam. Maybe also some notes on how it can be parallelized since you have a CUDA implementation.Details:- p5 last paragraph: "For any grid points with a signed distance d..:" The formulation is not clear enough.  Do you mean with positive signed distance. Prob not, because you can also have penetration. But why would a point then not have a distance to the rigid body?  - same paragraph: "By definition, s decays exponentially with d until d becomes negative (when penetration occurs)" Well it decays with increasing distance (and then it cannot become negative if it increases...)- 5.1: IoU definition: Are S always positive? I don't exactly understand what the mass tensor S is. I know it from rigid body dynamics, but this does not seem to be the same here. Can you clarify this better such that it becomes clear why the formula describes and IoU.- Fig 4. Consider removing the grey background that seaborn uses automatically. The plots will look much cleaner and better visible.  #### Summary:The authors leveraged and repurposed Noise Conditioned Score Network (NCSN) that was originally introduced by Song & Ermon (2019) for generative modeling to be used for detection out-of-distribution (OOD) images. The authors unfold the intuition and rationale behind score matching followed by the equivalence of denoising autoencoder (DAE) to derive NCSN as a score estimator and provide an analysis to demonstrate the value of multiscale score analysis. In an experimental analysis on SVHN and CIFAR datasets they demonstrate superiority of their method (MSMA) over previously reported findings in the literature using state-of-the-art models (ODIN, JEM, Likelihood Ratios) on OOD task.############################################################################## Reasons for score: I vote for accepting. While the objective foundation of the methodology is adapted from previous work, I find the repurposing of it for fast and effective OOD detection novel and meaningful. The authors have structured and communicated their findings remarkably and provided a well designed experimental evidence to support the methodology for the detection of OOD images task.  ############################################################################## Pros:  1. The paper addresses a relevant issue of OOD images detection using norms of score estimates and is highly relevant to the ICLR community.  2.  The multiscale score analysis was very well done and very well communicated. The visualizations captured very well the essence of the findings and were well highlighted in in the discussion. It was clear, useful and it well justified the following method development.    3. This paper provides comprehensive experiments, well related to the scientific context, to show the effectiveness of the proposed method. The additional performance metrics in the appendix provide a well complementary supprot.  ############################################################################## Major comment: While the paper is overall very well written, structured and communicated, I found the final discussion and conclusion quite lacking. 1) The claim that autoencoding taks better suits deep CNNs should be a bit more elaborated/ demonstrated. 2) The sentence on the peculiar phenomenon exhibited by multiscale score estimates is also not fully clear. It would be better if the authors explicitly mention to which phenomenon they relate. 3) I would find it important to add to the discussion a paragraph on the paper limitations, for example, any limitations the datasets present, limitations on the applied comparisons, limitations of the method application or others. 4) While the authors mentioned their plan to apply the methodology on a specific task, I think the discussion on future directions quite lacking. Are there other potential next steps that can be done on top of the proposed method? The analysis on range of scales mentioned in the end of section 2.1 could be an example of that. 5) As a minor suggestion, the authors may consider to relate to any wider impact of their work.#### Minor comments:At two points in the manuscript the authors mentioned a future application of the method to identify atypical morphometry in early brain development. Since this experimental analysis was not actually done, I found it quite distracting and out of the scope of this paper. I would therefore suggest removing it from both introduction and discussion. Section 5.3, I would suggest to briefly mention what preprocessing was done on *_all_images_. ############################################################################## Questions during rebuttal period:  Please address and above comments. This paper combines object-centric representations and self-supervised HER goal-conditioned policy learning to learn efficient RL policies for a robot manipulation task.They use SCALOR as an object-based state representation, and use it to propose semantically meaningful goals for a SAC policy to achieve. They can then leverage this on new tasks to solve them efficiently..Overall, I found this paper very interesting, clearly written, well executed (with very sensible decisions throughout) and presenting several good ideas, especially in how to leverage the additional object structure effectively. It demonstrates good early results in the novel field of Object-oriented RL.I have a few comments/questions:1. The explanation of how the goal-conditioned policies were trained was very clear, and I especially like how you use z_what and z_where to construct novel meaningful goals (which will tend to just force the policy to move objects around, but that is a good prior for your environment!). However how the evaluation on a novel task is done wasnt as clear (i.e. when we try to implement a given goal or sequence of goals). More precisely, it is said in several places that the goal z_g is decomposed in sub-tasks where only one of the slots is used as the target.    1. Could you provide more details on exactly how that is done? Do you learn p(z^where) on task later?   2. What happens when some of the objects arent achievable or controllable? E.g. Id expect that you see a slot which represents the robot arm, is this treated differently?2. How good are SCALOR representations in your environment?    1. It would be very helpful to show samples / traversals in the Appendix.   2. Similarly, comparing to the GT information you provide would be interesting (e.g. try to decode it? I understand youd have to match the slots up unfortunately)   3. Did you try continuing training SCALOR in the RL phase? It would make the results stronger and less reliant on a good random exploration strategy.3. Did the hard matching cause issues while learning? Id guess that the argmin is not too problematic because it is used in the reward computation only, but if youd consider extending this setting to learning the goal proposal function z_g, this seems like a limitation?   1. The explanation about the issue of using tracking as part of the model directly wasnt especially clear to me. It might deserve a bit more expansion, especially in the Appendix?4. How complex are the observations of the environment?   1. Could you add more samples of the environments observations?   2. It seems like the environment chosen is extremely similar to the standard Gym Fetch environment, did you try using it instead? https://gym.openai.com/envs/FetchPickAndPlace-v0/ 5. It is not entirely true that MONet/IODINE do not contain disentangled and interpretable features like position and scale. It is true that they are not explicitly enforced (like done in SCALOR), but they do arise quite easily purely unsupervised. Especially, in my experience with both of these models, obtaining (and identifying) position latents is rather easy. See for example Figure 5 in [1] and this animation [2].So in summary, I believe this is a strong paper in a budding field, which deserves publication at ICLR and may interest many people there.* [1] https://arxiv.org/abs/1901.11390* [2] https://twitter.com/cpburgess_/status/1091220207941701632  Note about NeurIPS 20 version: I was on the NeurIPS 20 program committee and I was assigned an earlier version of this paper. As such, Im deeply familiar with it. In its NeurIPS form, I felt it wasnt ready for tier-1 publication. However, my concerns were principally around the lack of experimental results. I strongly supported of the ideas presented in the paper, but I fought to ensure it wasnt accepted to NeurIPS because I felt it would have been a disservice to both NeurIPS and the authors, resulting in a rather mediocre paper that could have been exceptional if the proper experiments were run. I attempted to make this clear in my review and encouraged the authors to address this weakness.It appears the authors have addressed my primary concern. This version of the paper resolves my critical reservation of weak empirical results  the results, I believe, are now satisfactory of tier-1 publication, which include four different synthesis systems and over 100 different synthesized programs of two different domains. As such, I now support its acceptance at ICLR.Summary:This paper appears to be the first work to use program signatures (ICLR 20, Odena & Sutton) for program synthesis. The high-level concepts the authors present, as I understand them, is that by using program signatures for program synthesis, they can more closely replicate the process of program synthesis the way programmers develop programs. That is, by breaking one large program into many small sub-programs. Once enough of these small sub-programs have been generated, they can be composed together to solve the larger program  the actual goal.Assuming this hypothesis holds, the end result might be that such an approach would result in a program synthesizer that could generate both (i) more correct programs, (ii) faster than prior systems. For their experimental evaluation, this hypothesis seems to hold and BUSTLE does in fact generate more correct program, more quickly than prior state-of-the-art. The authors compare BUSTLE to three other variant systems: a baseline (less sophisticated BUSTLE system), RobustFill (ICML 2017), and DeepCoder (ICLR 2017). RobustFill and DeepCoder have demonstrated state-of-the-art performance, historically, so I believe these comparisons are sufficient for this papers acceptance.Overall, I think the paper provides a truly novel approach to program synthesis with its fusion of program signatures. I am admittedly biased in favor of program signatures, because I believe the future of machine programming / software 2.0 / neural programming / program synthesis with both stochastic and deterministic approaches (whatever we want to call it) is going to be heavily reliant on our ability to lift concepts from code (the what) which is notably more challenging than lifting the implementation (the how). This is because the what tends to not necessarily be obvious from the code, whereas the how almost always is  its the implementation. With that in mind, this paper presents what I believe is the first demonstrable evidence that program signatures can be used in this fashion. I suspect this is just the beginning of exploration with program signatures  I expect a flurry of follow-up research to emerge that uses them.When taken holistically, I strongly support accepting this paper, but I do have some minor nits Id like the authors to address.Minor suggestions:1. There appears to be no system diagram of BUSTLE. While an expert in the space of program synthesis and property signatures can likely understand what is going on, non-experts I think will really struggle without some kind of visual diagram showing how BUSTLE works. It should be relatively easy to add this diagram to the paper and I believe it will make the paper more widely accessible. If only one of my recommendations is addressed by the authors, I would request it be this one.2. There appears to be multiple locations where the authors seem to deem neural network inference is too slow without qualification. I think this is a mistake and is a bit of a turn off and its a bit of a confusing one given that BUSTLE uses neural network inference. Yes, I do agree with the authors that inference with large neural networks could make the problem of program synthesis slower, but I dont believe this is a universal truth. I think its proportional to the computational complexity of the neural network. I would request the authors find all such inference is too slow cases in the paper and properly qualify them. I suspect this will encourage future work to consider other neural network architectures that may be competitive or even outperform BUSTLE.3. Can you label the different variants of BUSTLE from something like Using model and heuristics to just BUSTLE, Using model only to BUSTLE (model only), etc. Right now its a bit confusing at first glance on Figure 2 to see which is the full BUSTLE system because there isnt actually any legend item that is called BUSTLE. Should be an easy fix and will likely make the figure easier for the audience to understand.4. Can you please drop the word very from the paper everywhere it appears? I do not have a mathematical representation of what that word means (nor does anybody I think) and, as such, I believe it introduces unnecessary ambiguity and also wastes paper space.5. I couldnt tell how the BUSTLE training time was factored into the analysis. Can you find a way to explain that more clearly? I realize that its a potential one-time only penalty, but it doesnt come for free (to my understanding) while some traditional program synthesis systems using formal methods can simply generate programs without any learning overhead. I think this needs to be captured somewhere so people dont forget about this cost.6. I got a little confused by the comments about removing restrictions of Concat() in the second paragraph in section 2.2. Can you try to explain that more clearly?7. Can you provide some intuition on the rationale behind keeping 100 positive and 100 negative values as explained in the last sentence in section 3.1?8. Can you double check to ensure all of your acronyms are fully spelled out first? Im familiar with all of them, but others might not. I dont think I saw the spelled out versions of DSL, AST, JVM, etc. While these terms are generally widely known in the programming languages community, Im not sure if the machine learning community is as deeply aware of them. Regardless, it seems to me that its usually a good idea to spell out all acronyms first.Future work:Do you really think an abstract syntax tree (AST) representation is the right representation for this approach? Im not so sure. I recommend taking a look at the Aromas simplified parse tree in the paper by FAIR, Berkeley, and Riverside (OOPSLA 19) and, more comprehensively, MISIMs context-aware semantics structure from Intel, Georgia Tech, MIT (arxiv). I suspect a next iteration of BUSTLE using either of these structures might result in even better performance than what youve currently achieved with an AST representation. But, thats just a guess. :) # Main IdeaThe main idea is that training to reduce texture bias in convnets in favor of shape bias is often thought to cause the concomitant increase in corruption robustness, but this has not been tested directly.  The authors propose a systemic study of this relationship and conclude that both shape bias and increase corruption robustness are both byproducts of style-variation during training, that is they share a common causal mechanism, but that shape bias does not itself cause corruption robustness.They accomplish this by creating a new augmented dataset which encourages learning shape features (created from the edge maps of the training images) but does not also induce robustness against common corruptions.#### Additional findings from the study:Shape bias gets maximized when edge information and stylization are combined without including any texture information.Corruption robustness is maximized by superimposing the image (and its textures) on the above stylized edges.They propose that corruption robustness seems to benefit most from style variation in the vicinity of the image manifold.# General StrengthsI think the paper makes a compelling case.The Geirhos result, while elegant and interesting, has always struck me as a roundabout way to get to the shape bias question.  These authors more directly attack it, and show that the relationship doesnt hold, while showing the real. Value of the Geirhos result was in showing that manifold-local variation increased robustness (and also consequently shape bias).In addition to the elegance of the result, the authors use of texture randomization via texture feature randomization is a an elegant (and much faster) implementation than regenerating an entire dataset with many different textures.   The insight is good, the image itself doesnt need to be rerendered, but the networks interpretation of its texture needs to be scrambled.  This doesnt restrict you to the statistics of any database on which you might extract textures# WeaknessesDoesnt report I-SIN results across ImageNetC corruption dataset.Would have preferred to see the specific corruption type results broken out for more than just the two shown and the average robustness (the variation and performance on different types of corruption could be useful information).Also make sure the names are consistent across graphs and paragraphs.  Not the best naming scheme (too many very similar acronyms which arent immediately evocative of the underlying point). ##########################################################################Summary:Authors start from an assumption: local negative sampling is the bottleneck of dense retrievals effectiveness. To overcome this limitation, authors propose ANCE (Approximate nearest neighbour Negative Contrastive Estimation), a new contrastive representation learning mechanism for dense retrieval. The basic idea is that of constructing negatives exploiting the being trained deep retrieval module. The idea is that the model considers as negatives borderline cases. They also show, theoretically, that this improves the variance of the stochastic gradient estimation thus leading to faster convergence.##########################################################################Reasons for score: I honestly enjoyed reading the paper. It has a theoretical justification that explains the intuition of using hard negatives. Experiments are thorough and they show improvements over the state of the art. The discussion section is thorough. I believe that this research results are very important also in practice  ##########################################################################Pros:  1. The paper addresses a timely and important problem2. The paper gives a nice theoretical justification for the reasons why they have to use hard negatives3. Experiments are thorough and nicely done. Results are very good. I particularly enjoyed seeing that both on public datasets and on real world search systems the novel retrieval mechanism helps greatly.  ##########################################################################Cons:  1. The only one thing that I believe might impair the utilisation of this method is that you need to reconstruct the embeddings every m batches. It takes 10h every reconstruction and it is not clear what happens every m batches. Do you start a novel reconstruction? Do you replace the current embedding version with a new one as soon as one finishes training? This aspect, in my opinion, is the weakest of the paper and it would deserve more attention by the authors. ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################Some minor issues (1) In equation (2) I would call D^+ and D^-, D_q^+ and D_q^- in order to remark that negatives and positives are per-query.(2) an negative > a negative(3) Citation Luan et al. > Its Toutanova not Toutanove