 ========Post-rebuttal========I thank the authors for answering my questions. While I am satisfied with the authors response to my concern about the "initial good architectures" assumption, I still remain unconvinced that adversarial learning can help search find new good architectures. I keep my score.I also encourage the authors to carefully re-read the f-GAN paper, which explains exactly how any f-divergence (including the KL) can be implemented for adversarial learning. Switching to any f-divergence requires only a simple change to the loss function. It also appears that the authors significantly misunderstand VAEs. The difference between GANs and VAEs is not JS-divergence vs KL-divergence. Using a KL loss for adversarial learning does not require switching to a VAE. Given the central role they play in this paper's motivation, a better understanding of these subjects is important. # Post-response updateThank you to the authors for answering some of my questions and clarifying the search and evaluation of GA-NAS. I believe my original assessment that the contributed method was complex remains accurate; while the authors note that other methods like ENAS also use an RNN controller, in my view those methods are also complex. This paper increases this complexity with a GNN and an adversarial training setup. Use of such additions require showing significant improvements over baselines like random search, which I do not believe is achieved. I thus stand by my initial rating. AFTER REBUTTAL I read the rebuttal of the authors and the other reviews. I will increase the grade to a 5.The reasons are that I think that the authors improved significantly the paper with this revision. However, I believe that the paper would benefit from experiments on a larger number of datasets, in order to better understand on which type of datasets their method shows better performance.  ##### Post-rebuttal updateDear authors, Thanks for the response. I strongly encourage you to revise the paper using languages other than flow-based generative models.From the rebuttal I could not see whether you understand my point or not: The conditional mean embedding operator defines an **integration** instead of an invertible **transformation**, which differentiate itself from any flow-based model (at least those you referenced).As for now, I cannot recommend for acceptance.-------------- **Updates**:After carefully reading comments of the other reviewers as well as the authors' response, I change my score from 6 to 5. ========= post rebuttal ========Thanks for adding the experiments for Replica - these at least seem to suggest that the approach can work on more complex scenes than shown initially in the paper. I still think the training scheme and required data is relatively specific for this approach that it is hard to see that it will generalize beyond - but that is probably fine for a research paper.  that addresses Q1for Q2 - please include some information in the main paper - not everyone will check the supplement and the paper should be self-contained. I still find the comparisons and ablations weak as the particular training setup and model are the key contribution for the paper and thus without a proper ablation it is hard to know what exactly to take away. Therefore I still remain somewhat skeptical but have raised my score somewhat. To me the paper is still borderline and thus somewhere between 5 and 6 really.  I have increased my score to reflect the revisions--------- After reading the response, my concerns are not fully resolved. For example, based on "For OMP-a and OMP-b, ... The robustness of single network is terrible. ... adversarial examples created from one of these networks can be successfully reclassified by other networks", I am feeling the OMP-a and OMP-b are less effective based on a real white-box attack. Also some other concerns are not fully addressed. Thus I am keeping my rating unchanged. ------------Update after author response: I've increased my score to 5. However, I still think this work is not ready to be published at ICLR in its current form. One of the other reviewers had raised an important point about the reliance of the proposed system on clean text which the authors should consider addressing in an updated version of this work. ** Comments after reading the authors' rebuttal **I would like to thank the authors for their clarifications. The threat model is now clearer to me - and I think it deserves clarifications in the paper as well.First of all, as far as I understand now, there's a net distinction between backdoors and clean-label attacks. Backdoor attacks assume that the attacker controls the design phase and the training process, and releases a backdoored model (which then someone else re-uses possibly with fine tuning). Hence, defenses against backdoors aim to detect whether models have been backdoored or not, and it is reasonable to expect that the defender doesn't know the training data as well as other design choices (as the attacker released the model). In this setting, clean-label attacks do not make sense (as the attacker controls the training labels too).Clean-label attacks assume a different setup. Here the attacker only injects poisoning samples into the training set but does neither control the training process nor the training labels. Hence, clean-label attacks make sense in this setting. However, it also makes sense that the defender knows the training data (as the defender is the one that trains the algorithm, and the purpose is to either detect and remove the poisoning points or reduce their influence over training) - and hence I'm expecting the authors to do consider previous defenses that assume knowledge of the training set in their work.To summarize, I think that:(1) the authors should clarify in the title that they restrict themselves to clean-label integrity/targeted poisoning attacks.(2) the authors should clarify the threat model, and clearly distinguish poisoning availability attacks (bilevel data poisoning) vs poisoning integrity attacks. Furthermore, in the poisoning integrity/targeted family, backdoor and clean-label attacks should be distinguished and the threat models clarified (in particular, w.r.t assumptions on what the attacker/defender know and have access to).(3) the authors should revise their sentences on the complexity of data poisoning (previous clean-label targeted attacks like poison frogs are not as complex as bilevel data poisoning attacks). A fairer comparison in terms of complexity should also be considered - how faster is this new attack w.r.t. poison frogs and the other clean-label targeted attacks? (poisoning availability should not be considered here as the goal is different in that case).(4) In general, there is need to disambiguate clean-label targeted poisoning attacks from the rest, and better position this work in context. Reading the paper in its current form, it seems that the authors are also able to improve scalability of poisoning availability attacks whereas this is not the goal of this work. I'm willing to revise my score if the authors agree on making these clarifications in the paper, better highlighting the net contributions of their work and the proper context of competing approaches (which do not include backdoors and poisoning availability attacks). ----The authors rebuttal and the revised version have not fully addressed my concerns. It is not surprise that partial updating outperforms pruning by a large margin, as the inference of small updating still uses the whole weights of the network. Comparing to pruning, the technical contribution of this work is limited, so I would like to keep my original rating.    [update] reduced score given the lack of  post-rebuttal: Authors pay much effort on addressing issues that I mentioned, and I appreciate that very much. My issues can be partially resoved, but the main issue is regarding the high-level thinking on semantic supervisions, which is impossible to be fully addressed in a rebuttal. Besides, I agree with AC that (a) the improvement is very limited and (b) semantic labels are hard to obtain. Overall, I raise my rating to 5 (not so good, but could be accepted), because I really appreciate authors' effort. ========Post-rebuttal comments:Thank the authors for answering my questions. I think the current version of the paper is below the bar of acceptance at ICLR and I hope the authors can incorporate the answers to make the submission stronger in the future. -----Post feedback edit:The authors did not address the (admittedly, small) terminology issue I raised in my review. More specifically, I argued that their method in general requires transmission of the value of the threshold function, to which they replied that this is not true if the threshold function has a particular form (not required by their general theory), and then added some discussion about removal of spikes which was not what I was getting at. My point still stands: in general, for arbitrary threshold functions, transmission of the value of the function is needed for succesful decoding, and this constitutes more information than what is generally meant when discussing spike trains (which are just sets of time stamps).Moreover, I was not convinced by the authors' reply to the points raised by Rev 4. This is actually more important than the point above, since Rev 4's objections are on the substance of the method (unlike my own points, which are little more than presentation and terminology matters).Therefore, I am lowering my score.I am not convinced by the authors' reply to Ref 4's comments. Moreover, the smaller terminology issue I raised in my own comment (about the Post-rebuttalI thank the authors for their response, addressing some of the issues/questions I had raised.After carefully reading the other reviews (and corresponding author responses), I agree with the valid criticisms raised (some of which I had overlooked initially), which unfortunately further dampened my enthusiasm for this work.As a result, I am slightly lowering my score.However, I very much appreciate the author's efforts in this very promising work, and hope that they will revise their manuscript to take into account the feedback raised in the reviews.###  -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing some of my questions and concerns.Unfortunately, I still think that some aspects could be better analysed,it is not crystal clear to me if the improvements come from the GP or their proposed variational informationtheoretic function. I am keeping my original score. Edit: I have read the authors' response and update  manuscript. I appreciate the new experiments and some of the clarifications. However, some of my fundamental issues with this work are not addressed.The distinction with Ingraham et al feels forced. It is important to compare head-to-head on Ingraham's dataset in order to truly understand the tradeoffs between these methods and to understand the differences in performance between flexible backbone approaches and a well tuned rigid backbone approach. Why are perplexities for Ingraham et al not reported for all cases? Improving over Ingraham when only low resolution structures or incomplete structures are available is interesting, but I also question how useful this case is. When attempting to do _structure-based design_ of new proteins, how often are fully specified structures not available for those tasks? Ingraham et al presented a variant of their method based only on a flexible structure specification and it isn't clear how that contrasts with this flexible specification. It is certainly possible that this approach is better, but we need a head-to-head comparison to know.With that in mind, I'll raise my score to 5. I think this work has promise, but is not yet ready for publication in its current state. Thank you for the additional comments and results. I have increased my score.Requiring a simulator for sampling is indeed weaker than knowing the whole dynamic model. However, it is still a strong requirement, so the applicability of the proposed method in real-world scenarios is limited. Addressed Concerns:- corrected the typos- Some figures are updated Not Addressed:- Figure 1b is not uniform as 1a (legend information is missing)- As mentioned in weaknesses: the novelty aspect of this work is missing.- Further, the authors have not answered question 1 in major comments.- Based on Fig 2 & 3, Calcium GAN generated signals are mismatching with recorded data. How one can deploy this model in real-time when model results are not identical at all?.  [After rebuttal: Increased my score from 4 to 5. Still think the approach needs some more work to appeal to the broader ML community and in its current state would be best suited for a conference focused on Alife] *****************After author response:I appreciate the updates you've made to the paper to better flesh it out, including the diagrams, pseudocode, and additional ablations. I've increased my score accordingly. Regarding generalization, I fully agree that it can be difficult to show. That said, when it's advertised in the title of the paper, I expect it to be clearly shown in the paper itself. It seems that the language has been greatly toned down in the updated version. However, without entirely re-reviewing the paper, I am unable to fully recommend acceptance.Aside: I would have appreciated responses to my questions directly so that I don't need to dig through the rewritten paper to find the answers to my questions. __________After author response:I appreciate the authors response. Previous topics:1) The authors add ablations on the hard vs. soft min during the graph search, the additional results are informative, but not conclusive. Given that the overall performance is similar, the authors need to demonstrate the soft-mins benefits for each experiment and over more training seeds. 2) For generalization, the authors confirm that this method is unable to generalize to new environments, though clearly it has other benefits in terms of data efficiency and robustness of solutions. I believe these are still important benefits, though it would be useful to discuss how generalization may be achieved.3) For harder experiments, the authors note that the baselines perform poorly there and thus these tasks were not considered. Though reasonable that the baselines are unable to perform in such cases, harder experiments would show the limit of the proposed algorithm. It would be useful to see for instance how well it scales with dimensionality, how quickly the success rate falls off.New comments:a) I believe the title change away from Generalization is an improvement, though the algorithm name "WORLD MODEL AS A GRAPH" seems to not capture the novel aspects of this work. This name I believe would be more readily applied to search on the replay buffer or semi-parametric topological memory.b) R2's point that much of the robustness may be a factor of choosing states further from the wall is an interesting one. It would be interesting to examine exactly *why* the method is robust.Overall, I believe the paper is interesting and proposes some novel ideas that have benefit, it requires more thorough analysis, and thus I am leaving my score unchanged. Updates: Thanks for the authors' response. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to show the superiority of the results. I retain my original decision. =====================Edited after author response: I thank the authors for their considerate responses. Overall, my opinion remains mostly unchanged, and I share similar opinions to reviewer 3 and 4 that although the proposed idea is interesting and intriguing, the paper is not quite ready at this point. I would like to see the authors present either: 1) stronger empirical evidence for the importance of their metric or 2) a more solid theoretical foundation of the measure they propose. ***I would like to thank authors for accurate answers and a lot of work put on reworking the paper. Unfortunately, I still find my concerns about motivation for the metric valid, which together with the rather weak performance creates a problem for this paper. I highly encourage authors to continue the work and try to explain the reasons for this correlation and find justifications for usage of the metric.  Post-RebuttalI thank the authors for the detailed answer. In light of the response of the authors and the other reviews, I still recommend rejecting the paper, with a rating of 5.Some comments based on the rebuttal:I find the data in table 6 to support my point on confounding variables. The churn with data augmentation fixed on GPU is systematically higher than with random data augmentation on TPU when model initialization is random. Note that the main differences here beside the fact that data augmentation is fixed or random, is that the accuracy is lower by 1.5%-2.5%. We see again an increase in churn related to a decrease in accuracy. Just as when the data augmentation was removed altogether. The authors say that accuracy change itself isn't predictive of churn because we could make a training perfectly reproducible with lower accuracy thus leading to 0 churn, but the same argument would hold for removing a fixed data augmentation from a perfectly reproducible training. When not fixing the whole training process, two different interventions leading to equivalent accuracy decrease could lead to equivalent churn. This for instance would be a direct consequence of a binomial modelisation of the model performance as a function of test set size and model average accuracy (and by the way which models fairly well test accuracy variation for the datasets-architectures in this paper). The lower the accuracy, the higher the variance.Table 2. We boldfaced the results in table 2 with the best mean performance, which we believe is a standard practice.It is unfortunately common practice indeed, but it is a bad practice. Results that are so close that random fluctuations could explain the difference should not be considered as different. ***Post-Rebuttal***I want to thank the author for addressing my concerns. However, the authors did not address the issues of the evaluation. I think the paper can be further improved by providing more convincing results and analysis. For me, the significance of the proposed method is minimal. Thus, I will not change my score. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Unfortunately,  I still think the results on the object representation is not convincing due to lack of comparisons with other embedding methods, and more needs to be done to study the generalizability of this method for complex non-street scenes.  I retain my original decision for these reasons. --------------------------------After rebuttal:Though the paper is interesting in showing improved proposal and detection performance by using attention, excitation, and transformers, the novelty is not significant, especially when it is shown that the claimed cross-modality attention and excitation do not help much. The other reviewers also show the same concerns. I incline to degrade my score to 5.  Opinion after rebuttal:Originally, my rating was a 5. The authors have rebutted my concerns regarding the experimentation, which have gained in insight. The limited novelty and fit to the conference do remain pressing issues and it seems that this is partially shared with the other reviewers. My recommendation is therefore to extend and resubmit. ---------Update after rebuttal:The updated paper addressed my concerns regarding missing details. I appreciated the efforts of comparing with more sophisticated backbone features. However, as pointed by other reviewers, the novelty of the paper is marginal. I keep my original rating as 5. ***Post rebuttal review***Having read through the rebuttal, the updated submission, and the reviews of the other reviewers I have upgraded my review from a reject to marginally below acceptance. This is for two main reasons. 1) the authors have vastly improved the presentation of the submission, which now looks a lot easier to read, 2) the authors have clarified for me, at least, what the main contribution of the work is. That said I am not entirely sure what this contribution adds to the equivariance community, hence why my recommendation still leans towards reject. As far as I am aware, solving the equivariance equations is not the large bottleneck to progress in our community. They are linear equations, and there is work back into the 80's solving them (check out people like Pietro Perona, Patrick Teo). I think more importantly we need to focus on pushing the boundaries in areas such as extension to non-Euclidean manifolds, convolution over non-compact groups, learning symmetries, etc. While this work is clearly mathematically sound and the authors have demonstrated deep knowledge of the area, it feels a little like retracing prior works. That said, if the other reviewers disagree then I don't mind this paper being accepted. Perhaps since I have worked in this area, what appears as obvious to me is not generally acknowledged and this paper may serve as a useful clarification for those wishing to dive into the literature. ----post-update----Hi, I thank the authors to give useful feedback about several concerns. But I am still questioned about the cross-sentence operation, even the authors give an example of the different tasks, which is hard to convince me for such kind of learning can achieve such results. The 5 tasks as the author described are much harder, which somehow indicates that bilingual data is not so necessary for machine translation (but it should be aware, this is not unsupervised machine translation, the training spirit is totally different).  Random data augmentation could be very very strong. The authors are expected to give a code implementation and the trained model to give a more convincing result.  --------------Post response-------------------Stock market is complex and the evaluation with few statistics is unrealistic and too simple. At least the evaluation should be more fine grained than prior work. The goal of interpretability is nice, but not sure the paper gets there by only tuning few parameters.Overall, I like the idea but needs better execution of those ideas. ##### AFTER RESPONSEThank you for answering. My concern on theory is resolved, but I still think an additional baseline of regarding it as a hyper-parameter would be helpful as a comparison, having at least a few trials with mean and standard deviation for the experiments would make the conclusions stronger, and unifying the baselines is important. ----- edit after the authors' answers -----I'm not convinced by the authors' answer on the meaning of the CF-metric when condition 2 is not verified, which is the case on real data. "... but we would expect fairer decisions to have smaller CF-metric in practice" doesn't seem enough to claim that FLAP is better because its CF-metric is lower. In addition we don't have confidence interval to assess if FLAP is really better on MAE.Finally the review of reviewer1 made me realize that CF-metric and FLAP are based on the same function. I partly agree with the authors' answer, i.e., we need a counterfactual function to construct a good metric and then it is optimal to use it as well in the algorithm. However it is not fair to use that metric to claim that the algorithm is fairer. Therefore I lower my rating to 5.  **Post-rebuttal update: the score is increased from 4 to 5. See the response to the authors' comments.** Post-rebuttal feedback-------------------------------I thank the authors for their reply. I still think that this paper has the major problem of presenting results about interference that are not strong enough to be published. In particular, I agree when the author say that "the paper tries to bring conceptual clarity to this important topic, and provide a clear empirical methodology to measure interference and understand correlations to forgetting", but still I think that the overall contribution consists "just" of one (of several other possible ones) intuitive method to measure interference, without a solid motivation. To me, this paper is promising but should present more significant results to have a stronger impact. I think the options are only two: stronger theoretical motivation, larger empirical analysis especially considering deep RL. Between the two, I consider the second option the best. **Post-Rebuttal**The authors have addressed some of my concerns during the rebuttal phase. Thanks! For me the major drawback of the current manuscript remains that 'sequential bias' is not really defined, and not really showed to exist in the datasets at hand. Only indirectly, by our new method performs better than existing methods. I think this should be improved, to increase the understanding of this topic. Update: After reading the other reviews/responses, I'm keeping my score of 5, due to pervasive concerns about the narrow focus of the experiments and incremental novelty of the method.  ----POST AUTHOR RESPONSE --------1.) Efficiency of our Vote-Attack: It is clear that the vote attack is more time efficient than other attacks, but there is no clear motivation for this improvement. To my knowledge the attack creation time has never been a barrier to adversarial research, nor has it prevented real world adversarial attacks. As a result this focus of the paper simply confuses the reader, be spending time addressing an issue that is not important in research or practical settings.  2.) Optimizing for activations of non-capsule components of a capsule network: In the authors response they discuss the semantic meaning of the votes of capsules. This too is a bit of a red herring. Although when discussing the motivation behind capsules, the potential for semantically meaningful capsule votes is invoked, there is nothing in the training procedure that ensures that the activations of the capsules correspond directly to features that humans would find semantically meaningful. In my original review i mentioned that by not attacking the output of the capsules after the routing procedure, this attack was simply optimizing for representations extracted from a standard neural network. In this way this work is similar to the representation attacks first presented by [1] which showed the success of representation attacks on standard neural networks.  3.) The undetected attacks often resemble the target class, under the class-conditional capsule reconstructions detection:Th addition of the attack visualizations is an improvement but the authors do not specify which attacks are successful and undetected and a few of the visualized attacks do indeed resemble the target class.   4.) The additional defense mechanisms presented in Deflecting Adversarial Attacks:The authors are right to point out the scope of the paper, and it is perhaps unreasonable to expect this paper to address these defence mechanisms, but their inclusion would greatly strengthen the paper. [1] Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J Fleet. Adversarial manipulation of deeprepresentations. In ICLR, 2016. ################################################Post-Rebuttal:I want to thanks the authors for their detailed responses. It addresses my concern of hyperparameter choices. However, after going through the responses and paper again, I decide to maintain my initial assessment. The main reasons are that 1.conceptually C-score is not very novel. The addition of this paper to our existing understanding is not that much. 2.practically the C-score does not improve the-state-of-art of outlier detection (comparison with traditional methods is lacking).I encourage the authors to further explore using C-score on improving outlier detection and comparing with existing traditional methods. Besides, diving a bit deeper into using C-score to analyze the dynamics of different optimizers will be very interesting.  EDIT: The authors' rebuttals have solved my concerns partially. However, there still exist some concerns about this paper.1. In the introduction, the authors indicate that nonparametric methods compute the class centroids for novel classes. However, I am not clear whether computing class centroids is the only mechanism to solve class-incremental learning. Besides, I recommend citing more papers about this interpretation.2. The category centers are usually affected by the number of samples. When the samples are scarce, the constructed centers are not accurate, which affects the performance. The authors should give more discussions about category centers.3. For the regression problem, it is better to give more implementation details and ablation analyses. Besides, whether MSE is the only loss to train regression network.Based on these concerns, I update the score to 5. ======Post Rebuttal Update======I would like to thank the authors for their rebuttal, which has addressed part of my concerns. After reading the authors' rebuttal and other reviewers' comments, I'm still concerned on the weak baselines and mixed results in this paper. Unfortunately, I will keep my rating.Concrete suggestions to improve this paper in the future:(1) Strongly recommended: use ResNet50 instead of ResNet18 for the small scale experiments, in this way you get directly the numbers from the literature (e.g. BYOL on CIFAR-100);(2) Nice to have: for the expensive ImageNet experiments, it would be nice to get comparable results using the smallest comparable architecture (e.g. ResNet50) from the literature. SimCLR claims that "With 128 TPU v3 cores, it takes <1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs" and MoCo claims that "For IN-1M, we use a mini-batch size of 256 in 8 GPUs, ..., train for 200 epochs ..., taking <53 hours training ResNet-50." *Update after reading the authors' rebuttal:The revision of the manuscript was much improved. However, the lack of controlled experiments does not convince me. This paper proposes a new penalty to deal with mode collapse, and the authors claimed that it could be easily added to any existing GAN variants. The authors may need to provide a controlled experiment to show their claim, e.g., what happens if adding their penalty to some GAN variants, fixing the same setting. One good point from the new revision is that BuresGAN can be competitive with the state-of-the-art baselines, with some slight changes in the network architectures. I suggest the authors to test their penalty with other GAN variants to stronger support their claim.-------------------------------------------- ---Update after rebuttalAfter reading the authors' response and the other reviews, I think the paper has quite clear pros and cons.The experimental results (especially at $\epsilon=8/255$) are strong and the underlying idea of finding a good starting point for single step adversarial training makes sense to me (see reply below).On the other side, the initial (and partially current) explanation relying on randomized smoothing given by the authors is not convincing, in particular when discussing the role of the random step in the success of Fast AT. The new experiments provided in the revision which rather analyze the smoothness of the starting point found by Backward Smoothing look like a much better explanation of the success of the proposed method (note that although the overlap of terminology I don't see a direct interpretation of the smoothness of the loss function at some specifically crafted point in terms of randomized smoothing). This should be more thoroughly analyzed and commented, which would consist in a quite major update of the paper in my opinion.The current version doesn't provide an exhaustive motivation and analysis of the proposed method (in any direction, randomized smoothing or others), although the revision improves in this sense. Then, although I appreciate the good empirical results and I'm still in favor of the proposed method, I have to lower the score. I encourage the authors to clarify the weaknesses of the paper, which might result in a better understanding of what's needed for a successful fast adversarial training. ------**Update after the public discussion:**Thanks a lot to the authors for clarifying many details and providing additional experimental data. In the updated version of the paper, the authors improve the results of the baseline "Fast TRADES (2-step)" and add additionally a stronger baseline of "Fast AT (2-step)" (except on CIFAR-10 with eps=16/255 where it's missing). However, at least for eps=8/255 on CIFAR-10, CIFAR-100, Tiny ImageNet, the authors show consistent improvements over the baselines with comparable computational cost ("Fast TRADES (2-step)" and "Fast AT (2-step)"). This is an interesting result, although it's not clear to me whether it's specific to eps=8/255 or it would generalize also to higher epsilons such as eps=12/255 or eps=16/255. On the other hand, I still think that the current motivation of the method is very incomplete and it is still unclear why the proposed method should work in the first place. Perhaps, it's a good idea to further develop the additional experiments about the curvature of the loss surface at different points in the input space.Then I think there is additional work to be done in terms of understanding what the proposed method actually does (even if we don't take into account how it was motivated -- via randomized smoothing or not). In particular, it's still completely unclear to me why 2 steps of PGD with respect to the original KL divergence (i.e. Fast TRADES (2-step)) works worse than first 1 step with respect to one KL-divergence and then 1 step with respect to another KL-divergence (i.e. Backward Smoothing). This seems quite ad-hoc and requires further explanations, in my opinion. Moreover, I find it also quite puzzling that Backward Smoothing even outperforms 10-step TRADES / AT as shown in Tables 3 and 4 -- not sure about a justification behind this.Taking into account all of this, I update my score from 4/10 to 5/10. I think the paper can still be improved in various ways: both in terms of the motivation and experimental results. ********After Discussion*************I thank the authors for answering my questions and performing additional experiments. Some of my concerned are addressed during the discussion stage. Therefore, I raise my score from 4 to 5. However,  I still hold my opinions that this work does not have a strong motivation, does not help too much for standard adversarial training and has a potential trade-off problem. Post author response:After having carefully read the author's response and additional reviews, I confirm my original recommendation.    ** The Good NewsI am increasing my score by one, because of the following:Having read through all the conversation, I realize didn't appreciate enough the reproducibility gains from amalgamating existing environments.The added ML hyperparameter tuning experiments (Keras and sklearn) make the work more relevant for ICLR.** Remaining Issues (referencing above)Relevance to ICLR: I still do not think ICLR is the right venue for this work. For instance, not a single previous ICLR paper is cited, the most relevant work all seems to be at GECCO. For example, ABBO is a direct follow-up to work that was published at GECCO.Highlighting new tasks: I appreciate the highlighting of which benchmarks are new. It is hard to know how impactful this is though. For instance, if someone were to publish a new blackbox paper, would they necessarily use these? Of course they're nice to have, but I suspect most people will still just use Nevergrad + MuJoCo.ABBO handcrafted: The arguments provided further reinforce my belief this is useful for industry/practical settings, but not for ICLR.Plots not clear: The arrows do help. However, the results still aren't clear/easy to read. Maybe it would be better to have a big table instead of Fig 2,3,4 and then put those plots in the appendix 2-4x the size they are now.RL Experiments: I suggest changing the language of the RL experiments, the results presented for gradient based are not at all state of the art. "SOTA with grad" is completely wrong. This was pasted from a 2017 paper (indirectly from other pasting). It is possible this could be fixed with communication. For example, I think for a linear policy some of these results would be SOTA specifically for that setting. But then you shouldn't compare against TRPO-nn, because once you open that door you should also compare against TD3/SAC (two very obvious RL algorithms that get ~3x the reward you get on HalfCheetah in significantly fewer timesteps). So it should be phrased as "blackbox optimization of a linear policy for RL" and then remove the gradient stuff, it isn't relevant, unless it was run on the same architecture. Another thing is that this is quite low dimensional. It would be interesting to see how these blackbox algorithms compare for optimizing larger neural network policies, as was included in the OpenAI ES paper (Salimans 2017).Which acquisition function was used for the BO baselines? It seems the comment that it "explores the boundaries first" is highly dependent on the setup. A greedier acquisition function like UCB will probably not do that. I feel the BO comparison is a little weak, in that no recent methods have been compared (for example TuRBO, Eriksson et al NeurIPS 2019). Thank the authors for their responses. I read through the responses from the authors and comments from the other reviewers. I would maintain my initial rating: I think this paper will benefit significantly from a major revision, either strengthening the theoretical contributions or improving empirical validations (by actually performing extensive comparisons with existing algorithms that are designed to handle negative transfer problems). I still have some concerns regarding the paper. The lack of baseline comparisons with spatio-temporal data (as also observed by fellow reviewers). My other concern also remains - from the authors' response, it is not clear how once can clearly attribute explainability of the results from their analysis of the model. Update: The authors addressed part of my concerns. For the factor estimation, the proposed method relies on first order approximations while learning the posterior of the factors; however, the approximation error does not enter into the posterior.  The approximation also raises concerns regarding the convergence of the algorithm. Overall, I think the approach is promising, but some justification of the quality of the approximation is needed. Thus, I tend to keep my rating.############## ########################### POST REBUTTAL DECISION###########################After reading other reviews and the rebuttal, I have concerns related to the stochastic trajectories mentioned by R2. The fact that the authors confirmed that all semantic actions have the same number of steps makes me question potential overfitting. I would imagine that some actions should take less steps than others based on the objects that are being interacted with, and potential multiple trajectories when performing the same semantic action. I still think the task is interesting, but the setup seems not appropriate to claim a general concept from the current method. Therefore, I have decided to lower my score. =====POST-REBUTTAL COMMENTS========I thank the detailed response from the authors. The authors addressed the novelty of this paper. The experimental results on toy tasks are convincing. However, the way this method increases Delta still seems very problematic to me and not seem robust in complex real world cases.I increased my score. ==========Post-rebuttal comments================Based on other reviews and authors response, I have decided to keep my score. I still feel this paper need more work such as experiments on real world datasets and more comparisons as pointed out in the review.  =======After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I really like the idea, but its important to evaluate the idea with respect to a downstream task to get a better idea on how to use the learned structure.  ==========I would like to change my rating from 9 to 5. The paper proposes an interesting idea and does achieve good results on several datasets. However,  after reading all the comments and feedbacks, I notice that the comparisons are not convincing enough, and I have some concerns about the performance of the proposed method on more general and challenging tasks. --------------**Post rebuttal**I have considered the revised paper, rebuttal and feedback+rebuttal of fellow reviewers and in the end decided to leave my score unchanged. Below is a summary of my reasoning.----The Rebuttal has addressed many concerns and the revised edition has further strengthened the paper in many ways but unfortunately lack in the empirical evaluation. It seems like the warmup of Apollo requires a [start, end] learning rate as well as the increase rate compared to the single learning rate of say SGD. It is not clear that the additional overhead of tuning these parameters could not be used to further improve the training schedule of the baseline for better performance (particularly in the CV). At the provided [[link]](https://github.com/bearpaw/pytorch-classification/blob/master/TRAINING.md) for the CV task it looks like the weight decay for CIFAR-10 with the Resnet-110 architecture is set to $10^{-4}$ (not $5\cdot 10^{-4}$) for which $\eta=0.1$ (SGD+M) was good, meaning that the used $\eta$ is not necessarily optimal for the higher weight decay. The results on the language modeling task are impressive but for the algorithm to be accepted as a particularly good algorithm for RNNs it should compare to the more elaborate SotA algorithms for this particular task (AdaQN was proposed). ----Some points that would be good to address regardless of outcome (no influence on my decision):- Does the algorithm work in the $\beta=0$ setting? SGD with momentum reverts to SGD and Adam reverts to RMSProp which both are competitive optimizers. Does that also hold for Apollo?- How are the values in Table 5 (D.3) calculated? Depending on the implementation of Apollo it looks like 3-4 parameter-sized vectors are required per update ($g$,$m$,$B$,$d$) which in the case of 4 is twice the amount of SGD with momentum, yet the memory is only a few % larger.- In algorithm 2 you should replace $\lambda$ with $\gamma$ to be consistent with the rest of the paper. After the author response:Although the authors conducted additional experiments, they have only partially addressed the first comment regarding the use of LSTM instead of cWGAN-GEP by showing LSTM's comparison against cWGAN-GEP in synthetic data generation, when the question was to measure the forecasting performance, not synthetic data generation performance. Therefore increasing the score to 5. ### Post-rebuttal updateI would like to thank the authors for their additions to the paper. I believe that the extra metrics improve the reader's ability to extract conclusions from the experiments significantly.Having said that, I believe that the extra experiments and numbers do not paint a clearer picture. For instance, in IWSLT and WMT indeed there is fairly consistent evidence that FFN Reservoir is more efficient to train than fully trainable transformers. However, for enwik8 the T Reservoir outperforms everything significantly when looking at figure 15 but judging from figure 13 we see that the best case scenario has been selected for T Reservoir (namely 32 or 48 layers). Even more importantly perhaps, the story is completely different when looking at the test set evaluation where FFN Reservoirs perform better and actually the T Reservoir performs the worst among all methods. Similarly on RoBERTa pretraining, fully trainable transformers seem to achieve the lowest validation perplexity and with the highest efficiency.To summarize, I believe that the reservoir transformers could be a useful tool for improving either the efficiency or the generalization of transformers in low-data regimes or both. However, a distillation of the experiments and conclusions is required in order for this to be shown from the paper. Namely, even with all those numbers I still cannot judge which reservoir layer will be better, in what sense it will be better (accuracy or efficiency) and why is it going to be better.Due to the above, I tend to keep my score but because the additions of the authors provide significantly more information (and in my opinion value) to the paper, I will increase my score to 5. ######updateI like the experiment added in the revision. However, it is only tested on a IWSLT which is a smaller dataset and can be influence by many hyper-parameters. It's not clear what frozen layers mean for LayerDrop and it need to be clarified with more details. I didn't find clear comparison with stronger baselines on WMT in the revision.As pointed our by the authors, I think the theory mainly come from previous works. I have also read other reviews. Overall, I would like to keep my rating. ---Update after the discussionsI appreciate the efforts that the authors make in their responses, some of which address my concerns and improve the quality of the paper.I have raised my rating. However, taking into account all information during the discussion phase, I stick to my original review that this paper still needs to explore more to be a mature publication. For example, if the main contribution comes from the prior encoder, as I said the contribution is limited since the usage of the encoder (or generator in the adversarial cases) in the latent space is widely discussed in previous works, such as vampprior, semi-implicit VI, doubly semi-implicit VI, etc. This also seems to make the contribution of the sliced Wasserstein part incremental. Plus, as this paper has several components, their relations need to be discussed in a more clear way. Thus, more ablation studies are needed to help this paper to present its insight in a much more clear way.Thanks again for the efforts that the authors make and I hope my reviews could help them to polish this paper to be a nice publication.  Thank the authors for addressing reviewers' comments extensively. After rebuttal, I agree with the significance of the proposed method in terms of performance improvement in this particular task. However, the technical novelty is still limited. Thus, I increased my rating to 5. ----Update after the discussion stageI appreciate the authors' responses to address my questions in the experiments. However, I agree with the concerns of the other reviewers, especially the redundancy of Theorem 1 raised by Reviewer3. After reading the reviews and the discussion, the authors seem not to provide convincing responses to this part and this raises my concerns for this paper. Thus I change my rating below the borderline. (I updated the score after the discussion.) ##########################################################################After the author responseAs the authors address the reviewer's concerns, I changed the rating. === Post rebuttal ===I would like to thank the authors for the very detailed response and the improvements in the manuscript. I found the ablation experiments and experiment with "No scores" particularly useful. However, I am still a bit confused about better performance of the smaller model. Maybe, more understanding could be gained if there was an additional "tiny" model that would show that when going beyond a certain size, the performance degrade. Additionally, if the overfitting is an important issue, some regularisation methods could be explored. Finally, in the light of many changes in the paper and the original request of all the reviewers to have the writing in the manuscript improved, I think the paper would benefit from another round of reviews. However, I find this method promising and if the paper is not accepted this time, I encourage the authors to re-submit a revised version. ### ---- Update ----The authors' response does not satisfactorily address my concerns. My main concern is that the paper does not properly evaluate alternative choices, even though a large literature on contrastive learning exists.  While in the revision one baseline was added to the appendix, the main experiment still only contains a comparison to Dreamer and ablations. Further, it appears that the proposed method fails completely when the non-markovian part is removed. This is rather concerning since learning markovian latent dynamics is important and also possible with other methods (e.g. PlaNet-RNN). As far as I can tell, the paper does not discuss this issue and does not explain why learning non-markovian dynamics is crucial.  Overall, the paper proposes an interesting method but fails to provide any insight into how the method compares with possible alternatives. I, therefore, maintain my borderline score.  After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I keep my original score. Hope to see a better version of the paper soon. ====UPDATE===My concerns were partly addressed in author's response so I have raised my score to 5. *** Post-Rebuttal ***Some of my concerns are addressed by the authors' rebuttal. The unfair experimental comparison has been fixed. The proposed model indeed has some merits (e.g., parameter sharing and negative sampling). However, to me, the technical novelty of the paper is incremental.  In addition, with additional large transformer networks, the proposed model only achieves limited improvements over previous methods. The authors claim that the proposed model is more effective in handling long videos. But, only results on Charades and KS are shown without extensive comparisons. Thus, I would like to keep my rating unchanged. -----REBUTTALI acknowledge having checked authors' rebuttal and the revised version of the manuscript Updates: Thanks for the authors' response. Some of my queries (1st and 3rd) were clarified. However, unfortunately, I still think more needs to be done to show the superiority of the results. I retain my original decision. ---Comments after the rebuttal:Overall, the attack settings are still questionable: low stealthiness, high poisoning rate, and intervening the training process. - The authors clarified that "Neither our random backdoor technique nor the transferred BaN and c-BaN requires that", however, in 3.1 Threat Model, it is clearly stated "The dynamic backdoor attack is a training time attack, i.e., the adversary is the one who trains the backdoored DNN model. To implement our attack, we assume the adversary controls the training of the target model and has access to the training data following previous works (this is actually not true)", the authors seem misunderstood the different between "controlling the training of the model" to data poisoning.In terms of the difference to static trigger,- The authors argued that they are different to static trigger and "a static trigger cannot be used as a dynamic trigger", which I don't find it is verified somewhere. Actually the random backdoor proposed in this paper proves that a static trigger when applied to a random location still works very well, even as good as BaN and c-BaN. Whether existing triggers can already achieve a dynamic effect or not is unclear, or verified.Without knowing how hard it is to make a static trigger works in a dynamic manner, or where it will completely fail, it is hard to evaluate the contribution and novelty of the proposed attacks. In other words, it seems that a static trigger attached to random locations is good enough to achieve the dynamic purpose, which I don't think is contributive enough.Stealthiness and poisoning rate is another concern, say when you apply the BadNets to random locations, how high the poisoning rate will be to achieve 100% ASR? Higher than 20% or need 100%?Overall, the lack of thorough comparisons of the trigger properties to existing attacks is the major weekness.So my rating will stay the same. ################after rebuttalAfter reading the responds from the author, I keep my score at 5. ---- After reading answers ------I'm still not convinced by the computation of counterfactuals, I still disagree with "flipping" the private attribute, because then the naive solution of removing them from the input of the model would appear perfectly fair. This approach seems to forgot about potential proxies, that are not the private attribute, but that a model can learn on and be biases.Moreover, if I understand correctly, the same computation of counterfactuals is used for the model and the test set. This is unfair as SenSTIR is the only algorithm to use the same kind of counterfactual data than the one used for the evaluation.I think this is a crucial point, I prefer to lower my rating to 5. **Post-rebuttal update:** Thanks to the authors for engaging in the discussion and for the responses. The authors have provided a satisfying argument that with an appropriate choice of hyper parameters, the SQLoss does promote cooperative behaviors whenever all utilities are negative -- this addresses my main concern regarding the validity of the SQLoss objective. I remain skeptical of the value of the visual Coin game, because the results do not disentangle the usefulness of the cooperation objective and the clustering from GameDistill. I (and it seems, R1 and R3) had several concerns about the presentation of the material: mine in particular about lack of clarity, statements provided without motivation, and lack of details in Section 3, both for SQLoss and GameDistill. The authors have provided clarifications to some of these concerns in the response, but the new revision of the manuscript does not seem to reflect any of these changes. I would not be strongly opposed to acceptance, conditional on the visual coin game ablations and clarifications being added in the final version. Nevertheless, it is hard for me to recommend acceptance, given the number of unseen changes that still need to be made to the paper.   --------------------------------------------------------------------------------------------------------------------------------------------------I want to thank the authors for the revision and the rebuttal. Even though the proposed method is promising for meta-learning with noisy gradients, the setup is not backed up with strong arguments/facts. Moreover, I do not fully agree to the statements in the response. For instance, these arguments are not fully observable neither by experiments nor theories:1. " Specifically, the meta-learner is prone to meta-overfit, as there are only a few available samples with sampling noise, even data is clean.". There is no proof that the meta-learner is prone to meta-overfit. The 'meta-overfit' term is not defined well. What I understand is that the learned meta-parameter is prone to overfitting in this context.2. "Due to the small amounts of samples, FSL is more easily affected by data noise, especially considering that human annotators are likely to make mistakes as training meta-learner requires a large number of classes. Besides, we are not the first to propose noisy FSL, [10] propose hybrid attention-based prototypical networks for the noisy few-shot relation classification task, as human annotators are also likely to make mistakes in language tasks.". A common sense is that a few data is more managable and less vulnerable to mislabeling.3. "Our noisy labels experiments aim to verify the robustness of Eigen-Reptile to noisy labels. Denoise is not the focus of our research. At the same time, ISPL is not a denoise algorithm in the traditional sense.". The statement is vague and not aligned to the introduction, abstract, and title of this work.The theoretical part is mostly trivial as mentioned by R2, and the contribution (Algorithm) is hidden in Appendices. I acknowledge the author's response by increasing the score but I think the paper needs to be further revised.I suggest to build strong background of this topic (meta-learning + noise/label noise) and relevant experiments to show the effectiveness of the proposed approach. Furthermore, some theoretical analysis about the limit to the noise can be verified for the future direction. For experiments with noisy gradients, I suggest to compare with the baseline on meta-learning + noise, e.g. the work by Simon et al. [2]. If the focus was noisy labels then it would be better to compare with other methods for noisy labels (not necessarily in the meta-learning setup). --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, it is still felt that this work needs to better clarify some key issues and strengthen both theoretical and experimental study. In light of these, the rating is maintained as follows.  ######################################Update after reading other reviews and author response:I have decided to lower my score from 6 to 5, as I agree with Reviewer 3 that more experimental analysis of the method is needed (ablations, sensitivities, etc.) given that the theoretical backing is not convincing. The authors also did not directly answer our questions. After discussions:I read the author's response and other reviews. The authors made a considerable effort to address my concerns. As a result, the paper has improved and I increased my score. Having said that, I am still not sure that the paper is ready for publication in its current form. My main concern is still the accessibility/readability of the results in this paper, which I think can be further improved for the benefit of both the community and the authors (more accessible paper => more imapct). To conclude, the results in this paper are interesting and are strong enough to warrant a publication, but the paper can really benefit from another revision.  ----post-rebuttal update I thank the authors for the responses. While I still think the idea is potentially interesting and original, I could not increase the score given the fact that this manuscript is naturally incremental without theoretical justifications. Post Rebuttal:Thanks to the authors for responding. I'm still not sure if the experiments are particularly compelling. There appear to be differences amongst the baselines with regards to class balancing, and the motivating section is still weak; there are new experiments on a larger dataset, but now with a different loss (simplified EL) which is close enough to the proposed loss that this does not work very well as a motivation anymore. Apart from this, taking some of the comments from the other reviewers and the authors' responses into account, I am retaining my initial rating. --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows.  ----------------Following the authors' response,  we have updated our rating accordingly considering the following facts: the authors didn't make any change to the paper within the rebuttal, while they had the possibility in response to several questions. They didn't address even our simplest concerns, about the title being inappropriate. They still want to keep the title too general, while the paper considers only graph neural tangent kernel. Even if the latter is equivalent to infinitely wide multi-layer GNN, it is still a very special case. Moreover, many of the major issues raised by the other reviews were not addressed. ------------- Update after reading authors response -------------I thank the authors for their detailed responses, they have answered most of my concerns and I raise my score to 5. I am still not convinced about the method covering both the aleatoric and epistemic uncertainties, without any theoretical or intuitive justification, and without any discussion/clarification on that part. If indeed this is the case, then additional experiments should be included, for example for a regression task, the standard UCI datasets [1].[1] Hernandez-Lobato, J M and Adams, R P. Probabilistic backpropagation for scalable learning of bayesian neural networks. In ICML-15, 2015. Thanks for the feedback. However, I think most of the performance gain came from behavior regularization, which is an already existing technique, rather than the proposed metric learning method (Figure3 and Figure 5). Ill keep my ratings unchanged.============ Thanks for updating the paper in light of my earlier comments.After a long discussion with other reviewers and ACs, we concluded that the paper would require another complete review process in view of newly added proofs, which were unfortunately missing in the first round. My lowered score signals this to the ACs.I would like to highlight that I became very upset to find out that many important details were skipped and left as exercise to readers. While this would make sense for some repetitive details within a paper, it may not apply to non-trivial proof details, such as the generalization from 2 to arbitrary S or similar. UPDATE: I would like to thank the authors for their rebuttal. I have read it, however unfortunately, I am not convinced the indicated differences from previous work is sufficient to warrant publication at ICLR. I am also not completely clear about the equivariance point.  === after author response ===I would like to thank the authors for their detailed response. My major concern is the strong assumption in this paper - that the function to be approximated is a fixed-depth GNN. This assumption makes the problem less relevant to the actual size of the graph, and avoids a major challenge in size-generalization - that larger graphs are expected to require deeper operations. As pointed out in the author's response, some efforts are made to **experimentally** (1) demonstrate the effectiveness of the proposed method on a real dataset which might not be solvable by constant depth GNN, and (2) demonstrate in real datasets there is a discrepancy between the degrees of small and large graphs. I would be happy to raise my score from 4 to 5 given these experiments. However, the theory part could still be further improved so I do not further raise the score. Update: Thanks for fixing the statement of the assumptions such that Theorem 1 can hold.   Update 4 -> 5. Some of my concerns are still not addressed in the revised version. ---------------Post-rebuttal- Figure 1 is helpful in understanding the model. Thank you for adding that.- The additional experiments are also appreciated. This seems to indicate that it's not just the architecture but the learning rules that make the BCPNN model work well.- It would also be helpful to visualize the features learned by softmax units in RBMs and AEs and see if this results in a similar pattern of HCs encoding broad regions and MCs encoding variations within those regions.- It seems that the RBMs and AEs were not trained using any sparsity penalty. For example, Page 11 of https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf and https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf. Having a target sparsity can have a significant impact on the learned features. Higher sparsity makes the features look more like stroke like and localized, and less spread out all over the visual field (as they do in Fig 4, C and D).  - Based on the additional experiments, I will be increasing my score to 5. However, given that the main contribution of the paper is a comparative study, the paper can add value by doing a more thorough comparison against variants of AEs and RBMs that have otherwise similar properties (such as keeping the HC-MC (softmax) architecture and sparsity levels the same). --------------------------------------------------------------------------------------------------UPDATEI read the other reviews and the author's responses. Thanks for replying to my questions, that made some things clearer!Some final comments: - "We performed a hyper-parameter search to find the best settings of the steps to take on $\phi$ and $\theta$ for each of the environments. To ensure that we dont over-fit the model when adapting for extrapolation, we use 80% of the available test-data to train the model, and use the rest for validation." - If I understand this correctly you tune the number of gradient steps on $\phi$/$\theta$ on part of the out-of-distribution adaptation data, and evaluate on the rest. That feels somewhat restrictive to me and might not always be possible - in which case your results are a bit optimistic. In the response to R1 you wrote that "the knowledge of in-distribution or out-of-distribution is not necessary to apply our method", but in this case the question is how to determine how to decide how often to adapt $\phi$, then $\theta$, before training the policy. At the very least, this should be discussed in more detail in the main part of the paper, given that this is central to your method. - Thanks for pointing me to the appendix of the paper that has the end-performance of your baselines. However I do think these should be included in your paper in order for the reader to get the full picture. There might be differences in implementation (as an example, some papers use different horizon lengths in MuJoCo) so it's not always possible to compare numbers across papers, and your paper should be self-sufficient. - Not meta-learning the initialisation of $\phi$ (but set it to zero before every inner-loop update) seems like an important detail to me - this means in Eq (2) you don't actually take the gradient w.r.t. $\phi$. It might also be an important to stabilise policy learning (because $\phi$ doesn't change too much over the course of meta-training). Even if it is an implementation detail, it should at least be mentioned in the appendix so that somebody who wants to re-implement your method can reproduce results!- Using something like [2] for good pre-adaptation exploration is a good idea, I agree! (And I agree it's out of scope, but might be worth mentioning in the paper.)I think overall the idea is promising but the paper falls short in terms of how the method is evaluated. I feel like too many things are buried in the appendix, and it remains unclear to me if MIER can, under realistic circumstances, really adapt to OoD tasks. I agree with R2 that Meta-World might be a suitable benchmark to evaluate MIER on, since  the training and tests tasks are distinct and there's a clear evaluation protocol for ML10 and ML45 (you have to adapt within 10 episodes) which would make it easier to compare to existing methods. Given the above I stand by my initial rating. ------- Update after rebuttal ---------Thanks for your response. Even though your responses clarified some of my comments, I still don't understand how Experience Relabeling can help with OOD and why your method doesn't good enough with in-dist data. As as result, I stay with my current evaluation and score. **Update**: Other reviewers have pointed out issues with this paper's ablation study. Additionally, it is difficult to trust the empirical results because they are based on only three runs. In light of these criticisms, I have updated my score from a 7 to a 5. I still think this idea is neat and am generally a proponent of introducing audio into work on RL, but the experiments as presented in this submission do not currently paint a complete picture. After Response:After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR. Dear Authors,the new version was improved greatly. Many mistakes have been corrected and most of the raised issues have been addressed. The derivation in App. B is very helpful, it wasn't clear before that you were doing a Taylor approximation and hence get twice the derivative. The analysis of the regularization hyper-parameter is very useful as well. I will adjust my rating. I do still have few issues:In Eq. (3), shouldn't the first term be E^{\beta}(s+, x, y) incl the loss for the target, rather than E(s+, x)? Then it would also be clear why beta can be negative.You still use f instead of roh in a few places, e.g. after Eq. (12) and in App. B.In App. B you also use both L and l. In App. B, better write g(\delta w) rather than g(0) in the line where you have the lim_{\delta w \rightarrow 0}.     xxxxxxxxxxxxxxIt seems that the authors provided a generic response to all the reviewers and I am not sure if they acknowledge the lack of clarity and lot of hand-wavy explanations in the paper. This issue has been raised by other reviewers too and is quite critical for becoming a good paper worthy for ICLR. Therefore, I am unable to update my score for this paper. However, I do appreciate the comparison with Moosavi-Dezfooli et al. (CVPR'17), this is a good addition as suggested by another reviewer. after rebuttal:The authors still did not address my concern about testing on only one task with only one evaluation metric. UPDATE:Thanks for your response. As you mentioned, methods like [1] and [2] do perform open-ended recombination. Note that these methods perform not only texture transfer but also color transfer, while the proposed method seems to perform mostly only color transfer. As shown in Figure 6, essentially what the method does is transfer the color of the style image to the content image, sometimes with a little tweak, making the image distorted. One could say that in terms of image style transfer, the proposed method actually underperforms [1] and [2]. Hence I agree with R2 that comparison is still necessary for the submission to be more convincing and complete.------------------------------ [UPDATE]: Given the discussion with the authors, I agree that the paper outlines a potentially interesting research direction. As such, I have increased my score from 3 to 5 (and updated the review title). I still do not find the contribution of the paper significant enough to cross the ICLR bar. Update: I maintain my scores after the rebuttal discussion. After rebuttal, I adapted the score. See below for original review.-------------------------------------------- Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences. Revision:Thank you for your response. &gt; In fact, our estimator in the theoretical and experimental analysis employs a continuous (ReLU) network. Though discontinuous networks are necessary for our setting (Lemma 2), we show that (continuous) ReLU networks can approximate the discontinuous network effectively (Lemma 3), hence the effectiveness of GANs is proved (Theorem 1).-That clarifies things, however I find that the discussion after lemma 2 rather missleading, if in the end the result ends up using continuous generator:"Because of the discontinuity, generative models with smooth functions, such as anadversarial generative model with kernel generators (Sinn &amp; Rawat, 2018), cannot work well withdisconnected supports."  - It is still unclear to me how the optimal value of S_f is obtained from eq (5). The author points out the work by Zhang+ (2018), but this should be clarified in the current version of the paper: What result in Zhang+(2018) do you use to get this value?- I find the experiments  not very convincing. I understand that the point is not to show that GANs are better than  other methods but it is important to be make meaningful compairisons (use comparable scores) otherwise there is little scientific value in figure 5 especially.  - As reviewer 1 mentions, lemma 3 is supposed to be one of  the main theoretical contributions of the paper, however, the proof seems very similar to the one in ([2], appendix B.1). Although the authors mention lemma 1 of [2] in the proof of lemma 3, it seems like the whole section in ([2] appendix B.1) is dedicated to show the very same result.For all these reasons I still wouldn't recommend accepting this paper.  ----------------Post discussion comment:I have decided not to update my score. While the theoretical analysis is interesting, I am not fully convinced about the utility of the proposed algorithm. It would be useful to have more experiments in the widely used benchmark such as atari or have better motivation explaining why faster convergence would help in the RL context as it would in supervised learning. *****post rebuttal updates*****I want to thank the authors for responding to my questions. The additional explanations are indeed helpful for clarifying my first two questions (selection of the binary kernel and the use of E). However, I still have concerns about Table 1 (and Table 2). For example, I have a really hard time interpreting the significance of achieving a 3.2x CR with a loss of 3% (92.3 - 89.2 from VGG-7) in acc with the proposed method (although the paper argues that it's a "bearable" loss). Considering that this is the main experiment supporting the efficacy of the proposed quantization algorithm, I think the paper needs more controlled experiments to demonstrate the practical usefulness of the proposed algorithm. As a result I'm keeping my original score and hope the authors can work on the improvements for the next version.  edit: I have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers)  -------------------------------------------------After reading the authors' rebuttals, they have addressed part of my concerns but I still think the current form is not below the acceptance threshold due to its weak experimental results and unclear technical details. ****************[UPDATE]I would like to thank the authors for implementing the changes and adding a plot comparing their algorithm with dropout. While the quality of the paper has improved, I think that the connection between the perturbation level and the Hessian is quite obvious. While it is a contribution to make this connection rigorous, I believe that it is not enough for a publication. Therefore, I recommend a rejection and I hope that the authors will either extend their theoretical or empirical analysis before resubmitting to other venues. UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions. The novelty and experiments are somewhat limited. Thus I am lowering my score.-----------------------------------------------------------------------------------------------------------------------------------------------------------------------  %% AFTER REBUTTAL %%  Thank you for all the updates.I would like to thank the authors for their humility in the rebuttal and for clarifying the paper's contributions. Accordingly, I will increase my score. However, I still believe Section 3.1's contribution, and the follow-up of using this to improve classifier robustness, is useful only for a very specific type of data and it is hard to assess its value from a practical point of view. The fact that the authors were able to showcase that such counterfactual data augmentation improves classification is, although expected, useful in itself. However, performance improvement is only evident in colored MNIST, relative to GAN augmentation. Furthermore, R4 points out the important issue that the relevant causal feature is assumed to be known in the experiments. This information is normally not available and must be inferred by the classifier. The additional experiments provided by the authors during the rebuttal are welcome but they should be in the main paper rather than the appendix since this is the main setting where spurious correlations create problems. I believe the experimental section should put more weight on this setting.In light of all this, I will provide a borderline score leaning towards rejection. I encourage the authors to expand section 3 to settings that do not restrict the images to have one foreground object and a single background.   =================================================================================================I've read the rebuttal. I updated my score but still not vote for accept. This paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments. ####Revision:The rebuttal does little to clarify open questions:1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.3. The authors argue in their rebuttal that "the grid" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm. This seems contradictory. Edit: Based on the rebuttal I've changed my rating from 4 to 5. Post-rebuttal------------------I have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.========================================= ----Edit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication. COMMENTS RELATED TO REVISION:The new analysis that has been added takes a step towards getting at the relationship to invariance. This is a positive. In general comments, the authors state as important contributions: "1) to the debate on the harmfulness/importance of selectively activated units in DNNs [1-3] by presenting concrete examples where selectivity is important for generalization, and 2) to the neuroscience community, where, although orientation selectivity has been extensively analyzed for these 60 years since [4], its functional importance in a natural environment has remained unanswered."On point 2, while this provides an example of an artificial network where orientation plays an important role, it's a stretch to generalize this to conclusions concerning functional importance in neuroscience.One point 1, I agree that this paper takes steps in the right direction, but ultimately the overall conclusions still feel as though they are a natural and implied consequence of limiting orientation selectivity. It is noted that  orientation selectivity is "not just a superficial byproduct of object recognition, but is causally indispensable for object recognition". The idea that selectivity for oriented edges is indispensible for object recognition again is a conclusion that feels as though it would be shocking if this were not true. The notion of it being a superficial byproduct of object recognition presupposes that the purpose of the system is to recognize objects. Again, it would be surprising if there were vestigial features in the network that are learned, but play no important role - especially among early layers. I think the paper might be strengthened by re-working this second point to more strongly establish the causality the authors claim.  ##########################################################################Thanks for the response to my feedback. Unfortunately, after reading the reviews/responses from the other reviewers, I have decreased my rating to 5, due to some concerns related the technical aspects of the paper After response: Thanks for the clarifications. The proposed method is a modification of existing averaging schemes to schedule learning rate, but more experimental evaluations are required to determine the benefits of algorithm.  === After rebuttal ===I am not convinced that the improved performance is because of the adversarial training. I trained a simple MLP and with the right amount of regularization it gets 42.0% f1 score on Bibtex, so I am not sure that the adversarial training is very essential here.  === after rebuttal ===The authors explain some of their model choices in the rebuttal, but I am still not convinced about the difference with Gygli et al. 2017 is significant enough. === after rebuttal ===I appreciate the response, but I still think further analysis of the model is needed to understand where the gains in performance are coming from. The claim is that this is due to the adversarial loss used, but without further ablations I feel this is too strong a claim to be making given the current evidence. ===After rebuttal:I would like to thank the authors for the response and updating the draft. They have addressed 1) the title issue and 2) adding domain adaptation baselines. Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines. However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty. The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion). ==== Post Rebuttal ===Thanks the authors for the response. I still have concerns about this work. Please refer to my comments "Reply to the rebuttal". Therefore, I keep my score as 5. ====11/26At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance.12/7After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed. Updated after reading author revisions:I appreciate the clarifications, the response answered almost all of my small technical questions.  That plus the new error analysis increases my opinion about the paper, and I'm no longer concerned that the rule templates are hand-generated given their generality and small number.  I am still concerned that we don't actually know how well the methods work, because the test sets are small and the performance differences between the methods (in Table 1) are quite close.  I will raise my score one point.The authors might try to evaluate using k-fold cross-validation with the training set, to obtain more examples for evaluation. After Rebuttal:I read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3. Update: I thank the authors for providing additional experiments on this part. I have read the rebuttal.The discussion was interesting, but I do not see a need to change my assessment.The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this.I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner.  ================ after rebuttal ====================I appreciate the authors' response and slightly raise the score. It is a good rebuttal and it has clarified several things. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. The explanation reminds me of the mixup data augmentation paper from last year. I also like the additional experiments which clearly show the benefits of GAN data augmentation. However, I still think it is borderline for several reasons.1. As the other reviewer has pointed out, CIFAR-10 is a bit too toy and some models (like LeNet for Figure 2) cannot really show the advantage of the method. I would suggest try ImageNet, and use more recent networks for ablation study.  2. As the other reviewer has pointed out, the compression ratio can be impractical. The compression ratio  depends on student-teacher training, which can take a relatively long time. 3. I would suggest the following experiments that may strengthen the paper. I would consider these as a plus, not necessarily related to my current evaluation. i) Try not use GAN, but use mixup (linear interpolation of samples) as data augmentation, and go through the student-teacher training.ii) Try evaluate the effect of generator structure for data augmentation. Does the generator have to be very strong? The GAN generated results did not improve supervised learning may suggest the generator is not necessarily to be strong. ====== Comments after author response ====Thanks for the detailed response. Having read the update and the other reviews, I have lowered my score.  I am still left unconvinced on (at least) two aspects.--> It is unclear to me as to how a method can improve upon minibatch-SGD can actually have better generalization. While nothing theoretically rules this out, it possible means that that either (A) the model architecture was sub-optimal and there is room for improvement, or (B) the optimization found a minimum that is not very good. In either case, there appears to be a different "lesson"  than the one described in the paper. This is certainly worth exploring in more detail.--> I agree with some of the other reviewers that the experimental results can be improved, specifically, the usage of just 2 replicas (now clarified in the paper) is limiting, and a more detailed analysis regarding how to tune the regularization parameters.I do think that the paper is on track towards an interesting discovery, but I would like to see a deeper/more detailed analysis to be convinced.   Summary After Discussion Period:After corresponding to the authors and reading other reviews, my assessment hasn't changed much, which is that the paper is a good line of research but still needs improvement readability and strictness of assumptions.The authors and reviewers all point out that this work is a relaxation over some previous works, e.g. Jin et al. Yet [1] has assumptions which are relaxed further than in this paper, and show that at regret bounds are possible with weaker assumptions.The author's correctly point out their algorithm is computational efficiency while [1]'s algorithm isn't, which is a point in favor of the author's algorithm. Unfortunately, the benefits in computational efficiency were not clear to me and none of the other reviewers highlighted computational efficiency as one of the algorithm's strengths. If indeed one of the author's algorithm's main advantage over other work wasn't clear to the reviewers, then the paper still has room for improvement in readability. Post-rebuttal:I appreciate the additional ablation study, but unfortunately the results did not strengthen the paper's distinction from related work. The explanation of the motives and related work comparisons only clarified differences between this paper and prior work that are either inherent to the task being addressed, or contribution unsupported by experiments. Unless the AC agrees with the authors that the paper is acceptable even without external comparisons (despite being a merge of two lines of work), I will not be changing my score. =================== Post Rebuttal ======================================I would like to thank the authors for the feedback.I realize that the diffusion kernel used in this paper is different from the AR filter in the CVPR 2019 paper, but I was misled to think they are the same because of a wrong claim made by the authors. In their paper, right below Eq. (11) (of the latest version), the authors state that the limiting case of the diffusion kernel is the solution of the classical Laplacian regularization problem, which is not true. The solution of this problem is exactly the AR filter with alpha = 1, and it is not equivalent to the limiting case of the diffusion kernel. As such, I increased my rating of the paper, but I still think the contribution of the paper is limited due to the following reasons.1.There is not much novelty in using the diffusion kernel as the graph filter. The diffusion kernel has similar response function as the AR filter. If you plot the response functions and observe the functions within [-1, 1] (range of the eigenvalues), you may see they nearly overlap. Yes, the diffusion kernel filter let more high frequency signals pass, but the amount is quite small, and it does not make much difference. The theoretical claims are not insightful either. Claim 1 is obvious, and the conclusion of Claim 2 applies to other polynomial filters such as AR (AR can be expanded into a polynomial form, the CVPR 2019 paper also proposed a fast computation method similar to what used in this paper). 2.The performance of the diffusion kernel is not significantly better than other graph filters. Notice that, it is not fair to directly compare with the results in the CVPR 2019 paper, because those results were obtained without using the validation set (which normally contains 500 labeled examples as in the original GCN paper). Also, the results in the present paper are obtained by running through a wide range of k and selecting the best. Given same conditions, it is quite likely other graph filters can achieve the same performance. Also, some recent methods such as GMNN and GCNII achieved better results than the present paper. I hope the authors dont think they are treated by harsher standards. The novelty of the CVPR 2019 work is not mainly about proposing a new filter, it is about providing insights into/unifying popular semi-supervised learning frameworks such as label propagation and GCN. Especially for GCN, there was no similar analysis on GCN two years ago (an early version of the CVPR 2019 paper is online (https://openreview.net/forum?id=SygjB3AcYX) months before the SGC paper). *Update after discussion*: The authors have addressed most of my concerns, although not always satisfactorily. They made a considerable effort running additional experiments to provide with additional standard deviations of the accuracy in CUB-200, as well as provided results achieved using a ResNet-101. They also clarified some of my concerns regarding the datasets used. In some situations, the benefit of the proposed approach versus already existing methods is not clear, but in others the experimental evaluation shows clears benefits. Given this, and the fact that the paper is well written and motivated, I am increasing my score. ---- Post Discussion ----The discussion with the authors improved my understanding of how the paper fits with recent work. ## After Rebuttal- I thank authors for considering my suggestions. I increase my score to 5. Having a quick look (I am sorry that I didn't have more time) at the new results; most results on structured pruning seem to agree with [1]; with some improvements over baselines when CLR is used when training from scratch. Results on unstructured pruning seems minimal and focuses mostly on one-shot pruning; and furthermore the baseline suggested above (i.e. scaling the entire learning_rate schedule)  is not added to the iterative pruning results. Overall, I like the direction of the paper, but I think the motivation should be improved and results should be distilled. --- After reading the authors' response ---The authors' response addressed some of my concerns. However, I could see that some of the concerns regarding novelty, relation to the prior work and experimental results are shared among several reviewers. Thus, I keep my original score and I believe that the revised manuscript would benefit from another round of reviewing. ----------------------------------------------Following the author's rebuttal I think the paper has benefitted from further experiments and from further clarifications. I would like to thank the authors for carefully considering my feedback and for modifying their paper in the directions I suggested. Ultimately, like I said in my original review, I think this is a very interesting and well-motivated problem, but I still have a few doubts. In particular, the doubt about the paper's use of the term of "conjugate" remains. In their rebuttal the authors use the term approximate-conjugate prior, but I am not sure that this is satisfactory as being conjugate means you have knowledge of the form of the true posterior's closed form, which is not the case for BNNs. I have increased my score to reflect that I think the authors are moving in a promising direction and I hope that they will continue with this work. One thing I will note on the experimental side of things is that having greater variance is indeed interesting, but it may or may not be correlated with increased uncertainty and this may be interesting to investigate in a future version of this work. ---_Updated review:_>The updated manuscript has some substantial improvements.>> I feel the biggest problem is that the authors didn't clearly state the problem settings. If I understand correctly, in their framework the equation is fixed but unknown. The training data are several points in the domain (with parameters input) and testing data are other points. So basically, we doing interpolations. But even the PDE is unknown, they do assume some structure of the PDE, I think.>> Other PDEs frameworks are either 1. solver-type: the equation is known and fixed, they directly solve for the solutions. 2. operator-type, the equations are unknown and changing. Train on inputs-outputs for several equations, and test on others. Their setting is quite different. I guess it's the reason their performance is much better in the updated comparison. On the other hand, it's also hard to evaluate their performance since there are no fair benchmarks.> In general, I feel this paper is novel and concrete, while it's not very complete and well-presented. I agree with other reviewers that this paper is not ready to publish. Post Author response update----------------------------------------Based on the author's response, I will raise my score to 5.  =========== after reading rebuttal ===============I do not agree with the author's response to my first comment. Considering parameterization $X = 2U$ (here I use the notation U to avoid the confusion), then the loss function becomes $L(U) = \frac{1}{2}\\|Y-2U\\| _2^2$ and the gradient flow with respect to $Z$ writes $\dot {U}= -2(2Z-Y)$. Then we have $\dot {X} = 2\dot{U} = -4(2U-Y) = -4(X-Y)$, which gives a rate $O(e^{-4t})$, which is faster than the $O(e^{-t})$ rate achieved without using this parameterization. Therefore, comparing the convergence rate in terms of gradient flow may still not be fair and valid. Based on this issue I would like to keep my score. POST-REVISIONSThanks for the revisions made to the theoretical results. I still find parts of the discussion in Appendix F to be unclear.Firstly, how do you derive eq. (5) from eq. (4)? In eq. (4), the denominators \sum_i q_i are independent of "\Phi(x_i)", but in eq. (5), they have a dependence on \Phi through z_{i,b}. I think the change in normalization important to show the invariance principle holds (as the invariance principle requires a conditioning on each value \Phi takes), but am unable to follow your derivation.Secondly, I'm not convinced that the maximizing partition for eq (5) assigns all examples with y=1 to one group, and those with y=0 to another group. Wouldn't the maximizing partition also depend on what \hat{y} evaluates to for those examples?Overall, I'm able to see what the authors are trying to get at with this example, but unfortunately the revisions aren't sufficient to address all of my concerns regarding the theoretical results. ------------------------------------------------------------Post-rebuttal-----------------------------------------------------------Given the effort of the authors of improving their manuscript, I am improving my original score. However, my evaluation is still "weak reject" for the reasons below:(1) I still fail to see clear differences between "assistance", as defined by the authors, and the other reinforcement learning-like approaches that assume that the reward function is unknown. I can see that they perhaps provide a more organized and methodological description of how that "assistance" can happen when compared to the previous works. However, the paper lacks practical advice, exactly how should I build an agent to leverage such "assistance"? I don't think their ideas are so novel that other methods couldn't be at least adapted to work in their scenario (to include some empirical evaluation in the manuscript).(2) The paper seems a little displaced to me in this conference. The paper neither provides practical and direct guidance on how to build algorithms to leverage "assistance", nor is a survey that focuses on organizing the area and discussing differences between works. Perhaps the paper would be better placed in a "Blue Sky" track. ----------------------------------------------------------- ### Post-rebuttal updateI thank the authors for the clarifications and discussion. However, and admitting that I may have missed something, I remain unconvinced regarding the contributions of the paper. ## Author response updateIn light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.The main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work. The main reason not to increase the score further is that I believe the contribution still is quite narrow.  I chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work. Post author response:I appreciate the authors efforts to clarify my questions and revise their manuscript.I am satisfied with the answers given differentiating this work from Alpha Zero as well as the additional experiments performed. I would contend though that the differences with the other environments I have provided in point 1 of my initial review are not sufficient. The three dimensions given with respect to differences with TextWorld (and hold for the other envs too) are not entirely accurate. There is nothing in the framework itself that focuses on reward maximization instead of next state probability - its the same as having a chess simulator where you can either focus on predicting the next state or just have an external reward the indicates whether or not you've won the game. It is possible to generate oracle traces, etc. equivalents to this chess dataset in most of these frameworks. Overall that is to say, chess can also be framed in exactly those three terms and given that these are frameworks and not agents, you cannot say that these three dimensions hold.This being said, in appreciation of the author's efforts for the other clarifications - I will increase my score to a 5. ***After rebuttal and discussion This paper proposes a new network quantization framework. In particular, the proposed DropBits is somewhat novel. However, it lacks sufficient and accurate analysis of SRQ+DropBits.  For example,  why SRQ can reduce quantization error has not been well motivated and explained.  The definition of distribution bias is still unclear.  I think that an accurate description of terminology is crucial and required for scientific research. Hence, the paper still needs minor polishing for publishing. I would like to decrease my rating to 5.  I will keep my score. The paper has merits, but the comparison is not fair since they have different parameters with the baselines unless they have smaller parameters like ALBERT.  ===post-rebuttal:  The authors have addressed some of my concerns, but the experimental results are still missing several important baselines.   Raising my score from 4 to 5. Post-discussion update: The authors gave a fantastic, thoughtful and exhaustive response that did clarify all of my concerns about the paper. They also updated the paper considerably (making much better), and crucially changed the title to be very accurate the contents.I like the paper now a lot, but the unimpressive results still stand. The PDE-based image classification performs ok, but also sometimes does not work very well. This would still be ok if insightful analysis of why the model improves would be provided. Unfortunately there is almost none of this, and then the contribution is more in the engineering side than science.I would not object acceptance, but I would prefer the work to be more complete in this regard first. I raise my score to 5.---- **Post rebuttal**I appreciate that the authors answer my questions. After reading through the rebuttal and other reviews,  I partly agree with R1's comments and I would like to downgrade my score by 1. My main concern is the novelty of this method. I disagree that the decoupling of generation from upsampling is interesting, it seems more like an engineering problem. Besides, I found that the authors changed the images in Figure 2 in Appendix A, not only changed the order, which is not consistent with their explanation in rebuttal.  ------------**Edit: Score raised from 4 --> 5 following discussion below.** ----Update----After reading all the other reviews and ensuing discussions, I maintain my original score. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. If the question is "How do policies learned by meta-RL algorithms compare to Bayes-optimal policies?" then I think more empiricism is needed (since RL2 in principle can represent the Bayes-optimal policy), or a comparison of multiple methods. ===Post rebuttal===I have read the authors response and would like to thank the authors for the clarifications. I am still inclined to keep my original score. Post-rebuttal:Authors, thank you for your feedback. The additional results around relative speed and performance have strengthened the paper. However, I still feel that the paper still needs significant polishing before final publication (figures, grammar, presentation), and that the paper is better suited for an NLP-focused conference, and so I have not updated my final score. ------------------------After discussion:The authors do not provide rebuttal. Hence, I keep the original opinion to give this paper a weak reject. *********************************After carefully considering the rebuttal from the authors, I am going to maintain the score based on my evaluation and also the current paper draft. Though the authors have tried to addressed the comments, the paper still requires more improvements, including theoretical novelty and experimental results. POST-REBUTTAL UPDATES========Thanks to the authors for the response and the efforts in the updated draft. The updated paper improves writing. The response resolves a part of the queries. The viewer yet believes the page limits should not be a good excuse to squeeze necessary information into the appendix, otherwise, as AR2 suggests, it may be more proper for other venues. I raised my rating according to the author's response. ------------------------------------------------------------------------------------------------------UPDATE:I have read the other reviews and the author's response. Thank you for the thorough answers - this cleared a lot of things up and I understand much better how the multi-head, multi-task and random decision planes work. I'll increase my score to a 5 because I've gotten more insight now with the additional information, and think that the paper raises some interesting points. Overall, I still tend towards rejection - even with the updated version, I still find the contributions of the paper difficult to tease out and evaluate, and not all claims in the paper are sufficiently backed up / analysed by experiments. I would encourage the authors to try and centre the entire paper more clearly around *one single* central message in the future, and present all experiments in this light, making sure that every claim is sufficiently backed up empirically.  ================ Post Rebuttal =============================================================Thank the authors for the updates.In the latest version, the algorithm flow is clearly stated in Figure 1, and now I can understand how the algorithm works. The authors also reported additional results on running time in the latest version, which are informative. Here is what I think after reading the paper again.1.This paper proposes a novel idea. Defining directions on graphs is not a well-addressed problem in current GNN models, and using the gradients of the low-frequency eigenvectors of the Laplacian to define directions seems novel and interesting to me.  2.The insight and analysis are not clear. Section 2.4 is still difficult to follow after the updates. More importantly, I am not sure about the correctness of the theorems and corollaries.        The K-walk distance is supposed to reflect the difficulty of passing information between two nodes, and a larger distance means more difficulty. In the paper the K-walk distance is defined as the average number of times that a K-step random walk from one node to hit another (formal definition given in Page 18), which really puzzles me, because frequent visits indicate ease of message passing. Did the authors confuse hitting probabilities with hitting times?  --- Post rebuttal updateThe authors clarified and removed problems in the paper and did additional experiments. The changes to the paper are quite substantial and I cannot make a full review again.  (Upvote from 4->5) ## Edit on second reviewI apologize again for the tone of my first review, I sincerely tried to understand the paper but I could not when I first read it. A re-read the paper and finally understood it during the review. I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations. With the new equation (1) the paper is hopefully more understandable now. I increase my grade from 3 to 5. The findings are quite interesting but I still believe that the paper is not well written: the equations are interesting but the explanations between the equations are often unclear. One has to understand each equation and be quite imaginative to finally identify the contributions of the paper (even for somebody only "very slightly" off from the research topic). *** Update after rebuttal ***I have read the rebuttal and I deeply appreciate the detailed response by the authors. I especially appreciate the introduction of a number of experiments on a simple domain that help to illustrate the main point of the paper.At the same time, I believe that some serious issues still remain unresolved. For example, my main concern remains: I believe that wrapping the problem in the embodied setting is not introducing additional insights. I must clarify that I fully agree that studying how embodiment affects cognition/behavior is an extremely important and exciting area of research. But in the present paper, the models have no chance to benefit from embodiment (since there is no prior / shared embodied experience), but rather have to solve the task despite being placed in an embodied setting.The main insight (in my opinion) is the observation that Zipfian intent distribution together with energy costs could be a good basis for zero-shot communication. I think that additional experiments that the authors introduced help to strengthen this point, although more experiments could still be beneficial (e.g. systematically varying the population size), as well as a more thorough theoretical discussion. At the same time, the limitations of the main "embodied" experiment remain (most importantly, the fact that fairly high accuracy can be achieved because of the unique "do nothing" action trajectory). In short, a large part of the paper (on embodiment) contributes relatively little in terms of its impact and conclusions that can be drawn from it, which necessarily limits the extent to which the main insightful point can be explored. The main point is truly interesting, however, which makes the paper borderline.Overall, I believe that the paper is extremely promising and I would love to see an expanded version published. I feel very torn about the decision, but at the moment, I believe that the paper is still below the acceptance threshold, although only marginally. I am happy to adjust the score up, and I regret that I can not switch it to an "accept" recommendation.As a minor aside - the newly introduced Colab Notebook does not fully run and crashes at the cell #4 (model loading), so I can not fully explore the newly introduced experiments. That being said, I think that after fixing, this resource can be very useful in the future. This minor issue did not affect my evaluation. Update ==After reading the rebuttal I have left my score unchanged. I appreciate the clarifications, but am very concerned about the result that the pixel-based models perform worse than the identitiy function in the FPA metric. When a model fails a sanity check like that, I believe the causes and consequences need to be thoroughly investigated. In its current form, the paper does not provide that. Rebuttal Update #####I thanks the authors for their responses to my questions. They were very helpful, and I think the work, when explored further, would be a great submission to a future conference. However I do share sentiments with other reviewers about following set of issues.(1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in-depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them.(2) The scope of this paper is a bit narrow where the authors only evaluated the methods on PHYRE that is fully-observable and only contains open-loop tasks with rigid objects of simple shapes in 2D space.###### === Post rebuttalHaving read the rebuttal and the reviews from other reviewers, my rating remains the same (5: Marginally below acceptance threshold). Many of the reviewers share similar concerns:(1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in-depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them.(2) The scope of this paper is a bit narrow where the authors only evaluated the methods on PHYRE that is fully-observable and only contains open-loop tasks with rigid objects of simple shapes in 2D space.(3) The conclusion that the pixel-based model does better than the object-based counterparts may be a bit controversial. The observation is very specific to the methods and environments the authors were using and may not hold when generalizing to more complicated partially-observable or 3D scenarios. _Post-rebuttal_:I do highly appreciate the authors trying to answer all our questions and adding more details and experiments. However, after also reading through [Chiu et al.,  SIGGRAPH 2020] I do find that this paper has a large overlap with the one mentioned. Therefore, agreeing with Reviewer #2, the contribution is reduced to applying the idea to GANs. Therefore I am keeping my recommendation. ================================Post rebuttal Comments: Thank the authors for the response. After reading the rebuttal, I decided to change my rating to 5 for the following reasons:* Theorem 3.3 in the original manuscript has a major flaw which is also mentioned in the review of reviewer 3. The remedied theorem is considered as a substantial change compared to the original one.* Experiments do not fully justify the superiority of the proposed method in training deep GNNs. The performance on ogbn-product degrades when the depth of GNNs goes to 15. It is not clear whether shallow subgraph samplers really help with training "deep" models. * The empirical results are not strong enough to support the theoretical claims. The best results achieved by shaDow-GNN on ogbn-arxiv and ogbn-product are 72.28 and 80.09 which rank 15th and 9th respectively on the OGB leaderboard. The results should be improved before being accepted. [Score updated From 6 to 5] Post response updateThanks for the response. However, my major concern is still that the technical contribution of this paper is limited.----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Post response updateI would like to thank the authors for their detailed response. The response addresses my confusion around the use of the terminology of model inversion (I would further suggest that the author use the term data reconstruction rather than than model inversion to avoid readers misunderstanding model inversion as referring to [1]). I still have concerns around the fact that differential privacy is not used as a baseline here, which would strengthen the argument made in the response that it provides orthogonal guarantees. ----------- ***During rebuttal:The authors should highlight some technical difficulties in the paper. In principle, stochastic line search under overparameterized regime does not make things harder because the stepsize is lower bounded. The difficulty of stochastic stepsize is to control the product of stepsize and gradient, while under this regime the product is separable. It seems the analysis of momentum plus this observation is enough for the analysis. It would be useful to provide some insights and challenges of the analysis. ----Post rebuttal commentsThanks for the responses from the authors. These responses partially solved my questions. I think that the initialization of hyper-parameters of combination weights seem to be heuristic, and it is unclear on the effects/robustness of its initialization on the optimization performance. My questions on more comparisons and more combinations with other optimizers are not well answered. I also read the other reviews and responses, I agree with other reviewers on the concerns of experiments, justifications, robustness, etc. Considering that it needs some additional works to solve all these concerns, I suggest the authors to improve the paper and submit it to one following conference.  Update: I really appreciate the response from the authors. Some of my original concerns have been addressed, and additional experiments help to show the benefits of CAM-HD, so I have increased my score to 5. But, after reading other reviews and responses, I still believe that this work needs to be compared to advanced learning rate adaptation methods. Most reviewers have pointed out the presentation and insufficient experiments, so it's better to submit the improved version to one of upcoming conferences. -----------------------------------------------Post-rebuttal comments:I'd like to thank the authors for adding the experiments; the paper looks stronger now but unfortunately, the results on the new experiments are not that encouraging. Considering the results and also other reviews; I'd like to keep my score as marginally below acceptance threshold.  After reading the author's response and other reviewers' reviews, I still find the novelty of this paper somewhat insufficient. Therefore, I would like to maintain my initial evaluation. --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows.  After reading the rebuttal and the other reviews I still lean towards rejection, the main reason being that I am not convinced at this point that exponential nonlinearities are a direction worth pursuing. I furthermore find the passages on dynamical systems and lie groups to be still lacking, for the same reasons as detailed above. Update:- Thanks the authors for the detailed reply. It addresses some of my concerns. Overall, I am fine if this paper gets accepted, given its novelty. My major concern is still with the experiment comparisons. The authors only compare the proposed method with FUNIT, which might not be fair since FUNIT is designed for few-shot translation, dealing with novel domains or classes with a few examples. The authors should compare with MUNIT or StarGANv2, with domain information obtained by clustering or any unsupervised methods.- The authors should tune down the eye-catching statement of truly unsupervised translation since translating between dish and dog might not make sense, although this is a minor issue. =======================After reading the authors' response:i. As shown in (new) Table 3 in the revised version, the results of using the global normalization are not better than that of using the softmax layer in the self-attention mechanism. Hence the motivation is not enough.ii. To have the faster computation, we have SGC[1], FastGCN[2]. To have powerful GNNs, we have GIN[3]. Inspired by DualGCN, we can build a new combination (e.g., SGC+GIN) together with using the vector concatenation/sum-pooling/LSTM over different layers [5] to further improve the performance and have a faster computation. That's reason why the novelty of LRGA+GNN is weak.[1] Simplifying Graph Convolutional Networks. ICML 2019. [2] Fastgcn: Fast learning with graph convolutional networks via importance sampling. ICML 2018. [3] How Powerful are Graph Neural Networks? ICLR 2019. [5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.I keep my score unchanged. [Edit] I changed my rating from 4 to 5 based on the author responses.=======  === after rebuttal ===I have carefully read the authors' response. I appreciate the explanation. After reading [1] in detail, my conclusion is still that [1] seems to be a stronger framework than the current one and easily extends to the setting with gradient penalty. Compared with Nagarajan and Kolter, the contribution of this paper seems to be minor, although technically involved. I have checked the updated pdf but haven't found the authors' rigorous "more stable" argument. Update after revisions: The authors performed extensive work to address my concerns. This showed that some concerns (RF appearance) were valid, and the authors removed them from the final manuscript. I raised my score accordingly. I have read the rebuttal.Regarding normalization: I think that there are at least two reasonable meanings to the word 'normalziation': in the wider sense is just means mechanism for reducing a global constant (additive normalization) and dividing by a global constant (multiplicative normalization). In this sense the constant parameters can be learnt in any way. In the narrow sense the constants have to be statistics of the data. I agree with the authors that their method is not normalization in sense 2, only in sense 1. Note that keeping the normalization in sense 1 is not trivial (why do we need these normalization operations? at least for the multiplicative ones, the network has the same expressive power without them).  I think the meaning of normalization  should be clearly explained in the claim for 'no  normalization'.Regarding additional mathematical and empirical justifications required: I think such justifications are missing in the current paper version and are not minor or easy to add. I believe the work should be re-judged after re-submission of a version addressing the problems. ---- Update since rebuttal ----I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters. I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement. More experiments based on different transformations that the authors have mentioned would make this a stronger contribution. The use of beta&gt;1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta&gt;1 (and other hyperparameters such as k in Table 1) in isolation. [Update]Thank you for the responses and clarifications. I appreciate the additional experiments in the real world and comparisons with the open-loop policy. Novelty still remains a concern however; in using a planner as an IL expert, it isn't clear what was challenging to adopt this strategy for the grasping problem and qualifies as a significant contribution. Additionally, the experiments to study 'contact-rich' and 'different dynamics' problems is unclear; the experiments don't indicate what aspects of the proposed method address these challenges and are able to do so with vision/depth-only feedback (no tactile); also the evaluation in simulation alone is insufficient to study such scenarios. I have updated my score accordingly. ---------------------------------------post-rebuttalI would like to thank the authors for their efforts to improve the methods and the draft. Part of my concerns was resolved. For clean accuracy, CAT-r did provide a better trade-off. However, it is improved after the submission deadline, it can't be counted into the original contribution in theory. For the concern that the comparison to the baseline presents unfairness as the proposed method was designed for the composite attack with a larger perturbation space, I think the author agrees with my point to some extend. I decided to keep my original score deal to the remaining weakness in the paper.  =============== Post rebuttal comments ===============First of all, I want to thank the authors for answering my questions.The clarifications confirm my previous concern about the limited technical novelty.In addition, they highlight that only three participants were involved to build the real life dataset used in the experiments.In my opinion this is not sufficient to carry out a significant evaluation.For these reasons I keep my original rating. Update:The rebuttal resolves some of my concerns. However, I still think the contribution is incremental. The current version looks too heuristic, more theoretical analysis or inspirations need to be added. Updated to reflect author response:This paper proposes a series of metrics to use with  a collection of generative models to evaluate different approximate inference frameworks. The generative models are designed to be synthetic and not specialized to a particular task. The paper is clearly written and the motivation is very clear.While there has been work like Forestdb to maintain a collection of generative models, I don't believethere has been work to evaluate how they perform on a series of metrics. There would be great utilityin having a less ad-hoc way to evaluate inference algorithms.While the idea is sound, the work still feels a bit incomplete. The only distributions used in the experimental section seem to be Gaussians and Mixture of Gaussians. Many more families of distributions are mentioned in Section 3, and it would have been nice to show some evaluation of them considering the code is already there. In addition to distributions mentioned in Section 3, it would help if there were a few larger dimensional distributions. Often for evaluation now, many papers use a Deep Gaussian model trained to model MNIST digits. I worry that insights drawn fromthe synthetic examples won't transfer when the models are applied to real-world tasks.I would like to see described a wider variety of models, including possibly more models with discrete latent variables as much recent literature is currently exploring.The paper is a bit confusing in how it discusses distributions and models. Distributions form the ground truth we compare different trained models to.  It would been more clear for me if the explanation with supplemented with some notation to describe who will compare draws from the true data distributions to samples from each of the trained generative models. Update:I appreciate the effort put by the authors into improving the paper. The revised draft is much better than the initial one. But I agree to AnonReviewer2 in that the degree to which this paper has to be modified goes beyond what the review process (even at ICLR) assumes. It is wrong to submit a very unfinished paper and then use the review period to polish it and add results. This incurs unnecessary extra load on the review process.The added 2D results are toy-ish and somewhat confusing (I am not sure I understand what the meshgrids are and what do they tell us). Generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation. The paper should have more results on complex datasets, like for instance ImageNet or LSUN or CIFAR or so, and should have comparisons to existing VAE-GAN hybrids, like VAE-GAN. Also, since a lot of the authors motivation seems to come from psychophysics, showing some application to that might be a good way to showcase the value of the method (although this may not go well if submitting to machine learning conferences). I encourage the authors to further strengthen the paper and resubmit to another venue.----- edit (19.11.) ---- updated score to 5 **********After rebuttalThe revised version has a better shape. In particular, I like the analytical experiment (Fig.2), which demonstrates that the proposed scheme can improve the wide dynamic range. Overall, this paper observes that BN with small variance influence quantization and proposes a protocol for training a quantized neural network combining filter pruning. Some issues still prevent it from being accepted. For example, PfQ is proposed to reduce the dynamic range. However, there is even no definition of the dynamic range in the paper which may make readers hard to understand the mechanism of PfQ.  Besides, the author claim that the weight widening the dynamic range in quantization is theoretically analyzed. But the analysis of Eqn. (14-16) is less rigorous. The readers may expect to see how weights in the case of $V_{c}^{L,\tau} \approx 0$ increase the dynamic range according to its definition compared to those weights in the other cases ($V_{c}^{L,\tau} \gg 0$).The paper proposes an effective approach of quantization, which reduces the model size and improves accuracy. I would like to increase my rating to 5. -- Post rebuttalI appreciate the response by the authors and the new experiments. I also read the other reviews and responses. I think the paper has improved in the revised version. However, I'm still concerned about the novelty, which still remains relatively incremental, as also pointed by other reviewers. I update my rating to 5. --------------------------------------------------------------------------------------------------------------------------Post rebuttal:I first want to thank the authors for their response to my concerns. My concerns have been partially addressed, but some concerns still exist. For example, I am not clear about whether the simplified theoretical estimator in Definition 4.1 really captures the training of GNN. To achieve this goal, some simulations or theoretical justifications should be provided. In addition, the analysis fails to motivate some useful and meaningful algorithmic designs for GNN acceleration, which I believe is much more interesting. For these reasons, I tend to keep my score unchanged. However, I  think the authors take a very good start in studying the impact of batch sizes for GNN training, and I encourage the authors to further enhance their papers from both the theoretical and practical perspectives. For example, it would be much better to explore some new designs for accelerating GNN training based on the developed analysis (e.g., batch size adaptation). Theoretically, the authors can study a more practical and general estimator (perhaps beyond two layers). With these improvements, I believe this can be a very good paper to be published in top venues.  === After rebuttal ===Thanks for the response.I believe that Reviewer2's criticism about the similarity to Park et. al isn't sufficiently addressed by the authors. Even if the hierarchical structure is different it's unclear whether this alternative structure is superior to Park et. al. There appears to be no evidence that the latent variables contain more global information relative to VHCR (Park et. al). These claims aren't tested and the results in the paper aren't comparable since the authors don't evaluate on the same datasets as Park et. al.In general, I think the claims of a superior hierarchical structure to models such as the factorized hierarchical VAE paper needed to be tested to show evidence of a more powerful representation for hier-VAE.I will keep my score. Update after the author response:The author addressed some of the concerns raised in the review(Thanks for the detailed response), in particular, the comparison to cuDNN.  I still think the paper is still borderline but the results might be of interest to some of the ICLR audience. The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper. after the rebuttal:After reading the different reviews, the replies of the authors and the updated version, my opinion that the "explanations" are simply intuitions (which is related to AnonReviewer3's concern "Regarding advantages of learning a joint model as opposed to unidirectional mappings") has not been completely addressed by the authors. Fig. 4 does address this concern by illustrating their point experimentally. However, I agree with AnonReviewer3 that the justification remains unclear. After Authors Response =====As developed in my response "On D_{\infty} assumption " to the reviewers, I think that the assumption that D_\infty bounded is a critical issue.That is why I am moving down my grade. ===========  comments after reading response ===========I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected. Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings. Thanks for the rebuttal. But, I am still not very convinced with the proposed results. For CIFAR-100 (0%), you get about 0.2% gain, for ImageNet (0%), you get about 0.2% loss in top-5 accuracy, and for WebVision, you get about 0.3% gain. I am not sure whether you can call these as statistically significant gains. I believe such gain/loss can be obtained with many other tweaks, such as the learning rate scheduling, as the authors have done. I believe extensive testing the proposed method on many real noisy datasets, not the synthetically generated ones, and showing the consistent gains would much strengthen the paper. But, at the current version, the only such result is Table 5, which is, again, not very convincing to me. So, I still keep my rating. =======  --I updated my score after reading other reviews and the authors' response. ------------------------Post rebuttal comments:Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:- I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic.- The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given.- The writing is still confusing. For instance, it is mentioned that "Concretely, for each state s  S, we define T as a teacher that gives an advice T(s)", while that is not true since later it is mentioned that "the teacher give advice based solely on the terminal state". These statements are contradictory, and it is not trivial at all to provide an advice for each state. post rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes. The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper. FINAL UPDATE--------------------Unfortunately, I am not entirely convinced by the additional experiments that we are truly looking into the classifier instead of analyzing the generative model. I believe this to be currently the key issue that, even after the revision, needs to be addressed more thoroughly before it can be accepted for publication.    xxxxxxxxxxxxxxxxxxxI appreciate the rebuttals from the authors, updated my score, but I still believe (just like another reviewer) that this is better suited for a workshop or a conference like SIGGRAPH. ===== After rebuttal ======The authors answered some of my questions but I still think it is a borderline submission. Post-response comment: while I do think the approach is interesting, the utility of it is mostly demonstrated currently through empirical experiments. These experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach. Further, upon highlighting this in my review, the authors disagree and think its empirical validity is rather superior. This leaves me concerned, and after thinking about it further, I do not think this is sufficient for acceptance. Therefore, Im reducing my score to a 5.PS: the characterization of irreducible error as a product of the limitation of a linear model may be inaccurate. After the rebuttal:1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is "using soft instead of hard constraint" for BNN. 2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].--------------------------- Post-rebuttal: I have read the authors responses and I keep my score **********************************************I thank the authors for the detailed response. After reading the rebuttal and other reviewers' comments, I feel that the consensus is the impracticality of the proposed method. I therefore retain my evaluation. Update: The authors have not fixed some of the errors I pointed out in my review. For example, they still refer to the k-dimensional probability simplex and did not address some of the other corrections. Since it is otherwise a promising paper, I suggest to carefully revise and resubmit. I will not however recommend a paper with errors for acceptance. ######Post-rebuttal####The authors have addressed some of my concerns and I have raised my score accordingly  ----------------------------------------------------------------------------------------------------Comments after rebuttal:I would like to thank the authors for their detailed response. Many things were addressed, and I have increased my score to 5 to reflect that the paper is not far from acceptance threshold. I think the main thing missing is a discussion of the effect of the sampling of subgraphs: i.e. showing that PageRank is indeed better than choosing nodes at random, and analysing how the results change when the reduction percentage is varied (between a low value and the maximum value that fits in GPU memory). =================== Post Rebuttal Comments ===================I would like to thank the authors for their insightful rebuttal and clarifications.Most of my concerns have been properly addressed, and I very much appreciate the discussion about PNA. However, since the authors train a "SuperNet", it is not particular clear to me why one even needs to decide for a specific aggregation scheme (in contrast to PNA), e.g., by simply using the softmax function instead of the gumbel softmax formulation.Furthermore, I'm still not that convinced about the transfer learning proposal. In my opinion, a more in-depth analysis is needed (both theoretical and practical) to justify the claims made in the paper. Since GNNs do not even need to be trained in full-batch mode (i.e. via GraphSAGE, Cluster-GCN, GraphSAINT, ...), I'm not sure which benefits the proposed approach brings to the table in comparison to any other scalable methods.Therefore, my rating will stay the same. Post rebuttal Comments:Thank the authors for the detailed response. I keep my rating as 5. Update post-rebuttal:The rebuttal clarified the motivation, but has yet to address the flaws that was associated with the choice of motivation and positioning among related work. Unfortunately that means I will not be changing my score. ---Update:- I think the idea of this paper is very interesting. The authors' responses also address most of my concerns. The remaining issues are: 1) the writing needs to be improved to make the paper easy to read; 2) I am still not so convinced with the experiments. It would be more convincing if the authors can demonstrate this on few-shot image classification to improve SOTA, for which the joint (x,y) distribution might be too big at the image level. On the other hand, I am not so familiar with this topic. If such experiments are quite normal in this area, then I am fine to increase the rating. I have read the author responses but my initial review remains unchanged.  The paper needs to be greatly improved in terms of clarity and the responses from the authors help but I think it is better off being resubmitted to another conference with heavy revisions.  Thanks for the author's detailed response.In terms of the first question, I do appreciate the value of the paper as a nice empirical study of Muzero and other similar MBRL algorithms. Meanwhile, many MBRL algorithms are not like Muzero. For example, value-based planning algorithms don't maintain an explicitly parameterized policy. Therefore the conclusions here may not apply to all cases. Making its conclusions more precisely will not undermine the value of the paper, instead, it provides readers clearer results. I do see in the revised version, the authors changed their language in the discussion about the result. But maybe clearer results themselves are better.My second concern is addressed in the updated version of the paper, with additional experiments. Cool!In terms of the third question, after reading your response, I think there is a very interesting question. When we test planning algorithms, should we give the agent a fixed model and a fixed representation or fixed algorithms learning the model and the representation? After thinking for a while, I can see the advantages and disadvantages of both cases. So I would change my mind and agree with the authors that their choice of testing is valid. But I do hope this choice being mentioned in the paper because people like me would typically consider the other one.I would like to raise my score to 5. ###################################################################Post Discussion Score:After reading the rebuttal from the author and the comments from other reviewers, I am still not clear if the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8). However, the authors have addressed all of my other concerns. I decided to increase my score for this paper from 4 to 5.  =================================================================== updates after rebuttal period =================================1. The authors added more equations, I now understand the methodological contributions required substantial effort.  The VAE/etc. work is outside my area of expertise.2. The authors added some figures showing both real and synthetic data. I've looked through them, and the statistics carefully.  Personally, reading through all of this, I would use PCA+OASIS rather than this new technique.  I understand R^2 is higher for this technique, but looking at the data, without actual confirmation of any of the inferences. There exists data with joint calcium imaging and ephys one can use to calibrate and evaluate methods, those are the best data to use for such purposes.  In the absence of those data, we are guessing.  My guess is that the OASIS+LFADS output looked better than the CaLFADS output. R^2 is a funny metric, especially when we are trying to compare spike trains, because translations utterly break R^2, unless the output is smooth, in which case, it does not.  It looked to me that CaLFADS had a smoother output than OASIS+LFADS, which could make R^2 higher, but is not actually what I typically want when analyzing calcium imaging data. Indeed, if we were ok with smoothing, we could simply operate on the calcium imaging data itself, without worrying about spike train inference.3. I got more clarity on the differences between LFADS and CaLFADS, though again, this is outside my area of expertise.  %%%% after the rebuttal:I would like to thank for the authors for their effort to address my concerns. The manuscript is now improved, and I am raising my score to 5.  After considering other discussions/comments, overall I still think the manuscript is below the threshold.  ------------------------------------------Post rebuttal:The authors has addressed several of my concerns regarding the method's generality and some experiments. While I'm raising my score to 5, I'm still not convinced that the paper proposed a valuable contribution to the community -- comparing RL algorithms in memory or compute footprints instead of the number interactions with the environment is not meaningful, especially when a simulator is in use. There are several much simpler things one can do to tradeoff compute or memory (for example re-render observations on the fly from stored low dimensional states). Thus, I'm voting for a rejection.  ----Update: thanks for the response! I read over the updated draft also but I'm still not sure what insights we learn about the fragility of NLP models under this evaluation paradigm. For that reason I'm still confused as to whether this paradigm is better or worse than the original approaches of Gardner et al / Kaushik et al 2020, and so I'd like to stick to my score. Post Rebuttal Update:Due to the remaining confusion among reviewers about the equations in the manuscript, I maintain my score. -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing some of my questions and concerns.I have now updated the score. **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the clarifications and improvements made to the paper and have increased my score 5.My concerns about the generality of the framework (as also pointed out by Rev1) still hold, however, as an evaluation on non-image data is still missing. I encourage the authors to extend their work further into this direction, but as is, I would keep my recommendation to reject.##### EditUpdated score from 4 to 5 ===========Update: Thank you for your clarifications and updated paper, which addressed several of my concerns. I therefore increased my score by 1.> First, we want to stress that we are not proposing a reference-free evaluation metric for quality estimation. I understand it is not meant as a general quality estimation metric, and my point was more about the *reference-free* aspect. I tried to make a broader point which I think applies to any kind of reference-free metric (whether it is for quality estimation or specifically about hallucination). ---Comments after reading author response and revised paper Thanks for showing more results of how token-level hallucination detection can be useful/effective: e.g., (i) labels per POS tagging in Fig. 5 and (ii) application to low-resource MT in Sec. 6.For (i), I don't think it resolved my question directly: "The proposed metrics treat all the tokens equally, while in reality, tokens such as noun phrases or verb, for example, may have a larger impact on the hallucination issue than prepositions or articles." That is, a hallucinated NN, for example, might be worse than a hallucinated II, rather than asking how the labels are distributed by POS categories. So I still wonder if it makes sense (as a reliable metric) to measure hallucination at the token level (e.g., Table 5 Hal words %) but it remains unanswered.For (ii), it is indeed an interesting plus to the paper, showing that the token-level labels appear to be useful for downstream applications (even if it's not quite meaningful to measure the % of hallucinated words). I would suggest doing more studies on downstream tasks as mentioned in your response to enhance the paper if you are "not proposing a reference-free evaluation metric for quality estimation" (which seems a bit contradictory to "we hope to create a large-scale pretrained evaluationmodel for any datasets or models to be evaluated" in the conclusion section btw).  -------------------------------------------------------------------------------------------------------------------------------Post rebuttal: I have updated my score based on the clarification provided by the authors. My remaining concern is that I still think the baselines considered by the authors is incomplete. In particular, the calibration under distribution shift techniques can still be applied, just using either just a single test image or their set of multiple test images. Admittedly, this approach would probably not perform well for a single image, but in Table 5, it seems like oftentimes multiple images are needed to even beat Ovadia et al. (2019). --- Post Rebuttal ---I read the author feedback.  The typo in Question 1 is fixed and the issue with the edge weights is addressed. However, the proposed method requires model-specific modifications and cannot be applicable to other tasks on graphs, e.g., link prediction. Due to the limitations, I will keep the original rating. ==========Post rebuttal==========I appreciate the authors' effort on answering my questions. Meanwhile, the authors' response does not fully address my concerns. I keep my rating as it is.  ----------- Updates during discussion -----------I have updated my score from a 4 to a 5 based on the authors' response. My justification is below in my comment to the authors. After the rebuttal:I checked comments of other reviewers and response of authors.First, thank you for the detailed response. Since some of my questions and concerns are partially addressed, I improve the overall rating.However, there are still parts which should be improved:- Regarding manifolds: There are some statements which should be clarified and appropriately analyzed in the paper. For instance, it is stated that "The manifold of parameter tensors is a linear space, and so any linear combination of the parameter set with a tangent vector (another tensor) will remain on the manifold. We are changing the metric on the tangent space from the ordinary Euclidean metric to our channel directed (Sobolev, and re-weighted L2) metrics, which effectively changes the lengths of paths on this manifold of parameter tensors, making it a non-trivial Riemannian manifold."-- This statement claims that the "structure" of the manifold of tensors changes by changing the metric on tangent space by changing geodesic or paths on manifolds. However, it is not clear how the geodesic or in general, geometry of the manifold changes by just changing the metric on the tangent space (while, the tangent space can change depending on change of the manifold).  -- Then, it is claimed that this leads to a non-trivial Riemannian manifold (a nonlinear space), while it is also claimed that the manifold of parameter tensors is a linear space (why is it a linear space?). - Regarding experimental results: Thank you for the updated results. However, these results show that the accuracy of the proposed method is pretty close to the baseline. To show superiority of the proposed method, the experiments should be extended using larger benchmark datasets.-- There are Riemannian optimization method that encodes this channel-directed structure, but there are various Riemannian optimization methods which claim to improve accuracy of models in various tasks such as the following:S. Kumar Roy, M. Harandi, R. Hartley and R. Nock, Siamese Networks: The Tale of Two Manifolds. (oral), Int. Conference on Computer Vision (ICCV), Seoul, 2019.Lei Huang, Xianglong Liu, Bo Lang, Admas Wei Yu, Bo Li, Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks, AAAI 2018 (Oral)Therefore, the proposed methods should be compared with these methods in the analyses as well. # Post-rebuttal updateI would like to thank the authors for the detailed feedback. I am now convinced about the statistical significance of the results. Regarding the additional study, while it is true that the combination of the changes, in addition to the softmax, was what made the results improve, the change is quite minor. Also, the biggest change comes from the fact that softmax is removed. The reviewer also finds the explanation of why this happens to be handwavy.Given the concurrent work [Wang et al. 2020] and the incremental nature of the innovation, the reviewer is not sure of the benefits of the current paper to be published at ICLR. **Update**Thank you to the authors for the revisions, and great to know that the experimental results have improved significantly. In the absence of an updated manuscript, it is difficult to update my score appropriately, so I will leave it as it currently is. However, I think the work is promising and that an updated manuscript that incorporates the new experimental results and more carefully teases apart the contributions of the different components (as the authors have started to do in this rebuttal period) would be impactful. Thank you to the authors again for all of their hard work. _____**POST REBUTTAL**The authors have provided some clarifications. I suggest they use them to improve the paper.I'm increasing my score to a 5, thus still not in favor of acceptance. edit after rebuttal:My opinion about the paper has not changed. Although the general idea is interesting, my main concern is that the approach aims at performing defense against a specific attack. The robustness of the approach w.r.t. other attacks (such as L_2 and L_0) needs to be evaluated.==== ### After authors responseAlthough authors have addressed one of my main concerns and some minor ones, I still have doubts regarding the fairness of comparison with the baselines (lack of proper hyper-parameter optimization) and therefore cannot trust the results. All in all, I keep my rating.  Post-rebuttal comments=======================================I appreciate the revisions and additional visualization provided the authors. The revised version of the paper provides clarifications that make the paper easier to follow. While the paper presents an interesting idea, I am not convinced of the effectiveness of the proposed method based on the results provided by the authors. Therefore, my final rating as 5 (same as the initial rating). Comments after rebuttal:-------------Thank the authors for the clarifications. I will raise my score to 5. Theoretical analysis of the proposed method is nice. But I still think the proposed approach was not well justified or motivated. Why is it the best option? And how are other simple baselines for improving both (not standalone) settings?  Post-rebuttal updatesThank the authors for the great efforts in addressing the concerns. The new experiments on two new backbones DenseNet-121 and EfficientNet-B0 show the method can work well with multiple backbones, which is good. However, my other concerns remain unsolved. 1. Combination with AugMix seems necessary to demonstrate state-of-the-art performance and its orthogonality. 2. I still think the generalization from GTA5 -> CityScape should be listed together with CityScape -> GTA5. 3. The running time comparison should take the model's pre-trained time into account in Table 8.4. Regarding the blocks in the ablation study, I remember a ResNet50 for ImageNet has 4 blocks. Table 3 only lists 3 blocks (2,3,4). So the first block is not the first convolution layer. Therefore, I still keep my original rating. =======After author response: I would like to thank the authors for providing the details of each corruption in ImageNet-C dataset. I understand that it might be hard to compare with Volpi et al due to lack of implementation details. However, I still feel that this submission is a bit lack of depth and thus it may not be a good contribution to the ICLR community. I would like to see more theoretical or experimental evidence that can help us get a deeper understanding of this approach. Overall I decided to keep my score. ---- Post rebuttal ---I appreciate the responses from the authors, which partially address the concerns I was having. However, I am still not fully convinced that the proposed method is significant enough. Thus I am increasing my rating from 4 to 5. Thanks to the authors for providing a response to the review comments. ########################## Update after rebuttal ##########################I thank the authors for their detailed response; several concerns have been addressed in the rebuttal. I would encourage the authors to use commonly used practices to improve the robust performance of models in Tables 5 and 6. The use of early-stopping [3] can significantly boost the robust performance, and produce models with better clean accuracy than is presently reported for Adv. Training. I would like to update the score to 5 based on the author's response. Aside from the robust evaluation, the improvement in clean accuracy is of a relatively smaller magnitude given the disproportionate increase in training requirements. Thus, I have not further increased the score.[3] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020, https://arxiv.org/abs/2002.11569  [After rebuttal] I think the authors improved their paper by taking into account the remarks. Given the last results obtained in Table 4, it looks like the structural features are indeed very good features in practice as they allow to boost the performances of a very simple invariant architecture like Deepset. I think the authors should explore how they can combine this approach with the coloring approach to get better GNN. ### EDIT: I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft. Unfortunately, I'm still not very happy with the motivation of attacking the drifting phenomenon on MINE. The main reason for removing the drifting effect is for moving average of history outputs. However, there are various ways for tackling this ( as pointed out in my original reviews, like using a non-drifted mutual information estimator with moving average or plugin some robust density estimators). Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. But actually, the stability of other MI estimators also allows them to avoids having exploding network outputs. Also, in practice, people don't usually run moving average on MI for representation learning. I encourage the author to explore the importance of moving average of MI estimators further.R3 suggests the author take some non-parametric estimators as baselines. But I think it's fine to only compare to some parametric(variational) methods on high dimension setting, where most non-parametric estimators fail. Nonetheless, it's always good to have additional experiments compared to some non-parametric methods in low dimension settings.Overall, I lean toward rejection given current concerns. After reading the authors' response, I would like to thank the authors to clarify some of the my questions. But my main concerns still remain so I plan to keep my marginally below score. The value of the key bound in Lemma 4.1 is unknown and it's not clear about the effect of the described empirical trick. For the experiments, the proposed SPACE algorithm indeed outperforms all considered baselines. But performing better than all baselines in the final performance seem to indicate that the choice of baselines and experiment design do not present a fair comparison. One would expect the final performance of a reasonable baseline algorithm to be similar to SPACE because the idea of the proposed method is to "accelerating learning" not achieving better final performance. The poor final performance of the baselines seems to be due to the inappropriate adaption of existing methods. ---Post rebuttal---Thank you for the detailed response. My main concern was regarding the scalability of the method to larger environments, e.g. w/ visual state space. I agree with the other reviewers regarding limited applicability of the method, and maintain my original score (Weak Reject).--- --------post-rebuttal--------I appreciate that authors have provided rebuttal that addresses many of my questions. I've read the updated paper and other reviewers' comments. The rebuttal has largely addressed my questions. However I am still not convinced by the following. Without a clarification I cannot see the significance of this work. So I'd maintain my rating. I still don't get the motivation why studying MIA in image-to-image translation. I really wanted to solicit a motivational example. The authors merely say "image-to-image translation has yet to be studied in the context of MIA." I'm not convinced by this.A.4 The authors simply draw curves of MIA performance vs. learning epoch. I still don't know where overfitting happens. From the context, it seems that the proposed method will fail if the victim model is not trained to be overfitting. In practice, surely victim models are not overfitting? %%%%%%%%%%%%%%%%%%%%%%%%%%%Added after author response. My enthusiasm for the paper has diminished because it seems to be more of an incremental step over Gao et al 2019 and the authors did not provide additional analytical insight into their new results.  **Update**I have briefly checked the updated paper and the corresponding Proposition 1. While I currently do not find any issues with the counterexample, there are still many tiny issues in the proofs that prevent me from recommending acceptance.- Prop 1 statement, $Q(a, s)$ appears again. It does not type check and not fixed in Def. 3.- Lemma 2 statement, the last quantity, what is small $z$ in the integral? I also don't see why the $\exists$ symbol is there, isn't $Z$ as a random variable already defined, so $p(Z | X)$ is simply the conditional distribution?- The step around (21) and (22) are very unclear. I can see what you are trying to do here, but I think it should be laid out step by step. ------ Update after discussion with authors ---------I would like to thanks the author for their efforts by adding additional experiments, which surely enhances the significance of the proposed approach. Based on these, I increased my score to 5.I have re-checked the final revised version, I think the current version still *requires proper organizations and justifications*. For example, the added experiments still talked about the accuracy, the in-depth analysis seems lacking. I think a substantial revision of the paper in terms of structure, idea presentation, and analysis is still needed. Based on this, my final score is 5.----------------------------------- ## After updating paperAfter reading the updates, I think the authors have considerably improved their paper and I have increased my score.While the core contribution, using MI over VAR, is clear. The evaluation is not strong enough, I still don't understand why adversarial defense is a reasonable way to evaluate the different metrics. Further it's now clear that the author's use the sklearn implementation of MI estimation, which depends on several entropy estimators which have high variance in practice. While the authors comment that it's "negligible" I am not convinced that this is actually easy or reliable.In summary I think the paper is not ready for publication. # UpdateThanks for the rebuttal. I have read the changes the authors did. The updated manuscript is more readable and more self-contained now. I am still of the opinion that the presented method should be put in a broader context (i.e., considering it in broader settings and/or for other methods) or be better analyzed theoretically. This way it will be much more useful for the community. For this reason, I keep the score the same. -----------------------I acknowledge that I read and appreciated the author's response (both parts). The authors' reply mainly answers my questions, especially regarding the difference between applying the proposed techniques to the real and binary setups.I agree with all authors comments but would tend to confirm my overall score for two reasons:the architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contributionthe proposed mixing of real and binary weights may preserve the advantages of fully binary networks but, again, makes less clear the net contribution of the paper from a more theoretical perspectiveHowever, as I recognize that the paper contains significant experimental results, I would be happy to support acceptance if all other reviewers agree on that. =======Post-rebuttal comments=========Thanks for the authors' response. However, my main concerns on the feature extractor and the contributions are still not well addressed. Moreover,(1) The authors agree that a good feature extractor is usually hard to obtain in practice. The feature extractor will significantly limit the applicability of the proposed approach.(2) I found that the authors use ImageNet pretrained models in the experiments. This setting is weird. In my knowledge, the pretrained models are seldom used in the existing federated learning studies. The model may already be good enough before training and thus cannot well show the effectiveness of the algorithms.  ---#### UPDATEI thank the authors for their response. I am still concerned about the paper's novelty and contribution compared to the existing work as the differences seem minor. Also, the comparison with the approach by Camacho et al. (2017a;b) is not sufficient since different values of hyperparameter $\lambda$ may perform better. Given the authors' revisions and the added results, I have increased my score. However, I am still inclined toward rejecting the paper. Update after rebuttal:Thanks for the response! The authors resolve my concerns except for the baselines. I agree that MCTS is the most relevant baseline, but the authors should also include stronger baselines as well. Obviously, MCTS is far from the best performing algorithm on grid world navigation tasks. I encourage the authors to either adding stronger baselines or switch to another task where MCTS is the dominant algorithm. I still think the current evaluations are not adequate to publish in a top-tier conference. Therefore I will keep my score unchanged. *** Post-Rebuttal ***Thank the authors for responding to my concerns in the rebuttal. For the contribution part,  the major technical contribution is the continuous noise schedule. However, it is very obscure in the original paper. As also suggested by R1, the authors should carefully revise the paper to make it more clear. In addition, I found the paper largely borrowed content from Ho et al. (2020). From the framework figure and Algorithms to texts and equations, only limited modifications are made. After reading the paper, my first impression is that the paper just uses a new model on image synthesis to address speech synthesis. The work is not well motivated and largely copying another paper in the method section is not a professional way.  Hope the authors can modify the paper by adding the new clarifications and explanations in the rebuttal to improve the paper. =======================================================================Post AR:Thank you for preparing detailed responses, which helped me to clarify several questions. However, one of my major questions is still unclear.Although 4 out of the 11 KILT tasks are already included in the main paper, most of them are LAMA knowledge probing tasks or zero-shot QA tasks. It is still unclear how much and how robustly KALM can transfer to other downstream tasks with fine-tuning (e.g. Wizard-of-Wikipedia, FEVER, QA with fine-tuning). For instance, CorefBERT paper (which uses a similar idea but as you mentioned it use bidirecitonal attention) shows its transferability on QUOREF, six extractive QA benchmarks, DocRED, FEVER, five coreference resolution benchmarks and GLUE, which can convince me to choose CorefBERT over BERT.Experiments on the paper are promising but not diverse enough to make me choose KALM over GPT2. Thus, I would like to stick to my original score. **Additional comment after rebuttal**Happy to hear that the authors plan to upgrade their draft. Since the submitted paper is not updated, the reviewer keeps the first rating but also looks forward to read a revised version in another conference or journal. Update: I thank the authors for their response and increase my score by 1.  ----**Update after author response**I appreciate the authors efforts in the revision. This has addressed my concerns about the related work and (partly) exaggerated novelty. As a result I have increased the score from a 4 to 5. In my opinion, the distinction between MPC and policy optimization is minor -- policy optimization when given enough iterations can match an optimal planner, and similarly MPC when equipped with a good terminal value function can match a policy learner. The practical differences between MPC and policy learning have also been extensively explored in prior work (e.g. GPS, POLO, AOP etc). Thus, while MBOP provides an interesting case study in the use of MPC for offline RL, there is limited novel insight. Nevertheless, I appreciate the authors for the thoroughness of the case studies, and for updating the paper based on reviewer feedback.--- Post Rebuttal--------------After reading the comments by the other reviewers, I have decided to keep my score at a 5. The authors reply, and the updated manuscript, helped my understanding of the paper. I was considering raising my score, however, the reviewers were nearly unanimous in their confusion regarding the framework or application of CNML. For future iterations of the paper, I suggest that the authors describe CNML, event-based control and their connection more explicitly. If the main contribution is using CNML as the classifier in event-based control, then it would also help to conduct experiments on meta-learning CNML in a supervised learning setting to further elucidate its effectiveness in the reinforcement learning application. I think your paper is very interesting, and I hope that the authors are able to use this feedback to improve their paper. Post-rebuttalAfter reading the rebuttal and other reviewers' comments, my score remained the same. The rebuttal helped to clarify some issues, but it is still not clear to me why the algorithm should work. I agree with other reviewers that a more careful revision of the paper, and a further analysis on the algorithm will be beneficial. ---### Updates after rebuttalI appreciate the authors' efforts in revising the paper into a more rigorous and illustrative way. However, I still feel the current version difficult to follow, and I am not sure whether the conclusions are really correct or not. The conclusions are drawn from assumptions for empirical observations, but the scale of the experiments is not large enough, and the conditions for the assumptions are not specified rigorously. Therefore, I feel quite reluctant in raising my scores.My final suggestions for improvements in the future version:1. Instead of using accuracies on $\mathcal{D}_{shift}$ as a proxy for how much of the models performance can be attributed to its learning predictive robust features, showing the accuracy of the model on the shifted dataset under adversarial attacks would make it more convincing to me. 2. I would prefer the figures to be close to the text that elaborate the figures. For example, Figure 1 is in Page 2, but the details of this complicated figure is not discussed until Sec. 4.2. Post Rebuttal Update:After reading the author's rebuttal as well as discussion with other reviewers, I will maintain my score. I do appreciate the changes the authors made, and believe that they improve the paper (especially the new example with cat and dog features and the associated figure). I would encourage the authors to incorporate the remaining feedback, as it will be helpful as well. -------------------------------------------------------------------------------------------------------------------------------Post rebuttal: I appreciate the improvement in clarity of the paper; however, I think some work remains to improve the clarity of the paper, especially regarding why the two pathways are interesting. I think expanding on potential implications of this mechanism would be very helpful, in addition to the addressing the remaining issues raised by the other reviewers. ----post-rebuttal updateI appreciate the discussions between the authors. I plan to keep my original score, for the reason that, at least in my point of view, the difference of the two methods is subtle and it is not clear whether the subtle difference results in drastic improvement. Thank you for your feedback. I found that the analysis of the method in the revision is informative. However, the comparison with the baselines is still lacking, and the experiments are only performed in simple environments. For these reasons, I keep my rating unchanged after the rebuttal.============= ===== After rebuttal =====I appreciate the fact that the authors took into account our comments and improved their manuscript (+1), however, I think that there is still space for improvement.My main concern, is the fact that we need the model to not be "fully trained" to do some analysis about "what the model learns". Therefore, regarding the data leaf, I am not sure if the trajectory that connects two points is actually moving on a "data manifold/leaf" or simply in the data space. Anyways, the behaviour of the metric away from the given data is kind of arbitrary since extrapolation analysis in neural networks is quite difficult. So the most crucial assumption is that the (learned) metric gives meaningful directions to move in the data space. In the experiments this seems to be the case "roughly", but the result could have been the same simply by the linear interpolation (which I think is missing). Similarly, if we just move linearly in a random direction, probably noise will appear gradually. Post-Rebuttal----------I thank the authors for their response. The additional baseline comparisons do strengthen the paper, and I have increased my score from 4 to 5. I agree with the authors that the mixture of Gaussians policy is substantially weaker than their method, and is a useful baseline experiment to have. However, the added experiments with random sampling are somewhat worrying---the performance improvement of the proposed re-sampling scheme is quiet minor over random resampling. In the future, the authors may want to investigate the random resampling for the systems in figure 14. ======================post-rebuttal:I have read all the comments from other reviewers and replies from the authors. The revised version partially addresses my concerns so I raise the score from 4 to 5. However, my concerns about the motivation of the work still exist, so I am still slightly leaning to reject this paper. UPDATE AFTER AUTHOR RESPONSE:I appreciate the authors' response and for clarifying the contributions. However I still feel that the experiments and datasets are too simple and not realistic. I am increasing my rating to reflect this.     =========    I have updated my score considering the paper has improved its quality after the revision (adding more experiments/baselines, comparison with the literature, etc.).=========   New updates: following the new comments of Reviewer 4, I also briefly check the code in the supplementary material and find it indeed seems to have the consistency problem (i.e., not reconstructing graph edges as mentioned in the paper). Thus, I am also wondering how the authors implement Graph-VAE in the rebuttal phase and whether the improvement of their proposed method over Graph-VAE is really from disentanglement or the differences in the autoencoder. Based on this potentially serious problem, I reinstate my original score and think the paper should be clarified before acceptance. EDIT: After reading the other reviews, the author's responses, and thinking more about the concerns raised, I have increased my score. However, I still recommend rejection because of questions around the hyperparameters used in the experiments.--- #########Edits after the rebuttal#########Thank you for the responses. After reading them and the discussion with other reviewers, I still think the current contribution of this paper is marginal and I keep my score as 5. ----- Post Discussion -----Taking the other reviews and the authors' response into account, I still maintain my score. While I agree that it's good to be thorough in something clear and simple, it can still be done to a point of redundancy, and consequently seem less thorough in the overall picture and claims made. I'm still largely unsure on the choice to only apply the supposedly modular extension to GTD2, and not try it with TD which seemed like a clearer winner (apart from Baird's counterexample). As others suggested, there are additional methods which might be good to compare to, and other evaluation metrics might make more sense for the claims being made. Many of my concerns were largely brushed off as future work, that little got addressed- without having to carry out the experiments, high level comments/current thoughts could be provided regarding how readily the approach can extend to the scenarios suggested, or if there are nuances that need to be worked out, etc. ---After reading the other reviews and the authors' comments, I sill think that the paper promises more than it delivers, even if the paper was extensively rewritten as a consequence of many problems in the original version, so I will keep my score. ----### Post-Response UpdateI don't think the authors have answered my second set of questions. While there are some doubts remaining in the paper, the idea looks fine. Although, I think a new paper with DWT will outperform this approach soon. I do not change my vote at this time. ----- Edit after rebutal ------Thank you for the answers.The authors only answered partly my concerns. I'm still not convinced by their explanation on the NN curves. I don't understand how their performance decreases with the number of iterations, it seems that they weren't well tuned.Anyway I've been convinced by R4 and R1 that a few baseline models are missing. Post-rebuttal:Thanks for the clarifications in the rebuttal. It addresses some of my concerns. However, I am still concerned about the unfair comparisons of baselines using different settings, and the newly added ssl baselines all outform the proposed method on LRS by a large margin. Therefore, I would like to keep my original rating. - - -### After rebuttal and discussion periodI want to state once again that it was a pleasure to iterate on this paper throughout the rebuttal period. It was gratifying to see it improve significantly. The reviewers all agree that this is a very valuable research direction and the authors have begun an extremely interesting line of inquiry. However, there is still a good deal left to be completed prior to this paper being fully suitable for publication. There are some critical areas of improvement still needed in improving the overall clarity of the proposed approach.One piece that stood out to me when re-reading the final submission and the other discussions with the reviewers, it's left unstated with any formal language how the causal interventions are chosen. My concern along these lines are a dressing up of monte-carlo approaches of varying the separate causal factors in more sophistication than is actually present in order to make the paper seem more concrete. Not saying that this is how I read the paper, but there is some space to wonder and be concerned by this. Along these lines, the distributions $q(z|S)$ are never concretely formalized nor is the inference process fully detailed surrounding how one may infer the $z_j^{(i)}$. I could maybe guess that the $z_j^{(i)}$ could perhaps be the cluster assignment but this shouldn't be something left to speculation and guessing... A simple statement of how these distributions are parametrized or even approximated would go a long way.As a final note, I would suggest that the authors revise their conclusion and discussion sections to incorporate the limitations inserted into the Appendix. This follows from the discussion I had with the authors about the general scoping and applicability of the proposed causal curiosity mechanism. It is implicit that the authors have robotics applications in mind yet have written in a more general purpose manner. I believe that adjusting their focus and naming the robotics-directed focus explicitly will help immensely.In the end, I unfortunately cannot recommend this paper for publication in its current "final" form for this conference. I do however really look forward to the complete and fully published form after another series of revisions and refinements by the authors. Best wishes. UPDATE:I have read the author feedback and the other reviews/discussions. I keep my original rating of 5. I think multiple authors raised the question of novelty relative to Barbu et al and the authors argue that they demonstrate the importance of context (whole image vs bounding box vs instance mask) for object recognition. Section 3.3 and Figure 5 is helpful in demonstrating it. However, the experimental setup is very limited (700 train + 300 test). COCO has 110K train and 5K val images and many more objects. If you argue that only 10 categories are common between COCO and ObjectNet, how many are common between LVIS and ObjectNet? I would strongly encourage the authors to leverage these pre-trained models and sharpen their message & contributions. I think they provide empirical justifications (important to the community) for expected results in moving from image to bounding box (same comment from multiple reviewers) but they need to de-emphasize that aspect and emphasize their results on context and robustness. A revision and resubmission to a different conference is encouraged. Author response: the authors answered my question about the absence of learning curves, and provided extra details. However, I still think that the paper could be clearer and more focused, a sentiment that I think I share with the other reviewers. Given my hesitation, I would therefore not vote for accepting this paper, but I acknowledge that the proposed method is original and interesting, so I would not mind if this paper were to be accepted. The author response answered some of my questions. But I cannot say that I now better understand the behaviour of the algorithm and the conditions under which it improves performance. I agree with reviewer 2 that analysing behaviour and performance in a simpler setting would be informative. While the writing has improved, it stills lacks the clarity and nuance one would wish to see in a paper at this conference.   === After rebuttal ===Since the technical contribution of this paper is not significant enough, I will keep my score as weakly reject. -- Update: The authors have done more to acknowledge this limitation. -- ---------------------------------------------------------------------------------------------------------------------After rebuttals:The authors made put a significant effort to improve the submission, but I am still non convinced by the experimental results they are presenting. For instance, in Figure 4 there is no way of distinguish between BSS and Nash-Q. I suggest you to increment the repetitions of the experiment to highlight the improvement of your method over the literature ones. ===== Post rebuttal update =====I like the addition of TD Gammon like baseline and as a result of that I slightly increased my score. However please add more details about that baseline (e.g. did it have a comparable number of parameters to NDMZ?). You can also use performance of just the policy head (as a model free agent) to show what is the contribution of search.Also showing how performance scales with search would still be a valuable experiment. ==== Updates after the response ====I thank the authors for answering the questions in detail. Providing an example application does help readers understand scenarios where the threat model could apply. However, I still think such scenarios are not common but agree that the findings in this paper could be helpful for future security research. I adjusted my rating based on this better understanding.  **Post-Rebuttal**I appreciate the authors' efforts to make this submission much more solid. The investigation of the tail sample memorization phenomenon is insightful and would be beneficial to the community. However, the experimental results on standard benchmarks (e.g. ImageNet) are still not convincing enough. Thus, I will only increase my score to 5 (marginally below acceptance threshold). I definitely encourage the authors to further polish the manuscript and submit to future venues. Post Rebuttal:I do appreciate the efforts and additional experiments and theoretical analysis that the authors made in the rebuttal. While this paper proposed an interesting approach to long-tail recognition, some connections, distinctions, and comparisons with related work and thorough experimental analysis were missing in the original manuscript, as mentioned by other reviewers as well. Some of these concerns were addressed in the rebuttal, but not fully clarified. For example, the new comparison on the more challenging ImageNet-LT dataset (Table 3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission. .------------Updates after rebuttal------------- Thanks the authors for answering my questions. However, I don't think my comments are well addressed. Even though the paper [d] may not provide public available code, the authors could either use results from the paper [d] or implement the proposed attack on the models used by [d] to see the difference *** post-response ***Thanks for the clarifications and revisions in the manuscript.- A2: Lemma 1 (and its M-step repetition) show that the outputs of mirror descent x_{t+1} are guaranteed to be close to the minimizers of f_t. But the loss of x_{t+1} is measured on f_{t+1}; this is what I meant by "bad decisions". Thus I'm confused about the statement "the learner is not expected to make bad decisions"; Lemma 1 does not imply this. It only implies this when the consecutive minimizers are close (which is captured by C_T and S_T being small).- A3: This example makes sense (though in this example, the function is L-Lipschitz, not L-smooth). I also noticed that the example in Remark 5 isn't both smooth and strongly convex in any norm.- A4: Understood; thanks.- A5: I now understand (from the addendum in the paper as well) that this "user control of L_r" is basically referring to the standard selling point of OMD that the user can specify a regularizer that adapts to the geometry of the decision set and loss functions; I buy this point. This is separate from the ability to handle Lipschitz constants being large (which gradient descent can also handle, by having a smaller learning rate).Additionally, I'm confused about the response to R2's "simple strategy". The convergence rates of GD and MD are stated in incompatible ways. The right comparison would be to pick M so that the RHS of Lemma 8 in the appendix to be smaller than \eps. I agree that this work strictly generalizes [Zhang et al. '17], but I don't agree that this work bypasses computational difficulties of approximately minimizing a smooth & strongly convex f_t; as the authors point out themselves in a different reply, when r is quadratic, OMMD reduces to running OMGD.In my opinion, this algorithm and analysis are potentially worthy of publication at a top venue, but the manuscript evidently needs an overhaul in its justification of the setting and G_T. Thus my overall score is unchanged. ___________________________________________________________________________________Update after author feedback:I thank the authors for improving my understanding of the paper. I feel better about it after reading it again. One of the most interesting findings--that a translation network trained on one domain/set of classes generalizes to another--needs more discussion. Using this phenomenon to improve robustness is a good idea, and the MRT/MDA/MAT methods explored in this work are nice choices for this investigation. However, I agree with the other reviewers that it would be nice to include more baselines from other work where appropriate.The results in Table 1 are also very interesting and deserve more discussion--perhaps an analysis of whether a translation network trained on weak corruptions can generalize to create something akin to the ground-truth stronger corruptions, as the results in the table for MRT imply.Overall, I think this paper has some strong experiments and investigates a good idea, but the claims of a "paradigm shift" are overly grandiose, and some of the most interesting experiments could use more analysis. Additionally, the writing clarity and presentation could be improved. My initial understanding of the paper was flawed in some places, so I'll raise my score to 5. After author's rebuttal:"The other paper that the review points to ([4]) addresses a similar setting to our paper, but the approach is completely different. ... Moreover, [4] was published within three months of the ICLR submission deadline, meaning that it is essentially concurrent with our work."Yes, the reviewer concurs that a work published so close to deadline should be treated as concurrent and will raise the score. ------Updates after author response------I thank the authors for the response and it helps clarify some points. However, I am still not unconvinced that the paper is a significant departure from current work on adversarial robustness (see weakness 1 and 2 above). I think it would be much more interesting if the approach could yield robustness against a wider/different class of shifts compared to what it was adversarially trained against. Therefore, I am keeping my score at 5 and will not advocate for acceptance, though I am not completely opposed to it. ### Final Recommendation after Author Response ###I would like the authors for the very active and productive rebuttal period.  Actually, the current version has significantly deviated from the initially submitted version. I have read most of the paper a second time to take all changes into account. The authors considerably improved the paper and have addressed some of my concerns. I increase my score to 5. The reason for not increasing the score to 6 or 7 is mainly that clarity of the paper is lacking. To give two examples: * Presumably because of the many changes during the rebuttal, the organization, structure and the quality of the manuscript is not always sufficient: to name one of several examples, the core Theorem 1 comes without any reference to its proof or any proof sketch in the main paper (there exists a proof in the appendix, finding of which required scrolling through the entire appendix). * As stated in my initial review, since DANN is a central method in the experiments, it should be briefly summarized to make the paper self-contained and also the specific trainingI think the core issue with the submission is that there is simply too much content for one paper: * formalizing 3 threat models * a proof of separation of maximin and minimax * empirical evaluation  on two datasets (MNIST, CIFAR10), three attacks (transfer, two adaptive attacks), three defenses (DANN, AdvS, TTT), and two settings (homogenous, inhomogeneous)Because of the page limit, a lot of details have been moved into the appendix, making the paper difficult to read. Moreover, even taking the appendix into account, details remain unclear such as how DANN was trained. Moreover, I do not find it convincing to move related work to the appendix; relating the current work to other work should be an integral part of the main paper.My recommendation for the authors would be to strengthen the focus of the paper: I think the inhomogeneous setting does not contribute much and could be removed. Also I don't see much value in MNIST and the weak transfer attacks. Moreover, the parts on preliminaries and threat models is too long for a conference paper and could be shortened. I think if focus were improved and the main document became more self-contained, the quality of the work would be considerably improved. For the time being, I see the submission still marginally below the acceptance threshold. Post-rebuttal feedback:Authors, thank you for your feedback. The additional comentary and results around diversity have strengthened the paper. However, my other concerns, as described below, remain, and so I have not increased my score, but I have indicated that if an effort is made to address these remaining issues, I would recommend that the paper be accepted.After reading the reviews and the authors responses, I have several remaining concerns.-First and foremost, the authors have confirmed that the on-policy results that they compare to are using weak baselines to normalize reward (constant, avg. of last 100 steps). Strong context-dependent baselines are known to be crucial to the performance of on-policy methods. The authors attempted to do some experiments with a MIXER variant without a learned baseline (Ranzato et al, 2016) given my concerns, but MIXER without a learned context-dependent baseline is not MIXER! This is serious, as the conclusions stated in the paper cannot be made until the technique is compared properly to on-policy methods (i.e. that at least utilize context dependent baselines... those with learned q functions [e.g. An actor-critic algorithm for sequence prediction, Bahdanau et al, 2016] are often even more effective)-The authors did not tone down their claims, or criticisms of on-policy methods requiring MLE pre-training in the paper, despite the fact they also initialize with and ML model, and interleave ML updates during training! The tone of the paper is clearly in need of revision, as I discussed in my review.-This is not a major concern of mine, but it worth mentioning that the novelty of this paper is actually on the low side. This is a standard application of truncated off-policy learning, and the cited paper out of Bengio's lab (Serban et al, 2017) is in the text domain, and describes essentially the same off-policy approach (although this paper is arguably more clearly presented, and more focused and thorough wrt investigating off-policy variations). In addition, as another reviewer mentioned, the connections to and related work that considers Jenson-Shannon and reverse-KL minimization are strong (and interesting), but they are not discussed/referenced at all.With all that said, for the most part, I actually like the paper. If the language/position of the paper is toned down/updated, and the results are updated to include stronger on-policy baselines (regardless of the outcome), I think that the paper would be above the acceptance threshold. ---Post-rebuttal update I have read other reviews and responses, and I am willing to decrease my score to 5.My initial review was based on assumption that RS (Hen et al., 2020) is significantly different to this paper, but after reading other reviews and responses now I can see that it is indeed questionable. Also, It was somewhat surprising to me that the paper copied wrong values from the RS paper, as pointed out by R5, considering that RS is the closest related work of the paper.Although I still think the proposed method is reasonable and the empirical results are impressive, I agree with other reviewers that the paper should have done more analysis on why the proposed method works to convince the reviewers (and the future readers), and to give more insights on why the combination of losses (and the use of WTA) is a right way to go. **Update: After considering the other reviews and subsequent discussion, I have decided to maintain my score. As covered elsewhere in the discussion, the key shortcoming of the paper in its current form is that it provides little insight into why the proposed method outperforms the highly similar method in Han et al. For instance, the fact that WTA is crucial for the method proposed in the paper but does nothing for Han et al. is rather mysterious. I think it is necessary to provide more insight into these differences before readers can have full confidence in the proposed method.** ---I have read the authors' comments.  The addition of the boundary attack experimentwas an excellent step; however, it underscores a requirement for further analysis tounderstand *why*, apparently, in some cases the additional theoretical bound *fails* toconfer significant robustness.  The suggested "natural robustness" is only sometimespresent. Often clean accuracy is much reduced, so the method is not yet one I would consideruseful yet.  For me, understanding when the method works well (or not so well) wouldbring this work out of the realm of interesting theoretical bounds into one of moregeneral interest. ****************************I've read the new comments from the authors and the new version of the paper. I think that the paper has improved significantly in terms of presentations, coverage of related work. I still see that the contribution is somewhat limited, but I'm updating the score to better account with this new version of the paper. ===================For reasons outlined in my comment below, I updated the score to 5 (for the new heavily updated version). ======= After revision =========I still think this is a very interesting, novel and relevant idea that desires attention. However, on the same time, I agree with the points raised by the other two reviewers which are all well-motivated and relevant concerns. Therefore, I join the view that the paper is not yet ready for publication but I do encourage the authors to improve their work and resubmit to another venue. --- Post Rebuttal ---I had some confusion and concerns about the paper. Most of the confusions were addressed and made me view the paper slightly more favorably. However, my main concern wasn't addressed. In particular, it's unclear if the benefits are because of the "Value Density Estimation" benefits or if it's because of the mix of non-bootstrap updates. Comparing to a method that mixes in more Monte Carlo estimates (e.g. n-step bootstrap) would address this concern. Theoretically, the authors suggest that the reward should be 1/epsilon with epsilon -> 0. However, it seems like this was not done in the experiments, and it's unclear how epsilon should be decayed in practice. Given these concerns, I will maintain my score. ***Post Rebuttal***Thank the authors for the response. My main concern about the generality of the proposed method is not fully addressed. The baseline HER and GAIL can easily be applied in the domains with discrete action space. Also, the base RL approach for discrete action space exists. Experiments will be more convincing if the proposed method can outperform the baselines on various domains. ============Update after discussion with authorsI had two main concerns:- The modification to MAML was unconvincing to me.- The offline meta-RL formulation should include behavior policy as part of the task definition.After a very detailed response from the authors, I am now happy with the response and extra experiments w.r.t. the MAML modification, but I still have concerns about the formulation. In particular, reading the final version I still think the policy giving the behavior data is treated as an after-thought and is instead assumed constant across all tasks. For instance, IMO figure 1 should contain multiple examples of the same "RL task" that are different "offline RL tasks"; i.e. learning to swim using guidance from a 3-year-old and learning to swim using guidance from Michael Phelps. This is one of the key differences, IMO, between meta-RL and offline meta-RL and given that this paper's main contribution is introducing offline meta-RL, I feel it really should be very clear about this point. It may be fine to first introduce the correct general version and then say something like "it may be useful to assume each RL task is given by an expert of roughly the same characteristics", i.e. we can assume behavior policy is constant across tasks. However, right now the original formulation directly borrows from regular meta-RL and I believe that may confuse future papers in offline meta-RL.I've increased my score from 4 to 5 since I'm now less concerned about the MAML improvement, but I cannot recommend acceptance given my concern about the formulation. ## Post rebuttal commentsThe authors largely modified the paper according to the comments, with a lot of additional content. While this is quite beneficial, the paper raised many questions, some of which may need further treatment (for instance, increasing the number of objectives has an effect on the number of Pareto optimal solutions that is is not trivial). Update after Author Rebuttal--------------After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The "hard/soft" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of "borderline rejection" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification.   [Post rebuttal]I would like to thank the authors for their clarifications. However, I am still concerned with the novelty. The absence of provable mixing rate is also a potential weakness. I think a clearer emphasis on the novelty, e.g. current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission. ****Final comments after reading the response and the reviews:Regarding the fairness of the review, success rate is not sufficient to evaluate navigation agents. A random agent can achieve 100% success if it is given enough time. So it is totally fair to ask for a metric (such as SPL) that is a function of both success rate and episode length. I am going to increase the rating to 5 since some of my concerns have been addressed. There are still a number of issues:- The authors did not run the experiments with the termination action. I disagree that this is orthogonal to the focus of the paper. This is not just an additional action. It indicates whether the agent has learned anything or it is just a combination of better obstacle avoidance and luck. The SPL numbers are so low (maximum SPL is 6.19%) so adding the termination action will probably make the method similar to random.- There is a huge gap between success rate and SPL numbers. For instance, success rate is 66.4, while SPL is 5.84 (note that for some reason the SPL numbers are multiplied by 10 in the table). I doubt that the agent has learned anything meaningful in comparison to the baseline. I understand that the task is hard, but this gap is so huge.- A separate policy is trained for each sub-target. This doesn't scale. There should be one policy for all targets. After rebuttal: I'm also still not convinced by the experimental evaluation. For this reason, I slightly downgraded my overall rating. --------------------------------------------------------------------------------Post-rebuttal update: Based on the improvements made during the rebuttal period, I have raised my rating. I believe Fig. 2 now makes the contribution of this paper more clear when compared to the conventional binary network. According to Fig. 2, the proposed method in this paper allows to multiplex and select binary inputs rather than performing XNOR operations, which makes the novelty of this paper rather limited and incremental. Therefore, I still believe this paper stands below the acceptance threshold. --- POST REBUTTAL ---Modified my score after the rebuttal, since (1) I believe the re-purposing achieved by this work can potentially broaden the applicability of flow-based generative model (2) the authors have toned down the abstract and clarified the contributions in the intro, which now better reflects the value of the work. I am still leaning towards rejection at the end given the limited originality of the proposed method and the lack of a more comprehensive discussion of different possible approaches, but as means to the same end. For example, the relaxed inference problem can be solved with an MCMC method. These should all be discussed and compared if the contribution is about repurposing a joint likelihood model using flows. PS. the last line (the references) of the last page might have been a mistake.  ========== Post-Rebuttal ==========Concerns on paper/results clarity still persist. Lowering my rating to 5. --------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:I would like to thank the authors for answering the questions. I believe that an updated version addressing all the concerns in detail will find its place in other future conferences. Original rating is maintained. ## After revisionThank you for answering my question!> Our approach can be directly used to fine-tune pre-trained language models if word level tokenization is used.It should be acknowledged that no multilingual word-based BERT exists as far as I know, partly due to the challenge with large vocabulary space and generalization. The sub-word approach you mentioned is a good proposal, but without evidence, I cannot assess whether it works. As a result, it's still a limitation of *this paper*, and should be clearly discussed in the paper.> The complexity of the proposed method.I am referring to the complexity w.r.t vocabulary size. The softmax normalization contains `nV` items, where `n` is the number of sense clusters and `V` is the vocabulary size. With the current limitation of word-based models, the vocabulary is already larger than sub-word models, the extra `n` factor cannot be disregarded.> We train bilingual language models without sense-level translation loss (and without projection), denoted by Bi-SaELMo-NT, for ablation study.Thank you for the ablation study! However, I cannot assess it as support for the claim as it's not an apple-to-apple comparison, and I cannot evaluate this paper based on future projection. As a result, I have to maintain my rating based on the revision. ======================================================================**Update:**I would like to thank the authors for their response. The lack of adverse impact of the proposed approach on monolingual tasks and the described ablations certainly help strengthen things on the experimental side. However, my fundamental concerns still remain:* I'm unsure why the ELMo baseline is so much worse than that reported in literature.* With respect to the difficulty of training BERT, I can certainly empathize with the authors about the limited resources available in an academic setting. However, given that BERT was a key area of focus of the paper's methods section, showing the experimental results for BERT, even if BERT-tiny; note that BERT-tiny has just 2 layers and 128 hidden units, as opposed to 4 layers and 512 hidden units which this work uses (based on Section 4.1 and Table 7-- which corresponds to BERT-small, refer [here](https://github.com/google-research/bert#bert)), which should help reduce computational burden by quite a bit.On account of this, I maintain my original rating of 5. Edit after Rebuttal/Response Period:----First, an apology to the authors that a dialogue did not occur during the response period; the authors response was prompt, and my (R4) response was not, thus they were not given an opportunity to respond this response to their response to the review (...the number of recurrences may indicate why this was not possible, given limited reviewer time resources).I think the authors misunderstand my questions about the nature of the uncertainty they're capturing:  Is there intrinsic uncertainty in the observed phenomena (e.g. medical images of tissues), are we capturing mixture proportions of deterministic states which have been mixed due to quantization, OR is the uncertainty due to the raters, i.e. found in the labels ONLY due to differences in label generation?  OR, a third case, is this moot because it does not change the outcome?I understand that there is no explicit modelling of raters. However, my concern was that what we are capturing is intrinsically the uncertainty due to raters, even though no actual rater indicator variable was provided. This would be analogous to learning, unsupervised, the writers of the various MNISTs digits. While for MNIST this is surely difficult due to the number of writers (and their anonymity), for medical images we will likely have a limited number of raters. Having the posterior code collapse to a rater indicator appears problematic, not a desirable outcome, and likely if any one rater has correlated outputs across samples (which seems reasonable; some raters may be more or less conservative with their tissue labeling, boundaries, better/more careful at delineating curves etc). What prevents the capture of this signal, or is this the actual variation we intend to capture in the first place?R3 further included this interesting question in their initial review:> In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput [sic]. Can the authors comment on this potential issue?If outputs are correlated i.e. overlapping in the original image domain, should the measured uncertainty be aggregated across codes?I disagree with the characterization of the Gaussian VAE calibration in the "Ranking Probabilistic U-NET" response section. Simply because it does not accurately fit the function (one mode vs. many) does not mean we can't evaluate the learned unimodal beliefs. Yes, it is misspecified. Does this mean the rankings are meaningless? Surely the discrete model is misspecified (there are more possible masks than codes), but the ranking is claimed to be meaningful. The discrete model may have a better fit, and make the argument that it is better specified, but this doesn't mean you _can't_ evaluate the Gaussian VAE.I stand by my initial rating and reasoning, though I note to the AC that, given space, this could make an acceptable poster. It is, in my opinion and in gross summation, an improvement on the Prob. U-Net by way of improving the Gaussian VAE sub-model of the Prob. U-Net to the VQ-VAE. This allows for sampling from a discrete set of codes which hopefully correspond with modes of the generating distribution in the data domain, instead of sampling from a parametric density, which, while continuous, has only one local maximum. =================================EDIT: I confirmed the revisions regarding the notation issues, but there still have confusing parts.* Definitions of norm operator \| \| is unclear.  * L_1 is mentioned below (1), and used other parts (3) or Algorithm 1. Equation (12) in Appendix uses |W1|_1^2, which looks like the l1 norm as well. Use consistent notations.  * Equations (12, 13, 14) uses \|\|_2 or \|\|_1 to specify the type of norm, whereas (5), (6), (7) and other parts after (15) use \|\|. This confuses me. What do you mean by \|\| without subscript?  * \|\| operator taking to symbols is a weird notation for me. Usually, norm is defined for a single vector (or a matrix). For example in (5), I would write \| b - G(z_b) \|, if you want to measure the difference between b and G(z_b).The experimental result is impressive, as the other reviewers mention. I strongly recommend clarifying the notation to better deliver the method. ~~~~~After Rebuttal~~~~~~The rebuttal still cannot justified such a random deep prior well. I keep my rating unchanged.  -----------------------------------------------------------------------The authors have addressed my comments about other deep generative models and hyperparameter sensitivity. However, I still think the paper is more suitable for other venues with readers from the neuroscience community. Hence, I change my rating to 5. -----------------------------------------------After rebuttal: after reading the answers, I got answers to most of my questions. Some parts of the paper are vague that I see that other reviewers had the same questions. Given the amount of change required to address these modifications, I am not sure about the quality of the final work, so I keep my score the same. ==== After reading the revisionThe revised version is indeed more clear about how the teacher network works, and I have tried to understand the later parts of the paper again. The result of the paper really relies on the two assumptions in Theorem 2. Of the two assumptions, the first one seems to be intuitive (and it is OK although exact conditional independence might be slightly strong). The second assumption is very unclear though as it is not an assumption that is purely about the model/teacher network (which are the x and z variables), it also has to do with the learning algorithm/student network (f's and g's). It is much harder to reason about the behavior of an algorithm on a particular model and directly making an assumption about that in some sense hides the problem. The paper mentioned that the condition is true if z is fine-grained, but this is very vague - it is definitely true if z is super fine-grained to satisfy the assumption in Theorem 3, but that is too extreme.Overall I still feel the paper is a bit confusing and it would benefit from having a more concrete example. I like the direction of the work but I can't recommend for recommendation at this stage. * Update:Thanks for you answer and clarification. While the Morph-net appears novel, the authors only report result for image classification task and don't achieve as good performance as standard convolutional baselines. Given the current empirical evaluation, I find hard to assess how significant is the contribution. I would encourage the authors to either compare on a task where dense networks achieve state-of-art performances or extend their approach to 2D inputs. ============= After Reading Response from Authors ====================The reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors argument. The target NRF model, the generator and the sampler are all different.It is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.As for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough. Post rebuttal===========================I thank the authors for their responses and additional experiments. I understand that the focus of the paper is the cross-silo setting, however one of the key questions of an empirical study is to identify the limitations of the proposed approach. Experiments in Tables 8 and 9 still consider a relatively small number of clients and do not provide empirical insights into when the proposed approach begins to degrade. For future revisions, I recommend an empirical exploration that helps the reader to understand the limitations of the proposed method. Edit: I have read the changes and the author responsed, but have decided the keep the score. ### Update after author replies and discussionI have updated the review score after reading the authors' reply and revision of the paper. ----I have read the response and am keeping my score. I agree that the simplicity of the results/model is valuable, but additional theoretical results (even extensions to Theorem 1, with more involved but stronger claims) would greatly improve the paper and make its contributions closer to what is expected of a ICLR submission. Extending the result to consider noise should be straightforward and yield a reasonably simple claim, which can be further verified empirically by adding synthetic noise to the gradients, adding label noise, and/or adopting extremely small batch sizes. As it stands now, the submission is still lacking in terms of contributions. After the rebuttalI thank the authors for the detailed feedback. I am a bit with AnonReviewer3 about the concerns on the descriptive languages of the paper "less nonlinear", "more expressiveness". Moreover, the behaviors of different nonlinearities in the Fig.3 and Fig.17 are related to the specific initialization and batch normalization, which is rather not a global view of the landscape. I would keep the score unchanged. -- post rebuttal---After carefully reading the authors response, I still think the assumptions are not well motivated. For example, 'On a clean bounding box, even a less well-trained classifier can produce a lower-entropy (high-confidence) prediction with low background scores. Furthermore, it is less likely that two different classifiers both predict the same wrong class with high scores.'. Both of these assumptions seems very strong. They might hold sometimes, but they can not be generalized. Also, I still think the technical novelty is not enough in this submission. Therefore, I am not very positive about this submission.  Update: Nov 30 2020 == Thanks for the authors for the reply. Thank you for running those experiments.I had a few more clarifications needed from authors. (a) Magnitude pruning typically invovles a fine tuning phase after removing the weights, was this carried out? For eg: Fig 1. a behavior was why I asked this question (b) I would recommend authors to add error bars Table 2. has results that are quite close between the methods.I raised my score but still below accept due to the above reservations. After rebuttal:- I  have read the response of the authors. - Some concerns are adressed: for instance some empirical results are moved to the main paper,  the tuning of the hyper-parameters is discussed and details about the groups of variables are provided.- Nevertheless it is still unclear why $s_t$ instead of $\tilde{s}$ is  used in  algorithm 1 and what might be the performances of the deep models  trained using $\tilde{s}$.- The baseline model is now moved to Adam with weight pruning. One concern of the review is to detail  how Adam, Adagrad are used to optimize the fitting term with a group lasso regularization. This point was skipped in the new version, hence it's not easy to assess the effectiveness of the Group Adam, Group Adagrad methods. I have raised my score to "5" after the author's response. While now I believe that once can come up with a scenario where the proposed theory of Homotopy SGD outperforms the vanilla SGD, it is still not properly demonstrated in the paper; there are a lots of hidden strings attached to the provided convergence bound (explained in my response).  ============ after discussion phase =============My important questions about locality of PL are not explained. For example, on why the same locality argument cannot be done on Vaswani et al, 2019's analysis on standard SGD. As I also stressed in my original review, I believe the idea of the paper is interesting and can be useful, however the merit of the paper is not explained clearly in the paper. Rather than comparing by SGD with vague arguments, I think the authors should clearly explain under what setting is homotopy preferable to SGD and why, which will make the paper much more accessible and impactful. Given the lack of explanations, unfortunately, I keep my score for rejection. #########################################################################Post-rebuttal.I would like to thank the authors for their reply, which addressed some of my questions. While I now agree with the main theoretical result when applied to a ReLU nonlinearity, this of course also reduces the area of applicability of the proposed technique. I am also happy to see additional empirical results, which I believe will be the key to making this paper much stronger (since the current theoretical result is actually quite straightforward when applied to the ReLU nonlinearity). But I think the results are still a bit insufficient to make this submission sufficiently strong. One of my concerns is the final accuracy in Figure 2. We can see that the unregularized model surpasses the accuracy of the refitted model and could potentially get higher (or even much higher) should it not have been cut at <100 epochs. I will be excited to see an updated and improved version of this paper in the future, but in my opinion, the current version still needs a bit of work and is not entirely convincing. ======post rebuttal: My concern regarding the experiments remains. I will keep my score unchanged.  ## UpdateI thank the authors for some of the updates and response that address my concerns with clarity, but my main concern related to the interpretability aspect has not been resolved. The authors suggest that one can discard some of the rules learnt by the algorithm if the model becomes too large, using the weights in the linear layer to determine which rules should be kept. This seems like a good direction to explore, but it is unclear if a few rules typically dominate the predictions, or whether one will sacrifice significant accuracy in order to gain interpretability. As stated in my original review, I would have liked to see analysis of the *tradeoff* between interpretability and accuracy of this method.A few other things:* I still do not understand exactly how the hyperparameters were tuned. The authors have provided a list of those that were tuned, but I still don't understand the process used for tuning them.* Some of the responses to other reviews have reinforced some of my concerns. In particular, it seems the distribution of weights attached to rules is not optimal for the proposed (but unevaluated) rule pruning method. Also, the authors conflate statistical significance with practical significance when replying to Reviewer 4. After rebuttal:Comment:I am suspicious of your argument on "complex due to multiple resolution pathways".Surely there is always a (unique) canonical deviation. For example, one could imagine extending Prolog to allow for any literal to be resolved; not just the left one. However that just adds extra complexity. Because we have to resolve all literals, we might as well do in a left-to-right order. Surely in your case, we can always do the leftmost (for example) one. Or the rules can be defined so that we can resolve them left to right.One counter to my argument might be (parent, ancestor) -> ancestor parent -> ancestor.Here we cannot go left-to-right, but need to go right to left. We do not need to search over orderings.This issue is faced daily by Prolog programmers; we choose whichever one of (using your notation): (parent, ancestor) -> ancestor (ancestor, parent) -> ancestor works with your engine. I don't see why they get around it without problem and you claim it is not possible. The rules may depend on the order used, but what is the problem with that if it drastically reduces the search space? You don't have to search for all proofs; just one. ----------------------------------------------Update after Rebuttal: I have read the other reviews and authors' responses but do no change my scores.I still share the impression of Reviewer 3 that several choices in the design of the benchmark seem to be very restricted and arbitrary (e.g., the single possible derivation sequence).Related work in terms of rule learning has been incorporated partly now, but there is no proper comparison -- especially in terms of the dimensions mentioned in the previous item. Also, there have been proposals similar to the E-GAT model the authors introduce which are completely ignored in the paper (e.g., MEMORY-BASED GRAPH NETWORKS, ICLR 2020; Graph Neural Networks for Social Recommendation, WWW 2019). ------ Post Rebuttal------Thanks to the authors for the extra experiments and feedback![Lottery baseline for Table-1] Although RigL does not need dense network training, it cost more to find the mask (Table 2 of the RigL paper).[Random tickets] Random Ticket = LT mask + random re-initialization rather than random pruning + random init. The front one will be much more interesting. "Because the LT mask + random reinitialization = random tickets fail in the previous literature. According to the explanation in the paper, it can also be the problem of random reinitialization. Thus, strong supportive evidence is that show proposed modified random reinitialization + LT mask can surpass random ticket performance." I personally do the experiment that performing proposed initialization on random tickets and the performance is unchanged. Of course, there may exist lots of reasons for the results. I will not degrade the paper according to my experiments.Other concerns are will-addressed. Thanks!Although I do like the idea of this paper, I think it might need to be revised and resubmitted, incorporating the extensive discussion presented by all the reviewers. I tend to keep my scores unchanged. But I dont think this is 100% a clear reject and depending on the opinions of the other reviewers I would not feel that accepting this paper was completely out of bounds. ----------------Post-rebuttal thoughts:I would like to thank the authors for their detailed response and the revisions made to the paper. I'm updating my score to 5 as part of my concerns are satisfactorily addressed, and I wished I could have more opportunities to discuss with the authors on their response. In general, my opinion is that the authors have introduced too many "artificial" components to the study (e.g., soft gradient coupling, the convergence/divergence indices) that make me slightly dubious of how generalizable this characterization is. For example, as the authors indicated, spectral normalization creates a different phenomenon (at a cost of worse performance), but with no change to the structure itself (so unlike the soft gradient coupling), a different phenomenon could be challenging the conclusion of the paper.My suggestion would be that the authors delve deeper into the observations here and better integrate the revisions with their original approach (e.g., the high-dimensional discussion; the spectral normalization discussion, etc.)----------------  ## After Author ResponseI think the changes to the paper have improved it. In particular, the reference to Spoerer et al. gives more weight to the motivations of the paper. I'm increasing my score slightly as a result.However, the relation to prior work remains hazy. The model of Ciccone et al., for example, has shared weights, stability over increased depth, and still performs as well as ResNets generally. Doesn't this go against the conclusions of this paper? **Update after rebuttal - I would like to keep my score**1. Although the authors have provided dev set numbers, the fact that test set numbers were computed, is highly concerning. In fact, for ATIS, the best dev model is different from the best test model. The other models should never have been evaluated on the test set. 2. Am still concerned about the datasets being old and saturated, and would love to see results on more recent datasets. =========After rebuttal I will still keep the score. The Assumption 1 is still quite strong. While it is true that $p(\mathbf{a}|s) = \prod_i p(a_i|s)$ holds for any policy with decentralized execution, the assumption of full observability is really strong and it doesn't seem that it can be got rid of easily. With this assumption, $p(\mathbf{a}|s) = \prod_i p(a_i|s)$ is actually a trivial assumption, since all information needed to make a decision of action $a_i$ for agent $i$ is already contained in the full state $s$ and can be determined independent of each other. In the revised paper, the authors suggest that communication can solve it but this would require thorough communication over the entire MDP, which can be hard to achieve (and if that's easily achievable then there is no need to study Dec-POMDP anymore). Without relaxing this assumption, I have concerns that the theory is not substantial (which are also concerns from other reviewers like R3). One baby step is to at least assume each agent may receive a noisy version of the full state $s$, and see what's going on. I thank the authors for additional experiments. Note that in addition to the proposed theory, there are many possible explanations of the empirical results presented by the authors. E.g., as a general rule of thumb in RL training, using offline data is often worse than using online data. Without detailed analysis, it is hard to tell. I would rather use an environment that is more complicated than the matrix game, but much more simpler than SMAC, which is partial observable and has too many moving parts.  --------------------- UPDATE Nov 30 --------------------------------------------------------------------------------------I find the updated manuscript to be a significant improvement over the initial version. The class of problems is now clearly described and the problem formulation explains the challenge and the need for an RL-based solution approximation.Seeing now that this work is not about a particular deployment scenario, but rather aims to present a method that can be used to solve a general class of constrained routing problems, another set of question arise.Probably the main limitation that precludes from evaluating the significance of contribution and seeing this work as general contribution to the list of solution for routing problem is the fact that is was evaluation on only single instance of such problem, with only single set of experiments parameters (20 robots / 200 tasks). Since the paper aims to propose a general method that is widely applicable is it only reasonable to expect to see experimental evidence that when applied to a diverse set of problems of this class it comes out on top. How does it fare against other similar routing optimization problems? How does if fare against other methods or AM is the only method that is worth comparing to?With this in mind I am raising my score for this paper from 3 to 5 "Marginally below acceptance threshold". The reason for not going higher is the lack of demonstration of applicability of this methods to a wider range of problems.   Update after rebuttal: I thank the authors for their responses to all my questions. However, I believe that these answers need to be justified experimentally in order for the papers contributions to be significant for acceptance. In particular, I still have two major concerns. 1) the faithfulness of the proposed approach. I think that the authors answer that their method is less at risk to biases than other methods needs to be demonstrated with at least a simple experiment. 2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating. ----------I appreciate the response from authors and the additional experiments. I do think the semantic search task adds value to the paper. However, the paper continues to be centered around the SentEval benchmark results. While SentEval is a useful benchmark to evaluate sentence representations, it doesn't reflect well how these representations will be used in practice. A fine-tuned BERT model will likely perform strongly on these tasks. The paper would be far more compelling if the authors can provide strong evidence that the sentence embeddings do well on tasks where using a BERT model is either less effective due to performance or computational reasons. I prefer to keep my score. ------------ After rebuttalThanks a lot for the authors' extensive experiments and good explanation to my questions. I would increase my score. The basic idea of this paper is promising and useful. However, there are still several problems after reading the rebuttal.Figure 2 shows that GD can distinguish different noisy levels, however, it is not a very realistic setup when training with a noisy dataset, to be specific, the dataset only has one noisy lever rather than multiple. Furthermore, GD is not able to give an early-stopping criterion on noisy datasets from Figure 2 where test error keeps decreasing but GD is increasing. Including noisy datasets analysis seems not a significant contribution. ----COMMENT AFTER REBUTTAL PERIOD:Given that there was no rebuttal, I keep my initial rating. Post-rebuttal update: I thank the authors for their response. The points raised above were largely acknowledged and the authors chose to not revise the manuscript, so my assessment remains the same. ----After rebuttal:Thank the authors for providing a revised version. The paper is more readable, however, I still hesitate to give 6 for the current version.  I would say that the fact pointed in this paper is really interesting, but this paper may need further improvement in the presentation. Further comments after the rebuttalFirst, I would thank the authors for the great efforts in trying to address my comments. I should say that many of my previous concerns have been clarified. Again, I like the theoretical part. Nonetheless, I agreed with some other reviewers that the experiments seemed not to fully convince me.  Particularly to myself, the authors may want to show some analysis on how their method could truly stand out by even a toy example, which may further enhance the paper. I tend to keep my original rating  after reading all the rebuttal messages in the whole thread.==================== Update after reading authors' response.The authors didn't address my question on the statistical significance of their results compared with baselines.The authors can address this question by "perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant."I suggest the authors to perform experiments to compare with SOTA baselines in the entire field of domain adaptation, instead of just comparing with a subset of algorithms that are based on adversarial training. After all, you want to see how your work stands in the entire field, rather than limited to a sub-field. ------------------------ #########################UPDATEThe discussion phase has highlighted several major issues: 1. There has been a significant conceptual shift in the problem definition (i.e. from estimating the intrinsic dimensionality of the class manifold to quantifying its geometrical properties). 2. I'm not convinced about the validity of some arguments/statements used by the authors to support point 1. For example, the statement "The intrinsic dimensionality of class manifolds is absolutely the full dimension of ambient input space, but this is a completely uninteresting observation" is not fully supported and I'm not even sure that is true.3. Furthermore, the paper is still in its original form. It has been difficult to keep track about the modifications that the authors should do.To conclude, the article is not ready for publication, yet and therefore recommend for its rejection. I encourage the authors to further investigate the topic and carefully consider whether the statements provided in the discussion phase are true. Update:Although the paper has improved, I still vote for rejection. The new insight of binary-classwise v/s multiclass UCE as a regularizer seems poorly explored in the paper and would benefit from closer study. This appears to be the basis of the improved results in table 1. Edit: Upgraded rating from 4 to 5-----Thank you for addressing most of my concerns.I have increased my rating, but the paper still needs some work in my opinion. More specifically,* a more thorough comparison with [1], e.g. by including the quantitative comparison you were working on.* an improved representative experiment from the problem domain. The newly added one is a bit simplistic and not motivated enough. Although stated to be 2D, it is (as far as I can tell) indistinguishable from a 1D regression task? How do you handle the 2D output in GPR? How are the data points generated that are used as training data, and what could motivate such a generation of data in a plausibly real setting? ## Update after rebuttal:The authors provided convincing evidence that the proposed similarity regularization performs better with regards to robustness than $L^2$-$SP$. I adjusted my rating accordingly. I still think the contributions are borderline because the proposed regularization is not backed up by theoretical arguments and the augmentation approach is incremental. ---- UPDATE: Thanks for the response, I have responded below and kept the score constant. Since I'm currently not actively working on the practice of adversarial robustness, the other reviewers are likely a better judge on the usefulness of the results of the paper for the community. ---Post rebuttal commentsThanks for the responses. After reading these responses and other reviews, I still has concerns on the justification of proposed convolution compared with other popular graph convolutions, and also the limited experiments on superpixel image recognition. Update: After reading the other reviews/responses, I think there are persistent concerns with the breadth of experiments and the substantiveness of the contribution; although the manuscript is somewhat improved by the authors' updates, I'm keeping my score at 5. Update I have read the author's rebuttal, and happy to see that a discussion regarding parameters is added (Figure 9). Other than that, my personal concern is similar to Anon Review 3's -- it seems that the core idea of the paper is drowned in too many technical details (granted, many of these are needed in order to implement this correctly). I wonder if a clearer discussion can be made like this -- you have a variational inference problem with certain independence assumptions, so write this out in the most abstract manner possible. To come up with a concrete objective, the question then becomes "given the factor graph, how do we add the networks (this basically lines 11 - 16 in Algorithm but not in the flow of the main text)". I think a better presentation and clarity in the main paper would greatly help acceptance. ***************************************After reading the rebuttal and considering carefully, I think the authors' response addressed some of issues in my mind so I raised the score. However, in terms of theoretical foundation, the current paper draft is still only marginal, which requires substantial improvement.  =============Update avec rebutal and discussion with AC and reviewers.After discussing with the others reviewers and AC, I have come to share their concerns with the overall fragility of the paper. We agreed that the methods is sound and likely to work better than AdamW, the proofs are not sufficient. In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. The authors do not systematically compare across learning rates which make it hard to interpret the results as being conclusive. In fact only CIFAR-10 is evaluated with multiple learning rates.The remark by Elya Loshchilov should also be adressed. Note that you cannot use stochastic noise as a justification, because for heavily overparameterized neural network, the amount of stochastic noise at the optimum is zero (i.e. perfect fitting of the training set). But there is another explanation: Intuitively for a fixed $\beta_2$, $v_t$ goes to zero as the current gradient goes to zero, and the ratio of the gradient by $v_t$ will converge to some constant, which prevents convergence. $v_t$ goes to zero at the same speed as the gradient but with a delay of $1 / (1 - \beta_2)$. If there is no convergence, then the gradients won't actually go to zero.The only way to prevent $v_t$ from going to zero is to have $\beta_2 \rightarrow 1$ (i.e. the previously mentioned delay going to infinity), but in that case $\bar{v}_t$ won't go to zero neither. This is only an idea of a possible justification and I would encourage the authors to think carefully about this stability issues in the next revisions.Finally I would encourage the authors the remove from the theoretical analysis parts that are not actually used (which I and other reviewers have noted). -----------------(Nov 26.) After reading Review1 I lower my Confidence.(Nov 29.) Taking into account the other reviews, the authors' responses to these reviews and the discussion, I now think the paper is not quite ready for publication and lower the score to 5. ------- UpdateThe response and update was a huge improvement. I now much better understand the goals, and the authors added some content that I think made the paper stronger and clearer. One outstanding issue is still that I do not understand why E_M[Q_M^pi] is the gold standard, and why bias is measured relative to that quantity. I explain this more below, but first mention some of the additions I really liked. For an upcoming paper, if this issue is remedied, I think this will be a good paper. Thank you for introducing Proposition 2 and the explanation about bias beforehand. This very much clarifies the motivation for using TD errors. The result itself highlights that using TDU will have a lower bias to the true expected TD error if the bias for the action-values for (s,a) and (s,a) are both in the same direction (both positive or both negative). Otherwise, however, it looks like the bias of TDU is strictly worse. One issue though with this result is that the magnitudes of these quantities could be different. The comparison between bias for TDU and the bias for Q seem a bit like comparing apples and oranges, and I would in fact expect the TD error to be smaller in magnitude and so naturally have a smaller bias. How much is due to this and how much to relative bias reductions? I suspect there is a real bias reduction here, but clarifying this would help.For the variance result, it seems better to directly report 47, and the discuss ramifications, rather than writing that they all need to be approximately equal. By the way, it is too bad that the result is a bit weaker for the variance, which is precisely the quantity you care about for defining your rewards. But nonetheless this formalization is helpful and provides solid insight.I like that the empirical work was improved, including adding some comments about significance. I also very much appreciate the ablation, where you use the variance directly from the bootstrapped action-values as an intrinsic reward. My only concern here is that the magnitude of rewards might be quite different, since the TD error should be smaller than the action-values themselves. This might mean different beta are needed.However, I remain unsure about the importance of this bias that TDU mitigates. You state: Our analysis shows that biased estimates arise because uncertainty estimates require an integration over unknown future state visitations. It remains a strong statement that uncertainty estimates require integration over unknown future states. As one example where this does not seem to be true is the Kumaraswamy paper you have cited. They show that if you use LSTD to get estimates of the action-values for a fixed policy, then you can get an estimate of the variance of the parameters. Maybe this setting assumes too much, and so it does not invalidate your result. But, I do believe a more clear argument is needed in this section for this result, as I expand on below.Methods that rely on posterior sampling under function approximators assume that the induced distribution, p(Q ), is an accurate estimate of the agents uncertainty over its value function, p(Q ), so that sampling Q < p(Q ) is approximately equivalent to sampling from Q < p(Q ).  This is a strong statement. I do not see why it is true. Are you suggesting that the agent must have the true Qpi? Is it not enough to use uncertainty estimates (epistemic uncertainty) for a function approximator? This result seems to show: if we want to mimic uncertainty estimates over true action-values for different models, then this is not possible under function approximation. But, that is maybe reasonable. Instead, shouldnt we ask: how can we mimic uncertainty over approximate action-values for different models? (i.e., relative to our function class)Additionally, you call E_theta[Q_theta] an estimator and discuss its bias. But, isnt that quantity not random? I presume E_theta[Q_theta] is the expectation, and the difference to E_M[Q_pi] is the bias. But, then what is the estimator that is biased?Minor comments:However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself.  What does this mean?state-action transitions  what is that? I think you mean just transition, since you condition on the whole thingIn the proof of Lemma 1, the Variance would remove the expectation over r term. Your result still holds, but the proof itself looks like it should be separated into the two cases.While Proposition 1 states that we cannot remove this bias unless we are willing to maintain a full posterior p(),  It is not clear how this result shows this. What if I maintained a full Gaussian posterior over theta? Would that solve the problem? What is a partial posterior? Update: Thanks the authors for their response. Based on the other reviews and authors' response I decrease my score by 1 point.   ** After discussions **I have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper.  =======================after rebuttal:I thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.Therefore, I will keep my original rating.  -----UPDATE: Thanks for the response, I have responded below and kept the score constant. ---------I have read the authors' response. The authors have addressed most of my concerns, but I still think the motivation is a little farfetched. Considering the paper indeed explorees some aspects (in theories and experiments) of the use of buffers in asynchronous Byzantine Learning, I will improve my point to 5. ## Edit after authors' responsesI have upgraded my score (from 4 to 5) based on the clarifications provided by the authors and the updated manuscript. Please see the details in my extended comments: https://openreview.net/forum?id=ZcKPWuhG6wy&noteId=V7Wy0Mpsz7Q Thank the authors for showing their effort in revising the paper.Some of my concerns have been solved thanks to the rebuttal.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Update after the author's comment:I appreciate the effort of the authors to add more experiments that all suggest that the ensembles tend to perform better in the small data regime. This makes the case stronger and the story more compelling, hence I raise my score. However, the paper is still missing the core explanations or a hint of why this may be happening, hence I still can not recommend the paper for acceptance. **After Author Response and Discussion:**Thanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I think that the number of independent runs used (especially on the smaller domains), and the way the results are presented with the min-max extent makes me less convinced of the results than I was in the initial review. Adding many more independent runs (seeds), especially on the smaller domains, would improve my confidence a lot. Overall I think the paper is of interest to the community, but the experiments and their analysis could be improved. [Post rebuttal]I am still leaning against the acceptance of this paper due to concerns about the limited impact of this submission. The topic of "limitations of episodic training" has been widely studied and in fact most SoA few-shot learning methods adopt a combination of supervised and episodic training for this precise reason. The exploration into the relation between the number of sample pairs and learning performance is indeed correct, but no strong conclusions can be reached due to the logical jumps required. The authors established that the performance is correlated with number of pairs with a log/square-root curve, and that ProtoNet performance is similar to that of subsampled NCA (fig3). The mechanism behind why this is has not been elucidated. I think many questions can be explored to strengthen this paper, for example:Are classes embedded tighter together? Are hard negatives pushed further apart? Does NCA induce a non-euclidean geometry that is more expressive than the euclidean geometry naturally induced by ProtoNets?Can the classification problem be converted into a pair comparison problem, so that PAC learning theory can be used to explain the shape of this curve? What is the sample complexity of the NCA classifier compared to the Prototypical classifier?Regarding the proposed method, NCA is certainly an improvement over ProtoNets, but performs worse than existing methods in most experiments. This makes me doubtful of the impact of this work on the methodological front. Better modelling of relationships between few-shot examples has been implicitly and explicitly studied too. For instance, many works adopt sample level set functions in the form of transformers, attention modules, and graph neural networks. Arguably, these additional architectures are more expressive "deep" alternatives to NCA, and hence achieves better performance than the proposed method. After rebuttal:Thanks very much for the detailed response! I do agree with the AC's comment that *I don't see a drawback the use of only one sample and no info about the target, rather I'd like to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair and still be competitive or outperforming them.* Also,  I would recommend the authors to have more experimental validation and resubmission. Thus, I keep my score. ---Thanks for the detailed response. I can agree that the observation function gamma in the form of addition can model many noisy signals. However, the argument in the paper that the proposed method works for an arbitrary gamma still lacks experimental validation. So I would like to keep my recommendation. My other concerns have been addressed. ==================================================================Post-rebuttal comments:I thank the authors for an extensive response and the other reviewers for brining up many relevant questions!= Main positive additions: LORL was successfully combined with another unsupervised approach, SPACE, on the CLEVR dataset.LORL+SA outperforms IET, which has the same amount of supervision. It does lose to NS-CL slightly, which has access to pre-trained object detectors.I like the added analysis of the QA types, data efficiency etc.= Some of the remaining issues:The positive impact of the objectness score on performance was not demonstrated. To show its benefit the authors had to propose yet another evaluation scheme (precision and recall of the reconstructed scene graph).The training objective for PartNet-Chairs should be discussed in the main paper, not in the appendix. Also, perhaps I am missing something, but would not one still need some negatives to train it?Minor: Fig 1 in the revision is still wrong, i.e. the example for PartNet-Chairs dataset still illustrates the QA task.Overall, I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited. The authors have considerably expanded their evaluation, but at the same time have introduced another confusion (as pointed out by R3 during post-rebuttal discussion): it appears that using 25% of supervision leads to lower segmentation performance, contradicting the main claim of the paper. I therefore decrease my score to 5. I hope to see an improved version of the paper (with more exciting technical contributions) in a future venue! ======After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I very much agree with the authors that the problem is very interesting, but as of now more work needs to be done in terms of "downstream applications".  ---------------------------------------------------------------------------------------------------------------Update after discussion:As stated in my initial review, I wanted to see an in depth discussion of what the implications of the binary tree assumption are for the types of transformations that are produced and the impact on the metric using the Jacobian. If these concerns are addressed, I think the paper will be greatly strengthened in the future.  ================Post rebuttal================I thank the work done by the authors during the rebuttal. However, my main concerns regarding novelty and experimental comparisons still stand - I think it is insufficient to say that your method is 'complementary to most if not all existing methods, so we could easily combine the proposed method to the others to further accelerate' without showing this or comparing to current approaches. The other reviewers have also brought up important concerns regarding experimental details which I concur with. I raise up my score after the authors' changes *AFTER REBUTTAL* I would like to thank the authors for their rebuttal. I increase my score to 5. I am still unsatisfied with the presentation of the idea, because it is still rather hard to follow. **Update:**Thanks to the authors for their detailed response to my review. Unfortunately after reading the response, I don't understand how it addresses some significant concerns I have about this paper and therefore I can't increase my score. In particular:- Author response says "Measuring the transferability from $M$ to $M_{adv}$ is actually the opposite of our concern, since our threat model deals with the transferability from $T$ (the protected augmented model) to $M$ (the target model)." I'm suggesting defining $T$ as a vanilla model and $M$ as a PGD-trained model. I don't see why your threat model would preclude such a choice.- Author response says "$M$ is hidden from the adversary. Then, the adversary does not have access to $M$'s output, and cannot mount a black-box attack to thwart a binary classifier $M(x)/T(x)$." The adversary doesn't need full access to $M$, only access to the output of the binary classifier $M(x)/T(x)$. Presumably they have at least indirect access to the binary prediction; otherwise the defender would be able to detect adversarial inputs but wouldn't be able to take any meaningful action based on the detection  (e.g. denying access to the attacking user, etc...).There is still a possibility that I've misunderstood something fundamental, so I leave my review confidence as fairly low. ---=====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to demonstrate the effectiveness of the proposed model on large dataset and analyze the effectiveness of each module (binary NAS and CMP). I keep my original decision for these reasons. **Post rebuttal comments**I thank the authors for the detailed response. I think that some points have been clarified and corrected, and I have increased my score slightly to reflect this. I still think the paper needs another iteration before publication though. **After Author Response and Discussion:** Thanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I agree with the other reviewers about the experiments: they feel rushed, and the design and presentation is not as careful as it could be. I think the paper could be greatly improved by a little more attention the presentation of the experiments and their analysis in the main text. Feedbacks after author response:I am maintaining my rating after reading the author response. It is a close decision. I like the topic, but I think the draft still has room for improvement to become a great paper. The updated draft is much clearer and answers some of my questions. Most importantly, the updated theory section explains why small beta can be bad: it slows training. While I appreciate the clarification, I think this argument still doesn't fully align with the experiment results. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain. To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. For example, I wonder if this is somehow connected to the slow training argument; perhaps the failed runs indeed suffer from slow training.  === EDIT: post-rebuttal ===I thank the authors for their patient and detailed replies regarding my concerns. I noticed that the authors have addressed the concerns to some extent in the updated paper (e.g., the remark in Conclusion). I'm also aware that the method can achieve the machine-learning goal of transporting a reference distribution to the data distribution, regardless of my concern. So I raised my score by 1 point.Nevertheless, I still feel uncomfortable to give a positive score. The current presentation of the motivation may confuse or even mislead the community. The authors present the Monge-Ampere Eq. (2), which solves for the _optimal_ transport from the reference distribution to the data distribution. But the method is constructed by simulating the gradient flow of f-divergence. Although the resulting transport serves for transforming the reference distribution to the data distribution (since the gradient flow minimizes their f-divergence), the transport is unnecessarily _optimal_ (there may be multiple transports to transform the reference distribution to the data distribution; the gradient-flow transport is one of them, which may not be the optimal one that solves the Monge-Ampere equation).For the authors' reply "Our method is based on a first-order approximation to the Monge-Ampere equation", I think the "Monge-Ampere equation" therein is different from the original one (Eq. (2)). If the method, which is constructed from simulating the gradient flow of the f-divergence, is to be treated as a first-order approximation to some Monge-Ampere equation, then the Monge-Ampere equation is between two adjacent/neighboring distributions on the gradient flow curve, but Eq. (2) is between the two ending points of the curve (i.e., the reference distribution and the data distribution). The two Monge-Ampere equations have different solutions unless the gradient flow coincides with the geodesic on the Wasserstein space, which is unnecessarily the case.I agree that "An important advantage of the proposed approach is that it allows general energy functional in constructing the gradient flow from the reference distribution to the target distribution". But I think it does not have much to do with _optimal_ transport or the Monge-Ampere equation. This thought works since the gradient flow minimizes the energy functional, whose minimum is achieved only when the two distributions coincide (when the energy functional is a proper divergence/discrepancy). --------------------------------------POST-REBUTTAL COMMENTSAs a result of the discussion the paper has improved, so I'm increasing my score. However, the core issue, that the proposed benchmarks don't seem to capture the difficulty structure of either real problems or more complex benchmarks, remains.-------------------------------------- After rebuttal-  I read the response of the authors. The spotted typos are fixed in the revision. Some  questions/concerns  have been tentatively. However the novelty in the paper is still not blatant or how the use of distance such as MMD or Wasserstein to match the features is under-explored. Hence I intend to keep my rating. Review update: I thank the authors for their response and for revising the paper based on the comments. The revision addresses several of my concerns. I still think the theoretical guarantees should be stronger and therefore change my score to borderline 5. ----------post-rebuttal----------I appreciate that authors have provided rebuttal that addresses some of my questions. I've read the updated paper and other reviewers' comments. In general, I'd like to maintain my initial rating as a borderline paper. Here are two main reasons.First, I realize that authors are not familiar with the policy, because they did not attach the appendix to the manuscript but uploaded as a separate file. As a result, their updated paper did not address issues in the review effectively. Authors simply say something like below - "We promise to improve our writing based on your suggestions in the revised version."- "Due to the space limit, we put our related experimental details into Appendix C." (9 pages are allowed by policy)- "We will supplementary the results of Pani Mixup(+hidden) to justify it if accepted."Second, in terms of other data augmentation, the authors merely say "As data augmentation is a common trick, consistent improvement can be easily expected across all methods". I don't know if this is true unless there is a justification.  Update after author response =====If you look a deeper look into Transformers, in case of BERT, the attention block only takes around 25% of total parameters. It is that suspicious that collaborative MHA takes 18% less time in practical since it requires many factors e.g., GPU kernel fusion.Regarding the performance on MNLI, from Fig 5, it shows that when D_k is larger than 512, MHA reaches to the baseline in terms of accuracy. Additional information from Tab 2, these models have more than 101.4M (in case of D_k = 384) which is almost the same as the original baseline. However, the performance on GLUE is dropped largely, thus it is hard to support papers' claims. Update: I have read the authors responses and am happy that they have addressed some of my concerns in my review, especially comparisons with previous works. However, I still have the concern that the improvement seems very minor compared to previous works. Thus, I decide to keep my score unchanged. Post-rebuttal:Thanks for the feedback. Some of my concerns have been addressed, but I am still worried about the consistency issue and how to handle the large-scale problem.  ----post rebuttal----I am downgrading my score as the authors did not address most of my concerns. The paper can be further improved with the suggested experiments as well as discussion on distributional uncertainty but the revised version does not appear to contain any of these suggested changes.  ----- i've upated my rating after the authors' response. *** Post response comments ***Thanks for the detailed responses and clarifications, especially regarding the manifold used. I suggest that the notion of $\epsilon$-manifold regularization is made clear upfront in the manuscript (e.g. in the introduction) to avoid misunderstanding.The new results with the dense $\epsilon$-manifold regularizer obtaining 5% less accuracy suggests to me that the specific approximation scheme used is indeed responsible for at least part of the benefit, and it would be useful to understand precisely why this is so since this is the core-contribution of the paper. I also agree with the other reviewers that the overall method as implemented seems a bit disconnected from the core idea of manifold regularization, and rather appears to be a variant of stability training (Zheng et al. 2016). As such, I will be keeping my original rating; nonetheless, I want to again thank the authors for the interesting and vigorous discussion. Update after the rebuttal:I read response from the authors and other reviews. I have increased my score to 5 given that authors now performed somecomparison with Liu et al. However, I still believe that the threat model is not realistic and that attacker can not be bounded by the budget that is produced by generator network here. While authors wrote quite detailed response, I did not find it convincing enough. As R5 points out similar issues, I would encourage authors to think more on how to tackle the problem better. Perhaps the threat model can limit the attacker to all possible perturbations under the certain volume budget, but that would be quite different from the idea in this paper.  Thus, I can not recommend acceptance at the current state.============================================================================ Update: I thank the authors for their response. I've read the other reviews as well, and indeed R2 had similar concerns to my own. I'm glad to see the more comprehensive comparison to Liu et al., which paints a fuller picture of the effects and trade-offs of the approach. The argument behind the motivation, however, feels much like setting up a straw man for Lp robustness. For example, the authors argue that their approach is label and semantics preserving unlike uniform perturbations; however this is quite frankly only the case for extremely large perturbations in MNIST-like settings which are unrealistic by design (most papers do not consider such large radii for exactly this reason). Uniform perturbations seen commonly in CIFAR10/Imagenet settings are practically invisible and consequently are equally semantics preserving and close to human perception. If the authors do wish to pursue this argument that these are truly more semantics preserving, then this needs to be backed by evidence. The authors weakly suggest this is the case because the budgets look similar to the content in the images. However, this does not imply that an adversarial attack within this budget is label preserving (i.e. many of the presented examples have large budgets in the background directly adjacent to the label-content of the image, which can easily change how the content looks), and so this needs to be justified carefully if this claim is to be made. The authors also incorrectly equate the restrictions imposed on an attacker from learned perturbation radii to that of a uniform radius. These are *not* equivalent, especially in the security setting where these are night and day; the first amounts to the defender choosing the rules of the game that work optimally for them, whereas the latter is a *defense agnostic* rule that both the defender and attacker must obey. This is a significantly easier setting for the defender that needs to be properly motivated, as restricting an adversary to a fixed perturbation set is inherently different from restricting the adversary to a fixed perturbation set that the defender gets to choose. The reason why one would want to maximize certification volume needs to be properly motivated, as it is no longer applicable to the usual adversarial security setting and comes at a cost to the usual robustness considerations. To recognize the addition of the necessary comparison to past work, I have improved my score slightly. However, I would still argue that this is below the threshold, as their central claim of learning *semantic preserving* perturbation budgets is not justified despite being a central component of the paper, as well as the motivation for why it's considered beneficial to choose the most easily certified volumes for robustness in the first place (and certainly not helpful from a security perspective).  -----------------------------------------------------------------------------------------------------------------------------------The author answers to reviewers' questions clearly with supportive details.  In addition,  the newer version has added useful new baselines and results. I have increased my score as all my concerns get cleared.Overall,  I think  this is an interesting paper and should be encouraged. But I still has some concerns as the application is very limited on medical images. The author has compared all the baselines and shown that the generalization capability for the proposed method is similar to fully supervised method, which seems not doing well.  I would be more convinced if the proposed can be tested on few shot benchmark which are natural images.  ----Update:Thank you for answering my questions and running the additional NDS0 baseline, this clarifies a vital aspect of the paper. I'm still concerned that in its current form, the learnings we can extract from the paper are limited.The new baseline confirms the expected, that for low-dimensional synthetic tasks where the ground truth model is known, pure sysid is a perfectly fine strategy. This is not surprising, and a good sanity check to perform. Unfortunately, this is also the largest part of the experimental results, and I'm not sure we can learn that much more beyond this.Results on 5.2 do look promising-- neither NODE nor NDS0 perform as well as NDS, and the new baseline has strengthened this result. But as the only data point demonstrating an advantage of the method, with little analysis to why and when it does well this still feels a bit limited.I still think this line of research is very valuable, and I encourage the authors to study the properties of this approach further (e.g. noise, missing systematic terms, investigate why partial NDS performs so well in some tasks), and investigate other non-trivial domains where NDS or variants can shine.I've raised my score to 5. -------update-----Thank you for your response. The discussion and the revised manuscript clarified some of my concerns regarding your work. I appreciate that the authors will focus on the parameterization of PartialNDS and the effect of the amount of employed prior-knowledge. However, my concerns regarding the overall performance, reported as very high errors overall, still remain. This might be due to how the experiments are designed, how the results reported or something else - but nevertheless it needs more attention and further investigation. ===================Update after discussion: The difference between excess loss and absolute loss is an issue I overlooked. After discussing with other reviewers, I realized a new question: how to compute the excess loss L(n, q) without knowing the the best possible estimator (oracle) in the class? This is a key step in the proposed algorithm when fitting the predictor. The paper has not explained it. As of now I do not think that is possible, except if strong assumptions are made, such as the oracle has 0 loss. In the experiments, if the authors use the estimator trained with full data as the oracle to compute the excess loss, that would invalidate the practical usability of the approach because the goal was to predict the performance without having full data to begin with. UpdateI have revised my rating based on the updates. The paper has good theoretical insights, however I agree with the other reviewers that more completeness is required to make the proposal stronger. -------------------------- Update: Thanks to the authors for the work done during the rebuttal period. The proposed method seems working but it's still not very clear when it is beneficial to use it. The authors provided some intuition about the cases when the method is effective, however, I think it is necessary to more thoroughly explain the applicability of the method before sharing it with the community at the conference, that is why I don't change my score. ## Post-rebuttal updateAuthors have addresses some of the issues outlined above, in particular the additional comparison in sec. 5.4 and table 7 is informative. However, the RAE numbers indicate that a simple 10-component Gaussian Mixture is superior to the complex model of NCP-VAE with a Gaussian prior on a small VAE, and the superiority of NCP with GMM prior base provides little information as RAE with 50-components GMM could have performed even better. It's also not clear how 2s-VAE, RAE and WAE would scale to bigger architectures such as NVAE, which raises the question if the NCP was the optimal one.It's surprising to see the 2-stage VAE to perform worse than the standard VAE, whereas (Dai & Wipf, 2018) have shown a significant improvement in the original paper. I think, this result needs further elaboration and validation.Regarding the qualitative evaluation: figure 12 does show that the NCP-NVAE has fever poor samples, although it's hard to tell whether the difference is significant. Another issue is that the comparison does not include prior works.As a result I bump by score to 5, but I think the paper still needs more work to make a sound argument for the proposed idea. =============================================================================================After discussion with the authors and reading the other reviews I still believe that the paper should not be accepted and therefore stay with my original review. While the authors have shown improvement over previous work (MH-GAN) using a very natural idea, this previous work in turn has not provided sufficient evidence that MCMC-GANs are useful.I believe that we should not accept further MCMC-GAN papers, before this methodology has shown any improvement on a plausible use-case. --------------------Post rebuttal comments:Thanks to the authors for the helpful comments -- they indeed help clarify some of the confusions. As a result, I have upgraded my score. However, I am still leaning towards reject because I feel there are still open questions that may hinder the adaptability of the proposed method in the real world. Specifically, given the response to question 3 above, it would help to know what are the real world situations where one uses a randomized classifier and is still interested in model interpretability (the two seem to be at odds with each other as randomness inherently seems a bit arbitrary). Another concern that I have is about Eq. 3 in the paper: Why is the sum function chosen to compute global explanations from local ones? There seem to be multiple ways to do this (e.g., median, sum of absolute values) and it would help to know what are the (dis)advantages of not using other aggregation functions. ################################################Post-Rebuttal:Thanks the authors for their detailed response! After reading the responses, I decide to maintain my initial assessment.The statement "full images are highly out-of-distribution for a model trained on images with only 5% unmasked pixel-subsets and hence such a model cannot properly generalize to fully unmasked images" makes sense. However, I still feel that the authors need an experiment of this flavor to support their claim of We show misclassifications often rely on smaller and more spurious feature subsets suggesting overinterpretation is a serious practical issue as mentioned in my initial review as well as pointed out by R1.Besides, I also find the point "the observed phenomenon is very model-dependent" raised by R1 is a valid major concern. In the authors response, they did not add extra experiments to address it. " We indeed find that models trained on 5% pixel-subsets can generalize to the corresponding 5% pixel-subsets of test images. " - the stated experiment trains and tests on the same model so it does not address the concern that the observed phenomenon is model-dependent. In order to address this concern, the authors need to add some experiments on transferring across architectures (e.g. train on SIS of ResNet and test on SIS of VGG).  ~~~~~~~~~~~~~~Post review and discussion remarks:I think the authors have improved the paper significantly during the review period. However, three of my main concerns about the paper remain to a degree that I'm not confident about the paper's value (or risk of misleading followups). (1) That the set of challenges is somewhat arbitrary, some tasks are using "real" ground truths while others are simply running on known trained oracles.  (2) That implementation of strong offline RL benchmark algorithms are missing (because they don't exactly apply across domains) even though they can always be applied in this setting even if "exact" conditions are not applied in every case (just like Gradient Ascent or BO were) (3) That the API needs to offer more for this to be a good benchmarking suite. I've been most concerned about 2 and 3, and after reading the code, I find that it is still too "bare bones" to be a good package. I looked back at OpenAI gym, and there are several abstractions that they make, including actions, observations, environments, spaces that help the implementer unify how they deal with the complexity underneath. So far as I can tell, most of what design-bench does is load a csv matrix into an task.x, task.score(x) , and also lets the user access some approximate oracle task.y. This are critical to the process but their abstraction as related to the paper are not clear to me at this time. How is task.y computed, how is the ground truth actually representative of reality. How does optimization depend on the choice of oracle for task.y? Having read the code, useful elements are in there to make for a good package, I feel like it needs improvements and another review for scientific soundness.I've updated my score to address the improvements made. The paper scores somewhere between a 4 and a 5 for me. -------------UPDATE: I have read the authors' rebuttal and revised draft and have raised my score to a 5. ----Post-discussion update:Having read the other reviews, author response and updated paper, I still think this paper is borderline. The insight that disentangling transformations as naively defined is impossible for topological reasons is valid and interesting, but seems to have been already observed by others, e.g. Falorsi et al. Nevertheless the paper does a good job explaining this so it could be useful, as some authors seem to not know about this issue. The definition of disentangling still seems a bit vague to me, and I'm not convinced of practical applicability of the proposed method.  Updates after author responses and discussion:(1) After the updates, most of the positioning issues have been improved(2) The technical exposition is improved, and the main argument I was bothered by is somewhat improved, though it still has a big jump near the end.(3) I'm still bothered that to demonstrate improvement the algorithm is tuned on a per-example basis but the baselines are not.  The results certainly show that the former is reasonable, but to make the comparison fair the latter needs to be done as well. **After rebuttal**The rebuttal does not address my concerns so I lower my rating due to the following reason:It was not clear where the improvement comes from so I asked for an image-level baseline, which is trained using contrastive methods. The rebuttal does not provide that. It is mentioned that adding the image-level baseline to the proposed approach even improves the results without providing any evidence. My concern was that an image-based method trained in a similar way might provide the same results. I cannot really judge if the proposed method is effective or not due to lack of this baseline. Several previous embodied representation learning works are outperformed by simple image-level baselines.  ####################AFTER RESPONSEI would like to thank authors for their detailed response and for their effort in improving the paper according to reviewers' suggestions. Unfortunately, after authors clarifications, I still have some doubts on the concerns raised with C1 and C2 (see below). Thus, I am keeping a slightly negative evaluation for this paper. Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. However, in Figure 6 MEPOL does not seem to suffer a particularly high-variance. To me, the most likely reason for the improved performance is that APT guarantees an action-level feedback as opposed to a trajectory feedback (see [1]).However, I am still skeptical about this action feedback: the reward-to-go becomes non-Markovian and Bellman equations does not hold anymore (see [2]). This casts some doubts on the actor-critic procedure APT employs to optimize the rewards. Authors may have a good point on the notion that the encoder is breaking the dependence between policy and rewards, but I think the topic warrant some additional discussion.I would suggest the authors to rephrase this work to give a more central role to the scalability to high-dimensional observations, which I believe is the main contribution of the paper, and to include a more thorough discussion of (Mutti et al., 2020) in the main text (beyond the related work section).[1] Efroni et al. Reinforcement learning with trajectory feedback. Arxiv, 2020.[2] Zhang et al. Variational policy gradient method for reinforcement learning with general utilities. NeurIPS 2020. UPDATE:Authors, thanks for your updates.Some of cons are gone, and the paper is better now, but my main concern stays: it's unclear if the results are about the problem solving ability or the zero-shot learning ability. Thus, I corrected my score to from 4 to 5. My rating has been updated after the rebuttal. --------update after reading the response-----------Thanks for the authors' response, but some non-trivial concerns are still not adequately addressed.1) The inconsistent comparison results between SNGAN and the proposed method over CIFAR-10 and LSUN Bedroom datasets.2) I can see the benefit such as compositionality from the proposed method of training EBMs. But the paper still seems to overlook the importance of giving the readers an overall picture of the state-of-the-art of learning EBMs. Table 1 should be expanded to include more state-of-the-art results from EBMs, whether using auxiliary generators or not. **Additional comment after rebuttal**Unfortunately, the reviewer would like to downgrade my first score. The remaining concerns are about R2-A1 and R2-A2.In R2-A1, the authors add some discussions about experimental results and explanations about Gu et al. (2020). The second discussion about generating pseudo labels would be the most considerable theoretical difference between the proposed method and Gu et al. (2020). The authors state that the proposed method addresses the problem by incorrect pseudo labeling of Gu et al. (2020); however, they fail to show quantitative nor qualitative discussions that support the statement. The third discussion seems just showing the proposed method achieves similar performance to that of Gu et al. (2020). As discussed in the fourth comment, the authors could add another result showing the proposed method is complementary to Gu et al. (2020).In R2-A2, the authors honestly state that the hyperparameters are tuned according to the test data evaluation. However, it should be avoided to evaluate unsupervised domain adaptation methods since there are no labeled data in the target domain in a real setting. Moreover, Figure 4 (a) shows that the proposed method is sensitive to lambda on Office31 dataset. When lambda=0.5 on all dataset, the proposed method achieves SOTA on VisDA as shown in Table 5, but seems to fail on Office31. # Update after initial revisionI have re-read the revised version multiple times now, and I thank the authors for the amount of work they put into their revision. I am raising my score for the next discussion round. That being said, I want to point out why I cannot fully endorse this paper yet. First, from the point of topological data analysis, the central algorithm is a comparatively small extension of the Geometry Score paper. I agree that this is a superb idea, yet the main contribution for me lies in the application of that technique to disentanglementand for this to be fully understandable, some more work is needed. For example, putting the central algorithmic details into the appendix will make the adoption of the method that much harder. Moreover, while I appreciate the overall story and description of the method, I do not think that readers will understand how this disentanglement is actually _achieved_ by means of the proposed TDA approach. I would therefore prefer to see a more 'technical' or 'algorithmic' description of the contributions in the main paper, in particular since I think that the ideas of conditional submanifold topology require more attention.My expertise is more the topology and less so the application of disentanglement; nevertheless, this paper strikes me as highly ambitious with a lot of potential, yet somewhat unfinished in its present form. I do believe that it has the potential to be extremely impactful with some additional modifications (concerning conciseness, but also experimental details).I fully realise that this is not yet the desired outcome for the authors; I shall endeavour to discuss this further with my fellow reviewers to see that we can reach a consensus! Update after author response: I appreciate the authors' efforts to address my concerns and to raise some interesting points I missed. I still find the paper's insights are lacking some novelty to be published, but I think that this line of research is worth it! ------------------------ --- Post rebuttal:I've read the author's response and there is no change in my scores. - The given argument regarding better-generalized aggregation function aid in training deep GCNs is not clear and convincing. - I agree that doing a detailed ablation study on a large dataset is expensive. In which case experiments on either synthetic or other smaller real-world datasets would be helpful. ----Update after rebuttal:I appreciate the authors' answers and revisions of the manuscript. The theoretical presentation is clearer with the new notation and I appreciate the improved Figure 2. I appreciate that the authors followed my suggestion to evaluate on more than 3 random seeds. Statistically, however, 5 random seeds are not much different. I was envisioning using at least 30 random seeds. Is computation a major bottleneck? Maybe a simpler and faster method could be used to showcase the benefits of the new objective. Overall, the new version of the paper is better, but I think the empirical evaluation and the writing should be further improved. I updated my score accordingly.  =============================================================================================================After author discussion: After discussion with the authors, I am now convinced that any applicability of the theory proposed in this work to GANs is fundamentally tied to univariate latent space or generator output since the "hidden convexity assumption" does not allow for a multivariate set of latent variables to be combined to a multivariate set of outputs.I still find the theoretical findings and method interesting, but I think that the work requires substantial refocusing and the identification of more examples of "hidden strong convexity" before being published at a top-tier conference. I therefore change my rating from 7 to 5 and recommend rejection, for now. After Rebuttal: I thank the authors for the rebuttal. I have also read the other reviewers comments. Unfortunately, the rebuttal is unconvincing and sometimes vague. I keep my original rating. #############post-rebuttal############I have carefully checked all other reviewers' comments, the authors' response, and the revised version. Thank the authors for their detailed feedback. They have addressed my concerns on the unclear presentation. However, joining the comments from other reviewers (particularly R3), I still think there are two major issues that prevent me from further increasing my score.Q1. It is still unclear why the proposed model can tackle the over-smoothing issue in existing deep GCNs.This paper has theoretically revealed the benefit of adaptive ensemble paths towards better trainability. Given the claim in Introduction, it is still unclear why such benefit can be used to relieve over-smoothing, particularly due to the missing analysis of the output dynamics. As already pointed out by R3, [3] has set up a nice notion of framework on explaining how over-smoothing happens and why deep GCN fails. It is a pity that this paper has not put their analyses into this framework and discussed the relation with the over-smoothing issue. Actually, a more in-depth discussion of over-smoothing on general GCNs (including ResGCN, APPNP) has also provided in an arXiv preprint paper [4]. It does show that the residual networks are capable of slowing down the convergence speed to the subspace and thus alleviating over-smoothing. Since the idea of random wiring is initially proposed in CNNs, the contribution of this paper that we expect is to answer how this idea can be utilized to solve the specific weakness in the graph domain.Q2. The experimental evaluations are still unconvincing.It is thankful that the authors have additionally provided the performance of SIGN and APPNP in the revised version. Yet, the reported accuracies of APPNP seem weird and much worse than other baselines. I do not agree with the authors' response that APPNP is not intended to address the over-smoothing problem. As experimentally shown in [5] and theoretically analyzed in [4], keeping the connection between each middle layer and the input layer is able to prevent the output from converging to the subspace caused by over-smoothing, and thereby deliver desired performance with the increase of depth. As this paper has conducted experiments on a newly-public benchmark under inconsistent experimental setting up (raised by R3), it is hard to justify the significance of the proposed idea compared with previous methods, specifically given the irrational observations on APPNP.Hence, I still believe this paper is below the acceptance line. [3] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In International Conference on Learning Representations, 2020. [4] Tackling Over-Smoothing for General Graph Convolutional Networks, arXiv 2020. [5] Simple and Deep Graph Convolutional Networks, NIPS 2020. ==== post rebuttal ====It is an interesting observation that federated averaging and several other variants of it may be cast as instances of expectation maximization (EM). However, unfortunately, these connections have been made in a rather informal way, not resulting in much of new insights into convergence of such federated algorithms, or other practical tweaks to them. The paper then moves on to propose FedSparse, an abrupt shift from the original observation to federated learning with sparse priors. The study of FedSparse is thin, with unclear motivations, and with not so strong experimental results. Connections with the plethora of existing literature on sparse signal processing, specifically as it is cast in the EM framework, e.g., approximate message passing, is missing. In the end, the reader is left with two interesting ideas but it is not clear what the take-home message is.My suggestion to the authors would be to "formalize" the connections between federated averaging and other instances of these algorithms (e.g., draw from EM literature to provide convergence guarantees, or maybe provide other variants of these algorithms with more favorable properties) and resubmit the paper with more formal connections to the EM framework. In my opinion, FedSparse is itself a separate paper that needs a separate motivation, set of hypotheses, and experiments. ---------------I thank the authors for their response. However, I am inclined to keep my rating after having read their response. POST-REBUTTAL UPDATE:I thank the authors for the detailed response. Based on the proposed changes I will slightly increase my score, but I still believe the paper needs additional work before meriting publication. After response: Thanks for the clarifications. However, conceptually important questions are not yet clarified yet. For example,  the objective itself can be made into a pure square function (and hence strongly convex) in both classical Wolfe's formulation and the proposed. As the authors are pointing out, the main issue is in designing separation (or projection) oracle for the constraints which corresponds to the base polytope in the context of submodular optimization and was the main motivation for Wolfe's algorithm. Moreover, authors mention that main difficulty in using projections is intractability but it is not clear why the linear optimization performed in the proposed algorithm is efficient. ### My final recommendation The authors have attempted to address some of my points but these points require more time to fully address as they require to run more experiments. For the current form, I remain my inital score and recommend rejection for this time.  -------------After author's response--------------As the other reviewers also pointed out, some baselines are missing and ablation studies on meta-data are missing as well. Meanwhile, my concern is that the presented idea in the paper looks very similar to that of the neural process, except the paper is optimizing over a point estimate of z while NP is optimizing over q(z|D). This remains unexplained in the author's response. Therefore I am keeping my original evaluation at the moment. Comments after authors' rebuttal:Thanks for addressing my comments. However, I think the current submission needs further work. - The authors agreed with me on my point that the "max-margin" claim might need further work, and since this is an important claim in the paper, I cannot improve my review score after the author response.- Different data splits should not be a barrier for comparison against previous work. # After rebuttalI would like to thank the authors for the hard work during the rebuttal. The ablation study of the data augmentation strategy and other added results are very helpful. Regarding the explanation of the flexibility and invariance, although I could get some intuition, I am still not fully convinced. So I keep my original rating. One possible way to make this work stronger and meet the acceptance criteria is to provide some empirical (or even better, theoretical) analysis of the influence of $\Sigma$ on the flexibility and invariance of a network. ===================Thanks for the rebuttal and the additional empirical results. In general, I really appreciate the "brave new idea" proposed by the authors, which could inspire new insights on neural architecture design.  However, my major concern still exists: the proposed metric seems to be sensitive to the choice of initialization functions. For example, uniform initialization [0, 1] seems not work at all but no further explanations, which may indicate the proposed method may work in a different way (e.g. taking advantage of some search space's bias).  So, I keep my original rating.    Pros:1. this paper studies an interesting problem, "imitation gap" in imitation learning. 2. the paper is easy to follow. Especially, the example part is easy to understand.3. the intuition for this idea is well-explained.Cons:1. from my perspective, the basic idea is dynamically using imitation learning and reinforcement learning for agent learning by a weighting function. It is a straightforward idea but lacks some novelty. 2. the main contribution of this paper is proposing an advisor and integrating it with the imitation and reinforcement learning process. However, only averaging loss to leverage imitation and exploration is a comparatively little contribution.3. in experiment PD, adding advisor from the beginning seems not fair enough, since at the beginning the reward is much higher than baselines. Maybe the experiments need a comparison before the advisor is added and after the advisor added.In summary, in this paper, the motivation is clear and easy to understand, and the problem is worthy to study. But the contribution of this paper is a little limited. A better solution is needed. =====================Post RebuttalI went through the authors' reply. My first concern is resolved by the reply. Form the authors' replies to all reviewers, I believe this is an incremental work. It is technically sound, but the lack of involved and novel technical contributions makes it more belong to an incremental work. Thus, I will keep my score unchanged. ## Post-rebuttal commentsThanks the authors for the response! I've read it and other reviewers' comments. I feel the authors didn't directly answer my questions and just reiterate what they have in the paper. Unfortunately, it is still unclear to me how to perform meta-training on standard FL training tasks, for example, shakespeare in [1]. In this training task, there're total 700+ clients. Does that mean in the meta-training phase, we need to sample 700+ clients for each episode? How to construct this meta-train dataset from a standard federated dataset? Update after rebuttal-----------------------------Thanks to the authors for addressing my concerns. I have updated my score. After reading the rebuttal: I increased my score to 5. I still feel that the writing style is hard to follow. Besides the examples that I wrote in my original review, there are many other places where the notations and definitions are not clearly written.  # Post-rebuttal commentsHello everyone,I have read the author's response and I am leaning towards rejection. The paper can be divided into two halves. The first half where the authors obtain bounds on ranks of DAGs is the main contribution of the paper and is clearly interesting. The second half of the paper tries to shoehorn these bounds into an algorithm for learning causal DAGs from observational data which is disappointing and is clearly below standard for the following reasons:1. The bounds depend on the underlying DAG which is unknown and therefore cannot be estimated from samples. Therefore the authors propose using "structural priors" to obtain these bounds. The authors don't mention where they get these structural priors from. Furthermore the bounds are only useful to restrict the hyper-parameter search space in the matrix factorization approach which is applicable to linear SEMs. These bounds can only be used "qualitatively" to guide selection of regularization penalty in the nuclear norm approach which is necessary for non-linear SEM methods.2. The theoretical results would still be useful if the authors could adequately demonstrate that for certain family of graphs the maximum degree can be high while the rank can be low therefore learning DAGs subject to sparsity constraints (whose sample complexity depend on the maximum degree) can perform worse than learning DAGs with rank constraints. However, this is not clear since in experiments the authors only show the SHD as a function of "average degree" and not "maximum degree". Figure 2 again compares rank against average degree and not maximum degree.3. The experiments are only performed in the low-dimensional regime at a fixed sample size (3000 samples and 300 nodes). # UpdateI thank the authors for extensive replies and updates to the paper. Most of my questions are answered, and the paper quality is substantially improved. I would not be opposed if other reviewers recommends to accept it. Unfortunately I still can't raise the score and advocate for it myself, since:1) Table 2 doesn't really show superiority of new nonlinearities, since the bolded (bottom) entry has both trainable $\lambda$, centralization and Mixup, while the baseline of dSELU only has mixup and trainable $\lambda$, no centralization. Without centralization (Mixup + trainable $\lambda$), lSELU/sSELU/dSELU perform comparatively. Without trainable $\lambda$ (only Mixup), they perform a bit worse than dSELU. With only centralization, BN is still better. Further, even after rebuttal, I still believe that making $\lambda$ trainable effectively cancels the preceding theoretical discussion, and makes this subset of results somewhat unrelated to the main idea of the paper. Finally, Table 1 shows a more robust benefit over dSELU on CIFAR-100, but not on CIFAR-10/TinyImageNet (where s/lSELU can be both better and worse than dSELU), which is in my opinion underwhelming given the added implementation complexity.2) Figure 4 is very promising, but I find that SELU doesn't look that bad on it, which again makes me wonder whether the new improved self-normalization actually matters in Tables 1/2, especially given gradient clipping and other heuristics in Table 2.So at the moment I find that the proposed nonlinearities are promising in terms of both self-normalization (Figure 4), and generalization (Table 1/2/6), but these results appear to be largely unrelated to each other, and neither of them in separation is strong enough to convince me to try s/lSELU over dSELU (given additional implementation complexity + the boolean hyper-parameter of whether to use lSELU or sSELU) or over BN (given additional hyper-parameter $\epsilon$). I wish the paper either showed clear use-cases where one can't train BN/SELU/dSELU networks at all in reasonable time (but new nonlinearities allowed it due to superior normalization), or more robust generalization results. ----------Update after rebuttal: I thank the authors for the detailed response. Some of my comments have been addressed. However, some of my major concerns remain such as the insufficiency of the hyperparameter tuning for the baselines to do a fair comparison. I am keeping my score. Post Rebuttal:I appreciate the authors' responses. The "novelty" over Zhu et al. was never under question in my review, I was mostly confused about how to weigh the significance of the findings, how useful it is to know some numbers for the version of the dataset created by the authors (which is not really the original Imagenet classification task), and if the submission actually does "pinpoint" what the problems are, how and when they manifest, to what extent the dataset is responsible vs. the training choices. Having read the other reviews, responses, looked at the updates, I'm still unsure --  if there were something in this paper that was new or surprising and not more or less already known from existing works (perhaps not precise numbers, but then the paper is essentially using a synthetic, modified Imagenet anyway), I'd be more enthusiastic about pushing up the rating. But as of now, I'm retaining my initial rating. === Post rebuttalThe authors' rebuttal addressed some of my concerns, but my primary concern still remains that that benchmark may be a bit too contrived, where the observations made in this paper may not generalize to more complicated real-world situations. The authors also made some far-fetched arguments in the rebuttal by claiming some concurrent works [1, 2] as "the 'real-world' version of the environments used in the paper," which, to be honest, further lowers the rating of the paper on my side: why is this paper worthy of acceptance if there exist more realistic benchmarks?I also agree with R1 and R3 that there are no new methods proposed in the paper, and the insights derived from benchmarking a set of existing methods may not be considered novel from the point of view of the ICLR audience. As a result, I keep my rating the same.[1] Physically Embedded Planning Problems: New Challenges for Reinforcement Learning, https://arxiv.org/abs/2009.05524[2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296 NOTE: a lot of disputes are around "the huge title 'AlgebraNets'". However, I did not receive justification response from the authors. A possible reason may be the authors are not aware of how big the topic it is, and were so attractive/confident in the current experimental improvements (which is also very appreciated). ===== Post-Discussion Update =====I appreciate the authors' efforts for responding my concerns and comments. The provided response and the update of the introduction do help better explain the motivations and the implications of the proposed generalization framework. Therefore, I raise my previous rating a little bit to reflect this. Although the proposed framework may suggests a potentially-interesting future direction for the deep learning research community, it still does not fully convince me its feasibility. In particular, the paper would be much stronger, if the author can dig deeper with the Real-World test (soft) error, which I view it as the end-goal of a real-world classification system, to provide more specific directions on how to make it smaller, or more importantly, how it changes the current deep learning training paradigm. -----------------------------------------------I appreciate the authors to improve the clarity and consistency of the method.I understand it has a practical value, easy to use and yet benefit from the method.However, if the authors want to stress its practical value, we need more convincing experimental results beyond MNIST and CIFAR10.Thus I concluded it is still below the acceptance threshold. **Post-rebuttal**Thank the authors for the rebuttal. It addresses parts of the raised issues. However, my rating keeps the same after reading the rebuttal and other reviews because1. the contribution and utility of the proposed method are not significant;2. the writing needs improvement; and3. the experiments are not convincing enough, and its advantages over previous methods are not clear.  ========================== Post Rebuttal ==============================The authors did a good job in addressing some of my concerns in the rebuttal. Thus I am increasing the score in response to the clarifications. However, I feel there is still some improvement space for the experiment part of this section, and I encourage the authors to incorporate the changes, including ImageNet experiment and following stronger baselines to make the results more solid and convincing. ---------------------------------post-rebuttal---------------------------------I appreciate that authors have provided rebuttal that addresses many of my questions, though I'd like to maintain my initial rating due to the following comments. I think this paper is at the borderline.In terms of explaining why "Euclidean distance is not sufficient for learning such a hierarchical regularization", I don't find the illustration example in Section 2.3 intuitive or concrete. I don't think Eq3 adds much as the paper does not explain further. Perhaps the confusion is from that the paper does not explicitly explain what "optimal classifier" mean in terms of Eq3.The authors only say "those parameters and schedule were optimized for SGD on plain networks, probably sub-optimal for our proposed methods. It is not clear whether other methods suffer severely from the choice of learning rate and scheduler. As far as I know, SGD is sensitive to the initial learning rate. So I am worried that setting the same learning rate is not fair to comparing different models that have different structures.From the updated paper, I find the blue line in Page-2 confusing. It is not clear about the logic: why diversity reduces over-fitting. (Xie et al. 2017) studies this point with a complete paper. But the way that authors simply put it is quite unclear how this statement is related in the context.Visualization is interesting to look at. But it should be better analyzed. For example, visually all methods produce similar tSNE visuals in Figure 5. But are there any essential difference? After reading the rebuttal, some of my concerns are addressed by the additional experiments. But I also agree with other reviewers that the result is not very surprising. As R4 mentioned, the proposed method depends on the a specific downstream task where the "small" "general" BERT can be further pruned. For a fair comparison to previous work, baselines that are applied to a specific fine-tuning task need to be compared. ===== ---Post rebuttal:I thank the authors for responding to my questions. While some minor points are cleared up, I am still not completely satisfied with the rather vague notion of "bilingual knowledge" (what even is the "correct axiomatic translation correspondence", is it the (idealistic) true data distribution one tries to model?). Similarly, the use of the MI as a measure of success is still unsatisfying to me since it relies on a "trick" (mismatch of vocabulary between source and target) which might not even be relevant in practice (since most models use sub-words anyway).Overall, I think this research direction is promising, but I keep my recommendation the same. The paper would greatly benefit from another round of revision to clear up these points and clarify the presentation if it is to be useful to the research community. =======================================================================================================Comments Post Rebuttal:I still find the technical contribution of this paper to be a good one. However, As stated in my original review, there were some clarity issues with the manuscript. While, I personally was able to follow along, the other reviewers are correct in their claim that the writing can be quite confusing (Reviewer 4 makes a strong case). To that end, I have decided to decrease my score to a 5 because I can no longer say the manuscript is ready for publication. Nonetheless, I urge the authors not to be disappointed. The presented work has many merits, and I am sure that with a thorough "clean up" of the text, this paper can be a great one! ----------- updates after reading author response -----------After looking at the authors' response, the revised paper, and the other reviews, I'd like to update my rating from "Accept" to "Marginally below acceptance threshold". While I still like the overall idea of this paper and I believe this is an interesting direction of research, the additional results in the revised paper actually raise more questions and there are issues that need to be addressed in this current study.1. Properly quantifying "proposal neglect"The improvements shown in Table 3 are very small, which do not support the claim that the proposed method produce "more and better boxes". Table 3 might actually not be the correct way to assess the "proposal neglect" (or resolving proposal neglect). I would expect to see a more in-depth result analysis. For instance, when comparing with the SOTA TFA approach, for each image on average, how many more correct objects are detected due to the proposed approach being able to find boxes that TFA cannot. Something like this would clearly indicate the benefit of the proposed approach.2. Sensitivity to hyperparametersIn Fig. 4 (left), there is a huge sensitivity to the hyperparameter selection. For instance, the performance of the proposed approach would drop drastically by just changing the number of RPNs from 2 to 3. This creates uncertainty for people to use this approach. Furthermore, for the COCO results, when the hyperparameters are not selected based on this set, the improvements are pretty small as compared to TFA. I believe a more robust hyperparameter selection method would be needed for the proposed approach to thrive. After reading the author feedback, I would like to thank the authors and I agree with them that it is critical to test hypotheses on large-scale datasets. However, I still think that the contribution is marginally below the acceptance threshold. ===================================After a revision===================================Thank you for your efforts to revise the paper. The revised parts about related work look good to me. I agree on that citing all those EBM application papers is not necessary. But doing so can provide a comprehensive and complete development of  the DeepNet-EBM. Again, this is not required and it will not affect the rating.   I also acknowledge the existing contributions in the current paper and admit that such a direction is promising, but I still feel that the current paper doesn't fully explore this area with more solid experiments. Thus, the whole contribution is quite marginal. By taking into account all these concerns, I will change my rating from 4 to 5.       ##########################################################################################The response addressed some of my concerns, but I am concerned about L-ALFA taking such a large part of the paper and then shown to not help so much over just picking the final laye nor being much faster than the baseline. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated. Although I do like the objective of this paper and some of the approaches, I think it might need to be revised and resubmitted, incorporating the extensive discussion presented by all the reviewers.    ---------- After feedback ---------- First of all, I greatly appreciate the authors patient response to me during the feedback period. The discussion was really fruitful. Unfortunately, I still have a concern about interpretability of the proposed method, which is a central topic in the paper.> First, we find it a bit strange when the reviewer says it is difficult to find importance/meaning of comparing motifs is unclear, we clearly show our method does find importance in NAS-Bench-101, all 3 tasks of NAS-Bench-201 and DARTS search space (Fig 1 and 7) -- if we can't find importance/distinguish different motifs, none of the results we've shown would've been possible. Even when a method works empirically, if a rationale behind the procedure is not clarified, a paper would not be scientifically convincing. Thus, I still do not think my claim is strange.> Second, the example the reviewer gives is not a case when averaged gradient fails. On the contrary, it is exactly an example of when averaged gradient works. A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability, as it doesnt consistently explain the network performance by itself (just based on such motifs, one cannot conclusively deduce the impact on performance of an arbitrary, unseen architecture in general) ...In the last response, the authors explained the interpretability issue through combination of motifs, but it did not resolve my concern. To simplify the discussion, consider a bit extreme case in which only one motif is employed in a network simultaneously, and assume WL parameter h = 0. Let g(c) = d \mu / d \phi^j |_\phi^j=c. Then, consider a hypothetical case as follows:motif a) g(1) = 10, g(2) = 10 ... g(10) = 10, g(11) = -10, .... g(20) = -10 : AG = 0motif b) g(1) = 1, g(2) = 1 ... g(10) = 1, g(11) = 1, .... g(20) = 1 : AG = 20In this example, b) has a larger AG, but a) can have larger importance in practice, and now, since only one kind of motif is employed simultaneously, the explanation of the authors cannot be applied. For the exploration purpose, I do not find any rationale to consider that b) is more important than a). I know that these are extreme examples and may depend on an application scenario, but my point is that these examples reveal difficulty of interpretation of AG. The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration (just referring other papers without discussing details in a sense of the above averaging). The explanation through marginalization also does not get rid of this question. Since the interpretability is a main theme of the paper, providing a better interpretability of AG would be desired. Post-rebuttal: The authors did not provide feedback and therefore I keep my score. Post-response update: I thank the authors for their response. I updated my score, but still think the paper needs improvement to be of interest to the ICLR community.--- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------The author response addresses some of my concerns. So I have updated my rating from 4 to 5. I recognise the novelty of the proposed framework but I feel that the main framework has not shown a clear performance advantage given several other components are added. Therefore, I am unable to give a clear recommendation for acceptance.  Post-rebuttal updateOverall, I am still borderline on this paper. I appreciate the effort the authors have put in during the rebuttal phase, and would say the paper is now clearer. The inclusion of mean/median normalized scores for the Atari results in the main paper has improved the experimental section. However, the main drawbacks that remain are empirical; the smaller scale comparisons between methods need to be updated to give a like-for-like comparison in terms of numbers of parameters etc., as the authors acknowledge, and the paper still lacks baseline distributional agents in the large-scale experiments. This is important, as it means it is difficult to assess the impact that, for example, SR(lambda) may be having on the experimental results. +++++++++++++++++I appreciate the clarified messages of the paper, and would like to see them emphasized more clearly in the next version of the paper. But due to the limited experimental scale on ImageNet (added in rebuttal, and in my understanding, it only verifies one of the multiple observations mentioned in the paper), I'm still leaning on rejection. I updated my score from 4 to 5. =============== UPDATE ==============After reading the concerns of the other reviewers and the author response, it seems that many of the concerns remain unaddressed especially the top concerns. Accordingly, I have decided to lower my score. ================ After the author response, I raise my score by 1 ================Thanks to the authors for the detailed response and extended discussion on theoretical results and experiments, and I have raised my score by 1.  =========after rebuttal===========I appreciate the authors effort to address my questions. I still think this paper is below my expectation especially if it is put into the context of RL.  I would expect to see how this method can solve or help solving a fundamental problem (e.g., reducing sample complexity) in RL or be applied to a novel application (e.g., nonstationary RL tasks). Otherwise I didnt see why it is necessary to compare this method with other statistical testing methods in the context of RL, if they can be easily made in other non-iid settings without mentioning RL.  ###post rebuttal###I have read the updated version of the paper and still feel that this paper may have errors regarding the flexibility and purpose of LIME. The idea is nice, but the paper and evaluations would benefit from more polishing before publication. I maintain my original score.Misrepresentation of LIME:Section 3: LIME does not assume that the sampling neighborhood is the same as the true distribution. It may assume something weaker, such as that the function being explained is fairly smooth in the sampling neighborhood. Note that this can be a feature of LIME and not necessarily a bug: if for example x1 and x2 are fully correlated in the data distribution but the classifier only uses x1, it would be impossible to tell this if sampling only within the data distribution. By sampling outside the data distribution it becomes apparent that the classifier is using x1 only. Also, LIME assumes black box access to the function, so I don't fully understand your statement that "we generally do not have access to the true labels of instances generated through sampling". It seems like you may be defining the "correct" explanation with respect to the true data distribution, rather than to the classifier. LIME is meant to explain a black-box classifier. If the classifier is wrong, LIME should reveal what the classifier does (that is, the explanation should also be "wrong" with respect to the true data). The "framework capabilities" is also simply not true: users can define the data point to be explained, as well as their own similarity kernel and/or kernel width.Evaluation:It's not entirely obvious to me how we can be sure that CLIME is producing the "right" explanation in C.1, C.2 without knowing the function f. If changing the training set changes the classifier f, then it is correct that the explanation should change. As mentioned above, evaluating whether or not an explanation is "correct" should be done with respect to the classifier, not the underlying data distribution. In "Detecting Adversarial Attacks", it's not clear from the text whether or not you retrain the adversarial attack with your perturbation function. Further, I suspect that LIME may also be able to identify the sensitive feature for sufficiently small neighborhood sizes when sampling in binary space Z'. It seems like a straw man argument to compare an optimized version of your sampling procedure to the default version of lime.Minor: Equations 1) and 2), if they are describing the usage in Ribeiro et al., should include a weighting function.Figure 2 seems not to be explained in the text and would benefit from more description. ==================== post rebuttal ==============================I do not think my concerns are addressed by the discussion. However, I also think this is a well-written paper in general and I will not be upset if it is accepted. My main concerns,(1) The authors fail to show that the proposed method is non-trivial. I think this concern is raised by multiple reviewers. The authors keep clarifying the technical difficulty (especially the theory) of applying LARS etc, but my main concern is the necessity of these knobs added by authors. After a few rounds of discussion, we reach to a conclusion that adversarial training is different from standard training, but I do not think that could be considered insights from this paper. I would strongly suggest authors consider explaining why LARS is necessary by either theory or intuitive insights, and make it clear what exactly the difference is. (2) The authors claim contributions for large scale setting (ImageNet with large number of available GPUs), but the experiments are somewhat worse than previous results. Lacking computation resources is a good excuse, but since the authors claimed they can use larger batch size with smaller number of GPUs, I do not see a technical reason why they cannot use their method to re-run the large scale experiments to directly compare with previous results. * Thank you very much for adding the comparison of GIZA and fastalign. However, simply swapping a model for alilgnment does not given us details about the tradeoff of alignment quality and the end-to-end results, since alignment models have different characteristic to capture the correspondence in two langauges, e.g., assuming linearlity and/or fertility. I'd rather like to see a much simpler approach of distroting alignment to avoid influence of the models employed for word alignment.* Thanks for the explanation. I'd like to see ablation studies regarding the loss.* Thank you for adding the tests. .## Post-rebuttal commentsThanks the authors for the clarifications! Most of my concerns are addressed. But the newly added asymmetric topology is still very benign and there is only 3 cells. I agree with other reviewers that this paper can be further improved -----### Edit: reply to author's response and updated paper (also see strikethroughs in the original review above)* Fold classification: let me withdraw my concern here, and will defer to other reviewers & AC judgement if this task makes sense with protein structure as input -- indeed it may not to be a trivial task. * Framing as (a) representation learning: improved in the updated paper, (b) convolution: still stands - the point cloud convs are not a very good comparison, since there is no graph structure there. (c) pooled coarsened graph stages: thanks for the pointer to end of Sec5.* Positioning wrt message passing: the paragraph is a big improvement, removing some claims about over-smoothing. However re: "the message passing function is learned": this is still very much within the default MPNN framework from Gilmer et al. Altogether, the whole method would still be much better framed as a graph-based network, rather than shoehorning this into a description of a single "convolutional operator".  This will allow a proper discussion of what is currently the end of Sec5, where the graph does not correspond to an atom-level graph anymore, rather they now correspond to amino acid or coarser level graphs - it is confusing that this coarser graph stages are so briefly glossed over.-- The citation to "can also be understood in a message passing framework (Kipf & Welling, 2017)" is off, should be "Gilmer et al., 2017" https://arxiv.org/abs/1704.01212 In conclusion, I am raising my score from 4->5, leaning towards 6. There is a lot of good work in this paper, and I would consider the paper a clear accept with the same method and same results, if it were thoroughly rewritten based on graph neural networks. Requiring full atomic structure as input to the method is the major limitation to the application and impact of the method. ### After the author feedbackI appreciate the authors for more control experiments and additional discussion in the manuscript. In the current manuscript, it is clearer that the "adapter network" predicting the "final linear layer" of the network is a unique component in this paper. I found that using an adapter network trained to predict linear layer parameters during training and used to infer parameters during testing is a setting that was not explored. However, I still see strong connections to previous works and I'm still not sure how to place this paper among such related works.I see the approach in this paper looks similar to RL^2 approach e.g. meta-training an LSTM based policy and meta-testing on unseen tasks. In this case, inferring the hidden state of LSTM looks similar to what the adapter network does in this paper.  Two differences of this work from LSTM based RL^2 are 1) neural network architecture, and 2) training objectives.One can construct a neural network architecture that is identical to an adapter network during testing time. One can train this network end-to-end during meta-training and use the exact same inference as this paper during meta-testing. This approach could be called RL^2 with a special neural network architecture that predicts the last layer of a neural network. If predicting the last layer of a neural network is an important component, it should be studied as an instance among variants of RL^2 with slightly different architecture. One practical concern about the architecture studied in this paper that this network may not scale to a case where the action space is large and the policy uses a large penultimate hidden state. In this case, predicting the parameter of a linear layer becomes very expensive. Because of the existence of this special case, I am not fully convinced about the claims that this method may work well in general.This paper trains the adapter network to predict parameters of a neural network instead of training the adapter network end-to-end during meta-training. Because of this difference, the method in this paper cannot be called RL^2 and it could be claimed that this paper explores a method that is not explored previously. If the training method is an important component, there should be at least one ablation for this detail.I still have a concern that it is not clear what's the main finding in this paper. Specifically, whether architecture is important or objective is important. I see that the paper already compared with RL^2, so adding an ablation study on using linear parameter prediction for RL^2 and discussing the relation to RL^2 would further improve the paper.  -----------After Rebuttal--------------I decide on up my score by 1 since indeed the authors are only required to cite papers that are peer-reviewed and published before 2nd August. I decide not to up my score any further because my other two concerns remain. I appreciate that the authors performed additional experiments. While the generalization performance is better than MLRN, it does not outperform other baselines such as MXGNet.  ====================Thanks the authors' response. Based on the originality of Theorem 1, I increase my score by 1 but still a bit worry about the sufficient algorithmic contribution beyond TRPO.  --------------Post-rebuttalI would like to thank the authors for the response. However, my concern on the theoretical part remains. For densities within the manifold, I suggest reviewers consider experiments similar to those in [1][2]. I also agree with R3, R4 on the reinvention of the connection between KL in the data space and KL in the latent space. In addition, I agree with R1, R3, R4 on the scale of their experiments and scalability of the approach. As such, I will lower my score from 6 to 5.[1] Mathieu, Emile, and Maximilian Nickel. 2020. Riemannian Continuous Normalizing Flows. arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10605.[2] Lou, Aaron, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser-Nam Lim, and Christopher De Sa. 2020. Neural Manifold Ordinary Differential Equations. arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10254. .Update:I thank the authors for their significant updates to the paper.  Given the extended effort made by the authors, I am willing to raise my score to 5. My conclusion however remains the same, this work is not a significant advancement that we would expect to see at a conference such as ICLR. ######## Post-Rebuttal Updates:I appreciate the authors' response, but my main concerns were not addressed. Particularly, I still believe the novelty of this method is extremely limited, as it directly applies an existing method on graph node embeddings. The main novelties are biased perturbation and unbounded attack, which are both very simple modifications, and were not adequately studied in experiments. Although the authors show extensive comparison on various datasets, their comparison does not particularly show the contribution of biased perturbation and unbounded attack. They only show the effect of biased perturbation using a single number in Table 2, which is not convincing.  After rebuttal: The long discussion with the authors to clarify the questions in my review and further related questions actually shows that the paper is not really clear and would still benefit from a substantially improved presentation. So, I slightly downgraded my overall rating of the paper. --- UPDATED SCORE ---First of all, I would like to thank the authors for carefully addressing my comments.In light of the authors' response, and after a thorough and careful discussion with the other reviewers, I have decided to update my score to 5 (five).In summary, I appreciate the authors' effort to signal the differences between their work and Elvin et al. While I agree with these, I still think this is only an incremental contribution. For further reference, after carefully discussing the paper with other reviewers, these two published papers were also pointed out where multi-hop attention is addressed. Namely,https://openreview.net/forum?id=rkKvBAiizhttps://ieeexplore.ieee.org/document/8683050I apologize for not finding these papers in my first round of reviews, but it does not really alter my evaluation of the paper.I think that the most novel contribution is on the spectral analysis. But this is only stated, with no real insights developed, and not emphasized enough. More insight on this would definitely bring a novelty. More specifically, novelties that would have made the paper more interesting: (i) a different way of computing the attention coefficients, that would be more parameter efficient (as opposed to eq. (1)) in the paper, (ii) actual useful insights into what the frequency response of the learned filters look like in the attention matrix as opposed to the given support matrix of the graph, (iii) a comparison between the spectral basis of the learned attention matrix as compared to the support matrix. --- Post-rebuttalI thank the authors for responding to all the questions and getting back with additional experiment results.Major concern: While I understand the motivation and how having attention scores over nodes multiple hops away can be powerful, I'm still not convinced with the approximate realization. It is not clear how diffusing attention defined over 1-hop neighbors is powerful over attention methods defined over immediate neighbors that contain k-hop information aggregated from diffusion.Also, the performance drop and overfitting issue with GAT or diffusion-GCN can be combated similarly by sharing weights across GNN layers and also using a higher-order diffusion matrix at each GNN layer. =============== after rebuttal: I thank authors for the responses. After reviewing the authors' response and other reviewers' comments, I keep my original rating.  --- Update after discussionI agree with other reviewers' idea that the contribution of this paper is somehow limited. Since my previous suggestion (7, accept) is based on that the author can successfully address how to control the error of the critic theoretically. However, with only the experimental elaboration study, as I said in the response to the author, the contribution of this paper is limited. Thus, I would switch back to 5 (marginally reject) based on that after reading other reviewers' discussion. After response:Thanks for addressing the concern about normalization. It appears that other reviewers have a concern about such normalization as well. I suggest the authors remove the results with normalization entirely from the main paper and only have it in the appendix for anyone that is interested in such normalization. On the other hand, without normalization, the results have changed for the under-parameterized regime (which makes more sense to me) and the proof looks quite different in the over-parameterized regime as well. I did not have time to check the proof and I believe it is better to resubmit the paper as new because of the major changes. Finally, I still have concerns about the fact that only variance is discussed. I suggest the authors state their results in a setting where both bias and variance exists and the features added to the model are related to the response. Otherwise, it is a weird message that it is good to add pure noise as features. It feels like although we can design multiple descents in the overparameterized regime when noise is large, it is very likely that the 0 estimate achieves the best prediction risk. So there is no point to go into overparameterization and multiple descents at all.    In summary, I have raised the score to 5. I believe it can be 6 or 7 if all issues are addressed, but I am afraid that the paper looks basically new after these changes and thus I am not sure whether it should be still considered for this conference.   Based on the problems found by the other reviews and having read the rebuttals I have modified my score. ### Post rebuttal Thank you to the authors for their detailed response and their effort in improving the presentation of the paper. I was impressed with how much the paper improved in this second version. In particular, I very much appreciate that the introduction starts with a simple one sentence explanation of the problem of neural network verification. This can be further improved if it included (almost) no maths, which can be deferred to the Background section. The figures in the updated paper are very good and a huge improvement of presentation. Finally, the paper now includes two clearly stated theorems, which also make the presentation and contribution much clearer. I have increased the score I gave to the paper. Regardless of what the outcome for ICLR will be, I would like to encourage the authors to re-iterate on the presentation to really crystallize the problem, definitions and the suggested approach --- the paper is already so much better than the first version, and even just a little more work can make it even better.  Post-Rebuttal Comment:I thank the authors for responding to my comments. The updated version fixes most of the criticisms raised in the review, and I have raised the score accordingly. My new score is "5", partly because I believe that after performing such a large-scale and comprehensive overhaul of the paper (which was certainly necessary), the paper should go through a full new reviewing process.  Update:  After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4.  There is potential here, but it will benefit from strong revisions. # Changes after rebuttalThanks to the authors for their answers to the questions and their revisions to improve the manuscript. It is useful to have further descriptions of reversible computing for an audience that may be unfamiliar with the topic. I would encourage the authors to make further revisions to more concisely show the scientific value of the work while leaving some of the details to tutorials or other documents.  Other venues more focused on scientific computing, programming languages, or Julia may also be more suitable. If the language also attracts more users and applications built on top of it, then the case for publication will also be stronger (consider that the PyTorch paper was presented at NeurIPS 2019 even though the first release was in 2016).--- --------- Thank you for the detailed reply and revisions. I have increased my review score because several of my concerns have been addressed. I am not entirely convinced that the baselines use current best practices, and several claims in the paper regarding the inductive biases and calibration. Nevertheless I think the algorithms proposed in the paper is practically useful.  ### My final recommendation The authors did not fully address my points. I remain my initial score and recommend for rejection.  ---Post rebuttal: The authors have partially addressed my concerns with regards to experiments on actual domains. I think this is a central part of the paper and these experiments could be improved, however I am willing to augment my score to 5. I am still ambivalent about the paper but I wouldn't fight against it being accepted. =====Update after rebuttal=====I have read the authors' rebuttal. I am increasing my score to 5 as some of my concerns are addressed. 1. I do not agree with the responses for the "Motivation --- Causality in Running Example". First, the GNNExplainer tries to maximize to mutual information, which means important edges for the prediction should be kept. Then why does the GNNExplaienr suffer from the confounding association? Second, "individually feeding the top edges into the target GNN" is not convincing. "An  edge is important for the prediction does not mean the prediction score is high when feeding individually.2. I still believe the proposed method is very straightforward and the novelty is limited.  Post-rebuttal:Thanks for the feedback. By optimizing Eq. (3), the learned graph is not guaranteed causal, so I would like to suggest the authors not emphasizing "causal". **AFTER REBUTTAL**I appreciate the author's efforts to improve the clarity of the submission and to add a comparison with Prob-NMN. However I remain unconvinced that this style of model can possibly scale to more realistic data. I've upgraded my score from 4 to 5, but I still generally lead toward rejection. --- Post rebuttal ---I'm keeping my initial score. My concern remains (and is apparently reflected by R1): the datasets and results do not allow to draw clear conclusions. The paper overall furthers our understanding on robustness only in a very limited way. The new dataset adds another specialized dataset to the mix. I disagree that the community should first exhaustively and randomly add datasets to the literature without coming up with a definition of robustness or at least try to categorize. The authors in their rebuttal criticize the community that they are looking at robustness and distribution shifts in a too simplistic way, but at the same time the presented work doesn't make an effort to change this either.To close the remaining question:- "Could you elaborate? We know that humans and primates can generalize to new renditions that they have not seen before (Itakura, 1994; Tanaka, 2006), while some other species cannot. Consequently more than training data matters."With "extreme", I mean distributions shifts that keep semantics, but change appearance strongly. If we go as far looking at biological systems, then yes, it is not only about training data. There is an additional mechanism at play that we don't know and currently can't replicate in ML. Given our current understanding, it is presumptuous to suggest that NN architectures and training approaches as they are covered in this work will be able to do these kinds of generalizations at the level of humans or primates.- "The empirical reality does not currently allow for a simple, single-cause characterization of robustness"I completely agree to this statement. However, this doesn't mean that characterization of robustness cannot be done by taking into account multiple factors systematically.I acknowledge that collecting a new dataset is a non-trivial effort and can be useful.  I acknowledge that the paper proposes an additional augmentation technique that seems to improve results in certain cases. All factors together taken together lead to my final score. Update after the rebuttal:Thanks for the responses. Given that the other reviewers also raise serious issues I will stick with my initial rating. The paper seems not to be ready for publication at ICLR Edit: I have increased my score in light of the response and manuscript edits. The manuscript is improved, but I think the method still needs more development. There are a number of interesting pieces but the final picture of an improved protein model is not fully resolved. **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the improvements made and clarifications given by the authors.I would keep my score recommendation at 5 (marginally below acceptance threshold) at this point, however, mainly due to (i) a limited novelty (as partly acknowledged by the authors in the response) and (ii) a limited experimental evaluation (missing high-resolution datasets such as ImageNet or real-world AD datasets such as MVTec-AD) of the current manuscript.I encourage the authors to build on their current results and extend their work with additional datasets and an analysis of geometric shifts also at training time.##### ---Rebuttal--I acknowledge having checked the authors' response and the revised version of the manuscript, that has been improved since the first revision. Authors did not answer to my request for more details about the hyper parameter selection procedure that has been adopted.  **************************************************************************************** POST DISCUSSION UPDATE ****************************************************************************************Thank you to the authors for the discussion. Given that the relationship between AF and F has now been addressed, I will increase my score. However, since the connection mainly hinges on the empirical correlation between the two, I still don't think that the paper quite establishes the theoretical connection between the three continual learning methods that it aims for. Finally, I'm not quite convinced by the potential impact of the insights, which seem fairly specific to choosing the batch size of SI, so overall I think the paper still is below the acceptance threshold. ******************************************************************************************** END OF UPDATE********************************************************************************************* *************Updates: Thanks for the authors' response. However, not of all my concerns are well-addressed. Based on the current methodology and the novelty of this paper, I remain my overall rating of this paper unchanged. ### Post Rebuttal UpdateI thank the authors for their response. However, I still have some remaining concerns about novelty, since to me this paper reads as another application of MAML to domain X, to speed up performance in the particular domain. Furthermore, I think it would be good to evaluate generalization to larger extent, for example on different conformations of dynamics as opposed to a fixed set of parameters on the synthetic dataset. =======Post-rebuttal=======I thank the authors for their comments. The current draft isn't a bad start, but in agreement with the other reviewers, it needs more work before it's ready for publication. Please carefully consider our recommendations for your next draft. ---------------------------------------------------------------------------------------------Thank you for the detailed response. I have updated my score from 4 to 5. Thanks to authors for the clarification.  I have updated my score. However, a easy search with the title of the paper "Recurrent Attention Walk for Semi-supervised Classification" in Google can find the code in GitHub.  -----Based on the issues pointed out by the other reviewers. I have decided to reduce my score for the paper.  __________________________________________________________**After rebuttal**I thank the authors for their responses, but I will keep my original rating of the paper. I think this paper has potential and a new submission will have a better result.Suggestion for improvements:1. As the other reviewers pointed out, the proposed method is computationally expensive. Although this is a downside, it is not really a major one, as long as the authors have a proper analysis of the complexity and of the actual inference / training time. The authors pointed out that their contribution consists of proving that the adaptive receptive fields are useful, and I agree that an efficient method is not necessary for this, but the analysis should be made.2. I think that additional ablation studies are needed. For example, the experiment in Figure 4 a) supports Proposition 1 but only on Cora that is homophilic and where the mean of the nodes is a good aggregation. The experiments with added noise are a good start. Maybe also compare GRARF with GAT with a simple strategy of selecting the edges (top k according to $e_{i,j}$, or all with $\alpha_{i,j} > 0.5$ ).3. Improve the motivation from Section 2.  **Post-Rebuttal:- I appreciate that the authors have conduct revisions on the current version. However, I think the current paper is probably still not strong enough for ICLR.  ----------------------------------------------------------------UpdateThank you for the comments. But there is a misunderstanding. MANGA as well as PEARL are online methods. They just need the observed data during the episode. It can encode the observation data in online manner. I think it is not evident whether IMPORT performs better than MANGA or PERAL. I agree that RNN is general, but on the other hand, I am afraid that the internal state of RNN does not converge and usually fluctuate from time to time. It may be difficult to get a persistent policy during the episode in the same environment. I would like to encourage the authors to perform more convincing experiment and make the claim of the paper consistent with the experimental findings. ======= UPDATE ========I appreciate the authors' efforts during the rebuttal period, but I still retain my initial assessment of the work.Overall, I find the proposed approach promising and easy to understand, but believe that the experiments can be improved to better substantiate the claims in this work. In particular, I believe that the benchmarks can still be more carefully chosen to better evaluate IMPORT's ability to perform sophisticated exploration. I find the 3D Maze experiment to be quite nice, as it clearly highlights a shortcoming of TS exploration, but I would find the experiments more compelling if there were additional benchmarks testing such exploration. The authors commented that exploration in meta-RL is about inferring the task to solve, which I agree with, but I think such exploration can still be made more "sophisticated" by requiring careful sequences of actions to lead to distant states, which reveal this task information.In addition, several issues were raised regarding the TS baseline during the discussion period. The results in the bandits setting appear to be lower than those reported in other works, and it still seems like PEARL can be adapted to be a drop-in replacement for the TS baseline. I agree with the authors' assessment that the basic form of PEARL explores the setting with multiple episodes, but PEARL could just resample from the posterior every few timesteps, which is already what happens in the TS baseline.I do think this work should be published in the future with a more careful selection of experiments. Note: initial score was 4; updated to 5 after taking a brief look at the experimental sections; will check again later to take a closer look at the updated "motivation" part.--- ---#### UPDATEI thank the authors for their response. My major concerns regarding the paper's clarity and the implications of its theoretical results still hold. Therefore, I would keep my initial score and recommendation. Update:The responses cleared up some aspects of the Hamiltonian engine and I adjusted my score accordingly. Still, additional baselines would improve the paper answer some open questions and validate some of the claims: Is the Ham. Eng. indeed faster than optimizing with a cheap force field? Is there then an advantage compared to featurization of the RDKit coordinates with one of the many MPNNs for 3d coordinates? This would be interesting to analyze. ** updating the score to 5 ** Update after rebuttal==================I will remain my score of weak rejection. ---### After RebuttalIncreasing rating based on the authors' clarifications on the source of the gains. Open to further changes based on further review and discussions with other reviewers   **Update after author response:** I have read the other reviewer's comments. My take is that at a high level the contribution of this paper is above the bar of ICLR, but the experiments aren't controlled enough so I vote for a weak reject.Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class. Going further, Hendrycks et al claim "but we find that even with OE, classifiers with the reject option... are not as competitive". As I understand, Hendrycks et al are basically saying the (simpler) method in this paper does not work as well - but Hendrycks et al don't provide experimental evidence for this claim, it's merely stated. The original paper might in fact discourage practitioners from trying this approach, and instead using the high entropy approach. In fact, this was a question in my mind when I read Hendrycks et al last year.Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. Conceptually, the approach in Hendrycks et al also seems more brittle and there are distinctions between these two methods (e.g. see Vernekar et al 2019, Analysis of Confident-Classifiers for Out-of-distribution Detection).So what's the potential practical impact? If this paper didn't exist, I suspect practitioners would use the method in Hendrycks et al, and not try the reject class method, because of that paper's claims. But with this paper, practitioners might use this method or try both, which seems like a good impact.So barring problems with the experimental setup, I'd give the paper a 7 / accept, and so I'd encourage the authors to continue on with this work.To me the decision hinges on the quality of the comparisons. I am inclined to agree with R1 on the quality of comparisons. Taking a closer look at their paper, they have no detailed discussion about the hyperparameters and experimental setup, which is key when the main contribution is a fine-grained comparison with Hendrycks et al. R1 raises a question about fine-tuning vs trained from scratch, and it does look like this paper trains from scratch whereas outlier exposure fine-tunes. The outlier exposure set is also different. While it is a smaller set in this paper, Hendrycks et al say "experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models.".Overall, I agree that the best thing would probably be for them to do a more careful  and controlled comparison, include all these details, and submit to the next conference. In my review I did mention that their comparisons were unclear, but they didn't take the chance to update their paper and misleadingly responded that "The OE method is closest to ours, so we are able to match their training regimen well", but as R1 points out there seem to be salient differences. ----Post-revision update---The authors have provided results on a second dataset 3DShapes and two more models  Factor-VAE and a perfectly disentangled model. However, the construction results of the GT decoder is much worse that other models, see Figure 2; it does not reconstruct the details of the "heart" shape even for training and the edges of the "square" are not straight. This begs the question how good the GT decoder is. The open question is, what is the generalization capability of a GT decoder that can both reconstruct and disentangle perfectly?Wasserstein auto-encoder has been shown to disentangle better and the regularization term is on the aggregate posterior instead of individual samples. Without results on WAE, the paper should refrain from making broad claims on disentanglement.  Furthermore, it would be interesting to investigate GAN based approach such as InfoGAN-CR as well.For an experimentation paper, it should be more thorough and go beyond just two shape datasets.I applaud the additional results the authors provided. I still think the paper is borderline (more toward 6 now). If it fixes the aforementioned weaknesses, I would recommend accept.  ==========Post rebuttal==========I appreciate the authors' great effort on answering my questions. The response clears many confusing points from the original draft. Meanwhile, I still have concerns on how the idea of contrastive learning is handled in this paper, which could have been better shaped. In sum, I have increased the rating accordingly. ------------------------------------Authors did answer some of my doubts through their response. However, I am still not very convinced if the paper has sufficient novelty to get accepted in ICLR. I increase my score from 4 to 5. I have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain. Update after author responses:--------------------------------------------Authors, thank you for your feedback.While it is true that generalizing RAML and SPG into a common framework is not trivial, the presented framework simply augments the dual form of SPG (i.e. REPS [16] in the SPG paper) with a RAML term. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.Wrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. However, the unusally poor performance of MIXER remains unexplained, and falls short even of scheduled sampling (SS), which suggests a lingering major issue. REINFORCE techniques rely on 1) strong baselines, verified by the authors, 2) larger batch sizes to reduce variance, and 3) pre-training to reduce variance and facilitate efficient exploration. If the MLE is undertrained or overtrained (the latter the more likely issue given the plots), then MIXER will perform poorly. Actually, it is now standard practice to pre-train with MLE+SS before RL training, and this is really the (also dynamically weighted objective) baseline that should be compared against. The current REINFORCE results (MIXER or otherwise) really need to be updated (or at the least removed, as they are not captured by the framework, but the comparison to PG methods is important!).More generally, I feel that the experiments are not yet comprehensive enough. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is.Overall I feel that the paper is still in need of substantial maturation before publication, although I have revised my score slightly upward. [update] After carefully reading the response (also from other reviewers), I decide not to change my rating. **** After Revision ***********I thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. **** After Revision *********** Update: I thank the authors for their response and revision of the paper. To me, results on WN18RR and FB15k-237 are inconclusive w.r.t. to the choice of using Riemannian as opposed to Euclidean space. I therefore still believe this paper needs more work before acceptance. Edited: I raised the score by 1 point after the authors revised the paper significantly.-------------------------------------------- --------After discussion:After reading the author's reply as well as the opinions from other reviewers, I will stick to my original rating since 1) the writing makes the paper hard to understand 2) current experiments cannot support the central claim of spatial-temporal reasoning. While the author resolve some of the concerns, they are encouraged to further polish the paper and use more evidence to support their claims. ------update----------I appreciate the authors efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it.It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as Improve our theoretical understanding of neural networks, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.Specifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even resnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as much thinner networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., winning tickets near or below 10-20%, depending on the level of overparameterization of the original network.The observation of winning ticket weights tend to change by a larger amount then weights in the rest of the network in Figure 19 seems natural and the conjecture of the reason magnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization.The figures could also be improved and simplified as the lines are hard to read and compare.  ------------ Response to rebuttalsThe writing of the introduction has been greatly improved. The authors suggested a practical scenario of using high level features. I am still somewhat skeptical. To do this the users need to map raw input to good representations. It seems that there are two ways this can work: users and service providers agree on publicly available representations, or the service provider is willing to share everything except the top layer. Both assumptions seem rather restrictive. Nonetheless even with practically useful scenarios, it is standard practice to use good features/representations whenever available, so not really an academic contribution. I am evaluating this paper only by the Lola contribution. The presentation of LoLa can still be improved, but I see the main ideas. I think the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade-off), what network modules can he choose and how to represent them? I think this type of general discussion can improve the significance and usefulness of the proposed approach.    edit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. After The Rebuttal: I really appreciate the author response and it is a shame that the revisions do not seem to be correctly uploaded. Unfortunately, the responses to my comments rely heavily on references to the revision that I cannot see, making it impossible for me to validate if my concerns were actually adequately addressed. The other reviewers have mentioned some very valid concerns about the submitted draft as well. As such, I continue to lean towards rejection of the submitted paper as significant revisions are certainly needed.  ### Post-rebuttal update:I am raising my score from 4 to 5, to recognize extensive updates to the paper experiments and numerous clarifications during the discussion, as well as to acknowledge that some of my initial concerns were not justified, e.g. asking for similar studies for MLPs (which was effectively done in the original submission), or questioning whether the Frobenius norm in Figure 7 really goes down (which was demonstrated better in the latest update).However, I cannot give it a higher score / advocate for acceptance because I still find section 4 (and follow-up discussion) to be more misleading/confusing than helpful when it comes to explaining the observed phenomena.Precisely, even after the discussion, I still believe that1) Experiments in Figure 7 should be done with circular-padded convolutions (to rule out the more trivial explanation of the decreasing norm), and on small subsets of CIFAR10/Imagenet32 (to establish whether low-norm is indeed associated with poor performance on image classification).2) A precise definition of norm for nonlinear networks should be provided, and a link with the Frobenius norm of the linear networks should be established. I don't understand what exactly "the norm in the corresponding RKHS" means, and how the reader was supposed to infer it from the text or our earlier discussion. Further, if possible, this norm should also be evaluated on the actual nonlinear networks, and compared to other notions of norms discussed in prior literature (e.g. https://arxiv.org/pdf/1706.08947.pdf), where lower norm is typically associated with better test accuracy.3) A proper discussion about the non-monotonic dependence on depth in the context of provided intuition should be given. Current intuition can be interpreted as either predicting monotonic decrease in accuracy with depth (learning solutions of lower and lower norms), or as predicting a sharp drop in accuracy at a certain depth (point where the minimum-norm solution can be learned), but neither interpretation explains the hill-shaped dependence, which again makes me question whether this is indeed the right explanation.Without section 4, the paper still has novel empirical results, but in my opinion they are neither surprising (e.g. a hill-shaped dependence of accuracy is my default expectation of any NN hyper-parameter) nor actionable (there are no hints regarding what the peak depends on / how to guess it) enough for publication at this time. A more rigorous investigation into explaining the phenomenon, or a more comprehensive empirical exploration to identify what does and what doesn't influence the best depth, would make this a great paper at a later conference.Best,R4. *****************************After reading carefully the rebuttal from the authors, I raise the score a bit as it somewhat clarifies some confusion. However, the paper still needs improvement, particularly in terms of analytical results.  I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.)However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited.As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation.  After Response:I appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue. After rebuttal:The authors addressed some of my concerns, and the proposed optimization plan is interested.1. I still think that the proposed architecture does not outperform SOTA architecture, like SlowFast.   - First, in the rebuttal, the authors mention that with hand-crafted strategies, the proposed model is 0.4% better than SlowFast; nonetheless, the hand-crafted strategies train longer epochs than SlowFast. (SlowFast is trained with 196 epochs.) As authors propose new strategies, they know better how different strategies could affect.   - Second, in the rebuttal, the authors improve I3D by 1.7% with optimization planning, which means, SlowFast might outperform the proposed model if involving optimization planning. Moreover, in the revised Table 7, the SlowFast does not have more FLOPs than the proposed network under the same backbone. Simply checking Table 8 might be confused. 2. About the optimization planning, it is still confused about how many epochs spent on the explored nodes. E.g. in Figure 4, for the Kinetics dataset, I do not understand why there is a black edge between S4 and S5 as S4 is not reached. (Authors noted that the black edges mean the explored strategies.) And why not there is no number on those explored edges? Another example, if we look at Table 3 and Figure 4 together, and again for the Kinetics dataset, the summation of those red numbers is 248, but this number is larger than any number in Table 3. As the authors mention that they included the epochs of the explored strategies in Table 3, that means they only spend 29 epochs in the exploration (if we treat Figure 4 for DG-P3D in Table 3.). For me, it seems too good to be true.Thus, I keep my rating.  == After Rebuttal ==Q1 provides a helpful clarification. Some of the more concrete possible drawbacks listed above, especially those about pruning "false positives," no longer seem as much of a concern.Some of the earlier investigation that the authors report, on more direct methods for finding dead connections, may need to be given a more complete treatment in the paper itself. I am not convinced that the direct methods cannot be done on arbitrary architectures, given that previous literature has managed to in a wide variety of examples. Without considering the simpler techniques, the leap to a more complex method for what could be a relatively simple task doesn't have the necessary support. ##### Post-rebuttalI appreciate the additional results in the rebuttal. I raise the score but it is still slightly below the acceptance. The reasons are 1) incremental novelty; 2) insufficient experiments. Also, I found in table 3 that, the larger-capacity model is less robust than the smaller-capacity model against white-box iterative attacks? This is strange. After rebuttalThanks for responding to the concerns in the review.The main point in the author's rebuttal is that the major contribution of the paper is a novel QNN that can be efficiently simulated. However, the proposed method of simulation is only an approximation of the real run through CLT. The author provided some conceptual justification of this approximation but there is no numerical verification. The approximation can be inaccurate when the variables in the summation are correlated or the number of variables is small. It's unclear how the accuracy of simulation can be guaranteed.Overall the paper proposes an interesting method of implementing BNN using quantum device, which has potential advantage over classical BNN and may be simulated efficiently with some approximation. More solid justifications of the advantages would improve the quality of the paper. --------------------------------------------- ###In light of the revision, I have revised my score given the rewriting of section 3 that addresses the second con I raised above. However, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, I feel that the paper still require improvement before it can be accepted. In response to the author's comments, I have increased my score.The practical implications of this theoretical work are unclear. It's nice that it relates to DQN, but it does not provide additional insight into how to improve existing approaches. The authors could significantly strengthen the paper by expanding in this area. Revision:I thank the authors for their detailed feedback, but still think the work isn't quite ready for publication. After reading the other reviews, I will decrease my score from 6 to 5. Some sticking points/suggestions:- Some of my concerns remain unanswered. E.g. the actor-critic method 3.8 is driven by the Bellman residual, which is not the same as e.g. the MSPBE used with linear function approximation. There is no harm in proposing variations on existing algorithms, and I'm not sure why the authors are reluctant to do. Also, Brouwer's fixed point theorem, unlike Banach's, does not require a contractive mapping.- The paper over-claims in a number of places. I highly recommend that the authors make their results more concrete by demonstrating the implications of their method on e.g. linear function approximation. This will also help contrast with Dai et al., etc. ---Update: I think the experiments are interesting and worthy of publication, but the exposition could be significantly improved. For example:- Not sure if Figure 1 is needed given the context.- Ablation study over the proposed method without sparse reward and hyperarameter \alpha- Move section 7.3 into the main text and maybe cut some in the introduction- More detailed comparison with closely related work (second to last paragraph in related work section), and maybe reduce exposition on behavior cloning.I like the work, but I would keep the score as is.--- Edit: Some of my suggestions were incorporated in the rebuttal, but my sentiment is still that this is almost at the acceptance threshold. The large focus on biology makes much of this paper harder to evaluate or appreciate. =============== after rebuttal ===================I appreciate the authors' feedback and slightly raise the score. Though the compression results look good, I still have some concerns about the method. The motivation of the proposed method is not strong. The proposed mask is greedy and sounds ad-hoc. The proposed progressive pruning looks expensive. The proposed method looks time consuming. For the experiments, I would love to see the training time comparing with baselines in table 1, not only the ADMM method in table 2. A fair comparison could be wall-clock time, or number of gradient updates for neural networks. Update: Lower the confidence and score after reading other comments. *** Updated ***After reading the updated paper, responses, other reviews, and looking at related works more closely, I have changed my score to a 5. This is due to several factors. Although the paper's core idea is definitely interesting, the fact that they use hardcoded features, rather the standard setup which uses pixels, makes comparison to other methods much more complicated. In particular, I think that the comparison to DQN-PixelCNN is unfair, as this other method makes very few assumptions about the inputs (only that they are pixels). The authors sort of point this out in the main text, but this is somewhat misleading. They say "PixelCNN uses less prior knowledge than our approach". In fact, it uses as much prior knowledge as any RL method which operates on pixels. Granted, this is nonzero, but it's vastly less than what this paper's method assumes. The other comparison is to SOORL (which uses a different state encoding altogether). The comparison to SmartHash is fairer, although the variant of SmartHash they compare against is not the main method the paper proposes (a generic autoencoder-based state encoding which makes minimal assumptions about the input). It would have been better if the authors included experiments for their method using such a learned state encoding.Reporting SOTA results on very hard tasks using extra hardcoded features or other domain knowledge is potentially misleading to the community as to how far along we are in solving these tasks, and extra care should be taken to put these results in context. Otherwise, for those not familiar with the subtleties, this makes it seem like these tasks are being solved when in fact they are not. My concern is that other works may then be asked to be compared against these artificially high results. Having many different task setups also makes comparison between different published works confusing in general. Other works (such as Ostrovski et al) have been able to make progress on these tasks while staying within the standard pixel-based framework.These concerns would have been partially mitigated had the authors made it *very* clear that they were assuming substantial prior knowledge, which makes their method non-comparable to others which do not make this assumption. This could have been done in the introduction (which was one of my comments, but this was not included in the updated draft). I.e., something to the effect of "We emphasize that our approach assumes substantially more prior knowledge than other approaches which operate only on pixels, and as such is not directly comparable with these approaches". In addition, I would have liked if the authors had followed the suggestion of Reviewer 1 to include results in pixel space, even if negative, but this was not done either (using a simple autoencoder-based representation, like the one in the SmartHash paper, would have also been fine). As it is, statements such as "Our approach achieves more than 2x the reward of prior non-demonstration SOTA approaches" and "our approach relies on some prior knowledge in the state abstraction function, although we compare against SOTA methods using a similar amount of prior knowledge in our experiments" are quite misleading and unfair to other methods which do not assume access to prior knowledge (the second statement is untrue for the case of DQN-PixelCNN). Another point which I had not noticed previously is the very high sample complexity (2 billion). One of the motivations behind model-based approaches is that they are supposed to be more sample efficient, but that does not seem to be the case here. ===================After feedback:Thanks for authors' feedback. Some of my concerns have been addressed. But the novelty is still not significant. On the other hand, the dataset used in this paper is simple. Specifically, at least the first 3 datasets are single-label and the number of classes is not large. They are too simple to support the claim. It's better to use multi-label datasets to show that the proposed method can really capture the correlation of labels. The authors have done a good job in the revision and have clarified points that were unclear in the first version. I have remaining reservations on significance, but move rating up a notch to reflect the extensive improvements and  the authors' confirmation that they will release the data. --- Thank the authors for the detailed response. This paper investigates an interesting issue. However, the overall technical contribution and novelty of this work remains to be a concern. After reading the response and the comments of peer reviewers, the rating is maintained as follows.  ######################After reading the author's response: While i still think that extending the experimental evaluation along the axes described above would improve this work, i decided to raise my score.  ---UPDATE: Thanks to the authors for responding to my questions and updating the submission. I will keep my score as is, since I think that the paper would greatly benefit from more practical/rigorous empirical evaluations to demonstrate the usefulness of the approach.  ---### Updates:I thank the authors for their response. Some of my concerns are addressed. However, unfortunately, I still think the assumption of the proposed approach is too strong to have broad applications. I will keep my original score. ## Post rebuttalThanks for providing detailed explanations.Overall, I think the authors tried hard to prove the concept of edge sparsification helps for speedup of attention GNNs which I also think they explanations/rebuttals succeeded in doing this, though the established theory seems quite standard and is not the same as in the experiments.However, pertaining the results I still do not see a claimed comparison of GCN, FastGAT-sparsified GCN in the revised work based on the replies to R3 (only FastGCN is reported. Note this is not a direct adaption of the authors method, but from the previous node-sampling literature). As is claimed by the authors, performing sparsification on GCN does not provide seminal speed boost. However, while GCN is a strong/simple baseline without heavy parameter tuning, its hard to make a clear justification on why a sparsified heavy-attention-computation network (FastGAT) would do any good if its final results are barely comparable/similar to a naive GCN baseline with a similar running time.Given the idea of sparsification is not novel which I stand on a similar point with R3 and specifically with its linkage to GDC. I think the current paper may need to be improved with some more evidence on proving the edge sparsification method is superior to other node sparsification versions of attention networks e.g. empirically/theoretically better (FastGCN-sparsified, ClusterGCN-sparsified models) on a complete set of datasets; or with a more rigorous comparison to other simple GCN-versioned sparse methods that do not need the heavy attention computation in the first place.For the current status, I would still lean towards a rejection. After rebuttal, the technical novelty is still not convincing. The masked encoder is widely used in image inpainting works. Combining multiple existing techniques may work better than existing methods, but it has little impact on the community. Thus, I do not change my rating. ### Post-Rebuttal Update ###I thank the authors for their response and edits to the paper. In particular, the authors have tried to address comments from the reviewers pertaining to further empirical evaluation of their approach. However, I still have two concerns about the paper post-rebuttal:[Baselines] I share Reviewer 4's concern that the authors do not compare to several natural baselines. As I mentioned in my review, I am still not convinced about the design choice of using multiple networks for each set of coarse/fine-grained labels. There are a host of approaches (both training losses and architectures) to deal with hierarchical classification in the standard setting. The authors do not justify quantitatively why adapting these methods to the robust setting (i.e., incorporating ADV/TRADES there) will not work. Further, I do not find the justification of fine-grained labels within a dataset being uneven for using multiple networks to be sufficient . This seems like a more fundamental problem that should be fixed by changing the dataset/class hierarchy. In fact, having even coarse/fine classes is essential to justify the merits of HAR in the first place.[Evaluation] Based on the new results added to the paper in the rebuttal phase, it seems that HAR does worse than vanilla ADV on coarse accuracy under untargeted attacks (Table 1); while vanilla ADV  does worse than HAR using worst-case attacks (Table 2). If we compare the minimum over the two attacks (as is standard to correctly measure robustness), the coarse robustness is actually *better for vanilla ADV than HAR* (24.60% vs 20.71%). This finding seems to go against the main claim of the paper.[Other] There seems to be a discrepancy in Table 1---the robust accuracies go up between the PGD20 eval and PGD50 eval.Based on the concerns listed above, I will maintain my original score. After rebuttalThanks for the feedback. However, I am not persuaded by the answers of Q1 and Q2 and would keep the score unchanged.  Update: In the revised version, the authors have addressed some of my technical concerns, therefore I slightly increased my score. #########################################################Final Recommendation:I have considered the authors' responses to my comments, as well as the assessments given by other reviewers. I still feel as though, while the method looks as though it may offer a potentially useful practical alternative to other graph learning methods, in its current form I do not think it is presented in a manner which warrants acceptance at a prestigious conference such as ICLR. If the decision overall is that the paper is not to be accepted, then I wish the authors well with their work and hope they take into consideration the comments of the reviewers as I do believe the work has potential. ####### Post rebuttal #########I thank the authors for their well formed responses. They address all my concerns effectively. I keep my original rating and recommendations.  ---------------------After the rebuttal------The authors partially addressed my concerns. I remain the current score. Post rebuttal Comments:Thank the authors for the response. I keep my score as 5. ------------------------------Post-rebuttal:I appreciate the authors' feedbacks. However the authors' response to the proof of Theorem 1 is not the most convincing, which is a big part of the claimed contribution.  After rebuttal: I thank the author for their response but I have to lower my rating by one step after reading the comments of Reviewer2. POST-REBUTTAL RESPONSE:Thanks for clarification on Theorem 1 of this paper, i.e. that the novelty is in interpretation. I agree that the "denoising" between observed and generated data is an interesting idea.I read the author's additional experiments on CelebA. In Figure 10, VampPrior is still qualitatively superior to the author's best result $SWAE(\beta^*=0.5)$. I have some skepticism over the reported results for WAE-{GAN/MMD}, which are much worse than the results in the original paper (Tolstikhin 2018). The authors appear to have used different encoder/decoder architectures, which complicates the comparison. Is WAE's decreased performance due to choice of architecture or algorithm? FID scores on CelebA would be also helpful.All told, I raise my score, but still harbor some doubts over the empirical advantage of this work. ============================================I thank the authors' for their response.I am satisfied with the answer regarding gradient clipping. As such, I raised my score to 5.However, I still cannot get away from the thought that random verification seems to speak for a rather humble a contribution. Combined with the lack of breakdown of "actual training" vs. "verification", I am rather hesitant to give a score higher than this. Regarding the experimentation on ImageNets, I understand that there was lack of time to add more experiments on this front. However, it would provide a more concrete message if the paper includes these additional experiments. ###########################  Post Rebuttal Comments###########################After reading the rebuttal, I am keeping my original score. For my first concern, they authors mention space as being a limitation for not providing analysis on the "surveyable" latent space, but as far as I know, additional experiments addressing my concern could have been added to the supplementary material. For my second concern, the authors talk about excelling in synthetic fidelity, however, there are fidelity measures for generative models that were not used in this submission. MSE is not a fidelity metric. I suggest that the authors address the concerns raised by the reviewers in future submissions, and it's highly likely that the work will be more solid. Post-rebuttal===========Thanks to the  authors for  their detailed response to  my  questions. Some of the answers are indeed satisfactory, but some questions remain - such as extensive comparisons to other methods (probably using more datasets), how  the method would  behave (practically)  with  a different fairness measure like DP, and more  carefully situating the method in  the  fairness literature. I encourage the authors  to keep pursuing this interesting  direction. Update======After much effort, I can say that I understand the paper. The edits appear to have incorporated all the identified missing information. I have thus updated my confidence and slightly improved my score, however I am not confident that the current presentation of the approach will be understandable by a reader without contacting the authors, given that the difficulty I had in understanding the paper (and my initial confidence) stemmed primarily from missing information and poor presentation for the approach. Although the results do seem to improve upon past work, its impact will suffer if it is difficult to understand for a non-reviewer reader. I would be more confident if a fresh set of eyes could understand the details of the work without having to go to the authors to clarify so many details.  **Post Rebuttal Comments**:I appreciate the authors' efforts on the rebuttal, most of my questions are answered and addressed. However, I still have a major concerns. As agreed by the authors' response, this paper lies in a detailed analysis of existing tricks. I thus hope the paper could bring some new insights, but the mentioned "other important conclusions" in response is somehow commonly known in the NAS community. This makes the paper like some empirical supplementary material for existing papers instead of a new one. Therefore, I keep my original rating. **Post-rebuttal update: the score is increased from 3 to 5. See the response to authors' comments.** ----post-rebuttal updateI appreciate the authors for the responses. Some of my concerns have been addressed, so I increased my score. However, I this the current version still lacks critical insights and some justifications are questionable. --after rebuttal--I appreciate the response, and it addressed my initial concern on the mutual information bound. However, the major concern remains which is the toy-ish setup and its practical value. The main result authors showed is that dropping some negatives doesn't hurt the performance if one adjusts m using EqCo rule. While this is interesting, the current setting of few negatives (by throwing out available negatives) is toy-ish, and may not reflect the real situation when a mini-batch is small thus only few negatives are available. I'd really like to see how this could be used to deal with the situation where you indeed only has few negatives. In MoCo, one could always easily buffer negatives with EMA the network, so there's no need to use a smaller set of negatives. In SimCLR, there seems to be a bigger potential of improvement from EqCo with actual smaller batch size, but the authors should also tune the learning rate (for smaller batch sizes) to make the baseline convincing, and compare to the results where large batch size is used. While the authors added a new point of alpha=4096, the current results of the paper are still incomplete, so I would keep my score. I'd encourage the authors to update/complete these results regardless the paper is immediately accepted by ICLR. ==== update ====I found the bound the author provides is problematic (see my response for details). In short, the bound is problematic in that the bound will be more accurate when only one sample from the product of marginal (or negative) is used, i.e., $K=1$, while more negatives leads to less accurate bound. This definitely counters the intuition in theory where more samples should let you estimate the KL divergence between $p(x,y)$ and $p(x)p(y)$ better.I believe there is a mathematical issue of directly setting up $m=\tau \log \frac{\alpha}{K}$ as in Eq(5). Though the empirical results look good, I recommend **NOT** accept papers with theory issues. ---Comments for rebuttal and revised paperThanks for providing a detailed response and an improved version of the paper. One thing that I am still concerned with is how come the updated ablation study is so different from the initial results. Originally, the differences between KGEDCg and KGEDCg-GE and KGEDCg-FKG were very minor (one of my questions above), but now the margins are as large as 7+ pts. Given such discrepancies without explanation, I'd hold my original evaluation. Updated review: Thank you  for your response and  for your efforts in addressing those points. I raised my score to 5 for the clarifications made to the document. However, I still think there are unjustified claims about the method, especially those related to the strong correction scenario and how introducing fresh generated samples is an effective procedure (conceptually / theoretically). **Acknowledgement of author responses**The authors' responses were helpful to clarify the settings and basic ideas of the proposed solution. I highly appreciate their effort. However, the limitations of the proposed solution and limited novelty still outweigh the merit; and therefore, I will keep my original evaluation.  ****************************************Post-rebuttal comments*******************************************************************************************************************************************After reading authors feedback, I've increased my score from 3 to 5; however, the paper in its current form is still a borderline. *************************************************************************************************** I want to thank the author for addressing my concerns. Many of my concerns are resolved. I have updated my rating after the discussion. I agree with the authors that the novelty of the submission lies in scaling up the existing methods. However, I'm not sure if this is enough for the paper's scientific significance required by this conference.  ##################################################################[Edit]After reading the authors' response and other reviewers' comments, I have lowered the scored to 5. The reasons are bellow.This paper provides good analysis of the coverage and balance of robustness benchmarks. Based on the analysis, the paper presents a method to construct a benchmark of Non-Overlapping Corruptions. I think such analysis is valuable and interesting, despite some concerns about the coverage comparisons (which are only partially addressed by the authors' response). I agree with Reviewer#1 that the comparisons between ImageNet-C and ImageNet-NOC are not solid. And that "the paper should have done an equivalent evaluation, comparing ImageNet-C as a whole vs. ImageNet-NOC as a whole." Since the core idea of this paper is to propose a better robustness benchmark, solid comparisons between ImageNet-C and ImageNet-NOC are critical.I also agree with Reviewer #3 and Reviewer #4 that, the improvements provided by ImageNet-NOC does not make a significant difference. The reviewer appreciates the analysis of the coverage and balance of the robustness benchmarks, which in my opinion, is valuable. However, since this paper focuses on proposing a better benchmark, solid evaluations between these two benchmarks are critical.  ----------------------------------------------------------------------------------------------------Comments after rebuttal:I have reviewed the response from the authors, and I decided to keep my score. Although the paper is certainly interesting, for ICLR it is borderline. While I agree that many works in the literature choose to treat their network as a monolithic architecture, and thus do not explore the effects of different components, I would still encourage the authors to add an ablation of the readout method, as that would help assess the interaction between that and the choice of the graph propagation scheme. --- **Update:** I would like to thank the authors for their answer. I acknowledge the improvement of the manuscript after the review process:- Added clarifications on the baselines,- Added helpful precisions for reproducibility (even though the code cannot be open sourced),- Evaluation on 2 requested public datasets with good results.However, I am still not confident to raise my score to 6 (marginally above the acceptance threshold) given the missing public manuscripts (or Appendix) to explain the martingale diagnostic tools. This hinder one of the main selling point of the paper that their model reduces the volatility of the forecast. ######Update: After looking at the revised version, I would like to raise my score to 5 since the motivation is clearer. ---------- after rebuttal ----------Thanks for the response and the updated manuscript. I'm raising my score from 4 to 5. I'm still leaning towards rejection since I still find the results quite subtle and I hope to see more empirical justifications.In the updated Proposition 3, the sparsity-inducing property 3 assumes the existence of a time $t_2>t_1$ when the ratio between the two weight norms deviate from $1/c$. However, it seems entirely possible that this ratio will have already converged $1/c$ after time $t_1$; in this case the two weight norms grow at the same rate. It would be good to investigate this more carefully to see which cases are more likely to happen. I'm also concerned that the advantage of EWN for pruning only shows up in extremely small loss value (Figure 7), and therefore the practical relevance shown in the current paper is not very convincing. Post-discussion update: The authors have clarified their work considerably, and I believe the work is probably correct. However, the paper still suffers from poor presentation and poorly-motivated or justified modelling choices. The current version of the paper has not been updated, and all below issues thus stand. The paper overall presents a good idea, but I believe the authors made poor modelling choices, which led the kludgy math. ---- ________________EDIT: Score changed from 6 to 5 during discussion, see comments below. Thank you for your response, which cleared up some of my questions so I increased my score to a 5. I would have liked more analytical / empirical evidence to substantiate some of the claims made in response to the questions numbered 4, 5, 12, and 13 in the comments provided by the authors. My overall opinion remains unchanged but I encourage the authors to continue this line of work on build on these results and incorporate the feedback provided by the other reviewers as well.### ====== POST-RESPONSE UPDATE ======I appreciate the authors' response and the additional experimental results provided. While I do think that they are a step in the right direction (I hence slightly increased my score to a 5), they still do not address my concerns. Specifically:- **Empirical results.** Taking a closer look at Table 1* of the response and the results on randomized smoothing (provided in other responses), I do agree that not all errors increase by the exact same multiplicative factor. At the same time, the discrepancy does not seem to be particularly large---i.e., the ratios have a mean of 2.5 with a standard deviation of <0.5. Clearly, given that this is a real-world dataset, it is natural to expect that the effect of adversarial training is not perfectly linear across all classes.- **Theoretical results.** After reading the response, reading the discussion with Reviewer4, and going through the paper again, I am still not convinced that the increase in class disparity cannot be attributed to a large extent to the gap in clean accuracy introduced by robust training. Perhaps there is a cleaner way of formulating there results or highlighting the key components of the analysis that resolves this. However, unfortunately, I still do not find the analysis convincing in its current state.Overall, I believe that this point is quite nuanced and the existing empirical and theoretical analysis is not sufficient to draw a confident conclusion. Given how this point is at the core of the paper's contribution, I still recommend rejection. =========Post Rebuttal==========Because no response is provided, I maintain my original rating. Post-rebuttal ReviewAs there is no response submitted by the authors, I would like to stick to my original rating to reject this paper. After rebuttal:My main concerns are addressed, and I changed my score to 5 accordingly.------ **after rebuttal**Some of the issues are clarified. I updated my score to 5. -------------------After reading the authors response, i think that this work would benefit from an experimental comparison of their random method against a method relying on a deterministic crop selection, even if the certification rates of the deterministic method are inferior, because the certificates both methods are yielding are different: The resulting certificates from the deterministic method would be deterministic instead of probabilistic. Hence i will retain my score.  Update after rebuttal:After reading the rebuttal, I don't think my questions are addressed very well, especially about the confidence probability, $p_c$. The certification is defined as a guaranteed yes/no problem but the $p_c$ will relax the certification to a probabilistic problem. Also, PatchGuard with patch transformation is out of scope for the original paper, so I think the experimental results in Section 4.1 and 4.2 are more like a fair comparison. However,  refer to the results in table 3, the proposed method yields worse performance than PG-DRS although the computational cost is saved. Hence, I will keep my rating. ------------------------------------Update after rebuttal-----------------------------The updated version is clearly better than the first one. However, the concernsof the other reviewers regarding the randomness of $p_c$ (and therefore of thecertification method) have convinced me that there are, indeed, some furtherclarifications and discussions needed prior to the publication, which is why Iwill keep my initial recommendation.To be more precise, the root issue here seems to be that the proposedclassifier is not deterministic, which means that the standard definitions ofadversarial examples and adversarial accuracy do not apply and therefore, thatthe problem that you try to solve is unclear and/or not well defined.  Inparticular: what is it that gets certified?  what does it mean to getcertified? and, more generally, is the word "certified" really appropriate inthis context?However, whether an analysis of the distribution of $n_{2to1}$ and $p_c$ willbe needed (as asked by other reviewers) might depend on how the authors willdefine adversarial vulnerability in the random setting, and what they try tocertify. Let me explain what I mean.A reasonable start might be to define adversarial risk as$$    E_{(x,y)} E_{\phi} \mathcal{L}(\phi(x), y) \ ,  \tag{1}$$which is the usual definition, but with an additional expectation over thevariability of the classifier $\phi$.  Adversarial accuracy would then be theadversarial risk for the 0-1-loss $\mathcal{L}_{0-1}$. Then the authors could, f.ex., set as goalto construct a (provably) unbiased estimate of this quantity.The advantage of such a method is that one doesn't forget the fact that, whatwe actually want to certify is this "distributional" robustness (i.e. whereexpectation is taken over the true underlying, unknown distribution), not therobustness on the test set. Even methods that have a non-random certificationprocess (so-called "provable robustness guarantees") will never be able tocertify this quantity: they'll only deliver certificates on test example. The"certified robustness on the test set" that they yield is also just a randomvariable which we hope "generalizes to" (1). Reviewers almost never ask authorsto analyze/certify this generalization gap. Similarly, here, one could see therandomness over $n_{2to1}$ and $p_c$ as just another source of randomnesscontributing to the variability of the generalization gap, in which case, maybeno rigorous analysis could be acceptable, as long as it is clear what theauthors want to certify (unbiasedness of the estimator, f.ex.). Therefore,whether this source of randomness could or should be explicitly captured/usedby the authors' method is, I think, a question of how the authors frame theproblem and their goal.### Minor points:- even the revised version still contains quite a few grammatical errors,  especially in the new/re-worked sections, where many articles ("the", "a")  are missing.- End of p.5, "to maximize number of certified robust images, the randomized  cropping classifier should maximize n2to1, which is equivalent to maximizing  classification accuracy of $g_\theta$": not sure about this equivalence.  Maximizing the classification accuracy is equivalent to maximizing n1, not  necessarily n1-n2.- are DRS and PG also probabilistic certifications (i.e. certifying robustness  with some probability, f.ex. pc>.95)? This should be clearly said in the text  and the captions, especially since it would make the comparison a bit unfair  if their certification were 100% sure. (This issue is obviously related to my  previous major remark on randomness.)- the name "worst-case certified accuracy" in the caption of Table 1 is very  unclear at that point. It becomes clear in Sec. 4.3, but you refer to Table 1  in Sec. 4.1 already. So this term should be clearly explained in the caption,  or there should be a clear reference to the relevant part in the text.- don't always re-cite Levine&Feizi and Xiang et al. every time you mention  de-randomized smoothing and patch guard. Cite them the first time, and then  say that you'll refer to de-randomized smoothing and patch guard as DRS and  PG in the rest of the text.- in the conclusion: "This paper proposes a new architecture for defense  against" -> "This paper proposes a new defense against". (You are not really  proposing a new architecture.) -----Edited after authors responses:I would like to thank the authors for the detailed response and the changes they have made to the paper.  - Regarding contextual data augmentation (Kobayshi et al., Wu et al., etc.): Thanks for pointing out that these method use the labeled data to finetune the LM to make sure that words are replaced with other "label-compatible" words. Note that the comparison is not intended to necessarily show that the proposed method outperforms these baselines. Rather it is intended to guide the reader into making a decision about which method is more appropriate for which problem. If the findings are that the performance is comparable but one method will eliminate the additional finetuning step, that would be a useful finding to share. Also , it is not clear that  these methods would require labeled data in the target language for the finetuning or not.  For example, can the source labeled data be used for the finetuning step?- Regarding translation: Cross-Lingual transfer via Machine Translation does suffer from the label transfer problem for sequence tagging tasks (transferring labels for sentence-level tasks is straightforward). However, there has been several methods to address this in the literature by using unsupervised word alignment  (e.g., Yarowsky et al., 2001; Ni et al., 2017) or attention weights from NMT models (Schuster et al., 2019), heuristic approaches (e.g., Ehrmann et al., 2011) or co-learning alignment and tagging (Xu et al., 2020).A detailed comparison and discussion of the trade-off between the performance of each method and the resources they require would make the paper much stronger  I have read the authors response and updated paper and appreciate the discussion of applications that admit factor graphs. ----------- Update after revision ---------------------Dear Authors, Thank you for the revised manuscript. The expanded sections have helped me understand the paper more deeply, and place the contributions in context to prior work better than before. I especially appreciate the new experiments and explanations on the Robust Manifold Defense. A few comments on the revised paper: * First, the success rate plots which were missing are quite revealing: While the biggest benefit of the reduced resolution query search appears to have been in robust models (Madry et al), the benefit is not reflected as clear in terms of the success rates (which is arguably more critical). For example, in CIFAR-10 experiment in Fig 2(a) while in the Natural case, BiLN+HSJA is slightly more query efficient it appears less successful than plain HSJA in Fig 6(a). (Once again, the plots and figures are extremely illegible). * For "Robust" CIFAR-10 models and "Natural" ImageNet , there appears to be a considerable gain in both $L_\infty $ distortion *and* success-rate.  For the "Robust" ImageNet case, while there are *huge* gains in terms of $L_\infty $ distortion, there is essentially no discernible difference in the success rate between reduced-resolution and the original attacks. * by the way there appears to be a typo in Fig 6(a): both BiLN+Sign-OPT and AE+SignOPT are the same color in the plot* In the new Figure 4 with MNIST experiments, in all the experiments, the base attack appears to be more query efficient than the resolution reduced attack. For a few it appears that before 5000 queries the reduced-resolution are better, but claiming this supports the earlier results isn't as convincing.  * The observations from these plots are confusing: >"The hard-label attacks consistently outperform against the Robust Manifold defense": Again not clear how it is claimed that the attack is outperforming the defense without mentioning success rates for this experiment. This also does not clearly outline why the reduced-resolution variants to not match up to the base attacks?* The reconstruction error plots are also very hard to see -- once again all the reduced-resolution suffer in these experiments. The explanation given is that:> "Due to the dynamics of the topological hierarchy discussed before, the dimension-reduction is less effective on MNIST" This is another example of very heavy terminology that makes it hard to understand what the argument is exactly. Overall, I think the paper needs more clarity in its experiments. The story is muddled by several inconsistent results, some showing improvements with reduced resolution (the finding is surprising and genuinely interesting.. but unfortunately doesn't seem to be consistent; especially when looking at it with the success rate metric)I think the authors have a convincing motivation and argument that query-efficient hard label attacks achieved with reduced-resolution can be closer to the manifold, this is clear in Robust CIFAR-10, Natural ImageNet models. The paper does a good job of explaining why these are the case, unfortunately the hypothesis doesn't hold up just as well in the other scenarios, which raises the question as to how these can be used in a more realistic scenario -- and more importantly as one of the other reviewers also pointed out -- what the reader must take out of these results to appreciate query efficient attacks better. I think in its current state the experiments portray a more confusing picture than the arguments in the paper suggest. As a result, I will retain my original score.  - In general, my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient.  - I also agree with AnonReviewer3  and AnonReviewer4 but feel less excited about the prons and more worried about the cons. At this point, I'm not against the acceptance of the paper, although I'm still staying on the rejection side. I'm increasing my score because we are at least talking about a borderline.-------------------------- **Final recommendation**I do not recommend accepting the paper. The results have been greatly improved. They now look decent and I have improved my score as a result. However I think the contributions are still not clearly highlighted. I however encourage the authors to improve their paper.  # UpdateThanks for the authors to address my questions. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. I appreciate the authors' efforts to fairly compare to Zhuang et al. in the rebuttal, and I do find the preliminary evidence sufficient to establish that their approach outperforms LA on clustering metrics. However, the authors seem to miss the the point that LA is not just another consensus clustering approach which they forgot to include into literature review since it does not report clustering metrics. The main contribution of their work is combining consensus clustering with representation learning, which is exactly what the authors of LA had done before. It does seems that the particular approach proposed in this paper results in a better clustering performance, so the submission contains a valid contribution, but the relationship between the two methods needs to be discussed in a lot more detail, and they have to be throughly compared experimentally. I encourage the authors to improve the manuscript in this direction and resubmit to a different venue. ### Final Recommendation after Author Response ###I have read the author response and appreciate their feedback. As the authors could not address some of the issues (error bars/statistical significance testing, other hyperparameters and other datasets/benchmarks) in the restricted time of the rebuttal period, I will keep my rating and recommend rejecting this submission for ICLR. However, I also encourage the authors to resubmit a revised version of the paper taking all feedback into account since I see clear potential. ### UpdateThe authors have not sufficiently addressed my comments regarding the choice of alpha in practice. While a possible strategy was proposed, it has not been evaluated empirically, see comments below for more details. I am thus maintaining my score. **Final Update**After reading other reviewers' comments and authors' responses, I decided to choose to reject the paper at this time for a few reasons.1. I am afraid that the impact of the paper won't be good with only one out of the four reviewers understanding a paper meant to present an entry point benchmarking dataset.2. I agree the binary, pseudo sensitive attributes, such as stripes, make the benchmarking and the analysis more manageable. However, as a future user of the benchmarking dataset, I see that there still remains the question of whether the metrics measured against the proposed benchmarking dataset with simple pseudo sensitive attributes can translate well to the model performance measured against some real world dataset with real sensitive attributes. It will help to add the best or chosen model's performance against some real world dataset with real and more complicated sensitive attributes.3. Add something like what is proposed in #2 to help the readers understand how to choose the best model and make tradeoffs for a given scenario using the presented dataset with the proposed methods. The goal would be somewhat similar to that of the "INTENDED USES" section of the LFW dataset presented in http://vis-www.cs.umass.edu/papers/lfw.pdf.Nevertheless, I believe the authors' core work is in the right direction. Just need some way to organize and present it better to make a bigger impact. Thank you for your work! ==================== After rebuttal =======================Thank the authors for their detailed responses, which have addressed my concerns regarding Eqs. (10) and (18).  Thanks for the efforts of the authors. Some of my concerns have been addressed. However, I think that  the paper should test SDL layer by plugging in any hidden layer. So I keep my score._____________________________________________ After rebuttal: I thank the author for their response and have raised my rating by one after reading the author's response, but my concern still holds unfortunately regarding the whole approach that the author has taken to analyze the effect of data augmentation. Therefore, I'm still not confident about my rating. -- post rebuttal --After carefully reading the authors response and the discussions with other reviewers, I am updating my ratings. The authors acknowledged that it is not a theory paper and therefore, the biggest concern is empirical evaluation. The shown performance is not comparable with existing high performing methods and therefore it is hard to judge without any direct comparison. The authors stated that their method can be applied on top of any existing method, which was not shown in this submission and therefore it will not be meaningful to judge just based on this statement. I believe showing this empirically will definitely strengthen this submission. ## After RebuttalThank you for addressing my questions. Since the performance improvement is still marginal and based on the other reviews I have decided to keep the same score. -------Update after rebuttal:I appreciate the efforts of providing a hyper parameter study. Thanks for the clarification about dataset used in the paper.  I would like to increase my rating from 4 to 5.  Since the proposed method is somewhat ad-hoc (shared concern among other reviewers), either experimental or theoretic analysis is important to understand when and why it works. However, I don't think these analysis are sufficient in current form.  After rebuttal: It is highly appreciated that the authors provide additional evidence in response to my reviews. My previous concerns about effectiveness and efficiency are addressed to some extent, however, this also means the original submission needs significant modification to address the concerns. I like the idea in general and the problem is well-motivated but it needs more work for a complete version. I would encourage the authors to further improve the quality of the paper. -------The rebuttal did not meaningfully addressed my concerns.Apologies to the authors for not providing a reference for my comment on approaches for reducing communication in graph-optimization methods being widely known in industry. GraphLab is an example (https://en.wikipedia.org/wiki/GraphLab)- Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin and J. Hellerstein. GraphLab: A New Framework for Parallel Machine Learning. In the 26th Conference on Uncertainty in Artificial Intelligence (UAI), Catalina Island, USA, 2010- Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin and Joseph M. Hellerstein (2012). "Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud." Proceedings of Very Large Data Bases (PVLDB).- Joseph Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, Carlos Guestrin (2012). "PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs." Proceedings of Operating Systems Design and Implementation (OSDI). ** after reading rebuttal**Thanks for clarifications and an improved version. I decide to increase my evaluation to 5.However, I think that the paper needs to take more content to illustrate the practical benefits of the proposed CSG frameworks, for the following reasons: - the framework is proposed based on empirical observations like 'intervening an image by e.g. breaking a camera sensor unit when taking the image, does not change how the photographer labels it', which is not mathematically rigorous; - the principles and assumptions are rather strong (though I understand one generally has to make assumptions in causality), and in practice it is not clear when such assumptions hold and how many applications satisfy these assumptions; - the interesting derivations and theorems are also based on the CSG framework, which means, if the framework is incorrect, then these results may fail; - the experiment settings are rather limited in the current version. I hope the authors to add further content in their next version, regardless of whether the paper gets accepted or rejected.Lastly, I still feel it a bit tricky to change the original formatting in the previous submission. Post-comments to the author's response:After reading the other reviewers comments and the authors rebuttal, I am still concerned with the novelty of the approach, experimental evaluation, and performance improvements over previous work. These issues are pointed out by the other reviewers as well. Hence, I will go with my original decision of rejecting the paper. Update:Thanks for all the authors' feedback and the make-up experiments as well as revisions. I appreciate the authors' efforts on solving the privacy problem and new input forms, but I really think the paper made no contributions to 'learning representation'.  The VAE and GAN-based model is not problem-specific or novelly designed for the task. Instead, it is a general framework for any reconstruction problems. I do believe this will make a strong submission to other conferences related to information. In this case, I will not change my rating. Update (2020-12-03):  Increasing score from 4 to 5.  Post-rebuttal response-----------Thank you for the detailed rebuttal responses.  The rebuttal suggests that there are not many other baselines against which to compare.  If that is true, I would want to see much more detailed analysis of the comparisons offered in the paper.  However, I am still missing details of the latent space generated by the task-unaware approach.  I will leave my score as it is. --- EDIT POST REBUTTAL ---I thank the authors for their response. I have read other reviews and answers too. I have appreciated the revisions (especially the proof for ReLu with biases). I am still a bit concerned by the applicability due to the assumptions on the architecture/data generation process,  but will not fight acceptance if other reviewers feel strongly about it (I changed my score from 4 to 5 to reflect this position). I understand that authors do not want to present unfinished/ongoing work, but mentioning the parallel with kernel methods (at least as a research direction) would seem fair to me. ---After rebuttal:Thank you for the revision and the clarifications."no one has made a clear connection between GCN and the heat kernel." For example, in (Klicpera 2019) section 2, the heat kernel is discussed as a special case."We dont simply combine SGC and heat kernel."Clearly, the only difference between the proposed method in section 3.4 and SGC is that the authors used heat diffusion as the spectral filter matrix, while SGC used a polynomial filter (the K'th power of the normalized adjacency matrix).Furthermore, the other reviewers raised similar works such as graph ODE, which further reduces the novelty of this work. Post-rebuttal feedback:Thanks to the authors for the provided extra experiments and clarifications. I feel that my concerns have been partially addressed, thus raising my score to 5. I still think that the proposed method is limited by the classifier, it's ability to capture a long-tailed distribution, which is not so easy to get when trained on imbalanced dataset. This significantly limits the applications of the proposed approach in real-life scenarios. The paper has also experimented only on artificially created imbalanced datasets, which contain small number of classes with the model being trained with the batch size higher than the number of classes. It would be beneficial to see how the model would perform in more realistic setup when the number of classes is significantly bigger than the batch size (e.g. iNaturalist or even ImageNet), to support more the claims of the paper. ----Updated:Thanks to the authors for the provided extra experiments and clarifications. Some of my concerns (e.g., how the batch size affected the performance) have been diminished, but I do agree with other reviewers that more baselines should be included.  ----------------------------------**After Author Response**Thank you for the detailed clarifications. I've raised my score to 5 since I found the random block skipping experiment quite interesting and surprising. However, weakness #2 and #3 are fairly important in my opinion and I don't think the response sufficiently addresses those weaknesses. I encourage the authors to show their method works on longer sequence datasets (where skimming intuitively makes sense) and compare against 1-2 efficient transformers using sparse attention, for better grounding the improvements in existing literature. -----------------------------------------------------------------------------------------------------------------------------------Update: I read the authors' rebuttal. The authors didn't address my concern "The study is conducted on a single dataset: ImageNet. It is unclear whether the conclusions hold for other datasets."  sufficiently. I would like to keep my original rating.  ## After Author ResponseMy concerns are only somewhat addressed by the authors' response. While the results on extra datasets are encouraging, I still think there is limited novelty in the proposed approach and some of the important details remain unclear. In particular, terminology seems to be used inconsistently, which results in ambiguity when describing variants of the model used in experiments. ----------Updates after rebuttal------------The author did not provide a revised version and additional experiments to address my questions. I keep my original score.------------------------------------------ Post-rebuttal:  I thank  the authors' effort for the improvement of the manuscript following the comments of the different reviewers. I think the overall quality of the paper has really improved. But, after many thoughts, I still think there is a limited novelty in this paper. I have increased my score to 5. But I can not recommend this paper for publication.   adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept. -------- Post rebuttal updateI appreciate the rebuttal made by the authors, as they have clarified my questions about the paper. It is also good that a new dataset and additional ablation experiments have been added to the paper, however, the empirical section still needs to be improved in my opinion. For example, more datasets and stronger baselines could be used, and I believe the authors could do it if they had more time. Therefore, even though I mantain my score, I encourage the authors to strengthen the empirical evaluation for future submissions.---- ==================== Post-rebuttal update:I would like to thank the authors for providing a detailed reply, which partially addresses some of my concerns. However, there are several issues in the reply and updated manuscript as provided by the authors:- The results on OGBN-arXiv are definitely useful, but there is no GAT-based baseline, which corresponds to the JAT's main comparison point. Therefore it is not possible to make strong claims about the method's effectiveness based on the following baseline choice alone.- The discussion of alternate structural learning approaches is not sufficiently detailed, and has no empirical backing to the authors' proposal. More experimentation is needed, in my opinion, to fully back the authors' claim of: "However, these methods may not appropriately determine which part, i.e., structure or node features is more important in the embedding space."- Lastly, I do not find that the authors have appropriately toned down their claims in the Introduction. Most critically, the authors write: "The experimental results show that JATs achieve the state-of-the-art performance". A lookup of the OGBN-arXiv publicly available leaderboard demonstrates that this is not the case: https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxivHaving state-of-the-art is not the most important thing, but it should not be claimed when it's not achieved.Overall I think the JAT paper has a lot of potential. For now I choose to retain my 'weak reject' recommendation, and hope the authors will take my comments into account for subsequent submissions. Post-discussion Update===========================Thanks to the authors for addressing my concerns.However, after viewing the other responses (especially the comments from Ekaterina Lobacheva) as well as the author's explanation, I think this submission missed out some quite important references. In addition, its contribution over previous works appears to be marginal after viewing these references. Therefore, I would like to lower my rating from 6 to 5. ================Thanks for the rebuttal. I appreciate the authors' efforts on polishing the presentation. It clears some of my concerns. So, I raise my rating from 4 to 5. However, after reading the rebuttal and other reviewers' comments, I still feel that the contribution of the paper is not that significant,  since the search space just includes a scalar and the empirical improvements are not strong (Table 5 to Table 8). I think the authors may consider further applying their method to multi-dimension HPO problems (e.g. joint searching initial learning rate, momentum and weight decay) to verity the effectiveness on more challenging configurations.  Thanks for your rebuttal. It solves some of my concerns. However, combining with rebuttal and other reviewers' comment, I think only choosing the initial learning rate is not that reasonable. It may be more convincing to me that the whole parameters are chosen together.  ##########################################################################Comments after the rebuttal period:Pros:First, it is totally acceptable to follow the notations and organization of Xu et al. (2017a), as long as the statements are clear and self-contained. The original submission failed on providing key details. The authors have made revisions to address this concern. Thanks!Second, I appreciate the extra experimental results and visualizations.Cons:However, the authors' responses do not fully address my concerns about the novelty, especially method-wise. I will raise my score to 5, but still recommend rejection. Update: "Noisy Student and Assemble-ResNet use without removing overlapping augmentations, yet they test on ImageNet-C." This is a bad practice and I should hope the authors of this paper only have experiments where there is a train-test mismatch (otherwise we're not testing robustness to distribution shift). POST-REBUTTAL:I thank the authors for the response and the revisions to the paper. I appreciate the authors' efforts towards the rebuttal. I am however left with some concerns which did not have a convincing resolution:* Regarding the comment on how the proposed analysis would look for stylized transforms, the authors say in the response that "...we dont expect that perceptual similarity is the only cause of improved corruption error, only that perceptual similarity is particularly salient for predicting generalization to dissimilar corruptions...". The work seems to be one-sided in this regard. If stylized transforms don't look perceptually similar to ImageNet-C corruptions but provide robustness, this counters the proposed hypothesis. It then becomes important to say where the proposed analysis is meaningful and where it is not. This seems to be lacking at this time in the work.* Regarding the robustness of MSD to the choice of feature extractor (as also asked by R1), the revised paper includes results on VGG as feature extractor (thanks to the authors for this), but uses a model that is finetuned for CIFAR-10. In general, perceptual similarity is studied directly taking VGG pre-trained on ImageNet - without finetuning on the target dataset. This leaves this question open, and makes one wonder if the latter features did not support the analysis.* The utility of Imagenet-C-bar as an additional benchmark to check the goodness of performance on Imagenet-C seems a bit convoluted. Would we need a Imagenet-C-bar-bar to check the goodness for corruptions that may be beyond perceptual similarity (such as stylized transforms)? This is not very convincing.Overall, I am still on the fence on this work (and retain my original decision at this time). I think the paper does present an interesting insight, but I am not very convinced it has been studied and explored comprehensively enough. I would have ideally preferred to give a borderline (neither positive nor negative) decision, and will not be disappointed if the work is accepted, considering the interesting insights it offers.  After reading author response ================================Thank you for the response.It's good to know that CRC outperforms random sampling on a couple other image datasets, but would be informative to see the results with respect to other AL techniques.I wasn't quite able to follow the logic of section 3.2, but this seems on the right track.The issue I see with using the final layer is not performance related, it's that it seems to throw out the theory you claim to be using. For instance, the fact that the performance dramatically improves is troubling and makes me wonder if the algorithm is working because of the theory or for a different reason. On the other hand, if you aren't able to make the theoretical justification more compelling, I think it would be fine to say the theory is just for inspiration.I agree there is no batch approximation when G=Q. I might be missing something, but the paper says "All experiments hereon use G = Q/10 considering the speed vs. performance trade-off".  Post Rebuttal:I'm totally fine with the topic, which is a very minor issue as mentioned earlier. My main concern was regarding the overselling of some presented results.From the technical side, Lemma 1 is a common approach in information theory/channel coding/privacy that one can linearly combine several designs by randomly sampling them, which was presented in the abstract as one of the main results. I expected something beyond a simple application, such as determining an interesting condition where this approach can be used, but the provided statement does not seem to hold after revision. As long as one can append additional messages to the query, the domain of Q could change completely, and the convexity of P_{Q|M}, interpreted exactly as stated, will not provide any guarantee. The condition actually needed by the authors is essentially random sampling does not hurt privacy, making the statement "random sampling can be applied when it can be applied", which is a bit trivial.I carefully double-checked the provided proof steps and found that an additional assumption was needed in the very last step, which further requires something like the metric \rho is invariant under permutation of Q, for which I didn't find in the paper, maybe I missed it. Adding this assumption totally makes sense to me, but requiring convexity+invariant under permutation is a bit over-complication, instead of just directly requiring random sampling does not hurt privacy. The theory part of the paper should be presented in the simplest possible way, without unnecessary complication.  The authors do have a novel contribution in terms of experiments. But the contribution and novelty in the formulation are rather limited given that the considered components have been studied either separately or jointly in cited works.For the experiments: I have a similar concern as mentioned by Reviewer 1, which is about the generalizability issue. A natural explanation of the presented results could be that the hyperparameters in the implemented solution are tweaked for datasets like MNIST or something similar. It is not clear if it would fail for more general datasets similar to synthetic Gaussian. I've read the authors' responses to review 1 and the revised paper. Overall, there is no more direct evidence of generalizability. But this issue itself is fine as it could happen in many other works, so I won't consider it as my main concern. Besides, the author has adequately addressed the issues of the missing conditions in the Theorems. Conclusively, my recommendation remains the same for the above reasons.  **Post Rebuttal**I thank the authors for the extensive experiments and answers. Unfortunately, I still feel that the contribution is rather marginal. I keep my score unchanged.---