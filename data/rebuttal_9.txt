 ### DecisionThe authors answered all my questions. My decision stays the same. ----------------------------------Edit after reading other conversations.I do think the other reviewers make some fair points. I've adjusted my score to a 9, though, and I'd very much like to see the paper accepted. Here is my thinking on a couple points that led to this score adjustment.AnonReviewer2's SimPLE-like ablation request: If I'm understanding this correctly, the reviewer would like to see the policy model trained on reconstructions instead of the latent space (or maybe would like the world model trained to future predict in pixel space? not completely clear to me). To me, this would be a potentially insightful ablation, but I think not a dealbreaker that they do not. Two points on this:(1) If I am understanding the paper and conversations, SimPLE significantly underperforms in the metric (Atari "end performance" under certain normalizations) that the authors care about (and is a fairly established metric). I, then, don't see a *particularly* strong motivation for careful ablations of the method.(2) Pixel-based future predictions generally perform poorly, and this is I think fairly widely thought to be a strong contributor to the failure of model-based approaches. Again, it would be nice to see that happen here (and it might be insightful to see the quality of the frame predictions) but I think there is a reasonable expectation that this would work poorly. AnonReviewer1's thoughts on the value of this sort of work in ICLR: I see your point, but I personally think there's great value to this sort of work in this sort of venue. End performance on Atari has been (for better or worse) an important baseline for the field, and the creation of performant model-based approaches has been a central question for several years, now. The proposed improvements over DreamerV1 (which has seen fruitful applications in other work) might be simple, but DreamerV1 did not work well on this baseline, and this does. I think that we far too often get excited by complex new methods, often evaluated with novel and poorly understood baselines and metrics, only to drop them as time goes on. That the innovation is a simple one should make us excited to try it in other applications.I also think that their discussion of evaluation metrics is very useful -- we continually need more careful conversations about the right ways to measure success. So, in short, the paper might be a technically simple innovation, but it puts forth strong evidence that the method is useful. Revision:The authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.I believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work. Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence "Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied."  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase "in the settings studied" replaced by "for an alexnet architecture".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.  Update: I have read the authors' response. My current rating is final. UPDATE: T`he authors' response addressed all my remarks. I've increased the score. ##################################################################Final update: Authors addressed all my comments in the rebuttal making significant improvements to the revision. I've increased my score.  Post Rebuttal:I thank the authors for their detailed and thorough response. All my questions and concerns were addressed and I appreciate the discussion on end-to-end learning as well as the Transporter + GNN experiment. I am happy to maintain my original rating and recommend acceptance. -------Revision---------Thank you for the response. I have not changed the original review. edit:  the authors nicely revised the submission, I think it is a very good paper. I increased my rating.----- I extend this review based on the replies.  One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise.  First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community.  (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies.  I do hope the dataset will be made public.  This is a major reason I gave a high score.)  Second,  the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there,  the impressive performance itself is an impact to the field.  This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. --REVISION--I would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2). UPDATE on 12/7/2020:I read the other reviews and the authors replies.I hold onto my score and recommend the paper to be accepted. Here are some points I would like to highlight.1. Anonreviewer1 suggests the work is incremental, yet theres no citation to previous work on protein data augmentation. This is in fact the first major work tackling this issue.2. Anonreviewer1 points out the authors did not use a SOTA model. This is true, but the model they are using is little more than 1 year old. Moreover, while I agree with Anonreviewer1 that different models need to be used to see how generalizable the data augmentations are, I dont see this as a reason to reject a paper that is tackling an orthogonal issue of what modes of protein data augmentations work.3. Both anonreviewer1 and anonreviewer3 are pointing out that different augmentations seem to have worked best for different tasks, and hence the results are not strong or useful. But how would we know this if no one had even tried it? The authors found the results as such and thats an important contribution to the community's knowledge.4. I am not seeing the problem of data augmentation done on the validation set that anonreviewer1 is suggesting. The test set results should not be affected in a corrupt way in this case.5. I disagree with anonreviewer3s comment that the augmentations are with unreasonable intuitions. I dont think thats an objective reason to reject a paper.6. I agree with other reviewers that the contact prediction task should have been evaluated with whatever dataset that was available. Even then, the other 4 tasks in my opinion are enough as a first attempt at this area.Overall, investigating the protein data augmentation regime is something that is not fancy like building a new model or beating the latest SOTA yet a necessary task for which we should thank the authors. There are of course problems that will indeed need to be addressed in further work but there are also important insights in the paper. ----After reading the other reviews and the authors' comments, I still think the paper is excellent and should be accepted. ## Update after rebuttalAll my concerns have been addressed, including the possible alternative explanation of the experimental results. I strongly recommend the paper to be accepted. --- Update after discussion ---After looking at the concerns raised by the other reviewers and the author responses, I have the following comments:1. I think the that authors have more than adequately addressed the concerns raised by reviewer 4. With that said, I agree with reviewer 4 in that the link between theorem 1 and the proposed objective and its approximation is not made sufficiently clear in the text. 2. I whole-heartedly agree with reviewer 1 that a clear and concise list of the assumptions made would improve the paper.Overall, however, I think the paper should still be accepted and will keep my original rating. Post rebuttal comments=========================I thank the authors for their responses. I encourage the authors to continue the line of work on replacing the random sampling of NAGO. Given NAS-BOWL surrogate, one can do Thompson Sampling instead of random sampling. On the MKL side, the same weights for all the kernels might be the cause of worse performance. I would also encourage the authors to verify that. Nevertheless, those are minor comments and I still think this is an important work to bridge BO and NAS. I will keep my score. **NOTE: updated score after seeing author replies and updated draft. I believe that this work is exemplary in terms of being careful about baseline construction, something that is unfortunately too often overlooked in our field. Additionally, it rigorously highlights another important point that I believe many often overlook, that "there are now enough optimizers"; community effort should be diverted from introducing small variations around Adam and instead invest focus on more meaningful improvements in scaling machine learning optimization. I do still believe that ImageNet and a larger transformer experiment would be extremely valuable to add to a later version, and hope the authors can eventually secure the computing power to add these.** post-discussion:I read the author's response and other reviews. I still think this is a very strong submission and would like to see it accepted.I encourage the authors to add a section discussing their generalization of SW theorem + Theorems 31,32 as the authors suggested. another small point: I couldn't easily find the definition of "stable by concatenation". ********Post-rebuttal. The rebuttal didn't raised any concerns and made the paper even stronger, thus I am keeping my score. Update:------------After looking at other reviews and author rebuttals to all reviews I am raising my grade.  ==== Updated Review Following Rebuttal ====The authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. (Score raised from 8 to 9 after rebuttal) # EDIT: UpdateThank you for the rebuttal.Despite the skepticism of the other reviewers, I still think that this is a valuable, thorough paper of high quality.  ==================================Comment after author responses.On reading the author's responses to my (and other) review comments, my recommendation remains unchanged. This is a solid piece of work.  ##Updated Review##I'd like to thank the reviewers for their responses, and for updating to the much clearer naming scheme for the different methodologies!  Much easier to follow with those names.I maintain that this is high quality novel work that contradicts a widely held belief within the field and as such is a clear accept.