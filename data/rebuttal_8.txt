 UPDATE:I have read the author feedback and other reviews/discussions. I have updated my rating to 8 from 7 reflect it. =-=  Comments after author discussion(Minor) concerns have been addressed.  Thank you for the revisions. ---- After Rebuttal ----After reading other reviews and the rebuttal, I stay at my current score. Given that the paper does not contain much analysis about "why", the paper is about an empirical discovery of a mode that is critical in getting good performance. I think this kind of discovery paper is also important to share with the community. # Rating and comments after the rebuttalThe authors addressed my concerns in their feedback and the revised manuscript they have provided. In particular, I find the claimed contributions much clearer now. In my view, they have also suitably addressed the concerns raised in the other reviews. As a result, I increase my rating to 8 as I think that this work is interesting, novel and impactful. After rebuttal:I appreciate the authors' detailed response to my questions, which largely addresses my previous concerns. It's very pleasure to reviewing this interesting, innovative and well-written paper.A clear accept. ****Update after rebuttal ****I am increasing my rating for the paper as they did all the experiments I had asked for and updated the paper accordingly.************************** **Post-rebuttal update: the score is increased from 7 to 8.** POST-REBUTTAL:I thank the authors for their response. I am happy with the responses to my concerns/questions, and retain my decision of Accept. -------- Post-rebuttal edit -------The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines. I believe that the paper is strong and makes a significant technical contribution to the field of CL. Hence, I recommend acceptance and I am updating my score to reflect the same. Edit: After reviewing the author response, I will keep my score as it is. I believe the paper should be accepted.  __UPDATE__Thanks for the incredibly detailed response! I've raised my score to a 8.  I do in general quite like the paper, and the responses here are thought-provoking. I'm not sure I'm totally convinced by the WSC results comparing CALM the classifier to T5 the sequence scorer. Not sure if it's an apples to apples comparison...but I'm not sure there's a straightforward setup for this, and perhaps it starts to get beyond the scope of what's being presented here.  **Update: I thanks the authors for their answers and revised version and keep my positive rating.** **Post rebuttal**With consideration of the authors' responses to reviewer questions and revisions to the submitted work I have changed my rating to _clear accept_. ## Post rebuttalI thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas. My previous rating still applies. ## Update after RebuttalThanks for the detailed replies to my questions and comments. I think the paper has been improved substantially and I have increased my rating. Congratulations on the good work! Update following answer:Thanks for your detailed answer, which confirms me in my initial assessment.------ UPDATE: I thank the authors for their detailed feedback and updated paper. I'm a bit more in favor of acceptance of the paper now.  __________________________________________________________________________________________________________________________________________**Updated review** following revisions:- A5: For future reference, I believe the kinetics dataset is now downloadable from a google drive folder: https://github.com/activitynet/ActivityNet/issues/28#issuecomment-602838701- A6: I am not convinced by your hypothesis in Appendix (Section E) as to why UCF-101 is so challenging. Surely if UCF-101 is not diverse enough, then a model based on stylegan should perform well on it. In-fact I would argue that it is the opposite, that the UCF-101 is very diverse for such a small dataset.  - Observe that the datasets compared against UCF-101 in Table 7 are not that diverse, there is a dataset of just faces (FaceForensics), of just sky time-lapses and of just dogs. These are all uni-modal datasets. On the other hand, compare just a subset of the 101 classes in UCF-101; Horse Riding, Military Parade, Baseball Pitch, Billiards Shot, Brushing Teeth,...  - I would argue that the limitation of your approach (and that of DVDGAN [Clark et al., 2019]) on this dataset stems from the fact that UCF-101 is a small but very diverse dataset. On average, just over 100 samples per class.Overall, the authors have adequately addressed my questions and concerns. They also appear to have done so for all the other reviewers too.My recommendation is to **accept** this work for publication to ICLR 2021.I recommend that the authors open-source their code and pretrained models.  EDIT: After reading the other reviews, responses, and thinking more about the issues raised and resolved, I'm increasing my score to an 8.--- .### Post Author Response Period UpdateMost of my concerns have been addressed by the additional experiments and updated language in the latest revision. I believe the techniques and analysis presented here for assessing reuse could be an important step between observations and explanations for the failure of NNs to generalize systematically. I have raised my score accordingly Post-discussion:I thank the authors for responding swiftly to all of my main concerns and for providing additional experimental results. I'm happy that authors have promised standard deviations and comparisons to ResFlow in the newly updated experiments for the camera-ready version, and I have adjusted my rating accordingly. I think this paper makes a very solid contribution to the normalizing flows literature. ============================================================**Update:**A few  suggestions on the latest version:* Adding pointers from Table 1 and 2 to their full equivalents in the Appendix would be helpful.* The Appendix ablations are now thorough and exhaustive, but parsing them and digesting what they represent is a little tricky. Adding a one-line summary (much like in Section 5.`1) would be very helpful here. For example, talking about in how many pairs FIPP+IP outperforms Proc.+IP,  FIPP+IN outperforms Proc.+IN, FIPP+IP outperforms FIPP+IN.and a few comments:* The sheer number of experiments that the authors have performed in general (and in particular in the relatively short time period of this rebuttal) is impressive, and is in my experience indicative of an extremely good, well-designed and easy to iterate upon framework/code-base. If my guess is correct, I urge the authors to release their code if possible, because I believe it would greatly help anyone working in this space (or even consuming BLI's output in a downstream task).* It is also very heartening to see how much R1's suggestion helps improve the 1k case! However, because the efficiency and the lack of need of a GPU are big selling points, the drawback of adding self-supervision (10x the time, need for a GPU) might be a good caveat to add in Section 6.1 as opposed to keeping it till Appendix A.Overall, I would like to thank the authors for their very detailed and thorough response,  and for taking into account so much of all the reviewers' feedback to make the paper clearer with much more comprehensive ablations. The paper, its techniques proposed and their performance and efficiency, and the detailed experiments it conducts will be helpful for both the field of BLI and other fields relying on it. In view of this, this paper is a clear accept in my opinion, and I raise my score from 7 to 8. --I am changing from "marginally above acceptance threshold" to "clear accept" after reading the response and thinking about the paper a bit more.  I acknowledge that the difference from previously published methods is not that large, but I still think it has value as it's getting quite close to being a practical method for generating fake training data for speech recognition. Post rebuttal update/comment:I thank the authors for the revision and have updated the score (twice!)One genuinely perplexing result to me is that the method behaves better than random pruning, yet after selecting the salient neurons the weights can be reinitialized, as per the rebuttal:> # Initialization procedure- It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. Given (variance scaled) initial weights, SNIP finds the architecturally important parameters in the network, then the pruned network is established and trained in the standard way.First, there is work which states quite the opposite (e.g. https://arxiv.org/abs/1803.03635). Please relate to it.Fundamentally, if you decouple weight pruning from initialization it also means that:- the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization- the second and higher layers will be pruned somewhat randomly - even if the connections pruned were meaningful with the original weights, after the reinitialization the functions computed by the neurons in lower layers will be different, and have no relation to pruned weights. Thus the pruning will be essentially random (though possibly from a very specific random distribution). In other words - then neurons in a fully connected layer can be freely swapped, each neuron in the next layer behaves on al of them anyway we are thinking here about the uninitialized neurons, with each of them having a distribution over weights and not a particular set of sampled weights, this is valid because we will reinitialize the neurons). Because of that, I wouldn't call any particular weight/connection architecturally important and find it strange that such weights are found.I find this behavior really perplexing, but I trust that your experiments are correct. however, please, if you have the time, verify it. edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.I also agree with the introduction of the Chi vector, and with the use of the term of "photometric BA", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.--------------- --------------------------------------------------------------------I have read the authors' responses and almost all my concerns have been well addressed. So I increase my point to 8. UPDATE 2 (Nov 19, 2018): The paper has improved very substantially since the initial submission, and the authors have addressed almost all of my comments. I have therefore increased my score to an 8 and recommend acceptance.------------------------------------------------------------------------------------------------------------------------------UPDATE (Nov 16, 2018) : In light of the author response, I have increased my score to a 6.------------------------------------------------------------------------------------------------------------------------------ Final score: The authors have addressed my concerns in the rebuttal. I believe this paper tackles an interesting problem, and the experiments are good enough since this is one of the first papers that tackle this problem. So I keep the initial score. EDIT: The authors have addressed my concerns and I have raised my score. Thanks for your final answers and changes. I increased the rating of your paper to 8. ** review score incremented following discussion below ** ---------EDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning. == Post-rebuttal Comments ==I am raising my score from 7 to 8, as author responses addressed my comments well (especially answer to W1 and the Figure 13) than expected. Updated review--------# SummaryThis work proposes an approach for model-based optimization based on learning a density function through an approximation of the normalized maximum likelihood (NML). This is done by discretizing the space and fitting distinct model parameters for each value. To lower the computational cost, the authors propose optimizing the candidates concurrently with the model parameters. Each model's distribution is encoded as a neural net outputting a scalar which is then encoded using a thermometer approach using a series of shifted sigmoid. Candidates are optimized based on the average value of the scalar of each model evaluated using parameters obtained from an exponentially weighted average of its most recent parameters.# Reason for scoreThis work proposes a reasonable approximation to an interesting estimator and demonstrate it is capable of achieving good consistent performance. This is likely to be of interest to the community and, as far as I'm aware, is sufficiently novel. Given that I see no noteworthy issues and all of my major concerns have been addressed, I don't see any reason for rejection. I strongly support acceptance.# Pros* Using estimates of the NML for model-based optimization is an interesting idea.* This work shows that the NML can be successfully approximated with a relatively coarse discretization and that both the optimization of the candidate and the various model parameters can be optimized in tandem. This suggests that this type of approach is viable and possibly warrants further investigation. after author feedback =======After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications.  The clarity can still be improved, but this might be more of a style matter.  The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ````   accept. ====== Update 12/12/18 ======Thanks for your notes in reply. I'll just add that if the dataset can be extended to slightly greater complexity either for this version or for submission to a subsequent venue, it would be impactful. Simple extensions could include scenes with multiple flowers and classes where the explanatory factor is tricker to uncover. For example, a dataset could be created with scenes of three flowers: two of one color and one of another color, with the class determined by the color of the lone flower. The correct explanation (the color of the lone flower) is still clear, and it would be great to see if the proposed LASSO approach (or a future approach) could correctly identify those pixels. Revision:The authors have taken my advice and addressed my concerns wholeheartedly. It is clear to me that they have taken efforts to make notable progress during the rebuttal period. Summary of their improvements:- They have extended their methodology to handle multiple strokes- The model has been converted to a latent-space generative model (similar to Sketch-RNN, where the latent space is from a seq2seq VAE, and SPIRAL where the latent space is used by an adversarial framework)- They have ran addition experiments on a diverse set of datasets (now includes Kanji and QuickDraw), in addition to omniglot and mnist.- Newer version is better written, and I like how they are also honest to admit limitations of their model rather than hide them.I think this work is a great companion to existing work such as Sketch-RNN and SPIRAL. As mentioned in my original review, the main advantage of this is the ability to train with very limited compute resources, due to the model-based learning inspired by model-based RL work (they cited some work on world models). Taking important concepts from various different (sub) research areas and synthesizing them into this nice work should be an inspiration to the broader community. The release of their code to reproduce results of all the experiments will also facilitate future research into this exciting topic of vector-drawing models.I have revised my score to 8, since I believe this to be at least in the better half of accepted papers at ICLR based on my experience of publishing and attending the conference in the past few years. I hope the other reviewers can have some time to reevaluate the revision. ====Post-rebuttalI've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly. After reviewing the author response, I adjusted the rating up to focus more on novelty and less on polished results. # HIGH-LEVEL ASSESSMENT (UPDATED)After reading the author rebuttal and going through the revised manuscript, I believe the authors have successfully addressed the vast majority of concerns I had about the original version of the paper. Based on the current version of the article, I lean strongly towards acceptance and have modified my score accordingly.# STATE OF PREVIOUSLY RAISED MAJOR POINTS1. In my original review, I raised issues regarding the way LEAP was motivated and derived; an opinion also voiced by Reviewer 2. I believe Section 2 of the revised manuscript has greatly improved in terms of clarity while simultaneously being more general.I apologise for the mistaken sign in  in the subsequent analysis. In hindsight, I should have definitely caught that error based on the very unintuitive conclusions that ensue!. The fact that LEAP reduces to Reptile when minimising the expected energy of the "non-augmented" gradient flow makes perfect sense and helps understand what LEAP's "place" is alongside MAML and Reptile. The authors have also extended LEAP to minimise either the length or the energy of the gradient path, rather than minimising only the energy. This possibility was loosely mentioned in the original manuscript, but not implemented. As pointed out in their rebuttal, minimising the length of the gradient path instead of the energy implicitly "normalises" the magnitude of the gradient w.r.t. the initialisation  across tasks (Eq. 8), which might make LEAP most robust against heterogeneity in the scale of task losses.The new ablation studies included in Sections B and C of the Appendix are also a great addition to study/justify empirically some of the more heuristic aspects of the paper.2. The original review also raised some concerns regarding Theorem 1 and its proof; a point also raised by Reviewer 2.The statement of Theorem 1 and, most importantly, its proof, have been almost entirely rewritten. To the best of my knowledge, I believe the revised version is correct (potential minor inconsequential caveats described below), and is now much clearer and easy to follow.3. Besides carrying out the new ablation studies, the authors have introduced two additional baselines in Section 4.2 and now report aggregated results for 10 different seeds in Section 4.3.I still believe that having included additional baselines also in Sections 4.1 and 4.3, as well as evaluating LEAP in a "less favourable" few-shot learning scenario, could have further strengthened the paper. Nevertheless, given the time (and possibly compute) constraints, the revised manuscript also improved considerably in terms of experimental results and, most importantly, already provides sufficient evidence that LEAP can outperform existing approaches when tasks are sufficiently diverse. (I have read your author feedback and have modified my rating according to my understanding.) ------------------------------post rebuttal-------------------------------------------------------------Response to the authors' response----------------------Thank you for the hard work in responding.I have read other reviewers' reviews and the response from the authors. The authors have addressed most of my concerns.I believe this paper deserves acceptance. As we know, variants of efforts have been made to improve NAS's effectiveness since 2016, and a great process has been reached. Despite the high expectation and solemn devotion, NAS's effectiveness is believed to be still low. This is inconsistent with many pioneer researchers' expectations four years ago, in which NAS is expected to be another revolutionary technique similar to 2012's deep learning. Currently, there are many NAS papers published every year. But their effectivenesses are unclear due to the lack of ranking correlation analysis. Differently, this paper comprehensively analyzes the architecture rating problem, which provides a timely analysis of the current NAS's ineffectiveness caused by inaccurate architecture rating. I think this paper can attract the community's attention, encouraging the community to pay attention to the architecture rating in NAS, especially when reviewing a NAS paper. Therefore, I recommend an acceptance for this paper to promote the analysis of the NAS's architecture rating problem.I agree with R2 that Yu et al. have proposed a similar idea (I assume R2 refers to "Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, Mathieu Salzmann, Evaluating the Search Phase of Neural Architecture Search"). But the analysis in this paper is more comprehensive than Yu et al.'s article. Many findings are new (at least they are not in published papers).I agree with R4 that the authors did not form a coherent logic flow to present these empirical findings, and the paper was similar to a technique report. However, many important articles, e.g., "Designing Network Design Spaces," "Exploring Simple Siamese Representation Learning," "Is Faster R-CNN Doing Well for Pedestrian Detection?" are also technique-report-like.I appreciate R1 for his devotion to finding similar observations in his experiments. I believe these observations are important and deserve publication. I agree with R1 that removing any operation leads to a smaller search space and a higher ranking.In summary, I will keep my rating as an acceptance. Undoubtedly, I also believe the comments from other reviewers can benefit the improvement of your paper. ### After-rebuttal commentsThe authors andwered my questions. My decision stays the same ---------------------------------------------------------------------------------------------------Satisfied with the response, will keep my score the same. [EDIT AFTER DISCUSSIONS] I thank the authors for their answers to my comments. Having read the various threads, I confirm my score and see interesting work in this paper.[/EDIT] ** Edit after author response **I've raised my score after the response from the authors regarding my concerns with the theory. ** Edit after further author experiments **I think the paper is now even stronger given the inclusion of additional experiments into the failure cases of EPIC. I have upgraded my score accordingly.  ####UPDATE: I have reviewed the author response and the revisions. All of my main concerns were addressed. I believe that these improved the paper further, and while they do not change my rating, the paper is still a clear "accept" in my view. ### UPDATE AFTER THE REBUTTALMany thanks to the authors for revising the paper, the new material is comprehensive and does a lot to address my questions about the feasibility of scaling up the model. I would know the authors are short on space, but I would request that the current Figure 2 be tweaked in some manner to makes the legend in Figure 2c bigger. The current legend is nearly illegible, and the difference between the model trained on size 10 vs 16 is important. Independently of this, I have raised my score in light of these new additions. Author response: the authors clarified my questions, so I maintain my recommendation for acceptance. **edit, after revisions**  As mentioned in discussion below, I think the addition of intervention experiments makes the effects very clear and strengthen the paper.  I revised my score from 7 to 8. After rebuttal: Thank you for clarifying details. I maintain my rating. Showing that the inductive bias from Z-order curve and shuffle-exchange allows generalization to larger sizes is very interesting. Overall I vote for accepting. ***Post-discussion period comments***The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers. Based on the latest revised version of the paper, I am increasing my score to 8 (from 7). I believe this paper is worthy of publication in proceedings of ICLR 2021 and I recommend it as such. POST-REBUTTAL: I have updated the score from 5 to 7 following the clarifications from the authors. *Post Rebuttal*Thank you for your responses. I am bumping up the score. Post discussion review----------# SummaryThe authors present evidence that the approximate rank of the features is correlated with the learned policy's performance and that this rank shrinks when using bootstrapping. They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks. Finally, they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases.# Reason for scoreThe authors isolate an interesting phenomenon and present some compelling empirical evidence. This is interesting work and I have have no doubt that it is of sufficient quality for publications.# Pros* The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent, a critical aspect of many RL methods. Using neural nets to learn (Q-)value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is. As a result, this subject of this work is quite important and likely of great interest to the field.* The experiments are well designed and relevant to the main thesis. The empirical results are well presented and easy to understand.# Cons* After a very productive and enlightening discussion with the authors, the only noteworthy issue is that this paper contains too many contributions for the format making some of them hard to appreciate. A more focused in depth dive into a subset of the theoretical contributions might have been preferable and possibly provide more insight.# ConclusionI strongly support the acceptance of this submission. After discussion with the authors and resulting updates to the paper, I don't see any reason for rejecting this paper. All of the major concerns from my initial review have been addressed. Udpate: with the additional experiments and corrections in the paper, I believe the paper is in good shape and contributes to the literature in the field. Hence it should be accepted. ##########################################################################Post rebuttal:I have read authors' rebuttal as well as the newly added contents.I appreciate the time and the details that the author goes in when addressing my comments. I am convinced that this method can be used even without the OPR module based on the ablation study. Overall, I think this is a high quality paper that deals with decentralised Multi-agent safety-awared learning. And learning the safe certificate is the shining point of this contribution. I recommend for acceptance and give my final score at 8.   ## Post-rebuttalThe rebuttal was convincing (see my comment below) and addressed my concerns. I therefore increase my score to 8. -------Post Rebuttal: I thank the authors for clarifying on my questions and updating the manuscript.  ##Updated Review##I'd like to thank the authors for addressing each of the points I made in my review and taking the time to include material that answers many of the questions I had.  This paper is in my mind a novel and interesting submission and a clear accept. Edit: I have updated my rating based on author response. ---Following the author's reply, I've raised my score from a 6 to a 8. [EDITED AFTER DISCUSSION: My concerns are largely addressed and the paper is now stronger. I very much hope to see it at the conference, and have updated my review and rating accordingly.] POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. Most of my concerns were clarified and I still think the paper should be accepted. However, I agree with Reviewer 4 that additional experiments would be good to better tease out the reasons for this method working. -------------Revision--------------I disagree with most of the points that AnonReviewer3 raised (e.g., second layer fixed is not hard, contribution is limited). I do agree that the main weakness is the number of neurons.  However, I think that the result is significant nonetheless. I did not change my original score. Thank you for your reply. I see no issue with the analysis failing to account for large , it obviously would be nice if it could but alas it does not.I think that this is a well-written paper with an interesting, novel contribution to parallel optimization, and I stand by my opinion that the paper should be accepted to ICLR. EDIT: On the basis of revisions made to the paper, which significantly augment the results, the authors note: "the call for papers explicitly mentions applications in neuroscience as within the scope of the conference" which clarifies my other concern. For both of these reasons, I have changed my prior rating. --REVISION--After reading the authors response and looking at the new version of the paper I decided to increase my score. The paper tackles very important problem and I strongly believe it should be presented during the conference. ### Update after rebuttalThank you for the clarifications.A couple minor comments:- regarding theorem 7, my comment was that it would be useful to include the conditions on eta in the theorem statement in the main text (though I do not feel strongly about it)- regarding "misalignment" and the relationship between the random effects model and the source condition, I appreciate the improved explanation of this analogy, but I still find that the last paragraph in section 3.2 could do a better job at providing the right intuition (skimming through the Richards et al. reference pointed out by R4 gave me a better intuition).   ---------------------------------After reading the responses from authors, I have clearer noticed some important contributions in the proposed methods:1) A novel regularization function with a scaling factor was introduced for improving the capability of binary neural networks; 2) The proposed activation function can enhance the training procedure of BNNs effectively;3) Binary networks trained using the proposed method achieved the highest performance over the state-of-the-art methods.Thus, I think this is a nice work for improving performance of  binary neural networks, and some of techniques in this paper can be elegantly applied into any other approaches such as binary dictionary learning and binary projections. Therefore, I have increased my score. Thank you for your further updates which I think improve the paper further.  I have consequently decided to increase my score to an 8.  One final question that would be good to answer in the camera ready if you can would be to try and see if you can establish what dependency structure (if any) for the generator is compatible will the learned encoder (i.e. is there any generative model for which the learned encoder structure is a faithful inverse).  This is not a critical point though and not something I would expect to be successfully addressed during the discussion period. Thank you for addressing all the comments.- I am satisfied with the explanation from the authors regarding Theorem D.4 and the revision adequately addresses most of the comments.- Regarding differentiability, it is fine to retain the original definition of differentiable games while your result requiring thrice differentiable losses. However, the justification you provided in your response needs to be added in to the paper and preferably as a note immediately after Definition 1 to avoid the misinterpretation. - Another question Lemma D.7 - Towards the end of proof, the application of Cauchy Schwartz is not clear. You show that ||-\alphaX(\theta)|| = \alpha^2||X(\theta)|| < c. However, it is not clear how the equation below that holds? Specifically, why does the negative sign in the first fraction disappear and somehow the overall term becomes >= \alpha||\Psi_0||/||-\alphaX||.It is recommended that the authors proofread all the proofs and equations and try to use notations and show derivations without making them confusing for the overall presentation. It would also help to number equations for quick reference.Overall the paper presents strong theoretical results with adequate empirical evidence. It certainly addresses an important problem of trade-off between convergence and stability in multi-objective settings and I have updated my score from 6 to 8 to strongly support it for acceptance. ########## Updated Review ##########The author(s) have presented a very good rebuttal, and I am impressed. My concerns have been addressed and my confusions have been clarified. To reflect this, I am raising my points to 8. It is a good paper, job well done. I enthusiastically recommend acceptance. ################################ post-rebuttal======thanks for the feedback, I update my rating of the paper Post Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8. Revision----------Thanks for taking the comments on board. I like the paper, before and after, and so do the other reviewers. Some video results might prove more valuable to follow than the tiny figures in the paper and supplementary. Adding notes on limitations is helpful to understand future extensions. Update - Thank you for the response and updates to the paper -- EDIT: Thanks for the nice feedback during the rebuttal. I am happy to stay with my rating of clear acceptance. **Update after the author's response**: The authors have answered my questions during the rebuttal period and I am satisfied with the response. Hence updating my score: 7 $\to$ 8. Update after rebuttal------------------I am very interested to see the results of the experiment mentioned in the rebuttal.  Although you mention that gameplay dynamics and ability to interact with objects is crucial, I still feel that this isn't as cleanly demonstrated as it could be without addressing these confounds.  However, in retrospect I also realize that my initial score suffered from tunnel vision on that single issue, which is important to several claims in the paper, but is by no means the only contribution.  The paper is ambitious in scope, novel, and well written, so I have increased my score. **Thanks to the authors for the response. The addition of a human study and CoCon+ has made the paper substantially stronger, as it resolves most of my concerns. The authors provided plausible explanations for the remaining questions. The paper should now be considered a clear accept.** * Update after reading author(s)' response: Thank you very much for the detailed answers to my questions (as well as the other reviewers' comments/questions).  I have upgraded my score; wishing you all the best.  ----Post revision update: my concerns have all been addressed. Update after author response:I thank the authors for their detailed rebuttals as well as for engaging with the reviewers. It'd be worth adding a small section on how to scale this method for higher dimensional action spaces (bullet 2 in author response). Comparison with Metz et al (2017) is valid and appreciated. Based on this, I am happy to increase my rating from 7 to 8. *********** post rebuttal comment ************Thanks for the comments and clarifiactions.Just to add that: I agree that Rosenfeld et al (2020)'s work on general poisoning is concurrent to yours (as that part is expanded in sub-sequent updates to their Arxiv, not the original ICML paper).  And anyway the bounds of this submission are stronger. Yet, I think that a brief discussion/comparison with this aspect of the  Rosenfeld et al (2020) paper is helpful to the reader. -----After rebuttal:Thank you for the revision and the clarifications.It is now clear that this work actually proposed two different neurons: the horocycle neuron defined on H^n and the Position neuron defined on R^n (removing one point). After the revision, they are proved to satisfy the universal approximation property. They share similar level sets (although the density of the level sets is different). It would be interesting to see their relationships through formal arguments and a more careful empirical comparison.This work needs background knowledge in hyperbolic geometry and may not be easy to read at the beginning. That could explain the criticism regarding clarity. Overall, I believe this paper developed important tools along the line of hyperbolic deep learning and still recommends strong acceptance. **Post-rebuttal update**The authors addressed most of my concerns during the rebuttal, added relevant discussions, experiments on Montezuma and clarified the reported evaluation metrics. I raise the score from 7 to 8. **Updates from Author Feedback**While I would have liked to see a draft with the changes, I feel reasonably sure the authors will improve Appendix C to make the statements mathematically precise. I am confident in the statements and proofs. While the presentation can be verbose and casual, I think it is justified to increase accessibility, so long as the proofs and statements are formal and precise. Based on Author responses I have increased my confidence. ---Thanks for the response. I've increased my confidence.  **AFTER REBUTTAL**I would like to thank the authors for their rebuttal and all updates. The paper is related to other ideas in the literature, however, it constitutes an interesting contribution to the field. I appreciate all new results and discussions. I keep my original score. ** Edit after author response **The authors have addressed the concerns I had so I've increased the score accordingly.  =================================**Update:**Thank you for you responses and clarifications to mine and the other reviewers' comments. Thank you also for baking so much of the reviewers' feedback into your latest draft, in particular by improving how the comparisons are presented in the abstract and the rest of the paper, by updating Table 2 to make it clearer and cleaner, and by adding experiments and analyses in Section 4. As a suggestion pertaining to the latest draft, I agree with R5 that a consequence of the additions to Section 4 have made it a little crammed, although I certainly understand that the constrained spaced forced the authors to make this trade-off. Overall, I continue to maintain that this is a good paper with novel ideas that are well-justified by experimental results and analyses, and would like to reiterate that I believe that this work is a clear accept. Post rebuttal: I have read the authors responses with to my 4 questions, and appreciate how detailed and honest they are. They satisfy my concerns. --------------------------## Post-rebuttal UpdateThe authors have shown new experiments on icy environments that show good results for the proposed method (DARC). This directly addresses my point (and recommendation) about trying out experiments similar to the aggressive driving on icy road example that was mentioned in the introduction. Having read through the other reviews and responses by the authors, I feel that most major concerns have been addressed. As such I am inclined to increase my score from 7 -> 8, recommending acceptance of the paper and entrusting the authors to include the new experiments in the main paper.A minor note: My second point under "Other issues/comments" section was not answered in the rebuttal. I hope the authors can clarify this in the future either in the main paper or appendix. I stand by my initial review that this is a strong submission, and having read through the other reviews and author responses, I am raising my confidence level as well (I think I have a solid grasp of this work's potential import). I disagree with critiques of the paper's novelty and practicality -- I think it provides new insights into OOD problems with substantive theory (not common) and provides actionable insights to boot. Also, the the revised manuscript is much improved. I hope this gets accepted.----- Update:I raised my score to reflect the added metric and experiments that improved the paper. ======= Review edit after authors' revisions ======The changes made by the authors in the title, abstract, introduction and conclusion to narrow the scope of the paper, better contextualize it, and make it more humble and truthful are very welcome. The extra experiments, figures, addition of error bars and new statistical tests are also a real plus. In doing so, the authors addressed all of my major concerns.For these reasons, changed my evaluation score from 5 to 8. The authors addressed my concern so I increased my score to 8. ----------------------- .[Decision after reading rebuttal]The authors appropriately addressed my concerns. The added experiments definitely reinforce the results of the paper and the added text help clarity. I recommend acceptance of this paper and would argue that its quality is reflect by an 8, up from my original score of 7 ----------------------------------------After rebuttal:Thanks for the answers. I have raised my score even though I still think the paper could have done a better job at comparing against other methods.- Regarding group-averaging methods for the equivariant case: It is a trivial extension, specially the approach of giving GNNs unique IDs and then averaging their representation, which (Loukas, 2020) shows it is universal (but (Loukas, 2020) did not consider averaging). Regarding training, it is always performed stochastically via data augmentation and Monte Carlo estimated in test. For the generalization error of the stochastic optimization, it is still unknown (some new results show promise (Chen et al. 2020) and (Lyle et al. 2020)) in the same way that the generalization performance of other universal methods is still unknown.- Chen, S., Dobriban, E., & Lee, J. H. (2020). Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond.- Lyle, Clare, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. "On the Benefits of Invariance in Neural Networks." arXiv preprint arXiv:2005.00178 (2020).- Loukas, Andreas. "How hard is to distinguish graphs with graph neural networks?." Advances in Neural Information Processing Systems 33 (2020). ### Post rebuttalI thank the authors for their response. My previous rating still applies. Update after Rebuttal:I'd like to thanks the authors for addressing the comments really thoroughly. The additional experiments using anti aliased networks are indeed very interesting. I agree with AnonReviewer4 that some of the material in this submission is probably known to people already. However, I do strongly believe that many people are unaware of the impact and importance of these effects and therefore I believe publication of this content is important. ------ UPDATEI have now read the other reviews, author response and updated paper, and have decided to maintain my rating. Update: I appreciate the authors response. The answer to my semiparametric efficiency question, however, is too terse and unclear. Theorems 1 and 3 provide only error bounds; I dont see how they, together with the semiparametric efficiency of OLS, directly imply semiparametric efficiency for the student model. Semiparametric efficiency involves the optimal asymptotic variance. The authors might have confused the concept with rate optimality. Nevertheless, the paper is a nice contribution, and I will keep my rating. -----------------after rebuttal------------I would like to keep my origin score due to the pros listed above. ---- Post-rebuttal comments----Thanks for the response. After reading other reviews' comments and the rebuttal, I think this paper is in a good shape now. Thus, I am willing to increase my score to 8 and recommend acceptance. Post-rebuttal.Thanks for a detailed response that clarified some questions and concerns that I previously had. I think the updated paper is stronger and I am inclined to raise the score to 8. ===============================After response ====================== The authors have addressed all my concerns.  I would like to keep my initial score. Updates after author responses and revisions:I was positive about this paper previously and am glad to see that the authors have done a great deal to try to respond to our concerns and strengthen the paper. I am more positive about the paper now and have increased my score to an 8. I think this paper is going to be useful for the community and I know I will reference it later and direct others to it who are interested in learning more about position embeddings in transformers (whether or not it actually gets published). -------------- UPDATE:As the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version. With this I think this is a good paper that should be accepted. ## Second reviewThanks for taking all my comments seriously. After clarification of the difference with RFLO I see that this work is even richer than I thought and I increase my grade to 8. It seems that other reviewers did not appreciate that training a network without back-prop requires nontrivial engineering and theoretical considerations which are well described in this paper, I truly think it is a pity if this work is not accepted.I fully agree with the difference between RFLO and Snap-1 that you describe in your reply, and I think it would be really great to put that somewhere in the paper. As you suggest it would be great to explain that you did not use random feedback weights for RFLO.This would also be a great opportunity to explain how did you extend RFLO to a GRU network in Figure 3. I find it a bit puzzling, that RFLO appears worse than an untrained network in Figure 3 (even early in training as in seen in Figure 3.B). Is there any additional difference in the network model for these two baselines, like one is using leaky RNN and the other one GRU or something like that?I find the piece of JAX code incredibly rich. It would be great to publish that along with the paper! JAX is not yet very well spread, and we see here that it is a very promising tool for custom gradient in RNNs. ----------------After the rebuttal: I'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications. Summarizing my impressions from the discussion, the two main points of criticism are that the proposed analysis is not fully predictive (depends on the norm of the gradients that depend on the empirical data), but rather provides the laws that  govern the dynamics, and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly. The validity of the continuous dynamics is demonstrated in numerical results only.  I do agree that a fully predictive framework on SGD directly would be very intersting. Yet, I think the authors are taking important steps towards such a framework, and considering the fact that SGD often behaves surprisingly/unexpectedly (as also stated by R3), I am still quite impressed how accurately the theory matches the actual SGD behavior. For our understanding of how symmetries/invariances in the weights of network architectures influence the training, I believe this paper does provide interesting insights such that I recommend its acceptance. As for a final score, I could go down to a 7 to account for the concerns raised by my fellow reviews, but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible, and this aspect should be reflected by the confidence rather than the rating. Thus, I'll give the authors the benefit of the doubt and keep my score, since I really enjoyed reading this paper.  Post-revision update------------------------Thanks to the authors, I think that the revision provided by the authors makes the paper substantially stronger. The inclusion of the more complex Shapes3D dataset substantially improves the experiments, and I think the discussion has improved. I have revised my rating to a clear accept in accordance. I am satisfied with the author's response and changes they made to the text. I still think the paper brings significant contributions to the area, by showing that even generating the pseudo-tasks via unsupervised clustering method allows the meta-learning to happen. --Revision: The authors have added HER baselines. Agreed with the authors that comparison of per-timestep perceptual reward vs terminal state perceptual reward is a good topic for future work. Update: score updated to 8 (from 7) following discussion below ----------- Update after rebuttal:I am very pleased by the answers of the authors, in particular, with the additional experiment showing that the algorithm could be extended with more advanced exploration strategies. I reviewed my rating accordingly.  Revision: Figure 1 is convincing and hints to why SN-GAN acheives slow decay while in principle it only tries to control the spectral norm. I think this paper is a good contribution as it provides a simple and efficient algorithm to precisely control the spectrum. Moreover, a recent work ([2], theorem 1 ) provides theoretical evidence for the importance of controling the whole spectrum which makes this contribution even more relevant.[2] M. Arbel, D. J. Sutherland, M. Bin kowski, and A. Gretton. On gradient regularizers for MMD GANs. NIPS 2018 Update: the authors' response and changes to the paper properly addressed the concerns below. Therefore the score was improved from 6 to 8. EDIT:  In their revision the authors addressed these concerns well and the paper is much more convincing (see longer comment below).  In light of this I have changed my rating from a 5 to an 8. Post rebuttal update: I have read the author response and updated paper, as well as the other reviews, and have decided to maintain my rating. AFTER RESPONSE:   Wow guys, what a great revision.  Thanks so much. --------Considering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.In the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.--------Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again. NEW:The authors have addressed the concerns I had with the manuscript. --REVISION--The paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of "preventing" or "avoiding" posterior collapse is too strong, as I agree with the authors that "it is unknown whether there is a better local optimum that [activates] more or all latent units". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like "reducing" or "mitigate" instead. --REVISION--I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR. Post rebuttal: I am satisfied by the points mentioned by authors!---------------------------------------------------------------- **Post-rebuttal**The rebuttal addresses the raised issues. An experiment supports the generalizability of the proposed metric. A new image-to-image translation experiment on COCO-Stuff is added. It shows the proposed method's potential for other translation tasks (the results are in Fig. 16, not Fig. 15). Taking the rebuttal and other reviews into account, I would like to recommend accepting the paper for its excellent results and simple yet effective idea.  **AFTER REBUTTAL**After reading the other review's and the author responses, my opinion is unchanged: This is a well-written paper with a simple and effective method, and a clear accept. ##Updated Review##I'd like to thank the authors for their comments, clarifications and modifications.  I still believe that this paper is a novel contribution with clear impressive results. UPDATE: I thank the authors for their detailed response and updated paper. I'm now more inclined to accept. Final recommendation:I have read the authors' responses as well as the comments from my fellow reviewers. I would like to keep my rating of the paper (8). UPDATE: The authors have addressed my concerns. Taking also into account the responses to the other reviews, my positive view of the paper has been confirmed and I have therefore increased my score. ============================================After rebuttal:According to the reviewers' feedback, I would keep my score to 8 and still vote for acceptance. However, there are still two details that are expected to be fixed in the future version. First, the reason why momentum update in SiMo is important is not convincing to me. It is not clear why letting $\theta_k=\theta_q$ cannot ensure the loss becoming smaller. More theoretical and experimental analyses are expected to address this issue. I still encourage the authors to rethink this detail. Second, the computational cost is provided in the feedback. I encourage the authors to include the numbers in the paper. 35\% additional cost cannot be ignored. ---- Post-rebuttal comments----Thanks for the response. The rebuttal addresses all my concerns. I am willing to increase my score to 8 and recommend acceptance. * I have read the authors' response and also other reviewers. In my view, this paper provides novel insights into batch normalization. # UpdateI thank the authors for their thoughtful reply and for incorporating the feedback.  The additional information is most welcome, and so I maintain my score of 8 for the paper. -----update: thanks for the clarification points + figure 9 :) I still recommend acceptance (score of 8). --- Post rebuttal ---After reading the other reviewers comments and the rebuttal, I'm keeping my initial score. ## Post-rebuttal commentsThanks again to the authors for the submission.  My concerns are clarified and so I will leave my rating intact. ================== After rebuttal ==================Several typos:In Figure 1, to be consistent the X and Z may be written in lower case.In Eq. (6), the transpose should be applied to the right $U_1^{(i)}$ instead of the left one.In Appendix C.1, ``It seems that MAW seems to learn" should be ``It seems that MAW learns" or ``MAW seems to learn".In Appendix D.3:In line 2 above Proposition D.2, ``the ill-posedness of (11) with $\mathcal{R}= W_2$": The $\mathcal{R}= W_2$ should be $\mathcal{R}= KL$.Please rephrase ``the KL divergence fails is unsuitable for low-rank covariance modeling", by e.g., removing ``fails" or inserting ``and" between ``fails is". # Post-Rebuttal CommentsI've increased the rating for the paper from 5 -> 8 as the authors have addressed all my substantive concerns.- Most critically, the authors demonstrated that, even without the tree-based exploration variant, FGP's performance improves significantly over the state of the art particularly for $l_2$ networks. - The authors have also significantly improved the clarity of the algorithm description, made the comparison in the section on generalization to other norms more clear, provided information in the paper allowing readers to understand that FastLin is significantly faster (but also significantly looser), and provided details that make reproducing these results far simpler. 