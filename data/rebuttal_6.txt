 ######## Post-Rebuttal Updates:After reading the authors' response and other reviewers' opinions (especially R3), I would like to downgrade my rating slightly from 7 to 6. I still think the paper makes valuable contributions, but I also think the contributions are overstated and not precisely justified. Particularly:1. I agree with R3 that the limitation of novelty should be considered from the two perspectives of "task" and "method". The task is certainly not new, which should be made clear in the paper. The newest version of the paper still claims "we define a new taskcross-supervised object detection"2. The method is indeed somewhat new, due to the use of multi-task learning and SCM, but its novelty should be clarified, and compared to all similar methods, not only some of them which are weaker. For instance, the authors did not adequately justify whether/why their method is superior to YOLO 9000, or Yang et al ([1] above). They did mention some differences in response to R3's comments and mine, but I am not convinced. Moreover, the authors did not explicitly discuss and compare those distinctions in the paper, neither quantitatively nor intuitively.3. In response to R3, The authors claim they "are the first to address this problem in situations of realistic complexity", which is not accurate. Particularly, the paper reads "While several works [...] have explored this problem before, [...] They struggle to learn under more complex and realistic scenarios, where there are multiple objects from potentially very different classes" This is not entirely true, as Yang et al. successfully evaluate on Open Images (and also VOC and COCO in their supplementary materials). The authors do not provide a convincing reason or evidence of existing methods "struggling" in realistic settings. YOLO 9000 is open-source, and could have been compared to the proposed method to confirm that claim.Accordingly, I strongly encourage the authors to refine their claims and lay more emphasis on the actually novel aspects of their method, by thoroughly comparing those novelties with all similar methods. I still hope to see this paper accepted, but cannot endorse it due to insisting on inaccurate claims. Thanks for your informative response addressing my comments. After the revision, the description of the method is clearer (Sec 3.2), and the experimental results are clearer (Sec 4). I'll stay with my original accept-score.=============== (Update): The score has been updated after a rebuttal from the authors.  =========After reading authors comments.- Well, still the Point 1 is generally not true in my opinion. The authors cited the work of Kidger & Lyons "Universal approximation with deep narrow networks", in which universal approximation is proved for a wide class of activation functions and long narrow networks. Moreover, if you do not strive to make the network as narrow as possible, then it is easy to convert a shallow wide network into a deep narrow one (but not vice versa). Point 2 seems quite possible. Point 3 coincides with what I mentioned --- the authors proved UAP for several examples of activations, although really quite important ones.- "Results will be accessible without the need for the technicalities". Well, I personally do not understand this statement in the conference article. From the main text, the reader only sees that a relatively basic property (universal approximation) is fulfilled for a couple of previously uncovered activation functions (or for a specialized class), and it is not clear why, since possibly new ideas are not in the main text. - This work clearly has strengths and weaknesses, which, in principle, are more or less clear. On the one hand, the universality property for approximations by dynamical systems and a specific class of activation functions is proved. How important, new and interesting is it? It doesn't seem to be super strong result, because this property is relatively weak, completely expected, and has already been proven many times before in various situations (wide shallow networks, narrow deep networks, dynamic systems with some classes of activation functions). In approximation theory, people have been studying more advanced questions like approximation rates for a long time. I also do not see any significant practical use of the results. On the other hand, it is possible that the authors have developed some new method when proving their results. But in order to check this fact the reviewer should check the whole appendix. If an article is submitted to a journal, the reviewer is expected to read full article, including the Appendix. But for this conference work, as far as I understand, this is not the case, the reviewer is not required to read the Appendix. Accordingly, if the authors could not convince the reviewer of the importance of the work within the main text, then this is their problem. As for the individual points that the authors wrote about - as I have already written, part of what they wrote is quite fair, no one disputes that they have some new results. At the same time, for example, I find it difficult to agree with point 1, as I have already written.- Well, in principle, there are two ideas that the authors mentioned (about just two values for all weights and memorising data), which are relatively interesting indeed. This is not something that would be mega-important, but it looks like non-obvious facts, which is a plus.In principle, I can increase the grade for one point. -------After reading the author's responses, I'm ok to promote the rating from 5 to 6. ------------------------------------------------------------------------------------after reading the authors' response and revision, I raise my score to 6. --- Post rebuttal ---I'd like to thank the authors for the response. I've updated my score in light of these additional results. Update: I appreciate the response and have adjusted my score accordingly.  Since most of my primary concerns are resolved, I have updated my rating based on the revised version. --------------------------- After Rebuttal --------------------------------The authors did a good job in the rebuttal. Most of my concerns have been addressed so I am happy to raise my score to 6. Edit after rebuttal: I have read the author response and I thank the authors for their valuable insights and answers to my questions. It is exciting that this does help in improving the performance on downstream VQA2.0 task though I would have expected the results to be conducted on one of the recent state-of-the-art models instead of very old BUTD model where achieving performance gains is trivial. I would like to keep my rating as it is. ============================================**Post-Rebuttal Comments**==================================I appreciate the revisions and additional results presented by the authors. The authors have addressed my concerns as well as improved the clarity of the model description in the revised version of the paper. While that results are not perfect, I acknowledge that the problem of video generation is difficult and I believe such multi-stage model can motivate future methods in this direction of scalable video generation. Therefore, I would like to improve my score to 6 and would recommend acceptance of this paper. ** Post Rebuttal**The paper has been improved based on the reviews, I think this paper is now ok to accept. ### Post rebuttalI appreciate the authors' efforts in conducting experiments to show the latency results. After reading through the rebuttal and the reviews from other reviewers, I would like to down-grade my score by 1.Specifically, I agree with R3 and R4 that it would be better if experimental results are done for MobileNets/EfficientNets to empirically demonstrate the effectiveness of the optimal convolution. Those networks present strong baselines and it would be more convincing if optimal convolution indeed outperforms those. With that said, I agree with the authors that the DARTS experiments have shown that the optimal convolution can be better than depth-wise separable convolutions. As a result, I still recommend acceptance for this paper. **After Rebuttal**I appreciate that the authors partly answered my questions and conducted experiments to show the runtime. After reading through their rebuttal and the other reviews, I will keep my original rating.  ******* Update after reading rebuttal ********I appreciate the additional comparisons to prior work added by the authors. I've raised my score to a 6, but I still view this as a borderline paper due to concerns about clarity and impact, along with the  concerns raised by Reviewer 3. =====================Post Rebuttal: I would like to thank the authors for the new results on WebVision-mini and ImageNet-mini, this has partially addressed my concerns as Reviewer3 raised similar issues on the SoTA claim. Overall, I think this paper is well presented and the results are solid, thus updated my rating to reflect this. ------------------Post Rebuttal:The authors' response has addressed my concerns. Based on the response and other reviewers' comment, I have updated my rating for this work. -------------------------------------After Rebuttal============My main concerns were about the similarity between SUR and URT and the lack of detail in their comparison. I also asked for a clarification on the efficiency of the method.On the first concern, they partially address it with the Coefficient characteristics, I say partially because I would have liked a more in-depth comparison of the characteristics, but technically they have addressed my question. For the second one, they now provide the training time, and the testing time could be found in Section 4.2.Overall, even though I still think that this work lies in the application side, it is interesting enough to be published at ICLR, so I have accordingly raised my score. ################ Feedback to the authors' response ###############As the authors have addressed some of my main concerns and provided nice extra experimental results, I will raise my score to 6. ---------------------------------------------------------------------------------------------------------------------I have read the response, and the rating is not changed. ----------------------------------Rating after discussion: lowering to a 6, as I share concerns with R1 about experiments and generalizability of the proposed approach. Edit ===Other more confident reviewers have pointed out some concerns. I am still positive about the paper, but due to lack of rebuttal I am going to downgrade from my initial score. ** POST REBUTTAL UPDATE**I went through the other reviews and the author rebuttals. I thank the authors for throughly addressing all of my concerns and running some baseline experiments which I think are quite interesting, and add to the completeness of the paper, including acomparison to EST and an optimal K-shot baseline. I think the idea is simple and interesting, and now the paper has enoughexperimental comparisons to be a valuable addition to the conference. I have updated my score accordingly. %%%%%%%%%%%% After rebuttal %%%%%%%%%%%%%%%%%%%%%%%%%%%%I appreciate the great efforts the authors have made in responding to the comments. Most of my comments have been well addressed, so I have increased my score. Nevertheless, another important question is in the over-parameterization setting, why the solution obtained by the convex approach has good generalization property. This question is not addressed in the rebuttal. Probably it is too much to have this in one paper and can be an interesting question for future work.  Update:Thank the authors for their responses, clarification, and additional experiments. I read through authors responses and the comments from the other reviewers. I still think this paper makes a borderline case for 1) its technical contribution on extending DPS and thereby achieving significant performance gain on a toy problem and MRI reconstruction tasks, still 2) with limited novelty and room for a more extensive experimental validation (perhaps, beyond MRI). My other concerns on clarity and significance of experiments have been addressed. I would raise my rating to marginally above acceptance threshold (borderline). ### UpdateI thank the authors for their comprehensive response. While its unfortunate they couldn't compare to any other active methods, the related work and overall clarity of the paper is significantly improved. The t-SNE plots were informative and interesting. While I have reservations about the paper's lack of comparisons, I think its publication still might be a net positive for the research community.I have updated my score.##### Other commentsLet A(X)=F^H D\circ F X. The expression A^H(Ax-Y\circ sign(A(x))) is a subgradient of 1/2|| Y - |A(X)|||^2 but A^H(|Ax|-Y) is not. I would avoid calling (9) projected gradient descent as the "gradient" isn't really a gradient."We have performed a statistical analysis on the performance gains made by A-DPS over DPS in the MRI reconstruction task, concluding that they are indeed statistically significant." It would be nice to see confidence intervals in Tables 1 and 2.#### Questions/comments that do not effect the review:Why use an LSTM/any network with memory? It seems the next sample depends on the previous samples, but not their order. The ablation study on pg 6 shows that memory helps (at low sampling rates), but I don't understand the intuition why. Could the LSTM just have more capacity?Typos:pg 2: "cells.During" spacepg 3: "However, This" capitalization UPDATE AFTER REBUTTAL:I thank the authors for their responses and appreciate the inclusion of some of the requested changes in the paper. However, the paper still misses the comparison to other adaptive methods which is the paper's greatest weakness. Therefore, I decided to keep my score at 6. post-rebuttal:I thank the authors for their clarification and ablation analysis. Most of my concerns are resolved. I will keep my original score and I think the paper could be accepted.  **After rebuttal, I think the authors did a good job in clarifying some points and adding an example in the experiment, which is why I upgrade the score I gave. However, I don't understand why they don't compare in the experiment with some of the methods from the cited literature concerned with the same model, especially Diolaiti et al. (2005).** (Added on 11/29/2020) **Post Rebuttal Comment**I thank the authors for sincerely replying my review comments. I think the authors answered my questions. Addtional Comments- Section 3.4: $(c_1(x[1]),...,c_D(x[D])$ $(c_1(x[1]),...,c_D(x[D]))$ (Add a right parenthesis)--------------------------------------------------------- Post-author response: I appreciate the extra experiment on GLoVe vs Linear on sentiment analysis, it is what I was asking for. I have raised my score in response to that, as well as the additional reporting on the results. The discussion in Bayesian terms was interesting, I think it would help. Nevertheless, I was thinking is it that it should be possible to construct embeddings that have known syntactic vs semantic properties. E.g. one could increase/decrease the context size, perhaps to extreme values in the case of models such as GLoVe. And then we could actually have a much stronger prior. If the paper is accepted, I think such an experiment would be very informative. ---The response from the authors satisfactorily addresses several points that I raised. However, I am not fully convinced that the LTH on GANs has provided significant new insights. However, based on the response I am inclined to raise my score to above acceptance threshold and tend towards acceptance. =========================================================================================Edit post Rebuttal:Thank you to the authors for their in-depth response and the effort they put into responding to this review, and my apologies for not engaging during the discussion phase. I'll respond to several points with the goal of furthering the discussion to try and avoid unfairly "having the final say" when the authors cannot respond.-"The biggest potential impact of our work is that it provides empirical evidence that lottery tickets exist in GANs"This reviewer's opinion is that this is not at all surprising. While GANs do have unusual and unique training dynamics on a number of levels, and interact interestingly with many typical building blocks, many of the aspects of neural-network based GAN models (prunability, the relationship between signal propagation and trainability, model capacity, etc) are largely indistinguishable from those of more "typical" neural nets. Nonetheless I do agree that, if one holds the lottery ticket line of work to be important or relevant (which I personally do not, but I will withhold my bias and not "legislate from the bench" here), its extension into the realm of GANs does first require verification of its existence in this regime. I apologize to the authors if it seems like they're pushing against a brick wall here because this reviewer is not a disciple of the lottery ticket hypothesis--please note that to the best of my ability this review is meant to be calibrated around this bias and instead focus on the strength of the manuscript.-Transfer between tasks: I appreciate the authors response on this point; transfer of masks between tasks (rather than pretrained weights) does indeed represent a different modality of transfer learning, and one which may be well-suited to GANs which are known to be difficult to re-train or fine-tune in many situations (although there is a growing body of work on this topic outside of the pruning/lottery ticket context).-"Lottery tickets without early weight rewinding techniques can still hold to small-scale tasks. "Yes, this is precisely the problem--lottery tickets without early weight rewinding *do not, in general, tend to hold on large scale tasks*. This again returns to my baseline issue (one of many) with the hypothesis in the first place, that it has to be modified to work on even models like VGG.-on checkerboard artifacts: This reviewer is very well calibrated to viewing GAN samples, and holds that the checkerboard artifacts are vastly more visible in the sparsified models. I would encourage the authors to, in future work,  consider why this might be (what about sparse networks leads to increased checkerboard artifacts?) rather than seek to claim that the unpruned models are also checkerboard-y.-Additional experiments: I commend the authors for repeating their experiments on a range of architectures, and agree that the improvements for the sparsified models are in support of their argumentation and conclusions.-Thank you to the authors for updating their notation and bibliography.On the whole, I still feel that this paper is borderline. While the author's responses are fairly strong in context, re-reading the manuscript, I still do not feel that the paper (which is most of what matters here) is especially strong. I am upgrading my score to around a 5.5 (which I will simply round up to a 6 on OpenReview). I think this paper is true borderline--I won't argue for its rejection, but I don't feel especially strongly in favor of it and cannot champion it. ----------Update after author response----------I thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score. -----I read a valuable author's response, and I keep my positive score. =======================Rated up after authors' clarification. =====================Post RebuttalI have read the authors' response. All my concerns are addressed properly. However, I still doubt that even the corner cases of \eta have a better performance, would there be a systematic way to find the optimal parameters reflecting the true potential of this method. Thus, I will keep my score unchanged. ---After author responseThe authors have improved the presentation and added the necessary ablation studies. I appreciate the authors' effort. I am glad to raise the score to reflect the changes. I hope this work will inspire future research on hierarchical planning algorithms. ---**Post Rebuttal**In my review, the main concerns were (1) validity of assumptions, (2) confusing writing/notation and (3) unclear takeaway. The rebuttal appropriately addressed (1), although I am looking forward to the revision to see how this is discussed in the paper itself. I cannot really say anything about any improvements on the writing (2) without seeing the revision, but I am confident that the authors can address most of the issues pointed out by myself and other reviewers. Regarding (3), unclear takeaway, after reading the authors' response as well as the other reviews, my concerns are somewhat assuaged (partly because the assumptions were addressed better), although I am still unsure how or if the results in this paper could be expanded to realistic attention models.There are additional issues I raised during the discussion (general lack of citations in particular), however this can be fixed fairly easily for the camera ready so I am willing to give the benefit of doubt and raise my score to 6 (borderline accept) Update after the revision and the author response ===I would like to thank the authors for the additional work and effort invested into improving the paper presentation. This has made me increase my score; I still have some doubts regarding the experimental setup (using target dev sets and taking this information for granted), but maybe a high-level question I posed in my review really does go beyond this work. Given that the main premise is 'quick adaptation' to new (and unseen) languages, the inclusion of at least one-two truly low-resource languages would have still been nice to link the motivation with the experimental setup. -----------------------------------------Post-Rebuttal-----------------------------------------I believe both the strong points and the weaknesses I pointed out in my review remain valid. Therefore I am keeping my score unchanged.In case of acceptance, I suggest to the authors to carefully address all the comments from my review and the other ones in the camera-ready version (especially the issues related to clarity.).---------------------------------------- ####################################### AFTER REBUTTALThe new baselines added make the experimental validation more convincing. Therefore, I have raised my rating to 6 (Marginally above the acceptance threshold). However, I still believe that the contribution is incremental, and I think the paper would gain in terms of novelty if it focused more on the detection of OOD data and adversarial attacks (which right now is more like a preliminary test). Update: Following the authors' clarifications and additional experimental work, I'm increasing my rating to 6. Edit: Based on the author response in terms of adding additional experiments, I'm raising my score to a 6. ##########################################################################Post-rebuttal.Thanks for a detailed response that clarified some of my questions. I think the overall quality of the paper increased and I am happy to see additional information (like additional literature and an ablation study in Section 5.4) and a somewhat improved explanation of the core idea. However, I am still hesitant to give this paper a higher rating, in part because I find Section 5.4 to be poorly written and somewhat confusing (it lacks any sort of conclusion or insight and I cannot read Figure 4 at all, the text is too small) and partly because the paper does not clearly put its results in the context of the recent model ensemble progress (and thus makes me doubt the impact of this result on the field; although it does seem to be promising compared to the mentioned baselines). -----------update-----------After the discussions below I have changed my score from a 5 to a 6.  ===========================Score raised to 5 following response to the rebuttal below, then to 6 following the re-rebuttal. Edit:Having read the reviews, rebuttal and updated paper, I have decided to maintain my score of 6. ------ After author's response:* The response of authors identifies the problem of using running loss in projected space. While one can try to get around it by projecting the loss function as well but that would be a convoluted way to solve the problem, and in any case, not a strong criticism of the presented approach. * The updated document has generalized the derivation to take general perturbations into account. * Updates Tables 1-3 resolve empirical analysis concerns of the reviewer. With these improvements, the reviewer is happy to recommend acceptance of the paper.  --------------------------------------------------------------------Edit after rebuttal and discussion.I thank the authors for extra experimentation to showcase the effectiveness of SkipW. While most reviewers here agree that the novelty is limited (that doesn't stop it from being useful), I strongly think the impact due to SkipW will be translated to the real-world. There has been some discussion on the datasets, which I agree are not extensive making the initial experimentation weak. However, the new experiments compensate to an extent and I would like to recommend a weak acceptance with a score of 6 (I am still between 6 and 7, waiting for other reviewers to pitch in).  ============================Update after going through the updated paper and discussions---------------------------------------------The paper adds better illustrations in the revised paper to explain the technique and the approach seems to be effective even though it is simple. Given the approach, 'transductive' in the title looks okay.Figure 1, Figure 3, and Figure 4 in the current version suggest that the one-class SVM/SVDD techniques [1, 2] would be important to compare against. Popular anomaly/outlier detection algorithms Isolation Forest [3], LOF [4] would also be relevant here. The paper can be strengthened with these additional baselines. RETO has at least one short-coming over the outlier detector techniques: it needs to wait some time until it can collect enough test data.[1] Lukas Ruff, Robert A Vandermeulen, Nico Görnitz, Lucas Deecke, Shoaib A Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In ICML, volume 80, pp. 43904399, 2018.[2] Lukas Ruff, Robert A Vandermeulen, et. al., Deep Semi-Supervised Anomaly Detection, ICLR 2020.[3] Liu, Fei Tony; Ting, Kai Ming; Zhou, Zhi-Hua (December 2008). "Isolation-based Anomaly Detection". ACM Transactions on Knowledge Discovery from Data[4] Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; Sander, J. (2000). LOF: Identifying Density-based Local Outliers. Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. SIGMOD. ---UPDATE: After reading the replies, I have updated my score from 5 to 6. AFTER READING AUTHOR RESPONSE-----------------------------------------------------------------------------------------Thanks for the response.Ideally, you would be able to show results for ensembles of decision trees or some model that is very different from neural networks. Feedbacks from authors are constructive to solve my puzzle and I decided to increase my rating.  #### EDIT------------I think the authors replied to some of my concerns in a convincing way, hence I raise the score to 6. Unfortunately, I think the theoretical analysis for the noise-robust loss is orthogonal to their sampling sieving approach. And the analysis for choosing $\beta$ does not dependent on their instance-dependent noise settings. We can get the same $\beta$ by very rough estimation(their approach) in instance-independent noise settings. In addition, I guess $f_x^*[y]=1$ is still problematic in the theoretical analysis since CORES2=\inf given the ideal classifier. Overall, following author's response, I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision. --------------------------------------------------------------------------------------------------------------The authors of the paper addressed carefully the concerns I raised above. As such, I am raising my score to a 6, and would like to recommend accepting this paper.  ### After rebuttal update.The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks. I was also convinced by authors responce on paper novelty, technical contribution and (after the re-focusing) potential usefulness to the community.Thus, I am raising my rating to weak accept.### Comments on authors response (as after Nov 24 I cannot post messages, visible to authors)> 1. However, we would be thankful if you might share any prior work (paper or published practice) where the authors automatically searched for the optimal combination of regularizers for deep learning models among a large set of regularizers, as presented in this study.I am surprised with the results of the googling, but have to admit that authors are technically right and I was wrong. While it seems obvious to me, that regularization (specifically, dropout and L2 weight decay) are the hyperparameters of the deep network training, somehow papers and guides online mostly consider mostly architectural things + learning rate + (sometimes) dropout rate as a hyperparameters to optimize. https://arxiv.org/pdf/2006.12703.pdfhttps://nanonets.com/blog/hyperparameter-optimization/Anyway, I lift my objections on novelty.> 3. "Neither software, nor dataset is proposed, the paper uses existing ones." : We engineered a source code that selects the application of 13 regularizers to a neural network, which required extensive programming efforts and several additions to the AutoPytorch library, as mentioned in Section 4.1.OK, I agree.> 4. "It is well known that the regularization/augmentation/ methods need to be tuned ... He et.al. (CVPR2019)" : The suggested reference is a collection of refinements (many of which are actually not regularization techniques), which have been suggested by the deep learning community for maximizing the generalization on Imagenet. That paper only summarizes a collection of some practices, however, it does not present a method that searches for the best combination of a large set of practicesNo, we are discussing different things. I gave the He at.al as an example, that community is well aware about the fact, the regularization and augmentation (and other things) have to be highly tuned. I agree that He at.al and the current paper use completely different methods for solving the problem (manual tinkering vs. auto-search). What I disagree is that the community was not aware about importance of regularization tuning before this paper.>5.  E.g. DropOut was present in only 35% of the dataset cocktails, hence was not selected in the cocktails of the 65% of the datasets.And experiments were using fixed-size network. Of course, is the network is not wide enough for the task, the dropout might not be needed. It is also quite strong statement that "there is no universal regularization", given that L2 weight decay and dropout are widely used in a such different domains as image, text, speech processings, RL and so on. ****I would like to point out the TabNet paper https://arxiv.org/pdf/1908.07442.pdf, which claimed "beating GB methods for the tabular data". I appreciate the fact, that unlike the TabNet, RegCocktails were using a standard deep MLP and not the attention model, yet one needs to add that reference.  === update ===I have read the revised paper, and decided to update the score (see response to authors' comment).I have read the other reviewers comments, the authors responses, and the updated paper. Given that the paper now seems to be focused on tabular datasets, Most of my concerns are addressed, and therefore, Im increasing the score to weak acceptance. I would like the abstract, introduction, and section 4.1 to be further changed to make the focus on tabular dataset more clear. *Updates after reviews and authors feedback: *The updates from the author are appreciated and make the arguments of the paper clearer.After reading the other reviews and discussions, I keep my original score of "6: Marginally above acceptance threshold". --- Update after discussion ---I thank the authors for considering my recommendations. I think the clarity of the updated paper is much improved, particularly the introduction. I further think that the authors have adequately addressed the concerns raised by the other reviewers and recommend accepting the paper. --------------------------------------------------POST AUTHOR RESPONSE1) Thank you for adding Fig. 3. My concern about the attacks being noticeable is resolved. 3 & 4) The scope of this work is still limited to Specifically dynamic routing, which we already know are susceptible to black box attack (the community that already exists on architecture un-aware attacks) and some other attacks. As author's mention it is not trivial to apply their attack to other capsule architectures either. However, I find their various analysis informative. Therefore, I am increasing my score to 6. ========== after reading the authors feedback =========Thanks the authors for addressing my concerns and I am convinced that this work is very much different from prior literature. In addition, the evaluation metrics are correct in the studied problem setup. Based on that, I would like to raise my score from 5 to 6. ========POST REBUTTAL=========I would like to thank the authors to answer my questions. It addressed most of my concerns. Hence I increase my score to 6. === Post rebuttal ===1. As reviewer 2, I am also not convinced by the reason why it could not be evaluated on a MOT dataset. The explanation suggests that the proposed method is more general and it should be able to apply it to broader domains (and thus MOT could be one of them). 2. The evaluation metric still looks suspicious to me. I can imagine we can usually get better numbers if we can directly minimize the metrics we want to evaluate. However, in my opinion, data assocision usually need to do hard assginments if we want to use it in practice (unless it is a intermediate task and in that case we can do soft assignments), so I do think it is better to set a hard threshold or draw a curve of F1 v.s. threhold. In the meantime,  I think it is unfair for some other methods, like k-means++ becaue it is minimizing a different objective.I still think the method has some merits so I am at borderline, though I will not defend it if it is rejected. # updateThe authors have addressed a number of issues and strengthened their submission. ======================================================I've read the rebuttal and the authors have addressed most of my concerns in the revised paper so I raise my score. UPDATE AFTER REBUTTAL:The authors have covered most of my concerns about the paper and I think that the paper has been substantially improved. However, my biggest concern was about the experiment results and  I think the paper still lacks on validation comparison with other methods.  ============================== Update after author's response ==============================The additional experiments look good to me, it solves most of my concern, I would like to lift the score to 6. === after rebuttal ===I thank the authors for the feedback. Regarding the proposed theorem, I was hoping to see if the iterate can stay in the flat minima instead of leaving the region eventually, as the constant of the local strong convexity is inside the log's, which is not very sensitive to the value (the landscape) and might be tricky to provide useful guidance to differentiate flat and sharp minima.I decided to maintain my score, but I still recommend a weak accept of this paper. Post-rebuttal: I thanks the authors for their answers to my concerns. After looking the reviews of the other reviewers and the ongoing discussions, I have decided to keep my score. I think this paper makes a novel proposal which deserves to be published.  **post-response update**:some of my concerns about the evaluation were resolved in the author response (for example regarding the meta-validation datasets) , and I have slightly increased my rating. ## Edit after rebuttalI have updated my evaluation (from 4 to 6) based on the changes made in the manuscript and the responses by the authors. See details in my response to the authors: https://openreview.net/forum?id=3q5IqUrkcF&noteId=qZzolK7E-wF After rebuttal:The additional experiment results provided in the rebuttal stage suggests the efficiency of the proposed method, as well as the congruent and conjugacy conditions are approximately satisfied. I therefore believe this paper should be accepted. -----Nov. 30th: On a second reading of the authors exchanges with the reviewers as well as of the updated paper, I am lowering my overall score. While I still believe that the *questions* that the paper raise are very worthwhile to the community, I agree with several reviewers that the *answers* provided in the paper are insufficiently supported by a convincing formalization and by experiments.   ######Update: after reading the other reviews and author response, I decided to increase my grade to 6. Update:Thank the authors for providing the additional results and updating the paper. Since the comparison to one state-of-the-art method has been added (though some other state-of-the-arts are still missing) and the benefit is shown across different settings, I increase the score from 5 to 6.(One suggestion: I would recommend you to highlight the changes in the revision with a different color, so that readers can identify the changes easily.)-------- Thanks you for the updated manuscript and the answers, I am reasonably satisfied.I am still not fully convinced that the model is novel enough (e.g. simply combines two well known models HNN with MAML ANIL, with the expected conclusions) to grant acceptance, however, since the other reviewers are ok with this, I am happy to increase my score and let the AC make a final decision. I'd like to thank the authors for addressing my comments. I've read through the other reviews and responses, as well as the revised paper. The presented method for learning "true spatiotemporal permutations" is novel, and does indeed seem to learn effective representations.What I'm not entirely sure about is how much this method manages to push the boundary of SSL. Comparing methods with different backbones is indeed tricky, and my intention was definitely not to discourage SSL works from academia. But the burden of proof should be on the new method to perform as close to an apples-to-apples comparison (in terms of backbone) to existing methods as possible. In the end, there are many many potential pretext tasks for SSL of video representations, and I do feel that in order to be publishable at a top-tier venue, they should either enable new tasks, or show clear superiority over existing methods.Regarding temporal action segmentation as a newly enabled task -- I honestly missed this section, since it's in the appendix. This should be moved to the main paper.If I could, I would be borderline on this paper. But since I can't, I'll give the authors the benefit of the doubt, and raise my rating to 6 (marginally above). Update after rebuttal period: Thank you for your clarifications. I have revised my score accordingly. As most concerns of mine are addressed by the rebuttal and I would like to rise my score.  ==========================================================================================Updates after rebuttal:Thanks to the authors for the reply. I have read the author response and understand that actually there are activations in the networks but just omitted from the figures. I am increasing my recommendation to 6. ---The new section 2.4 is appreciated, though it seems the paper still does not say that incomplete methods can deal with round-off error by sound overapproximation. -----### Post-Response UpdateAfter reading the authors' response and given the changes made in the paper, I increase my rating by one point. ##########################After author feedback:Thanks for the detailed feedback from the authors. Most of my concerns have been addressed and I will keep my scores unchanged. Please add the additional information in the feedback to the final version. ##################################################After author feedback:The authors have addressed my comments, though it is impossible to evaluate what the authors promise to do in future work. My evaluation remains unchanged.   (I have updated my review to raise my reviewer score by two points after discussion with the authors; this is still a preliminary evaluation as I have not discussed the paper with the other reviewers)-- [ Rebuttal / question responses are acknowledged, and also the other reviews.    I think the algorithm is sufficiently novel, and it does really well on some difficult new problems.    I see the paper as being about a new policy optimization algorithm leveraging differential dynamics, and not about sim2real.   However, the biggest limitation is pointed about by both R3 and R4, i.e., per R3: "With both algorithms, as well as the tasks, being new, its hand to establish the strength and credibility of both using one another. "  And so with this in mind, I am changing my score to a 6, i.e., marginally in favor of accept. ]  Update after rebuttal-----Thank you to the authors for addressing my concerns. I have updated my score. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. I understand the authors view that some of my queries fall outside their desired scope for the paper, however I still think the paper could benefit from such contents. UPDATE:I find the main contribution of the work to be the empirical analysis. I personally liked this paper, but I must agree with the other reviewers that improving the theoretical results will strengthen the paper's impact. This would require a major revision not suited for a conference rebuttal, and so, for now, I have downgraded my score to a borderline accept, but I look forward to seeing a future revision of this paper. I thank the authors for their detailed reviews. I have updated my score--- --- Update ---The authors have addressed several concerns that I had regarding the work.  While this is largely an application of a previous method, they have made some application-specific decisions in order to achieve the significant boost in performance on colorization that they saw.  While the metrics for this task are much improved, there are still some things to be desired on the qualitative results (i.e. the diversity of results tends to be in blocks, as opposed to high within-image color variability).  Nevertheless, I think the improvement from this approach my guide future work in this area.  Given the author's responses and changes made, I have amended my recommendation accordingly.  After reading the responses and checking the additional experiments, I changed the score for this paper. ----------------------------------------------------------------------Update after rebuttal----------------------------------------------------------------------Thanks for the feedback.Although there are some minor issues, my main concerns are addressed.  --------------UPDATE: Thanks for the clarification and the revisions to the PDF -- I will keep my score as is. **************After Rebuttal:I thank the authors for their extensive answers and clarifications.Overall, I maintain my positive outlook on this work. Although theoretical justification could be improved, I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty. UPDATE - Nov 30-----------------------After looking at the revised version of the manuscript I am still concerned that the claims made in the abstract (and implied in the main text of the paper) about the match of ANNs to the brain are misleading the reader into assigning greater biological significance to the reported result than it actually holds. While the authors made slight modifications in the text and added a few sentences commenting on the issue, these changes did not constitute a change would make the reader "extremely aware that when you say "80% match" you don't mean "80% match to the brain", but "80% match to the score"". I find that a softer claim that would explicitly acknowledge that 5% of "synaptic" updates explain 80% of the predictivity score and not 80% of the match to the brain would make this work more scientifically precise and thus more valuable. I am keeping my original assessment of this paper as being borderline. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%I have improved my score following the improvements made by the authors. See my reply below for details. ----------------------After author's response--------------------The response addressed most of my concerns and included experiments results of trajectory visualization and run-time comparison. I think the paper would be an interesting contribution to the conference. UPDATEFollowing the author's response and updated draft, I've raised my score from a 6 to a 7.UPDATE 2Following discussion among the reviewers and especially a summary of experimental results by Reviewer 3, I'm lowering score back to a 6.   # Update after rebuttal phaseThank you to the authors for engaging with reviewer comments. I think the paper is much clearer now, and the additional results in Figures 5 and 6 indicate that the analysis may be relevant for practical meta-learning settings. I am not sure of the necessity of the new data-generating plot for mixed linear regression in Figure 1 (my uncertainty here was resolved with words); the authors might consider using the space for putting Figure 6 (currently in the appendix) in the main paper, or for additional experiments. Two more notes:  1. It may make the paper easier to read if the appendix were part of the same PDF as the main paper and not in the supplementary material.2. While the experiments are perhaps not difficult to reproduce, code would be helpful to the community.I am increasing my rating to a 6, as I believe the paper presents an interesting result with sufficient evidence. I am not giving a higher rating as I think the paper's impact would increase substantially with experiments on actual data, either in the mixed linear or deep net setting. For future versions of the paper, I encourage the authors to consider adding such results.  update after reading authors responseI raised my rating to 6 due to the clarification on assumptions in the paper. I am now more convinced of the relevance of the theories in the paper to practice. ======================Post-rebuttal comments:I want to thank the authors for providing clear answers to my questions and comments. I found the answers satisfactory so I raised my score to 6.  The authors addressed my concerns in the rebuttal. I have raised my score. --After receiving authors' response--I would like to thank the authors for providing additional experimental results and giving clarifications. Since my concerns are almost solved, and I updated my score from 5 to 6. This study would be a great first step to tackle the challenging problem, open-world semi-supervised learning, though there are several remaining issues (e.g., validation is hard to conduct, the number of unknown classes should be known, etc.). I vote for "weak accept." --------------------------------------------------------- Post-rebuttal comments ---------------------------------------------------------------------------I carefully read the rebuttal and other reviewer comments. The author addressed my concerns on pseudo-label quality assessment and comparison against SOTA trackers. From the experimental perspective, I am very convinced the paper did a great job now. Please incorporate these additional experiments into the paper making it more complete. That being said, similar to other reviewers, I am not very convinced about the author's reply on novelty/contribution. It's true it has not been applied in 3D, which is new. However, I am not convinced by the claims in rebuttal, such as "using physics-based dynamics models" (I think you are referring to kinematics-based instead of physics-based), "3D extrapolations" (which could induce potential problems due to the multi-modal future uncertainty), and "self-training" (which is not new). Thus, if the paper gets accepted, I strongly encourage the author to rewrite the introduction and properly reflect the core contributions.Overall I am still on the positive side. But I am fine with both decisions.   After reading other reviews and the rebuttal, I opt for acceptance. ------Post Reponse Update-----Thanks to the authors for their updates. I have updated my score by 1 point here. I believe the exposition in the paper could still be improved at this point. In general, this work provides an interesting use of IV techniques for interpreting black box models. #########Edits after the rebuttal#########I have read the response and other reviewers' review/discussion. I will keep a score as 6. ---I have read authors' response and other reviews. Some of my concerns are addressed in the response. Especially the added discussion with related work is helpful. Thus I would increase my rating to 6. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. However, unfortunately, I still think a more realistic validation (e.g. on non-toy dataset) would benefit the paper.  ====================================================Post rebuttal:My concerns regarding the experiments are mostly addressed, though as pointed out by other reviewers, more convincing experiments under changing transition dynamics would be very helpful. I also stand by the authors explanation regarding limited resource in running RL experiments, especially for novel research directions.That said, for the same reason (pioneering research vs large-scale application), I do not agree with the authors explanation regarding the limitation of the proposed approach in assuming a known ground truth reward function. The main contribution of this submission is not in solving a specific real-world RL application problem as the cited references. As one of the first efforts in addressing the meta-RL under offline setting, I feel that setting this constraint is a limitation and should be relaxed by means of estimating the reward function. This should be addressed in future work.I raise my rating to a weak acceptance conditioned on the wording regarding "Bayes optimal" being more precisely presented, I think it is currently over-stated throughout the paper, which could be misleading to the community if published as it is.It is important to carefully reword in which sense the proposed algorithm "approximate Bayes-optimal policy", as explained in the authors' response, the algorithm is shown to qualitatively behave in a way that a Bayes optimal policy would do under this particular setup, this is far from sufficient to claim any approximation to the Bayes-optimality in a principled sense. I would like to also point out that while Bayes-optimality is generally intractable, it is possible and not uncommon for a method to start from an explicit pursue of Bayes optimal solution and specify where and how approximations are performed to overcome specific intractibilities, and further show quantitatively that such Bayes optimality is indeed achievable under well-controlled toy examples where the true Bayes optimal solution is known. The concept of Bayes optimality is in essence quantitative, rather than qualitative. UPDATE (after author response): I appreciate the authors' response. The inclusion of the hyperparameters are helpful. I also think it's an improvement that the authors added a comparison to ZSF+bal.(ground truth) to the Adult experiment.I still have a question about the experimental comparison to Hashimoto et al. (called "FWD" in this paper). Is the version of "FWD" implemented in this paper using exactly the same fairness criterion as in the Hashimoto et al. paper? If so, am I correct in saying that the "FWD" comparison in the experiments section does not directly constrain for any of the measured AR ratio, TPR ratio, or TNR ratio? The authors should clarify this in a later version.Overall, I'm willing to raise my score to a 6, but still think the paper is borderline. The paper could still use some improvement in covering related work on the problem of fairness where the protected attributes are not fully known (including the references I suggested). ===== Post-Discussion Update =====I thank the authors' efforts for responding my questions. Overall, I find the results presented in the paper interesting and worth publishing. It would be nice to extend the results to more general settings. =-= comments after author discussionThe authors were quite active in editing the submission, and addressing the concerns I had. I still find the paper a bit hard to follow, but none of my original concerns remain. -----After reading the author response and other reviews, most of my concerns have been addressed. I would like to increase my score to 6. It would also be interesting to compare the performance of DP-SGD and PDP-SGD when $\epsilon$ is relatively large as suggested by Figure 7. -------------------------------------------------**After Author Response**: I really appreciate the author's efforts over the course of the rebuttal period for rigorously testing their method with several new baselines in such a short period of time. For AIA attacks, the baseline numbers provided in the rebuttal are helpful but raise concerns about whether the proposed AIA attacks are working. I find it hard to believe that victim models have less private information than extracted models in 2 out of 3 datasets, and I suspect some other factors are contributing to this counterintuitive trend (like you said, maybe dark knowledge). I will stick with my stance that the AIA setting is broken since you are inferring private attributes using information from an identically distributed D_a (I think model inversion is a more valid setting to measure leakage).For adversarial attack baselines, I agree with your argument that conducting black-box attacks directly on the victim models may need minimal difference queries which can be detected on the API side. However, you are going to need several orders of magnitude more queries to do extraction in the first place (which may or may not be easy to detect). I still encourage you to run this baseline in the next version of the paper, instead of only doing black-box attacks on extracted models. These minimal difference checks may not be in place, and directly doing black-box attacks on the victim model are much easier than extracting and then constructing adversarial examples. It is good to know what additional benefit you get by doing model extraction.Overall, I have decided to raise my score to 6 (more like ~5.5-6). This is conditional on the authors performing much more rigorous hypothesis-driven testing in the next version of the paper (just like they did in the rebuttal) to really validate the hypothesis "extracting models make APIs more vulnerable to adversarial attacks". Post Rebuttal CommentI thank the authors for sincerely replying to my review comments. The authors' answers were reasonable to me. -------------------- UPDATE Nov 30 -----------------------------------------------------------------------------------I find the additional experimental work that was carried out for the revised version of the manuscript to be a step in the right direction which has provided more confidence in the claim of the paper.The presentation of results is a bit hard to follow... it's a bit hard to put finger on what is exactly the issue. One thing I would suggest is making the names of the methods a bit more telling, deeper into the paper it becomes hard to track which abbreviation stands for which method. It also messy when some methods are reffed to by a particular RL algorithm name "QD-TD3" some by just mentioning RL "CEM-RL" some just as "SAC" and this naming convention breaks when evolutionary methods are mentioned. Maybe for someone who works with these methods a lot it is easy to keep track, but not for a reader not directly involved with the QD field.Why in Table 2 the third column was changed from "Step to -5" to "Steps to -10"? How this number was picked?Figure 3, which is the main evidence for the main claim of the paper only appear in Discussion, leaving an impression that this figure is not that important and is a bit of a side-note.I sympathize with the lack of computational resources, which makes it very hard to compete, but if a 1-to-1 comparison with competitors if at all feasible, it would be worth it. If your methods is as strong as it seems it is, then it will beat the competition across the board in term of sample complexity and send a clean and powerful message that utilizing RL with QD is the way to go.At this point it might be that the main issue with this work not receiving higher scores lies not in the idea or experimental work, but in presentation. Try taking a couple of you colleagues who are not familiar with their work and observe how they read it, notice the moment when they start loosing focus. Your text jumps from one message to another making the overall narrative not as streamlined as it could. This might be reason for reviews like R3's, where, it seems, the reader gets lost and comes out without clear understanding of the outcomes of your experiments and how these support your claims.To summarize I still find the idea clever and with the new evidence I am more confident that the claim of the paper might hold in general. Since experimental evidence was my main concern and now there is more of it, I am upping my score from 4 to 6 - "Marginally above acceptance threshold". ########################### Post Rebuttal ##################I have read the other reviews and the author's responses. I thank the authors for conducting the additional experiments and integrating the feedback from the reviews. Accordingly, I am raising my score. Overall, I agree with the authors that combining QD with pg operators is novel - however, I am still not fully convinced that it is significant enough for a full paper at ICLR. This remains my primary reservation that prevented a higher score for the paper from my end. -----------------------------------UPDATE: The authors did a very good job at answering my questions and the new experimental results are very much welcomed, hence I'm updating my score from 4 to 6.Given that this is a highly empirical paper with relatively little novelty in the key idea, more comparisons would be necessary to justify increasing the score further. While I sympathize with the lack of computational resources and access to implementations, taking some extra time to implement and run those comparisons can be done. The code for RL methods with exploration bonuses (e.g. pseudocounts, RND) is accessible and these methods are not too costly to run. Methods like PBT (whose results are typically reported using large computational resources), could be implemented and compared in a regime with much more constrained resources. Updated review: Thank you for the clarification. The previous version was indeed confusing to me. I have raised my score although I think some points still need to be addressed in the revision following my previous comments as they were not fully addressed nor in the response neither in the revision:The concern with respect to previous works is not only regarding Parseval networks. There are other more recent works that use orthogonality constraints on the network. Such examples include https://ieeexplore.ieee.org/document/8877742 https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_All_You_Need_CVPR_2017_paper.pdf https://proceedings.neurips.cc/paper/2018/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.htmlAll these works show a similar observation to the one claimed in the paper that by using orthogonality (or frame-like) operators one may train a network without skip connections and get similar results. Indeed, in the paper, more observations are being made that are different than what is presented in these works but a more proper comparison should be made.This is the work the authors should look at by Mallat https://arxiv.org/pdf/1809.06367.pdf They get similar performance to ResNet with a scattering transform-based network. Indeed, also here it is not exactly the same network that the authors here are using but there are remarkable similarities and these should be well addressed. Post-rebuttal:I am mostly satisfied with the authors' response. After reading other reviewers' comments, I shared a similar concern on the marginal contribution. However, the newly added black-box result is a good addition to the paper. Thus, I keep my original rating toward the positive side.  ------Updates after response------I thank the authors for the detailed response and the revision. I am still not completely convinced regarding the suitability for ICLR and have similar concerns to reviewer 2, but am not opposed to acceptance.  In light of this, I have increased my score to 6. Update after revision------------------------------I thank the authors for their work on this paper. The second reading was more pleasant. I agree with the authors that performing a user-study is an important effort, that should be encouraged. I however still believe that, if not benefitial to the user, the complexity of the method can be a drawback. I also wished that more comparisons, but especially other data modalities were investigated. I have updated my rating to reflect the improvement in the text. --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------The authors have done a reasonable job at addressing my concerns and I have increased my score from 5 to 6.  [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. I do believe most of my concerns have been addressed. However, the concern on some possibly confusing technical details remains. The authors are expected to further revise their paper to make it more self-explained. #####################################After rebuttal: (from 5 to 6)Thanks for the clarifying the points in your response. I am happy to change my score to Accept the paper as most my concerns are addressed and I believe the paper is a good fit in this conference. A comment to the authors that didn't impact my score but raised some concerns:My concern is about mentioning the first place in VQA challenge 2020, both in paper and also in rebuttal comments. The review process is double blind and pointing out to other contributions that are public and reviewers may have already known about the winner teams, may not be fair. I know that papers can be online on arXiv but pointing to another venue as part of the contributions, may reveal the identity of the authors explicitly (if reviewers already know about the challenge) **Additional comment after rebuttal**Thanks to to the sufficient answers and results, the first rating is maintained. **Update after rebuttal:** I thank the authors for their detailed responses and the additional experiments. The responses addressed most of my concerns. I noticed that I had the wrong notion of redundancy ratio in my mind (I'm glad the authors now give a more formal definition of this concept as I think this would trip up many other readers). I'm also glad that the authors have clarified the difference between their results and those reported in Hanin and Rolnick (ICML, 2019). Given these, I'm happy to increase my score to a weak accept (a weak accept, because I'm still not quite sure about the significance of the results reported in this paper). -------------------------------------------------------------- "In addition, I am not sure the description would be enough to reproduce and no code seems to provide." In the beginning, I did not find the codes related to this paper, but later, the author(s) uploaded codes. Thanks. For the codes, if setting random seed program-wide in codes maybe it will be more helpful for reproducing. [Update after authors' reply]In light of the authors' reply, I have updated my review to favor acceptance. I appreciate the additional experiments. It will be up to the readers to determine how to interpret those additional results.--- * I update my score. UPDATE POST-REBUTTALMany of my questions have been answered, though I do think reviewers should explicitly note Ben-Zwi and Ronen's paper, and change their use of "incentive compatible". I think a deeper and more systematic analysis of the A matrix is also warranted, but I do now feel the paper has better scientific merit. =====================================================================================================The authors addressed my concerns in the discussion period and I therefore created my score to 6. ========post rebuttal review=========After reading the response from the authors and the new version of this paper, I decided to increase my score to 6. Update after rebuttal:Thanks for the response! I think it resolves my concerns on novelty and evaluation. Hence I raise my score to 6. 3-It would be great if the author can what would be the advantages/ weaknesses of their approach with the references. As the proposed approach has high similarity with the previous works, the minimum requirement is reporting more experiments and compare their method with exist methods. This extra study would present how their approach affect the performance.After rebuttal:Dear Authors, Thanks for providing more details. I believe more discussion and experiments are required to present the difference of you method. As you have mentioned, one difference is in considering summation rather than maximization, so it would be required to know what would be the advantages/ weaknesses of this difference. How does it make any impact on the performance? I would increase my score considering the closed-form solution as a nice contribution and requiring more experiments and analysis on the discussed references.Thanks! **POST-DISCUSSION UPDATE**I believe the authors to have addressed some of my concerns and I appreciate the additional experiments demonstrating that even the close results were more than a mere statistical artifact. Some other points remain still open such as the limited focus on Trajectron in evaluations.  In summary, I believe that the paper has now surpassed the acceptance threshold and am happy to recommend its publication. ***Post Rebuttal***I have read authors' response and other reviewers' reviews. After reading it is still unclear to me why sparsity of the networks could affect the performance of the proposed framework. Therefore I would like to keep my original score.  ---------------------Post-rebuttal---------------------I am improving my grade a bit. I recommend the authors to dedicate some time further improving the paper clarity, especially in the matters related to my review and the other reviewers'--------------------- Author response: discussion with the authors allowed to clarify what the agent see and how they exchange information. I still find the paper a bit difficult to understand, most probably due to its novelty, but I consider that the current version is acceptable. As such, I recommend accepting this paper.  After rebuttal, I think the author have addressed most my questions. However, I agree with other reviewers opinions that the paper need further polishing and clarification, especially regarding the cooperative settings of the problems (homogeneous team, number of options etc as admitted by the authors) and associated theoretical and practical issues. Given the novelty of the idea and the current status of paper, I maintain my score.  After the revision and the author response === I would like to thank the authors for their very elaborate response. I acknowledge that I might have asked for too many experiments in my review, but this was mostly because I really wanted to understand the various aspects of the method (as one other reviewer mentions - it has too many 'moving parts') and ensure that all the comparisons are valid by comparing to the latest work. While it is not possible (due to time and computational restrictions) to run all the required experiments, I am still happy to raise my score. While the paper might not be so impactful, it might still be a pretty nice and quick-to-compute BLI baseline for any future developments in this area.  Edit: Upgrade from 5 to 6 after the author's response. #######################Update:I thank the reviewers for their responses. I appreciate the effort they put into clarifying the paper. However, I still think Section 2.1 in particular is difficult to follow. I will therefore keep my original rating. --------------------------Thank you for your response. I raised my score accordingly. --------After author feedback, I feel that the authors did address a number of my points, including one or two that were indeed addressed in the text that I must have simply missed. Therefore, I am improving my score.The one point if any that I continue to disagree with the authors on is that variational dropout is sufficient to cover the space of explicit ensembling methods. Many of these "cost effective" ensembling approaches still continue to compare to direct explicit ensembling with the very explicit goal of having a baseline that is known to work well but removed from the computational cost concerns. I feel that this comparison should be standard practice. -------------------------------------Post Author ResponseThank you for providing the results and clarifying the setup. I am increasing the score to 6.  Post-discussion comment: I'd like to keep my rating (6: Marginally above acceptance threshold). I think R1's concerns regarding similarity between seen and unseen set are valid. As the number of seen combinations increases, the extrapolation problem becomes an interpolation problem. But in my view, that's not a weakness as long as the symmetric grid setting appropriately captures the relationship between the two tasks. I am leaning towards thinking that it does. It eliminates other confounding factors such as the relative weights of the loss terms too, and I think it is fair overall. Hopefully future work will provide more insight. Ultimately I think this submission is above threshold. ----Post-revision evaluation:The authors have modified the statements of the main theorems as well as including a more detailed comparison to previous works, which clarifies my concern. I have thus increased my score. The technical contributions bring new insight into the studying of scale separation of GDA, and enables a tight characterization of many toy examples. I believe these are solid contributions and should be valued.On the down side, I'd like to point out that the "practical implication" in this paper is a bit of stretch since the ImageNet experiments are run with RMSprop, whereas the analysis of this paper is highly specialized to GDA. Of course, studying adaptive algorithms in min-max games is exceedingly hard and well beyond the scope of this paper. What I recommend the authors is then:1. Explicitly notify the readers of the difference between RMSprop and GDA.2. Find a nontrivial but simple example where 1-GDA provides an okay baseline (say 7-layer CNNs for mixture of Gaussians or MNIST). Increase the time scale to show if it exhibits a similar behavior that a small $\tau$ gives the best result. This is directly verifying what the theory is saying, and hence feels more valuable to me. %--------------------------------%I thank the authors for clarifying my questions and concerns. The authors have included further theoretical developments in the revision, and they look satisfactory to me. Overall, I tend to accept this paper. ______________________________________ Updates in regards to the authors' response__________________________________________________________I appreciate the authors efforts to improve the manuscript and I think the manuscript has improved. I therefore raise my score to marginally above acceptance.In the abstract computational efficiency is highlighted but from the response it seems the approach scales as O(N^3) this is not particularly efficient as many procedures such as conventional SBM can exploit network sparsity for computational scaling. I think stating the approach as efficient is somewhat misleading based on the response explicitly made to Reviewer #4. I appreciate the added synthetic analyses and that a real network analysis is included but I am somewhat disapointed that only one real network is considered as opposed to including a series of existing networks with ground truth community structure. I thus find that the experimental validation on actual networks could be further strengthened - but it is good to see a real result.Minor comments:When stating "All of these approaches had theoretical guarantees." I believe this refers to some of the approaches discussed and not all. It would be good to clarify which approaches.estiamted -> estimatedwhich also been used -> which has also been used ---UPDATE: Due to concerns raised by other reviewers, and my own confusion about the computation of audio features, and clarifications on the causality stance, I have lowered my score from 8 to 7.  ---Update: After looking over the additional revisions and experiments, I'm bumping this to a weak accept. I agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples.I'm still not convinced by the TIMIT experiments, now that I better understand them, since the F+M baseline is quite strong and very simple to run. It simply doesn't seem worthwhile to introduce all of this extra machinery for such a marginal improvement, but the experiment does serve the job of at least demonstrating an improvement over existing methods. Thank you for the detailed reply and for updating the draft The authors have added in a sentence about the SLDS-VAE from Johnson et al and I agree that reproducing their results from the open source code is difficult. I think my concerns about similarities have been sufficiently addressed.My main concerns about the paper still stem from the complexity of the inference procedure. Although the inference section is still a bit dense, I think the restructuring helped quite a bit. I am changing my score to a 6 to reflect the authors' efforts to improve the clarity of the paper. The discussion in the comments has been helpful in better understanding the paper but there is still room for improvement in the paper itself.============= -------------------Revision. The rating revised to 6 after the discussion and rebuttal. Revision: The authors added many references to prior work to the paper and did some additional experiments that certainly improved the quality. However, the additional results also show that the shared experience buffer doesn't have that much influence and that for the original tasks (the humanoid results in the appendix look more promising but inconclusive) the reloading variant seems to catch up relatively quickly. Reloading and distributed learning seem to lead to the largest gains but those methods already existed. That said, the IPE method does give a clear early boost. It's not clear yet whether the method can also lead to better end results. I improved my score because I think that the idea and the results are worth sharing but I'm still not very convinced of their true impact yet. Update: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.The Pong case is also quite interesting, although it seems slightly "unfair" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards. == After discussion phaseBased on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating. i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted. I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. PS: I am downgrading my confidence in my evaluation.--- I have read the author's response, and I would like to stick to my rating. From the authors' response on the convergence issue, the result from [1] does not directly apply since the activation function that the authors use in this paper is relu (not linear). Having said that, authors didn't find any issues empirically.Q7: Yes, I agree that the result depends on the gradient structure of the relu activations. But my point was that, it is still a calculation that one has to carry out, and the insight we gain from the calculation seem computational: that one can regularize jacobian norm easily. True, but is that necessary? Or in other words, can we use techniques (not-so) recent  implicit regularization literature to analyze KFAC? I still think that the work is good, these are just my questions.==== EDIT: Updated rating after author revisions. #Update I thank the authors for addressing my questions and revising the manuscript, which clarified many of my concerns regarding this work.  Update: The authors have addressed the most pressing issues with the manuscript. I've increased my score and vote in favour of accept. The section of limitations was difficult to follow and would benefit from a more structured comparison with competing methods.  **Updates after Author Response:***Complexity Measure*: My concern with the complexity measure is that it seems to me that the biggest difficulty in reinforcement learning is in actually being able to estimate accurate value functions, while proposed complexity measure really only vaguely captures how far optimal policies are from the behavior distribution. In particular, at each state, it only depends on the true values of Q for each action, and can't capture how hard it is to estimate them.Even among similarly structured MDPs, we can consider a 2-action MDP with a behavior policy that was simply uniform. We could have one extremely simple MDP that was simply composed of independent deterministic bandit problems at each state with no transitions (always remain in your starting state), which would be trivial to solve even from offline data with full support.On the other hand, we could have a much more complex MDP with meaningful stochastic transitions, random rewards and so on. If we simply match the Q values in the two MDPs, they appear to be equally complex from this measure, despite the fact that the bandit MDP is far simpler to solve.As such, I think the proposed complexity measure doesn't really reflect the real challenges of an offline RL problem, and am not sure how one would extend it to be useful.*Re misc comments and access to true behavior policies:* My thinking here was that if we had a very stochastic behavior and finite samples, there would be actions that have reasonably high probability under the true behavior policy, but we never get to see in the data and wont' be able to evaluate well. One benefit of fitting the behavior model the the empirical data is that it would focus on those actions that do appear in the dataset (in an extreme case, we could simply have Dirac deltas on the observed data points), and so could benefit by restricting the actions to those that can be evaluated better.*Overall Opinion:* In light of the empirical results mentioned in the author's response as well as the comparisons to KL regularization, I have raised my rating. I still do believe it is a very borderline paper, and would perhaps benefit from more careful analysis and focus on how the different behavior modelling choices influences offline.  -----Post Rebuttal-----I have read all feedback and especially thanks to the authors' efforts on the extra experiments. I think the author has addressed my first concern. For the second one, I would prefer to see the errorbar.I tend to keep my scores unchanged. I think the findings are interesting [share similar thoughts as R4's], while the experiments part need to be improved. Although I like the ideas and observations, I don't feel especially strongly in favor of it and cannot champion it.  ------Edit:Thank you for the author response. Even if you consider the story to be the same across literature (which in this case is not, since the more recent models handle spatial relations that the previous ones failed on), it's still worth doing due diligence to the recent work, especially so that the reader gets a better sense of how to position your work amongst these. Updated=====================The authors addressed some of my concern, and I appreciated that they added more experiments to support their argument.Although I still have the some consideration as R3, I will raise the rating to 6. I took a look at the revision.  I am glad to see that the authors clarified the meaning of "optimality" and added time complexity for each algorithm. The complexities of the algorithms do not seem great (3rd or 4th order polynomial of N) as they appear to be checking things exhaustively, but perhaps they are not a big issue since the decomposition algorithm is run only once and usually N is not huge. I wonder if more efficient algorithms exist.It is also nice to see that the time overhead for one training step is not huge (last column of Table 1). While I still think it is better to see more complete training curves, the provided time overhead is a proxy. I hope the authors can further improve the algorithm description, for example, the pseudo-code of Algorithm 1 is very verbal/ambiguous, and it would be better to have more implementation-friendly pseudo-code.Despite the above-mentioned flaws, I think this work is still valuable in handling the memory consumption of arbitrary computation graph in a principled manner. ============================================================================= Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result. ----Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result. Update after feedback: I would like to thank the authors for their detailed answers, it would be great to see some revisions in the paper also though (except new experimental results).Especially thank you for providing details of a training procedure which I was missing in the initial draft. I hope to see them in the paper (at least some of them).I have increased the rating to 6. Given new experimental results both on real data and forecaster comparison I would like to increase the rating to 7. However, I am not sure that this is fair to other authors who would might not be physically able to provide new experimental results due to computational constraints, please note that the experiments in this paper are rather 'light' in the standards of modern deep learning experiments and can be done within the rebuttal period.  ==================================================== UPDATEMy most serious concerns have been addressed in the revised version. ========Thank you for the detailed responses. I have updated my score from 5 to 6.  --------UPDATE AFTER READING THE AUTHORS' COMMENTS-----------1. Appendix F lacks explanation. So I'm going to say what I meant in details. In order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). I would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced:1) principal components of digit 0 and digit 9 differs a lot AND 2) w_0 weights components of digit 0 higher but weights those of digit 9 lower2. "But for our regularized model, the number of weights with high values is smaller compared to that of normal model ..."I'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ##############Based on the authors' response, revisions, and disucssions we have updated the review and the score.  ########################I would like to commend the authors for making a significant effort in revising their manuscript. Specifically, I think adding the experiments for QSGD and Krum are an important addition. However, I still have a few major that in my opinion are significant:- The experiments for QSGD are only carried for the 1-bit version of the algorithm. It has been well observed that this is by far the least well performing variant of QSGD. That is, 4 or 8 bit QSGD seems to be significantly more accurate for a given time budget. I think the goal of the experiments should not be to compare against other 1-bit algorithms (though to be precise, 1-bit QSGD is a ternary algorithm) , but against the fastest low-communication algorithm. As such, although the authors made an effort in adding more experiments, I am still not convinced that signSGD will be faster than 4 or 8 bit QSGD. I want to also acknowledge in this comment the fact that these experiments do take time, and are not easy to run, so I commend them again for this effort.- My second comment relates to comparisons with state of the art algorithms in byzantine ML. The authors indeed did compare against Krum, however, as noted in my original review there are many works following Blanchard et al.  For example as I noted <a href="https://arxiv.org/pdf/1802.07927.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.07927.pdf</a> (the Bulyan algorithm) shows that there exist significantly stronger defense mechanisms for byzantine attacks. I think it would have been a much stronger comparison to compare with Bulyan.Overall, I think the paper has good content, and the authors significantly revised their paper according to the reviews. However, several more experiments are needed for convincing a potential reader of the main claims of the paper, i.e., that signSGD is a state of the art communication efficient and byzantine tolerant algorithm. I will increase my score from 5 to 6, and I will not oppose the paper being rejected or accepted. My personal opinion is that a resubmission for a future venue would yield a much stronger and more convincing paper assuming more extensive and thorough comparisons are added. ##### added after author response #####I appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). Several changes have been made to my comments, thanks for pointing out the mistakes.  Post RebuttalThanks for the author(s)' responses. The rebuttal addressed some of my questions. I have a couple of  suggestions : 1- Your proposed capsule network is not the first one that is applicable on large scale datasets like ImageNet, there are other capsule networks that are applicable on real world scenarios and also ImageNet dataset with improvements over the baseline - Dual Directed Capsule Network for Very Low Resolution Image Recognition (ICCV 2019)-  Subspace Capsule Network (AAAI 2020)please refer to them and also give intuitions about why your proposed Trans-Caps is not performing well on ImageNet. The intuition and analysis is valuable to the community.  2- To support the generalizability claim of Trans-Caps, I highly recommend reporting results on Multi-MNIST and also affNIST. Specially when you train the model on MNIST and test it on these two datasets.  Update-------I've updated by score in light of the discussion; as I said in the comments, from a purely experimental point of view there are good results, however the presentation of the paper confounds too many aspects. If the authors can address the terminology issues then it would make the work stronger. .**Post-discussions**: the authors have clarified the meaning of "invariant classifier", which is now tied to their specific toy problem, and I now believe such a classifier is indeed the minimax OOD classifier supposedly seeked by IRM. While I still think the paper does not shine on the side of clarity, my main concern about the incorrectness of the presented theorems has been answered and I believe the paper will be of interest to the community. I therefore raise my recommendation towards acceptance After reading the response from the authors, we raise our score by +1. ---The authors have addressed many of my concerns in the rebuttal, and so I am increasing my rating.  ---Post-rebuttal My main concern was assessing the value of the overall contribution of the paper. The other reviewers seem to appreciate both the new environment being offered and the combination of techniques deployed in the authors' solution. If there is an audience that will appreciate this work at ICLR as seems to be indicated by those reviews, then I would increase my score to marginally above the acceptance threshold.  === after rebuttal ===Thanks for the authors' response. Some of my concerns have been clarified. I increased my rating from 5 to 6. ----------------------------------------AFTER REBUTTAL:Thanks for your reply to my comments. The new revision has improved clarity and provided new supporting evidences. I would like to raise my rating to 6.That being said, (as you agreed) the link from the conceptual goal to the proposed objective has mostly empirical support. Therefore I hope it may encourage future investigation on when and why the proposed objective is successful in achieving the conceptual goal. Edit: After paper additions I am changing my score to a 6. (After reading the rebuttal, I raised the rating from 5 to 6.)  UPD: the discussion and the edits with the authors convinced me that I may have been a bit too strict. I have changed my score from 5 to 6. Thanks for the updates and rebuttals from the authors. I now think including the results for HAT may not be essential for the current version of the paper. I now understand better about the main point of the paper - providing a different setting for evaluating algorithms for combatting CF, and it seems the widespread framework may not accurately reflect all aspects of the CF problems. I think showing the results for only 2 tasks are fine for other settings except for DP10-10 setting, since most of them already show CF in the given framework for 2 tasks. Maybe only for DP10-10, the authors can run multiple tasks setting, to confirm their claims about the permuted datasets. (but, I believe the vanilla FC model should show CF for multiple permuted tasks.)I have increased my rating to "6: Marginally above acceptance threshold" - it could have been much better to at least give some hints to overcome the CF for the proposed setting, but I guess giving extensive experimental comparisons could be valuable for a publication. ===================== [REVISION]The work is thorough and some of my minor concerns have been addressed, so I am increasing my score to 6. I cannot go beyond because of the incremental nature of the work, and the very limited applicability of the used continual learning setup from this paper. -  My concerns about figures are solved; I want to thank authors for their efforts. *********EDIT: Changed my score from 5 to 6 after the author response/revision. **Update**Thank you to the authors for the detailed response. Overall, I'll leave my rating as a 6. The paper is clear and proposes a promising optimization method, with moderate comparisons to prior approaches and reasonable results on ImageNet (but not CIFAR-100). The impact of these results are somewhat diluted by the overall absolute low scores in the adversarial setting, but as a general DRO optimization method, the result is interesting. Thank you for all the discussion.Comments to the latest author response (posting this as an edit since public comments are now disabled):[1] Thanks, the updated version is clearer.[2] I agree with this characterization (for example, the unconstrained perturbation might cause optimization instability). The paper still seems to contain the language about the Sagawa paper requiring a custom sampler. Overall, I agree that there are reasons to believe that the proposed optimization method is more stable, but I think the authors should be clear that they only compared to Mohri et al. experimentally (in your response to Reviewer 3, you mentioned that you extensively discussed and compared to both Mohri and Sagawa).[6] Thanks. I also agree with this. I had two points: first, the differences between your proposed algorithm and the standard algorithm might be more stark when the training is not balanced. Second, in imbalanced settings, it is more likely that the test distribution will be skewed (e.g., rare animals in iNaturalist). --In light of the extended experiments w.r.t. to 2.1 I increased my score from 5 to 6.  Overall, I still have doubts about the interpretability and complexity of the proposed method.  Complexity:  "but all the intuitions needed would come solely from training NN".    I disagree with this response.   The architecture is a mix between a tree (hard, decision-tree like error surface,  non-local) and neural network (smooth, mostly convex error surface). This also implies that the training process and its behavior will possess patterns and challenges of both approaches. Interpretability:  I think the method misses "priors" that enforce credit assignment.  Partitioning the problem in subp-roblems should be done via the tree components, whereas processing (such as image filtering) should be done in the network nodes. However,  the method does not enforce, or encourage this behavior, for instancevia constraints:   also nodes can do partitioning (because neural networks can approximate decision trees)  and edges can do processing (e.g. decisions-trees can be used for mnist).So I still believe this to be a borderline paper, however, the experiments support a more general applicability. I'm happy with the revisions the authors have made, as I find that they call out the novel contributions a bit more explicitly. Specifically I see some novel work in the area of simultaneous multi-task/meta-RL and black box optimization of the policy net architectures. I don't think calling this NAS is justified; calling it bayesopt or black box opt is fair. NAS uses a neural net to propose experiments over structured graphs of computation nodes. This work appears to be simpler hyperparameter optimization. ------------------------------- After Rebuttal ---------------------------------I am very satisfied with the authors' response, so I will change my vote from rejection to acceptance. %%%%%%%% After rebuttal %%%%%%%%I appreciate authors' efforts to address my comments and am satisfied with their response. I will change decision from rejection to acceptance. Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.------ --------------------------Update:The authors addressed some of the weak points mentioned above adequately. The experimental evaluation was significantly improved and the results are a nice contribution. However, the theoretical contribution and the poor motivation of capsules in the graph context remain weak points. I have updated my rating accordingly.  Update:According to the revised version which addresses a lot of my concerns, I vote for marginally above acceptance threshold. I have read authors' response.  Revised: increased score after author response. I still like the overall mission of this paper and found it highly readable. However, after a more careful reading I do agree with the issues raised by the other reviewers. It seems that there is a fundamental question in the field as to a) how important meaning preservation is for adversarial attacks and b) how this should be assessed. In its current form, I don't think this paper provides satisfactory answers to these questions, but it does point at an important topic to be resolved. Edit: The addition of HER experiments push this up a bit (5--&gt;6). I'm still concerned about how significant the contribution is (as it is a straightforward extension to SFs), but the empirical results are now quite strong. - Thanks for pointing our these results in previous work of Bucila et al. I think this deserves to be explicitly mentioned in your paper, because it provides direct evidence for your claim.- I think you should report the effect of data-augmentation *alone*. Also, adding more than one data-augmentation strategy would strengthen the result (especially for image data, where there are plenty of effective methods). That being said, I agree with a point raised reviewer Reviewer-2 that for other kinds of data (e.g. tabular data) there might not be effective ways for data-augmentation.- I am not sure how you compare the execution time of inception score compared to your methods. Fundamentally, inception score requires only a forward-pass in a pretrained model, which can be done with a small number of examples (e.g. 1K or 5K). Training a model from scratch would require a forward and backward pass, and probably on much more data, but I guess the model is much smaller than inception. Also, I'm not sure it's clear from the paper that you do only one epoch of training.I am going to raise my score to 6, because I think the paper has some interesting aspects to it. I still believe it's a borderline paper, especially that I'm not convinced of the effectiveness of compression score and that GANs can be substantially more effective than data-augmentation. ---I thank the authors for their hard work addressing issues raised by the reviewers.Authors have answered many issues pointed out (by improved performance and showing robustness to hyperparameters) and I've increased my score from 5 to 6, and support accepting the paper.  ================================================================================================After reading the author's response and the other reviews I still lean slightly towards acceptance and have therefore left my rating unchanged.While not being an expert on the subject, I find the work interesting. In case the paper gets rejected, I recommend to the authors to the feedback provided by the referees to clarify the narrative of the paper.  ======after rebuttal=========Thank you for answering my questions.  I understood these points. The authors added a new simple experiment and a code, whereas the manuscript at the current stage can improve the clarity. All things considered, I increased the rating.  My final recommendationI remain my initial score after the discussion. **edit, based on revision and after discussion**.  Thanks to the authors for answering questions 2 and 3 in discussion and adding an experiment to the paper validating the tropical pruning method in a small setting.  The additional experiment provides some evidence that the tropical pruning method is working as argued, and I change my rating of the paper to 6. Update:The revisions are good. The paper is very easy to follow and most of the story is pretty clear. Theory sections are clearer as well. So I'll improve my score as the authors followed through with both mine and other reviewer's comments. There's one hitch: Pr1, the one having to do with the labels, is not well substantiated in the paper, though it gets first-class treatment in Fig 1 as well as being one of 2 main principles guiding this method. Some of this is carried over from the de-biasing work, but I have concerns that there's essentially a trade-off between hardness and label distribution depending on beta. Unfortunately, this paper does no empirical analysis on the labels in q, and I worry that readers may be mislead that something close to Fig 1 might happen in practice.==== -----AFTER REBUTTALI have carefully checked the author rebuttal, and it addressed several of my concerns. I thus improve my score to this paper. ### post-rebuttalThe authors have addressed most of my concerns, thus I will increase my score from 5 to 6. # Update post author response:Thanks to the authors for the response. The newly reported results (specifically, those of Appendix E.2) satisfactorily address my concerns about both the generalization of the pruning method v.s. re-training scheme results, both in terms of sparsity levels and unstructured/structured pruning (though the observation of the relationship between R-CLR and FT do not hold quite as strongly for unstructured pruning, they do hold at high enough sparsities to be interesting). Ive raised my score to a 6 as a result. # After rebuttalI would like to thank the authors for their extensive efforts during the rebuttal. My main concerns are resolved, so I change the rating to borderline accept. I encourage the authors to update the paper with the results provided in the rebuttal, especially the explanation of the novelty and the results for "Proposed metric with existing search algorithms". After rebuttal:The authors' response addressed some of my concerns and I'd like to adjust my rating to marginally above. --------------------------------------------------------------------------------------------------------------------------------------------------The author response addresses my major concern on the experimental results. Therefore, I have updated my rating from 5 to 6. Updates:The author addressed my concerns about the experiments. Though the improvement is marginal and I still have some concerns, Im ok to accept the paper. Ill change my score to 6.======================== ----------------------------------------------------------------------After Rebuttal: I would like to thank the authors for answering my questions and addressing my concerns on hyperparameters. I also appreciate the authors' efforts in the additional ablation study conducted. While I still do think that the novelty of the paper is a little bit lacking, I think the experiments are carefully conducted and the empirical results seem to be encouraging. As such, I have raised my score to 6. __post-rebuttal__The responses have been persuasive enough. I am raising my score, with an expectation that the authors will make additional textual revisions based on their responses to make it clear in the abstract and introduction that (1) authors only consider the "reconstruction-based" SSL, instead of SSL in general, and (2) address the discrepancy between the practice and the proposed framework. (I thought asking questions could make the authors revise the manuscript, but unfortunately that did not happen.) ------Update after rebuttalThanks for the author's response. Combing the author's response (though the authors didn't upload new versions to address my comments -- include more intuitions and discussions) and other reviewer's comments and discussions. I suggested this paper being marginally above the acceptance threshold. ---Post rebuttal---Thank you for the detailed response. Overall, I think the proposed work provides a valuable benchmark for testing generalization ability of RL agents. However, I agree with R3 regarding the writing being dense/difficult to follow. I keep my rating unchanged (Weak Accept).---- **Post-rebuttal**I read the rebuttal and the other reviews. The rebuttal addresses my concerns to some extent (writing has improved in the revised version, but it still has some issues). So I am going to keep my rating.  --I've read the authors' response and would like to maintain my original score ---## Updates:The authors have carefully responded to my comments. Their response addresses most of my concerns. I will keep my score high. I understand that the choice of the hyperparameters can be computationally heavy, but the authors have given an idea to solve this problem. It is good to find that the Power Spherical distribution also includes the Dirac distribution as a limiting case.  -----------------------------AFTER REBUTTAL-----------------------------I appreciate the authors response. Most of my trivial comments and questions have been resolved. I stand by my initial rating. This is a solid paper. The authors clearly introduce the problem and develop a clear story with a straightforward solution. The paper ends with extensive experiments. My main concern remains: the contributions of the paper to the ML community is moderate because the story is very narrow. I think the idea can be substantially extended to solving the fundamental problems in sliced Wasserstein distances but I don't object acceptance of the paper in the current form. I recommend the authors incorporate suggestions from all the reviewers and polish the language especially Section 2 to make the paper more accessible for readers outside the sliced Wasserstein community. Thank you. **After Author Response** I think the authors made a good effort to address the concerns and I have recommended to accept the paper. ==================================================================I thank the authors for their thorough comments and experimental details. I am more satisfied after reading them. I am raising my score from 4 to a 6. **UPDATE:** My major concerns were addressed in the revised version of the paper. **Update after author response:** I appreciate the authors' efforts to address my comments. The new version reads better. However, I am still not entirely convinced by the choice of the simple baselines. Since a positive rating is already given, I would keep it unchanged.  **Update to review**I have increased the score from 5 to 6. The authors have made the paper stronger with the inclusion of stronger baselines, cleaned-up presentation, and backward transfer. I am particularly excited by the new suite of benchmarks. ---Update after author feedback: I thank the authors for their reply. The authors have addressed all of my concerns. Therefore, I increased my final rating. ------------Post Rebuttal:I thank the authors for their response. I am mostly satisfied with the authors' response to my (and other reviewers') concerns about properly citing prior works that jointly consider coupled image generation with downstream tasks and reframing the novelty of their work in juxtaposition to them. I would like to point out, however, that the authors' statement in the rebuttal "(2) we achieve bi-directional feedback while this work only implements the feedback from viewpoint estimation task to the generative network." is technically incorrect. The viewpoint estimation network in Mustikovela et al. is directly trained with images generated by the synthesis network under various viewpoints and hence it also achieves bi-directional feedback much like this current work. The authors should clearly re-frame their novelty and make this correction in the final version if accepted.Nevertheless, I do feel that this work adds to the body of literature on joint conditional synthesis coupled with downstream vision tasks by (a) showing improvements in the quality of image synthesis achieved by considering downstream tasks and (b) by showing improvements in few-shot learning versus prior methods where only features are hallucinated, and (c) considering other applications beyond viewpoint estimation. Hence its contribution is above the acceptance threshold. I will maintain my previous rating. After rebuttalAfter reading the authors comments' and other reviews, I think that this is a borderline paper that could benefit from more rigorous experimental validation. ---Edit: score increased from 5 to 6 .**Post Author Feedback Comments**The authors have tried to address my main concern by adding an assumption on the distributions of gradients. Essentially they have assumed that the gradient distributions are sampled from a small, finite set of distributions. I don't know how realistic this assumption is, because each client can potentially have a different distribution. However, in practice, this may be approximately true. A better assumption would have been based on probability distances (TV distance, Wasserstein distance, etc.). I have increased my score to 6 After the rebuttal.The authors addressed my concerns. I have read other reviewers' comments. I decide to remain the current score. I have read the authors' response and the associated discussions, and based on that  raised my evaluation by 1 Update after author response: The changes made have improved the clarity of the paper, such as making assumptions and the threat model more explicit, and the heatmap addition provides a nice qualitative insight. However, I am inclined to agree with other reviewers that the paper's contribution is incremental. Given this I am retaining my score of marginally above acceptance. ==================I thank the authors for the additional experiments which have marginally satisfied my initial concerns; ideally, more setup can be experimented. I keep my original rating. Update: I read the comments of the authors and thank them for the clarifications. The additional baselines improved the paper. I raised my score to reflect that. Update: I think the authors did a great job of addressing my concerns, I'm happy to increase my score to 6 update:---Overall speaking, the added GP-BO results address my  concerns, and I've updated the score from 5->6. A final update will be given later. ---Edit after rebuttal: Changed score from 5 to 6 (see below) ##########################################################################Update after rebuttal discussions:- In the light of the considerable overlap with [Chiu et al., 2020] pointed out by the other reviewers, I decreased my score. I have familiarized myself with it and can confirm the said overlap. However, given remaining differences, I do not find it unreasonable to consider this paper as "complementary" to [Chiu et al., 2020], *provided that the authors explicitly address the similarities in the final version*.- I consider the sum total of contributions of the paper still tilting towards being sufficient for publication. ======== AFTER REBUTTAL ========I appreciate the authors' efforts on additional thorough comparison to existing works on interpretable axes discovery. From the updated manuscript, however, it is not clear what method is superior and the authors' approach appears to be a yet another method for this task rather than generalization of previous ones. Overall, I am still on the positive side since the observed findings deliver a clear profit for GAN inversion. But I am not increasing my score given that the "interpretable axes" part has become less impressive (in terms of weaker claims and conclusions) and the competing SIGGRAPH work.    #### Post rebuttalThank you for providing the rebuttal. The rebuttal addressed my concern on comparing to other baselines. And it's fine to keep the design choice experiments in the main paper. However a proxy evaluation of key point evaluation is still missing and it will further strengthen the paper (I don't have a clear idea for the evaluation either). I keep my original rating of 6.  ====after author response===I would like to thank the authors for the detailed response which resolves some of my concerns about the novelty of this paper. I agree that this is the first work (as far as I know) that brings NAS to unrolled algorithms. Unlike what I initially commented, 'my score is actually between weak acceptance and weak rejection', now I am happy to rate this paper as a weak acceptance. ----------Update after author response----------I thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score. After Rebuttal :Sorry for the delay. I checked the comments of the other reviewers, responses, and the revised version. The authors address my concerns in the revised version. I vote for marginal acceptance. The revised version of the paper with additional experiments has addressed my concern on the limited advantage over prior deep 3D representation.  Fig.6 in the revised version has shown the potential of the proposed approach in generating directly usable representation in downstream applications, including CAD design and manufacturing. Hence, I would change my rating to positive.However, the paper still lacks quantitative and qualitative evaluations. I hope it could be properly addressed in the final version. ===========================================================================================================Update:Thanks to the author's response, solving my concerns on this work. I decide to increase my score to 6. Besides, I suggest supplementing the mentioned discussions and empirical comparisons between Eq. (2) and Eq. (4) in the manuscript, as least in the appendix. Update: I'd like to thank the authors for their detailed response to my comments. - I respect the argument that CUT requires a model per pair of domains hence it does not scale to 10 domains. However  presenting the problem in such an aspect ("we evaluate only on multi-domain datasets such as the AnimalFaces, Food") limits a little the prior work that one can evaluate against. In my humble opinion the authors constrain the definition of unsupervised image-to-image  translation such that their proposed approach "is the first to succeed in this task" whereas I also think CUT is an unsupervised image-to-image translation method that "succeeds in this task". If the  argument was presented in the multi-domain setup I would be willing to buy it but as it is presented currently I find the first contribution as an overstatement. - As for SEAN my point was not to compare against it but that the architectural design of this work is not far from what prior work is already doing. - Sounds good thanks for the explanation. Having said that I still think this is a solid submission with interesting findings in an under-researched problem hence I do recommend acceptance.  =======Update:Thanks for the feedback from authors. Mostly my concerns are addressed. I suggest adding the discussion on the selection of K in the draft as this is important for readers to know when facing just a collection of images. ---Post-Rebuttal---Thank the authors for their response. I now agree with the authors and other reviewers that the authors' approach has its novelty (self-supervised, rendering, etc.), and the ablation study in Table 1 is reliable to prove each component is useful. Therefore, I increase my rating by 1. EDIT after rebuttal period---------------------------------------many thanks to the authors for taking into consideration my comments. I decided not to change my note because I still believe the paper lacks of a significative experimental Section.   ##########################################################################After discussion:Since the authors and other reviewers have addressed my concerns, I would like to change my score. The introduction of LRGA into the RGNN is interesting with the theoretical analysis, although the network design is incremental. I am now leaning towards borderline acceptance.  ****Reply to authors' rebuttal****Dear Authors,Thank you very much for all the effort you have put into the rebuttal. Based on the improved theoretical and experimental results, I have decided to increase my score from 5 to 6.Best wishes,Rev 1 [UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6. Thank you for the updated paper. The revised version is significantly better than the initial submission and addresses many of the points raised (most importantly, it provides quantitative comparison against existing methods).  I have updated my score based on the latest iteration of the paper  ==after rebuttal==After reading the authors rebuttal I increased the my rating to 6 as they addressed some of my doubts. I still think that the studied setting is too idealized, but it is a first step towards an analysis. Revision:Thanks for the work of the authors' and all the reviewers. I spent sometime reading the rebuttal as well as the revised paper. It addressed most of my concern. I would like to change my rating from 5 to 6. After rebuttal:It's better now. However, the revised introduction still says:  "GRU has become wildly popular in the machine learning community thanks to its performance in machine translation (Britz et al., 2017) ... LSTM has been shown to outperform GRU on neural machine translation (Britz et al., 2017).... specifically unbounded counting, come easy to LSTM networks but not to GRU networks (Weiss et al., 2018)." So better remove the first statement on Britz et al: "GRU has become wildly popular ... in machine translation (Britz et al., 2017)" because they actually show why GRU is NOT wildly popular in machine translation, as correctly justified later in the same paragraph.Pending the above revision, we'd like to increase our evaluation by 2 points, up to 6! [ADDENDUM: given the author feedback and addition of the benchmark experiments requested I have updated my score.] NOTE:  thanks for your good explanation of the Bayesian aspects of the model ...yes I agree, you have a good Bayesian model of the GAN computation , but itis still not a Bayesian model of the unsupervised inference task.  This is a somewhatminor point, and should not in anyway influence worth of the paper ... but clarificationin paper would be nice. updated score after authors revision In response to the authors' rebuttal, I have increased my ratings accordingly. I strongly encourage the authors to include those ablative study results in the work. I also strongly recommend an ablative study on importance sampling so as to provide more quantitative results, in addition to Fig. 4. Finally, I hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.================================= Post-rebuttal update:The authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).I did spot one typo on page 4 - "exterme", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.--- Update: I thank the authors for providing clarifications and additional experiments, in particular the comparison to open-loop grasping (SOTA grasp detection method from Mousavian et al.). I still find the technical novelty of the paper limited.  # After Rebuttal: Score Lowered from 7 to 6## Concerns AddressedI appreciate the effort the reviewers put into revising the paper to include the settings I suggested.I am generally pleased with the revisions the authors made to the paper (especially Section 3), and I appreciate their attention to these details.## Remaining Concern: Settings in the Main Body are Poorly-Tuned and May Overstate ResultsI am concerned by one aspect of Figure 2: the unpruned accuracies for VGG-16 and ResNet-20 are much lower than they should be. VGG-16 should get 94% accuracy on CIFAR-10 (vs. 91% in the plot), and ResNet-20 should get 92% accuracy (vs. 86% accuracy in the plot).  This is because the paper uses Adam to train all networks without any learning rate drops, whereas the typical learning rate schedules for VGG-16 and ResNet-20 use SGD with momentum with learning rate drops.This important difference raises the concern of whether the results shown in the paper will translate into fully-tuned, large-scale settings. As evidence of this concern, Appendix G does show a fully-tuned VGG-16 getting standard accuracy. In this setting, LAMP is no better than global magnitude pruning until very extreme sparsities.**I have lowered my score on the basis that the results in Figure 2 may overstate the value of LAMP in well-tuned settings. I no longer have unequivocal confidence that LAMP is an improvement that should be adopted in general. I implore the authors to replace the experiments in Figure 2 with well-tuned versions of these networks that achieve SOTA accuracies.**## Overall: Score Lowered from 7 to 6I am less confident in the method's significance in well-tuned settings, and I can no longer unequivocally trust the empirical evaluation in the paper. I still support acceptance, but only tentatively. --- post rebuttal update ----The authors successfully addressed my initial concerns regarding more analysis and experiments on a larger dataset. Therefore, I keep my rating weak accept. Update after author response:I appreciate the authors' efforts to address my concerns. I still find the paper's insights lacking in the critical middle ground between theory and practice, but understand that drawing these connections is a long game. I believe the paper leads research in an important direction and so can be published despite these flaws.  Edit: Having gone through the updates and the author's replies, I am increasing my score. =============== Edit ===============After reading the authors' response to me and other reviewers, I think my concerns are sufficiently addressed.Therefore, I update my rating from 5 to 6.There is still room for improvement in terms of the writing: as raised by the other reviewers, the text is a little bit dense to read. It would be great if the authors can further refine and brush-up the flow of the paper to make it more accessible. ## After rebuttalSome of the issues are addressed. So I change my rating to 6.  After reading the rebuttal:The authors have addressed my major concerns with regard to this paper.   I have lifted my score.  Thanks for the nice response. UPDATE: after revisions and discussion. There seems to be some interesting results presented in this paper which I think would be good to have discussed at the conference. This is conditional on further revisions of the work by the authors. ########################################### Updated review: The authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time. I have changed my score to a 6. #########################Post rebuttal: The authors have addressed most of my concerns regarding the poor presentation of the earlier version. I have updated my score. Update:  I am satisfied with the answers and have upgraded my rating. Summary:This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data. For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned. For the logistic loss, this paper further shows the linear function is the maximum margin solution.Comments:This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect. These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks. The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results. There are two weaknesses. First, there is no convergence rate. Second, the step size assumption (Assumption 5) is unnatural. If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption? Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar.However, I don't think this is a strong theory people due to the two weakness I mentioned. ========Thank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6. ----I acknowledge that the authors have made improvements to the paper and have increased my score to 6This is still definitely not my area of expertise and so I am leaving my confidence score low. --- After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances.   ----Updated review:After reviewing the comments and the paper in more detail (whose story has evolved substantially) , I have revised my score slightly lower. While in hindsight I can see that the paper has definitely improved, the story has changed rather dramatically, and appears to be still unfolding: the paper's many new elements require further maturation, and that the utility of empowerment for reward shaping and/or regularization to evolve AIRL (i.e. the old story vs. the new story) still needs further investigation/maturation. If the paper is accepted I'm reasonably confident that the authors will be able to "finish up" and address these concerns. (typo: eq. 4 omits maximizing argument) Update (22.11)I think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. - The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.- The derivation could be a bit more rigorous.As the presentation is now much more sound, I slightly increased my rating. EDIT: the new version of the paper is much better but still feels like a bit incomplete. I personally would prefer a more complete evaluation and discussion of the proposed method. After author response: I've read the response and other reviews and keep my score (weak accept). R3 says that "The second part of information maximization loss is nearly the same as that of SHOT-IM", but I don't see it as a problem. The first part seems interesting and novel, and to get strong results they need to combine it with an existing technique, which seems typical. I agree with the authors and disagree with R3: that their "method outperforms it (SHOT) in 7 out of 9 scenarios." So the positive is that it's a nice idea, simple method, and performs quite well.However, I agree with R3 that there should be more extensive ablations to understand the effect of each part, and the sensitivity analysis of lambda should be done on more datasets. Additionally, I'd like to see more detailed comparisons to related work like https://arxiv.org/abs/2006.10963 that uses batchnorm for domain adaptation. The results aren't stellar, so without a good conceptual explanation, or empirical investigation, I don't see this as a must accept. ################################################################After author response: The authors have addressed my comment about inconsistent evaluation setups among different papers. However, I sill think novelty of the paper is limited as it is a conditional counterpart of [A]. As mentioned by other reviewers, findings of the paper are quite incremental and are in line with LostGAN-v2 although the authors use a more consistent evaluation setup. Overall, I keep my current rating.  ---------------Update 21.11.2018I think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely:- The technical contributions are minor.- The theoretical discussion (in particular regarding absorbing states) is quite shallow.The merits of the paper are:- Good results due to off-policy learning- Raising awareness and providing a fix for a common pitfall I think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. Some suggestions: Section 3.1"As we discuss in detail in Section 4.2 [...]"I think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states."We will demonstrate empirically in Section 4.1 [...]"The demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)?Section 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics.Section 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how  the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. -------------Update 22.11.2018By highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. I think that the submission can get accepted and I adapted my rating accordingly.Minor:Conclusion should also squeeze in somehow that the reward biases are caused by the implementations.Typo in 4.2: "Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic]were previous hidden, [...]"  ## after feedbackSome of my concerns are addressed the feedback. Considering the interesting technical parts, I raise the score upward, to the positive side.  Update after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns.    The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement.     I just realize that my concern on the practical relevance is largely due to the title "Principled Deep Neural Network Training through Linear Programming". It sounds like it can provide a better "training" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like "look, here is a new method that can change training", but "hey, check some new theoretical progress, it may lead to future progress". I strongly suggest changing the title to something like "Reformulating DNN as a uniform LP" or "polynomial time algorithm in the input dimension", which reflects the theoretical nature.     That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably.  In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. I thank the authors for their detailed responses and revision.- The revision to section 5 explaining the training procedure is helpful.- The revision to section 3 is also helpful. It may have helped to go even further in explaining the objectives of the memory access learning task (as in the response to Rev2) with analogs to BF, and distinguishing them from some aspects that seem like engineering details, as the former is (to me) the conceptually significant portion of the paper.- I still could not find where it is stated that the BF plots are analytic; my apology if I missed it. This is not a major issue, and I understand the choice to use the theoretical bound, but there is some discord in including an analytic curve next to empirical curves on the same plot without clearly marking it as such, as it may give a wrong impression as to what the reader is seeing (not an actual experiment, but an estimate of what an experiment would have yielded based on probabilistic concentration). - I have revised my score to 6. -----------Update:The comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly. UPDATE:The revised version is much better in empirically demonstrating the value of the method; though, there is still some work needed. First, the artificial examples are still rather low-dimensional where independent MH is expected to perform well. Second, ESS does not help to assess the biasedness of the sample; maybe [1] can help with this. Third, NUTS might be a better baseline than standard HMC which is know to be sensitive to the stepsize/number of leapfrog steps. An effective sample size of 1 suggests that the method did not even start to sample - likely because of a bad choice of the stepsize and/or mass matrix. I would suggest using PyMC3's NUTS implementation. Finally, to show the correctness of the method, I would suggest to 1) let alpha converge to zero such that \phi will be fixed at some point, and 2) ensure that the proposal has full support under the target for any value of \phi. In this case, the sample drawn from the target will be unbiased for large enough n (same arguments as for adaptive MCMC should apply).The idea of reusing the samples from previous iterations for approximating the loss is interesting and worth exploring.[1] Jackson Gorham, Lester Mackey. "Measuring Sample Quality with Kernels", <a href="https://arxiv.org/abs/1703.01717" target="_blank" rel="nofollow">https://arxiv.org/abs/1703.01717</a> UPDATE: I've increased my rating based on the authors' thorough responses and the updates they've made to the paper. However, I still have a concern over the static nature of the experimental environments.===================== edit:In light of the revised experiments and inclusion of permutation equivariant deepset layers, I'm inclined to recommend publication. However, if I could nitpick further, I think it would be nice to make some edit (or addition) to Table 1 to include permutation equivariant deepsets. Moreover, it would be nice to have some additional description of permutation equivariant layers in Section 2.1. After reading other reviews and author comments, I have raised my rating to a 6. My main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments). However, I'm not against the paper as an interesting finding in and of itself. It would be great if the authors (or interested members of the research community) may analyze how general-purpose their proposals are (e.g., of Gaussian base distribution) and how extensive the results are on TTS benchmarks.-- Revision: in light of the relevant papers brought up by AnonReviewer3 and AnonReviewer4, that have not been discussed in the paper, I modify my rating to 6. After the rebuttal: I appreciate the authors' effort to revise the paper. The revision made clear that the data produced by the proposed generative model is not linearly separable in general while the theory (Theorem 2) still holds.I am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence. The fact that the classification algorithm motivated by the generative model can do as well as a similar-sized ConvNet does not quite support that the generative model itself is good -- getting a good classifier is still an easier task than getting a good generative model. ===================== EDIT: Updated score after second revisions and author responses UPDATE: Thanks for the clarifications and edits. FWIW I still find the depiction of the architecture in Figure 2 to be incredibly misleading, as well as the decision to omit dependencies from the distributions p and q at the top of page 5, as well as the use in table 3 of "ELBO" to refer to a *negative* log likelihood. -----------------------------------------------------------------------------Post discussion: Following the discussion phase, the significance of these results seems to be a bit unclear to me. For instance, suppose that we are allowed to construct the sigmoidal activation. Theorem 2.1 in  the following paper https://hal.archives-ouvertes.fr/hal-01256489/document  states that for any continuous + univariate function $f$, and $\epsilon > 0$, there exists a sigmoidal activation function in $C^{\infty}$ and an associated neural network with one neuron in the hidden layer (with the aforementioned sigmoidal activation) such that $f$ can be uniformly approximated to accuracy $\epsilon$ by this neural network. Therefore the set of N points in $\mathbb{R}^{d_X}$ can be first mapped to distinct points on a line, and then be  shattered/memorized by the aforementioned one-neuron neural network. Of course, the setup in the present paper considers the sigmoidal activation to be fixed (e.g., ReLU) and this is a non-trivial difference. But still, it is not clear just how big of a difference this is.  =====11/26At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.====12/7The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselineshttps://github.com/google/dopamine/tree/master/baselines) After reading the rebuttal, it is still unclear of how to determine the thresholds for finding incorrect labels. The authors empirically demonstrated a procedure to statistically find a threshold under the assumption that the label noise is uniform. However, theoretical guarantees are lacking. The extension to other types of label noise is also very intuitive. Although the proposed method is simple and effective, the lack of an effective method for choosing the threshold is a major concern for real-world applications. Are there some other ways to determine the threshold? For example, cross-validation method?  ################Revision:I would like to thank the authors for the extensive revision, additional explanations/experiments, and pointing out extensive relevant literature on BLUE scores. The revision and comments are much appreciated. I have increased my score from 4 to 6. ### After reading author feedbackThank you for the feedback. After reading the updated paper I still believe that 6 is the right score for this paper. The method produces better results using ensemble learning. While the results seem impressive, the method to obtain them is not very novel; nonetheless, I would not have a problem with it being accepted, but I don't think it would be a loss if it were not accepted. Revision:After the rebuttal, I increased my rating to a 6. I feel this paper could still be improved by better motivating why multi-modality is important for single tasks (for example, by plotting histograms of activations from the network). I also think that the paper by Kalayeh &amp; Shah should be presented in more details in the related work, and also be compared to in the experimental setup (for example on a small network), especially because the authors say they have experience with GMMs. ### post rebuttal### authors addressed most of my concerns and greatly improved the manuscript and hence I am increasing my score.   ----------------------Update: In light of the authors' rebuttal I have updated my rating from 5 to 6. [POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly. I think the ideas are of sufficient interest to the community to merit acceptance &amp; discussion, but I still miss the high resolution samples we got with the Glow paper. Responses to my concerns somewhat addressed, though simpler alternatives to uniform dequant would be nice.===== Revision: the rebuttal can not address my concerns, especially the image quality assessment and the novelty of the paper parts. I will keep my original score but not make strong recommendation to accept the paper. Thanks for the clarification and fixing the notations in Theorem 1. I think the discussion of unitary RNN models makes the paper more well-rounded. I hope this work will inspire more research in this direction in the future and help us understand the dynamics of recurrent networks. I would like to keep my rating. -----------------------------------post rebuttal: the authors have responded to my main questions, and I would like to increase my score, but I cannot agree with them on possile future extensions of this work, e.g. in learning from pixels. EDIT: Now this is a paper that makes sense! With the terminology cleared up and the algorithm fully unpacked, this approach seems quite interesting. The experimental results could always be stronger, but no longer have any holes in them. Score 3--&gt;6 I have read authors' reply.  In response to authors' comprehensive reply and feedback. I upgrade my score to 6. As authors mentioned, the extension to density estimators is an original novelty of this paper, but I still have some concern that OE loss for classification is basically the same as [2]. I think it is better to clarify this in the draft.  --------UPDATE AFTER RESPONSE PERIOD:My initial read of this paper was incorrect -- the authors do indeed separate the outlier distribution used to train the detector from the outlier distribution used for evaluation. Much of these details are in Appendix A; I suggest that the authors move some of this earlier or more heavily reference Appendix A when describing the methods and introducing the results. I am not well-read in the other work in this area, but this looks like a nice advance.Based on my read of the related work section (again, having not studied the other papers), it looks like this work fills a slightly different niche from some previous work. In particular, OE is unlikely to be adversarially robust. So this might be a poor choice for finding anomalies that represent malicious behavior (e.g., network intrusion detection, adversarial examples, etc.), but good for finding natural examples from a different distribution (e.g., data entry errors).My main remaining reservation is that this work is still at the stage of empirical observation -- I hope that future work (by these authors or others) can investigate the assumptions necessary for this method to work, and even characterize how well we should expect it to work. Without a framework for understanding generalization in this context, we may see a proliferation of heuristics that succeed on benchmarks without developing the underlying principles. After reading author response and the extra experiments, I have changed my rating to 6 (from the original rating of 5). ===========================================================================Update after Authors' response:- I thank authors on their detailed response and clarifications. I increased my score based on their response. ----------------------------------Edited after authors responses: I would like to thank the authors for the detailed response and the changes they have made to the paper. While I still have some concerns and questions about generalization to more complex tasks, models and labeling functions, I think the additional experiments demonstrate the value of the proposed framework and opens the door for future work to explore these questions. ------ Post Rebuttal------Thanks to the authors for the new experiments and feedback!The rebuttal addressed most of my comments and increased my score by one point. ------After author response:The authors presented new experiments on image datasets during the rebuttal, which demonstrate the flexibility of the proposed framework. However, all the experiments are conducted on simple tasks and models. It is still unclear whether and how this method can help more practical problems.Overall, I think this paper presents a nice exploration towards interactive weak supervision. I hope the authors can release their code and experiment details to encourage future work. After rebuttal:I still think this work has a interesting task setup, though it indeed has many faults (after reading the responses and other reviewers):1. It seems that IPM is not really useful in practice.2. It is also not sufficient to large occlusion, and thus there is no explanation for its advatange over `estimating accurate depth`3. Range is short and latency is high4. After reading reviewer1's comments, I think it could use the same experimental setting as the existing methods for a fair comparison. The other methods might be not properly trained with the new setting.5. It is still not clear how to emsemble several models (with different trained weights) in this work.Thus I am changin my rating to 6, and I will not fight for this work. ----------The reviewer appreciates the response in detail. The new figures (Figs 4-7) are helpful for demonstrating the differences between LOF and RM, while also demonstrating that their outputs can be similar or the same in many cases. The reconciliation between optimality and composability is a nice feature. Overall, the reviewer still feels positive on this work.  Updates: Thanks for the author's response. My concerns are mostly addressed. But I still believe explicit geometry modeling should be included for this task. This could be added in future works. Overall, I am positive on the submission and keep my original rating. **After rebuttal & discussion**I still tend to think that the paper's scope can be adjusted relatively easily (it is not too difficult to insert more disclaimers and change the title), and we can force apply the adjustment by conferring a conditional acceptance.But I'm sold on the point that there is a lack of argumentation on whether undisclosing the user-specific embedding will improve the privacy guarantee. I had taken this argument as granted, but this is indeed not so obvious, given that there exist many attacks that are applicable in this kind of scenario, as R4 has argued. It would be great if the authors could quantify the improved privacy guarantee.I'm okay with rejecting the paper then. I still like the paper quite a lot, but rejecting it will also give the authors a good chance to assimilate more points of views in the paper. (Edit: the authors considered it) -- The authors have attempted to address this point, but with limited time were not able to train a network to a high level of performance. -- ## Post RebuttalI thank the authors for their responses. The results with a larger CNN and increased batch size help show the benefit of the method further.  I still believe the presentation of the method would be considerably stronger if results were presented in a setting with larger CNNs and higher resolution. --- Response to author's answers> The target actor points up method can be used to predict multiple agents by running repeatedly, these predictions are each independent, whereas in reality future trajectories for vehicles are mutually dependent. Two cars may have independently likely trajectories, but if they intersect, they are unlikely to occur together.Yes, this is a commonly used argument used by works that predict all actors in the scene simultaneously. However, whether the claimed benefit is achieved by the proposed model is yet to be proved. The fact that the proposed model has higher prediction errors than the state-of-the-art single-actor prediction model makes me doubt whether this is true.Thank you for updating the paper with the inference time numbers. These results are really useful.> For runtimes on the same test machine, ECCO runs 684ms versus 1103ms for VectorNet.1103ms is really high a latency for any real-time robotics system. For completeness, could you also include the number of actors that are predicted by VectorNet in 1103ms? ==========Post rebuttal comments==========The rebuttal has addressed my main concerns. In general, I believe the core idea of modeling substructure interactions in GNNs should be shared in front of more audience. Therefore, I have increased the rating accordingly. ### UpdateThanks to the authors for their response to the reviews, and to the other reviewers for their comments as well.  It seems that we were all confused about many of the same things.The authors have clarified some of the points raised, and I appreciate that.  I also continue to appreciate that the empirical results are promising.  At the same time, I personally remain confused about how to reason in a quantitative way -- suitable for diagnosing problems, determining hyperparameters, etc -- about the resolution tradeoffs that are described.  In other settings involving graph coarsenings, wavelets, graph Fourier transforms, etc, there is usually a more quantitative way of expressing what information is lost and retained by a given type of compression.  The discussion of coherence in the appendices says something about this, but not in a way that I would understand how to operationalize.I also appreciate that the chemistry example in the appendix exists, but I do not understand how to visually interpret the picture.My recommendation remains a weak accept, as I think it is likely worthwhile to put the empirical results out in the world, and the theory may follow.  But I remain wary of my own lack of understanding of the theory in a quantitatively meaningful way, and would also welcome the chance to read a future version that had some of these aspects more clearly worked out and explained. Post-rebuttal:- I thank the authors for improving the presentation of the paper and including additional experiments comparing to latent ODE.  =====Update after rebuttal=====I have read the authors' rebuttal. Most of my concerns are addressed properly, and hence I am willing to increase my score from 4 to 6.  --- Post Rebuttal --- I read the author response and I keep the original rating due to the limited operating range of the proposed method. =====================================================================================================After reading the authors rebuttal, my major concerns are fully addressed and I decide to keep my decision as weak accept ***Post-rebuttal review***I have increasing my recommendation to a 6. This is on account of the improvements to the submission by the authors, a detailed rebuttal, and somewhat to align with the recommendations of the other reviewers. I still do find that the experimental results could be improved somewhat, but as another reviewer pointed out, this is not a huge concern within the majority of the equivariance literature. I thank the authors for an interesting read and thank you for a good rebuttal. UPDATE:I thank the authors for their detailed response and edits. Overall, the authors have addressed my comments, but I have failed to understand some parts. I am still (slightly) positive about this work. >> Please check again for typos. COMMENTS AFTER THE REVISIONIt was amazing to see the authors updated paper with the new experiment on the baselines (Table 2) and machine translation (Section 6). Although the baselines (overlap and synonym) were strong, I can now see that the proposed approach is better than these simple baselines both for machine translation and summarization. Section 6 also demonstrates the usefulness of this work for machine translation trained with self-training. For this reason, I increased my rating.The impact of this paper would be greater if Section 6 could include more results on different MT datasets (e.g., WMT) and/or self-training for summarization. Thank you for the additional experiments. Especially, Figure 7 and 8 look promising.My conclusion from the experiments is that the "contrast" corruption (used for validation) seems to be general enough, in the sense that for many other corruptions, encountered at test time, the performance is good.However, as AnonReviewer1, I am not sure about why the methodology seems to work well for very different types corruptions at test time, and completely OOD data (Figure 7 and 8). More empirical/theoretical analysis would be nice.Increased rating to 6.------ Post Rebuttal:Thanks for the response, and the new experiments. I continue to think that this is a nice simple method that works well enough to be interesting. I retain my initial rating. ## EDIT: UpdateThe paper was improved. In particular I like the added explanation of the batch-norm and some improved explanation and phrasing.Nevertheless, the experimentation remains weak both compared to the state of the art and previously published work (e.g. CIFAR reported in Trabelsi et al., 2017 and Gaudet & Maida, 2017).Therefore, I increase the score by one point, as this work is just very slightly above the acceptance threshold right now.  Update:I read the response of the authors. =======================================================================After reading other reviews and authors' responses, I upgrade my score to 6. Despite its relatively small evaluation data, I think the setup of the task of autoformalization could still contribute to the community and inspire more researchers to make efforts in this direction.  After Response==============================I agree that different DARTS paper usually uses some different settings, the lower PC-DARTS performance in the paper could be reasonable.Search Space design sometimes can greatly impact the result of the final result, but it makes sense to modify the search space for a better result. I am glad that you state the situation of the performance on the original DARTS space.I also read other reviewers' opinions. Based on the author's response and my previous rating, I decided to keep this score. This is an acceptable paper, but still has something to do in the future, like a more comprehensive experiment part. == Post Rebuttal ==I am satisfied with the response "ensemble baseline" and "NA-DARTS-ES". But my concern about "Improvement not significant" is not addressed, which is also mentioned by R1 and R4. I will remain my score as 6. # Rating and comments after the rebuttalI think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. Nevertheless, the paper still packs too much information which makes it difficult to read and appreciate.Regarding novelty, although I agree with other reviews that the core idea is not novel I think that it is important that the paper stresses the applicability and usefulness of considering likelihoods in deep learning models, as it appears to be not fully appreciated currently.Overall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form. Edit:### RebuttalI had read the rebuttal and the other reviews. It seems that there are some clarity issues, which are independent from my knowledge of the area chosen for the experiments. Moreover, outlier detection is not necessarily the only possible application.However, I consider that the Bayesian point of view (which comes from preceding papers) has been well highlighted in the revised version. These points put together, and given that I'm not sure of the significance of the experimental part, I do not change my rating. ----------------Post-rebuttal:After reading the rebuttal I maintain my score. The observations seem promising, the method a bit cumbersome but also interesting, but neither of them are fully fleshed out. The authors should either greatly expand the empirical analysis (in the non-linear setting) of their claims on intra and inter class variability in CL  and/or make the experiments of the DRL method more convincing and varied in scope AFTER READING THE REBUTTAL, I changed my score from 5 to 6. ~~~~~This major concern is relieved after rebuttal.  -------------------------------------------------------I have read the response, and the rating is not changed. Modifications after discussion:Increased score by one as the presentation in the revised version has clearly improved, along the lines requested in the original review. Post Rebuttal The authors have improved the context of their work and clarified their proposed method. While the technical novelty is somewhat limited, the proposed method does well and the benchmark introduced herein should be of interest to the community. As such I have increased my score. Post-Rebuttal Comment=====I thank the authors for their detailed response to my concerns.While my opinion of this work remains largely similar, I raised by score from 5 to 6 for the following reasons:I do buy the argument that the proposed method "allow both developers and researchers to start from a strong method with a low barrier to entry in diverse domains".I do not share the concerns of R2 & R4 regarding the quality of results and fairness of comparison. As my primary concern (amount of technical innovation) is not shared by other reviewers, I am swayed to change my initial "on the borderline" rating to the positive side.I still would recommend adding the following evaluations:more diverse initial state for Figure 10.a more interpretable dataset for Figure 15: I think ShapeNet would demonstrate this point better. Post Rebuttal Comments:I thank the authors for their feedback. I have no modification to make to my original review. ------------------------------------------Score updated after rebuttal, please see comment below Post-rebuttal updates:I thank the Authors for their response. After reading all the reviews and comments I feel that there are aspects of the proposed approach that are not fully understood, despite the improvements. For example, those related to AugMix, and providing fully symmetric comparisons between Cityscapes and GTA5, as several reviewers have pointed out. For these reasons, I have decided to revise my ratings as I also recognize the importance of these observations.  Final comments:I would like to again thank the authors for their time and responses. My concerns regarding novelty remain as described below, but the authors did clarify some aspects. For these reasons, I have raised my score from 5 to 6. If the work is not accepted at this venue, I would like to encourage the authors to continue with their work and submit to a later venue. Post rebuttal evaluationI thank the authors for providing answers to the raised questions and providing further experiments. Regarding Figure 3, I suggest that the authors provide accuracy as a function of wallclock instead of epochs currently reported in the paper. As a result of the authors' responses, I increase my score to 6. ---------------------------------------------------------------------------**Post-rebuttal**See my post-rebuttal response below. I have some remaining questions but the authors addressed some of my concerns. Therefore, I am increasing my score. ----------------------------------------------------------------------------Looks like the post-rebuttal response is not available for the authors to see. I am copy-pasting it here. I thank the authors for their detailed rebuttal. **Deeper layers get more affected by backprop:** I appreciate the authors' response. The paper they referred to does not seem to have conclusive proof of which layers change more during the training. For example, Figure 1 in that paper shows that on FCNs, the latter layers change more. On CNNs, Figures 14, 15, the results are less conclusive. Anyhow, when I wrote the review the setup that I had in mind was a solution of the previous task as an initialization of the next task and not a random initialization and standard training -- a setup that had been the subject of the referred paper. In the former case (continual learning case), the solution of the previous task should inform on the next task and I expect the earlier layer weights to have small changes. However, what the authors of this paper showed in the revised draft (Figure 31) is quite interesting. They showed that, in the L2 sense, the weights of the earlier layers change more than that of the later layers. This shows that despite the large weight changes in the earlier layers their representations don't change much (as measured by the CKA). Am I understanding that correctly? Could I request the authors to verify this observation on ResNets (I believe Figure 31 is for VGG), and perhaps, other datasets as well?**Sec 4.1**: Regarding freezing the earlier layers and not seeing any performance degradation on the subsequent tasks, I asked whether this would also hold for setups where input distribution changes (e.g.) Permuted MNIST. Could the authors please address this and add the experiment to the paper?My other concerns are addressed, therefore, I am increasing my score.  ***********************************After carefully considering the rebuttal from the authors, I think I am more positive about the paper so I raised my score. The rebuttal clarifies most of my confusion about the paper, though more improvement can be done.  -----Post rebuttal comment: I appreciate the authors' responses, especially on highlighting the theoretical challenges. I have raised my score accordingly. #######################Update:The authors have addressed my questions adequately. In particular, my main concerns with the theoretical results and proofs have been fixed. I have updated the score from 3 to 6 for now. ### Post-rebuttal updates- Thanks for the detailed response and additional evaluation.- Q1. Fair point -- it appears that defenses do assume attackers query from a reasonably different distribution. This defense does make this assumption explicit by including it in the training objective ($D_{out}$ in Eq. 3). But then again, it seems typical for OOD-based defenses (e.g., AM).- Q3. This concern also generally connects to Q6. It would be nonetheless interesting to analyze scenarios where the attacker attempts to use some auxiliary knowledge to break the defense. This is also shared by some other reviewers.- Q4. Thanks for presenting the curves. Assuming strictly non-overlapping OOD data does seem like a strong assumption though.- Nitpick editorial comment: Please serif-based fonts for text in equations e.g. $argmax(\cdot) \rightarrow \text{argmax}(\cdot), index \rightarrow \text{index}$- I am slightly increasing my rating. ---------------------------------------------------------------------------------------------Post rebuttal update:After reading the clarifications from the authors, it is now more clear that the dataset is about the learnability of certain hypotheses as opposed to that of test-time verification. I am generally satisfied with the responses given by the authors, and willing to buy the statement that having a per-concept breakdown of learnability can be seen as a feature rather than a weakness pertaining to the use of a "toy" dataset. I think there are some valid insights as provided by this work, though I am more skeptical regarding the superiority of transformers on disentanglement tasks as the improvement gap was relatively small and there are various implementation details in the used transformers and relationship networks that could presumably shrink or even invert that gap. Overall, I think the writing clarity is still a significant concern. Several detailed passes over the paper were necessary just to get the general picture of what was going on. I think the toy example provided in the author's response is certainly helpful, and should improve the paper in this area somewhat. Based on the above, I am upgrading my rating to marginally above threshold. ======= UPDATE after reading author response:  The authors' example answered my question about the nature of the "uncertainty" in their setting. I have increased my confidence score and have retained my marginally-positive evaluation.  ### Updates after the rebuttalI like the paper and found the revised version more transparent. I support the engineering approach of the paper; however, as we all know, these papers often require authors to go to greater lengths to convince. After reading the other discussion and reviews, I think the authors can consider a few additional experiments. I would suggest investing in a more involved toy-experiment to better motivate the engineering solutions. If possible, authors can also consider a more careful ablation study to establish the relevance of each component on this toy-model. Further, the authors offered explanations for the training time aberrations; if possible, authors can consider including the equally-fast-variants in the revision to be more convincing.  ============ after rebuttal =============Thank you for the response and revision, and I am sorry for my late response.  One experiment that I can think of to get around the issue of failure in RL optimization is to consider simple examples where RL algorithms can surely find the global optima.  This is the case for the tabular setting (like your GridWorld). This can be a proof of concept, but I admit that it might also be limited. So I think it is okay to keep your current presentation.  Update after author response I thank the reviewers for their response. I appreciated the more careful discussion of the discrete claims (as other reviewers also noted). I also appreciated the efforts to give more justification for your architecture changes. While the actual experimental results still seem incremental in terms of raw performance, I think the other contributions of the paper are solid and worth having in the literature. Thus, I've updated my score. **Post-discussion update:**I would like to thank the authors for addressing (albeit partially) my comments, as well as the comments from other reviewers. While I understand that some connections can be made between the proposed approach and other approaches or aspects that go beyond local smoothing or oversmoothing, this is somewhat anecdotal in my opinion. More generally, it is still not entirely clear to me how significant are the contributions here. Reading the other reviews, it seems these concerns are also shared by other reviewers, although I still think the analysis here is not without merit and this warrants it at least a borderline score. Further, there are some interesting insights provided here, which place this work slightly over the threshold. Since marginally above the threshold was already the score I gave the manuscript initially, it remains unchanged. =================================================================Post-rebuttal update: Thanks to the authors for their clarifications. I have read author response and understand that the cons I mentioned above might be orthogonal to the focus of this paper. Thus I have revised my rating to 6. Post-author response: I have read the response and am satisfied with the answer. I am leaning more towards accepting this paper. ===========================================================================================================Added after author response:--------------------------------------------------------------------------------------------I have read the author responses and other reviews. I believe authors have a sufficiently addressed my concerns regarding the quality of the paper. I understand the improvement in using momentum in this way and I think it is decent contribution. However, considering this improvement and the concerns raised by other reviewers, I maintain my score. Post-rebuttal update: Thank you for your response.  My concerns are relatively minor and I believe this work is above the acceptance threshold. ### POST-REVISIONThanks for revising your algorithm and clarifying its theoretical properties.I think the changes made to the algorithm do seem substantial: you've changed a complicated combination of multiplicative and best response updates on alpha to a simpler projected gradient update  (I am assuming that the experimental results have also been revised accordingly). The description is now easier to follow and the convergence results now follow directly from Chen et al.I'm raising my score to 6, but still find the novelty in the formulation to be somewhat limited.One pending concern is that the algorithm maintains one parameter \alpha_i per training example, which can be prohibitively expensive for large datasets. Of course, one way to alleviate this difficulty would be to replace \alpha_i with a parameterized function of the features, which however, would make your formulation very similar to Lahoti et al. (2020).Is there a way you could measure the violation in the pareto constraint in your experiments, to showcase how the classifiers learned by your method are different from those learned by the approach of Lahoti et al?****** ## After Rebuttal ##I thank the authors for their clarifications and efforts to improve their work. I still support acceptance. ========Update:Thanks for the author's feedback that clarifies my concerns. Adding the difficulty score into training looks like a more promising future direction.  I have read the response, and my rating is not changed. _**Update after author response**: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score._ Update:I have read the authors' responses and other comments.  I still think that the theoretical results on anti-symmetric functions of this submission are novel, which is, however, not well-delivered. A lot of space is wasted to discuss symmetric functions, for which the contributions of this paper are not clear.  I suggest substantially rewriting this paper by only focusing on the anti-symmetric functions. =============================================================== ########### UPDATE #########I thank the authors for clarifying the method description, providing additional experiments and actively engaging in discussion. Since my main concern regarding the writing part was addressed I change my rating and I agree with the acceptance. ===================================================================================================The response and the updated version clarify and address many of my concerns regarding contributions and empirical conclusions. Overall, I lean towards acceptance. After rebuttal: I am uneasy about the overstated claims made in section 2. That the architectures are small should really be mentioned more prominently. However reviewers #1 and #2 make a good case that what matters are the improvements presented in Section 5. Thus, I reluctantly recommend acceptance. Update: the authors have greatly improved the figures and tables, and expanded the captions, removing my major concern about clarity. I have improved my rating. I not not share the objection of reviewers 5 and 4 about the small size of the CNN, resulting in an inferior baseline in section 3, as the approach is still impractical and mostly a proof of concept that should encourage further research. What matters is that improvements in section 5 are built upon a SOTA baseline, inspired by section 3. ------------------------------------------------Updated after author response------------------------------------------------I have updated my overall rating in view of the author response. --- Post Rebuttal ---I've read the author response and do not intend to increase my score. Thank you for answering my questions. It would be good to clarify in the main paper that conditioning on the trajectory is implemented in the way described in the author response.  Update after rebuttal:I increased the score to 6 and appreciated the revision of the paper. The readability is improved. However, I also have different opinions with the authors in terms of how empirical evaluation of algorithms should be regarded in active learning research. So I would further encourage the authors to apply their method on high-dimensional large scale data, even it may take a lot of computing resources or require actual sample acquisition.I agree that the goal of active learning is to reduce the burden of labeling data. But it does not conflict with the requirement of dealing with high-dimensional (feature space) data. Also, I see a lot of active learning works focusing on theoretical analysis but cannot be easily put into real-world applications, which actually undermines the significance of the theory to some extent. In the real world, a lot of assumptions would be violated. As the authors also mentioned that, it is "expected" that different feature space and data quality affects the performance. Therefore, I think the theory does not spare us from justifying our methods in practice.Last but not least, actual sample acquisition is not unrealistic if given real-world problems. So I encourage the authors to further demonstrate the nice properties of the proposed algorithms in more realistic settings in the future. ---This reviewer wants to thank the authors for their detailed reply and for updating the paper to make it more self-contained. It reads much better now and is much clearer. The review score has consequently been updated from 4 to 6. Stronger adversarial experiments would be encouraged. Could the authors also include a definition of the VAR criterion (even though it is trivial, just to avoid any ambiguities) and maybe include a paragraph for the future applications of this? For this reviewer, it is not entirely clear yet what the value of this finding is: is it going to help with downstream tasks for ICA? Is this another argument in favour of preferring Mutual Information as a metric over variances in general? Update: The authors response has addressed my major concerns. Particularly, the revised version is more clear about how this paper relates to prior work, more comparisons/metrics are added, and issues I had related to clarity were addressed. The revised version has significantly expanded the scope of experiments, which better demonstrate the advantage of hierarchical features for normalizing flows. I have update my rating to marginal accept UPD: I am satisfied with the authors' response, and therefore I increase the score.  Post-rebuttal: I am satisfied with the authors' response and decided to keep my score. ***Post Rebuttal***I appreciate the authors's improved analysis. After reading the new version it is unclear to me whether the results are interesting enough to the ICLR community, and thus I would like to keep my original score.  Edit: after reading the author response, my score remains unchanged.  ### Update after author response ###Thanks again for the clarifications - After reading the author responses, the other reviewers' comments and the new version of the manuscript, I increase my score for the paper, as the authors now better state the relationship to Circuits and GRAB, and provide a significantly improved evaluation. The enhanced experimental section is now adequate for the paper's claims and offers additional insights into the usability of the generated rules.  I would like to thank the authors for the rebuttal. The added experiments have made this work better.While the collection of training data still seems hard (require exponential computation), I like the idea and the new experiments on generalizing from small training size to large ones. I still think the application domain of the proposed work is limited. The application to quantum computing is definitely interesting, but adiabatic scheduling is a more restricted domain and the contribution in this work is not applicable to gate-based quantum computers explored by Google, IBM, Rigetti, etc.Because the revised manuscript is stronger now, I have modified my rating accordingly. Updates:I have read the author's response and the comments of other reviewers. Although as an "approach as the proof of concept", limited experiments on large-scale GAN models are still necessary. Just provide evidence to prove it is feasible. This is a little disappointment for me. For my other questions, I agree with the author's response. Since I have given a positive initial rating (6), I will keep this rating. Thanks to the author's reply and AC's efforts. ### Edit based on the authors' responseI believe the authors have addressed part of the major concerns that I and the other reviewers had. Comparison with FST approach and triplet loss is clarified and supported by the extensive experiments now, however, most important things are in Appendix only. Based on the updated paper's version I change my rating from "5: Marginally below acceptance threshold" to "6: Marginally above acceptance threshold".One of the reviewers mentioned about comparison with language model (LM): here we could use character/phoneme based LM and vocabulary which can help to solve ambiguity too. So this could be considered as a good experiment for future work to show the great potential of ANE if it outperforms LM usage. Still I have several concerns, probably more philosophical from some point of view:- Rely on the force-aligning data for practical applications (in experiments it was ideal segmentation)- Usage of private data in experiments.Authors state "The goal of our paper is to introduce ANE and highlight some interesting things about it.". I don't see any points in the paper and author's comments why this is helpful/applicable/better than some another ideas. Authors mentioned in the comments that they cannot state that ANE is faster than the FST while having the same performance (this could be one good point that we have speed up using small embeddings + simple L2 distance computation as pointed by one the reviewers). About applications for continuous speech authors gave the comment "However, we are not even sure if speech recognition per se is the best application for ANE. We are hoping that the community will find other interesting uses, either with the embeddings themselves or the distances between embeddings.". Thus, I am feeling that the paper is not finished in that respect.  ----------------------Update after author response:Thanks to the authors for addressing some of my concerns by conducting additional experiments that are listed in Appendix A. I have now increased my rating from 5 to 6.  ### Edit based on the authors' responseI believe the author(s) were able to address many of the major concerns that I and the other reviewers had.  One issue is that much of this is placed in an appendix, so it doesn't form a core part of the main thread of the paper;  I also disagree about one small point (see my separate comment to the Part 2 message below), but this is minor.  Based on their more extensive investigation, I am changing my rating from "4: Ok but not good enough - rejection" to "6: Marginally above acceptance threshold". Thank you for the explanation and the update of the writeup.I still find the main message interesting but not critical for any real application. Hence my evaluation remains as weak accept.Thank you.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ### Update: the author's revision have clarified many of the points of confusion above, and have largely addressed my concern re: the value of formal languages in this setting. I continue to be concerned about the scalability of this class of methods, but since it is a problem common to other work in this field, I do not want to hold this too much against the current submission. Given this I have increased my score to a 6. ### update after rebuttal: I think the paper, with the added discussion, improved.  .-----------Thank the authors for lot of these responses. I'm still around neutral for this paper, but I will raise my score to marginally above acceptance I have read the authors' responses to all reviews and ultimately elected to leave my score as it is (weak accept). I think the empirical results are strong, and while I am not as troubled by the motivation and framing of the work as reviewers 3 and 4, I think their more conceptual and methodological critiques have merit, dampening my enthusiasm for the submission. Update:thanks for your reply. i remain not fully convinced of the improvements with the proposed method and i look forward to additional experiments in NLP. i do think this approach is valuable for additional study however and updated my rating to reflect this ### Original Rating**Rating** - 5: Marginally below acceptance threshold**Confidence** - 3: The reviewer is fairly confident that the evaluation is correct### Post-Rebuttal UpdateI applaud the authors for providing detailed responses to my (and other reviewers') questions and for updating the manuscript appropriately. Several follow-up thoughts to my original questions:1. **Binary classification.** I think I understand what you mean now. Basically you are still performing multi-class classification, but then you treat the calibration problem as a binary: is the top-1 model output correct or not? I think this still can be made clearer in the manuscript.2. **Simulation procedure**: Thank you for the clarifications.3. **$\text{ECE}_\text{SWEEP}$ equation**: I am still concerned that the equation given for $\text{ECE}_\text{SWEEP}$ differs from Algorithm 1. (Thanks for moving the algorithm into the main manuscript!) The issue is that I don't think Algorithm 1 is taking the maximum over the quantity in the parentheses of the $\text{ECE}_\text{SWEEP}$ equation. Algorithm 1 is not actually computing $\max_b g(b)$ for some function $g(b)$. Instead, it is first finding the maximum $b$ that satisfies some criterion, then calculating $g(b)$ at that selected $b$.  If the equation and the algorithm are the same, it would imply that increasing the number of bins will increase the estimated calibration error. However, this does not seem to be true. Consider a binary dataset that is perfectly balanced: $\bar{y_1} = \frac{1}{n} \sum_{i=1}^n y_i = 0.5$, and consider a constant model, i.e. $\forall x: f(x) = 0.6$. Then for the case $b=1$, $ECE = 0.1$. But for the case $b=2$, $ECE = 0$ (assuming equal-width binning).4. **Monotonicity constraint**: Thanks for pointing out the trivial constraint satisfaction.5. **"optimal bin count grows with the sample size"**: I understand the empirical and intuitive reasoning for why this should be true. However, I still wish that this notion could be made more formal.6. **Theoretical notions for why/when $\text{ECE}_\text{SWEEP}$ is less biased than other estimators of calibration error**: I am now more convinced that this is difficult to show, and I understand that a lot of the literature is based on empirical evidence. However, given that the results are empirical, I still would like to see experiments are non-image datasets.**Updated Rating** - 6: Marginally above acceptance threshold Thanks for the efforts of the authors. After reading the response and other reviews,  I raise my score from 5 to 6. However, the proposed algorithm lacks analysis. So I cannot improve my score further. ___________________________________________________ ___Thanks for the response. After reading the authors' response and other reviews, I would like to keep my recommendation. ---------------------------After reading the authors feedback and comments from other reviewers, I raised my score from 5 to 6.  I agree the feature averaging is another testing strategy. ***************************************************************************POST REBUTTAL UPDATEThe authors provided a detailed rebuttal which addressed many of my concerns, and clarified some points. I will update my rating from 5 (marginally below acceptance threshold) to 6 (marginally above acceptance threshold). ========After reading authors comments.In principle, I tried to understand the main steps of the proof, they looks OK. Although, I can not verify the details of the proof.I still think that ICLR venue is not OK for such long submissions. In principle, I am OK to increase the grade for one point. **Post Rebuttal**My main concerns for the paper are not withdrawn, yet not shared with the other reviewers. I think that is fine, it shows that perspectives differ, and that it is in part why multiple reviewers should read a paper. I also do see that the paper has been improved based on the provided feedback. Given that the paper does not contain major flaws, I upgrade my vote to 6.  ==================UPDATEThanks to the authors for their detailed replies and paper updates.I am still quite skeptical about the necessity of the use of INT for the 2% improvement of the Metamath GPT experiments. As I mentioned, just using the millions/billions of internal ITP/ATP lemmas has raised the performance much more in previous experiments. And given the "feed our GPT anything mathy" approach, I would expect that training on any sufficiently big ATP corpus might help the GPT.In general, I am also still quite skeptical that generating more and more synthetic corpora without good relation to real-world math (and motivation by it) has much meaning and will lead to much progress in the ML-for-TP area. The fact that some teams have burnt a lot of CPU/GPU power on dubious AI/TP setups does not mean that this is the way how things should be done (and how good AITP research is actually done in many cases). In fact, ATP is quite a good example of a domain where brute force helps only to a very limited extent. Rather than being impressed by the wasted resources, I would advise the authors to focus on resource-controlled setups and competitions such as CASC and CASC LTB.All this said, I do appreciate the relatively large amount of work the authors have invested in this research. So I increase the score and vote for the paper being published mostly for that reason, while encouraging the authors to focus their energy on more realistic setups.     ### Post-rebuttal CommentsI thank the authors for the response and the efforts to update the drafts. I  think this submission made some original contributions.  --------------------Post-rebuttal thoughts:See the comment block below.I would like to thank the authors for the detailed response and patient discussion. After the various discussions with the authors, I found this paper still has certain flaws unresolved (e.g., I still share the same opinion with R4 on that the belief-state-related arguments are somewhat not convincing enough and in shortage of stronger empirical support; and the lack of any large-scale RL tasks, even though the authors say it's a "future work", makes the value of this architecture more incremental in a lot of sense). I also agree with R1 that both the architectural modifications and the empirical results feel a bit incremental. I strongly encourage the authors to apply this on a larger-scale RL application (you don't even need to try something too large, but something of even a reasonable size is lacking now). It would be a much better and more natural choice than the synthetic tasks here to evaluate some of the core issues of the Transformer that the authors identify.However, I do appreciate the authors' rebuttal efforts where some of my questions are answered in a detailed manner. For instance, although the training is still slower, I feel its efficiency in a reasonable range now, and there's generally a slight speedup in the generation scheme because the K and V are shared.I have (cautiously) updated my score to 6, though still noting the various concerns I have above. ### My finial recommendation After the discussion and revision, the authors have presented more convincingly and more clearly the empirical significance and applicability of their method. I highly recommend the authors to highlight the lastest discussion in the final paper, especially the ill-conditioned argument, as it is highly relevant to the practitioners. I think this paper can be interesting for a moderate number of readers, especially the use of the open-sourced Roboschool could also increase its reproduciability. I agree to increas my score to 6.  ---I read the rebuttal and the updated version of the paper. I think the authors did a good job resolving my concerns on novelty. I updated my score. --------- Post rebuttal ---------------I've read the rebuttal and I appreciate the additional efforts by the authors.Specifically, the authors have addressed my concerns comparing LWS (the proposed method) and LARS. Additionally, the authors have addressed my concerns regarding LWS by conducting more experiments. With another detailed read, I figured LWS updates $\alpha$ in a lookahead fashion. Specifically, $\frac{\partial \mathcal{J}}{\partial \mu}$ in Eq. 6 essentially requires one to compute the loss after the gradient update is being made, which gives it the potential to outperform the analytical gradient.Moreover, the authors have run additional experiments to demonstrate the usefulness of GRU, which makes the proposed method more convincing.While the authors argued that it is not fair to compare to AutoSlim, AOWS, and DMCP, I disagree. They are all relevant and strong channel pruning methods and the authors should have cited them and discuss the main differences (can be used to prune a pre-trained model or not) in the related work as opposed to omit them entirely.Overall, I find the paper interesting and it provides several novel aspects: GRU and LWS. Both are empirically verified to be useful in the channel pruning setting. However, the related work section can be further improved. As a result, I raised my score to weak accept. ###################### Post Rebuttal #############################Most of my concerns on the current manuscript are addressed. I tend to increase my overall evaluation score to 6.############################################################# ------------After Rebuttal------------The authors have addressed my main concerns, and I have updated my score to 6. -- POST REBUTTAL --The authors addressed well most of my concerns.  I increase my rating. However, the authors need to address all comments of the reviewers and also discuss all missing related works in the updated version. ######## Post rebuttal ##############Thank you for your response. After going through other reviewers' comments and the authors' responses to those, I am comfortable with my original score.  ----------------------------------**Update after author response**:  Thanks to the authors for answering my questions during the rebuttal paper and for incorporating the feedback into the paper! My original concerns were about clarity, high-dimensional experiments, and visualizations. Since the paper has been revised to include nice visualizations and improve the clarity, I am increasing my score 5 -> 6. I think the experiments on HalfCheetah are a great proof-of-concept of the method! I'd encourage the authors to include some comparisons against baselines for that task. -------## Post-rebuttal updateHaving read through all reviews and the author's response, I am updating my assessment in light of the responses and new experiments. I agree with the authors that the derivation has theoretical value and is not a simple re-derivation of VIC. The new experiments and visualizations have been helpful (I am happy with the author's responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. In light of this, I am increasing my score from 4 -> 6, slightly leaning towards acceptance. EDIT: The qualitative results help illustrate what the variational bias entails in practice, and indeed the worse coverage constitutes a problem worth overcoming. The Ant experiment was a good attempt at showing scalability, but the deterministic version isn't terribly informative since then the correction term does nothing. Could add stochasticity by taking some number of random actions between the states the agent sees. I suspect that as you increase stochasticity in this way the uncorrected method would degenerate. Would be a clear accept if you could show that, but as is the paper's contribution is bordering on acceptance. 5-->6   After reading the response from the authors, I would like to increase my rating to 6: Marginally above acceptance threshold. - The authors have answered all my questions and they revised the manuscript as well, so I will keep my initial rating (weak accept).  **================== UPDATE / EDIT AFTER REBUTTAL PERIOD AND UPDATES TO PAPER ========**Summary of improvements during rebuttal and remaining concerns on the updated manuscript:Main improvements:- Improved the comparison with existing EM datasets by extending descriptions and text in Sec 2.1.- Added an experiment that shows that the same model (Unet) performs significantly better on previous databases than on the proposed (Appendix VIII, IX). This acts as a solid empirical evidence that the current database is indeed more challenging than previous ones, which was previously missing. This supports the value of the database.- The authors also clarified that they will release all 3 sets of annotations, which can serve various types of methodological developments, as databases like this are not common.- Added a short discussion of previous work on metrics for segmentation quality, which previously was entirely missing.- Extended the analysis by incorporating multiple more metrics (~10) that were previously missing, extending significantly over the first version, where only overlap-based metrics were considered (Dice, IoU). The new results support that for the specific problem of cell-segmentaiton, PHD agrees more with human perception than other metrics on this task, including the very related HD and ASSD.- Showed that the Skeletonization process, part of the proposed metric, improves various metrics (among which the very related HD-based metrics), which fulfills the previous gap of empirical evidence to support its incorporation in the proposed metric.- Rephrased most points where the text was ambiguous or incorrect.Summary and Reviewers Score adjustment:Overall, the revision has improved the document significantly. My primary remaining concerns have to do with the actual paper being of interest primarily to the audience interested in the specific task of cell-segmentation, and that the technical value of the PHD metric is relatively limited (as per my initial comments). However, the updated document supports much better the main claims of the paper, and this database could serve as a benchmark for general ML segmentation methods, benefitting the greater ML community. These improvements make me increase my score from a 3 (Clear Reject) to a 6 (Above acceptance threshold).-- new minor problems in the updated version --Some new minor points that I noticed. In case the work is accepted, please try to address them for camera ready:ASSD& which is not widely used in deep learning researches: Not a true statement. HD/ASSD/etc are very popular metrics in segmentation tasks (including with DL) and there have been efforts to even turn them into losses. Please rephrase/remove this statement.the consistency of the F1 score, IoU and Dice with the human choices was calculated (Sec. 3.2): Wasnt this done for the other metrics as well now? Update the text.Sec.3.2, Six popular& segmentation results: Refer readers to appendix for details on training/test config.1?Sec. 4, Then four evaluation&: four is not correct after the updates.Sec. 4, Discussion: it can be seen& methods: This no longer holds after the new metrics. Taking into account all metrics, if we naively count for how many metrics a method ranks 1st (which is what the paper did in the first place), then it seems the best is LinkNet, followed by U-net++, not CASENet (which only ranks 1 for PHD-1 and PHD-3).Please update the argument.Appendix V: texture => text ####Authors have engaged in the discussion, clarified questions about the paper and addressed comments in its newest revision. I have consequently revised my score from 5 to 6.  ------------------------------------------------------------------------------------------------------------Post rebuttal:The authors have devised an adaptive attack to craft the adversarial examples against edge maps and shown that the proposed technique is still remain robust. However, the essence of robustness in this work lies in the BINARIZATION of the input (i.e., binarized edge maps) which is shown in the previous work [1] and need not necessarily attribute to the shape information obtained through edge maps. I recently came across state-of-the-art deep edge detector [2] that produces non-binarized edge maps, which could be interesting for authors to validate their approach using such non-binary inputs. Hence, I maintain my initial rating and marginally accept this paper. [1] ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS, ICLR 2019[2] Richer Convolutional Features for Edge Detection, CVPR 2017 ------------------------------------------------------------------------------------------------------------------------------After rebuttalFrom the revised version of this manuscript, the authors resolve my major concerns such as clarity/reproducibility of the method, differentiation from the previous works including semi-supervised learning, and scalability. So, I've raised my score to 6. Thank you for the contributions! ### Update after author responses: While the author address some of my comments, I would have still liked to see a more detailed discussion of how the algorithm compares in terms of algorithmic scaling, which I think is relevant because it is a fundamental property of the algorithm, even if it is targeted towards understanding biology. So my score remains the same. ### Modifications since discussion * retracted objection about inappropriateness of MNIST example * raised score from 5 to 6 ##### Post-rebuttal I thank the authors for their thorough comments and detailed explanations for each question. I carefully read the whole, and it helps me understand the entire decision and the processes. However, I would like to suggest two things, regardless of its acceptance, but to make their claim more attractive to general readers: (i) re-sort out the indexes (maybe after rebuttal but before submitting your camera-ready version), and (ii) update figures of the sample with a bit more realistic one (with training the model on larger batch size, for instance). I hope this review phase would make your paper more powerful. (disclaimer: I did not check the soundness of the mathematical equations thoroughly but did check all the rest) Post author response stage: Given the response from the authors and the input from other reviewers I increased the score from 4 to 6. #########################UPDATEAuthors have clarified the doubts raised by my questions. I believe that the task proposed in the paper provides new insights on the weaknesses of deep learning models. Therefore, solving the task is important to advance the automation of proof assistants through machine learning. Based on this, I recommend for acceptance. ================UPDATEThanks to the authors for their replies and paper updates. My overall evaluation remains on the slightly positive side: I believe that conjecturing is an important task and Isabelle provides a nice corpus for that. Even if declarative proof corpora based on Mizar have been used for similar ML/TP tasks before and the work is not quite novel.Further notes:- I would recommend exporting the corpus in the TPTP format. This typically makes the ATP evaluation and building of ML/ATP feedback loops (much) more accessible to ATP researchers, allows including the benchmark in the CASC LTB (large-theory batch) competition, etc. This has been done before [6] for the large corpus of declarative Jaskowski-style Mizar proofs that can be easily used in a similar way as the Isabelle data provided here.- I do not quite agree with the response claim that E_conj is synthetic and focused on ranking and classification. It is derived from a real-world (Mizar) problem set and the ATP-synthesized lemmas are equipped with a metric of their real-world usefulness. So the various tasks such as regression, ranking, classification and (indeed) synthesis (there is nothing hard about synthesis in the E_conj scenario) have a direct impact in terms of suitable splitting of the real-world ATP problems and their easier solution. It is the same cut introduction task as the authors consider here, just with much more data derived from ATP runs and their characteristics rather than from human proofs. This kind of ATP-based data augmentation is one of the most useful ones in the ML-for-TP domain - quite often more useful than working with human proofs [7] because the ultimate evaluation scenario typically involves ATPs. So it is certainly not the kind of artificial/synthetic task that has an unclear real-world value.[6] Josef Urban, Geoff Sutcliffe: ATP-based Cross-Verification of Mizar Proofs: Method, Systems, and First Experiments. Math. Comput. Sci. 2(2): 231-251 (2008)[7] Daniel Kühlwein, Josef Urban: Learning from Multiple Proofs: First Experiments. PAAR@IJCAR 2012: 82-94 # Update after rebuttal & discussionsI thank the authors for their thorough rebuttal. While the technical details are acknowledged and addressed for the most part, the experimental setup could still be improved. R1 mentioned that the work by Lacombe et al. might also be applicable as a comparison partner. Investing in a more thorough scenario would strengthen the paper by a lot.# Further update after discussionsThe primary subject of our discussions concerned the experimental setup. While I still see this paper favourably, it would be strengthened by a more in-depth comparison with the work by Lacombe et al. The core of the paper would be more convincing if the utility of the fuzzy clustering could be highlighted better in a set of scenarios that are more comparable with existing TDA literature. Final review:The authors updated the manuscript and removed tuning experiment on ObjectNet. I am still a bit concerned about the definition of "robustness", but the paper overall does look good for ICLR publication.  Post rebuttal: The updates clearly explain the resampling procedure of this paper, and strengthen the theoretical part of this paper. As a result, I'd like to change my rating to 6 and recommend an acceptance.Additional thoughts emerged from the discussion with authors (which are irrelevant to the rating): I agree that experiments in the paper demonstrate the modified SAC algorithm has a significant improvement compared to baseline algorithms. And I believe that the paper could benefit from including some theoretical justifications to the loss function and data collection scheme (though I completely understand the difficulty of theoretically justify deep RL algorithm). For example:- Assuming all the loss functions can be optimized to optimal, will the policy converge to optimal or near-optimal solutions?- Assuming the value net can be optimized to optimal, how the resampling process change the gradient of policy net? In which case would the on-policy sample with truncated trajectory (i.e., the value function computed by Eq. (8) where the length of the trajectory is $n$) out-perform off-policy sample with full trajectory (i.e. SAC without resampling)? If I understand correctly, without resampling the error of the value net suffers from the amplification caused by distribution mismatch (which is potentially exponential?). And with resampling, would the error of value net come from the truncation?Additional minor issues:- Definition 5: There is no base (i.e., $n=0$) in the recursive definition of $\hat{v}_n^{soft}$.--------- -----------------------------Post rebuttal:I thank the authors for addressing my comments and updating the manuscript. I plan to keep my positive score. == Post rebuttal update ==See my response to author's rebuttal below. In light of new experimental results, I feel this now meets the bar of acceptance at ICLR, and hence  updating my score to 6. ### POST-REBUTTAL UPDATEAfter reading the other reviews and the authors' rebuttal, I stand by my rating of 6. ------Post Rebuttal:I thank the authors for their response and additional experiments to show the performance of MOCO trained on ImageNet for the various downstream tasks considered in this paper. It is evident from the results that the authors presented in Table 5 that their best method (using their multimodal data) performs worse than MOCO trained only on ImageNet with InfoNCE. Hence, while this current work has some interesting novel insights of theoretical value, I don't think the complete proposed method of data collection and training is very practical or broadly scalable. It is likely to be of limited practical applicability.  In contrast to the authors' proposed cumbersome method of collecting annotated data using expensive gaze and motion sensors, cell phones and cameras are nowadays ubiquitous and image and video data is routinely uploaded to the internet by users all over the world. Using such abundantly available existing data on the web, which can often times simply be downloaded for free without any annotations, is what I believe is likely to be a much more practical and broadly applicable approach to solving the problem of representation learning via self-supervision. This concern is also shared by Reviewer 3. On weighing the various pros and cons of the proposed approach, I will maintain my previous rating. ===================(Nov 24) The authors addressed my concerns in the reply so I increased my score to 6. .------------------------------------------------------------------------------------------------------------------Post-rebuttal updates:After reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. Nevertheless, I've increased my score to a 6 -------Post Rebuttal---------Thanks for the feedback from the authors. After the rebuttal and discussion period, I believe that the contribution of the paper are mainly empirical. The Theorems (Theorem 1 and 2) are proposed to show the intuition of the bootstrapping and backward-update methods, and to connect the algorithm with recent theoretical results. During the discussion, there are mainly three concerns about the theorems among reviewers. - Firstly, the algorithm uses eps-greedy to help exploration, which is theoretically unclear. - Secondly, the theorems focus on episodic setting, while the algorithm is designed for discounted setting. - Lastly, it is unclear whether the algorithm is a frequentest approach or Bayesian. I believe that the former two concerns can be addressed in the following way, and don't weaken the contribution of the paper.- From the theoretical perspective, the feedback from the authors does tackle this problem. The eps-greedy will only add an epsilon-term to the final regret. I am eager to see the discussion in the final version. From the experimental view, the authors also claim that all the algorithms in the experiments use eps-greedy to help exploration. In that case, the improved performance of their algorithms are mainly due to their bootstrapping and backward-update methods, instead of eps-greedy trick. Besides, I think applying several widely-used tricks (such as eps-greedy) to the algorithm is acceptable, which makes the results comparable to other methods that also use the tricks. - To the best of my knowledge, there is currently no paper studying linear MDP in the discounted setting from the theoretical perspective. I think this is the reason why this paper studies the connection of the theorems in episodic setting. Meanwhile, it is hard to conduct experiments in episodic setting (In that case, the algorithm needs to learn the value function of all the possible horizon). Moreover, we can reduce an MDP with discounted rewards to episodic MDP by setting the effective horizon $H = \frac{1}{1-\gamma}$. This means that the theorem for episodic MDP can hold in the discounted setting with slight modification. Maybe such discussion should be added to the paper.However, for the last concern, I am still puzzled whether the algorithm is a frequentest approach or Bayesian, as there are many unclear statements.Overall, I believe the main contribution of the paper is from the empirical perspective. The theorem is intended to show the intuition of the algorithm and to connect the approach to the recent theoretical results. The experimental results look nice in general. I am a little disappointed that the authors missed the comparison with two related literatures in the initial version. I agree with R5 that these results may be done under a time-crunch. As a result, I change my score to 6, and I hope the above problems can be addressed in the next version. **Post response update:**I would like to thank the authors for their response and revision addressing my concerns, at least to some extent. However, after careful consideration as well as looking at the other reviews (and responses), I am still not convinced the case can be made for increasing the score beyond marginal leaning towards accept, which was the original score I have marked for the paper. The paper does provide some interesting empirical evidence showing that GNNs can be simplified to by sparsifying, or even removing, the learned weights for combining different channels between message passing steps. However, this kind of simplification is not particular to GNNs (indeed, many popular network architectures are overparameterized and can be significantly sparsified), and has little effect on the incorporation of graph structure in the network. The activation-only experiments are perhaps more specific to GNNs, but on the other had, they mostly establish the importance of message passing, which has already been pretty well established in previous work (together with certain well described limitations of current designs). As other reviewers indicated, the insights provided here are intriguing, but it is not clear whether they meet the level of significance and impact expected from an ICLR publication. --- **Update after authors' response**I want to thank the authors for their responses. My responses to the authors' comments are in the respective threads.--- **Update** : Since most of my issues have been addressed, I have changed my rating from 4 to 6 **Update**: Thanks to the authors for addressing my comments. As it was pointed out by the authors, temperature rescaling is mostly applicable to non-linear loss functions. For linear loss functions, temperature scaling only linear rescales the gradients. The difference between the proposed PGD++ attack and PGD with linear DLR loss is small (see the author's response to AR4). The improvements are most significant for FGSM but FGSM is not recommended for the robustness evaluation. Given the limited technical novelty and small improvements for linear loss functions, my score remains unchanged. Update: I'd like to thank the authors for carefully addressing my concerns.. Reviewer 2 claims $P^4$ training is a special case of previous work. It seems the authors agree at least to some extent *"we agree that the P4 update scheme is closely related to dynamic trivializations"*. It is not clear to me how harshly this should be penalized. In some cases, it is interesting to research special cases of known general results. Unfortunately, it is hard for me to judge if this is the case here, as I know of the previous articles reviewer 2 refer to, but I have not studied them carefully. As a result, I am decreasing my confidence from 3 to 1 and my score from 7 to 6. I hope this does not discourage the authors, and wish them good luck with future research.  Update: I increased my score following the clarifications and addition of the BNN experiment during the discussion phase. Update: I've increased my score. ---------------------------------------------------------# Review UpdateBased on the response and update from the authors I will improve my score from '5: Marginally below acceptance threshold' to '6: Marginally above acceptance threshold'. I generally believe that the presented approach merits publication. The methodology is sound and the experiments are convincing. I appreciate the fact that the authors made an effort to improve the writing. UPDATE after author response:Estimating arbitrary quantiles to approximate the predictive distribution of a black-box model is a novel problem to look into. In response, the authors have attempted to address my major concern and they are mostly clear. I would suggest the authors add more discussion on the motivation of the problem settings, e.g., whether it arises from an actual business problem, why satisfying constrained black-box uncertainty problem is a must-have, etc. <After Rebuttal>Thank you for your detailed response.I will keep my positive score because my concerns are resolved partially. After rebuttal: I'm glad they added some experiments and analysis I wanted to see, so I raise the score to 6. As the authors said, the paper is a proof-of-concept of unsupervised hierarchal scene graph learning, and the rebuttal to some degree reassured me. For example, modeling a cube on top of another top as two parts of an object (which was weird to me: why not each cube as an object?) helps edit tasks where the top cube is enlarged but the "on top" relation is maintained. The downstream representation transfer also makes sense. Of course experiments are still toy from computer vision perspective, but I'm now okay with acceptance. **AFTER REBUTTAL**The rebuttal largely addresses my concerns about implementation details.I am pleased to see the additional experimental results provided by the authors; I think that these do improve the paper. I still feel that some well-tuned handcrafted approach could likely perform on-par with the results of the proposed method, but the comparison with [Wei et al] show that achieving such results is at least not trivial, which does help to better ground the complexity of the task. The additional experiments with a three-level hierarchy show a bit more evidence for scalability than provided in the original paper. While these extra experiments do strengthen the paper, I feel that they don't really address the core issue with the paper, which is whether there is any hope for the proposed method to scale to more complex and realistic datasets.Overall I think that this is a reasonable paper and I still lean slightly toward acceptance, so I maintain my original rating of 6. Update ==Thank you for your response and clarifications. I have left my score as is. ===============================================================================After reading the rebuttals and comments from other reviewers, I decided to keep my original score.My biggest concern hasn't been addressed. I understand there is a comparison between different alternatives in appendix A, but only WTA is outperforming RS in the existing work by a large margin. All the other reviewers and I are curious why this is the case.   Performing an A/B testing type of analysis can only tell us WTA is the best in some limited benchmarks. However, what would really make this framework principle and shine are the insights and messages we can learn from your solution. I further thought about the phenomenon and came up with some hypotheses. For example, WTA is an LSH that the collision probability of h(x) and h(y) corresponds to the ranking/ order alignment of $x_0...x_{d-1}$ and $y_0...y_{d-1}$. Because the choice of K is usually 2,4,8,16, it is more like local ranking information within a few random dimensions, while RS in Han et al is comparing the ranking of x and y globally (across d dimensions). Also, if you see your table 7,  when k goes larger, your performance saturates and degrades. If you start by analyzing the fundamental difference between RS and WTA along with your appendix B and C, you can maybe hypothesize that local ranking (among random dimensions) is more effective than global ranking and verify it with further experiments.  E.g. Intuitively with simple calculation, the half of the pair-wise order information among 4 elements (K=4) could be preserved with WTA hash code ( it is the index of the max so if the hash code is 0, it records 3 out of 6 relations: element 0 > elem1, >elem 2, >elem 3).  ##**Post Discussion Comments:** So the author did a **filibuster** and **flooded the discussions** with bloated comments. In this manner it was close to impossible to keep track of anything. **There has to be character limit for responses otherwise this is not feasible**. I looked at the videos and your physics simulators looks **really catchy**. At one point in time, the pole of the cartpole is at 10-11 o'clock and the cart starts moving right (the pole has close to no velocity and hence only a very small angular momentum). In this setting it would be natural that the pole would fall down if this state is maintained for a longer period (which it is in the video). However the pole goes upwards into the balancing position. This is really weird. And don't get me started on the pen-orientation as the pen sometimes floats mid-air. For this setting the gravitational constant really does not seem right. I also wouldn't consider the task solved as this is more an really uncoordinated movements for three specific configurations. For the simulation studies some doubts remain, but the authors improved the paper. Therefore, I am going to increase my score to weak accept. Nevertheless, the experimental evaluation could be improved and the paper would really benefit from real experiments.  Rebuttal update: the authors have gone beyond the normal scope of a rebuttal phase to update their experiments and the motivation of the work, and for that reason I have improved my recommendation to be above the acceptance threshold.----- ### Original Rating and Confidence**Rating** - 4: Ok but not good enough - rejection**Confidence** - 4: The reviewer is confident but not absolutely certain that the evaluation is correct### Updated Review after Author ResponseThank you for the thoughtful responses to my questions, and apologies for the delay in updating my review. In general the responses have addressed my questions, and I have updated my rating accordingly. However, there are still several issues in my mind:1. **Connection to active learning** - The authors correctly point out that active learning does not assume access to labels, whereas the proposed method does. However, I believe the comparison against selection algorithms such as k-center greedy should be included in the paper for completeness sake, and not just left as a comment in this forum.2. **Dataset Distillation: Instance Selection** - Upon re-reading the paper carefully, I found this subsection in Section 3.6 to be confusing. Why are there multiple $D_i$? The section seems to only talk about one $D_i$.3. **Set vs. Tensor**: In the new "representation learning" subsection of Section 2, the paper stresses that the model learns a representation of a set, instead of an individual data point. However, many tasks (e.g., image reconstruction) do impose some sort of ordering. The paper lacks clarity about where permutation-invariance is used vs. not used.4. **Other places that need clarification**:  - Equation (2): presumably you threshold the output of the sigmoid function, right? Because in Algorithm 1, it seems like $z_i$ are supposed to be boolean-valued.  - Appendix: there are several missing closing parentheses **Final recommendation**I support accepting this paper. While I am a bit disappointed the authors did not add in the paper the results they discussed during the rebuttal, I think the paper is interesting and clearer than at submission. ###############################################################################################After the discussion period:I thank the authors for their responses and for updating the paper. The authors have added a deeper analysis on the impact of the dynamic graph, however I still believe that the novelty of the paper is a bit limited. I have slightly increased my score to 6. ############# UPDATE #############I thank the authors for their response and for updating the manuscript according to our questions. I agree with the other reviewers that the novelty of this paper is quite limited. However, the idea of using a dynamic, learned graph from a general large search space of models is interesting, provides good empirical results on both image classification and object detection and the authors provide ablation for the new components that motivate the paper. I maintain my initial score: 6. Edit following authors' response  ==========Thank you for your detailed response and updated version. I think the new revision is significantly improved, mainly in more quantitative analyses and details in several places. I have updated my evaluation accordingly. See a few more points below.1. Thank you for clarifying your definition of concepts. I still think that the word "concept" has a strong semantic connotation, while the linguistic elements your analyses capture may do other things. The results in appendix E do show that some semantic clusters arise. It's especially interesting to see the blocks in some of the heat maps, where similar "concepts" are clustered together (like the sports terms in AG); consider commenting on this. 2. The new quantitative analyses are helpful. One other suggestion that I mentioned before is to connect detected concepts to external resources like WordNet or ConceptNet. That would help show that "concepts" are indeed semantic objects.  3. The motivation for replicating as normalizing for length does make sense, although the input would still be unnatural. The comparison to "one instance" is helpful, but it's interesting that the differences between it and replication in figure 2 are not large. It would be good to show results that substantiate your assumption that without replication there will be a bias towards lengthy concepts. Does "one instance" detect more lengthy concepts than replication? 4. The results on frequency and loss difference in 4.5 are very interesting. There is another angle to consider frequency: words that appear frequently often carry less semantic content (e.g. function words), so one might conjecture that they would require less units. It may be interesting to look at which concepts are detected at each frequency bin.5. Minor points: section 2.2 still mentions "regression" where it should be "classification".  6. A few remaining grammar issues:- "one concept has a less activation value.." - rephrase - end of section 3.3: "this experiments" -> "these experiments"#ERRO!  UPDATE:After reading the rebuttal, I am increasing my score to 6. The authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains. [New comment:] I have read the authors' explanations and clarifications that make me increase my rating. Regarding the technical novelty, I still don't think this paper bears sufficient stuff. If there is extra quota, I would recommend Accept. Edit: I am changing my rating from 5 to 6 based on the authors' response. update: The authors' feedback has addressed some of my concerns. I update my rating to 6.================= Update after the author response: I am changing my rating from 6 to 7. The authors did a good job at clarifying where the gain might be coming from, and even though I maintain that decoupling the two variables is a simple modification, it leads to some valuable insights and good results which would of interest to the larger research community.------- The authors have put in an effort to answer the earlier questions, and acknowledged the aspects which cannot be directly addressed at this point. The draft/appendix has been updated to include new material and technical results. While the work and ideas are still in somewhat early stages, it will be good to see the ideas (of using random learning rates) get discussed more widely -- so I am updating my score to be above the threshold.  EDIT: See bellow, raised score from 4 --&gt; 6.  * EDIT: I have re-evaluated the careful and comprehensive response to my concerns by the authors. I thank them for their effort in this. As many of the concerns were related to communication and have been addressed in the most recent draft, I think it is appropriate to move my review upwards. The revisions make this paper quite different from the original, and I am happy to re-evaluate on that basis--this is a peculiarity of the ICLR open review procedure, but I consider it a strength. I note that "data augmentation" in machine learning appears to have collided with a term in the Bayesian statistics literature, and the authors have provide a number of citations to support this. I strongly recommend "variable augmentation" going forward, as that is an accurate description (you are augmenting a random variable, rather than the input data domain). This appears to be one of the growing pains of the field of ML which has distinct and often orthogonal concerns to classical statistics around density approximation and computational issues.* ============================================After discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating. Final Comments =======I thank the authors for updating the manuscript with clarifications and for clear replies to my concerns. I agree with R2 to some extent that the empirical performance of the method, as well as the formulation,  is interesting. In general, the authors addressed my concerns regarding how optimal transport training model interfaces with MLE training and the choice of using scaled softmax for computing the Wasserstein distances. However, I still find myself agreeing with R3 that the choice of sets to compute Wasserstein distances (as opposed to sequences is somewhat unjustified); and it is not clear how the theory in Sec. 3 justifies using sets instead of words, as the data distribution p_d is over sequences in both the W-2 term as well as the MLE term. This would be good to clarify further, or explain more clearly how Sec. 3 justifies this choice.Also, I missed this in the original review, but the assumption that KL(\mu| p_d) = KL(\p_d| \mu) since \mu is close to p_d for MLE training does not seem like a sensible assumption under model misspecification (as MLE is mode covering). I would suggest the authors discuss this in a revision/ camera-ready version.In light of these considerations, I am updating the rating to 6 (to reflect the points that have been addressed), but I still do not believe that the method is super-well justified, despite an interesting formulation and strong empirical results (which are aspects the paper could still improve upon).  After reading through the authors' comments and rereading parts of the submission, I have become a little more positive about this paper. I am still unsure about the contributions to the ICLR community. The authors merely state "We believe our contributions to ICLR community are clear and valuable" without backing up this claim with an argument. But in the rest of the authors' comment, they make some good points. I think those points should be made more prominently in the paper itself. I would suggest that the authors describe their approach as using different, complementary encoders of the input sentence and consensus maximization. If they wish to describe this as multi-view learning, that's fine, but I think using the term "consensus maximization" (or something more descriptive like that) in prominent places would be helpful. If the approach is applicable beyond sentence embedding learning, then it would behoove the authors to describe the approach in a general way so that readers will see how they can apply it to their own tasks. As currently written, the paper is very much focused on sentence embedding learning, which causes me to think that the paper is more appropriate for an NLP venue. But it is true that ICLR publishes papers that are application-specific, so I can't consider this to be a deal-breaker for the paper.I raised my score to a 6. [Post revision update] The authors' comments addressed my concerns, especially on the experiment side. I changed the score. ===== POST REBUTTAL UPDATE =====I maintain my score of 6.  While the proposed approach is interesting, the experimental section could be strengthen to firmly stake a claim that it's broadly applicable.  (c.f. point 2. in the review) -------------------------------------------------------------------------------------------------------------------------UPDATEI have read the other reviews and the author's response. Thank you for your thorough answer! It's great to see you've taken all feedback into account and updated the paper significantly. After looking through the changes in the paper I'm raising my score from 5 to 6. Some last comments:- Several other reviewers also raised concerns that the "fully" offline setting might be unrealistic. I saw you added a motivation for this in Sec 3, which makes me a little bit more convinced. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks.- I understand how the reward function issues in ML45 could be the cause of the inconsistent results. Maybe ML1 would be a better choice at this point, or indeed waiting until v2 of the benchmark is released.  Update:I thank the authors for their clarifications. I have raised my rating, however I believe the exposition of the paper should be improved and some of their responses should be integrated to the main text. [Second update] I'd like to thank the authors for their detailed response. The authors have made changes that I believe improve the overall quality of the submission. I now lean towards accepting the paper, and have increased my rating from a 5 to a 6.Most notably: (i) they clarified that their secret-detection model was retrained on sanitized data in their experiments, (ii) they added details about their experimental setup and the algorithms used for their experimental evaluation, and (iii) they added experiments to the appendix of the submission that evaluated their framework on synthetic data. I do, however, still have some concerns about how well the privacy guarantees of the proposed algorithm would hold up in practice against a motivated adversary (since formal privacy guarantees appear to be relatively weak right now).As a minor comment, there may be a typo in Equation 20 of Section 7.2: the case (u, s) = (1, 0) is handled twice, whereas the case (u, s) = (0, 0) is never handled at all.[First update] I find the authors' problem statement appealing, but share concerns with Reviewer 1 about the privacy guarantees offered by the proposed method, and with Reviewer 3 about need to clarify the experimental evaluation. No author response was provided; I've left my score for the paper unchanged. (Note: this update was posted a few days before the end of the rebuttal period; the submission was subsequently updated.) --After rebuttal: thanks many points were answered. Thanks for your answer corrections, and additions to the paper. This mainly resolves the issues I noted. As such I see your contribution as an alternative parametrization compared to the FC-VAE which changes the optimization landscape, favoring more sparse latent graphs. This is worth of interest, and I'm raising my rating to reflect that.The comparison of performance however remains fragile in my opinion. Indeed, it is completely relevant to compare models changing only a single thing (here the latent structure and way it is trained). However different factors of change do not interact in ways that are easy to predict, and it is not obvious to me that applying the same change to the original LadderVAE would necessarily similarly improve its performance. EDIT: After the rebuttal, resulting in several changes and additions to the paper, I am changing my rating from 5 -&gt; 6. Thank you for these clarifications. You local stability analysis is interesting but what it says is that EMA does not change anything in term of second order stability (fortunately it is not worse, but it is also not better). Moreover, your simple bilinear model has pure imaginary eigenvalues and thus the eigenvalues of its Jacobian never lie into the unit ball (whatever the step-size).I'm not sure the discrete local stability analysis you provided is a good point in favor of EMA. (Look like it actually shows that your iterates cannot converge to the equilibrium in the bilinear case)However the experimental approach of this paper remains interesting as I developed it in my review. That is why I will not change my grade. =====I appreciate the authors' efforts to improve the paper. However, there is still substantial room for improvement in writing clarity. For example, the authors optimize the reverse KL from typical supervised learning, which makes even the title of the method confusing. The method that was experimentally evaluated can be derived more simply without the two-step procedure by directly taking the gradient and add the heuristically motivated per state indicator. This in itself is interesting and the authors demonstrate that it works well experimentally. I think the paper would be substantially more useful to the community if the authors focused on that contribution alone. As it stands now, I find the paper difficult to read because most of the theoretical results have no bearing on the method. ######### After author response #########I thank the authors for their responses and updates to the manuscript. While I still feel that the connections between optimization instability and the observed phenomena could be developed further, the updates strengthen the paper and the experimental results are extremely interesting. I have increased my score for these reasons. ** after rebuttal: thanks for the efforts in updating the paper. I will stick with my score. ### Update after the rebuttalI think that the ideas in this paper are interesting and can inspire new uses. All of us agree that the problem presented here was important, and there is a lot of work to be done in this domain. However, after reading the discussion with other reviewers and their reviews, I believe the manuscript can benefit from another review round. Specifically, the authors can benefit from a thorough revision of the claims in the paper. Further, I would encourage authors to at least investigate how naive amortization approaches fair (irrespective of the result, authors will develop a stronger case for their line of work.)  ### UpdateThe authors have addressed most of the above criticisms. Even though part of the definite answer about directly extracting the posterior from the MI network is left to future work and no test of their method on a high-d example was included, I believe that the paper is a valuable addition to the literature. I have increased my score accordingly to acknowledge the authors' replies. ################################## Updates: I have changed my score after reading the author's responses. After reading the authors' response, I'm revising my score upwards from 5 to 6. Thanks for updating the paper and addressing my concerns, mainly regarding the novelty. I will change my rating accordingly.  Response to rebuttal:The authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. Therefore the paper is very borderline. However, I would like to bump my rating a bit higher. Update on Nov 30:I updated my score to 6 and my confidence level to 3. AFTER READING REBUTTAL I've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. I strongly agree with authors when they state: "We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings". I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.  == I think the authors' response is reasonable. They have added clarifying material to the paper addressing my concerns. I have raised my rating from a 5 to a 6. ============================I think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6. Update after author response: I am changing my rating from 4 to 6 in light of the clarification and new experiments.------- ------------------------The author response have addressed most of my concerns. Thus I have increased my score. ---After paper revisions: Thank you for the updates. The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community. === post-rebuttal comments ==I maintain my pre-rebuttal rating. Although the authors address my concerns reasonably well in the rebuttal, they are mostly not reflected in the paper revision (particularly on the choice of using SDN, which is quite an important piece of information for readers). There were no attempts to make a comparison with Seesaw loss, evaluating on LVIS, and releasing code anonymously (as other ICLR submissions did). I have read the reviews of others and the author's response. My main impression of the work remains as it was: that it is  nice idea with small but significant empirical success. However, my acquaintance with the previous literature in this subject is partial compared to the acquaintance of other reviewers, so It may well be possible that they are in a better position than me to see the incremental nature of the proposed work. I therefore reduce the rating a bit, to become closer to the consensus. Post rebuttal update ===Thanks to the addition of better baselines, I've increased my score for this paper. While I'm still not super convinced of its potential for application, I find the idea original and worth discussing at the conference.  == I have read the rebuttal. Thanks for the response. # UpdateThe submission has been drastically rewritten (the diff is massive) and I think it is in much better shape, answering some of my concerns around reproducibility and generalization. Furthermore, it reinforces the strengths of the approach (esp. around its flexibility).I am willing to recommend acceptance, but I have some further questions (hence I have only updated my score to a 6 for now). They are mostly related to the comparison with IL (important to validate the claim in the paper that the proposed approach is quantitatively better than both existing IL and RL methods). See discussion below for details. REVISIONBased on the authors' responses, I withdraw points 3-5 from my original review. Thanks to the authors for the additional experiments. On (3), I indeed misunderstood where the moving average was being applied; thanks for the correction. On (4), the additional experiment using the global mean features for real data convinces me that the method does not rely on the stochasticity of the estimates. (Though, given that the global mean works just as well, it seems like it would be more efficient and arguably cleaner to simply have that be the main method. But this isn't a major issue.) On (5), I misread the learning rate specified for "using the autoencoder features" as being the learning rate for autoencoder *pretraining*; thanks for the correction. The added results in Appendix 11 do show that the pretrained decoder on its own does not produce good samples.My biggest remaining concerns are with points (2) and (6) from my original review.On (2), I did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset. That being said, the paper shows that this doesn't tend to happen in practice with existing classifiers, which is an interesting empirical contribution. (It would be nice to also see ablation studies on this point, showing the results of training against features from single layers across the network.)On (6), I'm still unconvinced that making use of ImageNet classifiers isn't providing something like a conditional training signal, and that using such classifiers isn't a bit of an "unfair advantage" vs. other methods when the metrics themselves are based on an ImageNet classifier. I realize that ImageNet and CIFAR have different label sets, but most if not all of the CIFAR classes are nonetheless represented -- in a finer-grained way -- in ImageNet. If ImageNet and CIFAR were really completely unrelated, an ImageNet classifier could not be used as an evaluation metric for CIFAR generators. (And yes, I saw the CelebA results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old DCGAN results, I can't tell.)On the other hand, given that the approach relies on these classifiers, I don't have a good suggestion for how to control for this and make the comparison with prior work completely fair. Still, it would be nice to see acknowledgment and discussion of this caveat in a future revision of the paper.Overall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is well-written and has some interesting results from its relatively simple approach, I've raised my rating to above acceptance threshold. Score updated after reading authors' response. The authors' response clarify the difference between this work and the Natasha paper. My concern is addressed. I am glad to see that the authors have fixed a number of the issues discussed in my review. Per these improvements in the draft, I am increasing my score from 5 to 6.  ----Post-rebuttal review:I appreciate the authors' efforts in including the new experiments in Sections 4.4 and 4.5. In my opinion, these new results and the discussion in Section 5.2 add great values to this work and make the contributions of this paper substantially clear. I've increased my rating to 6. My apologies for the delay. I'd like to acknowledge that I had a few misunderstandings that were clarified by the other reviews, the rebuttal and a few more passes through the latest manuscript -- thank you for that. I have updating my score to reflect this. I agree with R2 that the approach is novel but I am not yet convinced it is a good paper. There are a few axes of variation important to consider for proposed solutions to this problem:(a) Different kinds of missingness:MAR, MNAR, MCAR are three of the common kinds of missingness enountered in data. Currently, the manuscript does not experiment with the latter two kinds of missingness. The paper studies the MCAR case in Table 1. I'm left wondering what the table of results would look like when stratified by different kinds of missingness. Furthermore, for imputation with the goal of downstream classification, not all features of the data are created equal and only a small handful of them may be relevant for the classification task. How well does the proposed method reconstruct the most predictive subset of features (selected via some simple method such as L1 regularized regression)? (b) Multi-modality in the observed features:I think the paper does do a good job at tackling this axis of the problem by capturing multi-modality in the latent variable. This is where I see the utility of the qualitative experiments in image inpainting and the comparison to the Universal Marginalizer.(c) Computational complexity vs accuracy tradeoff:As pointed by external readers, there certainly are other deep generative model based methods for imputation. The authors point out the alternatives are slower *at test time*. This is true but it is also important to note that it is so because this approach amortizes that cost during *training* by attempting to condition on all possible missingness masks. It still remains to be seen whether this approach yields more accurate results than the other slower approaches.For reasons (a) and (c), I think this paper is borderline. ####################  After Rebuttal  #################### I thank the authors for responding to the comments and have read them carefully. The authors have addressed my concerns in the rebuttal. Following the rebuttal:I checked comments of other reviewers and response of authors. Most of my questions were addressed in these responses. Therefore, I improve my rating. -----EDIT:I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft and undertaking the additional experiments. Most of the points I've raised have been addressed. The manuscript has been made more self-contained. Additional material on autoregressive models suitable for channel-wise quantization and the inductive bias of ordered representations is especially useful. As a result, I've increased my score.Unfortunately, I don't feel my concerns re. paper's relationship with Rippel et. al's work have been fully addressed. It is still my understanding that the role of the analysis in 3.1 is to add rigour to the otherwise similar general argument in 2.1 of Rippel et. al. While more rigour is always good, in this case it doesn't lead to new insights, and feels tangential to the rest of the paper. As such 3.1 could be moved to the appendix, which will also help making the page limit.In addition, I am still not fully happy with how Rippel et. al's work is discussed. A reader not fully familiar with the earlier work might get an impression that it's simply a worse version of the proposed method. It's important to make it clear that the considered method extends the work by Rippel et. al's, applying nested dropout to an autoencoder that is discrete and "variational" (arguable), and considering a different application. This would not diminish the importance of this work, but would give credit where it's due.Overall, following author's response I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision. After RebuttalThank you for your detailed response, and I will keep my positive score. .Update: Thanks to the additional experiment and discussions, I have increased my score --Post-rebuttal--I have increased my score in response to the authors' clarifications and additional experiments and updates added to the paper. ---- Post-rebuttal comments----The rebuttal and the paper revision address my concerns. I keep my original rating. ### *Edit after author comments:*I have read the author comments and the latest paper revision. The authors have noticeably improved the clarity of the paper in several places and adjusted their claims about the stability of the algorithm, and the improved ablation studies are appreciated. Unfortunately, after thinking it through very carefully and despite the author comments, I have not been able to understand some aspects of the model, for example why gradients from the tracking loss are backpropagated into the value function that is supposed to track the "true" action-values. Several parts of the architecture seem to have a complicated dual purpose, which makes it difficult to understand what is going on and why the model is performing better. I suspect that other readers might also encounter similar issues, which makes it difficult for me to raise my rating. I've decided to leave the rating at 6 (marginal accept). ----------------------------------------------------------------------**Updates**: After reading the other reviews and the rebuttal, I still maintain my current score. The additional experiments on the converged results are good to me. As I'm not very familiar with the performance in MARL literatures, I have decreased my confidence from 3 to 2 to reflect some of the concerns of other reviewers involving the performance for the baselines. Thanks for your replies!From your reply, I think this paper would be much stronger if you include empirical comparisons to Task2Vec, or improve Task2Vec. I am increasing my rating to borderline accept, but to be honest, the current version would still be a weak paper without such comparisons. ===== POST REBUTTAL UPDATE =====I have updated my score based on the response. I hope the authors can add PWIL+TD3 results (point 1. in review) in the next revision. ------------------------------------------------------------------------------Thank the authors for their good job during rebuttal.Most of my questions have been addressed. However, my main concern is still that the improvement of CO2 on MoCo v2 is incremental. During the rebuttal stage, the authors did not include more empirical evidence on whether CO2 can improve MoCo v2 well, or theoretical analysis on why the improvement is incremental, both of which are accepteable to me. This severely limits the contribution of this paper. In all, I would not change my score. --------------------------------------------------------------------------------------------------------------Update:Updated score from 5 to 6. The author response clarified some key questions and the updated paper incorporated some of the feedback. ** after rebuttal: thanks for addressing the comments. I have revised my score based on the discussion. Updated score from 5 -> 6 after clarifying feedback from the authors.  After Discussion---I increased my review from marginally below to marginally above acceptance threshold. Most of the remarks I had were at least partially addressed. If the paper gets accepted, I'd still recommend looking at some of them again and clarifying more. A simple, explicit remark somewhere around Table 1 explaining that N_p can indeed exceed C due to relevant parts of the search tree being preserved across time steps would help a lot. Some more explicit discussion about why the difference between using a trained value functions vs. heuristics / terminal results matters so much that it makes this substantially different from prior work would also help (I understand that it is because in prior work the only advantage of storing all those extra nodes was really just that it could retain slightly more information from backpropgations in those nodes, whereas in your case it changes which state is the state that gets evaluated by a trained value function, but this should be more explicit in the paper). ---### After rebuttalSee my reply below for my comments after rebuttal. Overall I feel the paper still has room for improvement and there are several open issues, but it has been improved so it is marginally above acceptance threshold now. # Post-Rebuttal CommentsI've maintained my score at 6.During the comment period, the authors made progress in improving the clarity of their presentation. As with reviewer 3, I feel that there is still room for improvement; in particular, moving some experiments in Section 5 to the appendix could make for a more focused paper with a clearer message for the reader. (Unfortunately, we did not have enough time during the comment period to get there). I'd also note that the paper is now at nine pages; this means that I am holding it to a higher bar.Overall, however, I continue to recommend an acceptance as the method to trade off natural accuracy and certified robustness is simple and significantly improves on the state of the art; for me, these strengths outweight the remaining issues.  Post rebuttal====Thanks the authors for clarifying and revising the paper. The updated version does look much clearer to me, so I updated my ratings. I am still wondering the difference of biprop vs. a classical quantization-aware training for ternary networks. I did read the response to R3. From my understanding it seems that:1. biprop doesn't count 0 as a parameter, while TWN does;2. biprop prunes a larger network (WRN50), while TWN trains a network of the original size (ResNet-50);I am not sure if the superiority of biprop comes from these reasons, instead of LTH itself. biprop still looks more like a QAT algorithm than a LT-finding algorithm in the sence that1. it does not train the pruned network after finding the LT as the original LTH paper;2. it directly learns the binary weights.Just out of curiosity, but I think clarifying these concerns would make the paper stronger. Update:I thank the authors for their response. I read the authors responses and the updated paper. As the authors have addressed some of my concerns and questions, I have adjusted my rating. However, I still have my concerns regarding the novelty and methodological contribution, and the classification setup. I also agree with other reviewers that the presentation can be improved. *** Post response comments ***I thank the authors for the clarifications and additional data provided in the revision; most of my concerns have been addressed. I have to say it is a bit worrying that the indistinguishable statistics in the first layer also occurs for the WideResNet models, which needs more explanation and exploration since it is a key motivation of the method. Also, the point raised by R1 on the evaluation is somewhat concerning. As such, I think this paper is borderline even though I am raising my rating to 6. ---------After rebuttalAfter the discussion phase, I agree that with R3 that in cases such as with periodic patterns, auNN would be a better way to incorporate such knowledge. In this case, the paper would be stronger if related experiments on such patterns are included, to highlight what the key difference between auNN and typical BNN.Gap uncertainty is indeed a real problem in BNN, and I am glad that auNN performs well on this, this is also why I would like to see more comparison to other baselines (e.g., NPN) with good performance on gap uncertainty, to better gauge its improvement/capability (e.g., see https://arxiv.org/pdf/1611.00448.pdf Figure 1(right) where there is also a gap in the middle, although not as large as the example from the auNN paper).If this paper is accepted, it would be great if the author could include R3s summary during the discussion into the final version (see below) as well as proper BNN baselines, which I think will be quite helpful in positioning the current work:Typical BNNs use stochastic weights and deterministic activations, while the authors model (auNN) uses deterministic weights and stochastic activations. You are definitely correct in asserting that uncertainty in weights manifests as uncertainty in activation functions. Directly modeling activation function uncertainty however provides distinct advantages, has not been studied carefully before.Experiments mentioned in the first paragraph would also be a good addition to make the papers point. ----------------------------------------------------I raise my score to 6 after the rebuttal given the clarification the author added to answer my confusion. ================================================================================================================The authors has addressed my concerns well in the discussion period, I have increased the score to 6. ------- **Written after rebuttal:**Thank you for the substantial improvements to the paper in terms of clarity (in particular Figure 2 is helpful) and additional experiments/human evaluations. Despite some underlying questions (from me and other reviewers) that remain, I have updated my score and am now leaning towards acceptance. Post discussion: I'll defer to R2 and R4 for judging the theoretical contributions and am convinced that they are not as significant. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response. All my concerns are addressed. I will increase my score to 6. **Additional comments after the discussion**The authors have thoroughly replied to all the comments from the various reviewers.After reading all the discussions (other reviews as well as replies from the authors), it appears to me that the practical relevance of this contribution is not completely clear. The computational cost of each iteration is large. The benchmarks do not show clear improvements in computational. [Update after author's rebuttal]I do not see any reason to modify my rating. I also identified the self-citation , but it did not affect my rating or evaluation of the paper.   (edit: see discussion below, I have adjusted the score to above the acceptance threshold) # Update after the rebuttalThank you for including the comparison to TDMs - the results are now much more convincing.I think this should be one of the main results in the paper (which should be presented in the main text rather than in the appendix).Also, I'd suggest revising the paper to highlight new contributions compared to TDMs in the intro/method section. I increased the score assuming that the authors will reflect my final suggestions.Regarding distributional RL and cross-entropy loss, distributional RL does allow a probability distribution as target (not just either 0 or 1 and just like yours). I'd suggest the authors to take a closer look at the paper and clarify differences (I still think the cross entropy loss is a special case of distributional RL with discount factor of 1). --------update after reading the response-----------Thanks for the authors' response. Mainly empirical and limited in methodology novelty. So I tend to keep the score. UPDATE: I would reduce my score to a 6 based on the opinions of my fellow reviewers. It appears that the restricted scope and lack of experimental results is quite a problem within this community and venue. ~~~~~Based on the discussion and the updated manuscript, I am happy to say that most my concerns were addressed and at least partially resolved.  I have updated my score accordingly. After reading the response, I still think that the work is promising and would like to keep my recommendation.  ----------------------The rebuttal did not change neither my confidence, nor my impression about the paper. So I did not change my rating, however I acknowledged that other reviews may be more proficient to judge this paper properly.  ------------------------------------ Post-rebuttal comments ------------------------------------With the authors' answers, the inclusion of miniImageNet results in the main text, and revisions to the writing, my concerns have largely been addressed and I have increased my rating from a 4 to a 6.I have a few further suggestions to make: first, make it clear in Table 4 via captioning and/or markings that UMTRA and LASIUM results for miniImageNet depend on the use of full unlabeled ImageNet data to train the AutoAugment and BigBiGAN models, respectively, while the CACTUs results do not. This results in an unfair comparison, and naturally raises the question of how LASIUM would do if we used a generative model trained only on the meta-training split of miniImageNet, which would be more in line with the protocol used for Omniglot and CelebA. It seems fair to assume that it would do worse than the current LASIUM results in Table 4, and probably the CACTUs results as well. This relates back to my point in the original review about sample fidelity: miniImageNet has the most diversity among the datasets considered, and it is hard for generative models to capture this diversity given a relatively small dataset. It would be good for this to be conveyed to the reader.The usage of the extraneous ImageNet data for UMTRA and LASIUM does not conform to the definition of unsupervised meta-learning proposed in Hsu et al. (2019) and adopted in this work. Some discussion of the problem assumption may be warranted: in what practical circumstances would extraneous, relevant, unlabeled data be available? And when it is, why would one not use all of the data (e.g. the entirety of unlabeled ImageNet) to do unsupervised meta-learning and/or unsupervised representation learning, like in Table 12 of Hsu et al. (2019)?Overall, despite the unfair miniImageNet protocol, I still advocate for weak acceptance as the method does show competitive results for domains that are more suited to generative modeling. For the sake of clarity for readers, though, I strongly encourage the authors to implement my additional recommendations. Update: After reading the other reviews and responses, and in light of the authors' updates to the paper, I have increased my score to a 6. ---- update ----In response to the author's comments and extensions, I've raised my score. ################### After the rebuttal ################I thank the authors for addressing the issues that were raised. The paper is indeed addressing a very practical issue in the ML community.After reading other reviewers' comments and concerns I decreased my score.  ##############  Update after rebuttal ############## I would like to update the score to 6 based on the author's response. I have not increased further due to the limited novelty of the paper. However, the observations in the paper certainly add value to the research community.I request the authors to consider reporting performance of other defenses in Table-16 using the recommended settings in their final version.  Thanks for your response and the updates on the paper. This addresses some of my concerns and clarifies my questions. I have updated the review accordingly.I think that it's up to the AC to decide if the two pending concerns are important or not. Specifically,If the evaluation on a single task is sufficient for evaluating the representation contribution of this paper.If the delta of the contributions of this work to those of Dai et al. (2019) and Yang et al. (2019), are sufficient for this paper to get it accepted. I have read the author response and have decided to keep my initial positive rating of 6. Update after rebuttal: I appreciate the authors' response and the new ablation study. I maintain my original score (marginally above acceptance threshold). I continue to think it's a strength that the method works in many domains, but this strength is slightly diminished due to the variability of domain performance (e.g.: image domain). __________________________________________________________Post Rebuttal:Thank you for replying to all of my questions.. Plugging in your new metric to DARTS seems to be promising, especially if it alleviates the DARTS collapse problem. Given that the community is more interested in one-shot NAS algorithms, this might be worthwhile pursuingFrom the new plot in Figure 4 and NAS experiment in Figure 5, it is evident that the sum of training loss is able to rank the networks more effectively in the first 50 epochs. So one could use SOTL-E for early stopping rather than validation accuracy. This would also be effective in hyperband where the architectures are discarded after training them for very few epochs. --------------after the rebuttal---------------I appreciated the authors' effort in addressing my comments and questions. I maintain my score of weak acceptance for this paper.  **After rebuttal responses**:I have read the authors response to my concerns, as well as the other reviews. I maintain my current evaluation with a weak acceptance of the paper.  ---## Post-revision updateI cannot seem to add a new comment at this time, so I am editing this review instead. The updated paper seems to be an improvement, although some of my original concerns remain.Improvements:- Thank you for adding the additional baselines, which do give more context as to how this approach relates to prior work.- The section describing $c$, $f$, and $\Phi$ is clearer in the revision, and is much easier to follow.- I also appreciate the clarification about nondeterministic choices in Z3.- The overly-strong claims about outperforming state-of-the-art approaches have been qualified appropriately.- Some of the confusing details regarding the evaluation method have been moved from the appendix to the main text, which makes them much easier to find.Remaining high-level concern:- I'm still not convinced that the results of this evaluation method are that meaningful.  + NLL under a pretrained model doesn't necessarily imply better low-level structure, for reasons stated in my initial review.  + Ease-of-discrimination of your extracted constraints doesn't necessarily imply better high-level structure, especially since the proposed model is trained on those constraints directly.Other remaining issues in the revision:- A few comments from my initial review have not been addressed:  + "To train model (i), ... resulting relational constraints" remains unclear  + In section 4.1, $p_\phi(c|z)$ is referred to as "a VAE" but is just a small part of a VAE  + In section 5, the description of how A3 is applied to the music domain is missing  + Table 1 is still difficult to interpret, in particular regarding the higher-is-better vs lower-is-better columns, and the (new in this revision) bolding of the human data, which I don't quite follow.- Figure 1's caption now states that rhyme and meter constraints are the reason for the green and purple edges, but this is confusing because there are purple edges between poem lines that don't have meter constraints.- Regarding semantic content, the authors state in their response that "The semantic content of the poems (or lack thereof) reflects defects in the underlying deep generative models". But Section 4.2 approach 1 seems to imply that each of the lines are sampled independently of one another, which seems like a strong limitation that the underlying models do not have. Perhaps the notation is unclear, and the model does get the full input context; if this is the case, I would suggest revising the notation.I have raised my score from 5 to 6 in light of the improvements, and with the understanding that my concerns in "Other remaining issues in the revision" could be fixed in a final version of the paper. I'm still borderline on accepting the paper, however, due to my concern (shared with Reviewer 3) about how meaningful the evaluation results are and whether they match what humans mean by high-level and low-level structure. UPDATE: Based on the Author's response, I have updated my score. After rebuttal, I raised my score from 4 to 6. Thanks for the clarification, and for updating the main paper to address these concerns. I'm going to keep my rating for now (leaning towards acceptance), and will discuss with other reviewers later. ### **Post Discussion Comments:** The authors provided videos of the task. While I still find that this paper misses clear benefits/use-case for the machine-learning community. I still don't clearly understand why one would need 54,000 parameters for a  3 parameter system. One could store a really big table of this one 1d system for this amount of parameters. However, the paper is well executed and clearly written in regards of technical aspects. The motivation remains doubtful for me. Due to the execution and clear writing, i increased my score to weak accept.   ---------------------------------Post-Rebuttal evaluation.I would like to thank the authors for their detailed answers, especially regarding the interpretation of the gap function.Based on their answers, I decided to increase my score to a 6. ===== Update =====I have read the authors response, the updated paper, and the other reviews. I believe that the changes made by the authors address my concerns about the motivation and the lack of clarity; section 2 reads much better now. It seems like the main complaints of the other reviewers are in the lack of more difficult workloads, and the lack of theory. I personally dont find the lack of theory very important. I think the novelty comes from the simple observation, which no one to my knowledge has come to before, and the experiments support the idea empirically (which I think is what actually matters). I also find it a bit uncomfortable penalizing the authors for not running experiments on ImageNet, and I think the variety in architectures that the authors tried, compensates for this. I do agree that a more modern set of workloads (transformers, or even the same setup as AdamW) would have made the paper much stronger. I increased my score to a 6 because I think the paper in its current form is enough to get accepted, but there are still improvements that could be done to make it much stronger. ------ Post-rebuttal update ------The reviewers have addressed my concerns for the most part and I am happy to update my score to recommend acceptance. I also hope to see the A3C experiments in the final draft of the paper. ################################################Update after reading other reviews and author responses:I am happy to keep my score and remain positive about this paper; the authors have answered my questions and partially addressed my main concerns in the revised paper. Like Reviewer 3, I would hope to see complete results for A3C in the final paper. POST-DISCUSSION: The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality (as opposed to theoretical rigour). I will thus raise my score to a weak accept. ----### EDIT: reply to authors' responseI thank the authors for their in-depth response and revision. The additional results comparing to pre-trained features definitely adds perspective, and it shows OTKE giving a very modest boost indeed, mostly over the weaker input features. The revision of the paper also improves the clarity, but my comment around Sec4 remains.I stand by my original score for the 3 reasons I had originally listed. Thank you for the detailed authors' response, and for meticulously taking the feedback into account. The response has addressed most of my concerns.Some further comments:1. In Eq. 6, I think "up to $i \leq I/2$" should be replaced with "up to $i \leq (I/2 - 1)$", since $\lambda$ would be negative with when $i=I/2$. Other than this, the equation looks good to me.2. I look forward to the addition of the results with the "decay curriculum" into the main paper. It is encouraging that the proposed approach outperforms this simpler "decay curriculum" baseline.Since the authors have addressed most of my concerns, I am therefore raising my recommendation to a "6". Modified score: thank you authors for your thorough response. Given the new information and baselines, I think this is a promising paper that passes the acceptance threshold. Post-rebuttal:I concur with R1 in that the results look poor when taking into the account how close the proposed method is compared to CenterNet and that it was finetuned from a CenterNet. Therefore, I'll lower my rating to 6. Post-rebuttal updatesThank the authors for the efforts in answering the questions. The responses have addressed my concerns about the robust training and using training loss to measure diversity. Exploring the data augmentation for robust training will be interesting. Besides,  training loss is usually sensitive to some other hyper-parameters such as optimizer and learning rate. So, investigating the robustness of this metric is also meaningful.Moreover, there is still a gap from applying the proposed metrics to training guidance. The current research mainly shows there are relations between the two metrics and data augmentation effectiveness.  How to merge them into one easily observable is still missing. Therefore, I keep my original score. .UPDATE: I am downgrading my score to a 6. Based on the opinions of my fellow reviewers, it seems that perhaps the theoretical results are based on scenarios that are too simplistic for the community at hand. Moreover, there are clearly some readability issues, based upon the reactions of the other reviewers --------- Update after author response ---------I thank the authors for their response. I have updated my score. I am slightly leaning toward acceptance though I think the paper might benefit from a revision based on some of the points raised by other reviewers, including comments about motivation for correcting the bias as well as the scalability (the proposed method requires storing n^2 additional values which is expensive for large networks).  ---Update: I have the other reviews and rebuttal. I am still leaning towards acceptance, although I do agree with the other reviews that there is little motivation for the idea. While the experiments show improvement, a discussion on convergence rates or variance could show why one should try this idea in the first place, and if \mu_* (rather than regular momentum) is indeed an ideal that the algorithm should approximate. #################################################################################### Updated:The authors' rebuttal addressed my concerns and I lean toward acceptance. **Post-comments to the author's response**- Replies for Q3 and Q4 are good.- However, the responses for Q1 and Q2 did not address my concern well. - For Q1, the authors claim that the representation capacity is in the CNN features not in the fully-connected layer. Then what is the meaning of utilizing the teacher's projection matrix for better representation learning? - Overall, I still this paper has more strengths than weaknesses, so I will keep my rating (6) **Note**: I updated my review score after the original review was written. See comments below for details. POST-REBUTTAL:I thank the authors for their response.At the outset, I am satisfied with the authors' responses to my questions - all the questions were answered. I do agree with other reviewers that the idea is incremental. Learning the prior across tasks is not very novel, as pointed out in references cited by other reviewers. However, on the bright side, the authors have done a good job in answering the questions, and the comprehensive experimental results are promising. The overall idea looks a bit incremental from the GAN literature side but maybe a good lead for meta and incremental learning literature.I change my decision to "Weak Accept". ######################### post-rebuttalI appreciate the additional explanations and experiments by the authors. I also read the public discussion threads. I raise my score to 6. Two things for future:- Make it work on bigger and more realistic images, imagenet, pascal, coco, etc. Now the adversarial community and deep learning community in general, highly relies on experiments, because theoretical guarantee is still mysterious. So we should push the field forward, by proving ideas on harder datasets. - Explore stronger attacks, particularly gradient-free attack to avoid the obfuscated gradients. Given the rebuttal, I am willing to raise my score to a 6 due to the added StyleGAN, PGAN, and other experiments and improved paper layout / clarity. The added experiments are welcome addition to the paper and demonstrate this technique. The lip and eyestaining are interesting results and I do hope this direction gets explored in the future. =====POST-REBUTTAL COMMENTS======== I applaud the authors for the very detailed response and the efforts in the updated draft. Many of my questions were clarified.  However, there are still important questions remaining.1. In addressing my Q4, "We do believe that a separate discussion is needed on whether the registry or the user-end devices should retrain the models, given that users can potentially be the attackers in the malicious personation case. This discussion will entail an exploration of the tradeoff between training efficiency and training security of generative models, which is beyond the scope of this paper." This raises the feasibility question of decentralized approach proposed in this paper.2. In addressing my Q6, "We note that just like problem settings for adversarial attacks, there is a potential mismatch between our expectation of the attackers' capability and their actual capability. Therefore, we cannot assess whether these constraints are necessary, but we acknowledge that they are not sufficient." This needs more investigation.3. Additionally, in addressing R3, "Lastly, we agree that it is definitely interesting to understand which latent space the keys should be embedded. So far on FFHQ, attributability is achieved for 20 keys derived through the proposed method. More keys on the way (see wall-clock costs for computing keys and models in the supplementary). All of the keys share the same structure (eye shadows and lip stains), which may suggest a limit space for attributable keys. We polished the discussion on approximating the capacity of keys through an optimal packing problem, which is an open challenge." If the number of keys are limited, this seriously impacts the contribution of this paper.Overall, I am more positive. I am willing to raise my score to 6. However, the paper is still somewhat borderline. [EDIT AFTER DICUSSIONS] I thank the authors for their answer to my comments. I agree with the summary of the Area Chair and do not wish to modify my score.[/EDIT] ## After RebuttalThe authors' response clarified some of the issues and partially addressed some of my concerns. Based on this and the remaining reviews I have decided to keep the score unchanged.One additional comment regarding the evaluation: In the authors' response they state "Finally, we would like to point out that it is common practice to use GCN as a subroutine for Meta-attack against different defense methods. This was shown in the original Meta-Attack paper, as well as multiple follow-up defense papers." I would like to again point out that the fact that this is a common practice is not ideal, even though multiple follow-up defense papers use the same strategy. We have already learned the lesson in the computer vision literature that adaptive attacks are the least we can do to evaluate heuristic defenses (see [1]) and even that might not provide strong evidence. After rebuttal: First of all, I would like to thank the authors for all their effort on the rebuttal and the revised paper and I really appreciate that. After carefull discussion with AC and other reviewers, I would, however, have to decrease my score to 6 due the lack of significant technical novelty. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Post Discussion--------------After discussion, i have raised my score from 5 to 6. ##########################################################################I thank the authors for their responses. I carefully read them (revised manuscript) and also read other reviewers' feedback. The reviewers clarified some of my misunderstandings. However, I would like to remain at the current rating because my original concern about the novelty remains the same.  [Update after reading authors' comments]The critical issue is resolved: it seems their method is indeed better than plain BO, which in turn is better than random parameters. Though I still have a little doubt about how practical it can be in real scenarios, I increase the score to marginally accept. -------------Post-rebuttalReporting PyTorch-computed FIDs risk the fairness when comparing against previous work. The repo quoted by the authors had a well-known issue, leading to much lower numbers compared to those computed by the original TensorFlow repo. Although this issue has been alleviated following some code update this year, the numbers still won't exactly match those of TensorFlow. Therefore, from a scientific perspective, the FID numbers have to be recomputed with the original TensorFlow code for a rigorous publication.In addition, the FID number reported by the authors for their model is computed between 10k samples and the test dataset, while most other FID numbers in Table 1 are computed between 50k samples and the training set (following the original settings of [1]). This also makes the comparison not fair.I completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality. However, since the authors choose to report FIDs and compare against those directly ported from previous work, they have to follow the convention and ensure a fair comparison. Since I didn't get a satisfying response from the authors (authors claimed to "have corrected it", they just changed phrasing of their response but didn't re-compute scores), I decide to lower my scores from 7 to 6. I am still marginally inclined to acceptance, but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation.[1] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. and Hochreiter, S., 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems (pp. 6626-6637). ####EDIT####I am satisfied with the reviewers response and will raise my score to a 6.I think the authors should take care though in speaking practically about their work. The authors highlight the difference of 'excitation-inhibition mixture" in their response, which is indeed a mathematical difference to the existing work but, at least in the context of neuroscience, I am not sure if this serves any scientific purpose. Do the authors believe this has biophysical meaning or bears a relationship to neural coding? I do still think some mention of GLMs will be useful as context for models that also infer neuron-neuron E and I interactions.I appreciate the authors efforts to explicitly highlight the advantages of this work as compared to Apostolopoulou 2019. Rate updated 30 Nov.Rate updated 30 Nov. final recommendation:after looking at the other reviewer's scores and exchanging some more information with the authors (thanks for the great exchange btw), I would maintain my score of 6. I feel this line of work is definitely useful for program synthesis, especially in the analytical aspects (using adversarially generated examples as benchmarking tools and as representative explanations). the comparisons against other genetic programming approaches, I feel, is not too necessary once the paper is framed as "using adversarial examples to instrument / analyze synthesizer behaviours".  Thank you for your through response.The answers 2 to 5 have clarified all of the respective questions I had made.For question 1, though your answer was clear, I still have slight reservations with respect to the novelty of the paper with respect to GNODE, even after considering your response here, since the contributions described in the answer amount to a simple (though effective) modification to the previous method and a new motivation. Nevertheless, despite the possibly incremental updates with respect to previous work, your results are still stronger than the ones seen in GNODE. Moreover, the other reviewers seem to be content with the novelty of the paper as this was not an issue brought to attention by the others. Therefore, taking all of these factors into account, I will update my score to a "marginally above acceptance threshold". ---Post-rebuttal reviewIn my initial review, my main concern was the motivation for the patch-based classification is unclear, and I asked some questions related to the further potential usage of the proposed method, particularly focusing on robustness.During the rebuttal process, the authors address most of my concerns well in their responses.- Main motivation: this work more focuses on the mathematical analysis of the image classification tasks, rather than performances (accuracy, runtime, ...). It makes sense to me, and I think this motivation needs to be encouraged to explore by many researchers.- Runtime or learnable parameters: tracking the runtime comparing to deep methods is non-trivial as the author clarified. However, the authors showed that the number of learnable parameters is much less than deep models (580k for the proposed method, a few M for AlexNet). I think this comparison fairly shows that the proposed method is efficient than deep models in terms of the number of parameters.- Other analysis that can support the motivation of this paper: I'd expect to see more robustness analyses such as the black box adversarial attack results, but I agree that this is out-of-scope of this work. To me, the responses to the additional comments are not helpful, but I understand that my questions can be out-of-scope of this work.Hence, I'd like to change my score from 5 to 6. ### **Rebuttal Update**I thank the authors for updating their draft. I think this paper is worth accepting due to its ease of use and many different features, but I will keep my current score at a 6.**Here's why:**My experience in these types of benchmarks have usually shown that **completely static** backgrounds (when cleanly used - there can be very subtle things that can still correlate with progress) do not actually affect generalization, but rather, anything correlated with progress will cause overfitting. The difference between this benchmark's background and e.g. a ProcGen/CoinRun's background is that CoinRun's background still moves when the character moves (which implies a slight correlation with forward progress).From the authors' responses to Q 3.2 and Q 3.3, I believe that the main visual overfitting is occurring with the floor tile, as it is the only spurious object that is correlated with progress. This is because, as the authors have stated, the actual background image does not actually change when the agent is moving as the background is too far away. The authors also reinforce this aspect when they added the saliency/spatial attention maps in Appendix A.1, which shows that most of the attention is focused on the floor tiles, especially on test environments.This means that I suspect that the randomized static background portion of the benchmark does not affect generalization, and I urge the authors to rethink this portion of the benchmark.  *Post Rebuttal*The authors have mostly addressed my concerns - I still think SAC should not only be the only RL method in the paper, and though I'm still not convinced more drastic camera variations wouldn't make this more interesting I think all in all this is a decent contribution and probably should be accepted. -------------------------------------------------------------------------------------------------------------------------------------I appreciate that this paper has some merits. But I lower my rating because of the limited usage of the proposed method.It seems it cannot handle high-resolution nature images. Most of the experiments use constrained images e.g. biomedical images or images with small resolution. *****Post Rebuttal*****I would like to thank the authors for the rebuttal. While I think more discussion / comparison to approaches without an explicit independence prior would further improve the paper, the authors have clarified many of my doubts. I have therefore decided to raise my final score. POST-REBUTTAL COMMENTS:=========================Several of the original concerns, mainly about the scope of the contribution, have been addressed by the rebuttal, so I've increased the overall score. See my responses to the rebuttal for the concerns that remain outstanding.========================= ################################################Post-Rebuttal:Thanks the authors for their detailed responses! The authors responses addressed most of my concerns. In particular,1. the authors showed that their results generalize to Tiny-ImageNet and ResNet50 with additional experiments. 2. the authors added some discussions on the theoretical side. Besides, I also appreciate the addition of experiments on AugMix and PGD which imply the bidirectional causality of  class selectivity and perturbation robustness. Overall, I think this work will potentially be a good addition to the existing understanding of trade-off between worst-case robustness and average-case robustness for the community. Therefore, I decide to increase my score to 6. score: 5 -> 6 **After rebuttal**I'd like to thank authors for their efforts to address my concerns. They have addressed most of them, so I increased my score from 5 to 6.However, there are two concerns that couldn't be resolved during the rebuttal period:(1) I am still not sure if the proposed task is practical. At glance it looks realistic, but I couldn't find a detailed scenario that can only be solved by the proposed task. Any real world scenario I can think of is closer to [Lee et al.], which is a prior work of this paper. Authors provided an exploring robot example in the thread of responses, but I think [Lee et al.] fits better for the provided one. I recommend authors to find a concrete use-case in real-world applications, which can only be solved by the proposed setting (or at least [Lee et al.] is not applicable; in the revised intro, you may emphasize that there are some real-world problems that [Lee et al.] is not applicable but yours is). R1 and R4 seem to have a similar concern.(2) the scale of experiment is too small. As CIFAR-10/100 have a limited number of data for your purpose,  you can borrow some data from tinyimages (FYI, CIFAR-10/100 are a subset of 80M tinyimages) or focus on ImageNet.I am okay with the lack of novelty on the proposed method. For a newly proposed task, I think proposing a simple and effective baseline is good enough. However, because of the two concerns above, I cannot strongly agree with its acceptance. Post rebuttal comment: Having read all the reviews and in light of the additional experiments, I am slightly raising my rating. The reason I am still not fully convinced is no experiments indicating the quality of descriptors learned by the method.  Considering the modifications made on the paper, I increase my score --- after rebuttal ---Thank the authors for their response which addressed my concerns. Based on the response, the revision, and other reviews, I would like to keep my score unchanged at 6. -------------------------------------After rebuttal:Thank you for the dedicated consideration of my comments, but there are a few remaining concerns that are not clear.1. The editing effects of edge maps are not distinct from those of segmentation maps. More specifically, except for the background in Fig.3 and the second face in App.D, most of the examples demonstrate the same kinds of manipulation as shown in the samples of the segmentation map manipulations. This includes moving, stretching, and erasing the objects. I think that the qualitative results of edge modification are not sufficient to prove its effectiveness, compared to those of segmentation maps.2. As shown in the image segmentation video the authors provide, a user needs to segment every single object which are selected for the manipulation. Moreover, segmenting small and fine objects requires further elaborate and laborious annotations from the user, resulting in a critical bottleneck for practical use.Due to these concerns, I would keep my previous rating of 6. Marginally above acceptance threshold. ------update after rebuttalAfter reading the author's response, the authors stated that they indeed identify the optimization difficulty and consensus distance in theory, while only empirically justify its generalization on training performance. As also pointed out by reviewers 1 and 3, the gap between the convergence rate/consensus distance and the generalization capability still exists, causing the mismatch between the theory and the simulations. But at the same time, the work can also serve as an initial good start and raises good points for the literature. I will keep my score unchanged. Update: I appreciate the response to address my concerns carefully. My major concerns were the lack of novelty and some unclear descriptions on details in methods and experiments, but most of them have been well addressed. At first, I didn't see the technical challenges when applying DKL or any multi-task GPs into hyperparamter optimization. After reading the response and the manuscript again, I'm convinced the task augmentation plays a critical role in this setting. And, additional information from the revised manuscript helps to understand the details in method and experiments. So, I increased my score by two points. **Comments after Author Response**I thank the authors for their response. The explanation of the main approach has certainly improved and the details of task augmentation and warm start are much clearer now. I also appreciate the added discussion on bias in Stochastic Gradients for GP training. The reference cited does seem to indicate that the training will converge despite bias in gradients. The warm start and task augmentation approaches still seem a bit heuristic to me. The task augmentation approach seems to assume an inherent linearity in errors/noise for the metric of interest which may not hold in practice. The choice of loss function for warm start also seems rather arbitrary. It is not entirely clear why choosing $\mathbf{X}^{\text{(init)}}$ that minimizes (12) is a good idea. However as my main concern about the validity of SGD in training the model has been addressed and both the task augmentation and warm start approaches seem somewhat intuitive, at least at the idea level, I am increasing my score by one point. ------------------------------------------------------------------------------------------------------Update after author response------------------------------------------------------------------------------------------------------1) Context and content attentionThank you for updating and saying that only some of the implementations include bias! I think now this part is not misleading and can be of interest. I still have some concerns that this part does not fit the whole story very well - but this is the matter of taste. In the current state, I think it is ok :)2) On the comparison with head pruning and on the paper going beyond practical realm.I agree with your comments, but I do think you should make it very clear in the paper. In the current state, the paper tried to make practical contributions and, since they mostly do not hold (e.g., head pruning is simpler in practice), it's hard to appreciate the paper's value. I think you need to modify the things you highlight, and with proper discussion it would be much better. For example, if you state explicitly that in practice pruning may be simpler, but your results say/illustrate something other than practical applications. You won't lose because of it; in fact, I think the opposite.Overall, I think the paper has improved during the discussion period. In a hope that the authors address my later comments and discuss the pruning in the text, I'm raising the score. **EDIT** my original score was 5; i have read the authors response and updated my score to 6.My score represents the experimental side of the paper, i do not feel confident judging the impact of theoretical contributions. ================= after response Thank you for providing the response. I would like to add some comments to the response.About Q1-- I agree that it is important to work on the original ordinary learning as the first paper in this direction, but it makes the story of the paper inconsistent. The adverse affect of LSR might only show up in other problem settings (distillation/few-shot/transfer learning) and not in ordinary learning, and if that is the case, it is unclear why the paper focuses on ordinary learning. (This is just a comment on the story/motivation of the paper.)About Q2-- Although Fig.1 shows the benefits of the one-hot label over label smoothed version in latter stages of training, I believe it does not show how the number of K plays a role in the performance and the difficulty.About Q3-- I agree that 90 epochs is sufficient for the LSR baseline, and the current figures demonstrate the benefits of the proposed method, but I was more interested in the difference of (s) in TSLA(s). From Fig.1, it still seems like TSLA(60,70,80) are improving at the end of 90 epochs, while TSLA (30,40,50) seem to have converged. ********************************The rebuttal is very detailed and contains many useful new baselines and results. The authors answer to reviewers' questions carefully and with great supportive details. I appreciate the efforts and many of my questions are answered, therefore I have increased my score. I'm not  an expert from medical image field. So it's a little bit hard for me to evaluate the significance of the reported results and proposed methods. However, I think this paper is well-motivated and current version has greatly improved over the first draft.  For the machine learning community, it's an interesting and important application and thus should be encouraged. I was concerned about the method because the proposed method seems to overkill the original version's simple experiments. The newly added experiments and clarifications make sense to me.  === AFTER REBUTTAL ===The authors have raised a few fair points in the rebuttal (especially point 1), so I've adjusted my rating accordingly.  Post Rebuttal: I believe the positioning of the paper could be improved but at the same time empirical results in the paper are strong. I am adjusting my score to 6 with a confidence of 4. ### Updates after the rebuttalI appreciate the authors' timely response. The latest update explaining the regularization effect of using a fixed canvas $x_c$ and a learnable mask to avoid overfitting to spurious features, which seems to be inevitable if using a learnable $x_c$, does seem plausible and differentiates itself from the general idea of UAT. From the updated content in Appendix H, using a learnable $x_c$ does not seem to be able to find interpretable patterns. I feel contributing a new method with some improvements is worth an accept.However, I still feel the results are not conclusive enough. Regardless of the final decision, I hope to see more comparisons with simpler baselines in the future version. To highlight the novelty and effectiveness of the proposed method, the authors should try to compare with more baseline approaches that do not have presumptions on $x_c$ and initialize $x_c$ randomly. Currently, $x_c$ are selected as images having highest prediction scores from the model. Instead, we can learn $x_c$ from random initializations by using some approaches that enhance the transferability of adversarial examples. There are simple methods on improving the transferability of adversarial examples, e.g., adding Gaussian noise when crafting adversarial examples [1], or adding different random data augmentations [2]. These methods may also lead to interpretable patterns but the effectiveness of the resulting patterns could be stronger. I do not fully agree that a desirable canvas has to be a neutral or unbiased image. [1] Wu, Lei, Zhanxing Zhu, Cheng Tai, and Weinan E. "Understanding and enhancing the transferability of adversarial examples." arXiv preprint arXiv:1802.09707 (2018).[2] Huang, Qian, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. "Enhancing adversarial example transferability with an intermediate level attack."  CVPR (2019). ===============================Post-RebuttalI thank the authors for their detailed answers.  After reading the other reviews and the author's rebuttal, I maintain my rating of 6 for the paper. My concerns on the description of the SIS methods and results on the proposed mitigation are not addressed.I am not convinced as R1 and R4 that training on the SIS and testing on the full image is the correct way of testing if SIS is sufficient for the model's predictions. If we would present SIS images with unrelatable labels (A instead of Cat, B instead of Dog, C instead of Boat, etc) to humans and ask them to learn the mappings, I am confident they could achieve good results. As pointed out by other reviewers we can see some patterns in the SIS. Showing a full image afterwards and asking to predict (A, B, C, ...) would be quite difficult however. It's easy to infer the pattern from the full image, but the other way around is more difficult. To me the most important is that a given model architecture can be trained on the SIS of another trained model (with different random initializations) and still be able to learn and generalize. That alone shows in my opinion that the dataset contains undesirable statistical artifacts shared by training and test sets, and as the authors says in the paper 'interpretability method that faithfully describes the model should output these nonsensical rationales. I believe R4 makes a valuable point when saying '[...] I think it might be more useful to look at the mean SIS size of those that are wrongly classified by humans'. This seems to be a better way of gauging what threshold should be used for the size. # RevisionAfter a fruitful conversation with the authors I am changing my decision.The conducted experiment presented in Figure 4 confirms an advantage of L-conv.The paper has a room for improvement. However, it contains valuable discoveries and can be clearly placed in the field of symmetry-aware models. The proposed approach is well-described and is properly evaluated on shallow networks. While the generalization of the results to deeper models is not possible (and the reasons are described by the authors correcly), the approach is interesting by itself and is important to the field. AFTER READING THE AUTHORS' REPLY, I HAVE CHANGED MY RATING TO 6. **After rebuttal responses**: I have read the authors updated draft and response to my concerns, as well as the other reviews. The updated paper provides a clearer framing and some missing baselines have also been included. I raised my evaluation to a weak acceptance for the paper.  -- Update after author responseThe authors have addressed some of my concerns, I have revised my score accordingly. I am not convinced that the method is significantly better than the baselines as it performs worse on some tasks/datasets and all the tasks in the paper are similar. Even the heuristic policy used for gathering the data is similar to the downstream tasks. The addition of other types of downstream navigation tasks such as objectnav or imagenav would make the paper much stronger. I like the noisy pose and depth experiments but there's no information on the type and amount of noise. I am assuming a zero-mean gaussian noise, which is not very realistic in my opinion. The standard deviation of the noise is not reported. I encourage the authors to add some other downstream navigation task and add realistic noise with relevant details to the camera-ready version if accepted. ### ProblemThis paper considers the effect of label noise on stochastic gradient descent. The setup is that there is a vector $v \in R^d$. We observe samples from $v^2\cdot x$. We only have $n < d$ samples but $v$ is $r$-sparse for $r < n < d$ which makes recovery possible information theoretically. The main result is that stochastic gradient descent with label noise, and without any explicit regularization will recover the ground truth. whereas adding spherical Gaussian noise does not.### Pros and ConsThe problem is a clean toy problem with which to illustrate the gap between algorithms. It shows a clean separation between the power of label noise and that of random Gaussian noise. The model appears to be the simplest model where one can hope to see the regularization effects of noise (the simpler linear regression model wouldnt show these effects). One possible criticism could be to ask if understanding this model is truly getting us closer to understanding what happens in deep nets. At this point it is hard to say, but proving such a result even in this simple model is not trivial, and is definitely a contribution.### EvaluationI think this is a solid theoretical contribution on an important problem, and the paper should be accepted.### Further commentsI was a little confused by the comment that the coefficients are assumed to be in ${0,1}$ since they then satisfy $v_i^2 = v_i$ as this seems to linearize the model. The authors should probably clarify that this is actually not what is going on. It might be better to use a different setting of parameters even for exposition.  Post-rebuttal:After reading the author feedback and other reviewers' comments, I would change my rating to 6. I partially agree with reviewer #3 and reviewer #5 that this work is an incremental extension of LORD. In addition, like I mentioned in the "Cons", the styles are decided by the pre-defined transformations that are dataset-specific. For different datasets the transformations are defined differently based on the properties of that dataset. This might restrict the method from being extended to new datasets with styles that are not easy to find corresponding transformations. However, I think this paper still have some inspiring aspects, like the idea of disentangling class-specific attributes and common attributes among all classes. And the results of this paper seems good compared with previous approaches. So I would give the rating of 6. Post Rebuttal: The authors have resolved my main concerns. However, the reliance on the nature of transformations applied is still an important factor. Nonetheless, I do see value in the approach presented in this paper. I am revising my rating to 6.  **update: the authors have answered most of my questions** Thanks for your response, clarifying. -------------------after rebuttal----------I thank the author for your answer. Here're the response to your latest reply.For point 1, I am aware of your performance and encourage you to add the discussion in the main paper.For point 2, I still doubt it since I think your claim may only hold for the ImageNet experiment on the Outputs Norm term. According to figure 2, the network does not faster converge with the help of BN. The regularization gap happened in the late stage. But, the gap of two networks on the feature embedding norm and mean output norm happens at the very beginning and keeps increasing. (except  ImageNet experiment on the Outputs Norm term)Overall, I think the paper may bring some insights to understand the BN and would like to keep my original score. I thank the authors for providing further information and answering the question raised by the reviewers. Based on the responses and clarifying the issues I had, I have adjusted my score accordingly and improved it from a 5 to 6.  i have read the author(s)' comments and I have updated the ratings based on their replies. Thanks for very extensive clarification. Adding these comments in the final revision would significantly improve the quality of the paper. **Post rebuttal (round #3)**Thanks to the authors' effort on the rebuttal. Despite the extensive efforts, I feel the review/rebuttal iteration is not satisfactory, possibly due to some miscommunication.To be clear, I want to re-emphasize that significant parts of my concerns were about **misleading claims** on prior work, and comparison to them was the next step.- For example, I just wanted to clarify that the claim "type A ignores the noise and cannot learn the stochastic mapping" is wrong. The paper could simply fix the claim instead of including a massive related work section. Maybe my review also has some responsibility: I could simply say **fix** the wrong claims, instead of indirectly delivering by pointing them out.Also, as I explicitly mentioned the concerns A,C,D,F, the rebuttal could address them point-by-point.- In particular, I'm not convinced that cBN/sBN is **not applicable** to continuous conditions, as sBN predicts the BN parameters from the continuous latent variable.Despite the remaining concerns, I raised the score (from 5) to 6 as the architectures with multiplicative interactions are an important and timely topic. However, I think the paper is on the borderline, and the rebuttal and revised paper could be much stronger.------ Update: After seeing the changes the authors have made, I have slightly increased my score. Thanks! EDIT: After discussion, I have increased my score and am recommending weak accept. See the discussion with the authors for detail ==========Post rebuttal============Thanks for the authors' response. The authors have addressed some of my concerns. I have raised my score accordingly. [Update after author responses]: The authors have addressed most of my concerns and I've updated the score accordingly. #####Update######Thank the authors for the response. I keep my score as 6. ---------Update post-rebuttal: Thank you for addressing most of my comments. The new results on ImageNets are greatly appreciated too. However, in light of other reviews, I would have hoped that the authors try better training procedures on SWEEN-7 (e.g., MACER even if it means using other ResNets instead of VGG), as otherwise it is difficult to judge whether ensembling really helps. It is unclear why MACER was not used on ImageNet (since all models are ResNets). Overall, the work explores a really important direction of research, but could benefit from further improvements. Post rebuttal response:I thank the authors for their response to my review and I take their point that establishing theorem 2 points to a limitation of using smoothing with ensembles, but I think this point could be made much more clear in the main text. Following a reading of all of the other reviews and re-evaluating the paper I remain optimistic and slightly positive about the paper as I think it is an important and interesting research direction; however, I am not fully convinced to increase my score given that some of the empirical comparisons could be greatly strengthened to be more than marginal improvements over MACER trained networks. I think that any insights that arise during the process of strengthening the results would contribute to a better understanding of the method and a stronger paper.  --------- Comments after reading the author response:I thank the authors for adding the experiments and applying the suggested modifications! I have updated my score based on the changes and the clarifications made on the related work, and also the results of the mounted attacks.  ==== Updated reviewIn light of the authors' revisions, I'm happy to raise my score to 6. I think the paper is better, although I still wish the presentation was more compelling -- as given, this seems more like an exercise than a contribution with a demonstrated impact. On the other hand, this level of contribution seems relatively on par with e.g. other deep RL papers.Regarding the revisions, I would encourage the authors to integrate them with the main text. For example, Figure 3 really wants the weight=1 result (maybe as two separate panels -- comparison to other algorithms, a); ablation on weights, b)). **Update Based on Author Feedback**I am grateful to the authors for their detailed replies to my questions. I understand some of the minor points better and am happy they have revised some unclear parts. Overall, I think this paper has some real strengths: theory-guided design, important problem, novel methods. I do feel (similar to R3) that the experiments and applications could have been far more convincing. Weighing these strengths and weakness, I still tilt slightly towards accept (6). **Post-Rebuttal**The authors have done well with their additional page, and many of the concerns I had have been dealt with the latest iteration of the paper. I have increased my score. Re: Motivation for c and d. The current *objective* for c and d is described in the paper, but the motivation is not. I hope the authors add to this discussion in the next iteration.  **After Authors' Response:**In my understanding the main contribution of this work is the identification of a new search space for activation functions. This might be very similar to earlier ones but the authors convinced me that it is a useful contribution. All my comments were addressed and as far as I saw also the comments of all other reviewers were addressed. For this reason, I increased my score. *********updated after rebuttal periodI still consider this as an interesting contribution, and stand with my original rating. It would be useful if the discrepancies and similarity between the connectivity structures in the model and the anatomy could be more carefully discussed in the paper. ============Post Rebuttal=============================Thanks for the additional experiments and the updates. The new results are informative.  ==========================================================================================While I still hold my concern on the i.i.d. assumption of the context as it is less interesting both practically and theoretically, the author response and the revised paper clearly resolve my other questions and concerns.  I am increasing my score to 6. Edit after rebuttal: I have read the author response and thank the authors for their comments and answers to my questions. I would like to keep my rating as it is. I recommend authors to tone down the claims around being first MultimodalQA dataset and position themself properly with respect to previous related work if accepted. EDIT: Based on the author's modifications to the manuscript, I have increased my score from 5 to 6. ---------Thanks for the clarifications. I have raised my score.I agree that the method is easier and more general than methods such as Adversarial Autoaugment. It will be interesting to see how the approach generalizes to larger/complex datasets without an expert specified family of transformations or without a good generative model. Update after reading author response:Thank you for the thoughtful response.  I appreciate that you have added extra implementation details and am changing my score to a 6.  Regarding the limitations in scope:  My apologies if my review was confusingly worded.  I just wanted to clarify that I had meant that the counterfactual generation method itself may have limitations (not the high-level DST task, which I agree has broad uses).  The concern is that the adversarial example generation strategy might be too domain-specific to transfer easily to other tasks, and that this might limit the impact of the proposed counterfactual generation method.  I think this is somewhat related to the parts of the methodology that R1 described as "ad-hoc", "engineering intensive", or reliant on heuristics.  I still have some reservations about the transferability of the proposed methods, though the authors' response to R1 did clarify a bit on this point.------------------------------------------------------------------------------------------------------------------------------------------- ===== update =====I am very grateful for the patient and detailed response. Due to limited time, I wasn't able to quickly follow up on the discussion.  I think the current quality of the paper is improved, so I increase the score slightly. However, I still struggle to follow some of the statements even after reading the response, it could be my comprehension or something to do with style/writing. /*************************Post-Rebuttal**********************/The authors address many of my concerns well, and I agree with their rebuttal.The modified manuscript also looks good, too.I raise my rating. =======================(Nov 24) The author response addressed my concerns and I therefore raised my score from 5 to 6. I particularly like the idea of using successor representation for density ratio learning. --------------------------------------Most of the concerns are addressed by the authors, and I raised my score accordingly. Update after rebuttal: Thank you to the authors for their detailed response. The clarification and updates in the geometric skews section and the more explicit justification and connections between the framework/theoretical results and the experiments improved readability and clarity. I also appreciate putting greater weight on the theoretical contributions and "easy-to-learn" task definitions, which provide a simple but non-trivial test case for robustness research. I'm increasing my score accordingly. The author addressed my concerns. Ill keep the score 6.======================= Post rebuttal:- The authors have addressed most of my main concerns and the additional experiment looks good to me. Therefore, I increase my score to 6. However, some experiment results are still missing and the paper still needs some editing before published, especially the experiment section. For example figure 6. is misleading since the comparison is not fair. _UPDATE AFTER REBUTTAL_The authors have improved the paper somewhat by expanding and clarifying the discussion on some key parts. While I think there is still much room for improvement in the paper, the general consensus seems to be towards acceptance. I will not oppose if that is the decision, and have increased my score accordingly. However I remain very borderline and I am not sure if I am fully convinced by all the claims.One specific issue: I think the authors should make it more clear in the paper that the experiments are done in 128 pixel resolution, in light of R1's questions. It is important that the reader be aware of this, as the noise inputs arguably become much more important in high resolutions where there is more stochastic detail. I personally did not realize this when writing my review, and now wonder how the results would be at e.g. 256 or 512 resolution. If possible I would suggest the authors still run such experiments. This also probably explains my comment above on the lack of apparent visual differences in inversion results. ### Post Rebuttal ### Thanks for the clarification, I have adjusted my score accordinglyWhile the paper's second contributions allow the incorporation of a more refined resource constraints, it appears to me that the the proposed approach is unlikely to yield a computationally tractable algorithm even in the tabular setting, when the original state space is small. Indeed, the vector of remaining resource levels are embedded into the state, so that it will results in a curse in dimensionality even when the original space is of a manageable size. Overall, based on my evaluation on the theoretical and practical impact of the paper, I find that the contributions fall marginally below the acceptance threshold.  Finally, after the statement of the regret bound in Theorem 2, it is helpful to compare with the regret bound in the recent work by Tian et al. 2020, similarly to how the proposed algorithm is compared to Osband & Van Roy (2014). ############################################################Post-Rebuttal:After reviewing the concerns raised by the other reviewers, and the responses provided by the authors, I have decided to adjust my scores.Moreover, I was disappointed that the authors did not use the extra one page to move some material from the appendix to the main text in order to elaborate on the proposed method. # Post-response updateThank you to the authors for very helpful clarifications. This paper provides a reasonable start for a new potential direction in NAS research and so may be worth presenting at the conference, but the justification and applicability of the method is somewhat limited. I therefore stand by my original assessment. [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. I  do believe some of my concerns have been addressed while the main issue on the lack of some necessary evaluations (e.g. num. of demonstrations) remains. Thus I may not be able to escalate my justification to even higher. *********after reading rebuttal *******I have increased my rating, but may still have some concerns. See below. ##### Update ######Although it is responded that the simulation setting used in the paper does cause blowing up samples or marginal variance, it is in general impossible or the setting assumes too sparse case where considered graphs are almost empty. In addition, as you mentioned, I also ackowledge that it is a widely-used setting; however, there are a lot of papers that are rejected because of the unfair simulation setting. I like the main idea of the paper a lot, and hence, I hope the authors set the simulation setting more carefully. Furthermore, it is really frustaring answer that the authors consider the only case where graph is uniquely idenfiable from the pure observations. As you know that is really rare when the number of nodes is large (p > 50).  Update:The authors have explained the issue raised in the review. It's not ideal that the algorithm requires the knowledge of rank beforehand, but it's okay if this point is clearly communicated in the paper. I would keep my current score.  -----------------------------Rebuttal: Thank you to the authors for addressing the comments and for the changes to the paper, and thank you for adding the examples on Scurve and Swiss Roll and the comparison with Isomap. I find it slightly surprising that Isomap performs very well for local metrics on the spheres dataset, because Isomap tends to preserve larger distances. Not sure I fully understand why.Related to my initial concern about clustered data, my impression would be that in addition to the spheres visual example in the main paper, it would have been good to add the Scurve or the Swiss roll to emphasize that UMATO is not specifically designed for clustered data (all the examples in Table 2 are for data with implicit clusters). How do the methods perform in terms of the local and global metrics for the Scurve and Swiss roll datasets? Would zooming in on PCA reveal similar local structures to UMATO? What is the difference between y_i and y_i' used in eq (8)? Are they the same?Second line on page 2: Should it be UMATO instead of UMAP?   ##########################################################################I raised my score based on the author's response. ##########################################################################Post rebuttalI'm happy with the author's response and would like to keep my original score. The paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency. In the beginning, "I think it is an interesting piece of work, and it may be helpful to develop the basic structural design of neural networks. ". However, after getting the response from the author(s), I more doubt the significance of the work in this paper: although many types of models have been proposed in this paper, the improvement over the baseline models is limited. I did not lower the grade on this paper since I thought it would be interesting and important (if effective) to extend the traditional real number field to more complex algebraic structures. 6. Post-rebuttal UpdatesIn light of the authors new experiments and response, I have increased my score.  The bootstrap error shows slight deviations under the MSE, and thus, I do feel that the scope of generality of this work is still limited.  However, I feel that the presented framework is novel and the experiments consistently demonstrate that the bootstrap error is consistently low under soft-error in a number of settings.   ---Thank you to the authors for their comments and clarifications.  I still am skeptical that the proposed dataset is a fundamental improvement over Quick Draw---ultimately, both datasets contain compact sketches from a single category containing relatively few strokes. But given that your updates and clarifications have addressed many of my questions, I am raising my score. Post-Rebuttal:I would like to thank the authors for addressing the questions and concerns. I still believe that the general idea of conditional networks might pose a relevant contribution to improving OOD generalisation. However, after rereading the submission, reading the other reviews, and taking the rebuttal into consideration, I think there are some aspects which need some revision and clarification. I comment on this in more detail below. Therefore, I stand with my initial rating of borderline, but I would like to encourage the authors to revise their paper taking the points raised by the reviewers into consideration and submit again.- **Section 4.2, discussion of figure 4 and hypothesis 2:** I thank the authors for the added discussion. However, the results are a bit at odds with the premise of the proposed approach, in my opinion. The bullet points which detail why hypothesis 2 could be interesting, are refuted by the results presented below in that section. I believe the result that activations look different (what concerns scale of activation and which features are active) by itself is less surprising as the approach tackles the normalisation of activations explicitly. But more importantly, other than that, I would say the activation patterns for different cities look qualitatively similar in both models and does not align well with the story of the paper. So, in my opinion, the activation patterns on the left (AMLL U-Net) in figure 4 look very similar for all cities, and also the patterns on the right (Cond. U-Net) look quite alike for different cities. Therefore, the interpretation of these results in the context of conditioning on auxiliary information remains speculative. I believe this part of the submission requires a careful reconsideration. - **Geocoordinates as metadata *t_n*:** A potential reason for the difficulties in the previous point could be the choice of metadata *t_n* in the segmentation example. I thank the authors for elaborating again on the choice. Still, I am not convinced that this is the most suitable choice to present the advantages of the proposed approach. If the conditioning network really just performs city ID classification, I would not be sure that any kind of useful (generalisable) features are extracted. This could be a potential explanation why no conditioning influence on the results in figure 4b is observed.- **Difference Cond. U-Net:** I shouldve been more explicit in my question. On page 6, last paragraph of Generalization via conditioning it reads: *We identify as Cond. U-Net those models in which both the encoder and decoder are modulated, which yielded a small gain in performance over just modulating the encoder or decoder alone.* If I see correctly, Cond. U-Net should be replaced by Fully Cond. U-Net, as the provided answer suggests, too.###  --Update after rebuttal--The reviewer thanks the authors for addressing the key questions and concerns and have updated the confidence score accordingly. * Post rebuttal update:  Several of my concerns about clarity of the experimental section were addressed. Therefore I increased my score and now I am inclining towards accepting the paper. Thanks for the clarifications from the authors. The discussion was very helpful.  ####update####The experiment results are not surprised, but strong enough. Still no very strong baseline provided in this submission, but it might be good to set up a benchmark in this direction. However, T5 model needs more computational resource and the experiment results are hard to replicate. Overall, I would like to keep rating. After authors' response:Thanks for the response! I will keep my score. **EDIT: with author feedback and changes to manuscript (and supplementary) I think that the study is more interesting than expressed in my original review, however the sim-real gap with respect to applicability to robotics is still a major concern IMO. I have updated my score accordingly.** ======== POST REBUTTAL RESPONSE========After reading the feedback and revision of the paper, most of my above concerns are addressed. I agree that the comparison with HER/CER is not fair. I also notice that the author added more references and details based on all 4 reviews.  Thus, I decided to improve my score on this paper. ===================== Update =====================Raised to 6. A theoretical calculation on 4. would be great, if it can be done before publication. NN can be simplified, say linear, or one hidden layer linear.  --- Updated score ---The authors did a nice job of addressing many of my concerns. But there are still some lingering issues with the experimental design (especially with the reinforcement learning experiments). The main concern I have is why the variance for your results is so low from run to run. This could suggest the problem is too easy, or that there is a bug somewhere. While I don't think it is enough to outright reject the paper, it still puts me on the fence about a strong accept. ==== Update ====Increased score according to the revision and discussion. ************After Rebuttal:I thank the authors for their multiple clarifications, and apologize for my initial misunderstanding.I understand now that the flow model is used to compute $p(x|z)$ as a function of $x$ and $z$. Maybe the "decoder" terminology is a bit confusing here, but this is quite a nice idea overall. It would have been nice to see multiple samples from  $p(x|z)$ for a fixed $z$, to evaluate the expressiveness of the model.I'm raising my score to acceptance.PS: Some typos remain in the revised version, eg: "varnishes", "tne" ------------------The authors answers one of the most important concerns, I have raised score to 6. ---Post rebuttal---Thank you for the response, and thank you for checking the performance comparison against the white-noise perturbation. It would be interesting to see a future work involving means other than Adversarial training (e.g. including other simple mechanisms like weight decay and dropout) to help reduce the overfitting effects in the pretraining phase. I would like to keep my score as is.  [post discussion edit]After discussion, I lowered my  score to 'marginally above acceptance threshold':- the theory section of the paper looks very interesting to me- I find it hard to see clearly from the experimental section the extent to which the method beats existing methods- I feel that the experiments could be made more rigorous to clearly show the benefit compared to other techniques- concretely, I feel taht the tables could be structured in such a way that one can glance at each single table, and see clearly in what way LTP is better than the baselines. Concretely, for the results tables:- table 2: LTP gives worse accuracy than Renda, and worse compression. The text mentions training epochs are fewer for LTP, but the table doesn't show this benefit (there is no column with number of training epochs)- table 3: this table is a little apples and oranges I feel. it shows that the number of training epochs is less for LTP than Renda, but the compression ratio is slightly less. I feel that you could have compressed a little more, to make the compression ratios comparable. In addition, I feel it is important to include the accuracy in the table. Without accuracy, then I feel it is not possible to compare.- table 4: I feel you could do whatever is needed to do to ensure that the baseline model you are using matches the baseline that other teams are using. This could mean porting NTP to caffe, or porting caffe network into torch. Currently, the LTP pruning is on a worse 'parent' model, and performs worse than the other baselines in terms of accuracy. I'm not sure it's sufficient to hand-wavingly just add/subtract the delta in performance between the baselines to the LTP results (which is not explicitly being done, but if one doesn't do that, then one would have to assume that LTP performs worse, I feel)- which only leaves table 5 that plausibly provides an apples-for-apples comparison, but only for a single baseline Post-discussion update: I have read the updated paper and other reviews, especially from reviewer #2. While I am still positive about the approach/methodology, I am not confident about the technical details of the experiments, without which, it's very hard to justify the effectiveness of the method. I share other reviewers' views regarding inconsistencies, e.g. Tables 2-5, that have not been fixed in the updated paper. === Update ===The arguments in the rebuttal clarify some confusions I had and illuminate the contributions of the paper further.   I am now completely on the fence about acceptance, but will tip toward positive. ===Update===After reading the authors' feedback and other reviews, I would keep my current rating. -------------------after rebuttal-----------------------I thank the authors for their rebuttal. Since the authors reply near the discussion phase end, I cannot ask follow-up questions. The answers partially address my concerns and thus I would raise my score to 6. For novelty , the author answers three points. For the first point, fuse two channels of information are not so convincing on the novelty aspects. Also, there needs an extra cost to collect process the images. For the second point, how would you formulate a regularize? EDIT after discussion:Thanks for the authors' comments and revisions to this paper. I'm glad to see that some of my concerns have been addressed so I decide to change my score.Just some minor comments about writing for the revised version. I think the proof sketch is clear and easy to follow the logic with some room for further improvement. Like the other reviers' comments, I think some part of the paper like notations should be with clear explanation.------------------------------------------------------------- ### Post rebuttal feedbackIt's good to see that RMSD experiments are added and the results are better than Simm et al. Therefore, I am raising my score to 6. I also realized that the validity calculation is different from standard graph generation methods. The validity results now look reasonable to me. =========== POST REBUTTALThe rebuttal is very helpful and "to the point" in clarifying the issues that I had raised as concerns the impact of the assumptions taken in the theoretical proofs. The fact that complexity hinges on average node degree and that any vertex cover sufficies for the proof confirms that the approach put forward in the paper might work out of the theoretical box. The Authors also suggest that the theoretical framework can be translated to a running model with a certain ease and that, in fact, its practical implementation and empircal assessment is on the way. Which brings me to the key point in my assessment. I am convinced there is value in this work and in the theoretical contribution in the paper.  I am not convinced that this paper can have a strong impact without an empirical validation. As I have underlined in my review, there are several related works in literature, with a similar theoretical flair which, nevertheless, provided at least a simple empirical validation. I believe that this paper shold do the same: it would be stronger, more complete and with a higher potential to influence the community. As it is, this is a borderline paper (leaning on the positive side).  Update: I read the reply and thank the authors for the clarifications. Post-rebuttal: Thanks for the authors response. After reading the responses and other comments and checking the updates to the paper, I retain the score and my recommendation at weak accept. Post-Rebutal: I really thank the authors for their efforts  following my comments. I think they have really addressed my concerns. I therefore raise the score of the paper and recommend it for acceptance.  --- post-rebuttal feedback ---The authors have addressed most of my concerns. My rating for this paper therefore remains on the positive side.  The authors have provided standard errors which improves the confidence that the improvements in the experiments are significant.  ==========================After rebuttal=================================Thanks very much for your efforts to address my concerns. I kept my score unchanged. I agree with most of the responses, except for the response to "L2 penalty, backtracking line search for determining the pruning ratio". This paper is not proposing a practical algorithm but a revisiting, so I don't think the computational cost is a bottleneck in preventing you from using more advanced methods to get more robust conclusions. After rebuttal: I thank the authors for comprehensive rebuttal, which addressed most of my concerns. I update the score accordingly.  --------------------------------------------------------------------------------------------------------------------------------------------I have read the rebuttal. The experiments in the rebuttals shows the effectiveness of TOQ-Nets in other large, real-world dataset. These experiments should be added in the revision of the paper and I would like to change my score to 6. =======================After reading the authors response ============================I thank the authors for answering all the questions that been raised by the fellow reviewers. Looking at the responses and changes made to the paper, I have increased the score from 5 to 6 after the authors clarified the issues I had with the paper. Overall this paper demonstrates the effectiveness of using a GNN for negotiation dialogues. I feel that this approach can be applied for any non-collaborative dialog settings and the claims of interpretability make this approach better.  ***After Rebuttal***I have read the reviews by other reviewers and the responses of the authors to the questions posed by other reviewers. I enjoyed the additional experiments that the authors added to the paper during the rebuttal. The paper has shown interesting observations on the informative samples present in real-world datasets for different architectures. However, I still believe the method proposed by the paper is too inefficient for simple algorithm changes like changes in initialization. Hence, I am keeping the score the same after the rebuttal. ================After the author response, I raise my score by 1 (see my comment to them) --------------------------------------------------------------------------------------------------After rebuttals the authors significantly improved the presented work, including and discussing some relevant work which was previously missing.  After author responseThe authors have responded to most of my questions/concerns satisfactorily.Changing my score from 4 to 6 _______________________________________________________________________________________________________________________________________________________UPDATE: After reading the rebuttal I think most of my concerns have been addressed and I am updating my score accordingly.   Update: The authors have included an additional experiment around fold generation in Sec 6.6. However, no baselines are included, so it is difficult to understand the result in context and understand how this method generalizes compared to existing methods. The authors have also included two additional baselines: Bepler, et al. and MSAs. More analysis is needed to compare this with SOTA in representation learning. The authors compare to "Elnaggar et al. (2020)" but it isn't clear which model was used. Elnaggar et al. (2020) have released a series of different models. The authors should clarify this in the camera-ready and ensure they used the best models released by Elnaggar et al. I have increased my score. Post Rebuttal Comments: Authors have addressed most of my concerns and as a result I have increased the rating from 5 to 6. Thanks! ### Response to RebuttalThank authors for taking the time to clarifications and considering my comments.We appreciate authors' efforts to add additional experiments results in Table 1 and Table2. However, the performance improvements are marginal (more or less 0.7) and speed of Graph Transformer is slower than transformer.Even though additional explanations about positional encoding (Appendix 8.1) can resolve our concerns, layer iteration (Figure 6) are not still clear for us, e.g., what are orange blue, yellow nodes? how layer iterations are used in graph transformer? Authors should refine its main context to increase understanding instead of adding lengthy Appendix for us. This such paper presentation and organization are not clear to understand.Considering the above points, we still remain our decision. =======11/22======I am deciding to keep the same scores as before. Some of the initial concerns remain. I think the paper still lacks motivation wrt the GPT2 model generating missing edges. Thank you for getting the latest results, the paper is stronger than before and with some more work, I am confident it will be a good contribution to the research community.=====11/24======After having read through the explanation behind using GPT2 as edge features (and sufficient backing by 2 closely related work), I am increasing my score to 6. I think the discussion helped in somewhat convincing me that this approach would work for ConceptNet because of its limited schema. ## Response to the author feedback:We appreciate the authors for the extra effort to demonstrate the abiltiy of FLAP by adding more experimental results and discussions. Meta-reinforcement learning for adaptation to OOD tasks is an interesting field. Although the idea of FLAP is simple, the strong experimental results have demonstrated that by predicting weights rather than optimizing, FLAP is a fast and effective meta-RL algorithm for adaptation to OOD tasks compared to previous approaches that did not focus on this area. I believe FLAP will help draw more attention to this field and provide a direction for more theoretical analysis, thus I have increased the score. ============ UPDATE =============Thanks to the authors for their feedback. I appreciate the efforts on clarification and loose-end tying.One outstanding thing to clarify to help us understand whether Claim 1 can be fully supported: - AFAIK the Table 1 / Table 5 that underpin this claim are comparing numbers copied from previous papers with numbers generated from Domain Bed benchmark? However I suspect the splits are not the same. For example, some previous benchmarks have a fixed split by default, while I understood Domain Bed use multiple random splits? If so the numbers are not directly comparable, and it still may not be fair to make a strong claim that tuned ERM outperforms prior work. ---Update: upped score from 4 to 5; see comment thread.Update again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words. Update after rebuttal:My initial concerns were clarified by the authors and the paper substantially improved.  Post rebuttal update:I am still positive about the paper and believe that it deserves to be published. However, I agree with Reviewer 2 that some non-trivial rewriting is necessary. The flaws are most likely not very hard to be repaired but it will require substantial additional work and the new version needs to be carefully checked. This is the main reason why I downgraded my score from 7 to 6. Post rebuttal update:I read the other reviewers' responses, and, although I am still positive about this paper, I agree with R2 and R4 that safely fixing the theoretical proofs would require a full revision. For this reason, I am lowering my score to 6.%%%%%%%%%%%%%%%%%%%%%%%%% Update: I thank the authors for providing additional 3D metrics, and comparisons to unsupervised 2D segmentation techniques. I have updated my rating. Update: Thanks for the authors' response. The authors have clarified the equations, addressed the concerns of claims of no supervision, and provided more experiments and metrics to back the current set of claims. I do believe this paper to be interesting enough for an ICLR paper, and have updated my score accordingly.Also, do note that the motion segmentation works do not assume a single foreground object, they assume an arbitrary amount of them. However, you are correct in that they assume an entire video as input, as opposed to the proposed method which can segment individual frames. After the discussions and the interactions with the authors, it came to my attention that one of my comments was wrong - I looked at a different paper that led me to have the conclusion that the authors cited a wrong performance number from a competitor in the literature; it turns out that I was wrong and I apologize to the authors.The paper at the initial submission version read very rough, with a lot of grammatical errors, typos, or misleading/incorrect statements. In the experiment section, it is stated that three datasets were used for evaluations but in Table 1 it appears there is the fourth dataset ILSVRC2012 used, which never mentioned in the text, nor is mentioned in the paper on how that dataset is used (a portion like WebVision or the full). The whole paper ended up with no conclusion or discussion. After the discussions with the authors, it became clearer that the mentioned three datasets in the text were for training and ILSVRC2012 was used for evaluation. But still it would be a lot clearer to have such a statement in the text.I had the comment that the title of the paper was misleading. The title reads: ROBUST CURRICULUM LEARNING: FROM CLEAN LAEL DETECTION TO NOISY LABEL SELF-CORRECTION. However, the proposed method, together with all the reported evaluations, focuses on learning the noise (in the labels); neither clean label detection nor noisy label correction is addressed. The authors disagreed with me but I was still not convinced by their argument. For noisy label self-correction, it may be relevant; but for clear label detection, I dont think so.I had a comment regarding the scale of the datasets used in the evaluations. But after having read the competitors work such as MentorMix, I now took it back and agreed with the authors. On the other hand, I agree with the comments raised by the other reviewers on lacking the ablation studies. I appreciated the authors efforts to report back the ablation studies, though only in part, and the results appeared to be convincing to me.So overall, after the discussions and the revision provided by the authors, I am convinced that the paper is above the acceptance threshold. The paper does have presentation issue, and lacks extensive ablation studies. Update:The authors have revised the paper, which helps the presentation somewhat (though headings like "The Grouplike Structure" still come at the reader without much context).The authors added a more application-oriented benchmark, which makes the more convincing case for practical speedup of 210x.Certainly the new "Intuition" section is helpful in explaining the transform.The NT library is an interesting counterpoint on the library front. It feels a little bit apples and oranges to me because of the broad scope of that library (give me DNN, I give you NTK) as opposed to the narrower scope of this one (give me sequence, I give you [log][invert]signature). NT is munging your entire DNN into a GP kernel; signature is an implementation of O(a dozen) ops.I remain somewhat skeptical that the signature transform can enjoy wide applicability given the exponential scaling behavior, unless first and perhaps second order terms suffice for practical use-cases.--- -----**Update:**After the rebuttal, I update my score to 6/10 (see the justifications below). ___________________________After the rebuttal I raised my score. === Post Rebuttal ===I appreciate the careful response provided by the authors, which reminds me of the significance of extending existing theoretical results from invariant to equivariant models. Therefore I have raised my score by 1 point. Update after rebuttal:The author responses and manuscript revisions have addressed most of my concerns. Now I lean towards acceptance.  Rebuttal Edit (Increased score form 4 to 6)Thanks for the discussions. The primary reason for my score increase is discovering that the power of the framework is finding a representation that is quick at exploiting new opponents. I have been sufficiently convinced that this approach is not simply finding a NE (a sticking point in my review). I think that there are a collection of ideas here that are publishable and are of interest to the community.The reason for not giving a higher score is that I think the points the paper made could be clearer: specifically I think the phrase "base policy" could be better replaced by "base representation" / "base model". I was stuck on the idea that the base policy had to be a strong one (eg a NE), and close to exploiter policies in policy space rather than parameter space. Re-reading after the paper update, I am worried that a significant portion of readers may fall into the same trap despite the authors' additional edits. Tightening the story would make this paper more appealing. I also broadly agree with the other reviewers suggestions / concerns.For future work (also mentioned by another reviewer), I think there is no reason the "base policy" could not also be a strong policy too. I believe with minor adjustments to your framework this could be achieved, and one would have a model that both has low exploitability and is fast to adapt to new opponents - a potentially powerful combination. **After rebuttal:**The responses address most of my main concerns, and I have increased the rating from 5 to 6. As discussed during the rebuttal, in the future, having additional experiments that compare between OSG and other appropriate population generation baselines would be helpful. _[Edit: I read the response of the authors. The lower bound for vanilla KD seems good. But the author still fail to adequately explain the gain of Thm 4 (now Thm 3). The requisite upper bound on the critical radius uniformly over all potential functions generated by the teacher means that the result of the theorem still has an implicit dependence on the teacher's complexity. For this reason, I will leave my recommendation the same.]_ After Rebuttal:The authors did a great job in addressing most of the issues I have, and made many changes that helped with the clarity of the paper. There are still some remaining issues like Figure 2 assigning a wrong geometric meaning to the clusters formed by taking means of hyperplanes, and the uncited references, which are simply added to the references section (which should be fixed). But I think the added survey results are a great addition and a persuasive proof about the increased interpretabililty of these models. Therefore I'll increase my score to 6. ---------After rebuttal: Thank you for the response. I have decided to leave my score unchanged. After reading author responses:Thank you to the authors for your detailed responses. With regard to the highlighted implication that "the harder feature can be obscured completely by a spurious one; i.e., there are settings in which the model just won't adopt the harder feature at all" --  to clarify, while my phrasing may not have made this apparent, I was assuming this implication in my interpretation of the results. So my impression of the finding is not changed substantially by the author response. However, I do want to give appropriate acknowledgment of the value of explicitly testing/confirming intuitive explanations of model behaviors, and it is clear that other reviewers find value in the contribution, so I am bumping my score up a bit.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Comments after author rebuttal:Looking at the author's comments (as well as the other reviewer's feedback), I think that the authors have made a good job with the responses and I'm now more convinced about the usefulness of this work. I'm increasing my original recommendation to 6 "Marginally above threshold". --------AFTER DISCUSSION WITH AUTHORS---------Thanks for the clarification. Some of my concerns have been addressed and I have raised my score. But I keep the concern that the proposed defense framework may be easily broken in practice given that the attacker can have unlimited power.  ------------------------------------------------------------------------------------------------------------------------------------Post rebuttal:The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper. ### Updates after the rebuttalI found the approach clean and believe it has some merit among competing approaches. However, as detailed in discussions, I am skeptical of the scalability of the approach. I understand that an approach with scale limitations is acceptable and believe authors can benefit from an honest discussion about tradeoffs--which is currently missing. Further, I would suggest adding non-trivial experiments in the same vein as the ones in $\mathcal{M}-$flow paper. I think with these revisions, the paper can make for a welcome addition to the flows-for-density-on-manifold-literature. [I think the paper provides a useful case study and I appreciate the caveats added to the conclusion, but based on additional discussion with the other reviewers, I think that the paper's more general claims remain unsupported and insufficiently moderated. I am reducing my rating accordingly.] Due to the many relative improvements of FLAG,rather than absolute rankings, on the OGB leaderboards, I think that the overall contribution is above the acceptance threshold.1. Node property predictiona. ogbn-products: +2: DeeperGCN+FLAG, +5: GAT+FLAG, +6: GraphSAGE+FLAGb. ogbn-proteins: +1: DeeperGCN+FLAGc. ogbn-arxiv: +1: GAT+FLAG, +8: GraphSAGE+FLAG, +5: DeeperGCN+FLAG, +4: GCN+FLAG, +1: MLP+FLAGd. ogbn-mag: +1 R-GCN+FLAG2. Graph property predictiona. ogbg-molhiv: +3: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +2: GCN+FLAG, +3: GIN+FLAGb. ogbg-molpcba: +2: DeeperGCN+VN+FLAG, +2: GIN+virtual node+FLAG, +1: GCN+VN+FLAG, +1: GIN+FLAG, +2: GCN+FLAGc. ogbg-ppa: +1: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +3: GCN+virtual node+FLAG, +1: GIN+FLAGd. ogbg-code: +2: GCN+virtual node+FLAG, +4: GIN+virtual node+FLAG, +4: GIN+FLAG, +2: GCN+FLAG .------ Post-Rebuttal Update ------I have read the author's response, thank you for adding more experiments with other graph data augmentation techniques. However, the explanation and justification for the "biased perturbations" still seems a bit weak and under explored in the experiments section, so I've decided to keep my initial score ----- Post Rebuttal Update -----After the author's rebuttal and the discussion with other reviewers, I have decided to lower my score to 6.The reason is that during the review and discussion process it was pointed out that the main contribution of the paper is more incremental than novel, as both multi-hop GATs and diffusion for GNNs have been explored before in similar ways. Additionally, even though the authors provide some theoretical grounding for their work, some of it is a bit disconnected from the rest of the paper, like the relation with PageRank introduced in section 3.2, which is not referenced or discussed in any other section. ----------------------------------------------Update after Rebuttal: I have read the other reviews and authors' responses. While I do think the novelty of the contribution is sufficient, given that  the paper referenced in another review has not been peer-reviewed yet, the new ablation results in Table 1 show that the paper's contribution is not outstanding. I adjusted my score. Updates after discussion/revision period:I think the revisions have improved the paper, but I'm not willing to increase my score or to fight for the paper. Overall, I think the paper represents a minor contribution, with its rigorous experimentation and some of its ideas, and that others may benefit from reading it, but I don't know that it is at the level of a typical ICLR publication. -------- Update: The issue with normalization is fixed in the new version and I am increasing my score. *AFTER REBUTTAL*I would like to thank the authors for their hard work! I increase my score to 6. ################################ After response:Thank you for the clarifications. The response and changes address several of my concerns. While I will keep the score currently, I would consider it slightly higher given the additional information (i.e. ~6.5). Update (Nov 24, 2020):After reading through the author responses and the updated version of the paper, I feel like a sufficient number of my concerns have been addressed to increase my score to 6.  Specifically, the motivation has been made clearer, the related work section is no longer redundant with the intro, and the authors gave an adequate explanation about the necessity of their attention-based alignment method.   =====POST-REBUTTAL COMMENTS======== The authors provided additional experiments on CIFAR-FS and Omniglot. The results show that their methods outperform the baseline method adversarial querying (AQ). It is still not clear whether the methods work in the reinforcement learning setting. As the original MAML paper shows that MAML works for RL problems, it is important to address this question. Otherwise, it could potentially limits the applicability of the proposed methods in the paper.I still have concerns over their novelty and the significance of their contributions. Overall, I applaud their effort to address my comments. I am more positive on the paper than before. My rating is a solid 6. The paper in the current stage does not warrant a higher rating for ICLR in my opinion. EDIT: Raised score from 4 to 6 after the reading authors' response. The response clarifies some of the novelty issues, and it clearly shows the advantage compared to previous methods like TAS. However, I still have concerns about the novelty; the insight why the proposed method is better than TAS is still not very clear to me. I hope the author can further improve the draft for the final version. EDIT: Raised score from 4 to 6 after the authors clarified some points and added additional experiments. Update after rebuttal: I appreciate the authors' response and clarifications. I maintain my original score. Update:Thank you for the answers to my questions and additional experiments.  ---------------------------After rebuttalThank the authors' response to my comments. They also provided new results (i.e., graph generation tasks) to address the Q.4 in my comments. I was about to lower my score when the first reply came as graph generation tasks are just future works in the original paper, but I changed my mind with these results. Although other reviewers may still question the novelty and contributions, I think I would stay with my score according to the comparison and good results reported in the paper. Looking forward to seeing the code someday. **Update to review**I am increasing my score from 5 to 6. I believe this paper is a good paper. However, an extremely extensive discussion with other reviewers has left some questions / concerns. Although I disagree with some of these, I agree with others:- Some claims are overstated in the paper. The authors already changed these claims somewhat in the updated paper. Some reviewers are arguing for further changes. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC).- One of the biggest reason I find this paper is interesting is not mentioned (enough) by the authors. In my opinion, this is a big reason why the work is significant, and if I were writing the paper, I would put it as one of the biggest motivations:    -  There have been works recently looking at the Generalised Gauss-Newton approximation (= EF for classification), and trying to view optimisation algorithms as approximating the Hessian matrix. For example, see Khan et al., 2018 ("vAdam"), Kessler et al., 2020 ("BAdam"), Zhang et al., 2018 ("Noisy Adam"), Osawa et al., 2019 ("VOGN"). Such works provide evidence that different approximations of the EF can work well. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. Should we try and approximate the Fisher matrix in more ways in CL? - Finally, it is my personal opinion (although others disagree) that the current paper is significant enough / provides enough insight already to be a good paper. However, performing a further state-of-the-art experiment or similar would undoubtedly improve the quality significantly.I very much look forward to an updated version of this paper. ------### Comments after the discussionThank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6. From the discussions, the authors make it clear that "compressing" the computation graph is possible without the need for expensive operations (as is the case in typical "lifted" inference literature). The approach does seem to be simple to implement , maybe a bit more detailed analysis and clarity as suggested by others as well could strengthen the paper further. Rebuttal phase:I am updating my score based on the numerous clarifications of experiments and improvements of framing the paper.----- ------Update:In general, I am happy with the authors' responses. They did show the advantage of the introduced self-supervised loss. Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. I am willing to increase my rating. Post rebuttal comments:Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score. **[UPDATE, 30 Nov]: Rating raised after reading the authors rebuttal.** Update ==Thank you for your detailed response. The newly added clarifications and sanity checks have greatly improved the quality of the paper, and I am therefore increasing my score from 4 to 6. I believe the model capacity comparison (Table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper. === Post rebuttalThe authors' response and the revisions to the manuscript have greatly improved the quality and clarity of the paper. Most of my major concerns regarding the implementation and evaluation details have been sufficiently addressed; hence, I decide to increase the score from 5 (Marginally below acceptance threshold) to 6 (Marginally above acceptance threshold). #### Post-rebuttal comments:The authors' comments addressed my concerns on method and experimental details mostly well. I keep with my rating "6: Marginally above acceptance threshold". #####################################################################Thanks for the  response from authors! I will keep my score. ### After response to authorsI thank sincerely the authors for providing a detailed answer to my concerns. I changed my note to 6. I believe the paper is interesting and show strong empirical evidences that the method is worth considering. I am not giving a higher grade because in my opinion the writing of the paper could be signifcantly improved.  ----EDIT:after seeing all reviews, and most importantly thinking about it and pondering the answers given by the authors, I am sorry that I must lower my score. I still like the paper, that could be accepted in my opinion, but I think it oversells some contributions that are unfortunately not exploited (the brownian interval thing in particular, or am I wrong ?)---- ***POST DISCUSSION UPDATE***I am satisfied with the authors' response and will increase my score to an accept.***END OF UPDATE*** =========================================================================================================After rebuttal (and a final question):Thank you for your responses and the experiments I requested. I hope the authors would add discussions with M5 and quantitative results in the final version. Before I finalize my ratings, I have a final question after seeing supplementary material L. I believe M5 is enough to achieve the goal of this paper since $[Z, C]$ captures disentangled factors in $Z$ and $[Z, C]$ improves the reconstruction quality. Then, what is the advantage of MSVAE over M5?=========================================================================================================After the final question : Thank you for your reply. I misunderstood the experiment M2 and propose M5. I believe M5 is just a variant of M2. The authors should add quantitative results (FID and disentanglement score) comparing the baselines (in supplementary material L) in their final version. I would raise my score to 6. Minor : M2-M4 denote mutual information (paper) and baselines (supp). Please use different notations. ================================================================Post Rebuttal:Thanks for the revision and detailed rebuttal. I think the clarity is largely improved now. As the paper has sufficient technical novelty with fairly clear description, I'm slightly inclined towards acceptance. However, I would recommend further clarifying the following point.>We want to point out that NeMo has exactly the same amount of supervision as all other methods- The fact that NeMo jointly reason occlusions in an unsupervised manner was not clearly explained. As the authors described, this property makes difference in terms of robustness to partial occlusions. Clarification on this in the text is very helpful.  (Nov 24) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6. ----------------------------------(Dec 3) Taking into account the other reviews, the authors' responses and the changes made by the authors, as well as the extensive and controversial discussion, I rate the paper still with a score of 6. ==========After rebuttal: The rebuttal resolves most of my concerns. It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training, which seems novel to me. I also realize that the closed form dynamics are not for individual parameters, but in terms of some statistics of the parameters, e.g., the sum of the parameters. This makes the results not as existing as what I thought. That is why I decide to raise my score to 6. Update after the rebuttal: I would like to thank the authors for the detailed reply and for addressing raised issues in the submission. I appreciate the authors' rationale, but the "standard" structure of papers makes it is easier to follow. The same for a conclusion, for the authors it may be reiterating the same ideas, but personally I found conclusions the best place where one can quickly get a flavour what has been done in the paper to assess whether it is actually worth spending time reading it in details. Also, they are helpful in cases like this when a reader (me) is outside of the research field of the paper. Regarding discussions, I appreciate that there is discussion for Theorem 1, but there are theorems and propositions stated in the formal language only which would benefit by being repeated in plain English. If they are just technical results required for the main proof, they may be moved to appendix then.Overall, I am increasing my score to reflect positive changes in the submission. There is a minor mistakes:- In the first line of Conclusion: "obtained aN explicit"========================================================================================================= **After rebuttal**Thanks for the detailed response.This paper can be seen as an interesting attempt to use self-supervised on time series data. Although the basic idea is similar to SimCLR, It is still interesting work considering the  computation complexity and new loss function.So I update my score to 6. ######## Post rebuttal ###########Thank you for your response and the efforts to improve the presentation of Section 2. After going through other reviewers' comments and the authors' responses to those, I am comfortable with my original score.  *** post-response ***Thanks for the response and extensive modifications. I have increased my score to "weak accept" to reflect the improved clarity. I think the paper remains on the borderline, for the following reasons:- I still doubt the practicality of this algorithm in any realistic settings. The experiments demonstrate that one can gain on the variance term from importance sampling in SGD, but not the practicality of the moment-sketching one-pass estimator of multiple gradient queries on practical problem sizes. Thus I disagree with the "vastly superior" characterization; it is well-known that the gradient estimator this paper seeks to efficiently approximate is a good choice; a convincing proof-of-concept is that it's useful to use sketching to efficiently approximate it (whether it's worth the computational overhead + approximation error). The wall-clock time experiment is independently interesting, perhaps showing that you don't even need the sketching ideas from this paper to benefit from importance-sampled stochastic gradients. That said, my evaluation discounts the experimental part and evaluates this as a purely theoretical work.- The theoretical work is an application of [Mahabadi et al. '20], without discernible major theoretical innovations. Though, in my opinion, the application to SGD makes it creative and relevant enough for publication, despite practicality concerns.- Some clarity issues remain, rendering the paper hard to digest: squared-norms in main paper vs. norms in appendix, hidden dependences on p, "follows immediately". After thinking about it, the "implicitly computed" tensor contractions in the comment to R4 makes sense, but this should probably be pointed out clearly in the paper, if I correctly understand that it removes all d^p factors from the analysis. ---### Response to AuthorsI'm satisfied with your responses, especially strengthened experimental results and the clarification of methods in the revision. As my concerns were on doubtful empirical results in the submission---although I thought the approach of the submission was sufficiently novel---I updated my score from 4 to 6 accordingly. I don't know if the authors will keep working on this direction, but I think it will be interesting to see the performance of *off-policy RL* with the proposed method.  ### [Comments on the Revised Paper, Rating and Confidence] The revised version now contains a much clearer description of how the layers are integrated into the algorithm, fixes several typos and reorganizes (and enriches some details of) the numerical experiments following comments of the other reviewers. However, most of my major concerns above still remain (which is expected as the authors didnt get a chance to see my review, and I sincerely apologize for this). 1. For example, although the authors now provide some more detailed explanations about impractical projections in Section 4.4, it is still unclear why one cannot use the projection as a post-processing step instead of a layer, and what the authors are trying to convey in the more detailed discussion about impractical projections with a growing storage of previous policy networks in the revised draft here. 2. Also, the authors may want to clarify some new terminologies and notation introduced in the revision. For example, are contextual policies just policies with state-dependent covariances? And what is the index $t$ in the Adam updates in Algorithm 2, and should $a$ and $s$ also be $a_t$ and $s_t$ here? 3. Another issue I noticed is that compared with the revised draft, the results (in terms of which method is optimal, and whether or not the proposed layers improve over PPO/PAPI) shown in the plots and the tables are not very stable, which indicate that different runs give pretty different results. Such kind of instability may also be relevant to the inconsistency between Table 1 and Figure 2 in the original draft mentioned in the original review above. Overall, I decide to maintain my original rating. **Post-rebuttal comments.** Thanks to the authors for their response. The updated version of the manuscript addressed my main concerns and recommendations. Now, it is clearly improved, figures and metrics updated and the proposed methodology is better presented. Authors even did major changes on the structure of the paper, what I recognize as an important revision. Having said this, I raised my score. ---- UPDATE: Thanks; I have read the response, kept my score, and responded below. .UPD: The authors addressed my concerns and added additional experiments. The paper is improved, therefore, I increase the rating ---- Post-rebuttal ----The additional ablation studies on the attention and MADDPG-p are helpful, and address most of my concerns - though it is still not clear to me whether any of the baselines compared to is exactly equivalent to the method used with attention weights (1, 0, 0, ...). I share the concerns of Reviewer 4 about the significance of the results - this is still not in the paper, and not present for the new experiments. Overall, I have not changed my rating. I raised my scores from 4 to 6 after the author updated their draft and answered my questions. I appreciate their efforts in addressing my concerns and improve the paper. The current version is good enough to be accepted and it also compares with other approaches thoroughly. However, I still feel the symbolic module is too simple in this work and does not distinguish it from other works.  ===Post-rebuttalI'm upgrading my score from 5 to 6, because some of the ablation experiments do make the paper stronger. Having said that, I still think this is a borderline paper. "Co-ordinate conditioning" is an interesting approach, but I think the paper still lacks convincing experiments for its main motivating use case: generating outputs at a resolution that won't fit in memory within a single forward pass. (This motivation wasn't clear in the initial version, but is clearer now).The authors' displayed some high-resolution results during the rebuttal phase, but note that they haven't tuned the hyper-parameter for these (and so the results might not be the best they can be). Moreover, they scale up the sizes of their micro and macro patches so that they're still the same factor below the full image. I think a version of this paper whose main experimental focus is on high-resolution data generation, and especially, from much smaller micro-macro patches, would make a more convincing case. So while the paper is about at the borderline for acceptance, I do think it could be much stronger with a focus on high-resolution image experiments (which is after all, forms its motivation). [Updated after reconsidering other reviews]Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation. After considering the author feedback and their effort to address my concerns, I've decided to raise my rating to 6. Thank you for the hard work. ######### After considering the proposed improvements, I decided to raise my mark to 6. Thanks for the good job done! Update:Score increased. ********* updated review *************Based on the issues raised from other reviewers and rebuttal from authors, I started to share some of the concerns on applicability of Thm 1 in obtaining information on low k Fourier coefficients. Although I empathize author's choice to mainly analyze synthetic data, I think it is critical to show the decays for moderately large k in realistic datasets. It will convince other reviewers of significance of main result of the paper. ==========================I change my rating on this paper to be 6, after the authors' response. Update after response:The authors have provided improvements to the introduction of the problem setting, satisfying most of my complaints from before. I am raising my score accordingly, since the paper does present some novel results. The latest revision is a substantially improved version of the paper. The comment about generalization still feels unsatisfying ("our model requires choosing c* in the support of P(c) seen during training") but could spur follow-up work attempting a precise characterization.I remain wary of using a neural net reward function in the simulated environment, and prefer a direct simulation of Eqn5. With a non-transparent metric, it is much harder to diagnose whether the observed improvement in List-CVAE indeed corresponds to improved user engagement; or whether the slate recommender has gamed the neural reward function. Transparent metrics (that encourage non-greedy scoring) also have user studies showing they correlate with user engagement in some scenarios.In summary, I think the paper is borderline leaning towards accept -- there is a novel idea here for framing slate recommendation problems in an auto-encoder framework that can spur some follow-up works. Thanks for addressing my comments. The social pooling mechanism improves Indep-RNN as expected, however, as you show, it's not better than your method. This makes the results stronger. Additionally, the plotted trajectories shine light on the behavior of the trajectories. The trajectories are still better than the baselines after this additional information. Given the authors' response, I have increased my score. It will be nice to see this work take a semi-supervised or unsupervised route in the future :) ========Thank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score. I appreciate the author response and additional effort to provide comparison with MoNet. I have raised my rating by 1 point. It should be noted that the edits to the revision are quite substantial and more in line of a journal revision. My understanding is that only moderate changes to the initial submission are acceptable.----------------------------------------------- ### Review update following author discussionI've read the author responses as well as some of the discussion with the other reviewers. Overall, this is valuable work and I've considered raising my score, but I think a weak accept is appropriate, all things considered.  I've raised the confidence score for my review, as I understand the technical details better now. I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN.  However, the significance of the paper in terms of the novelty of the ideas, both conceptual and technical are overstated in my opinion, hence the weak accept. One other point of feedback for the final version in case of an accept:I also share R3's concern about the paper's positioning in relation to the extremely general dice enterprise framework (or, even the bernoulli factory, for that matter) as somewhat misleading for the particular use case in Section 2.2.  The exact same multinomial sampling scheme is in fact more succinctly presented (proposed?) in BRPF [Schmon et. al, 2019] as the "Bernoulli Race" (which the authors have cited).  I would think that the current scheme is a special case of the "Bernoulli race" that uses a particular form of the acceptance probability parameterization similar to prior work (e.g. VRS [Grover et. al. 2018]). See Section 3, specifically e.g. Eq (10) and Proposition 2 from BRPF [Schmon et. al., 2019], which can be compared to Eq (3) and Proposition 1 in the current submission, respectively. Minor nit: In step 3 of the algorithm (right below Eq (8)), you use the notation $z_t$ for sampling a new variable from $q_\phi$. However, this $z_t$ has nothing to do with the latent variables that are used in computing the constants $c^i_t$. The way it's written makes it appear as if there's a circular depdendence of the $c_t$ on $z_t$ and then the $z_t$ is again resampled, which changes the $c_t$. For this reason, it maybe better to use a completely unrelated variable for the sample from $q_\phi$ in step 3.  ===== Post Rebuttal =====The reviewer is satisfied by the authors' response. I am fine with raising my score. In the final paper, it would be nice if the authors can include a detailed discussion about the dependence of $m$ in their results.  Since the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers.  I have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation.  -----------Revision-----------In light of the discussion with the authors, the revision made to chapter 4 and in particular the proposed modifications to section 2.4 for a camera-ready paper, I revise my score to 6. Update post rebuttal-----------------------------The experimental setting that is a little lacking. Qualitatively and quantitatively, the improvements seem marginal, with no significant improvement shown. I would have liked a better study of the tradeoff between visual quality and diversity, if necessary at all.However, the authors addressed well the issues. Overall, the idea is interesting and simple and, while the paper could be improved with some more work, it would benefit the ICLR readership in its current form, so I would recommend it as a poster -- I am increasing my score to that effect. UPDATE (after author response):Thank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.There's one point that the reviewers didn't clearly address:  "It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions." Please consider adding such an experiment.The current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that "Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd." Showing that the method outperforms other methods would definitely strengthen the paper.Section 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5. Post rebuttal comment: Having read the reviews from other reviewers who are subject matter experts, and the authors rebuttal which helped clarify most of my concerns, I am increasing my rating for the paper.  I recommend acceptance as two of the reviewers are convinced about the positive impact of the paper.  **Post rebuttal**With consideration of the improved readability of the new submission and comments of other reviewers, I have modified both my initial rating and confidence. [Post-rebuttal] I've read the rebuttal, which answers to many points I raised. I've reflected it in my score. [Update post-rebuttal: I thank the authors for addressing some of the concerns raised by the reviewers. My stance remains, also given the outcome of e.g. the 10% experiment -- verification of the main claims remains difficult. My score already reflects that I'd be happy to see the submission accepted.] I have read the feedback and discussed with the authors on my concerns for a few rounds. The revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments.I have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. I have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score "6" reflects my opinion that the approach is not principled.========================================================= [UPDATE]I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6.------------------------------------------------------------------------------------------------------------------------------------------------------------------------ The revision motivations are much clearer and to-the-point, and the inclusions of Figures 2 and 4 are very helpful for understanding where this method lies w.r.t. VIB.My primary concerns in my original review were the framing of this work as being "finding good representations", but it seems like this is really about finding good representations relevant to some other known variable (such as the labels). Since you appear to be making some broader claims w.r.t. representations, this opens you up to comparisons to methods that are arguably more general in that they operate primarily as unsupervised methods, including the self-supervision and data-augmentation-driven methods I mentioned. You are correct that they do not address MNI as directly as in your work, as each of these could encode information irrelevant for predicting some other known variable. However, as "representation learning" tools, they are far more powerful, as demonstrated in their ability to work on high-dimensional datasets.As a study of how to learn MI models between two known variables, X and Y, this work has a lot of value, but I would make the setting a bit more clearer in the beginning.I do wish that there were more datasets here, as a study with CelebA attributes or CUB captions / attributes would be very convincing.Your concerns about MINE are duly noted, but MINE comes with the strong advantage of not needing to specify the posterior (for instance, a noise-injected nn will work). The additional network needed is just another encoder similar to the one used in your e(z_x | x). In the IB setting, this works precisely as with GANs, except in this case the encoder tries to (adversarially) make the joint distribution resemble the product of marginals. Besides, MINE is demonstrated to work better than your baseline, so shouldn't that be of note?Anyways, as the revision is a bit better, I'll increase my score to a 6, but I need to read more thoroughly the other reviewers' concerns before I move any further.One thought on making this fully unsupervised, and possibly a stronger tool for learning representations: what about sampling X and Y from a random mask (i.e., a crop) and the corresponding negative mask on the image? Enforcing MNI would result in a latent representation which contains the information that is shared between the positive and negative masked areas, which is closer to these self-supervision methods I mentioned. [UPDATE] I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ ----Updated after author feedback----Upon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.I still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold. [UPDATE] I appreciate the authors' detailed response and revisions to the paper. I've updated my score accordingly.  Updated Thoughts- The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am inclined towards increasing my rating.- In addition, I have mentioned some updates in the comments which might make the paper stronger -- centered around clarifications regarding the computation of the top-k info-gain term. Update: I have read author's response (sorry for being super late). The response better indicates and brings out the contributions made in the paper, and in my opinion is a strong application paper. But as before, and in agreement with R1 I still do not see technical novelty in the paper. For an application driven conference, I think this paper will make a great contribution and will have a large impact. I am slightly unsure as to what the impact will be at ICLR. I leave this judgement call to the AC. I won't fight on the paper in either direction. Authors have addressed most of my issues and hence I have revised my decision. After reading the revision: I have raised my score by 1 point and recommend acceptance. After reading the authors' response and their additional experiments, I still see this work as a very decent paper, but not a very exciting one. This is why I rank this paper somewhat above the acceptance threshold. I could be proven wrong if this approach becomes the method of choice to prune networks, but I would need to see a lot more comparisons to be convinced.   After rebuttal:I read the authors comments and I understood the technical contribution more and raised my score.  Implementing/appriximating the response oracle is non-trivial. For MWU, I still think that the above paper should be cited (citing the Adaboost paper is not enough) since the paper shows MWU solves the min-max game. ----------------------------------------------------------The authors have addressed my comments and as a result I changed my rating to 6. ************* Revision *************I am convinced by the rebuttal of the authors, hence have modified my score accordingly. === After Authors response ===The authors fixed some major issues. That is why I improved my grade. However I'm still concerned about the scalability of this algorithm ===== after updates =====Thanks for the edits  I believe the overall paper is more clearly presented, now.I still think it is a stretch to consider the calculator domain is a program induction problem: it is a regression problem, from an input string to an output integer, or alternately a classification problem, since it computes the result mod 10. The only way I could understand this as a program induction problem is rather obliquely, if the meaning is that any system which is able to compute the result of the calculator evaluation has implicitly replicated internally, in some capacity, the sequence of instructions which are evaluated. I don't think this is really very clear though; for example, given two calculator programs, one a subprogram of another (e.g., "4*(3+2)" and "3+2"), do the resulting "induced" computations share the same compositional structure? The examples of program induction in section 2 are largely architectures which are explicitly designed to have properties which mimic conventional programming languages (e.g. extra data structures as memories, compositionality, &). In contrast, the calculator example in this paper simply uses an LSTM. That said, I think it's still a great example! Learning a fast differentiable model which accurately mimics existing non-differentiable model has tons of applications, and has exactly the same challenges regarding synthetic data. I have to say I find the new section 8.3 a bit intuitively challenging; e.g. it's not clear really how long a waiting time of 48 log(2|X|/\delta) / (p|X|^2 z^2) really is. But, to that end, I appreciate the empirical discussion in 8.48.6.I've updated my review to increase my score  I lean towards accepting this paper, as it is a timely contribution and I think it is important for future program synthesis papers to take the results here to heart. I've reduced my confidence slightly, as I have not fully reviewed the new proof in 8.3. After revision ====The authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.  ####### . After rebuttalThe author makes more clear indication of the performance contribution of the completeness of recovery. Update After rebuttal:Given the authors' rebuttal to all reviews, I am upgrading my score to a 6. I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach. After reading the rebuttals, I appreciate the authors addressed most of the my concerns above. Thus I have adjusted my scores to reflect that. I think the work contains some theoretical contributions as opposed to [1], which is used to prove convergence for many actor-critic algorithms, under certain regular assumptions. It would be great to see this result working on more general conditions. Please address the above concerns in the final version, and also stress the theoretical contributions as compared to existing results.  **********I would like to thank authors for their feedback. After reading their feedback I still believe that novelty is incremental and would like to keep my score.  ================================I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. [Second Update] I still find the method proposed in this paper appealing, and think that it may have practical applications in addition to providing significant research contributions. A key question that was raised by the other two reviewers was whether the proposed approach was fairly evaluated against existing state-of-the-art solvers. The authors have responded to these concerns by adding clarifications and new baselines to their paper. However, based on the discussions to date, I feel that I am not sufficiently familiar with related work on SAT solvers to say whether the other reviewers' concerns have been fully addressed. If they have been, I'd strongly lean towards accepting the paper. As for the concerns from my original review: the transferability experiments reported in the author comments below are quite informative, and I'd encourage the authors to incorporate them into the paper (or an appendix if space is an issue). I'd also encourage the author to incorporate the full comparisons against Z3, PicoSAT, MiniSAT, Glucose, Dimetheus, and CaDiCaL from Section 5.1. (I've updated my rating for the paper from 5 to 6, and my confidence score from 3 to 2.)[First Update] Based on the feedback of the other two reviewers, I believe that I was missing some important context about SAT solvers when I wrote my initial review. Reviewer 1 and Reviewer 2 both raised serious concerns about the types of SAT instances that were used to evaluate the experimental setup, as well as about the use of Z3 as a baseline for solving random SAT instances. (No author response was provided.) Given this additional information, I've lowered my score for the paper from an 8 to a 5. I do think that the approach is interesting, but have reservations about the experimental evaluation and the claims made by the current submission. (Note: As the paper authors point out in the comment below, this update was mistakenly submitted a few days before the end of the rebuttal period.) ################################################################Summary:This paper provided an efficient algorithm (LLBoost) to boost the validation accuracy without spending too much time tuning hyperparameter. The algorithm is theoretically and empirically guaranteed.################################################################Reason for Score:This paper provides an innovative way to improve generalization performance. My major concern is about experiment part. Since the algorithm use valid data to tune the parameter, it should another held-out test data to show the result. However, the author only did experiment on test-data for model ResNet-18, which is not sufficient to support the paper.################################################################pros:1, This paper gave an efficient LLBoost algorithm to quickly improve the validation accuracy. The algorithm is theoretically guaranteed.2, The paper clearly stated the intuition of the algorithm. The paper considered models that have fc layer as the last layer (most of the current models have this property), and transformed the problem into a linear regression problem.3, A surprising point of the algorithm is that it does not impact the training loss.################################################################cons:1, While the algorithm has a theoretically guarantee, the experiment part did not convince me. This is the major concern for the paper. The author tune the parameter using valid data and say valid accuracy is improved, which is not enough. It should another held-out test data to show the result for all the experiment. However, the author did an experiment on test-data only for ResNet-18, which is not sufficient to support the paper. Also, The author should put this test-data-ResNet-18 experiment in main part of the paper, not Appendix.2, Section 3 (preliminaries and method) is not well-organized. I cannot see why the author put this two lemmas here.(1) why "Lemma 1 implies that LLBoost does not affect training predictions since it only ever adds a component orthogonal to the span of the training feature matrix"? The lemma 1 seems having nothing to do with the LLBoost algorithm(2) what is the purpose for lemma 2? The paper doesn't clearly state it.I understand the reason after the author explained it in response. But I strongly suggest the author to explain it in paper for the final version.3, Based on my understanding of this paper, the algorithm has to be applied to an existing pretrained model which is sufficient good. If we don't have a good pretrained model, does this algorithm provide a better (or comparable) result than the well-tuned model? I am just curious about it and hope the author to do some experiments in the future. -- Post-rebuttal -----------------------------------------------------------------------------------------------Given the improvement of the last revision, I increase my rating to 6. The revised version has been very much improved, especially in the abstract and introduction Sections. Still I think it's important to additionally have one or two sentences to make very clear on the meaning of calibration, as to not confuse readers. -----------------------------------------------------------------------------------------------------------------------------------------------------------------POST REBUTTAL-----------------------------------------------------------------------------------------------------------------------------------------------------------------The rebuttal has addressed most of my concerns and I am happy to increase the score.----------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------Post rebuttalI thank the authors for providing detailed answers to my concerns. Considering the concerns of the other reviewers and the authors' answers and additional experiments, I think that the paper provides a sufficient contribution to an important research topic. Therefore, I retain my initial rating. Additional reviews ====================== The authors have resolved some concerns, especially the explanation/justification of the experimental results in section 5. As I have commented in the initial review, this paper provides suggestive experimental results (the effects of training anomaly dataset) for future anomaly detection research. Therefore, I have decided to raise my rating from 5 to 6. Update: Thanks for the response from the authors. The comments 3/4/5 from authors convincingly address my concerns. Regarding other classifier-based metrics, note that not all of them requires separate training and testing procedure. For example, the leave-one-out 1-NN accuracy [1] does not. Also, I'm still not sure about the technical novelty. Therefore, I keep my original rating.[1] Xu, Qiantong, et al. "An empirical study on evaluation metrics of generative adversarial networks." arXiv preprint arXiv:1806.07755 (2018).--------- Update======I have read the author response, which largely does not change my score. I would kindly point out to the authors that the "the technical challenge is that the feasible region is not an ball, and computing the projection is challenging in high-dimensional settings" is in fact not very challenging at all. As mentioned in my initial review, the "split and merge" is exactly the standard projection operator on the proposed set and is not a "new heuristic", and so it is exactly PGD and not an adaptation of PGD. I also understand the other reviewers concerns on suspiciousness, which we all brought up. This likely needs to be thought about and presented more carefully, for example by posing it more formally if the authors insist on this framing.  UPDATE:After reading the author's updated paper and comments I have decided to improve my rating to a 6 since some of my concerns have been eased. The most important concern being eased was the type of bias was too constrained at first and now there are experiments with a more unconstrained version of bias that is more convincing.Overall, I would say that future versions of the paper could look into a task and dataset that are close to their domain of applicability and where they can contribute an increase in performance. That would strengthen the case of this paper. I think a rating of 6 is fair for this version of the paper and I thank the authors for their efforts in updating the work and addressing my concerns directly and efficiently. ============= Post-rebuttal:I thank the authors for their careful responses. After discussing with other reviewers:- I agree that the sparsification method proposed here is also in principle applicable to GCN-like models;- The authors should have provided results for "FastGAT"-style sparsification on GCN, rather than countering the reviewers using passages like "Hence, it is mainly the question of necessity rather than applicability which guided our choice of studying the GAT model in depth."- If such a GCN model ends up competitive, the focus of the paper could switch to the sparsification method itself rather than the GNN model it is applied to.In light of these discussions, I am decreasing my score to a weak accept, and I hope the authors will take this advice for the next iteration of their work (which otherwise, in my opinion, deserves being published in a strong venue). === Post-discussion update ===I thank the authors for engaging in the discussion.I am left somewhat divided on the paper. During the discussion phase, the statement of the main algorithm in the paper changed quite significantly: in the original version each node could attend only to its neighbors in the sparsifier, while in the current version each node attends to all its neighbors (i.e., full graph attention is computed), and the sparsifier is only used in the subsequent feature update. The authors eventually explained that their analysis holds for the latter algorithm (even though its running time is not faster than non-sparsified GAT), while the former algorithm is what they actually implement since it has better running time (albeit no formal guarantees).I don't take issue with the divide between the theory and the implementation (as long as it is made clear in the paper). I do think, however, that perhaps the theoretical content ended up doing the paper more harm than good. Effective resistances measure the "importance" of edges to the connectivity of the graph - this is a general phenomenon, and the sparsification algorithm of Spielman-Srivastava is just one (beautiful and useful) manifestation of it. I can see why the practical algorithm would work even if it cannot be explained formally via spectral sparsification, and including the slow unimplemented algorithm just for the sake of its analysis feels a bit forced. What does it add to our understanding, and was it worth making the paper that much more confusing? Ultimately it's the author's choice, but even now the way the writeup deals with the two algorithms is still "evolving", and it is not clear how a final version would look. (I don't think the current form makes sense, since the algorithm now titled "FastGAT" is not faster than GAT, and section 3.1 zigzags between the two algorithms somewhat awkwardly.)Nevertheless, in the end the authors were straightforward about all this in the discussion. As I said originally, I like the overall approach, so as long as the clarifications about the gap between the analysis and the implementation are included, and pending other reviewers' concerns about novelty and experimental validation (I am less versed in the empirical literature on GNNs so prefer to defer to them on those points), I think the paper could still be accepted. This paper studies ways of adding edges to graphs to improve the result of spectral embedding / clustering. It refines existing embeddings using by measuring edges' effect on Laplacian eigenvalues, and adjusting such edges to reduce the distortions. The performance of the algorithm is justified using developments of worst-case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy.Strengths:+ the paper puts together a lot of different ideas, many of which have solid theoretical foundations.+ full experimental evaluation on a moderate sized data set that demonstrates both good results and good performance.Weaknesses- the presentation could use significant improvement- the ideas are a bit disconnected, at least when one try to follow the ideas mathematically.I find the approach taken by this paper quite interesting: node embeddings are now widely used to preprocess graphs into vector data more friendly to learning pipelines. Many of these methods add additional edges imperatively / via local methods, without taking the overall data set into account. Iterating this process based on the overall embedding is an interesting, but algorithmically more intensive approach. To carry this out, the authors combined a variety of interesting tools, as well as some high performance packages that they developed. I feel the novelty of this combination, as well as the gains obtained, should make this result of broad interest to those working on spectral clustering/embeddings. However, many of the other reviews point out gaps in both the ideas and presentation, and I'm inclined to agree with them that this paper can benefit from a thorough revision before appearing at a major conference. I have read over the rebuttal and discussion and will keep my evaluation score unchanged as I see value in benchmarking papers such as this for the community. ==== Updates after the response ====I thank the authors for answering my questions and the updated manuscript. Im keeping my score and recommendation. Though I like the paper, and I believe it has good results, I am now seriously concerned about the quality of the numbers in Tables 1 and 2. The origin of many of the results in these tables is in doubt. The authors say that the results for TransE, RotatE, ConvE, ComplEx , DistMult, and Minerva are taken from the corresponding papers. But when I look at these papers, I see different (or no) numbers.The values for Minerva in the current paper match for FB15k-237, but do not match for WN18RR. The values in the Minerva paper for UMLS or Kinship are way better than in the current paper. Now Minerva uses a different evaluation protocol, but I seriously doubt the Minerva numbers for UMLS and Kinship.RotatE has no numbers for UMLS or Kinship.ComplEx has none of the numbers reported here.Distmult has none of the numbers reported here.The numbers in the current paper for TransE, DistMult, RotatE, ConvE and ComplEx seem to be taken from the RotatE paper. But RotatE has no numbers for UMLS or Kinship.So overall, the provenance of the numbers in Table 1 and 2 is in serious doubt. I realize that the authors do not have a chance to respond and modify the paper. I hope the PC members can weigh in on what can be done at this stage. Even though I like this paper, my score will go down based on the poor quality of Tables 1 and 2. Updated review: I appreciate the authors' response and have updated my rating. However, I still believe that the clarity of the exposition could be improved.  Update after rebuttal: I appreciate the authors response and have updated my score. Please see some lingering thoughts below:1. It looks like the experiments show that just the NN based Gaussianization of latent space is not effective when the noise distribution is significantly different from Gaussian. The authors had to  introduce an additional VST in the latent space to make it work. I think this is a limitation of the work - the method just by itself is not capable of handle of noise type significantly different from Gaussian. In my opinion this should be acknowledged in the paper. 2.  I think it is unfair to use a pretrained HQS model for comparison. The authors should retrain the model on the particular datasets they are interested in.   **Update**I appreciate the effort by the authors to clarify some of the issues, most of which are addressed in the rebuttal, so I will raise my score to 6.I still feel like the $I(w, y)$ part needs to be dealt with a bit carefully, especially there is a invertible mapping between the two on the generative side. The simple graphical model seems like $w \leftarrow x \rightarrow y$, where left is encoder and right is data generation procedure.  -- After rebuttalI've read the authors' feedbacks and other reviewers' comments. My major concern was the clarity of the manuscript as other reviewers mentioned, and I believe the concern has been resolved during the rebuttal period. I adjusted my ratings representing that. *********The authors partially addressed my concerns. Therefore I raise my score to 6. Update: after reading the feedback and discussing with the other reviewers, I decide to keep my score unchanged. --------------------------------------------UPDATE AFTER AUTHORS RESPONSE: The authors partially addressed my concerns and I (slightly) increased my score.  **================================== Update after rebuttal ==================================**I appreciate the reviewers' hard work for adding this many new materials in a short period of time. A large number of my concerns have been addressed and the quality of paper has improved significantly. Some newly added experiments give a lot more insights than the original draft. As such, I am in favor of accepting the paper and have increased my scores from 4 to 6.However, I still have a few lingering questions in the light of the rebuttal and would love to see them addressed should the paper be accepted:1. Re: figure 4, the question is why the performance of the green curve is so bad. I was wondering if some of the labels are wrongly labeled to induce this effect.2. For reducing the feature map to a single quantity, why is the max operation chosen? Intuitively, it makes more sense to use mean or even order statisitc to make the metric more robust.I also understand that many of the experiments cannot be added in a short period of time and hope that the authors add them as promised in the rebuttal. ------------ Post Rebuttal ------------------I read other reviewers' comments and the authors' responses. I like the idea of applying the GAN framework to compute counterfactual distributions. However, I could also see why other reviewers are not particularly excited about it. The authors managed to apply the GAN approach to obtain counterfactual samples in some specific datasets. However, many questions regarding the proposed methods are left unanswered, e.g., under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples. With this being said, I think this paper could be most improved by further elaborating how it contributes to the existing causal inference literature, especially in computing counterfactual probabilities. Due to these reasons, I intend to keep my score but won't strongly champion for it. ## Post-rebuttal responseI have read the other reviews and the authors' extremely thorough responses  much appreciated! See the thread below for some brief responses to the rebuttal sections in turn.I regret posing far too high a standard in my original review. The authors' rebuttals have helped to quiet my doubts a bit, and better understand the utility of this paper as a product for cognitive science. I have accordingly revised my judgment quite a bit upward. **Post-Rebuttal**After reading the rebuttal and other reviewers' comments, I am actually on the fence for this submission. On the one hand, it provides several interesting observations about the phase transition in long-tailed recognition, which would be valuable to the community. On the other hand, its experimental evaluation needs to be strengthened. The authors are encouraged to include more many-shot/medium-shot/few-shot analysis across the dual phases.Therefore, I upgrade my score to 6 (marginally above acceptance threshold). POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft, in particular with respect to :the new experiment (complementing the original one on blobs) whose results are reassuringcorrections and improvements in the proofsthe new experiment on climate changeThe two-sample test part has been modified to take into account some of my comments but still lacks rigor. It doesn't explain the permutation procedure used and whether the p-values are just valid or exact. Now, the term "significance level" (replacing "p-value") is used to refer to the probability of type I error, which is not totally accurate in general. If the p-values are just valid and not exact, the significance level is a bound on the type I error. This should be fixed in the final version.Regarding my main concern, i.e. showing the interest of a decision-theoretic divergence using some custom loss, the new experiment on climate change is a good illustration of the interest of such divergences.In summary, I see useful theoretical results and a step in this not-much-explored direction of tailoring divergences and two-sample tests with a decision-theoretic perspective, i.e. "detecting differences that matter", and thus I decided to raise my score. ### Update after response: The authors have quite thoroughly addressed most of my concerns with updates and new experiments. So I will increase my score to an accept at this point. ----- Post Discussion ----Updates to the paper have helped make technical parts of the paper more clear. The paper has also been edited to improve the motivation and experimental explanation. ##########################################################################Post RebuttalThe paper has been updated to include additional reviews about nested optimization. I would like to keep my original score. The authors have made clear some of my concerns and made revisions accordingly. Thus I am in a position now to recommend this paper, thus I update my initial recommendation from 5 to 6. --- After reading the authors' response and other reviews. I still believe the paper has made a good contribution thus I would stick with my original rating. Post-rebuttal comment:I acknowledge having read the authors' response and I have also glanced over the updated version of the paper. Updated: The authors added the requested timing complexity data and additional experiments with better baselines to compare the proposed RC algorithm against. They present valid issues with reproducing some of the previous work. While a theoretical proof establishing the worst-case time complexity of RC to be better than random sampling plus Seq A* would be ideal. The empirical data presented does support the claim that RC algorithm is useful for finding more optimal solutions for large maps faster than Seq A* plus random sampling. Response to rebuttal: the authors have drastically improved the quality of the submission with the new experiments and clarifications, I have therefore increased the score to a weak accept.------------------------------------------- I have read authors' feedback and will keep my original score. --Post-rebuttal edit: I read the authors' reply and thank them for the clarifications. I maintain my score of 6. The authors' rebuttal has addressed some of my confusion regarding the paper, which is greatly appreciated. The additional baseline of early termination would still be interesting to have, though I agree it's not critical for the presented line of work. In general, I think the work is interesting and will keep my current score (6).================================= ------------------------------------UPDATE:After reading the author's response, the reviewer's comments, and the revised version, I have increased my score. See my response to the authors for more comments.    =================Score raised to 6 after inclusion of MA + SA results in rebuttal. UPDATE after rebuttal ==I am willing to update my recommendation to weak accept after the rebuttal. Some of my main concerns remain, but added experiments, comparisons and discussions alleviate some of those. I still believe that there is too much focus on fairly useless metrics such as number of parameters which might lead to future work that will follow this trend, but the results are overall strong enough to warrant acceptance. UPD: the score was updated after the rebuttal stage. Updated review:I thank the author for their new experiments during the discussion period. Given the superior performance of GNN over MLP, I am more convinced that the usage of GNNs in this application is justified. I have updated my review rating from 4 -> 6 to reflect this.But just to harass the authors a bit more, I have this curious question:- Is the worse performance of MLP due to generalization or expressive power? In other words, can the MLP fit the training data well? Also, when comparing MLP and GOREN, are the authors controlling the number of parameters when comparing MLP and GOREN? As the authors mentioned in their reply, which I agree, "MLP generally works better than LR due to model capacity". We want to make sure that MLP and GOREN have similar model capacity but GOREN captures better inductive biases.I believe this submission finds an interesting application for GNNs. I encourage the authors to bring out the full potential of this idea by having solid, rigorous empirical studies.  After rebuttal:I appreciate authors' detailed responses and an updated version of the paper. The new version is a lot clearer. After reading other reviews, I agree that the algorithmic novelty is limited, but the model is well-adapted for multi-horizon forecasting problem. Overall, I increase my score to 6. marginally above acceptance threshold. ---------------------------------------------------------- Update:The paper has considerably improved:the title is now accurately describing the papercomplex scenes have been added (and the method works there too)comparison with knn has been added and it shows a clear improvement for the proposed methodI think the paper has improved, but I still find the comparison with direct solvers problematic. For small scenes like the one shown in this paper, a direct solver is fine, there is no need to use an iterative one. If the scenes are large enough to require an iterative solver (i.e. a direct one runs out of memory) then it should be shown that the proposed methods provide benefits in that specific setting. It could be that my bar for comparison is too high, as I usually publish in a different community where quantitative improvement against the closest baseline is always required.Overall, I am still mildly positive, but not willing to champion this paper given the many issues raised in the reviews. POST REBUTTALThe updated results make sense. The limitation/assumption mentioned in weaknesses 1 must be sufficiently disclosed in section 3.2.  ======= updated ======The authors response partially addressed my concerns and I would raise the rating to 6. ## Update The new sparse tasks and comparison to Dreamer + Curious improve the paper and address some of my concerns. Specifically, a sizable improvement due to exploration is now seen on 3 tasks, Hopper, Cartpole Sparse, and Cheetah Sparse. The new maze task is also more challenging than the bug trap task.  --- Final Decision ---After the significant improvements in the experimental evaluation, I believe the paper provides a reasonable case for the proposed latent UCB method. It also provides an interesting discussion on the advantages of UCB-style methods, and an interesting observation that optimistic reward-based exploration can be effectively used even in absence of (positive) rewards. Even though the experimental evaluation of prior work on exploration is still rather lacking, I believe that these contributions are enough for the paper to be interesting to the ICLR audience. I raise my score to 6.--- Remaining weaknesses ---The experimental evaluation in the paper is still quite lacking in terms of baselines, making it impossible to judge whether the paper actually works better than prior work.First, the proposed method contains two improvements, model ensembling, and optimistic exploration, but doesn't go much in-depth analyzing either of these improvements, instead trying to focus on both at the same time. This makes comparison to prior exploration methods hard because the proposed method receives an additional boost due to an ensemble of dynamics (the paper conveniently quantifies this boost in the LVE method, and it is shown to be rather large). For a more fair comparison, the ensemble of dynamics might be ablated (leaving only the ensemble of value functions), or the competing baselines could also be built on top of LVE.Second, the paper only compares against one competing exploration method, Dreamer + Curious. There has been a large amount of proposed exploration methods, and it would be appropriate to evaluate the proposed UCB method against at least a few of them. For instance, the paper could compare against similar value function ensemble techniques (Osband'16, Lowrey'18, Seyde'20), or other cited work (Ostrovski'17, Pathak'17). Burda'18 is not cited, but perhaps should be compared against. All these methods can be relatively easily implemented on top of LVE for a fair comparison.Burda'18, Exploration by random network distillation.--- Additional comments ---Would be great to clarify what is the observation space for the bug trap and maze tasks. For instance, you could add observations and predictions for these tasks to the appendix. I am happy that the authors improved the paper with reviewer feedback. In particular I think the new ablations and comparison and mention of previous work makes the work more complete. The results on new sparse tasks are also interesting. I still think more can be done in terms of experimental validation (in particular my original note regarding early cutting of the curves has not been addressed). However overall I think the paper does meet the acceptance threshold as things stand. ---- update after authors' response ----\Thanks for clarification and providing additional experiments. I'm changing my final evaluation to weak accept. Yes, this paper does provide some interesting insights, but I still think that it has a limited potential impact (see above for major drawbacks). # Updates after rebuttal1. The authors have provided more results I concerned, which seems to be accord with their conclusions in the paper.2. Initialization of CNN or other networks is an interesting topic which affects the performance of pruned models. However, there are few papers about the topic. I think the paper is a good example which may arouse more concerns about it.3. I am willing to increase my rating to 6.### -----------------------After Rebuttal============My main concern was that it is already known that BatchNorm has a great expressive power, and thus the authors should have gone broader and deeper in their search for valuable insights to explain what makes BatchNorm so special. I proposed different ways to do that, such as comparing with other parametric transformations like squeeze-and-excitation, or trying to understand what is the role of normalization. My concerns were shared with Reviewer 4, who proposed an interesting experiment that involves comparing multiple normalization functions such as LayerNorm.The authors have agreed with some of these points, and updated the text to reflect the discussion. However, they believe all these suggestions belong to future work and, although the form of the paper has changed, the content is still the same.Overall, I think this work is interesting and I think studies like this are necessary (although more in depth). Thus,  I have raised my score to borderline.  ***********************After rebuttal: I maintain my rating and think this paper is on the borderline. I think the paper is interesting, but the novelty and significance is limited by previous works along the direction. **POST REBUTTAL**I read through the other reviews and the author rebuttal. I thank the authors for addressing all of my concerns in the rebuttal. Overall, this is an interesting contribution in the multimodal VAE space and would make for a good poster at the conference, and I am happy to keep my original rating for the paper. In terms of energy based models here is one recent work which comes to mind which might be useful to extend to multimodal settings: Du, Yilun, Shuang Li, and Igor Mordatch. 2020. Compositional Visual Generation and Inference with Energy Based Models. arXiv [cs.CV], April. https://arxiv.org/abs/2004.06030.(note the quality of generations in Fig. 5) ##########Update##########I'm pleased with many of the changes the authors have made in response to the reviewers' concerns. However, I'll mention a couple lingering concerns.- The authors have updated their paper regarding use-cases for layer-wise explanations, but it remains unclear whether this feature is impactful. While it is possible to perform instance-level pruning with ShapNets, is there any reason to do so? Finding features with low SHAP values requires actually calculating the SHAP values, which takes longer than just evaluating the function. Therefore, it seems that this technique would not lead to saving either time or memory. If the authors disagree, they may consider improving this aspect of the paper.- The authors have clarified that they use the "reference values" approach to holding features primarily because it is convenient. To my knowledge, no existing research actually advocates for this approach. Most research advocates for either the "interventional" [2] or "observational conditional" approach [1]. The authors cite a paper that mentions the "reference values" idea (Baseline Shapley) [3], but not even this work truly advocates for this approach. In my view, this is a rather severe limitation of the proposed approach, and the authors did not suggest that they see a way to overcome it. To call these "SHAP values" is almost misleading because it silently changes one of the core aspects of SHAP. As an example of the consequences of this limitation, any feature that is equal to its reference value will have a SHAP value equal to zero, but it is easy to image how such features can be informative (e.g., black regions in an MNIST digit, such as missing arcs on the left-hand side of a "3" that distinguish it from an "8").While it is very helpful to calculate SHAP values faster, this is a flawed version of SHAP that is not supported by existing research that considers the question of how to model missing features. Unfortunately, the proposed approach apparently lacks the flexibility to work with different notions of missingness (e.g., the interventional or observational approach). For that reason, I'm lowering my score by one point (6).[1] Frye et al., "Shapley-based explainability on the data manifold" (2020)[2] Janzing et al., "Feature relevance quantification in explainable AI: A causal problem" (2019)[3] Sundararajan and Najmi, "The many Shapley values for model explanation" (2019) *** Update ***I thank the authors for their very detailed response. Most of my concerns have been addressed and I now recommend acceptance. *** ===========================================Updates:After considering the author's response and updates to the paper, I have bumped the score to a 6. The addition of Appendix H, in my opinion, considerably strengthens the paper's story and case for acceptance. I still have minor concerns about the writing surrounding the use of the Multinomial likelihood - for example, the paper still claims to be dealing with high-dimensional data, but bucketing into 25 buckets immediately reduces the complexity of the observation space to 25-dimensional counts. However, the authors have addressed most of my major concerns. ------Response to author's replies:I am impressed by the detailed response and the changes they have made to their paper and I am happy for this paper to be accepted. I still feel like the XLA framework is quite involved and it would have been good to understand which components of this framework are crucial to its success which is why I do not increase my score.  Final reviewThe authors have addressed some of my concerns so I am updating the rating. I still feel that the mode's novelty is limited and thus my highest rating will be 6.  After The Rebuttal: I really appreciate the rebuttal and revisions submitted by the authors. They did a good job of detailing domains of interest where their approach may be applicable. I also agree with some of the points they made about the complexity of domains that they considered in their experiments. As such, I have revised my score and now lean towards acceptance of the paper.  Update after rebuttal: After seeing the author response below, no change to my score. Update after rebuttal: the authors have clarified some of my concerns, and I have therefore increased my score.--------------- Update after the author response: I've read the other reviews, and agree with R2 and R3. I think the paper is useful (emphasizes you need to calibrate the final ensemble, not enough to calibrate members), and has some nice conceptual contributions (explaining that if ensemble accuracy > average member accuracy (which is usually the case), and the ensemble is calibrated even in just a global/weak sense, then the members must be uncalibrated). This could spur more research into conceptually analyzing ensembles, and seems interesting. But I understand the other reviewer's concerns that it's not clear what practical impact this will have, so I'm keeping my score at a 6 (instead of raising to a 7). I read the authors' feedback and appreciate the clarifications. I keep my score.  ----------------------After rebuttal:I thank the authors'  clarification. I keep my rating. ---------------------After reading the author response and the feedback from the others ------------------------Thank the authors for their response.  I still think the assumptions made in the paper are strong.  For example, the assumption in equation 2.3 and 3.3 is hardly true. Even in the revised example 2.6, the missing rates might also depend on other factors. It is hard to identify every related attribute in general. I am still not convinced that "machine-learning practitioners are unwilling to sacrifice the performance of their ML model for fairness". First, there might be many other possible reasons that some machine learning applications do not incorporate these fairness considerations. For example, there is no consensus on what fairness constraints we should achieve. In fact, existing works show that considering fairness might has bad effect on protected groups (e.g. Liu et al Delayed impact of fair machine learning). Machine-learning practitioners might just be confused what to do rather than unwilling to do. Second, I have seem many applications that have fairness incorporated. (e.g. Search Google image CEO, we now see many female CEOs on the top). From my perspective, responsible businesses would like to pay extra money to obtain a fair ML solution for the benefit of the society and the reputation of the business as long as they know what to do and how to achieve fairness. So, I would like to keep my score.  ##### Post-rebuttal updateI've read the rebuttal and updated my score.--------------- ---## Post-rebuttalDuring the rebuttal period, the experiments largely improved, resolving the majority of my concerns. I raise my rating accordingly. I expect the authors to reflect other suggestions as well in the final version. Especially, R1 raised a serious concern about the similarity between GMED and GEN/AE-MIR. A proper comparison should be included in the final version. After rebuttal: They authors have addressed my questions. I hope that the authors will carefully review classical literature on weighted minimax linear estimator in their final version. I raise my score to marginally above acceptance threshold, since I am still not convinced in the importance of domain adaptation for fixed design regression.  _[Edit: The authors clarified that the flaw I was talking about isn't there. I updated my review accordingly. The score is based on the fact that the paper can use some polish to improve clarity and the fact that some of the assumptions, even if traditional, are too strong (in essence, sample complexity bound will become meaningless if what is assumed known itself is too sample-intensive to approximate).]_ ## UpdateThe authors' response addresses some concerns, and I would like to keep the initial scores.  == After Rebuttal ==I had previously missed that Lemaire et. al. had some of the key comparisons (simple baselines done for the same architecture/dataset pairs) that are necessary to judge this method. I thank the authors for pointing this out. It is also helpful that these baselines are now included for completeness.C.6 still seems like an indirect measure of optimization stability, but it is reasonable to conclude from it that the differences from BAR are real, given the very low standard deviation in accuracy.I remain on the side that this is still a relatively incremental addition to the pruning literature, but I am now satisfied that it is well-validated. =======================================================================================================After rebuttal : Sorry for the delay. The authors address my concerns and reflect them on the revised version. I'm keeping my rating. +++ Points after discussion about Meta-RL, HiP-MDPs, and MARL.Like Reviewer 4, my view of MARL is more general than requiring all agents to be learning, even if most work does have one algorithm training multiple seats. I don't think this actually changes anything in the current draft, but should hopefully not appear in future edits.The related works should include discussion of the HiP-MDP paper.These points fit together: there should be a consistent placement of this paper, and related works. Given fixed opponents, the multiagent problem is equivalent to a single agent problem. It doesn't seem relevant whether or not the unobserved, unknown variables correspond to different but similar environments, or different but similar opponents in a single fixed environment.+++ **Update**: Revised score from 4 to a 6, mainly because the authors rightfully pointed out that they did have an ablation study in their paper which demonstrates the efficacy of their proposed methods. After response: Thanks for the clarifications especially the piecewise linearity of deep network as a function of input and not as parameters (like I incorrectly suggested in my review). I think the empirical results have some merit, but are preliminary and I fail to see the bigger picture. For example, more carefully designed ablation studies are needed to determine the practical utility i.e., when the approximations suggested are reliable and when they are not. After reading rebuttal, I think the paper is somewhat improved hence I increase the score.  Comments after the rebuttalThe author's response resolves my concern in part, and I will keep my positive score. ---------------------I thank the author for clarification. I would like to be with my score. -------------------After rebuttal:I thank the authors for clarification. I would like to keep with my score. ------### Post rebuttalGreat thanks to the authors for the detailed replies. After reading them, I decided to keep my rating. Update after response: I appreciate the authors making their contributions clearer, and adding details about the training loss and error. I have increased my score accordingly. Thank you for the clarifications.I did not change my rating, since I am unclear how the proposed method compares to SOTA beyond CIFAR-10/SVHN.Table 14 suggests that Likelihood Ratios is considerably better than the proposed method.\nFurthermore, neither in Table 1, nor in Section 5.4, I can find any results of the Likelihood Ratio method. ======Comments after rebuttal: I know the authors develop two components for FSL, my concern is that these components are incremental and have limited novelty. However, I admit this paper is a high quality paper in presenting its idea, organization and empirical evaluation. Hence I increase my score to accept now.  === UPDATED ===Given that the reviewers have not reached a consensus, I want to add more discussion to my review to facilitate the AC to make the decision. I would also include a few more quick TODOs for the authors and hope they can help add evidence to my argument.1. This submission proposes to replace the propagation in GNNs with heat kernel. The main motivation for this method is that the laplacian filters tend to oscillate, as illustrated by Figure 1. The heat-kernel provides a continuous convergence process and intuitively may address the oscillation process. Importantly, I believe the motivation of this method is *not* to prevent oversmoothing but to prevent over-oscillation. I believe this intuition is sound but encourage the authors to do more to validate this hypothesis. I appreciate the ablation study in Figure 6. As one more analysis, I suggest the authors to add the performance curves of SGC to Figure 6 (under the same setting). If this theoretical intuition is valid, we should expect HKGCN and SGC to behave more differently when the propagation degree is low (within 2-10). My understanding is that both HKGCN and SGC are efficient so this experiment shouldn't take long. Ideally the authors can update this result, at least on 1-2 datasets, within the discussion period.2. I am also very impressed by the efficiency of HKGCN. This submission has experimented with, to my knowledge, the largest publicly available graph dataset (arXiv), which contains more than one million nodes. According to the authors, HKGCN can be trained for this arXiv dataset in 48.5s, which is impressive. Note that here the heat diffusion matrix does not need to be computed/stored explicitly. Following the setup in SGC, HKGCN only needs to compute the propagated features in a preprocessing step.  I suggest the authors to give more concrete numbers to illustrate the efficiency of this method. For this arXiv dataset, what kind of hardware is required? How much actual RAM did you use to compute the preprocessing step?3. After reading other reviews, I now realize that this submission is not the first to introduce heat kernels into GCNs. Among the papers pointed out by other reviewers, I find [1] to be most relevant in that they also proposed the usage of heat kernels. Can the authors also clarify the difference between this submission and [2]?Based on this novelty concern, I have lowered my review score to 6. [1] Xu et al, graph wavelet neural network, ICLR 2019[2] Xu et al, Graph Convolutional Networks using Heat Kernel for Semi-supervised Learning, IJCAI 2019.  After reading the authors' feedback, I decided to increase my rating from 5 to 6. The authors convinced me that setting $E>1$ can reduce the number of communications.  After reading the author feedback, I upgrade my rating to 6. See responses in this thread for reasons. After the rebuttal, the authors added a section on large-batch training, which shows that the catastrophic Fisher explosion also occurs in large batch training. This makes the paper more convincing. However, the main concern that this paper lacks theoretical contribution still exists. Therefore, I keep the weak acceptance recommendation.  -------------------UPdate: the authors' reply address my concerns well, so I raise my rating to the acceptance side. --------------after rebuttal-------------------I've read all reviews and the rebuttal, and thank the authors for their efforts and extra experiments. I think it is ok to be accepted due to provide further understandings about the learning representation with solid experimental results. ._Post-rebuttal_:I really appreciate the authors adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept Update post-rebuttal==================- I am happy with the thorough engagement by the authors and their clarifications. So  I am bumping the up the score a notch. I am increasing the score to 6. I appreciate the authors' response to the reviews. They did the additional experiments I asked for. As I stated in the original review, I really liked the unified approach. It is elegant and is nicely presented.After reading the authors' response to R4, it is clarified that the 1D essential homology is because the computation over all threshold is too expensive. I think there might be an opportunity to better justify this paper: we often have to stop the filtration early due to computational concern. This unified representation could potentially be a good solution for this: without computing the actually death time, the unified solution can still 'learn' the real death time of the 1D essential classes. The authors might want to discuss or ideally empirically verify this in the final version. For example, can you show that using the new approach, and stopping earlier during the filtration, the unified classifier can be as good as when we run the whole filtration and compute the real death  time for all 1D classes. Moreover, it will be ideal if the authors can manage to show that the unified approach can actually learn the real death time for these fake essential classes (I do not know how). This way the paper can potentially have a bigger impact.  ======Update======Thank you for the rebuttal, which resolved most of my concerns. I increased my score.  POST-REBUTTAL COMMENTS======== I would like to thank authors for their clarifications. Accordingly, I have increased my score to 6. ------------------Post-rebuttal comments: Thanks for the detailed answers. Many of my concerns were addressed, and I am increasing my score as a result. A follow-up thought: It would be nice to add some discussion on the runtime of the proposed framework. AFTER REBUTTAL:The authors have toned down some of their claims and I am increasing my score accordingly.  ----After rebuttal:Novelty: my assessment remains the same. It is not non-trivial enough to combine several linear operators into a unified optimization framework. Although the unification is useful, it is not a major novelty.Thank you for the additional experiments on testing the hyper-parameter. As you mentioned instability, it is worth to have some toy example to demonstrate the instability and study the cause of such instability and show how to avoid such instability using the proposed regularizer. Clearly (19) is bounded. When \alpha_3 is large enough, the solution will be trivial.Regarding non-linearity: the authors' framework is for unifying a graph convolution operator (that is one layer in a graph neural network). Nonlinear activation is another operator. This is not a major problem from my perspective.Overall, I think this work has some value (although the novelty is not strong) and still recommend weak acceptance. ---Update after rebuttalI thank the authors for their response. After reading it, the revised version and the other reviews, I think that there's evidence of the effectiveness of the proposed method.As I tried to convey in the initial review, I consider the main weakness of the paper not showing the performance on models achieving state-of-the-art verified robustness. I think the case of $\ell_\infty$ is emblematic, where $\epsilon=0.01$ is used. Since there exist models achieving provable robustness >90% for $\epsilon=0.3$ (on MNIST), the scenario considered in the evaluation doesn't seem interesting, regardless of how FGP compares to the other methods.While I agree with the authors that training for provable guarantees sacrifices clean accuracy in most of the cases, as far as I know that is currently the way to achieve good VRA at meaningful thresholds. Along this line, also the authors used FGP on larger models when those were trained with MMR or RS. In my opinion the most interesting application of a (incomplete) verification method like FGP is to provide better VRA than current methods for training (IBP, CROWN-IBP, Wong & Kolter's method), for example using less heavily regularized models which can retain higher clean accuracy. I think this is the missing piece in the current version of the paper.For these reasons, I keep my initial score. Update after rebuttal: I appreciate the authors' response and clarifications. I maintain my original score. ---**After rebuttal**The responses address my main concerns. I have increased the score to 6. But, I also agree with other reviewers that the novelty of this paper is somewhat limited.  Update after rebuttal:I agree with Reviewer 5 that this paper has good ingredients, and the discussion and update of the draft clarifies the novelty and provides better review on the related work. However, the experiments presented in this paper are not very comprehensive, particularly the baselines and the ablation/alternative studies. I am not fully convinced by the authors response of "because MI-VAE performed worse than $\beta$-VAE in PixelCNN-VAEs ... we expected that a similar tendency would be observed." PixelCNN-VAE uses an autoregressive decoder, which are known to exhibit issues that are not observed from non-autoregressive ones like DSAE. This explanation of why MI-VAE was emitted seems slightly hand-wavy. I decided to decrease the rating to 6 to reflect the insufficiency in experiments, but hope that this experiment can be added if the paper is accepted. **Update after discussion with authors**I want to thank the authors for their incredibly detailed responses and engaging so actively in the discussion. Some of my criticism could be addressed, while other issues are still somewhat open. If the main merit of the paper is to make the "sequential VAE community" aware of issues that have been discussed and addressed before, then I think the paper does an OK job at that (though I'm not entirely sure what community that is, the issues have been discussed before in the fields of vision and VAEs with autoregressive decoders). I want to strongly encourage the authors to be as precise as possible when describing the novelty - maximizing the mutual information that a representation carries w.r.t. some relevance variable while simultaneously minimizing information that it carries w.r.t. to another variable is NOT novel. What is novel is the application of that principle to separating "global" from "local" information in sequential data (and how to actually perform this originally intractable optimization in practice). I also want to encourage the authors to state what's known and what's new as clearly as possible and improve the quality and clarity of the "educational" review of why maximizing mutual information is not enough as much as possible.Viewing the paper as "showing how a known problem also appears when separating global from local information, and how to apply known solution-approaches to the problem in this specific context", shifts the relative importance of the issues raised by me. Essentially, that view emphasizes the paper as mostly an application paper (rather than a novel theoretical contribution). Accordingly (but please make sure that that shift in view is also clear in the final paper), I am weakly in favor of accepting the paper and have updated my score accordingly.--- ---UPDATED After RebuttalThanks the authors for answering my questions. Theorem 1 now looks fine. **It can be said that BC is to maximizes the off-policy expected...**I do understand that the existence of $\epsilon$ makes the RHS of Eq (3) (All labels are in the revised version. ) smaller. However, Eq (3) only means the maximum *guarantee* (the lowest performance) of BC is small, and doesn't mean that the BC's performance can't exceed something. **definitions of $S_e^{\pi_e}$ and $S_{e+*}^{\pi_e}$**Thanks for clarification. It's much clearer now. However, in this definition, how about the states where the policy has always followed the expert policy so far? I guess this is the reason why you have 3 sets before revision. **The first term on the RHS in (7) thus works as a noisy regularizer.** (in the revised paper)In my opinion, it goes a little bit beyond regularization, as the coefficient is as large as 0.4. For example, in the image classification task, can we call corrupting 40% labels a "noisy regularization"? **It could happen that removing $S_e^{\pi_e}$ from the training set changes the performance in such a case where the noisy expert recovers to be in the optimal state after adopting the non-optimal policies.**But with much larger probability, it will go out of the manifold (of expert policy's states), right? Even BC can go out of it easily (otherwise you can't beat it). Overall, I think this is a neat algorithm and it seems to work pretty well. The revision also makes it much easier to understand and I do appreciate the effort behind it. I'll then raise my current score (5) to 6, although I'm still not fully convinced so won't give a higher score.  ---Post-rebuttal: Raising my score from 5->6 after authors provided a detailed list of changes. ########After rebuttal#########I appreciate the author's effort in addressing my concerns. After reading the rebuttal and other reviews, I am raising my scores to 6.