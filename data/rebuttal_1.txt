 ---------------------Updates after author response---------------------The authors basically posted their response at the last minute of the window which eliminates the possibility for further discussion. While it is lengthy and point-to-point, I found it failed to clear up any of my concerns.I am very disappointed that even after they recognized the misleading arguments in "convergence" analysis, in Theorem 3 of the revised version, they are still claiming $\| \nabla F(w_R) \| \leq \epsilon$ when $\delta < O(\epsilon^2)$ as "convergence". Such conclusion will be EXTREMELY MISLEADING if readers missed or did not carefully think about the condition on $\epsilon$. Authors may want to refer to a calculus textbook for the rigorous definition of convergence.Regarding technical contribution,- Authors acknowledged that their analysis does *not* guarantee convergence to stationary point, since there is an additive $O(\delta)$ term in the gradient upper bound. This immediately diminishes their theoretical contribution.- Authors did not justify their technical novelty. In fact, if we carefully look at their analysis in the appendix, it follows from standard SGD with a slight adjustment to the biased gradient induced by label smoothing (which is acknowledged by the authors). This is an additive gradient and thus is easily controlled; it is also the main reason why in their main theorem, the gradient upper bound suffers a non-vanishing $O(\delta)$ term.- Authors argued that compared to running SGD from scratch, the benefit of LSR+SGD is the introduction of $\delta$. However, this quantity itself is out of control. Note that even they were able to show $\delta=O(1)$, it is not strong enough here since this can easily be obtained by running SGD. The only way that I see will save the paper is to show under distributional assumption of the data, that $\delta = O(1/d)$ for example. However, I did not see how to make it happen, and authors completely ignored such analysis in their response.Overall, this is a paper playing tricks on its technical parts. It is decorated with bunch of mathmatical analysis most of which is known and standard, and the introduction of new insights is minimal. I will be shocked if it gets accepted in ICLR or equivalent conferences. Updates after discussing with the authors1. The paper is not very clearly written, and I had misunderstandings. Some of my comments above are not right.2. However, I will not change my rating. I found the convergence rates stated in the paper are misleading. The paper claims $O(1/T)$ convergence rate. In fact, this is WRONG. The authors assume the Frobenius and trace norms of $n\times n$ matrices are CONSTANTS. This is not possible. The norms are $O(n)$. Simple arguments can show $|| \xi ||_F = G$ is $O(n)$.3. Based on the right assumption that $|| \xi ||_F = G = O(n)$, the required number of iterations is $T = O(n^2)$. The algorithm is not communication-efficient. It is more expensive than communicating the $n\times n$ kernel matrices.4. After reading my comments, the authors changed their notation from $G$ to $\gamma$, $C$, $G$, and $H$. They are also Frobenius and trace norms of $n\times n$ matrices. The authors assume $\gamma$, $C$, $G$ and $H$ are constants. This is WRONG. They are $O(n)$.     - For example, if they use the bound of Rahimi and Recht,  then $|| \xi - K ||_F^2 = G^2$ is $O(n^2)$.  A bound as good as $|| \xi - K ||_F^2 = O(n)$ would surprise me; if the authors know such a bound, please let me know.     - Let me strengthen my point again: IT IS WRONG TO ASSUME MATRIX NORMS ARE CONSTANTS! If the authors can prove they are constants, they need to show me the proofs. If they cannot, they should assume Frobenius norm and trace norm are $O(n)$.