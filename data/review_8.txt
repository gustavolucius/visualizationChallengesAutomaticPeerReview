 *Summary and contributions:*This paper proposes a new data augmentation strategy to train image classifiers and object detectors. The key insight is to use an image saliency signal to guide where to crop-and-paste images when mixing them. The paper includes an exploration of the design space of such approach, and multiple experimental results showing the empirical superiority of the proposed approach compared to existing data augmentation strategies.*Significance:*The paper is interesting because it provides a new trick in the bag of tricks that is both simple to understand, reasonable to argue for, and (now) has good empirical support (for classification, detection, and adversarial attack robustness).Originality: Limited. Although no previous work provides the experimental results presented here, the results are expected. This work is good A+B incremental work.*Strengths:** Overall the paper read easily. The general argumentation, method description, and experiments are all reasonably well described.* Simple ideas that provide good results. In retrospectively it might seem obvious, yet not explored before.* Widely applicable for all methods using pre-trained image classifiers. *Weaknesses:** Although the experimental section is good, some elements are missing. E.g. adding non-data-augmented as reference point in the plot, or considering CAM as a saliency strategy.* Some sections of the text would benefit from revisiting the English.* The method implicitly relies on having simple images with one dominant foreground object  like the ones in CIFAR and ImageNet. The saliency / object of interest  detection method depends on these characteristics. Ideally the paper would be more upfront on these assumptions. Especially in the context of the Rethinking ImageNet Pre-training, ICCV 2019 work.* The related work section for saliency is very partial.* Some of the saliency methods evaluated use training data, even the ones that do not have been tuned using additional data. The paper would benefit from a discussion of this additional information.*Correctness:* Paper seems correct. There are minor shortcomings in the experimental protocol, but nothing that would foreseen invalidating the main conclusions of the paper.*Clarity:* Paper presentation is clear.*Relation to prior work:*Related work section has a reasonable extent. Regarding data augmentation, the paper compares with the main methods.The text mentions Lemley 2017, however I think it would be also worth mentioning AutoAugment Cubuk 2019; and justifying why it is not included in the results comparison.The saliency detection is very partial, and does not cover the main works in the area. For one the text does not clarify which kind of saliency is considered (where will a human look ? which are the main objects of the scene ? which is the main object of the scene ?). Depending on which one, discussing the main performers in the related benchmarks (e.g. table in Qin 2019 paper) seem relevant. From what I grasp, the proposed method would actually want to have as input a weakly supervised class-conditional segmentation. And saliency is used as a proxy for this. Discussing the relation to (image-level labels) weakly supervised segmentation would also be welcome.(I would guess it could provide even better results, but at a much higher computational and system complexity cost). In particular CAM is discussed in section 4.3. Would that not be a task-specific way to obtain the desired semantically important regions  ?When preparing the camera ready, please consider discussing the concurrent work of https://arxiv.org/pdf/2009.06962.pdf which seems related (seems recent enough, ICML 2020, to give the benefit of presumed concurrency).*Reproducibility:*The overall algorithm is simple to understand and re-implement.The selected saliency method Wand and Dudek, 2014 seems to be a video saliency method. From a quick inspection of that paper it is not immediately clear to me how to transpose it for single image saliency. Since this method is used in most experiments, providing more details of the implementation and its parameters are necessary to reproduce the key results.*Specific per-section feedback:*Section 1:- every field: is too broad of a statement, remove/rephrase the first sentence.- extremely complex -> complex- generalizability -> generalization- undesired to the CNN since -> undesired since- does not allow & to have any uniformative pixel: double negative, consider simplifying.- semantically important region: these are task dependent. How do you ensure the saliency to match the task / semantics ?Section 2.1:- comes into account that aims: unclear, please rephrase.Section 2.2:- unable to & 1 or 0: this is not true, please rephrase.- intermediate values: you mean closer to 0.5 ?- Thereby, helps: unsure if this is proper English.Section 2.3: See comments above regarding related work.- Which kind of saliency is considered here ? Which are the relevant benchmarks ? Which training data / evaluation data are these methods bringing (indirectly) into the system ?- BAsNet for example, was trained on 10k images. Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ?Section 3.1:- selected training image -> selected training (source) image. In general do give hints for the subscript meaning of I_s, I_vs, etc.- Why only one pixel with maximum intensity ? What happens if (due to quantization) two pixels have the same value ?This is clarified later in the text, but some context would be welcome in the mention here.Figure 3:- Add non-augmented result bar as reference point. - Consider showing the five points per bar, or some hint for the variance in these results. From the plot it is left unclear if the fluctuations across methods are significant or minor (since not reference point, nor sense of the variance).Section 3.3:- Tiny-Imagenet: saliency methods tend to _not_ be scale invariant (in particular trained methods like BasNet). How is this handled ? Why would one expect Tiny-Imagenet to provide conclusive data ?- What about CAM, or any other class-conditioned saliency / weakly supervised segmentation method ?Section 3.4:- found out -> consider- What about Salient to Random ? That seems a reasonable option too ?- are identical -> are similarSection 4.1:- SOTA top-1 error: there are 20 methods that claim better results in https://paperswithcode.com/sota/image-classification-on-cifar-10 and https://paperswithcode.com/sota/image-classification-on-cifar-100 that claim better results. Maybe temperate the SOTA claim, with phrasings like best known results for model X  or similar. One specific model result, far from best known, does not constitute state of the art  in my understanding.Section 4.2:- Because in -> This is because in The paper presents some novel contributions regarding recurrent neural networks. Building on the work of Chang et al. (2019),  the authors provide a global convergence result for the hidden representation of a family of recurrent neural networks using standard techniques from the Lyapunov analysis of dynamical systems.The requirements of the theorem are met  (within the limits of discretization) by their proposed algorithmic scheme. Numerical evaluation on a variety of benchmarks shows that the proposed algorithm yields systematic improvement over other RNN approaches. For all of the above reasons, I recommend the acceptance of the paper. Some concerns to be addressed:- The connection between stability and trainability or refer to Chang et al. (2019) if their analysis applies here. - specify the functions \sigma_min and \sigma_max used in Theorem 1- specify the meaning of the one-arg function f(h^*) as opposed to the 2-arg f(h,t) appearing in Definition 1.  Summary:This paper focuses on the problem of semi-supervised semantic segmentation, where less pixel-level annotations are used to train the network. A new one-stage training framework is proposed to include the process of localization cue generation, pseudo label refinement and training of semantic segmentation. Inspire by recent success in the semi-supervised learning (SSL), a novel calibrated fusion strategy is proposed to incorporate the concept of consistency training with data augmentation into the framework. Experiments on PASCAL VOC and MSCOCO benchmarks validate the effectiveness of the proposed method.  Pro:+ The proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training. + The new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.+ Achieve a new state-of-the-art on both PASCAL VOC and MSCOCO benchmarks compared with recent semi-supervised semantic segmentation methods.Questions:- CCT (Ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features. I'm wondering if authors can provide some insights about why the proposed method can achieve better performance than CCT when they both include the consistency training and data augmentation in the designs.  - In table 3, I suggest to include the segmentation framework used by each method in the table. In early works, old version of deeplab is usually treated as the standard. I understand using deeplab v3 is a fair comparison with CCT. It would be good to make this information clear in the table.- It is also suggested to report the performance on PASCAL VOC test set as it is a common practice in this area (although CCT does not do so).- Sine the unlabeled data training branch does not rely on any pixel-level annotations, I'm wondering if the proposed method can also work under weakly-supervised setting, where no pixel-level annotations are available during the training.  The paper introduces a decentralized framework for adaptive momentum-based gradient descent optimizers, such as ADAM. The proposed method is novel and is among the first works to consider a decentralized communication graph without a master node. The author discovers the divergent properties of the recent work of DADAM (Nazari, 2019) and proposes a way to fix it by adding a similar consensus step for the adaptive learning rates of agents. The mathematical derivation seems to be correct to the best of my knowledge. Finally, the author tests their method on a simple CNN and show their superiority compared to DADAM to achieve a close to the centralized performance.However, I have some minor comments to improve the manuscript.1. The experimental evaluation of the work is quite limited. I understand the space limit but it would have been nice to see more experiments instead of showcasing Algorithm 2 with an extra example in Algorithm 3. It is important to see the convergence behavior of the method (on the training data) with respect to the DGD on various datasets/networks in practice, rather than observing how the testing accuracy behaves. Note that your method does not guarantee any specific generalization behavior and therefore I believe it is more suited to report the experiments only in terms of training performance when you are out of space.2. What are the drawbacks of this method? I can see more memory requirements for the agents due to the new variable \tilde{u} for instance. Do you have any quantified evaluation in this respect? I suspect it can be significant especially if the trained model is large and the agents have limited memory/computational resources3. In Section 3.2, the author says "Algorithm 2 can become different adaptive gradient methods by specifying r_t as different functions. E.g., when we choose ..., Algorithm 2 becomes a decentralized version of AdaGrad." This sentence is not accurate as algorithms like AdaGrad and Adadelta do not use momentum on the past gradients. They only use the squared values of the past gradients. I believe your method, as I mentioned above, is a general framework for momentum-based techniques including ADAM, AdaMax, NADAM, etc, which brings me to the next question.4. Is it possible to generalized your method for an adaptive gradient descent algorithm that does not use the momentum of the gradients? For example, take AdaGrad with a fixed learning rate of \eta instead of m_t. How does your convergence behavior change?  This paper shows that transformer models can be used to accurately learn advanced mathematical computations from millions of examples.  The problems are drawn from the fields of differential equations and control theory.  The selected problems are ones that are solvable using known algorithms; however, these algorithms involve a sequence of advanced mathematical operations (e.g., differentiation, calculating the rank of a matrix, calculating the eigenvalues of a matrix, etc), for which no known simple shortcuts exist. For the experiments in this paper, for each problem a large number (50 million) of training examples are randomly generated, and are then used to train a transformer model.  Across these problems, the paper shows that the neural network is able to solve these problems at high accuracy (96-99.7% accuracy).Strengths- The paper ask a well-motivated question regarding whether neural networks are able to learn complex mathematical operations from examples.- The paper is clearly written, and the experimental rigor/quality appears quite high.- The empirical results are quite intriguing, and raises many interesting questions for future research.  For example, (1) how is a transformer model managing to attain such high accuracy on these problems, (2) what other complicated mathematical problems might be similarly learnable, (3) what are the practical implications to real systems of these results.- The paper does a good job considering the various potential counterarguments to its conclusions in the discussion section (Section 5.4).  For example, the paper argues convincingly that (1) its unlikely the model is exploiting some trivial distinction between the positive vs negative examples because the examples are sampled randomly, and that (2) its unlikely the model is interpolating between solutions because the problem space is so much larger than the training set (and because small models which would be unable to memorize the training set also perform well).(Weaknesses- I would have appreciated more of a discussion about the computational cost of solving these problems mathematically vs. solving them with a neural network.  What is the computational complexity (Big-O) of each of the known mathematical algorithms for solving these problems?  Are there large computational savings from using a neural network?  What are the practical implications of the results shown in this paper?- I think including a few more baselines would have been useful.  Also, a brief description of the FastText model would make the paper more self-contained.   One question: Why is the FastText model in Section 5.1 only trained with 2 million examples, while the transformer model is trained with 50 million examples?- In the current experiments, the test set is drawn from the same exact (random) distribution as the training set.  I was very curious whether the model would have been able to attain high test-time accuracy, had the test examples been drawn from a different distribution.  In particular, Id be curious how difficult it would be to construct a test distribution on which the current model performs terribly.  This line of questioning would be able to start better answering whether the model is solving the problem in a way that truly generalizes.- I think the paper could be strengthened by adding additional error analysis to try to better understand the errors made by the transformer model.- The paper says training is performed on 8 V100 GPUs with float16 operations: It would have been nice to hear more about the training process: For example, low long did training take for the various problems, and how was training distributed across the GPUs?- I would have appreciated more of a discussion about the broader implications and significance of these results.Overall, I thought the paper was very interesting and well executed, and I think it would make a very nice addition to ICLR 2021. This paper generalizes and build on top of the MENTS/E2W, and shows that the entropy regularization can be replaced with any convex regularization. It uses the tools of convex conjugates and duality to derive the theoretical results and the algorithm/updates. Empirical results on Atari games confirm the value of policy regularization in MCTS.Strong points: - Theory generalizes MENTS/E2W- Experiments further support the value of regularized polices, showing that entropy is not the only thing that "works".- Paper brings important and interesting insights into MCTS, which is potentially very impactfull.- Previous work is (to my understanding) well cited.- While the paper relies on non-trivial operations, it's well written.- The resulting algorithms/updates are "easy" to implement.Weak points:- This paper is very much incremental to MENTS/E2W, one could say it "just generalizes" MENTS.- Missing connection to previous results of policy/values dualities (please see additional feedback)- The empirical results are not very exciting.Reasons for score: While the empirical results don't bring anything exciting (especially when contrasted to MENTS), they still bring interesting insights. It almost seems that any regularization is relevant. Furthermore, the presented theory/connection coming from the duality is important - I do not think this connection of duality was presented in the MENTS paper. Thus it's satisfying to know that this is where the "magic" of softmax and entropy as used in MENTS is coming from.  The reason I really like this paper is that it helps to build more intuition about the regularized policies in MCTS, all the while generalizing the underlying theory.Additional feedback:On high level, this paper essentially explores the duality of policies and corresponding (regularized) values. This idea/result/notion of duality between policy and value appears quite often (a quick example that comes to my mind being extensive form games but surely others), and I think few sentences on this would help the reader to feel "less surprised" about some of the presented derivations and techniques, and overall better place it in the context of previous relevant work. Reading this paper, one could think that the duality of policies/values is novel observation, while in general it's not. Summary ==== This work studies the performance of different Graph Neural Network (GNN) from a spectral perspective. In particular, it shows the kernel for all kinds of proposed GNN models can be expressed in a general form with a specific frequency response definition, which indicates the spectral property (spectrum) of the kernel. Based on this definition, this work empirically studies the band-pass property for kernels in different models and demonstrate the importance of such spectral perspective. ==== Pros ==== + I like the spectral analysis, which is interesting, novel and appears to be important. + The discussion on different GNN models is comprehensive and solid enough. + The experimental result shows that it is important to have different kinds of filters ==== Cons ==== I do not have too much criticism about this work. I am interested to see if there could be some extra discussion/example about how should we choose the kernel for some specific problem, based on the spectral perspective. The experimental result in Sec 5.2 shows that for tasks on images using some kernel with spectrum cover the whole region has the best performance, then *how about other tasks such as semi-supervised learning?* It would be great if there is some guidance/analysis, otherwise we still need to determine the most suitable model based on empirical result. Another question is that, the expressive power analyzed by WL-test involves the depth of the network, while in this work it appears that only one layer is considered, so I am wondering *what is the relation between the depth and the expressive power, from a spectral perspective?* To me it seems to be difficult to analyze, but I believe it is worth to address, as we have seen the importance of depth in CNNs. ==== Reason for score ==== This paper is clear-written, the spectral perspective looks very interesting to me, the theoretical analysis appears to be correct and solid, and the experimental result verifies that the analysis from such perspective is important indeed. Therefore I tend to vote a clear accept. ==== Minor comments ==== - Below Eq. (2), '$g_0, g_1$ ... trainable models', a typo? In this paper, the authors propose a spectral-based analysis method to analyze the modeling abilities of major GNNs. Specifically, the first use the concept of convolution support to unite the ideas of spatial-based methods and spectral-based methods. By further identifying the frequency profile of different models, the authors obtain an overview of which spectrum range different models may cover. The evaluation on regular and in-regular graph datasets validate their arguments. In general, the paper brings an interesting perspective in addition to the WL-test to reveal the expressive power of GNNs, and both the theory and evaluation sound solid. It would be better if the authors can further address a few issues.First, the authors summarize that in certain cases (e.g., sum pooling), the spatial based methods can be considered in the form of Eqn. 3 shows. Though the authors describe that advanced methods like GAT can also be described in this way, it would be great if the authors can make an explicit argument in the beginning of Sec. 3 to describe whether general spatial based methods (e.g., using max pooling, or other $upd(x, y)$ functions) can also be described in this way. I also suggest the authors to revise Definition 2 to further explain why each convolution support has the same frequency response over different graphs in a spectral designed case, as there might be some training parameters that can be affected by different graph structures. In addition, in Corollary 1.1, the authors indicate the frequency profile of any given graph convolution support can be defined as Eqn. 7, it is unclear if this also includes spatial-designed graph convolution.The analysis in Sec. 5 is interesting, but it would be better if the authors can expand the discussion on how we can further utilize the obtained frequency profile to analyze the expressive power of different GNNs. Dividing different filters into low-pass, high-pass, and mid-band categories seem to be in a very coarse granularity and it would be better if the authors can discuss some potential directions for more detailed analysis. Im also interested in if the number of kernels of the same category (e.g., low-pass filters) may affect model performance. It would be interesting to see if any experimental results are provided to analyze the impact of the number of kernels, the distribution of types rather than their categories only. In summary, this work provides a new perspective in analyzing the expressive power of GNNs and I suggest the authors to further address the above issues. This paper proposes a combination of imitation learning and search applied to the multiplayer, simultaneous-move game of no-press Diplomacy. While both techniques have been used before, even in concert, there are some domain-specific challenges: more than two players, simultaneous moves, and a very large branching factor per player. The authors use imitation of human play for value estimates and selection of candidate actions, and External Regret Matching to generate a one-step policy. Experimental validation showed the agent to have strong human-level performance, and was hard to exploit using other machine-learning techniques.To the best of my knowledge, I agree with the author's claim that the imitation/search agent from this paper is the first to demonstrate human-level performance in the game of Diplomacy. This is a non-trivial result: Diplomacy is a large problem, and is thought to require fairly complicated interaction between players throughout a game, as well as the ability to handle the uncertainty of other player's moves. There is no claim of guaranteed performance across a general class of problems, but the paper's method seems general enough to apply elsewhere.The writing was generally clear, although asking "could I duplicate these results?" did raise a few questions about some details.The experimental results against humans seem well constructed, and do a reasonable job to support the claim that the agent demonstrates human-level play. The assorted exploitability-inspired experiments help support the notion that the agent's play is strong, rather than just doing well based on some peculiarity of human play.---------- specific issuessection 2.2 "(which can also be represented as v_i(a_i, a_{-i})."Closing parenthesis missing?"ERM guarantees that R^t_i(a_i) in \mathcal{O}(\sqrt{t}). If R^t_i(a_i) grows sublinearly for all players' actions, as in ERM, then the average policy over all iterations converges to a NE in two-player zero-sum games and in general the empirical distribution of players' joint policies converges to a CCE as t approaches infinity."How does the sampling interact with this? The regret bound is w.r.t. the probabilistic policy pi^t_i, but sampled a_{-i}. Connecting this bound to describe the joint behaviour (i.e., Nash eq'm) prThis paper proposes a combination of imitation learning and search applied to the multiplayer, simultaneous-move game of no-press Diplomacy. While both techniques have been used before, even in concert, there are some domain-specific challenges: more than two players, simultaneous moves, and a very large branching factor per player. The authors use imitation of human play for value estimates and selection of candidate actions, and External Regret Matching to generate a one-step policy. Experimental validation showed the agent to have strong human-level performance, and was hard to exploit using other machine-learning techniques.To the best of my knowledge, I agree with the author's claim that the imitation/search agent from this paper is the first to demonstrate human-level performance in the game of Diplomacy. This is a non-trivial result: Diplomacy is a large problem, and is thought to require fairly complicated interaction between players throughout a game, as well as the ability to handle the uncertainty of other player's moves. There is no claim of guaranteed performance across a general class of problems, but the paper's method seems general enough to apply elsewhere.The writing was generally clear, although asking "could I duplicate these results?" did raise a few questions about some details.The experimental results against humans seem well constructed, and do a reasonable job to support the claim that the agent demonstrates human-level play. The assorted exploitability-inspired experiments help support the notion that the agent's play is strong, rather than just doing well based on some peculiarity of human play.---------- specific issuessection 2.2 "(which can also be represented as v_i(a_i, a_{-i})."Closing parenthesis missing?"ERM guarantees that R^t_i(a_i) in \mathcal{O}(\sqrt{t}). If R^t_i(a_i) grows sublinearly for all players' actions, as in ERM, then the average policy over all iterations converges to a NE in two-player zero-sum games and in general the empirical distribution of players' joint policies converges to a CCE as t approaches infinity."How does the sampling interact with this? The regret bound is w.r.t. the probabilistic policy pi^t_i, but sampled a_{-i}. Connecting this bound to describe the joint behaviour (i.e., Nash eq'm) proceeds easily when considering the joint pi^t. However, in this case the joint is only connected by the specific sampled actions. Does the statement about convergence to NE apply to sampled ERM (With high probability? In expectation?) or only to ERM? Table 1What are T=0.5 and T=0.1?  I assume they're the temperature parameter, for action selection in DipNet, but this isn't described until later.Figure 1, "Sampling a single action leads to poor performance"How is the graph intended to be interpreted? In the graph, 1 search action looks to be significantly better than the blueprint."M_i is a hyperparameter that is proportional to the number of units controlled by player i."Proportional how? If power i controls n units, the algorithm will sample M_i actions? M_i*n actions? k*n actions for some hyperparameter k? Please clarify this sentence.Are the samples select independently across units somehow, or just the top orders for the round (for that possibly very large number of actions)?"We compute a policy for each agent by running the sampled algorithm described in Equation 1 and Equation 2".Section 2.2 describes a few modifications, one of which would seem to modify equation 2. Maybe saying instead that it is running the sampled algorithm described in Section 2.2?How many iterations are used?"If our agent is an ESS (or Nash equilibrium), then a population of players all playing the agent's policy could not be 'invaded' by a different policy."Wouldn't this only apply to ESS, not Nash eq'm in general? Section 4.2.1What does a sum-of-squares score of 0.54 or 0.42 represent? Are those scores good? Huge? Tiny? The drop is statistically significant. Is that drop of ~0.12 large? Small? Are we supposed to care about the drop? Or just how low they both are? Or just how low the SearchBot-clone result is?It seems like this section would be helped by having a short summary statement of the conclusions the authors would like to draw.Section 4.2.2It was nice to see that the humans beat the human-imitation blueprint, while they struggled quite a bit against SearchBot. It might be worth specifically noting that not only was the blueprint imitation an improvement on DipNet in this setting, it was specifically the search method making the largest improvement.Section 4.2.3CFR->ERM?  The text talks about plotting versus the number of ERM iterations, while the figure talks about CFR.Figure 3What exactly is the average being talked about in the first paragraph? The averaging mention in Section 2.2 (which is not used during search in normal play)? The self-play strategy profile which is a combination of one strategy from each of the 7 runs (not really an average)?  Something else?Is "average" in the first paragraph different than "average" in the right figure? The average line on the left doesn't seem to match the average line on the right: on the right, there seems to be a slight bump upwards at the end, which doesn't seem to be present on the left.On the right plot, why not include all (or at least multiple) independent plots, to give a more complete picture of the variance?Include some mention or mark indicating how many iterations are used during play? As is, I don't know how this graph relates to the search as used in SearchBot.oceeds easily when considering the joint pi^t. However, in this case the joint is only connected by the specific sampled actions. Does the statement about convergence to NE apply to sampled ERM (With high probability? In expectation?) or only to ERM? Table 1What are T=0.5 and T=0.1?  I assume they're the temperature parameter, for action selection in DipNet, but this isn't described until later.Figure 1, "Sampling a single action leads to poor performance"How is the graph intended to be interpreted? In the graph, 1 search action looks to be significantly better than the blueprint."M_i is a hyperparameter that is proportional to the number of units controlled by player i."Proportional how? If power i controls n units, the algorithm will sample M_i actions? M_i*n actions? k*n actions for some hyperparameter k? Please clarify this sentence.Are the samples select independently across units somehow, or just the top orders for the round (for that possibly very large number of actions)?"We compute a policy for each agent by running the sampled algorithm described in Equation 1 and Equation 2".Section 2.2 describes a few modifications, one of which would seem to modify equation 2. Maybe saying instead that it is running the sampled algorithm described in Section 2.2?How many iterations are used?"If our agent is an ESS (or Nash equilibrium), then a population of players all playing the agent's policy could not be 'invaded' by a different policy."Wouldn't this only apply to ESS, not Nash eq'm in general? Section 4.2.1What does a sum-of-squares score of 0.54 or 0.42 represent? Are those scores good? Huge? Tiny? The drop is statistically significant. Is that drop of ~0.12 large? Small? Are we supposed to care about the drop? Or just how low they both are? Or just how low the SearchBot-clone result is?It seems like this section would be helped by having a short summary statement of the conclusions the authors would like to draw.Section 4.2.2It was nice to see that the humans beat the human-imitation blueprint, while they struggled quite a bit against SearchBot. It might be worth specifically noting that not only was the blueprint imitation an improvement on DipNet in this setting, it was specifically the search method making the largest improvement.Section 4.2.3CFR->ERM?  The text talks about plotting versus the number of ERM iterations, while the figure talks about CFR.Figure 3What exactly is the average being talked about in the first paragraph? The averaging mention in Section 2.2 (which is not used during search in normal play)? The self-play strategy profile which is a combination of one strategy from each of the 7 runs (not really an average)?  Something else?Is "average" in the first paragraph different than "average" in the right figure? The average line on the left doesn't seem to match the average line on the right: on the right, there seems to be a slight bump upwards at the end, which doesn't seem to be present on the left.On the right plot, why not include all (or at least multiple) independent plots, to give a more complete picture of the variance?Include some mention or mark indicating how many iterations are used during play? As is, I don't know how this graph relates to the search as used in SearchBot. The authors consider "no-press Diplomacy", a complex game played by humans which involves (limited) cooperation and competition.The method uses a policy and value function learned from human games, together with a test-time search. The imitation-learned policy is used both to restrict the actions considered in the search and also to roll out the leaf nodes of the search (to a fixed depth, after which the value function is used). The search process is a sampled form of external regret matching.Several wrinkles required for good performance are clearly motivated and explained, e.g. handling low-entropy policies and large action spaces.The authors evaluate against existing bots, against multiple human players on webdiplomacy.net, and against two human experts. In all cases, the authors algorithm outperforms its opponents.It is not feasible to compute exact exploitability in a game of this size, or even to train an approximate best response against the searching agent. The authors therefore train an approximate best-response RL agent against both the imitation-learned policy and a distillation of the searchbot, strongly suggesting that the distilled search is less exploitable than the imitation-learned policy.The method described is novel, but also a fairly straightforward extension of prior work to this domain, incorporating a single-ply search on top of an imitation-learned policy. Although there is nothing radically new, the combination of very strong empirical results, and clear & detailed explanation of the methods involved combine to make this a clear accept. I was particularly happy to see the thorough evaluations including bots, a field of humans, and world-class players, and an attempt at investigating exploitability. Comments on the paper:* In figure 3, it might be clearer to plot the two graphs on the same scale.* In the Qualitative Assessment of SearchBot section, I would have been interested in any comments the human experts might have made. The paper considers the problem of learning a latent k-vertex simplex K, given a collection A of n points that are obtained by randomly perturbing latent points in the simplex. It improves the prior work [Bhattacharyya and Kannan, SODA 2020] by removing the multiplicative k factor in the running time given an (necessary) assumption about the mass of the top k singular values of A. Empirically, their algorithm is faster than the prior algorithm on both the synthetic and the real-world datasets. Overall, I vote for accepting. The work is tough and technically novel.Pros:    1. The paper provides a detailed discussion on the application of their latent simplex model, which I really appreciate. These discussions make their additional assumption reasonable.    2. The paper provides a detailed discussion of the technical novelty. Specifically, the idea of obtaining a low-rank approximation to A introduces a small angular distance is interesting and accelerates the running time.Cons:    1. The experimental results that your algorithm not only outperforms on the running time but also produces solutions with lower least squared loss are interesting. Could you give some explanation of this phenomenon? SummaryThe paper presents a method for tackling multi-domain few-shot image classification problem where it obtains a task-adapted representation by weighing representations from pretrained domain-specific backbones according to the support set at hand. The desirable property of this framework is that the model can leverage information from other domains to make predictions. The effectiveness of Universal Representations have been discussed in the past work - SUR [1], and this work builds on top of it and introduces a learnable component (self-attention), and showed the improvement both quantitatively and qualitatively.Strengths- The paper is well-written- The hypothesis is clearly conveyed, tested and is interpretable as seen from the attention weights- The model improves over the results of the past works that were based on conditioning backbones using FiLM layers - CNAPs [2], Simple CNAPS [3]. While these past works have used additional modules such as a small CNN set encoder to encode task-representation, FiLM layers for conditioning; the simplicity and effectiveness of this model is appealingWeaknessesI have a high-level comment.- Domain mixing during training:    - If I recall correctly, the way sampling works in Meta-dataset is that a dataset domain is picked and then a task is sampled. Is there a way to try mixing domains in a task? I guess then the class-specific attention scores would vary a lot among classes (because some classes would prefer a different backbone that the other classes). So task-adapted representations would change to class-specific representations. And the only change in eq 9 and the expression of $p_c$ would be to replace $\phi(x)$ for a query image $x$ to $\phi_c(x)$    - I dont know if the above makes sense, but this will allow you to generalize to any real-world setting, and will also allow similar classes in different datasets to share information. Right now, the model is good at figuring out what domain does the task come from and find an appropriate mix of backbone for that task, however, what if its geared to do that for classes?Minor concerns (suggestions, typos, etc.)- Section 3.1    - Mention dimensionality of weights and representationsPreliminary Rating and its justification- I dont see any glaring faults in this paper so I recommend accept.[1] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Selecting relevant features from a universal representation for few-shot classification. arXiv preprint arXiv:2003.09338, 2020[2] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast and flexible multi-task classification using conditional neural adaptive processes. In The Conference on Neural Information Processing Systems (NeurIPS), pp. 79577968, 2019[3] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved few-shot visual classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. The authors provide a clear review of different divergences used in contrastive learning and their relative strengths and weaknesses in terms of training stability, minibatch size dependence, and usefulness on downstream tasks. This motivates the need for a new divergence which they introduce based upon chi-squared divergence.They provide strong empirical and theoretical support for the new divergence, with extensive experiments on large-scale image and speech classification tasks. They also perform comparison studies across batch size and training stability that support their earlier arguments, and a hyperparameter sweep across term weights to make it clearer how to tune them in later work. Further, they demonstrate the decreased bias and variance in MI estimation experiments.The paper is well-written, and provides helpful context to not just motivate the value of the new technique, but quantitatively and qualitatively contrast with existing techniques that helps inform the reader about the broader field.  This paper discusses a possible method for training a deep neural network without using backpropagation.  Backpropagation has been very successful in minimizing output errors for both supervised and unsupervised methods and has stood up to challenges from other methods.  The motivation for finding suitable replacements or approximations is to reduce the computational complexity of training neural networks.  This can take the form of reducing the size of the NN required to accomplish a task or simply train an NN with a smaller number of operations. I believe this is a very important new topic to find viable alternatives to backprop.  These kinds of methods have advantages on better-utilizing memory bandwidth, making cheaper hardware more relevant to the training side of NNs.The authors do a good job of giving background by citing node perturbation methods, lottery ticket hypothesis, and genetic methods.  They all appear to be pointing to an underlying concept that random initializations in overparameterized networks already have embedded, sparse representations.The main result of the paper is that a small number of sequential weight updates using the authors' proposed algorithm rivals the performance of an established method like backpropagation.  The proposed algorithm is simply to perturb weights from a randomly initialized neural network and keep the perturbation if it reduces the loss on a minibatch. This relies on an assumption that a randomly initialized network is close to a final solution. I really enjoyed this paper. Nearly every question I asked myself while reading it was answered in a subsequent section.  As pointed out, this is the first step at a new concept.  As with any good paper, this paper begets a lot more questions than it completely answers.  Suggestions:Section 3: what is the motivation for using a Gaussian distribution to initialize weights?  Not that I see anything wrong with that, but is there some reason this might be better or worse than other initializations?Section 3: We first update the weights of the layer closest&.  This could be an area of additional research as to where to update first.  If we look at neuroscience, we see that layers closer to inputs seems to learn first, so might be good to include some discussion on that here.Section 4: These are good networks to start with, but I would like to see larger networks that are more relevant to problems today&.transformers being trained to useful levels using this method could be a huge and important step. Section 4.1: It could strengthen the paper to include some analysis on the number of MAC operations required and the number of reads/writes to memory for SGD vs RSO.  This could be useful in this paper, or a subsequent one.Section 4.2: Some theory likely needs to be developed here.  It would good to add some discussion about the tradeoffs between these options. I believe this is more for future work.Section 4.5: If the RSO algorithm is more amenable to parallelism, that could be an important advantage.  Some discussion of that vs SGD could also build a stronger case.ZS SUMMARYThis paper uses the statistical physics-inspired energy-based model formalism to study the by now "canonical" problem of retrosynthesis using deep learning. The authors use an interesting variant that combines forward and backward prediction. The authors use template-based and template-free models.PROS- This reviewer believes that energy-based models have an elegance and connection to statistical mechanics that should be explored more in the area of machine learning. This work goes in this interesting direction.- Based on the above, and as far as the reviewer is appraised, this is a unique, non-derivative direction in the field and therefore deserving of consideration for acceptance.- The dual model seems to be very useful given the increase in template-based and non-template-based model performance. This could be applied to other transformer-based tasks in chemistry and graph-based ML- The authors compared their models to a variety of SOTA models and approaches, they also were thorough and explored both DeepSMILES and SELFIES.CONS- Some of the mathematical formalism could be moved to supplementary to allow for better discussion.MINOR FORMATTING- The authors may want to give the manuscript a pass for grammar. There are missing articles in a few sentences. ## SummaryThe paper proposes an elegant baseline addition to policy gradient / self play to encourage truthful signaling in Comm-POSGs. It outperforms self-play on various communication domains and uses a number of interesting concepts. Cool paper - I think ICLR will enjoy reading!## ScoreClear accept - pending a more thorough check of the appendices.## PositivesThe addition is elegant and the imaginary component does not leak into the networks.Does not add a bias to the PG objective.Utilizing an imaginary reward is a very interesting approach - maybe there are domains that this is useful?Clearly good results on the domains tested.## ConcernsHow is beta chosen in the experiments?Limited alternative baselines. Is there another algorithm that will perform well here?## Other ThingsPage 1: P*e*rtially observable&Page 2: I prefer identity matrix over unit matrixPage 3: ...but interacts each other - ?Page 4: weight is not defined yet (maybe call parameters?) - clarify if theta and phi are scalars or vectors. Wording implies they are scalars.Page 4: CRA could do with a small diagram showing the interactions The paper describes a framework for artificial life, where basic building blocks are artificial neural networks (ANN) elements (matrix multiplication, and other linear algebra operations), intended to be open-ended and without any guiding objective. The authors emphasis that in their proposed approach there is no predefined distinction between environment and agents, as they attempt that the 'physics' of their proposed framework allow agents to emerge on their own. Pros:- The paper is very well written. Language is clear at times, the paper is well organized and the there is no overuse of math notation.- The framework advanced by the authors seems solid enough to experiment on the emergence of life. - Results need to be further analyzed, but they are encouraging at this stage of the research.- Paper provides in-depth, or even philosophical, discussion relating topics such as emergence of life and current advances in computer science. It is very re-freshing to read a paper that is not only about surpassing other methods by a marginal score in benchmark datasets.Cons:- Although the paper provides a thorough discussion on related work, it is surprising that it does not make more emphasis on results related to cellular automata. Conway's Game of life seems to comply with their main aim at developing an environment where no distinction between agents and environment is made, and where agents emerge on their own. It would be good if authors provide a bit of contrast discussion on this issue.- There are some strong claims across the paper or concepts being left without neither references or definitions. For example:1. In Sec. 2.1 authors introduce the concept of "general intelligence". What exactly does that mean?2. Also, in Sec. 2.1 authors claim that human brain computation closely resembles recurrent networks, but they do not provide a reference that backs such strong statement. Last time I check, there was little evidence that processes similar to backpropagation were happening in the brain, let a alone that artificial neurons models are now considered poor approximations of actual neurons.3. The whole idea that life is objective-less can be controversial. A recent hypothesis [1] actually suggests that the life's objective is to accelerate entropy, that is, that emergence of life-like molecular structures, and even evolution itself,  are only a consequence, or even a particular case, of second law of thermodynamics. Authors should be more careful when dealing with such motifs.References:1. England, J. L. (2013). Statistical physics of self-replication. The Journal of chemical physics, 139(12), 09B623_1. The paper addresses hierarchical classification, where the classes live in a hierarchy, and the cost of a mistake is the tree distance between the nodes.The paper tests the latest cool algorithms for hierarchical-aware loss functions, versus a very old idea: CRM. In CRM, you make your best estimate of the posterior probability of a class y given input x P(y|x), and then you make a final decision based on minimizing the expected loss.There seems to be a belief that modifying the loss function to be hierarchy-aware is clearly better than doing boring old CRM. But there is not much evidence in favor of that hypothesis. This paper offers negative evidence for that hypothesis, with two experiments:1.  By comparing hierarchical loss to top-1 loss with modified loss functions, there is a tradeoff, and there does not seem to be an advantage in using the modified loss function.2.  For the top-k case, using CRM clearly dominates the proposals for modifying the loss function.These support the use of CRM.I find this paper to be really nice -- I'd far rather have a paper with good experiments with known algorithms, where I can learn something useful; than a paper with a new algorithm with somewhat useless experiments. So I would argue for acceptance.One thing for the authors to think about:When they test the calibration of the modified loss functions, they find them to be poorly calibrated. This is not surprising, since the modified loss functions are not proper scoring rules. They attempt to calibrate by using a softmax  with variable T. Wouldn't it make more sense to train exp(alpha_i x + beta_i) / \sum_{i=1}^N exp(alpha_i x + beta_i) ? that is, a gain and offset for all classes after the first one? **Summary**The authors study the lottery ticket hypothesis  for generative adversarial networks. Specifically, they attempt to answer the following questions: the existence of winning tickets in GANs; the effect of discriminator pruning in finding such winning tickets; the effect of initialization during the rewinding steps; and finally if the subnetworks found transfer across datasets. They provide extensive empirical evidence using that ```winning' tickets exist in GANs. Further they show that iterative magnitude pruning and channel pruning successfully find such `winning' subnetworks. They analyse the effect of discriminator pruning and find that initialization during the rewind step matters more than the actual pruning of the discriminator. Finally, they show state-of-the-art results on GAN compression through channel pruning.**Strengths**1. The paper is well-written and has cited relevant related work. 2. The work is well-motivated and novel. The authors answer some important questions about LTH based methods.3. The experiments are extensive and help prove the authors' claims. I especially appreciate the attention to detail in the experiments, with various comparisions and ablation studies. **Weaknesses and Clarifications**From the given qualitative results, there does seem to be a loss in the finer features (edges and textures) upon sparsification. However, the FID scores do not seem to reflect this. Could the authors provide more visual results to analyse this? Summary:This paper aims to prove and illustrate that attention components are defined during training by gradients that mutually amplify the embedding and score associated with crucial features. In particular, a word embedding with a high magnitude increases the gradient following the attention score for the same word, while a high attention score increases the gradient directed at the word's embedding. In addition to a proof that treats behavior during training as a dynamical system under a large suite of assumptions, they test the analytic predictions on a synthetic dataset following the same suite of assumptions. They then test on a natural language data set and discuss where it diverges from the analytic and synthetic findings,  concluding that the difference is a result of competition between different words associated with a label.Pros:1. We currently lack any substantial theory about attention modules and why they work. Although their model is simplistic, it could provide essential groundwork for analytic understanding of these popular systems. I would even consider it fairly realistic relative to a lot of the assumptions required for theoretical results in training dynamics research. Currently theory of attention is grounded in infinite-width networks, an assumption this paper does not make. 2. The synthetic results appear to substantiate this theoretical result.3. They find an interesting result that, in more realistic settings, the learning dynamics follow particular patterns on the words that are paired together with more versus less predictive words. The framing of these effects in terms of competition between possible topic words is clearly inspired by considering which assumptions behind their proof have failed, which is evidence that the thinking behind their proof is potentially valuable.Cons:1. It's not clear how dynamics like these would generalize to multilayer attention networks like BERT.2. The assumptions behind the theoretical and synthetic empirical results are simplistic:  The existence of a large vocabulary of "non-topic" words required to keep the variance of embedding negligible in out of focus words, the presence of only one topic word. There is also the very common assumption of Lipschltz continuity.3. The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity, but only presents an example of two words as evidence. I want to see quantitative evidence of the pattern.4. The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals. Questions:1. Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency?2. In Lemma 1, there is a reference to the attention block's capacity which is difficult to decipher. What do you mean here by capacity?3. The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training, but not later. Have you checked the actual variance that would be associated with word embeddings late in training?4. What is actually meant by a word being "paired" with another word in the natural language experiments?5. Did I misunderstand something in interpreting gradients amplifying the embedding and score as directed towards v and k respectively in this simplified model? Minor:1. Notation is difficult to follow at times because several unrelated concepts use almost the same symbols: $s_i$ indicates score, but $S_i$ indicates a sentence; $\tau$ indicates learning rate, but $\mathcal{T}$ (which looks identical as a subscript) indicates a set of sentences.2. In discussing early alignment of attention to syntax, Clark et al. 2019 was concurrent with https://www.aclweb.org/anthology/P19-1580/ Paper proposes and extension of neural path framework to include composite kernels which comprise of a) FC networks (Hadamard product of gram matrices), b) residual networks (sum of products of base kernels), and c) CNN max-pooling layer. Furthermore, they also include learnt gates instead of static initialized random gates and show learnt gates perform better.  Paper is well written with main technical contribution being theorem 5.1 which shows for infinite width case $w \rightarrow \infty $ the NTK is independent of the weights. It also presents experimental result on MNIST and CIFAR for four proposes regimes of (Definition 5.1) that models are robust to combinatorial variations in layers and inputs.  This results in novel makes an important theoretical contribution towards understanding of why DNN with composite kernels perform well in practice. The authors propose an algorithm for estimating the correct backprop gradients that is described as biologically plausible. I have very little to contribute as I think this is a clearly-written paper  as such I recommend acceptance.I would be happier if the authors gave a very very strict definition of the kind of plausibility they wish to capture with falsifiability criteria and deep theoretical underpinnings, as I think it's a bit of a semantic trap without reference to some explicit level of analysis, etc. But this is a field-wise terminological issue, and thus can easily be seen as out-of-scope. Minor: A paper the authors might appreciate is, given some of the work they mention to motivate their neuro/bio plausible claims: Xu, Y., & Vaziri-Pashkam, M. (2020). Limited correspondence in visual representation between the human brain and convolutional neural networks. bioRxiv.https://doi.org/10.1101/2020.03.12.989376 The problem of good predictive uncertainty-based out of distribution (OOD) detection is essential for classification systems to be deployed in safety-critical environments. The authors present a method RETO that achieves state-of-the-art performance in a transductive OOD detection setting. Like other predictive uncertainty-based approaches RETO can ultimately be used downstream on problems like active learning or abstaining on OOD samples in combination with selective classification.Benchmark data such as CIFAR, SVHN, and MNIST are used to compare conventional and proposed baseline methods, such as k-NN, Vanilla Ensembles OE, Mahal, Mahal-T, and MCD. Experimental results, including those of supplemental materials, show that the proposed method provides good accuracy while reflecting the hardness of the task.However, there is not enough discussion on how the proposed method can achieve such a high level of accuracy compared to the conventional methods; early stopping is used in RETO, but is it promised to reproduce the same level of performance in other tasks? **Summary:** This paper proposes a way to do batch mode model agnostic active learning. In this task, the agent has to query a batch of data points from a set of unlabeled examples for which it will get labels. The paper puts an additional requirement that the algorithm is model-agnostic. The key idea here is to sample a batch of points that provide the most "information" about the remaining unlabeled examples.  Authors argue that this will result in higher performance on the unlabeled examples. The proposed approach called ICAL (Information Condensing Active Learning) uses Hilbert-Schmidt Independence Criterion (HSIC) to measure dependence between a chosen batch and the unlabeled examples. The goal is to pick a batch with a maximum value of HSIC which should intuitively give us a batch which is representative of the unlabeled set. HSIC can be easily estimated unlike other dependence measures such as mutual information. Given a batch size $|B|$, a dataset of unlabeled examples $D_u$  and $m$ samples to estimate HSIC, the ICAL algorithm computes a batch for label acquisition in $O(|D_U|m^2|B|)$ steps, where a greedy strategy is used to search over batch.  Results are presented over MNIST, variants of MNIST and CIFA and show improvements over five previous active learning approaches and a random acquisition baseline. **Strength:**1. ICAL makes a useful contribution to the active learning literature which has wide range use. Particularly, experiments are presented on realistic domains, with neural networks, and show gains over a few different baselines.2. ICAL is model-agnostic which means it can be applied to decision trees, neural networks, complex ensembles, etc. 3. Experiments show that ICAL acquires a more diverse batch for acquisition.**Weakness:**1. For HSIC, one has to decide the kernel which may be difficult for some domains.2. No comparison with BADGE (Ash et al. 2019) is provided. 3. Time complexity of $O(|D_U|m^2|B|)$ seems expensive particularly if $m > 1000$. What is the value of $m$ that one should expect in practice?**Questions:**1. For Repeated MNIST task, how do the different baselines compare in terms of the number of times they pick a datapoint and its replica for label acquisition?2. Is the set $X$ and $Y$ assumed to be countable in Section 3 since summations are used everywhere. **Writing:**There are issues with writing in several places. Some are enumerated below:1. Grammar error on the second line of the third paragraph of intro in "directly focus on 'minimize' the error rate". It should be minimizing.2. Unexpected full stop after "model's parameters" in the third paragraph of the intro. 3. No $d \theta$ in Section 3 when taking integral in the first paragraph.4. What is $B_{D_u}$ in the definition of $B^\star$ in the third paragraph of Section 3. Also, how is the training set used in this definition? ***quality***This paper is quite well-written. The contribution is critical in instance-dependent label noise learning. Moreover, both the theoretical and empirical justifications are convincing.***clarity***Although this paper contains heavy mathematics, it is not difficult to understand. I can see that the authors have spent a lot of efforts in paper writing. ***originality***In this paper, the authors proposed a novel sample sieve approach for instance-dependent label noise learning. The proposed model is novel, and the theoretical contributions are also new to the community.***significance***The proposed algorithm is simple, but the theory behind is rich. I like this kind of work, so I feel that the significance of this paper is high for future research.***pros and cons***Pros:1. The topic is very important for realistic machine learning problems, and is helpful for reducing the human annotation efforts.2. The theoretical study of this paper is quite impressive.3. The experimental results show that the proposed method achieves SOTA performance.Cons:1. The authors claim that their method does not need to estimate the label transition probability or noise rate, which I think is nice! However, it would be better if the authors can explain why the proposed method can avoid this, namely which component helps to avoid the estimation for noise rate?2. Since this work is an extension of cross-entropy loss to dealing with label noise, I think the comparison with Symmetric Cross Entropy for Robust Learning with Noisy Labels(ICCV 2019) is necessary, as this paper also aims to design a robust loss via modifying cross-entropy loss.3. I feel that the sample sieve/filtering process (i.e. Eqs. 1-4) looks like self-paced learning (SPL) (see Self-Paced Robust Learning for Leveraging Clean Labels in Noisy Data, AAAI 20), as SPL also selects some important data for training in each iteration. Maybe the authors can discuss the relationship between these two methods? 4. The authors misuse the terms sample and example. Statistically, we say that we have a sample X={x_1,x_2,&,x_n} from some distribution, in which every x_i is an example. 5. Some recent typical works on label noise learning can be cited, such as Are Anchor Points Really Indispensable in Label-Noise Learning?(NeurIPS 19) and A Bi-level Formulation for Label Noise Learning with Spectral Cluster Discovery (IJCAI 20). Summary:The paper shows that a two-layer neural network (although an extension to deeper models seem unproblematic) may outperform a class of linear functions in terms of the excess risk learning rate, and in a minimax optimality analysis, and when approximating a target function from the neural network class. The paper essentially shows that linear functions have a problem with the non-convexity of the neural network class, and approximate the slow rate of 1/(n)^(1/2) for increasing dimension. A neural network trained with noisy stochastic gradient descent on the other hand has a faster rate, depending on several parameters.==================================================Pros:Well written and polished paper.Technically sounds as far as I can tell. (Randomly checked some parts in more detail.)Setting and results may be interesting for a large audience.Main results and message of the paper are to the point.==================================================Cons:Very technical and on some parts I would have liked some more intuition and discussion. See detailed feedback and also questions for rebuttal.==================================================Scoring:Overall I think this is a worthwhile contribution in understanding the difference in deep and shallow learning, and as the paper is very sound I will vote for accept. I will acknowledge, however, that there is a flurry of related work, as it is a very popular topic, and I can not vouch for the novelty of this contribution. The authors, however, covered much ground in that regard.==================================================Questions for rebuttal:It appears to me that the neural networks are not part of the linear functions class, and thus having a neural network target makes the linear functions being misspecified. Is that true? If so, does that play a role in the learning rate gap? In case it is not true, what is the essential difference then between the linear functions and the neural networks? Regarding that, what is phi_i in the definition of linear models?Instead of noisy gradient descent you actually use semi-implicit euler scheme for optimization, do you have any thoughts on how that might effect actual performance?As far as I can see your current analysis does not hold for relu-activations, how easy might an extension to that be?Are you aware of any lower bounds for the neural network case, are your rates optimal?==================================================Additional feedback:The result that the minimiax rate of linear functions over a space F is the same as over its convex hull was not known to me. For me it would have been very useful if you could provide some intuition on why that is the case.You show that the rate of the neural network is independent of the dimension. Do you have any intuition on why that is the case?Under Equation (5), instead of "more faster,...,more faster" write "the faster,..., the faster" #### General commentsThis paper aims at proving superiority of neural network models to any linear estimators, including kernel methods.  To attain this purpose,  this paper focuses on  two layer neural network class with an infinite width. For the non-parametric regression models within this neural network class, this paper establishes a sharp excess risk error of the least square methods with  noisy gradient descent update, although such optimization may be heavily non-convex. Moreover, a lower bound of  all linear estimators under the $L_2$-norm   are accordingly given when the true function is within the two layer neural network class, thereby showing superiority to kernel  methods.  Overallthe contribution of this paper is obvious and the literature review is full to some extent.This paper is organized well and stated clearly. #### Specific Comments1	After Theorem 1, the sentence "for relative high dimensional settings, this lower bound becomes close to a slow rate $\Omega(1/\sqrt{n})$, which corresponds to the curse of dimensionality. "  I argue  that this sentence may be uncorrected, since the mentioned rate is independent of the input dimension, which is not a real curse of dimensionality. (2)  A constraint on  $f_{W}$ should be added, otherwise, it is impossible to identify $a_m$ and $\bar{w}_{2,m}$ simultaneously. (3) What is the role of noisy term in NSGD algorithm, is it a similar conclusion when the standard SGD is applied? (4	What  is the additional difficulty encountered when analyzing a thin but deep neural network? This paper presents a impossibility result for value-function approximation in batch-mode RL. The chart below puts this work in context. This work essentially shows -- through a constructive example of an MDP -- that the amount of data needed for approximating Q values must increases exponentially with the horizon in episodic RL tasks even if we assume that the Q-values are realizable and that the features gathered by the behavior policy -- that collected the data -- are uncorrelated. This problem arises because the data gathering policy can fail to get data from all states even though the features themselves are uncorrelated. The authors claim that this setting is interesting because these conditions only require polynomial-number of samples in the standard supervised learning case, and I agree with that. Broadly I checked the main construction in the paper and the result seems correct. I think this paper will be a good addition to ICLR.      |                | Strong Concentratibility|        |                | extra conditions on     | (Xie Jiang 2020)      |                |  the dynamics of the MDP|  []      |                |      |Coverage of the | Concentrability         |       Szepesvári and Munos 2005      |behavior policy | (experience gathered    |                      []      |                |  from every state)      |      |                |       |                | Uncorrelated            | This work      |                | Features                | [x]      |________________|______________________________________________________________      |                                           Realizability     Bellman Closedness      | bellman closed = Every intermediate value function arrived at after       | bellman update is also linearly realizable      |_______________________________________________________________________________      |                             Representability of Q values## Some comments1. Assumption 2 will actually imply that  Ã_max = 1/d as well because (Ã_min + ... + Ã_max) e d Ã_min = 1 and sum of eigenvalues equals the trace, and exchanging trace and expectation will give (Ã_min + ... + Ã_max) d 1. I.e. the covariance is necessarily scaled identity matrix Ò the features are uncorrelated. The paper investigates robustness of Capsule Networks (CapsNets) under adversarial attacks and makes several interesting observations around the behaviour of CapsNets under attack regime which have been used later to design a new attack against these types of networks (vote attack).  Through several experiments, they show the proposed vote-attack can reduce the robust accuracy of CapsNets significantly across different methods. Authors analyze the effectiveness of this attack from different perspectives (i.e.  transferability of adversarial examples, the adversarial robustness on affine-transformed inputs).- Correctness and Clarity: The paper seems correct and concise and they experiment different aspects of the methods for effectiveness. The paper is also in general  well-written and easy to follow.- Reproducibility: They have also provided several tables and figures on the main paper and appendix to help with understanding of the results and method. They also provided detailed description of the methods, training and experimental setup.  It would be great if authors could also release the code to reproduce the results.Relation to prior works: I am curious to see if the proposed attack can still be effective for Self-Routing Capsule Networks (Hanh et al.)  and the other family of CapsNets in which the different routing mechanism has been used. Additional Feedback and Suggestions: There are some editorial issues, like missing ref to equations (e.g. Equation ??) , and missing name tags for numbered items (e.g. Table 4.) Summary:This well-written paper re-visits the idea of logit adjustment to tackle long-tailed problems. The paper begins by setting up a statistical framework and use that to deliver two ways of realizing the logits adjustment effectively. They further prove the potential of such an approach by benchmarking it with several related baselines on both synthetic and natural long-tailed datasets.###########################################################+ves:+ The paper motivates the proposed method very well, by exposing the cases of failures of certain existing approaches and addressing those shortcomings in the proposed method.+ The explanation about how the proposed methods standout - that is post-hoc logit adjustment with respect to the weight normalization and logit adjusted loss with respect to the LDAM loss is clear and well written.+ The proposed methods are very versatile as it can both be incorporated into the training, used after the training, and in combination with each other.+ The experiments and the analysis are both comprehensive, and the paper has a nice technical depth to it.+ The paper is very well-written.###########################################################Concerns/Potential Improvements:- There is no justification as to why results on synthetic datasets are provided only for the imbalance ratio of 100 while it is a community norm to benchmark on a range of imbalance ratios (typically 100, 50, 20, 10, 1)- Weight normalization has a term multiplied to the logits and the proposed post-hoc logit adjustment has a term added to the argmax of the logits. Therefore both the terms are independent of each other. It would be interesting to see how both of these methods work in conjunction. Does it improve the results or the cons of one just penalizes the pros of another?###########################################################Minor editorial/typo issues:- For post-hoc logit adjustment, the paper provides a separate subsection named COMPARISON TO POST-HOC WEIGHT NORMALISATON. It would have been nice to see something similar for logit adjusted loss. The content is already there in the paper, and just has to be modified a bit to stand out separately.- Near equation 9, the paper mentions that (8) immediately suggests two means of optimizing for the balanced error and goes on to provide the two methods. This was not very evident from (8). Some description to link would have been nice.  [Summary]This paper provides a statistical framework for long-tail learning by revisiting the idea of logit adjustment based on the label frequencies. The proposed framework then yields two variant techniques that follow the paradigm of weight normalization or loss modification. Compared with the existing methods, the proposed methods are generalized and Fisher consistent for minimizing the balanced error. Finally, empirical results on four real-world datasets validate the effectiveness and statistical grounding of the proposed methods.[Pros]-The idea of this paper is novel and interesting. The proposed framework is proved to be Fisher consistent for minimizing the balanced error and generalizes several recent methods for long-tail learning. Meanwhile, some insights about the logit adjustment technique are also revealed to help understanding.-The experiments are sufficient and supportive to validate the effectiveness and statistical grounding of the proposed methods.-The paper is well organized, which makes it easy to grasp the core idea.[Cons]-From the left panel of Fig.2 it seems that the error bar of logit adjusted is even thinner than that of the Bayes predictor. It would be better to provide some explanation.-It seems that the proposed framework could work well with linear classifiers. Does it also applies to other classifiers such as cosine classifier?-Captions of tables should be put above the table contents.-Besides, there are some grammatical errors and typos to be corrected. Some are lists as follows:  1)Page 3, an non-decreasing transform should be a non-decreasing transform;  2)Page 4, which overcome should be which overcomes;  3)Page 5, has negative score should be has a negative score.  4)Page 5, another label with positive score should be another label with a positive score. Summary:The paper proposes one of the most scalable approaches to sequential continual learning with known task boundaries and related tasks, while taking steps towards enforcing data privacy and removing some of the task label constraints. At all levels in expressive deep models, SVD is used on learned representations to identify important bases of task gradient spaces and a memory is populated with such directions. Learning progresses only in directions orthogonal to gradient memory. Several recent evaluation methodologies are used to empirically validate the approach with significant success. Strong points: - Principled and relatively simple approach, yet scalable to interesting deep models.- Manageable computational overhead.- Memory of gradients introduces a layer of data privacy, and advantage compared to many other memory-based approaches.- Strong empirical results, although not apples-to-apples in all cases, see the comment posted earlier.- Clearly written paper.- Analysis of scalability in terms of memory and computation is a welcome bonus.Weak points:- The sequential task learning setup is of limited practical use and not too realistic.- Its hard to say how the algorithm would be used in practical continual learning settings, e.g. reinforcement learning of single tasks without forgetting, or where clear task boundaries do not exist.- Little effort is made to see the approach relevance for non-similar sequential task learning, where there could be interference between learned representations in terms of negative forward transfer.Recommendation and Rationale:I strongly recommend acceptance because the method is simple, practical and the paper is well written both in terms of clarity and also analysis.Further questions:- Do you see any positive forward transfer, e.g. later tasks are learned quicker due to previous tasks?- What are the limitations and roadblocks to extending this method to sequences of hundreds of tasks which are not necessarily related? Please discuss in the manuscript. I found the idea quite novel. Lately in continual learning the focus has been more on NAS type ideas and algorithms, but this work is a nice divergence from this direction. The idea of optimizing in a space orthogonal to the previous task is novel. The execution of the idea is nothing special since it's using standard linear algebra, but I gave the authors full merit to the idea itself. My higher score is mostly due to the fact that the experiments are limited. The benchmark algorithms definitely miss some recent works from 2019 and 2020. They should be included as otherwise the superior performance of the algorithms is questionable. See for example:https://arxiv.org/abs/2006.04027Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Processing Systems, pages 899908, 2018 This paper introduces an approach for masked language modeling, where they mask wordpieces together which have high PMI. The idea is relatively simple, has potential for high impact through broad adoption, and the paper is clearly written with extensive experiments.The experiments on GLUE, SQuAD, and RACE are very well set up. For example, evaluating multiple learning rates for each downstream task is expensive, but really adds to confidence I have in the results. Reporting the best median dev score over five random initializations per hyperparameter, then evaluating that same model on the test set, definitely improves the reproducibility of the results. In addition, showing how performance is affected by the amount of pretraining data is very useful, and the experiments range from small scale to large scale. The ablations adjusting the vocabulary size (which, in turn, changes the size of wordpiece tokens) is a valuable contribution, and I would have asked to see something like this if it wasn't included. Table 4 is a nice addition -- it's interesting that the MLM loss is not predictive of downstream performance.I suspect this approach will become widely adopted (or built upon) in future work pretraining language models. I give this paper an 8, only taking off points because the idea is relatively intuitive and doesn't really open a broad new area for future work. I don't see any obvious methodological flaws, which frankly, we can find in most papers.I would be interested in seeing if this reduced the variance of the fine-tuning process. That might be something the authors could include for the camera ready, maybe in the appendix. ### SummaryThis paper proposes an improvement to how tokens are selected for masking in pre-training large masked language models (BERT and family). Specifically, it stipulates that purely random choice of words (or word pieces) makes the MLM task insufficiently hard. It then goes on to propose a data-driven approach for selecting n-grams to mask together. The approach, based on an extension of pointwise mutual information for n-grams, is shown to outperform random token and random spans masking strategies on performance of downstream tasks.### Strong and weak pointsStrong points:- The paper is very well written throughout, and easy to follow.- The problem is well motivated with empirical evidence. I think Section 2 demonstrates well the case for random masking being too easy.- The multivariate version of PMI proposed is simple and well motivated.- The evaluation experiments are convincing the results are robust across the tasks shown,Weak points: - The one main drawback in this study is the lack of comparison with entity-based techniques for masking.  In particular [1] has recently defined salient span masking based on named entity recognition and dates. Salient span masking has been adopted in [2] where it is shown to boost performance of open-domain question answering by 9+ points (Table1 of [2], models tagged with + SSM).I think it would be extremely interesting to compare these techniques (SSM specifically, but entity-based techniques in general) with PMI-Masking. Specifically, it is currently unclear whether the PMI based n-gram masking vocabulary simply ends up rediscovering popular named entity mentions, or whether there are more interesting sub-phrases (e.g., idiomatic sub-phrases) that a NER system would not select. Finally, it would be interesting to empirically test whether these extra non-entity n-grams provide further performance boost over the entity-based salient span masking.[1] REALM: Retrieval-Augmented Language Model Pre-Training (https://arxiv.org/abs/2002.08909).[2] How Much Knowledge Can You Pack Into the Parameters of a Language Model? (https://arxiv.org/abs/2002.08910)### RecommendationI recommend this paper for acceptance. The analysis and ideas throughout the paper are well executed. I also think the topic should be of high interest to the ICLR and NLP communities, given the importance of MLM pre-training on most state-of-the-art models at the moment. Despite the lack of comparison with entity-based techniques, having a statistically principled alternative, solely based on co-occurrence, without linguistic grounding, seems interesting.### Questions for authors1. The main question here related to the entity-based approaches discussed above. I think it would be interesting to address this issue given how closely related it is to this work, and the good performance the cited papers demonstrate using it. I can think of a couple of ways to address this comparison: (1) qualitative analysis of the masking vocabulary to better understand the differences between PMI ngrams and entity mentions, (2) experimental analysis incorporating some entity-based masking into the experiments in the paper.2. Irrespectively of how you choose to address the entity-based comparison, I was interested in some analysis, or sampling, of the PMI-Masking vocabulary to understand what type of n-grams are being selected (entity mentions, idiomatic phrases, noun-phrases, etc.). Would be interesting if you could make this vocab available or add a small sample in the appendix.3. Throughout the paper there is an assumption that contiguous words are considered for masking. This was not immediately clear in the beginning of the paper (I realized it only in Section 3.2 with - What about contiguous spans of more than two tokens?).  But one question came to mind: what about correlated non-contiguous spans? For example eigenvalue and eigenvector are unlikely to be present in the same n-gram, but have reasonably high chances of showing up together in the same passage. Have you considered extending this work to non-contiguous spans? Is there any expectation that this would help learning, or is it just a bad idea?4. Was there an attempt to mix masking strategies during pre-training? Although Table 6 is convincing in demonstrating that single-token perplexity is not correlated with performance of downstream tasks, the differences seem curious. One is left wondering if there is any benefit in adding a small number of easy masking cases (i.e., random-tokens or random-spans)? The paper starts off from the recent realization that there exists divergent examples for any set of hyperparameters for algorithms in the Adam family, such as RMSProp. It sets out to study the effect of the beta2 parameter on convergence for a fixed specific problem. The analysis shows that there exists a beta2 < 1 that leads to convergence for realizable problems, and to convergence to a bounded region of interest for non-realizable problems, without requiring a bounded-gradient assumption. Experiments confirm this new theory.Overall, the paper is well-written, clear and easy to read. One of its strongest points is how well the analysis and the relevance of the results is motived. For instance, the importance of removing the assumption on the bounds on the gradient because it effectively removes one of the convergence/divergence regimes is well executed.There is also significant efforts on providing clear simplified examples from rather complex theorems, which is very appreciated (e.g. Corollary 4.1).Further, there is a real effort to contrast the results with the previous work, and to explain how it complements them, resolving clearly what initially appears as direct contradictions.The results are relevant, both from the point of view of the theory, where it adds to a body of work explaining how and why the Adam family of algorithm performs well on modern machine learning taskloads, and from the point of view of the practitioner, outlining what hyperparameter tuning is necessary to achieve convergence. They are also original, in the sense that they provide novel insights, while removing problematic assumptions that permeate most of the related work.A couple of things could be improved:- as pointed out in the paper, if beta2 = 1, the algorithm degenerates to SGD. While there is a remark explaining why as long as beta2 < 1 the two algorithms differ, it would be informative to compare the convergence regimes with high beta2 to SGD directly, to validate that there exists a set of hyperparameters that not only provide convergence, but improved convergence properties compared to SGD (otherwise the results are a lot less relevant), as well as give an order of magnitude of what value is typically necessary for beta2.- condition (4) in theorem 4.3 is quite difficult to apprehend, with a slightly worrying beta2^n term. More exegesis would be beneficial for reader comprehension.Overall, this is a nice, well-written and relevant paper that clears the bar for publication in its current version. This work revisits a famous counterexample on the convergence of Adam (originally presented in Reddi 2018). The authors show that, if the EMA parameter beta2 in RMSprop and Adam is chosen high enough, then both methods converge to a bounded region in the stochastic setting. In addition, the authors provide some results for the full-batch case. Crucially, and differently from many other papers on the topic, the gradients are not assumed to be bounded and the beta2 hyperparameter is not chosen to increase to 1.The paper is well written and the logic of it is convincing. I like the introduction and Figure 1 (this nicely illustrates the relevance of this paper). It is also very well organized. Unfortunately, I did not have the time I wish I had to dig into the proofs (just had a quick check), but the methodology of the authors and the results are convincing.This is overall a very nice paper, with clean and easy to read results, that clarifies an important point: it is misleading to claim that Adam does not converge (which was pointed out in Reddi 2018 to introduce AMSgrad). I have heard this (wrong) claim many times in the optimization community  hence I think this paper deserves attention (therefore my clear accept). This work truly does merge the gap between theory and practice in non-convex stochastic optimization.Just a few suggestions: I think the authors should cite and discuss the results in Defossez et al. 2019 (On the convergence of Adam and Adagrad). Also, I think Figure 1 deserves better quality. It's done in matlab so in the xlabel command you can put 'interpreter','latex' and 'fontsize',20. Finally, I spotted 1 typo: in Remark2 cases of non-divergence cases. This work proposes a novel approach for unconditioned image synthesis of complex scenes by intelligently coupling two major tasks; unconditional label generation and label-conditioned image synthesis. To overcome the limitation of failing to generate high-fidelity complex scenes using current GAN-based approaches, this method proposes to divide this into two parts: unconditional segmentation map synthesis network and conditional segmentation-to-image synthesis network. The former is based on the ProGAN with some modifications in losses to deal with discrete semantic labels, and the latter leverages the existing method (Park et al. 2019) based on SPADE residual blocks. Experimental results demonstrate superior performance on the complex scene synthesis. Additionally, the latter part for segmentation-to-image synthesis task also outperforms the existing method (Park et al. 2019) thanks to joint end-to-end training with the former using ProGAN.* Pros1) Decomposing the complex scene synthesis into two sub-tasks (segmentation map generation and segmentation-to-image synthesis) looks novel, also validating outstanding performance over SOTA.2) Segmentation-to-image synthesis is also boosted, thanks to joint end-to-end training with the unconditional segmentation map synthesis network.* Cons1) Though this paper is well-written, some parts need more details.- In Section 3.1, it would be nice to explain how to generate semantic segmentation maps progressively by referring to Figure 1.- Eq (3) uses loss functions from two sub-networks (semantic bottleneck synthesis network and semantic image synthesis network). Why did you not use L_D_SPD and L_G_SPD in (2) for training the whole network with a pair of real RGB images and real segmentation maps. It seems that L_1^VGG and L_1^Feat can also be used in (3).- Semantic bottleneck synthesis in Section 3.1 needs more explanations, e.g., how to convert real segmentation maps into probability maps, using argmax and soft argmax in forward and backward passes. The paper explores an under-researched problem, that of minimizing the number of policy updates in an RL setting (or being deployment efficient). This is an important aspect of using RL agents in real production environments where there may be many reasons why updates are costly and limiting them is an important consideration in the choice of RL method (or whether to even use RL).The paper shows that so-called "off-policy methods which, by their naming as such, it is implied that they should work in a sparse-deployments environment are, in fact, not suited (and often not evaluated) for this regime.By introducing a set of simple and strait-forward steps to the update and deployment process, the paper shows performance that approaches the continuous-deployment performance of comparable un-constrained methods.The main ingredients of the proposed method seem to be:Model based approach to support model-based offline exploration (an ensemble of models to prevent exploitation of model inaccuracies)Re-estimation of a model by Behaviour Cloning (BC) with data from last deployment (appears to have a large contribution though I dont understand why)Conservative off-line policy updates (constrained by KL to BC policy) using offline rollouts (with forward model ensemble)The evaluation presented in the paper and extensive (13-page) appendix clearly show the advantage of the proposed method in the sparse-deployments regime and the overall competitive performance with regards to sample efficiency as well. Code was made available as supplementary material.I am unclear on why each training iteration should start with a policy learned from behavioural cloning of the last deployments data instead of the model that was deployed which would be available at that time. Figure 4 clearly shows BC to be the better approach In practice but I would appreciate some intuitive reasoning for this (unless this is standard practice).Perhaps my main concern with this paper, given the problem it addresses - that of a real deployment setting, is that it seems that some of the parameters that need to be tuned to achieve great performance require fitting to the specific task (i.e. deployments !). As far as I understand it, parameters such as the number of offline policy updates per deployment or the weight of the KL-divergence in the policy update step are crucial to good performance yet the paper does not explain how to choose them without engaging in the true environment. If that is correct than this seems to defeat the aims of the paper and question the overall methodology for deployment-efficiency. Summary ==The paper proposes to use a contrastive cross-entropy loss during fine-tuning, for improving transfer accuracy of both supervised and unsupervised pre-training methods in image classification. The paper builds on the intuition that both class discriminative information and intrinsic structure of the downstream task are useful for fine-tuning, and existing fine-tuning approaches only use the former. The authors conduct experiments on four image classification datasets, using a modern ResNet-50 architecture.== Pros ==The authors use two contrastive losses during fine-tuning, which has not been deeply explored, since most works typically use only class cross-entropy loss. One contrastive loss acts on the classification head, while the second, an extension of InfoNCE, acts on a projection head (on top of the representation layer).The proposed method achieves very good results across the four image classification datasets (CUB, Cars, Aircraft and a custom version of MS-COCO), using both supervised and unsupervised pre-training. The authors perform 5 runs for each experiment and report the average as well as statistical significance metrics (although it's not clear if the provided interval is standard deviation or a confidence interval).When using unsupervised pre-training, the authors explored 6 different pre-training algorithms, and Bi-tuning improves the standard supervised fine-tuning in all cases.The authors also show that their method outperforms 4 baselines: standard (supervised) fine-tuning, BSS, DELTA, and L2SP by a significant margin across different data sizes available for fine-tuning (25%, 50%, 75% and 100% of the original dataset size).Despite the fact that they have three terms in the final loss, no additional hyper-parameter needs to be introduced (in addition to choosing the number of keys, which depends on the amount of training data).== Cons ==Despite the fact that contrastive losses have not been widely used for fine-tuning (as far as this reviewer is aware), the authors should probably tone-down statements such as "Bi-tuning, a general learning framework to fine-tuning both supervised and unsupervised pre-trained representations to downstream tasks". There's a plethora of works using multiple loss/regularization terms during fine-tuning, such as DELTA or BSS, which the authors compare to. The proposed work only proposes a different type of loss/regularization. One could even argue that "Bi-tuning" is only a particular instance of multi-task learning.The authors could have run additional experiments with other modern deep neural network architectures alternative to the ResNet50 (Inception, EfficientNet, DenseNet, AmoebaNet, etc.) to show whether the benefits transfer to other architectures.The fact that the authors coined the method "Bi-tuning" but the loss has three terms (CE, CCE and CCL) is confusing. Probably, the "Bi" is due to the two heads, but still.== Typos ==The paper contains several typos and some parts are not clear or could be improved, please read it carefully, correct them. Some typos that I've found.Not a typo, but you should indicate whether intervals in Tables are +/- std. deviation or confidence intervals.Not a typo, but there are much earlier works showing that fine-tuning (usually) performs better than training from-scratch. In the introduction you cite He et al. (2019). A quick search on the Internet yields a survey from 2009 on the topic, for example.Equation 1, bold q, k_+ and k_i, since these denote vectors.Figure 1, missing h_0^k (according to equation 3, j starts at 0)."Previously, we propose" -> "Previously, we proposed"."As shown in 1" -> "As shown in Table 1"."Results in Table 2 reveal that Bi-tuning brings general gains for all tasks, even provided with sufficient training data". I don't understand the phrase after the comma."SimClR" -> "SimCLR" in Table 4.The result of Bi-training with MoCov2 (Table 4) is not statistically significant than the supervised baseline, thus it should not be bolded.== Reasons for score ==The authors show that the proposed method achieves significantly better results than strong baselines across multiple datasets, data set sizes, pre-training paradigms (supervised vs. unsupervised) and algorithms. Despite introducing an additional head and two additional losses during fine-tuning, the proposed algorithm does not introduce more hyper-parameters than other alternatives, which makes it easier to be applied by other researchers. This reviewer believes that some claims are a bit overstated (e.g. claiming that the proposed approach is a "framework"), but there's no doubt that the proposed algorithm achieves excellent results and the experimental work is solid. This paper examines RNNs trained to perform a range of text classification tasks, and demonstrates that they can be understood using a dynamical-systems analysis. Specifically, the paper shows that RNN dynamics explore low-dimensional subspaces determined by the categories to be distinguished. Within those low-dimensional subspaces, RNN dynamics integrate evidence towards each category. The approach is based on recent works by Maheswaranathan et al, but extends them to a large range of tasks. This is a big step towards interpreting RNNs trained on NLP tasks, and I strongly recommend it for ICLR 2021.Strengths:- a strong conceptual framework for analysing trained RNNs- large battery of categorisation tasks on both natural and synthetic datasets- clear and compelling interpretation of trained RNNs.Concerns:The results for ordered classification (Fig 4) are a little puzzling. One would expect that 1-star and 5-star reviews should be further away from each other in PC space than 1-star and 3-star reviews. Related to this, I don't understand why "sentiment" and "intensity" scores should be orthogonal - intuitively they do not seem to be orthogonal for the words used in the synthetic data. One possibility is that the specific 2d organisation in Fig 4 results from the manner in which the readouts were implemented. Presumably, the readouts were implemented as 5 independent categories as in other tasks? If that's the case, the readout structure does not necessarily take into account the ordering in the task. A different readout structure (eg 5 different levels on a single readout) might lead to a different low-d structure.Other feedback:- I did not quite understand what is plotted in Fig 4c and similar. The legend says "fixed points", but are these fixed-points in response to different inputs? Which inputs? What do coloured regions and coloured lines respectively represent?- how important is non-linearity in these tasks? Eg how big is the difference in performance between RNNs and linear accumulation models based on LSA?Related work:- a recent paper by Schuessler et al (arXiv:2006.11036) suggests that the low-d dynamics in the sentiment-classification task can be traced to low-rank structure in connectivity, as in neuroscience tasks. This paper presents a generative speaker model that selects actions at each timestep that facilitate the generation of the instruction, and demonstrates that a combination of discriminative and generative action prediction models outperform either alone, with the generative model primarily facilitating better generalization to unseen environments.It would help to have more details in the paper rather than relegated to the appendix, maybe at the expense of the long TENT analysis? I didn't get too much out of that, but would have liked to see more details of the main algorithm in the paper (e.g., details of how the discriminative/generative combo was done).Improvements:- Assumption that p(a_t|h_t) is uniform is reasonable, but might be improved by using priors from the training data since there are strong action-conditioned biases in VLN R2R [ https://arxiv.org/abs/1811.00613 Figure 2 ]; some geometric form of this likely holds for the panoramic setting as well (especially for what were formerly "forward" actions -> continuing along headings close to previous heading).- The (A) and (B) notation in Table 1 was super confusing and it took me a while to figure out it was just an alias for Disc + Aug versus Gen + Aug since that isn't written explicitly anywhere.- That the combination of discriminative and generative policies gives best results is a major point in the paper, but the combined policy description is relegated to the appendix which feels weird.Nits:- typo Introduction Chang et al. is \citet but should be \cite- Possible typo after Eq (3), "penalizes all the actions" should this be "penalizes all actions except a_t", or is a_t penalized as well (e.g., global optimum is zero)?- 5.3 typo "Figure 5" -> "Figure 2"- Typo 5.5 "as good as" -> "as well as"- Typo 6 "parametrization" This paper introduces the generative modeling of the task of vision and language navigation. At each timestep, for each action, the generation of the instruction is scored. This modeling achieves the-state-of-the-art or competitive results on standard benchmarks Room-to-room (R2R) and Room-for-room (R4R). The proposed generative modeling also allows us to interpret how the model process the instruction. To do so, token-wise prediction entropy (1-TENT) is introduced. The core idea is that if a token is critical for the trajectory, the entropy of scoring the token for all actions will below. Thus by visualizing the 1-TENT for each timestep we can analyze how and when a model fails or what kind of capabilities are missing in the model. Below I list my questions (Q) and suggestions (S):S1 Introduction 4th paragraph: It was rather hard to understand what's the difference between Fried et. al. and this work. Please try to simplify or add a figure or give examplesS2 Introduction penultimate paragraph: Please clarify the concept of 'richer learning signal'. This paragraph is rather more cryptical compared to the rest of the paper.S3 Figure2: please add a sentence or two to clarify how this is generated and the punchline of the caption. Also, add the legend for x axis.S4 Section 5.3: I believe you are referring to Figure 2 not Figure 5 which is in the appendix.S5 Figure 3: You are trying to achieve a lot with two analyses. Please move one of them to the appendix -- which will give more space to panorama images. Also, the figure is already hard to interpret without the guidance for the level of 1-TENT scores for tokens. Q1: Section1: The assumption that the action probabilities are uniform given the state makes sense. However, in reality, it's not true. Have you done any experiments on this or any ideas on how to address that?Q2: Section 4: This might be a lot to ask but is it possible to have results on TouchDown or any other outdoor navigation datasets? Problems are structurally the same however the way people give instructions in outdoor scenes will be different. It would be great to see the generalization of the generative approach to outdoor scenes. SUMMARYThe contribution of this work is two-fold: it collects an extremely wide range of benchmark problems for black-box optimization, and it proposes a new algorithm called ABBO. Both contributions are significant.The proposed ABBO method is designed to be a swiss army knife solver, suitable for a wide range of different types of problems. To this end, naturally, it builds on existing components. Overall, the combination looks extremely convincing to me.CRITICISMAlready the first sentence of the abstract is problematic:"Existing studies in black-box optimization suffer from low generalizability, caused by a typically selective choice of problem instances used for training and testing different optimization algorithms."This statement is very general. However, black-box optimization is an extremely wide area, ranging (at least) from mathematical optimization over evolutionary computation all the way to machine learning. The statement applied to various degrees to most studies in various subfields. I completely agree with the statement only when restricted to machine learning papers, where experiments are typically limited to very few RL benchmarks. In other areas things are far from perfect, but generally much better (it is understood that avoiding a bias completely is near impossible), since benchmarks e.g. with (YA)BBOB aim at general insights, not (only) at demonstrating peak performance. Please qualify this statement accordingly.There is one more problematic statement in the abstract:"A single algorithm therefore performed best on these three important benchmarks, without any task-specific parametrization."Well, this "single algorithm" is really an algorithm selection machine. Technically it is "a single algorithm", but it is much more useful to think of ABBO as a selection and configuration method. In my understanding this makes a big difference. For optimal performance we need both: powerful components and powerful configurators. Please make absolutely clear that this "single algorithm" really is a configurator, which encompasses multiple components.Personally I object some of the methods forming the basis of "Algorithm 1", although overall the choices look very solid. [Side note: I very much like the use of Powell's algorithms for fine tuning of approximate solutions found with more robust methods.] I have one suggestion: Diagonal CMA-ES is an outdated method. Please consider low-rank approaches as an alternative, like LM-CMA-ES, VD-CMA-ES and LM-MA-ES.@inproceedings{loshchilov2014computationally,  title={A computationally efficient limited memory CMA-ES for large scale optimization},  author={Loshchilov, Ilya},  booktitle={Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},  pages={397--404},  year={2014}}@inproceedings{akimoto2016projection,  title={Projection-based restricted covariance matrix adaptation for high dimension},  author={Akimoto, Youhei and Hansen, Nikolaus},  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference 2016},  pages={197--204},  year={2016}}@article{loshchilov2018large,  title={Large scale black-box optimization by limited-memory matrix adaptation},  author={Loshchilov, Ilya and Glasmachers, Tobias and Beyer, Hans-Georg},  journal={IEEE Transactions on Evolutionary Computation},  volume={23},  number={2},  pages={353--358},  year={2018},  publisher={IEEE}}I appreciate that the code is available. I understand that you start out by forking nevergrad. However, that's not a viable long-term strategy, at least when thinking in terms of utility for a wider community. Please put effort into merging your (im my opinion very significant) contributions back into nevergrad.Algorithm configuration and selection for optimization is not entirely new. In the bbcomp results, the AS-AC-CMA-ES by Nacim Belkhir seems to be a very successful competitor method. I do not know whether the code is available or not -- I found Nacim's profile on github, but no code base corresponding to his competition entries. If possible, it would be very interesting to compare to his results.This brings me to one of the few weak points of the paper. Experiments are performed for ABBO, but the authors rely (solely) on the nevergrad leader board for comparing with competitors. This has pros and cons. The huge advantage: results are not biased by running competitor methods with sub-optimal parameters. This is a huge plus; in effect, this is a rare case where I fully trust all experimental results. However, this means that some interesting baselines may be missing, in particular methods that predate nevergrad (which is still rather new), like AS-AC-CMA-ES.MINOR POINTSThe fonts in all plots in figures 2 to 5 are far too small, in particular when printed. On screen I need to zoom in quite a bit. I understand that there are space constraints, but in this form the presentation of the results is of limited value. I really do not understand why the machine learning community keeps talking about losses (and sometimes regrets) when it comes to optimization. The term "fitness" in evolutionary computation is no better. A long-established terminology exists already: the thing we minimize is an "objective function", and its value at a specific point is an "objective value". I vote for paying more attention to using the standard terminology (in general, not only in this paper), since it is compatible across multiple sub-communities of optimization.Last paragraph of section 2: "Rocket" is listed twice.It seems that some of the URLs in the references do not work (any more). The bbcomp website has moved here: https://www.ini.rub.de/PEOPLE/glasmtbl/projects/bbcomp/index.htmlI did not find a replacement for the Artelys link, but maybe referencing the bbcomp results does the job.RECOMMENDATIONOverall this is a very nice and valuable paper with two significant contributions. I strongly recommend to accept the paper. The paper presents a novel method to incorporate experts' knowledge into BO. This is done through introducing  Prior-guided Bayesian Optimization (PrBO). Different experiments where conducted to compare PrBO vs different baselines and to show the effect of the user provided priors in the cases where it is well-specified or mis-specified. The design of PrBO enables it to guide the search in the early iterations and as optimization progresses, more emphasis is given to the model and the effect of the prior is washed out.Strong points:- The paper presents a method for incorporating priors over good solutions in an elegant way.- The method can benefit from the prior information and incase of misspecified priors, it can still recover.- Extensive experiments were conducted.- The paper is well written.- This type of work is definitely needed to increase the adoption of BO methods.Weak points:- Although the experimental design is extensive and covers several aspects. I wish the experiments included one realistic experiment for tuning the hyperparameters of a famous architecture. This would have been a great example to demonstrate the benefits of this method versus the manual search used by most researchers. However, this is not a huge concern.Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.- I recommend to accept this paper.- Choice of priors affect the performance of BO. This paper provide the intuitive way to add priors by modelling probability of good configuration. This is what the user thinks. I am sure that this kind of work will encourage more researchers to use BO in their problemsQuestions:- It is clear how PrBO is incorporated in TPE, but what about GPs and RF? How is PrBO implemented in this case?- Are the experiments in the main paper using GPs or TPE?- Could you please explain more what 10,000×random search is?- How are the weak and strong priors for PrBO generated?  In this paper, the authors study the problem of unsupervised representation learning from data augmentations. Specifically, the authors claim that existing methods are prone to getting stuck at local minima owing to easy-to-learn local representations that optimise the commonly used MI objectives, and then propose a hierarchical method that tackles optimisation at multiple layers of the feature hierarchy.Strengths:- A highly novel solution to an important problem in representation learning.- Strong results obtained with respect to the state of the art.- Extensive analysis and comparisons.Weaknesses:- "We demonstrate that current methods do not effectively maximise the MI objective" + footnote: "We show this by finding higher mutual information solutions using DHOG, rather than by any analysis of the solutions themselves." => This claim calls for an analysis of the solutions and since this is missing in the paper, I would rephrase the claim.- It would have been nicer to have experimental analysis on different features (e.g. colour) being more prone to local optima, though this is intuitive. This could have significantly increased the impact of the paper.Minor comments:- "a reasonable mapping need only compute colour information" => "a reasonable mapping needs only compute colour information".- "Learning a set of representations by encouraging them to have low MI," => This should be high MI?- "CIFAR-10, CIFAR-100-20 (a 20-way" => The parenthesis is not closed.- "A network was learned to associate" => "A network was trained to associate". Summarize what the paper claims to contribute. Previous work developed CORnet-S, a biologically inspired network that leads the Brain-Score benchmark of similarity with the primate ventral stream. A limitation of CORnet-S and other deep networks with high Brain-Scores is that they require many more weight updates than seem biologically feasible. In this paper, the number of weight updates used to train CORnet-S is reduced by two order of magnitude, while retaining a fairly high Brain-Score. This is done by combining three approaches, including reduced training, initialization of weights using compact distributions that describe trained weights, and updating only a minority of layers. List strong and weak points of the paper. Strong Points:-The paper addresses an important problem that has not been given much attention previously-The work builds on the state-of-the-art model in this domain-The three approaches to reducing updates are complementary and interesting in different ways; the second and third thought-provoking with respect to their biological relevance-The experiments and analysis are thorough-The paper is well written-The context of the work is clearly described and well referencedWeak Points: I wasnt able to discern any substantial weaknesses. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend acceptance. The number of updates needed to learn realistic brain-like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain-Score. I was surprised that it worked so well. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. -Is the third method (updating only down-sampling layers) meant to be biologically relevant? If so, can anything more specific be said about this, other than that different cortical layers learn at different rates? -Given that the brain does everything in parallel, why is the number of weight updates a better metric than the number of network updates? Provide additional feedback with the aim to improve the paper. -Bottom of pg. 4: I think 37 bits / synapse (Zador, 2019) relates to specification of the target neuron rather than specification of the connection weight. So Im not sure its obvious how this relates to the weight compression scheme. The target neurons are already fully specified in CORnet-S. -Pg. 5: The training time reduction is less drastic than the parameter reduction because most gradients are still computed for early down-sampling layers (Discussion). This seems not to have been revisited in the Discussion (which is fine, just delete Discussion).-Fig. 3: Did you experiment with just training the middle Conv layers (as opposed to upsample or downsample layers)? -Fig. 3: Why go to 0 trained parameters for downstream training, but minimum ~1M trained parameters for CT? -Fig. 4: On the color bar, presumably one of the labels should say worse. -Section B.1: How many Gaussian components were used, or how many parameters total? Or if different for each layer, what was the maximum across all layers? -Section B.3: I wasnt clear on the numbers of parameters used in each approach. -D.1: How were CORnet-S clusters mapped to ResNet blocks? I thought different clusters were used in each layer. If not, maybe this could be highlighted in Section 4.  This article deals with the problem of partially labeled dataset: if some entities are missing, how SOTA approaches are going to behave? To answer this question, the authors degrade CoNLL classical dataset by masking a pourcentage of the labeled data. Then, they wonder which part of the missing performance is due to the lack of labels and which part is due to the incorrect labelling of discarded supervision.The experiments are well explained and interesting on synthetic dataset. Then the authors propose a new cross entropy loss to test their hypothesis on real data by sampling high confidense negative samples as ground truth. It is a way of performing distillation on the model using negative sampling.Consistant & relevant work that deserves to be publised in ICLR.* We wonder what would give a classical distillation process on this task. [even if relevant comparison are made with results from the litterature]* Given the architecture, we wonder what is the detailed learning procedure: it seems clear that the network is first trained on the real ground truth and then refined using the distillation loss. Section 3.1 is a little bit short on this point.* Regarding the model, equation (4) is not discussed nor analysed.* The approach is rather simple but elegant. The paper introduces a new method to retrieve entity by auto regressively generating unique entity name as a sequence of word pieces, instead of pinpointing the ID representing an entity. This method stands out in novelty compared to existing various entity retrieval methods, which always assigns a single ID to each entity. Practically, the proposed method has two nice properties: (1) When the entity vocabulary is very large, this approach requires less parameter space and memory compared to other methods (as shown clearly in Table 4) (2) The model can address novel entities, which was unseen during the training. The paper is clearly written and extensively evaluated on three relevant tasks, entity disambiguation, entity linking, and entity retrieval.I have one big concern with the current format of the presentation. In the current draft, It is not very clear whether the strong gain is coming from large scale pretr aining or the proposed method itself.  To this end, the ablations shown in Table 7 and Table 8 should be reported in the main paper, with clear explanations. As we all know, the model architecture (which is the focus of this paper) cannot be properly evaluated when the training set up is different (i.e., how much pretraining has been done, on what dataset?).  Could you elaborate on this? In appendix, there's only result tables without in-line explanations. The experiments on cold start, as well as table 5 which shows performances on entities divided by name match is pretty cool!If space allows, adding some more analysis on what types of entities do this model do better compared to other methods would be interesting (would lengthier names easier or harder? would it do better on popular entities or more long tail ones? are these systems complementary to existing methods or mostly succeed and fail on the same set of examples?)   ### SummaryThis paper proposes to tackle the entity linking task using a sequence-to-sequence neural model, trained by producing unique entity names, in autoregressive fashion. The paper makes a case that this approach can scale better with larger entity vocabularies than previous methods with dedicated entity representations both in terms of memory as well as computation costs. The model is studied under a number of tasks including entity disambiguation, entity linking and document retrieval for question answering.### Strong and weak pointsStrong points:- The work is well motivated: I believe there are, or will be, many systems where retrieving information about billions of entities is useful. Making simpler and more efficient models to deal with larger entity vocabularies is important.- Doing seq2seq entity linking is really a novel idea and is surprising that it works so well for several of the datasets presented.- The idea of constrained decoding using a Trie is neat and makes intuitive sense.- The empirical evaluation in the paper is quite exhaustive, in terms of number of datasets.Weak points:- There is no discussion or analysis on the performance of this new model on tail entities (entities that have few examples in the training set). A believe such a discussion would be interesting for two reasons: (1) if one wishes to use this type of model for much larger entity vocabularies, it is likely that a larger fraction of entities will have low number of examples, and (2) one effect of contrastive learning (e.g., negative sampling) has on systems with explicit entity representations is some implicit training of _all_ entities, which is lacking in the described autoregressive proposal.Finally on this topic of tail entities: the IDs experiment, shown in Table 5, indicates that when entity mention and decode target are not related, performance suffers. Similarly, for tail entities that are ambiguous with another popular entity, the model may be biased with the popular entity (results from Cold Start may support this hypothesis). Hard to say without some analysis.- While the constrained decoding is really interesting, it was not clear from the paper how crucial this was for good performance. Is this something absolutely critical for performance overall, or does it provide a modest performance improvement? While it may be clear to the authors, it would be highly informative to explicitly describe the performance impact on unconstrained beam decoding.### RecommendationOverall, my recommendation is to accept this paper to ICLR. I believe the problem of representing entities in natural language systems is of high interest to the ICLR and NLP communities. The ideas in this paper are novel, the paper is well written and the empirical evaluation is well executed.### Questions for authors- See comment above regarding analysis of tail entities. Could any further insights on this topic be added to the paper? - See comment above about ablating the constrained beam decoding. This would not need to be a long or complex analysis. Simply 1 or 2 data points for the reader to understand the magnitude of the importance of this decoding. The paper proposed to use autoregressive approach to solve entity-based problems. They proposed a uniform framework and showed that their model achieved the state of the art performance on 3 different types of tasks (~20 datasets). The GENRE model also significantly reduced the memory usage compared to previous models that stored a big memory table. It's also capable of linking novel entities at inference time. This paper is clearly written. The experiment results are convincing.One limitation of this paper is that they required the vocabulary of the entities to be the ones that have a Wikipedia page. And that their model relied on copying the surface form of entities (as suggested in the paper). From the experiments, the "copying" approach worked very well on Wikipedia entities, that are often common entities. How can you improve the model to link rare entities not in the Wikipedia pages? Another concern is the efficiency at inference time. Compared to the models with a large entity memory whose retrieval is performed with Maximum inner product search, how is the efficiency of your decoding strategy? The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs. The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x, z) VAE space using reparameterization. The method is shown to achieve high quality samples on several modern image datasets, good FID scores and mode coverage. Ablation studies show the contribution of the different elements.This is, in my opinion, a very good work, which combines a novel and well-motivated idea with clear writing and extensive experimental evidence.Some comments and questions:- Does the separate twos-stage training enable the model to reach the optimal point that can be reached in joint training, or is it an approximation? If its an approximation, I think it should be discussed or perhaps bounded.- Does the combined model allow computing the likelihood? Can it be evaluated and compared to other models in terms of bits/dimension (e.g. as in VAE or NVAE)?- It might be interesting (not something that I think is mandatory) to measure the NVAE log-likelihood of samples generated by the combined model compared to samples generated just by the NVAE.To summarize:pros:- novelty- significance- experimental evidence- quality of writingcons:- combining two separately trained models - perhaps sub-optimal This paper provides several surrogates for the Information Bottleneck (IB) and Deterministic Information Bottleneck (DIB) loss functions that are more friendly to optimization. For the decoder uncertainty part, the authors show that using Dropout and cross-entropy loss provides an unbiased estimator for the decoder cross-entropy which upperbounds the decoder uncertainty. For the regularization terms in IB/DIB, the authors inject noises to the latent features to lower-bound the conditional entropy of latent representations, and further proposes three types of surrogate objectives for the regularziation terms. Emprical results on CIFAR/ImageNette (a subset of ImageNet of 10 classes) show that the proposed surrogates yield similar behaviours in terms of adversarial robustness and information plane and the scalability of the proposed method.Strengths of the paper:- As this paper claims, this is the first work that proposes some surrogate of IB loss functions that can be easily optimized and thus be scaled to large models and datasets (CIFAR/ImageNette). Results on both datasets show similar behavior (adversarial robustness, two-phase information plane) to IB loss based optimization.- The injection of random noises into the latent representation is interesting and able to enforce lower-bound on the conditional entropy of latent representations, which further induces some surrogates that are optimization-friendly.- This paper is well-written, fully-prepared and contains a large amount of results that are of wide interests of researchers working in this topic.I don't have specific criticisms for this paper. Summary:This paper gives much needed attention to the theoretical underpinnings of modern meta-learning algorithms such as MAML; it introduces a novel formal argument, discovers surprising implications and follows through to show that such predictions hold experimentally, despite being counterintuitive.Strong points: - Novel analysis of a practically important aspect of MAML-like meta-learning algorithms: setting the learning rates for training and adaptation; surprising theoretical result (of negative learning rates being optimal in some cases during meta-training) is well evaluated in controlled conditions.- Honest discussion of limitations and good intuition is provided for applicability of the work. This is not too hard to do, but so many papers dont provide it. Great job!- Writing is clear enough, although the paper is dense.- The authors dont discuss previous empirical works (e.g. Meta-SGD/LEO) where meta-learning of the learning rate leads to negative inner-loop learning rates for some parameters, but such experiments actually provide further evidence to back up their claims, this time in SOTA deep models.Weak points:- Unfortunately, the paper includes only toy-task experiments, even by the standards of  meta-learning research.Recommendation and Rationale:I strongly support acceptance because this paper contains much needed fundamental work on theoretical underpinnings of modern meta-learning. The authors consider training neural networks with a variety of losses and regularization (such as weight decay).  The authors introduce a novel convex-dual formulation which allows them to characterize optimal solutions as being extreme points of particular convex sets.    For multi-layer linear networks, the authors prove that the optimal weight matrices have rank equal to the number of outputs of the network, and whose singular vectors align with those of neighboring layers.  For ReLU nets in one-dimension, the authors prove that optimal solutions act as linear spline interpolators (the kinks between linear pieces occur at data points), and the authors prove closed form expressions for optimal weights at intermediate layers when input data is whitened.  The conclusions of this paper are strong and apply to a wide variety of neural networks.  The extension of linear spline interpolation results from 2-layers to more than 2-layers is a significant contribution.  The empirical comparisons of the presented analytical formulae for optimal weights to solutions via SGD are impressive (even though they are only performed on MNIST and CIFAR10 datasets).  The results of this paper themselves form a substantial contribution to the field and motivate clear followup work to test the performance of the provided formulae to larger and more realistic datasets, along with weakening the whitening assumptions.  Analytical expressions for weights of trained deep ReLU neural networks is a significant development, and the authors may want to provide more commentary on its significance (possibly with a small review of the most closely related works in this sense).  While I am not an expert in this subfield, there appear to be multiple significant contributions.Minor comment:Spelling error in footnote 3. Summary:The authors introduce a framework for sufficient conditions for proving universality of a general class of neural networks that operate on point clouds which takes as input a set of coordinates of points and as output a feature for each point, such that the network is invariant to joint translation of the coordinates, equivariant to permutation of the points and equivariant to joint SO(3) transformations of the coordinates and output features of all points. Notably, this class contains Tensor Field Networks (TFN). The authors accomplish this by writing the network as a composition of an equivariant function from a class F_feat and followed by a linear pooling layer. When the F_feat class satisfies a D-spanning criterion and the pooling layer is universal, the network is universal. For a simple class of networks and for TFNs, the authors prove D-spanning. Linear universality of the pooling layer follows from simple representation theory.Strengths:-It is useful to know whether prevalent classes of neural networks are universal-The authors use a general construction for proving universality of equivariant networks for the point cloud group, rather than being specific to certain architectures.-Reading the proofs along with the main text, the argumentation is clear and relatively easy to follow for me as reviewer, unfamiliar with similar universality proofs.Weaknesses:-The paper would benefit from providing more intuition behind the proposed constructions, lemmas and theorems, in particular this holds for: theorem 1 based on the split between the linear universality and D-spanning; the construction of Segol & Lipman (2019) and how this relates to the Q functions and lemma 2-In addition to the previous point, the proofs, currently critical for understanding the paper, are given in the appendix, which is not ideal for the self-containedness of the main paper.-As the authors of the TFN paper note: in practice not all higher order irreps of the tensor product of the filter and the features are computed. This seems to indicate a big difference between the theoretical analysis - which includes all irreps and thus is computationally intensive even when modelling low order polynomials  and the practical application of TFNs. It would be interesting to know how expressive such practical low order TFNs are. Another difference between the described networks and practical TFN is that in the described networks, all relevant parameters are in the pooling layer, which sums a large number of terms (looking at the proof of lemma 2, exponential in D), while in practical TFNs, the parameters are in the filters.Recommendation:The authors proof the useful statement of universality of a prominent class of neural networks, which is why I recommend the acceptance of this paper.Suggestion for improvement:-Make the big picture clearer by providing more intuition.-Comment on the differences between the class of networks described and TFNs used in practice.Minor points/suggestions:-P3 Add definition of W^n_T as n direct sums of W_T-P3 where W_feat is a lifted representation of SO(3), what does lifted representation here mean? Just any rep?-I get a bit confused by the wording in Def 1. Unless I am mistaken, it appears like the quantifiers are reversed. Should it mean for every polynomial &, there exists f_1, & in F_feat and linear functionals Lambda_1, &, : W_feat -> R ?-Around Eq 5, perhaps the authors could clarify the clarify the domain of the Q functions, which I suppose is Q^r : R^3n -> T_T, where T=||r||_1-Around Eq 7, are X_j and x_j the same?-In lemma 4, is A_k any linear map or an equivariant linear map? -In Appendix B, perhaps a new subsection B.2 would make sense before theorem 1?-In the proof of thm 1, it says p: R^{d \times n} \to W_T, should that be W_T^n?-In the proof of lemma 2, it says we see that that exists a linear functional ## Summary ##The paper proposes a modified loss function for supervised learning, in which the original loss at w is replaced with a maximum of the loss in a small p-norm ball around w. An approximate way to compute gradients for this loss is presented, and evaluated in high detail on a variety of supervised learning problems where it is shown to consistently improve the overall generalization error. Furthermore, based on PAC-Bayes theory, a generalization bound for learning under that loss is presented. ## Explanation of Rating ## The main strengths of the paper are the simplicity and convincing evaluation of the method. Another strength is the sound theoretical generalization bound. The paper is also very well written, and I therefore clearly recommend acceptance. Perhaps a weakness of the paper is the lack of a "broader/high-level perspective" on the method (see detailed comment #1) and its comparison to variational inference methods which also optimize PAC-Bayes bounds (see detailed comment #2). ## Detailed Comments ##1. In the past, Gaussian convolution smoothings of loss functions have been considered a lot in the context of homotopy continuation methods or variational inference. To me, the proposed loss function \max_\eps L(w + \eps) - \delta_{|\eps| < \rho} can be seen as a convolution of the loss, but on the tropical semiring (max, +) with the convolution kernel being the indicator function \delta_{|\eps| < \rho}. Such types of convolution have been heavily studied in the field of convex analysis, where they are known under the name infimal convolution / epi-addition. This opens up the question on why this specific convolution kernel has been chosen. The practical results in this paper are quite strong and have been elusive so far for variational inference and Gaussian smoothing methods, even though they have been around for a long time. Do these good results mainly stem from the efficiently implementable algorithm, or are they more due to favorable geometrical properties of the loss due to performing the convolution in the tropical geometry? 2. What is the main advantage of the proposed approach over a variational inference (VI) method (e.g. with mean-field Gaussian approximation) to minimize the PAC-Bayes bound? In variational inference, the free parameter \rho is also be optimized by minimizing the right-hand side of the PAC-Bayes bound. Have you tried minimizing over \rho, or perhaps consider a diagonal approximation of \rho? Minor comments / typos:- p.12 "Adding a Gaussian perturbation should increase the test error" -> "Adding a Gaussian perturbation should not decrease the test error."  This is expected to hold in practice at a minimum. But it is not clear if it holds for any w. - p.12 Typo: "Then we KL divergence" -> "Then the KL divergence". - The steps (12) and (13) in the proof are not easy to follow. In particular, it was unclear to me what meant with "each bound" before Eq. 12, and why the bounds should hold with this specific choice of probability. Furthermore, I couldn't follow what happens from (12) to (13). - I did not notice where the assumption ||w|| >= 1 is used, perhaps it can be mentioned in the proof. - \lambda is another hyperparameter, but it is not mentioned later on how it is chosen.  The paper is a nice read. It builds on a line of research on multi-modal video understanding that utilises transformers where these works: 1) fix one of the transformer models (e.g. BERT) and 2) utilise tokens and thus do not train the approach in an end-to-end fashion. This typical trend is due to the memory requirements for training a multi-modal transformer end-to-end.To allow for end-to-end learning, the paper argues for shared parameters (across the network), primarily:a) sharing weights in CNNs of the same model [understandable]b) sharing weights between layers of transformer for the same modalityc) sharing weights between modality transformersd) performing mid-fusion by having modality-specific transformer followed by cross-modality transformer,e) sharing position encoding parameters between modalities and transformer layersf) decomposing transformer weights, so some are distinct and others are shared-- among other suggestions [I didn't list fully].In addition to the above, the paper showcases the need for context-aware negatives, rather than random sampling, in line with a number of concurrent works that address this issue, some submitted to ICLR [which I reviewed as well coincidentally].The paper then runs a number of experiments to prove their approach, all showcasing the combined advantage of end-to-end learning with shared parameters. This is tested on the "usual suspect" set of datasets: Kinetics, Audio Set, with downstream tasks on short-range datasets (e.g. UCF) as well as long-term (e.g. Charades). Performance improvement over the baseline is consistent.Two aspects of the paper are disappointing,First, the motivation that the approach will enable end-to-end learning with transformer, should be re-written to say: "This would enable end-to-end learning with multi-modal transformer for a handful of labs who have 64 V-100 GPUs which can be trained for 220K batches [I'm presuming that's many days/weeks]." It is quite impossible for almost all researchers to utilise the findings of this paper. It's true that the number of parameters has dropped significantly, but in any forward/backward pass, the memory requirements of multiple slow-fast (ResNet-50) with all transformers in memory and a necessarily large batch size keeps the same limitation of an "end-to-end-to-end" approach more likely to be used by the community. Apart from knowing of this finding, I am not sure how this will transform the community's go-to solutions.Second, the experimental results (tables and commentary on tables) are not designed for easy consumption. This makes the readability of the experimental section below acceptable bar IMO. This should be fixable, but disappointing that it is submitted in the current form. Let me give you a few examples of how difficult it was to read the tables of results:Ex1: Table 1, the caption talks about a, b and c but these are not referenced in the actual tables. It took me several minutes to realise you are referring to the two tables on the first row as the gap between the two tables is not easy to observe. Ex2: Table 1, names of sampling methods "similar/dissimilar" do not align with how the paper describes hard negatives. Using varying terminology you need to rely on the brief description to know what's happening.Ex3: Table 2, the decision to list the dataset references like this makes the table and checking the references an impossible task, many abbreviations (e.g. KS for Kinetics-Sounds) are not common. On the first two table within Table 2 you use Ours, which I presume is M-BERT in the right table? It is not clear why V- and A- were not tested independently for the tables on the left, but are ablated on Charades and Kinetics Sound. These tables were very hard to follow/read and check for correctness. A few minors:*) I am not sure the results on Charade represent the SOA on this dataset. They seem to only reference the first 2017 paper as a baseline? The same for Kinetics-Sounds, this seems to be a very old baseline?*) The manuscript talks about the audio being a "real valued audio signal" and it's only in the appendix that the log-mel-scaled spectrogram is explained. This can be deceiving to the reader.*) Given very few can replicate these results, the fact that an input of 1 second was only tried in all experiments limits our knowledge of the impact of this critical parameter. *) On the issue of Task 2 (correct pair prediction) other works have discussed the need for asynchronous understanding from the audio-visual signal that are worth referencing, e.g.Kazakos and Zisserman (2019). Audio-Visual Temporal Binding for Egocentric Action Recognition. ICCVMorgado et al (2020). Audio-Visual Instance Discrimination with Cross-Modal Agreement. ArXiv April 2020 [recent work understandably] https://arxiv.org/abs/2004.12943v1  1/ Summary of the paperThis paper introduces the use of local batch normalisation layers in order to circumvent data shifts issues in FL, called FedBN.Building on a simplified model of BN (neural network with 1 hidden layer and BN rescaling) that was previously introduced by [1] to study the impact of BN on convergence, it is proven that the convergence of the proposed FedBN on training data can only be faster than the convergence of FedAvg on the same data points.Parametric experiments on a simple heterogeneous dataset (built as the union of digit classification tasks) show that the proposed method yields a better performance, and a more stable one, than standard FedAvg.Experiments on 3 real-world datasets containing heterogeneity to simulate different centers show again that the proposed FedBN method improves upon FedAvg.2/ Acceptance decisionWhile ideas related to BN and FL have been floating in the community this year, this paper proposes a novel approach with extensive and convincing numerical results, and interesting theoretical results on the effect of fedBN.Provided the paper is updated to better acknowledge other very related works, I think it should be accepted as it will be a very valuable contribution to the FL community.3/ Supporting argumentsA/ NoveltyA very close reference not acknowledged by the paper is [2], which builds on the same ideas of domain adaptation as this paper and proposes to keep local BN weights in the FL setting, called SiloedBN, showing that results are improved and more stable with respect to FedAvg.However, the proposed FedBN is different from siloedBN [2] in the sense that SiloedBN proposes only to keep local BN statistics, while FedBN keeps local BN layers altogether (trainable and untrainable parameters thereof).Therefore, although not entirely new, FedBN is still novel.B/ Theoretical resultsSetting aside the drift issues encountered when the number of local updates increases, the paper uses the same formalism as [1] to get a linear convergence bound for FedAvg on the training set, which is controlled by some key constant mu_0 (the higher, the faster the convergence).For this analysis, the contribution of this paper is to put FedBN under the same framework as [1] and to show that the resulting constant mu_0^* is larger than the constant mu_0: therefore, FedBNs training error diminishes faster than FedAvgs.Although the underlying model is a simplification of the experimental reality, the problem tackled is very complex, so this result is still a significant contribution for the community.C/ Experimental resultsThe experimental results are in three parts: - An introductory toy example, which helps to better understand the importance of local BN layers in the case of domain shift.- Benchmark on a simple hand-crafted FL digit recognition dataset built by concatenating different digit datasets with different domain shifts (e.g. MNIST, SVHN&), thereby borrowing from the domain adaptation literature. While I have some remarks to make the results even more complete (cf next section), I think these results help to better understand the behaviour of FedBN and FedAvg under heterogeneity.- Experimental results on 3 hand-crafted FL datasets with a varying number of centers (4, 6) and different tasks. In almost all cases, FedBN significantly improves the final testing accuracy upon FedAvg and FedProx in almost centers.These experiments are exhaustive and convincing, and are definitely an asset of this submission.D/ WritingAlthough dense, the paper is well written and easy to follow. There are minor typos.4/ Additional comments1. How could FedBN be adapted to transfer a model on a new center from another domain?2. There is a minor error in the proof of Corollary 4.6, which does not invalidate the results. Indeed, the authors claim that since G^{*, \infty} = diag(G_1^{\infty}, \ldots, G_N^{\infty}), one has, for all i, \lambda_min(G^{*, \infty}) > \lambda_min(G_i^{\infty}). This is false in general: one can only claim that \lambda_min(G^{*, \infty}) \geq \min_i \lambda_min(G_i^{\infty}). Since all the minimal eigenvalues of G_i^{\infty} are lower bounded by the minimal eigenvalues of G^{\infty}, the final result still holds.3. This is extremely minor, but in Corollary 4.6, I am not sure that in all generality one can have strict inequalities: only >= statements may be available4. Related to the previous remark, although very interesting, the result of Corollary 4.6 is frustrating as it does not quantify the speed improvement. Even if a closed-form quanitification is out of hand, it may be interesting to produce numerical experiments on synthetic data on this simplified model to quantify the gap between the convergence bounds.5. In section 5.1, how is computed the testing accuracy reported in figure 4? Is it the mean of the per-center accuracy on the local testing datasets? The training sets are artificially balanced, but how balanced are the testing datasets?6. In order to better understand the significance of all the numbers reported, it would have been nice to report the performance of a model trained in a pooled-equivalent fashion, i.e. with all data points in a single center.7. In Fig 4 b), one studies the effect of the local dataset size on the performance, and, not surprisingly, the performance of all methods diminish in this case. Why isnt FedAvg reported here? In particular, why was the 10% fraction chosen in the other experiments, even if it led to suboptimal results?8. In Figure 4 c), are the points with different x values related in any case? Or are they coming from independent experiments. The text « we started with including& then, we simultaneously added n clients » is a bit ambiguous.Typos and other remarks9. The use of both m and M in Sec 4 can be confusing10. In Lemma 4.3, assumption 1 should be replaced by assumption 4.111. There are 2 typos in theorem 4.4: missing parentheses around Equation 2, and n should be replaced by N.12. In Figure 5, the colours are hard to parse for colour-blind people.Refs[1] Dulker, Gu, and Mont\{u}far, Optimization theory for relu neural networks trained with normalisation layers, in proceedings of ICML 2020[2] Andreux, Ogier du Terrail, Beguier, and Tramel, Siloed federated Learning for Multi-Centric Histopathology Datasets, in proceedings of MICCAI DCL 2020 The paper proposes a method of improving the generated samples of differential-private synthetic dataset using GANs by boosting them post training. They support their proposed method using theory, and then empirically show that it works on 3 types of machine learning tasks.This paper presents a novel way of utilizing the sequence of generators and discriminators during training as they are already part of the privacy budget. So it significantly improves the quality of GAN-generated samples for different experiments under the same privacy budget.The experiments provide evidence of the utility of the proposed method in all three tasks. The community can definitely benefit from this paper. **Summary**This paper addresses the problem of video synthesis --- generating diverse, realistic videos. This paper's core idea is to leverage a fixed, pre-trained GAN model for image synthesis and train a motion generator to produce a sequence of latent vectors to generate image sequences (using the pretrained GAN and the generated latent vectors) are temporally coherent. The specific technical novelties lie in (1) predicting the motion residual and (2) adding contrastive image discriminator to ensure that generated contents in a video are similar. The paper provides an extensive set of experiments demonstrating the proposed method's effectiveness over the state-of-the-art video synthesis models.**Strength**+ The quantitative and visual results are extensive. The performance over existing methods across multiple datasets is significant. The paper also provides an ablation study (Table 4, 5) highlighting the importance of individual components.+ The capability of cross-domain video synthesis could be beneficial, particularly when high-quality video datasets are difficult to collect.+ The appendix and the supplementary material provide extensive details about the experimental settings and results that would help reproduce the results.**Weakness**- When discussing the difference over [Tulyakov et al. 2018], the paper states &applies h_t as the motion code for the frame to be generated, while the content code is fixed for all frames. However, such a design requires a recurrent network to estimate the motion while preserving consistent content from the latent vector, & difficult to learn in practice. I do not fully understand why this is the case. It would be clearer if the paper can explain why such a design causes difficulty in learning and why the proposed design could alleviate such problems.- For motion diversity, why maximizing the mutual information between the hidden vector and the noise vector can prevent mode collapse?- It seems to me that the proposed method can only handle 1) subtle motion, such as facial expressions and 2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps). In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.   This paper draws connections between training neural networks, and trajectory optimization via differentiable dynamic programming (DDP) from optimal control.The central idea is to think about the propagation of inputs through a deep neural network as a dynamical system, where the inputs/activations are the signal being propagated, and the weights of the network are control inputs that influence the trajectory of the activations through the network. From this perspective, training the neural network is like trying to control the trajectory (hence, trajectory optimization).The paper does a good job of presenting this connection, with helpful figures and text to aid the reader.Given these connections, the paper then goes on to draw explicit connections between trajectory optimization using differentiable dynamic programming (DDP), and standard optimization algorithms for training neural networks (e.g. gradient descent or even approximate 2nd order methods such as KFAC).The crux of the algorithm is similar to backpropagation in that it involves forward and backward passes, but different quantities are computed for the backward pass. While this seems like a straightforward application of known techniques from control theory to deep learning, the paper does a good job of highlighting similarities and differences to backprop.The paper tests the proposed algorithm on a handful of classification tasks. My main concerns are with the experiments, I think they could be more thorough and more clearly show the purported benefits of the DDPNOpt algorithm.First, the paper claims that the DDPNOpt algorithm is more robust. Robust to what? Stochastic gradients? Larger step sizes in the optimization algorithm? I think the paper could do a better job of stating how the DDPNOpt algorithm is more robust, and providence direct evidence demonstrating that. As far as I can tell, the only evidence presented for robustness is the toy illustration in Fig 2.Second, as the DDPNOpt algorithm optimizes the same total objective as the baseline optimizers, I would expect them to (eventually) reach the same loss. Is this correct? If so, why report just the final accuracies in Table 3? It seems more pertinent to show the entire training trajectory for these problems.Third, for the vanishing gradients experiment, it is hard to tell if the reason the other algorithms perform poorly is strictly due to vanishing gradients. How are the step sizes for each algorithm tuned? Are the other optimizers stuck at a saddle point? Does the stark difference in performance go away if one were to switch to using ReLU activations, which presumably do not suffer from vanishing gradients as much?Overall, I think the connections drawn between optimal control and neural network training are themselves interesting and thought provoking, even with my caveats about the experiments. This submission deals with the classical value-based Greedy-GQ algorithm for off-policy optimal control, and develops a two-timescale variance reduction scheme to reduce the stochastic variance of Greedy-GQ thus improving its sample complexity. Specifically, a variance reduced (VR)-Greedy-GQ variant that applies the SVRG-type variance reduction technique to the two-timescale updates of Greedy-GQ. Assuming linear function approximation and Markovian data samples, it is shown that the VR-Greedy-GQ achieves a sample complexity of O(\epsilon^{-2}), which is order-wise lower than the sample complexity O(\epsilon^{-3}) of the original Greedy-GQ. Convincing experiments are also provided to demonstrate the effectiveness of the proposed variance reduction algorithm.Overall evaluation: This paper is reasonably well written and presents interesting technical results. The Greedy-GQ algorithm is an important and efficient value-based approach for off-policy control. Detailed comments: In the existing study, variance reduction techniques have been successfully applied to value-based TD learning algorithms for policy evaluation (e.g., VRTD, VRTDC), but they have not been explored by value-based algorithms for control, especially in the off-policy setting with Markovian samples. This paper fills this important gap. Below please find several related technical comments.i) The main contribution is to show that VR-Greedy-GQ achieves an improved sample complexity over that of Greedy-GQ. In particular, the authors showed that (as commented in the contribution section), VR-Greedy-GQ induces a small bias error caused by the Markovian sampling and a small variance error of the stochastic updates, both errors are inverse proportional to M  the batch size of the SVRG reference batch update. Hence, a larger M should gives smaller error terms, and this is also suggested by the bounds in Theorem 4.5. However, it is not clear why Corollary 4.6 chooses the special M=\epsilon^{-1} to achieve the desired sample complexity, can the author clarify the trade-off in choosing these hyper-parameters?ii) A key technique in the finite-time analysis is the introduction of the fine-tuned Lyapunov function R_t^m. In particular, the coefficient c_t is specially chosen so that the quadratic term on theta can be totally absorbed into the Lyapunov function for telescoping. Although this technical development is very interesting, how is it different from the traditional analysis of nonconvex SVRG? For example, see the paper Stochastic Variance Reduction for Nonconvex Optimization by Sashank J. Reddi et.al. iii) More recent results on finite-time analysis of TD/Q-learning algorithms dealing with Markovian samples should be discussed, as well as how the current analysis differentiates/improves from existing e.g., drift analysis in [Srikant et al, COLT'2019] and multistep Lyapunov analysis in [Wang et al, AISTATS'2020] in terms of accommodating the bias and correlations introduced by the Markovian data samples.  This paper presents SALD, a new type of implicit shape representation that, in addition to predicting the signed distance function, aligns the gradients of the distance function with that of the neural distance field. The resulting algorithm, for example, has improved approximation power and better preserves the sharp features than the ancestor SAL (sign agnostic learning). The formulation is such that the architecture can consume raw point clouds. STRENGTHSThis paper certainly speaks to me. First of all, learning implicit representations directly from raw point clouds can allow for interesting applications such as better generative models or efficient 3D reconstruction networks. The approach is very sensible. In fact, aligning gradients of the implicit surface with the ones of the data is not a new idea and has been done for instance in quadric fitting:* Birdal, T., Busam, B., Navab, N., Ilic, S., & Sturm, P. (2019). Generic primitive detection in point clouds using novel minimal quadric fits. IEEE transactions on pattern analysis and machine intelligence, 42(6), 1333-1347.* Tasdizen, T., Tarel, J. P., & Cooper, D. B. (1999, June). Algebraic curves that work better. In Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149) (Vol. 2, pp. 35-41). IEEE.[the paper might benefit from including those especially because it has related work sections called 'primitives' and 'implicit representations'.]. This is not a drawback but just the opposite: there is a strong prior evidence that such approaches are useful. I also like that the authors spend a reasonable amount of effort for theoretical analysis. Though, I believe that this can be extended to more realistic scenarios (as the authors aptly explained in the limitations).  WEAKNESSES / ISSUES - In addition to aligning the gradients, many works benefit from constraining the gradient norm of the implicit function be |\nabla| = 1. See for instance:* Slavcheva, Miroslava, et al. "Killingfusion: Non-rigid 3d reconstruction without correspondences." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.Can we think of a similar approach here? Could the paper show some ablations with regularizers concerning the gradient norm?- Nowadays, the use of implicit 3D representations is omnipresent. In the evaluations, would it be possible to compare against the variants of DeepSDF (e.g. Curriculum DeepSDF or MetaSDF etc.)? With that, it might also be nice to include some more qualitative results in the supplementary. - Would it be possible to include additional real objects that are non-humans? This might involve for instance cars in an autonomous driving scenario.- Some discussions on the following aspects could be valuable for the reader: (i) What would be a good suggestion to handle thin-structures? It seems to be a common issue among many SDF-like methods. (ii) The use of raw point sets is good, but such data usually come partially observed. Could this method support partial observations? If not, could there be workaround?- The Chamfer distance and the variations thereon are obviously not well suited to assess the accuracy of the deep implicit representations. This creates an urge for better quantitative metrics, maybe the data driven ones. For the future, I would strongly suggest thinking about those to have more meaningful evaluation data.- Some minor remarks:* Can we already compare D and D' and give an intuition about what they might refer to at the place they are first defined?* "they strives to" -> they strive to* "tested SALD ability" -> tested SALD's ability* "the surfaces produces" -> "the surfaces produced" This paper is based on the "sign agnostic learning" (SAL) method for capturing signed distance functions with neural networks. It extends this method by incorporating derivative information, which interestingly can likewise be handled in a sign agnostic manner. (Maybe I missed this somewhere, but if the derivatives are sign agnostic, couldn't it happen that the inside is positive? Did the authors encounter that in some cases?)The paper presents and motivates this extension together with an additional theoretical insight about the minimal surface property of SAL and SALD. In line with SAL, the paper presents a nice variety of results for shapes from different shape databases. The quantitative results are also convincing. It's interesting to see the substantial difference between the VAE and AD architectures. For the comparison with SAL it's good to see the direct improvements from the derivative loss with a VAE.The paper leans heavily on SAL, and the change in terms of the overall method seems to be fairly small. Nonetheless, I think it's an interesting insight that the sign agnostic derivatives can be included in this way, and I found it interesting to see how much they improve the results.Given that learning signed distance functions is a very active topic, and a very useful building block for a variety of adjacent works that use learned SDFs, the proposed SALD approach seems like a very nice advancement of the state of the art.So, overall, I really liked the paper. Figure 2 alone is impressive, and makes a good case for the method. Together with the nice presentation and set of results I think this paper makes for a very good addition to ICLR. This paper presents a novel approach to structure learning for cause-effect models, focussing on the influence of mediators.This approach is supported with an algorithm, which is implemented and tested on various datasets. The authors summarize important preliminaries and recap related approaches. The derivation is driven mathematically, supported by definitions and theorems. It is remarkable that the authors build on Rubin's potential outcome framework and Pearl's graphical models and the do-formalism, where these "schools of thought" have been developing in somewhat seperated strains.I recommend to accept the paper.Overall, the paper is very well-structured, the accompanying appendix is detailed and provides extensive supplemental material.The approach is driven theoretically and tested experimentally. \The choice of COVID-19 data as experimental data adds a good final touch,  taking a recent and prevalent real-world example into account. One downside is that no code implementation is provided (e.g. as supplementary material).  The authors present a simple and efficient method for training offline RL agents. They estimate the epistemic uncertainty of their model through dropout  variational inference. While this method is not novel, it has never been applied to offline RL as far as I know. On the basis of this epistemic uncertainty estimate, they regularize the policy search to avoid sensitivity to overestimates in poorly known states. The way this regularization is performed is taken from [Kumar2019]. The authors then experimentally demonstrate the effectiveness of dropout uncertainty estimation for RL, and achieve the best performance on a classic offline RL benchmark, with the best offline RL algorithms for continuous state-action MDPs.There is not much to say about the paper. It is well written and positioned. One could that the method is the straigthforward application of dropout variational inference to [Kumar2019] algorithm, but it remains that the empirical results are quite significantly improving the state of the art, and as such, it deserves to be known.I only have one minor remark to share: Section 4.1, the explanation of the variance decomposition is a bit misleading. Red minus blue is the dropout variational variance, and this is what measures the uncertainty of the model (not singlehandedly the red term).  Summary:This paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research: normalize+scale and attribute normalization. It also demonstrates these tricks are not enough w.r.t. normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue. Both theoretical and empirical analysis are provided and results look convincing. Finally the authors propose a continual zero-shot learning problem scheme and illustrate some pioneering experimental results.================ Reason for my score:There is rare work on the normalization trick in the context of zero-shot learning, although techniques like attribute normalization are widely used in practice. This paper investigates the normalization effect extensively for zero-shot learning, and provides many insightful thoughts for utilizing these tricks. The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results. Such work makes good contributions to the related community and hence I give my score.================ Pros:The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning, in the perspective of data variances.The paper proposes a class normalization trick to alleviate the variance inflation/diminish in the non-linear model, and demonstrates its effectiveness on benchmark datasets.The empirical analysis in the paper are extensive and convincing.The paper also proposes a new framework of continual zero-shot learning.================ Cons:The paper didn't evaluate on another widely used benchmark dataset aPY, can author explain the reason?On CUB dataset, the proposed method has a considerably large margin to the state-of-the-art methods, in contrast to other datasets. Is there any explanation on why it is the case? Have you tried to explain this failure especially from the perspective of the proposed normalization trick? This paper describes a nearest-neighbor enhancement to NMT, where internal token-level context representations are used to index into a large data store to find relevant (source, target prefix) pairs. Since the index representation is taken from a pre-softmax representation in the decoder network, no additional training of the NMT model is required. The authors show a diverse range of strong results, from improvements using a data store over the models own training data, to improvement from using a collection of domain-specific corpora not present during training used for domain adaptation, to language specific collections to improve capacity of multilingual models. They are also able to show by example how the model makes MT more interpretable.This is a very strong paper. It's well-written and easy to read, the method is very novel to MT, and the results are great. The method isnt practical right now (decoding is two orders of magnitude slower), but its very interesting and thought-provoking. I can imagine it influencing a lot of work, even if the actual method doesnt see a lot of use.The only complaint that I could imagine raising against this paper is that the method is not particularly novel in light of recent work on nearest-neighbor language modeling, but in this day and age, with so many papers available, I think its actually very important to make these incremental stops in neighboring fields to make the connections explicitly clear. All the great experiments on multilingual MT and domain adaptation also help a lot. To their credit, the authors provide a concise section discussing the changes that needed to be made for the conversion to conditional language modeling (MT).Small concerns:The exp(d) in Figure 1 is missing a negative: exp(-d).Table 1: what does the bolding indicate? It looks like statistical significance, but if so, please be clear about what test was used. # SummaryThe authors concerns the question of how expressive invertible functions can be constructed. Their ansatz is the defining an invertible layer implicitly, using the root of an equation. While this approach is more general, they employ residual flows (ResFlows) to formulate a particular realisation of such an equation, calling the model ImpFlow. They show that the resulting function space is strictly richer than that of ResFlows. They further demonstrate how ImpFlows can be trained and evaluated. Empirically, ImpFlows outperform ResFlows on all considered tasks.# Strong and weak points## Pros:- Implicit functions are a new way to formulate invertible functions- The proposed formulation using ResFlows is clearly presented- ImpFlows are strictly more expressive than ResFlows, allowing arbitrary Lipschitz constants- Experiments highlight the improvements- In general: very concise and well-guiding writing## Cons:- Implicit functions are more expensive to evaluate than explicit functions (here, +50% in execution time)# RecommendationThe development of expressive invertible functions is key in applications that involve invertible functions, like density estimation using normalising flows. This paper proposes a novel framework to formulate such invertible expressive functions implicitly. The results are not game-changing, but consistently outperform its closest relative. Together, this is a solid work that should clearly be accepted.# Questions- I would like to see some analysis on how a Lipschitz function influences the Lipschitzness of the inverse. In particular, in Section 6 you could mention the relation between the "Lipschitz coefficient" c and the achievable Lipschitz constants of an ImpFlow layer. I naively would guess the bound $L < (1 + c) / (1 - c)$.- Can you give an intuition how large the improvements on the density estimation datasets are? I checked [paperswithcode](https://paperswithcode.com/sota/density-estimation-on-uci-power) on the POWER dataset and the models are clearly outperformed by other methods. To be explicit: I don't require that every new architecture has to beat the state of the art in all possible tasks, especially when it comes with a fresh idea and thorough theory. But can you give your best guess about why other approaches achieve significantly better results?- When comparing ImpFlow and ResFlow, I suggest that you should additionally have a variant of ResFlow with the same *execution time* in addition to *number of parameters* as a corresponding ImpFlow. I think the tradeoff involved is not only between quality and the number of parameters to store, but the also execution time of a model. The main contributions of the paper are the following ones (informal): 1. The approximation theorem of linear functionals with linear RNNs in continuous time settings. The main difference with the previous results is that the class of approximated functions is linear, but does not necessarily come from the same differential equation that describes the class of approximator models. 2. The upper bound on the approximation error for some "exponentially decaying" linear functionals, where the upper bound depends on the weights matrix size (i.e. memory size). The memory growth rate is polynomial with respect to approximation error. It is no longer the case when linear functional is not "decaying exponentially" and memory growth rate is exponential. 3. The optimization dynamics analysis. In particular, the authors showed that under some conditions the optimization process can be stuck if the "memory" of the target functional is large.Overall, it is a good paper and I enjoyed reading it. It is very well written and easy to follow. In many cases, the authors provide clarification for used assumptions. The authors also emphasize the difference between their results and the previous results.Pros: the authors showed that difficulties encountered in practice, where the target functional has long term dependencies, emerge even in simple linear settings and can be explained from a theoretical point of view.Cons: in many settings we are interested in not just recovering some dependencies, but recovering it from the data or recovering the dependency only on some subset of the all possible input sentences. The role of input data is significantly ignored in the given analysis.Several questions and remarks:1. The condition on supremum in (14) seems purely technical (at least based on provided proof in appendix). Could the authors please clarify whether this assumption has some qualitative explanation or can be replaced with stronger but more "meaningful" assumptions (of course it will make the result weaker)?2. In the dynamic analysis x is assumed to be white noise. This assumption seems too restrictive and is used to apply Ito's isometry theorem. What else stochastic processes can be used here to make this result stronger?3. In (50) in  (-(alpha + 1) / beta)^{i}. i should be replaced with j. Summary:This paper proposes Group-Supervised Learning (GSL) that can learn disentangled representations by swapping components in latent space, and enable one-shot novel view synthesis.  They created large dataset to evaluate their method, and demonstrate its effectiveness in controllable image synthesis, and disentanglement, outperforming existing baselines.Pros:1. This paper is good-written and clear to follow. The authors can demonstrate their idea well in Section 3 and 4.2. Swapping attributes is an interesting and novel idea to encourage disentangled representation learning, which are easy to implement and could have wide applications in many fields.3. The experiments are extensive, and the results are able to support their claims.Cons:1. In Equation (4), can you explain the defination of  separate losses, L_r, L_sr and L_csr along with the equation? They are not mentioned above, but simply in the algorithm 1. 2. If the cycle loss is removed, will the performance degrade dramatically? Can you please offer ablations for each loss? **Paper summary**The paper gives theoretical proof showing that the recently proposed data augmentation technique Mixup can indeed improve generalization and help in robustness. The theorems cover GLMs and certain classes of neural networks. The paper also contains numerical experiments supporting some aspects of the theory.**Strengths**1. Currently, there is only a limited theoretical understanding of why Mixup works. This paper shows that Mixup is essentially equal to regularizing the first and second derivatives (with respect to the input $x$). Intuitively, this means that changing the training samples slightly shouldn't change the output of the model much. Further, the paper proves that the mixup loss is an upper bound on the $2^{nd}$ order Taylor approximation of the adversarial loss, and hence reducing mixup loss reduces adversarial loss. Finally, the paper proves that mixup helps in reducing the Rademacher complexity and hence improves generalization.2. The results seem fairly general and apply to many models such as GLMs and neural networks.3. The paper supports its approximations and claims by numerical experiments.**Concerns**1. The regularizing term $\mathcal{R}_3$ looks like it is minimizing $z^T\nabla f_\theta(x_i) z$ (for some $z$). This promotes the Hessian (wrt $x$) to have negative eigenvalues in the direction of $z$. Ideally, we would want the Hessian (and also the gradient) to be 0 around the training samples so that perturbing the input doesn't change the output much. Thus, I don't see how the $\mathcal{R}_3$ term helps regularize the Hessian properly.2. The paper claims that Assumption 3.1 holds when the minimizers are not too dispersed. Does it still hold for practical neural networks where the minimizers can possible be fairly far apart?**Comments**Although the paper seems well written, I have a few suggestions:1. The notation $cos(\theta, x)$ which refers to $\frac{\langle \theta, x \rangle}{\|\theta\|\|x\|}$ should be explained in the preliminary section.2. On page 6, the statement $f_\theta(x)=\nabla f_\theta(x)^Tx$ should be proven. It will save the reader some time if the proof is provided.3. In Remark 3.1, I think Theorem 3.2 should actually be Theorem 3.4**Score justification**There isn't much prior work on the theoretical understanding of Mixup. This paper provides theoretical guarantees for Mixup on two fronts - robustness and generalization; for both GLMs and ReLUs.  This work proposes an online variational Bayesian (VB) approach to continual learning. The prior over neural network functions is both over the neural network structure and parameter values, where the structure is modelled by an Indian Buffet process (IBP) and the weights are drawn from a Gaussian.Similarly, the approximate posterior is assumed to be IBP and factorised Gaussian as well and inference is performed through variational inference and reparametrization of the respective distributions.The approach is similar to VCL in that it uses online VB for learning, however, the prior and approximate posterior is more general in that it also considers the neural network structure as a random variable (prior and posterior). Theory:The approach is theoretically sound and well motivated; the paper is presented well and easy to follow.My main concern is that that the second paragraph of the paper motivates with scenarios where the ability to *adapt to dynamically changing environments or evolving data distributions* is essential. However, online VB assumes iid data. That is, the online algorithm should infer the posterior over all tasks rather than adapting to dynamically changing data distributions. Inference is sequentially, but the ordering of the task should in theory not matter - it only matters in practice as we perform approximations. See e.g. [1] (ICLR 2020) for an approach that explicitly adapts (through forgetting) the distribution over neural network weights. It could be possible to extend this work to similar adaptation mechanisms, although for multi-task learning such adaptation/forgetting may not be desirable. I would appreciate a few comments on this and I think it should also be discussed shortly in the paper. Experimental evaluation:The experimental section considers scenarios that are very common in the CL literature. Unfortunately these are not the most interesting or insightful, as these are variants of MNIST. But since most related work considers these settings as well, the choice is justified. The results are quite strong. I find the results on classification from the latent space of the unsupervised learning approach especially convincing and interesting (Table 1). Related work:The relation to [2] needs to be discussed in more detail. (What exactly is the difference if the IPB is put on the activations rather than weights? What are pros and cons? I am aware that there are additional experiments in the supplementary material comparing to Kessler. Why do you think your approach outperformed the one of Kessler?Another interesting aspect is that the coreset does not help much, which is in contrast to VCL. Do you think this is because the performance is already high? Or because the coreset selection algorithms (k-center, random) are unsuitable?(([1] Continual Learning with Bayesian Neural Networks for Non-Stationary Data, ICLR 2020[2] Hierarchical indian buffet neural networks for bayesian continual learning The paper describes a novel implementation of RED, regularization by denoising, which better leverages multicore architectures to achieve a significant speedup. The proposed implementation splits the gradient step into smaller components, which can each be executed independently on different cores and then used to update a shared copy. The crucial result is two sets of convergence guarantees showing that this delayed update will not cause too much error, even if the updates from different cores arrive at different times. The speedups achieved range from 6× to 8× on two tasks (compressive sensing and computed tomography reconstruction).The paper is well organized and the details are explained well. The numerical results are convincing and the analysis is adequate. The main weak point of the paper is motivating the problem. In other words, it is not clear why a multicore method is needed, although the numerical results demonstrate this later on. For example, Section 3 starts by stating that ASYNC-RED addresses the computational bottleneck&, but what that computational bottleneck consists of is never explained. Figure 1 helps in explaining this, but much is left unexplained at this stage. Some discussion of the regime where multicore processing makes sense would also be in order. That being said, I think the results are interesting enough and the description of the method compelling enough that I recommend this work be published as part of the proceedings.There are some small issues: On p. 2, H(x) is never defined. On p. 2, G is defined twice: once in eq. (2) and one in eq. (4). Presumably these refer to the same non-linear mapping. There seems to be some mixup between BC-RED and GM-RED throughout. Are these different methods? This paper presents an interesting technique to generate multimodal trajectory GAN and a carefully designed latent intent space. This latent intent space allows an operation termed as hallucination, which switches agent intents to enrich the latent spaces' coverage. The paper is overall clear and well-written. However, there is one point regarding the hallucinative learning probably can benefit from more elaboration: the time step "t" is only introduced in the Hallucinative Learning paragraph. How is the time step being used in the network? Is the network only predicting y_{gt} as a simple waypoint (t=1), or a sequence of waypoints (a trajectory, in that case, what is the horizon)?  If I understand correctly, the main hallucination idea is to extract as many intentions in the latent space as possible, from the training data, and then the hallucination is implemented as randomly switching between different learned intents to generate augmented data? This point needs to be carefully clarified. Regarding the idea of using "hallucination" to generate more training data, many recent works have used the idea of hallucination for data augmentation, or even generating training data from scratch. For example, https://arxiv.org/pdf/2010.08098.pdf and https://arxiv.org/pdf/2007.14479.pdf generate training data using hallucination based on geometric feasibility, instead of latent intent, https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Few-Shot_Learning_via_Saliency-Guided_Hallucination_of_Samples_CVPR_2019_paper.pdf uses saliency to guide the generation of hallucination, https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Adversarial_Feature_Hallucination_Networks_for_Few-Shot_Learning_CVPR_2020_paper.pdf also used a very similar adversarial approach to generate feature hallucination. These works can help to set stage in the related work section.  This paper proposes a novel variant of LSTM by analyzing its behavior againstscale-free distributions generally found in natural languages. Since theprediction of LSTM is essentially a convolution over each hidden unit, theauthors derived that the bias parameter should obey an inverse Gamma distribution. This is a very neat and interesting result, which is also validated by a number of experiments in natural languages with scale-freedistributions and artificially generated corpus with non scale-free distributions. My only question is the setup of the proposed LSTM: in Section 3.1.1, theauthors say that the first layer of LSTM has a fixed timescale, and only thesecond layer has an inverse Gamma bias parameters. The third layer does nothave inverse-Gamma distribution and simply optimized.Is this architecture necessary for the result? If so, why the third layershould not have the proposed inverse-Gamma time scales?Finally, in Figure 3, infrequent words actually use longer time scales, butthey also leverage short scales (i.e. red lines are U-shaped, not linear forlonger scales). I would like to know why this phenomenon happens.That being said, this is a very interesting paper leveraging the structure ofLSTM and scale-free property of natural languages. In addition to Dyck experiments, some other languages, such as a generation from PCFG or someprogramming languages might be also interesting for experimentation. SUMMARY:The paper presents a graph neural network (GNN) architecture with learnable low-rank filters that unifies various recently-proposed GNN-based methods. The local filters substitute the graph shift operator (GSO) by a learnable set of parameters that capture the local connectivity of each node in the graph. Moreover, a regularization penalty is proposed to increase the robustness of the model and prevent these local structures to overfit. The paper provides proofs to justify the generality of the approach and how different methods can be seen as a particularization of the proposed scheme. Two theorems are also proved to claim the stability of the GNN architecture against dilation perturbations in the input signal. Several numerical experiments are conducted to empirically test the usefulness of the model.STRONG POINTS:The paper introduces a new GNN-based approach with larger discriminability power.The proposed approach generalizes various previously existent architectures. This is proved in the appendices.A regularization technique is proposed to avoid overfitting to local particularities of the data.Two theorems are introduced to proof the stability of the network against dilation in the input signal. The numerical experiments are extensive and convincing. This is one of the strongest points of the paper.The paper is well-structured and the style is appropriate. It is easy-to-follow and the points are clearly stated.WEAK POINTS:Replacing the GSO with a set of learnable parameters increases the discriminability power of the network, at the cost of sacrificing various properties of common GNN architectures. For example, this technique is no longer permutation equivariant, and transferability to larger networks will no longer be an option, as long as the learnt parameters are graph-dependant.Scalability problems appear when the network grows in size. This, and some possible ways of tackling it, are discussed in the conclusion.Although the theorems offer insights on the robustness of the network against perturbations on the input signal, they are restricted to dilation perturbations (which are proportional to the input signal). This is not commonly the case, perturbations often follow distributions that has nothing to do with the input signals. OVERALL ASSESMENT AND RECOMMENDATION:The paper introduces a new architecture with larger discriminative power that generalizes various state-of-the-art methods. Although the theoretical results are not particularly strong, they are undoubtedly insightful. Then, the empirical performance of this technique is exhaustively validated through several experiments. Thus, in my opinion, this paper should be accepted.RECOMMENDATIONS TO THE AUTHORS:Using matrix notation would be helpful to clarify various equations and capture the attention of a broader range of researchers. Equally important, it will also contribute to establish links between the schemes proposed in the paper and well-established techniques in the field of graph signal processing (GSP).Describing the connection between the operator $B_k$ and classical graph operators (adjacency, Laplacian, normalized Laplacian...) would be clarifying. Consider adding a couple of lines pointing out this relation.  This paper proposes a method for binarization of neural networks of 3d point clouds. Two modules of entropy maximum aggregation and layer-wise scale recovery are proposed to conquer the problems of discrimination loss induced by feature homogenization and scale imbalance, which are caused by model binarization. The authors provide theoretical analysis about the proposed method. Experiments on various backbones and tasks demonstrate the effectiveness of the proposed method. A practical implantation of BiPointNet on ARM also demonstrates significant speedups over PointNet and large memory savings.Strength:1. Binarization of CNN models designed for 2D images has been studied in the past years, this paper extends this problem into 3D point cloud models. The authors show that the existing methods for binarization of 2D CNN models can not work well on this new problem. 2. For this new problem, the authors analysis its performance degradation based on PointNet and proposed effective solutions.3. Experiments on several tasks show that the proposed method can obtain highly compact models with acceptable accuracy degradation. Experiments on other backbones also show that the proposed method is general, although its analysis is based on PointNet.For the weakness, I only have some minor comments.1. The discussion on related work could be enlarged. For example, the following papers are well known point cloud networks proposed recently(a) PointConv: Deep Convolutional Networks on 3D Point Clouds. CVPR 2019(b) Relation-Shape Convolutional Neural Network for Point Cloud Analysis. CVPR 2019(c) ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics. ICCV 2019Mixed precision quantization is also an active direction after binarization of neural networks, it could also be mentioned as a possible improvement in the future.(d) Mixed Precision Quantization of Convnets via Differentiable Neural Architecture Search. ICLR 2019(e) Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization. ECCV 2020.2. Some references miss publication type, i.e.,  conference or journal and where they are published.3. Figure 1 can be improved. It is unclear how LSR works in the whole framework. This paper studies learning in stochastic games, which are extensions of Markov decision processes (MDPs) from the single-agent setup to the multi-agent one. Here the objective of each learner is to optimize her own reward function. Similarly to the case of MDPs, here one can devise learning algorithms with controlled sample complexity or regret (or both simultaneously) even when reward and transition functions are unknown. The main contribution of the paper is a model-based algorithm called Nash-VI, enjoying simultaneously a regret bound of $\widetilde O(\sqrt{H^3SABT})$ after $T$ steps and a PAC-type sample complexity of $\widetilde {\mathcal O}(H^3SAB/\epsilon^2)$ for finding an $\epsilon$-approximate optimal policy. The paper also presents an extension of Nash-VI to the case of reward-free exploration, which is called the VI-Zero algorithm. The sample complexity of VI-Zero depends logarithmically on the number of candidate reward functions, but has a worse dependence on $S$ than that of Nash-VI. Main Comments:The paper is very well-organized and well-written. There are a number of minor easy-to-fix typos that are reported below. The paper overall delivers a clear presentation of algorithms and results, and expect for a few unclear sentences (listed below), it is very a nice read. On the technical side, one strong aspect of Nash-VI is that its sample complexity (almost) matches the lower bound except for dependencies on the action set cardinalities. Achieving a near-optimal regret bound is another strong aspect. Finally, the main algorithm outputs a single Markov policy, and not a history-dependent policy. For the MDP setup, it is already well established that model-based algorithms are minimax-optimal in terms of both regret and sample complexity. The paper makes a good step towards understanding the benefits of model-based algorithms for stochastic games, thus indicating that their sample complexity could almost match the existing minimax lower bound. It turns out that the presented algorithms mostly rely on existing tools already developed for the MDP setup, which are by now fairly standard. That said, the paper does not present any fundamentally different technique than existing ones for MDPs. However, I believe suitably combining all these tools is not a trivial task and deriving such sharp sample complexities requires care. This is in particular true for bonus $\gamma$ discussed in p. 5. Overall the paper conveys interesting messages advancing our understanding of model-based algorithms for stochastic games, and in my opinion is worth accepting.  I was unable to check the proofs in such a limited review period, but they appear correct to me. Minor Comments:- In p. 6 during the training phase please clarify. - In the statement: would not hurt the overall sample complexity up to a constant factor: Did you mean would hurt the overall sample  complexity only up to a constant factor- In p. 7, when referring to $\mathcal M(\mathbb P, r)$, it is not clear which reward function $r$ is. Does it one belonging to a set of reward functions? Please clarify.Some typos:p. 1: with the problem multi-agent -> & the problem of multi-agentp. 1: Model-free algorithms has -> & havep. 3: Markov games is -> & arep. 3: distribution of actions -> & over actions p. 6: the policies & is -> & arep. 6: such policy -> such a policyp. 6: a $\epsilon$-approximate -> an &p. 8: since MDPs is a special case of -> since MDPs are special cases of p. 8: rerunned -> rerun Summary-------The authors introduce new algorithms to solve two-players zero-sum Markov games, as well as two-players Markov game in the reward-free setting. The approach is model-based, based on successive episods of planning and counting for updating the model estimate. It involves solving a matrix game at each iteration, looking for a notion of equilibria that is computable in polynomial time (unlike Nash equilibria) An extension to multi-player games is proposed for both reward and reward free setting (in the appendix).The sample complexity of the zero-sum Markov game algorithm (Nash-VI), has asample complexity that is closer to known lower-bounds than existing work, and which is better in the horizon dependency than the best model-free algorithm.In this zero-sum setting, the improvement to existing work (VI-UCLB) are the following:-  use of an auxiliary bonus that is proportional to the gap of upper-confidence and lower-confidence value function, that allows to use much-smaller standard Hoeffding/Bernstein bonuses- use of a relaxed notion of equilibria when looking for the next policy given the Q functions of both player. This relaxed notion is introduced in Xie et al. 2020The reward-free setting is simpler, in that it simply use greedy policies for each player, with each player maintaining artifical rewards based on Hoeffding bonuses.Review------The paper is very well written and presents some exciting results. It is completely theoretical, but provides two different algorithms for two different settings, in both the two-player and n-player setting. The improvement in existing bounds is significant. I have only slight concerns:- The algorithmic contribution (Alg. 1) could be seen as incremental, as it changes two elements in known algorithm, one of which (coarse correlated equilibria), having been used for a similar purpose in a previous paper. Similarly, Alg 2. is at the end of the day a rather naive extension of zero-reward exploration in single player MDP. Yet the notion of auxiliary bonus is very original, and the reduction of complexity non-trivial.- The absence of experiments, even in toy setting, is regrettable. It isespecially true as the use of coarse correlated equilibria may be expensive, andI would have appreciated seing Alg. 1 implemented. As this is ICLR, extensionsto function approximations would also be interesting. In particular, comparisonwith model-free approaches would be welcome, as constants before the samplecomplexities may vary.- Some parts of the text could be further explained: in particular theintuitions behind coarse correlated equilibria, which is introduced only mathematically.- The paper theoretical content is rather heavy, which may make this manuscript more suitable for a journal venue, where it would be more thoroughly reviewed. I must admit that I could not proof read the entire appendix. I have several questions, as follow:- I do not understand the note "Our results directly generalize to randomized rewardfunctions, since learning the transition is more difficult than learning the reward." Could you elaborate on this aspect.- Is there any reason why we would like to use Hoeffding bonuses instead of Bernstein bonuses in the rewarded case ?- In the multi-player, rewarded case, it appears that using coarse correlated equilibrium instead of Nash equilibrium yields non-product policies, which is unfortunate. Is there any way that we could obtain product policies solving a relaxed notion of equilibria that would be computable in polynomial time ? Similarly, is there a foreseeable way in which $\Pi A_i$ could be transformed in a sum ? Lower-bounds in this case are not discussed in this case, is there any ? The authors consider the problem  of differential  performance across subgroups commonly present in classifiers, and propose to mitigate it. They augment subgroup data for each class using CycleGAN, and balance the performance across subgroups in each class using a consistency regularizer and a robust objective that minimizes the difference between the minimum and maximum subgroup performance.  This approach is demonstrated in several datasets including the ISIC skin cancer data.The paper is written well, easy to follow, and I was able to understand and appreciate the contributions quickly. The appendices are also very  thorough and the code is organized well. There is sufficient detail to help readers reproduce the results.Comments:1. The authors can specify very early on that the subgroups are pre-specified by the user, and not automatically discovered.2. Relations to the  area of group fairness may be helpful. The goal there is also  to ensure parity of predictions among groups, and various methods are used. The groups though are assumed to be common across classes. It may be possible to extend this approach to that area, and the idea of data augmentation may be very appealing there. A small discussion on this can be quite informative to the community. There is also the notion of  subgroup fairness discussed here which considers exponentially or infinitely many subgroupshttp://proceedings.mlr.press/v80/kearns18a.html 3. Have the authors considered how their methods can be modified when the subgroups are common  across classes (this points directly to the group fairness comment  before)?4. Just to clarify, the robustness metric used in the experiments is the same as the metric of interest for GDRO in Table 1, correct? And is the gap same as the metric of interest for SGDRO? Does the performance for groups in each class vary between robust accuracy, and robust accuracy + gap? How are the metrics reported in the experiments consolidated across classes? It may be helpful to define the experimental metrics explicitly.5. Saying that your minimization of I(\hat{Y}; X | [X]) is parallel to Lemma 1 is confusing, since you minimize the upper bound (in Thm. 1) and Lemma 1 minimizes the lower bound. Agree that your minimization is stronger.6. Have you tried  any preliminary experiments with greater than 2 subgroups per class? Just curious what it takes to use something like StarGAN like you mention.7. What are the risks of letting the users specify the subgroups? In some problems it may be hard to diagnose which subgroups are meaningful. Is minimizing the worst case performance over many possible automatically discovered subgroups an interesting future  direction in your opinion?  What does it take to do that?8. How well  will  this method work when there is considerable imbalance in the subgroups considered?9. In table 5,  what do the quantities in the parenthesis mean?Typo:Sec. D.4.2 (GDRO): weighte ->  weight This paper defines a new problem named slowdown adversarial attack to decrease inference efficiency of early multi-exit networks.Pros:1. Slowdown attack on efficient-inference networks in an innovative new problem with wide real-world applications.2. The proposed method, basically pushing the model prediction to uniform distribution, is straightforward and intuitively correct.3. The universal/class-universal variances of slowdown attack provide more flexibility of the proposed method, and the blackbox experiments show good generalization ability of slowdown attack across models and datasets.4. Last but not least, the authors show that traditional adversarial training is not a cure for the newly proposed slowdown attack, showing the challenging character of this new threat model.Comments:1. Limitation in application scenario: From the prospective of an attacker, one may want to jointly attack inference efficacy and accuracy. Is it possible to design a more generalized setting, where the adversarial attacker can generate images that both slowdown inference and leads to wrong prediction? The current method pushes model predictions to uniform distribution, so it may be ineffective to generate adversarial images to fool classifiers. This is validated by the results, where the slowdown adversarial images have an 5% or 15% lower accuracy compared with clean images. 2. Any discussions on how to conduct slowdown attacks on other efficient-inference models (e.g., SkipNet [1])?3. Please consider releasing your codes.[1] SkipNet: Learning Dynamic Routing in Convolutional Networks. ### **Evaluation**- The paper is tackling the topic of explainable Deep One-Class-Classification. In contrast to comparable methods, like for example gradient-based heatmaps, FCDD has a spatial context. This fact facilitates the interpretability. - The approach is well-motivated and compares well to the state of the art AD-methods.- The paper provides sophisticated theoretical as well as empirical insights. ### **Detailed Comments for Authors**- The visualisations given in the paper are well suited for the addressed problem. The Appendix gives a good insight into the functioning of the FCDD as well as the decision process of the networks. However, in figure 18. the output for "toothbrush" is shown a huge anomaly blob for all samples, while the ground truth shows just minimal areas. Why is that the case? In contrast to this, the mean AUC in table 2 is 0.94 (and 0.95 for semi-supervised FCDD), which is quite good comparing to the visual impression. - The section addressing the "Clever Hans" effect was a good addition, since it shows another perspective of the usage of the methods while clearly showing the learned characteristics. It might be beneficial to explain in more detail, why using the "horse" class as anomaly class, since it is kind of in inverse logic (it makes sense, but while reading one had to think twice about it). - For the heatmap-overviews given, it might be valuable to have a comparison of one sample of the nominal class and multiple of the anomaly one. In this way, the trained class is shown and directly comparable to the anomalies.  #### **Strong points of the submission.** - Very well written paper, which is easy to follow, based is based not least on good visualisation.- The paper provides a good and detailed description of the theoretical concepts.- It is positive that the examples also pointing some weaknesses of FCDD, like not recognising the anomaly, while simultaneously comparing to other methods. The method shows its strength in the spacial context. Given the extended appendix, the authors deliver a critical view on the possibilities as well as the potential limitations.    #### **Weak points of the submission.** - the conclusion is quite sparse, and even missing an own section. This paper presents a neuroimaging study investigating the way syntax is represented. The authors compare models that encode syntax with fMRI data. They find that syntax and semantics are computed/represented in overlapping brain regions and that "complex" syntactic information is decodable.I liked this paper and think it is valuable in, amongst other things, furthering the theoretical position that the dichotomy between semantics and syntax is a more conceptual/high-level one than can be found at the level of neuroimaging data. The authors give an exposition of their research questions, however I think these can be phrased even more clearly in some cases. For example, for Q1 do they mean for humans in general (as in in the brain) or do they mean us as scientists researching human cognition? For Q2, do they mean they will use a model-based fMRI analysis? For Q3, is this multivariate or univariate? Just adding those short phrases or words to the research questions will help situate the reader, in my opinion.Why is it in and of itself surprising that (complex) syntax is encoded in the brain? In other words: "Several regions of the brains language system were predicted by ConTreGE, hinting that the brain does indeed encode complex syntactic information."  why would "the brain" not? Please do not get me wrong, this is obviously important/required to be shown but the research herein actually has even more value (or could have) and can be framed and discussed as such. Surely, the interesting results (since we know from other sources and common sense that syntax is, has to be, encoded in the brain somewhere since it plays a role in cognition) is the actual relationships of the results to the overarching theory and should be foregrounded more. A potentially useful theory paper is, and which might interest the authors: https://doi.org/10.1162/jocn_a_01552Minor point: the in-text citations would look better without double brackets  which is easy to fix in LaTeX. # Synopsis of the paperThis paper presents WEGL ("Wasserstein Embedding for Graph Learning"),a technique for embedding graphs for graph classification. The primarynovelty of the paper lies in their smart choice of embedding, whichcombines the expressivity of optimal transport paradigms (i.e. theWasserstein distance, in this case) with improved computationalperformance; more precisely, the embeddings permit the use of effectiveclassification algorithms, as opposed to scaling quadratically with thenumber of samples.# Summary of the reviewThis is a well-written paper with a powerful algorithm and a strongexperimental section. It will make an excellent contribution to theconference, and I am excited to endorse it!There are some issues with the current version of the write-up, though,which, if fixed, will make this an even stronger publication. My primaryconcerns at present are:1. The paper spends a lot of space in outlining background information   that is not pertinent to the method. I appreciate the amount of   details that are being packed into Section 2.1, but as a reader of   this paper, I would prefer a more in-depth discussion of the method   rather than a discussion of GNNs, which are used as mere comparison   partners here. My suggestion would be to put some of these   information into the appendix.2. The section on Wasserstein distances could be streamlined. At   present, too many different concepts are introduced; I feel that   a non-expert reader will just be deterred, even though the remainder   of the paper is very hands-on. Maybe some additional intuition could   be provided here?3. On the other hand, Section 3 is *missing* important information; in   this section, I would be very much interested in knowing more about   the theoretical properties of the method (and its empirical   performance). The characteristics mentioned on p. 4 are intriguing,   but it would improve the paper if a more detailed write-up would be   provided; a reference with proofs for these properties would be   equally helpful. I am fully aware that it is hard to satisfy both   theory and experiments in a paper; I have some more detailed comments   about what could be added (potentially in the supplementary materials,   if the authors think that it detracts from the flow).# Detailed Comments- The point about the 'true metric' in the introduction is slightly  ambiguous. My understanding is that one obtains a metric between the  embedded feature vectors. This does *not* constitute a metric on the  graphs, though, unless the embedding is injective. I think the  sentence means to say that one obtains a metric in an embedding space  and this metric serves as a proxy for the Wasserstein distance.- In the related work section, I would suggest citing other variants of  the WL algorithm that have recently emerged:    - Morris et al.: *Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks* (https://aaai.org/ojs/index.php/AAAI/article/view/4384)     - Morris et al.: *Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings* (https://arxiv.org/abs/1904.01543)    - Rieck et al.: *A Persistent Weisfeiler-Lehman Procedure for Graph Classification* (http://proceedings.mlr.press/v97/rieck19a.html)- The way a graph is defined in this paper could be misconstrued as  *directed* upon first reading. Why not define edges in both directions  by using a subset notation instead of a tuple notation?- How are categorical attributes treated? In the supp. mat., the usual  one-hot encoding is mentioned. This could be discussed earlier (i.e.  on p. 2).- When discussing the embedding, I would suggest briefly mentioning the  concept of reproducing kernel Hilbert spaces (RKHS). Such a discussion  will provide the relevant backdrop for this publication.- As suggested above, I would shorten Section 2.1 somewhat (or put some  of the text into the appendix). While it is good to provide some more  details about the inner workings of GNNS, this is not required for the  paper.- Section 2.2 would benefit from more intuition; a lot of the  terminology is introduced too tersely and not re-used. I fully  appreciate that the paper is mathematically precise here, but at the  same time, I would suggest a more streamlined introduction of the  concepts that are necessary to define the embedding in the end.- Section 3.1 and Figure 1 differ slightly in terms of their notation.  I would suggest to harmonise the description here in order to be more  consistent.- For property 4 on p. 4, is it possible to quantify the strength of the  approximation? How good is this approximation in practice, and what  are the factors influencing it? I would suggest adding some more  details here (or citations, if appropriate).- To add to the previous point, I would in general like to know more  about the stability properties of the full embedding. Can this easily  be quantified? For example, what happens if I add some noise to the  attributes? I would assume that stability is a function of the  selected aggregation function *and* the Wasserstein embedding itself.  Assuming that the former is fixed to be the function described by  Togninalli et al., does the latter satisfy, for example, Lipschitz  continuity? (I am asking this out of professional curiosity;  understanding the inherent properties of the embedding seems key to me  for us to understand better ways to generate such embeddings; the  empirical performance of the proposed method speaks for itself, of  course!)- Section 3.2 is rather technical at present, making use of  hitherto-undefined concepts. I would suggest improving this section  by providing more intuition, relegating some of the more technical  results to the appendix.- Does the pseudo-invertibility of $\phi$ cause any problems in  practice? It is my understanding that the embedding will not be  injective in general anyway, or am I mistaken?- How stable is the reference embedding? In Section 4.2, it is my  understanding that the reference embedding is by default obtained from  $k$-means, with $k$ being the average number of nodes. The appendix  depicts the results for another data-independent reference embedding  and states that there are no differences. Would this not suggest that  the way the reference distribution is obtained is irrelevant? If so,  why not use a normal distribution (which I would expect to be simpler  to calculate than a $k$-means embedding) for all experiments?  This point should be addressed somewhat better.- Why is the virtual node inclusion necessary? Only for the  simplification of the message passing itself? I am aware of this  standard modification, but I do not see why the proposed algorithm  could equally well work with the original graph.- Why is the variance of the proposed method high for some of the data  sets of the 'TUD' repository? Is this a consequence of the model that  was picked for working with the embeddings?# StyleThis paper is well written; it was a pleasure to read and review. Hereare some minor suggestions to improve style/clarity:- I would suggest so sort citations by some criterion (alphabetically,  for example) when citing multiple authors.- "See the recent survey" --> "see the recent survey"- "such transport plan" --> "such a transport plan" **Summary**The paper studies the implicit bias of gradient descent in the problem of matrix factorization. The objective is to gain some intuition on the effective regularization performed by gradient descent in more involved problems, e.g. neural networks. Previous papers conjectured that gradient flow (i.e. the continuous-time limit of gradient descent) implicitly regularized by the nuclear norm, while more recent ones conjectured that the regularization was done purely in terms of rank. This work provides an answer to this question in an important setting, i.e. they show analytically and numerically that for the problem of factorizing two matrices without any rank constraint, the gradient flow (GF) algorithm effectively minimizes the rank rather than the nuclear norm, if starting from an infinitesimal initialization. This is done by showing that GF is equivalent to an algorithm called Greedy Low-Rank Learning (GLRL). They also provide evidence to show that this phenomenon transfers to the factorization of a larger number of matrices, and that in this context it is actually stronger as its dependency on the infinitesimal initialization weakens. Note that given the length and available time, I did not check all calculations and experiments given in the supplementary material.**Main comments and overall decision**I found the paper well-written and pleasant to read. The results are quite clearly stated and explained. I think the paper would deserve some re-organization to better fit the 8-pages limit, as it feels a bit compressed, especially towards the end. I also believe that some numerics in the appendix should be included in the main text, clearing space by removing some technicalities. Moreover, the previous literature on greedy rank algorithms should be perhaps better discussed. I believe the paper should ultimately be accepted at ICLR 2021, after the authors have taken into account the comments and criticisms I expressed.**Strengths of the paper, by section**- The paper is overall well-written and clear. In particular, the introduction and related works give a very clear overview of the previous literature (however I have to say that I am not extremely familiar with the previous works on the implicit regularization of gradient descent, so I could be forgetting important works). - I genuinely enjoyed Section 4 which gives an intuition on the reasons behind this low-rank learning behavior of GF in very simple toy models. On a general note, the splitting of the paper into clear sections is very good for the readability of the paper.- Section 5 is quite technical, and I believe some of these technicalities could be moved to appendix to improve readability and gain space. Having generalities in Section 5.1 makes the intuition quite clear, especially with Thm 5.3. I found the counterexample 5.9 to be particularly simple and appropriate. Section 5.3 could also perhaps be shortened, by making the results even more informal: the intuition behind the iterative rank growth of the solution of GF is very interesting and should be emphasized more than the technicalities.- The treatment of depth in Lemma 6.1 and Theorem 6.2 is well presented. I noticed that in both Thm 6.2 and 5.3 the times considered in GF and GLRL are not the same, could the authors comment on this time difference, and its dependency on alpha ?**Concerns and remarks**- In general, the paper has many results and claims, however it feels like the 8-page limit is quite limiting the results, resulting in numerous references to the supplementary material. I think the paper should perhaps be shortened, removing some technicalities, for instance in Section 5. For instance the last paragraph of the paper (page 8) should either be removed, or the experiments should be included in the main text, but its current state makes it unclear. In this paragraph I also did not understand the sentence For depth-2 GLRL, the low-rankness is raised to some power smaller than 1, which depends on the eigenvalue, could the authors clarify it?- On Greedy Low Rank algorithms, the paper (if I am not mistaken) does not compare this GLRL algorithm to previous algorithms developed in the literature, e.g. in On Approximation Guarantees for Greedy Low Rank Optimization , ICML 2017 or Greedy Learning of Generalized Low-Rank Models (IJCA 2016). I have to admit that I am not very familiar with this literature, but I believe that this type of algorithm is not entirely new, and the authors should perhaps discuss it more, or at least clarify this in their response.- Figure 1 is the only numerical analysis presented in the main text (there are more in appendix). The numerics show that it requires really an extremely small initialization to see the closeness of GF and GLRL (it starts to be clear for initial norms around $10^{-12}$ !). This is counterbalanced with Figure 2 in the depth $L\geq 3$ setting, but this is only shown in appendix. In general, I believe that at least some part of Figure 2 should be included in the main text, perhaps merging it with Figure 1.**Minor remarks**- I was a bit confused when reading: after analyzing the simple linear case, the authors write But what if $W(t)$ grows to be so large that the linear approximation is far from the actual $f(W)$?, suggesting that the real behavior is not captured by the linear approximation, however to me it seems like Theorem 5.3 precisely shows under which conditions on the initialization one can use the linear approximation for the dynamics of GF.- In Section 4, when considering Matrix Sensing, appealing to Thm~1 and 4 of (Gissin&al 20) seems completely overkill and does not read well, as the first result is simply writing the ODE in the diagonal basis, and the second is simply solving an ODE by separation of variables.- What do the authors mean in Algorithm 1 by if $W_r$ is close to a local minimizer of f among PSD matrices ? Do they mean that going from rank r to (r+1) in GLRL does not change $W_r$ ?- In Theorem 6.3, why do the authors consider $inf_t$ and not a limsup? Is this simply for technical reasons in the proof, or is this genuinely possible that the trajectory escapes the GLRL solution for long enough times ?Finally, a list of typos :- Introduction: we show GF [&] converges  we show that GF [&] converges ?-- before GLRL reaches first stationary point  before GLRL reaches its first stationary point - Related works sections: which remains it unclear- Beginning of page 7: Which is 40 when $R = 40$  Which is 40 when $R = 10$]- Thm 5.10: the eigendecomposition of $W$  the eigendecomposition of $ \nabla f(W)$- Page 7 : A parenthesis is misplaced close to the end of the page. - Page 8 (middle) : Gf  GF- Page 8 : depth-2 solution of have  Something missing. - Page 8 : a but should be removed. Summary:The present paper proposes a novel approach for model selection for individual treatment effect (ITE) estimation in the unsupervised domain adaptation (UDA) setting. The motivation for this approach, called interventional causal model selection (ICMS), is to exploit causal invariance to choose a model that has both good predictive power and fits the a priori belief for causal relationships.More precisely, the authors prove a necessary condition for optimality of an ITE model and combine this condition with the classical target risk minimization from UDA model selection.This necessary condition states preservation of all conditional independencies of the causal DAG by the interventional distribution of the target domain.Recommendation:Clear accept. The framework proposed in this work constitutes an interesting new approach to model selection exploiting ideas from causal invariance and also allows to adapt existing ITE estimation methods to tackle covariate shift problems.Strong points: - This framework is theoretically motivated, using the structural causal model framework, and allows to improve various methods by allowing for model selection in UDA setting, as illustrated on a wide range of examples (simulations and real data). - The flexibility also extends to the quality of the data or prior knowledge, as this framework allows to incorporate as much expert structure knowledge as possible, or to specify its importance in the model selection via the hyperparameter $\lambda$.Weak points: - The role of the causal DAG is key to this approach, it is however difficult to assess from the experiments and discussion, how much the proposed framework relies on the correctness of the used DAG, or, put differently, how sensitive this framework is to mis-specifications of the DAG. - It would be interesting to see the code for the presented examples and also to test it on other applications. Will it be made public at some point? - The presented work is considerable and it many experiments are provided. However I think that the experiments section is too long compared to the main section (Section 4). For instance, (parts of) the appendix A could be moved into the main part while shortening the Section 5.2.Questions/Issues: - Sec. 5.2.: Reference for FGES is missing in the main part of the article (it's in the appendix but should also be in cited in Section 5.2) - Appendix B (additional related work) should be moved to the main part (Section 2). - Sec. 5.1: What is the reason for only shifting one ancestor of $Y$ in $G$? And can it also be, at the same time, an ancestor of the treatment, i.e., a confounder in the observational data (I think yes, since the intervention on the treatment variable cuts of incoming arrows, but has this been tested in practice as well?)? - Sec. 5.1: How do the authors choose the 30 candidate models for each model architecture? Is the variability across the different models similar for all comparable for all compared architecture?Minor comments (that did not impact the score): - p. 3: unconfoundness $>>$ unconfoundedness - p. 6: i.e $>>$ missing comma (i.e.,) - p. 6: return $>>$ returned - p. 7: For completeness, add reference for domain shift between rural and urban populations. - p. 7: hospital admission $>>$ hospital admissions This paper aims at devising a targeted approach that takes into account the specific structure of combinatorial problems with multiple solutions. The proposed approach leverages RL to select the best targets among the solution sets at each iteration. SelectR convincingly outperforms both the naive and a cleverer baseline, showcasing the applicability of the method.OriginalityThe problem of interest is relevant to a number of machine learning applications but has largely been ignored by the community up until now, as is made clear in the very complete related works section of the paper. Notably, the question of selecting the best target for learning is, as far as I am aware, novel. Consequently, so is both the approach and the baselines it is compared against.SignificanceAs ML practitioners try their hand at more and more complex problems, this approach will become more and more relevant. Further, since this paper is the first to define the one-of-many problem, sets out to define the general framework, and defines reasonable baseline, it is very relevant.The effort made to link the problem of interest with existing other problems mean that it's easier for readers to draw parallels, and helps bolster the paper's significance. For instance, the experimental results tend to show that naively using multiple possible solutions is worse than ignoring these data points. This is in direct contradiction with the general consensus for tasks such as machine translation, where multi-solution datasets are not available, but are longed for.ClarityOverall, the paper is well-written and easy to follow. A couple of things might be made clearer, though, including:- the description of the pretraining regimen, which is a bit convoluted. It would probably help to refer to the internal M for the selection module as a target network, which it seems to be.- the description of the reward for the selection module is a bit complicated too, and the the fact that solutions can be split into r components could be reminded here.It would also be helpful to give more insight into why this reward was chosen (I imagine this partial-reward makes it easier to 'see' some reward than a reward for exact matching, but I'm speculating here), and what the consequences of this choice are (aside from improved performance). Does the selection module opt for the 'easiest' targets to predict? Do the targets chance as training goes along? Could the selection module be trained at a meta-level rather than at the transition-level?Overall, this is a nicely-written paper offering a novel approach to a significant problem, and showcasing its performance improvements. It would make a nice addition to this year's ICLR. Summary: The paper examines the security of a recently proposed privacy scheme, InstaHide Huang et al. 2020, that can be used to generate synthetic training data. Under a standard Gaussian distributional assumption on the data, the authors propose an algorithm that can extract private information from the synthetic vectors generated by InstaHide.Comments:InstaHide is a recently proposed technique Huang et al. 2020, that aggregates local data into synthetic data that can both preserve the privacy of the local datasets and be used to train good models. InstaHide assumes presence of both public feature vectors (e.g. a publicly available dataset like ImageNet) and a collection of private feature vectors (which we want to hide). This scheme is based on an interesting looking idea, and also seems to achieve good empirical performance (in that by training a network like ResNet on the generated synthetic data one can achieve good test accuracy on the real data). This paper investigates the security properties of the InstaHide scheme. The main result shows that if the public and private data are both drawn from an i.i.d. Gaussian distribution then it is possible to reconstruct (some of) the private data if we have the access to output of the InstaHide on multiple queries and public data. The reconstruction is correct up to sign of the coordinates..This is an interesting result, implying that a careful discussion of security for InstaHide should be sensitive to the properties of the distributions generating the private and public feature vectors.The paper is well-presented, however, most of the interesting theoretical results are in the appendices. The idea behind the reconstruction can be broken into four steps: a) identify the public coordinates of the selection vector, b) construct the Gram matrix of the selection vectors, c) identify a particular submatrix of the Gram matrix, and d)  use this particular submatrix to identify private feature vectors.I have not verified all the technical details, but overall the reconstruction idea looks sound. The theoretical results are also backed by some simple proof-of-concept experiments. Some questions:1)Would have nice to see the formal statement of the primary theorem in the main paper. Also what is the overall probability of success in Theorem A.1?2)Can be the Gaussian assumption on the X matrix be relaxed to a subgaussian assumption. Side-remark: I thought the title of the paper is slightly misleading. The result is specific to InstaHide scheme and is not a general statement about private distributed learning. Summary:This work proposes the use of sound prediction as intrinsic reward to guide reinforcement learning (RL) exploration. Sounds are often directly related to causal effects of actions and interactions. By modeling different sound event classes via a clustering algorithm, the cluster prediction errors are used as intrinsic rewards for RL. The proposed approach is tested on three different simulation environments. The results on 22 different environments (20 Atari games, Habitat and TDW environments) illustrate that using exploration capability of the proposed approach and improved performance in comparison to other baseline methods leveraging intrinsic reward modules for RL exploration.########################Pros:- The idea is novel and of interest to the RL community at large. The approach is clear and easy to follow. - Comprehensive experimental analysis and convincing results. Specifically, the analysis of dominant sound types in Atari games is well done. It helps understand that the Atari games for which there are no gains with the proposed method have dominantly background sounds. It also helps isolate the games in which sounds are causal effects of actions or events.- The ablative analysis demonstrates well that sound event prediction is a beneficial way to use audio as intrinsic rewards.- Testing on different variety of test beds also highlights the exploration capability of the proposed work in comparison to other intrinsic reward methods.########################Cons:- The design choice of using texture features for audio is unclear and not motivated well. What properties do they capture that are desirable for the RL domains used for experimentation? It is also unclear how these are computed. Some more insights about the choice of audio features will be helpful to the reader.- If distances between sound texture features do not capture causal structure of the auditory events, then why are they suitable to be used for clustering? Wouldnt the same problem carry over? Why should clusters be assumed to capture inherent causal structure when distances between sound texture features are used to create the clusters in the first place?- The writing requires more clarity (suggestions below).########################Reason for score:The idea is novel and results presented look great. The writing needs some rework (suggestions for improvements below) along with some explanations for feature computation and experimental conditions. I recommend for acceptance provided the authors revise the manuscript as per requested changes.########################Questions during rebuttal:- Please refer to the questions in the Cons section and other feedback.- Comparisons are performed with methods using intrinsic motivation modules. How would the proposed work compare with other prior work where audio is used to aid RL (not necessarily as an intrinsic module), such as Aytar et al., 2018; Omidshafiei et al., 2018?########################Some typos and other feedback:- Consider using the \citet command when referring to the authors of a reference paper in a sentence. (Eg: Silver et al., 2016 concluded that& versus (Silver et al., 2016) concluded that &)- Introduction, paragraph 1: &a range of intrinsic reward function. -> &a range of intrinsic reward functions.- Introduction, last paragraph (prior to bullets): provide citations for the Atari domain, and expand the first usage of TDW.- Related Work, RL explorations: Tompson sampling -> Thompson sampling- Related Work, RL explorations: Osband reference missing the year in the references section and the citation.- Related Work, RL explorations: &uses the errors of predicting the next state& -> use the errors of predicting the next state&- Figure 1 caption: The agent start to collect a diverse set& -> The agent starts to collect a diverse set..- Figure 1 caption: ..and then cluster them into & - > ..and then clusters them into &- Figure 1 caption: .. the agent use errors of auditory events& -> .. the agent uses errors of auditory events&- Figure 2 caption: end the sentence with respectively- In several places: Forward dynamic network -> forward dynamics network- Section 3.2: space missing between s_v,t & and- Section 3.2: ..agent actions A_t -> ..agents actions a_t- Section 3.2, paragraph 2, sentence 1: The audio clip is represented by s_t or s_s,t?- Section 3.3, paragraph 1, last sentence: ..details of these two-phase below -> ..details of these two phases below- Section 3.3, Sound clustering: Formally define silhouette score or give an insight for what it captures for the reader.  What is the criteria to determine a new cluster will be created?- Section 3.3, Sound clustering: automatic decide the best K -> automatically decide the best K- Section 3.3, Auditory event predictions, last paragraph: It will reward the agent at the that stage& This is a grammatically incorrect sentence. Possible correction could be: It will reward the agent at that stage and encourage it to visit more such states since it is uncertain about this scenario.- Section 3.3, Auditory event predictions, last paragraph: &avoid dying scenario in the game& -> &avoid dying scenarios in the game..- Section 3.3, Auditory event predictions, last paragraph: .. and keeping seeking novel events& -> .. and seeking novel events&- Section 4, paragraph 1: Provide citations for Atari, Habitat and TDW environments.- Table 1 caption: ..bold front -> ..bold font- Section 4.1: Consider formally defining extrinsic rewards since it is used several times in the draft.- Section 4.1, Atari Game Environment: ..also support an audio API.. -> ..also supports an audio API.. - Section 4.1, Atari Game Environment: ..contain the sound effects to compare.. -> ..contain sound effects to compare.. - Section 4.1, Atari Game Environment: The last sentence is grammatically incorrect. Consider replacing it with We follow the standard setting in Pathak et al, 2017; Burda et al., 2018b, where an agent can use external rewards as an evaluation metric to quantify the performance of the exploration strategy. This is because doing well on extrinsic rewards usually requires having explored thoroughly.- Section 4.1, Audio-Visual Explorations on Habitat: ..with Habitat simulator for experiments. ->  ..with the Habitat simulator for experiments. - Section 4.1, Audio-Visual Explorations on Habitat: We follows the setting from & -> We follow the setting from &- Section 4.1, Audio-Visual Explorations on Habitat: & can hear the different sound when moving. -> &can hear the different sounds when moving.- Section 4.1, Baselines: four baselines are used instead of five as stated in this paragraph.- Section 4.1, Baselines: state-of-the-arts -> state-of-the-art- Section 4.1, Implementation details: ..our model use 10K interaction for stage 1& -> ..our models use 10K interactions for stage 1& - Section 4.2, Result Analysis: What is implied by positive, negative and meaningless sounds? Examples?- Section 4.2, Result Analysis: .. our algorithm works well under what circumstances. -> .. under what circumstances our algorithm works well.- Section 4.2, Result Analysis, paragraph 2: .. event-driven sounds compare with those with action-driven sounds -> .. event-driven sounds compared to those with action-driven sounds- Section 4.2, Result Analysis, paragraph 2: ..when the sounds effects mainly consist& -> ..when the sound effects mainly consist&- Section 4.2, Sound clustering or auditory event prediction and Fig 4.: It is unclear how the Clustering only condition is different from the proposed approach? What is the loss for clustering only that is used as intrinsic reward?- Section 4.3, last sentence: Several grammatical errors. Consider replacing with: We also compute the cluster distances of both models and find that the sound clusters discovered by active exploration are more diverse, thus facilitating the agents to perform in-depth explorations.- Section 4.3, sentence 1: Consider replacing with: To evaluate the exploration ability of agents trained with our approach, we report the unique state coverage, given a limited exploration budget.- Section 4.4, Setup: The action is repeated 4 times on each frame. -> An action is repeated 4 times on each frame- Section 4.4, Result Analysis: .. intrinsic reward rewards in Figure 7. -> ..intrinsic rewards in Figure 7.- Section 4.4, Result Analysis: ..prediction errors on latent features space& -> ..prediction errors on the latent feature space..- Conclusion: & prediction as an exploration bonuses, which allows RL agent& -> & prediction as an exploration bonus, which allows an RL agent&- Conclusion: Based on the experimental result above, we therefor conclude that sound conveys& -> Based on the experimental results above, we conclude that sound conveys& This is an interesting paper on an idea introduced by the authors as active tuning. This paper is well-written and clearly explains the proposed active tuning scheme. I read the paper carefully multiple times, and feel that a few inclusions will help the readers better understand the proposed method.At the base level, this paper builds on optimizing the internal dynamics of a recurrent neural network unlike optimizing internal weights in traditional sequence-to-sequence mapping. This is achieved by decoupling the recurrent neural activities from the input temporal signal and propagating the error (the difference between the estimated input value and the observed input value of the input signal) to tune the internal dynamics of the network. To demonstrate the effectiveness of active tuning the authors trained a distributed graph recurrent neural network (DISTANA) on three datasets with increasing complexity. Datasets included: multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics.  On average ten independent experiments were performed and the effectiveness of active tuning was evaluated using root mean square (RMS).  Samples for the experiments were generated using five different noise ratios between 0 and 1 to measure the effectiveness of the proposed method for noisy data scenarios. The network was also trained on no noise to 0.05 noise induced into training data to see if it would help the models better generalize. The results as depicted in graphs show that active tuning is not only robust but generalizes well on noisy data. Recommendations:1. The active Tuning algorithm itself is missing from this paper. Even though the explanation is clear, it would help the readers to see the algorithm itself for better understanding. The reviewer referred to Hidden Latent State Inference in a Spatio-Temporal Generative by karlbauer et. al. 2020 (arXiv:2009.09823) for the algorithm. 2. The authors confirm that 10000 and 1000 samples have been generated for all the problem domains tested. However, it is not clear if steps were in place to make sure that no bias was introduced during this sample generation. 3. While the tuning length and tuning cycles were fixed for all three datasets, it is important to see how these values can be optimized based on the complexity of the time series data. Experimental results using a range of values for tunning length and tuning cycles would be beneficial. In the manuscript entitled "Neural Causal Discovery with Learnable Input Noise" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).  The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state ("learnable noise risk") with a flexible functional approximation (neural network).  Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.  The simulation and real data experiments are interesting and seem well applied.A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.  In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.  Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.  In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.  Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream). This is a nice improvement on Equilibrium Propagation (EqProp) based on training a separate network to initialize (and speed-up at test time) the recurrent network trained by EqProp. The feedforward network takes as laywerwise targets the activities of each layer when running the recurrent net to convergence (s-). The surprising result (on MNIST) is that the feedforward approximation does as well as the recurrent net that trains it. This allows faster run-time, which is practically very useful.My main concern is with the mathematical argument in section 2.2. s* is not the same as s- , and in general, it is not clear at all that there should be a phi* such that s*=s-. Also, the derivation in eqn 12 assumes that w is very close to w*, which is not clear at all. So this derivation is more suggestive, and the empirical results are the ones which could be convincing. My only concern there is that the only experiments performed are on MNIST, which is known to be easily dealt with using the kind of feedforward architectures studied here. Things could break down if much more non-linearity (which is what the fixed point recurrence provides) is necessary (equivalently this would correspond to networks for which much more depth is necessary, given some budget of number of parameters). I don't think that this is a deal-breaker, but I think that this section needs to be more prudent in the way that it concludes from these observations (the math and the experiments).One question I have is about biological plausibility. The whole point of EqProp was to produce a biologically plausible variation on backprop. How plausible is it to have two sets of weights for the feedforward and recurrent parts? That is where a trick such as proposed in Bengio et al 2016 might be useful, so that the same set of weights could be used for both.It might be good to mention Bengio et al 2016 in the introduction since it is the closest paper (trying to solve the same problem of using a feedforward net to approximate the true recurrent computation), rather than pushing that to the end.In sec. 1.1, I would replace 'training a Continuous Hopfield Network for classification' by 'energy-based models, with a recurrent net's updates corresponding to gradient descent in the energy'. The EqProp algorithm is not just for the Hopfield energy but is general. Then before eq 1, mention that this is the variant of Hopfield energy studied in the EqProp paper.I found a couple of typos (scenerio, of the of the). Summary: They propose a biologically motivated short term attentive working memory (STAWM) generative model for images. The architecture is based on Hebbian Learning (i.e. associative memories are represented in the weight matrices that are dynamically updated during inference by a modified version of Hebbian learning rule). These memories are sampled from glimpses on an input image (using attention on contextual states, similar to [1]), in addition to a latent, query state. This model learns a representation of images that can be used for sequential reconstruction (via a sequence of updates, like a sketchpad, like DRAW [1], trained in an unsupervised manner). These memories produced by drawing can also be used for semi-supervised classification (achieves very respectable and competitive results for MNIST and CIFAR-10).This paper is beautifully written, and the biological inspiration, motivation behind this work, and links to neuroscience literature as well as relation to existing ML work (even recent papers) is well stated. The main strength of this paper is that the author went from a biologically inspired idea to a complete realization of the idea in algorithmic form. The semi-supervised classification results are competitive to SOTA, and although the CIFAR-10 reconstruction results are not great (especially compared to generative adversarial models or recent variation models [2]), I think the approach is coming from a very different angle that is different enough compared to the literature to warrant some attention, or at least a glimpse, so to speak, from the broader community. The method may offer new ways to interpret ML models that is current models lack, which in itself is an important contribution. That being said, the fact that most adversarial generative models achieved a far better performance raises concern on the generalization ability of these memory-inspired learned representations, and I look forward to seeing future work investigate this area in more detail.The authors also took great care in writing details for important parts of the experiments in the Appendix section, and open sourced the implementation to reproduce all their experiments. Given the complex nature of this model, they did a great job in writing a clear explanation, and provided enough details for the community to build biologically inspired models for deep networks. Even without the code, I felt I might have been able to implement most of the model given the detail and clarity of the writing, so having both available is a great contribution.I highly recommend this paper for acceptance, with a score of 8. The paper might warrant a score of 9 if they had also achieved higher quality results for image generation, on Celeb-A or demonstrated results on ImageNet, and provided more detailed analysis about drawbacks of their approach vs conventional generative models.[1] https://arxiv.org/abs/1502.04623[2] https://arxiv.org/abs/1807.03039 Overall Score: 8.5/10.Confidence Score: 3/10. (This paper includes so many ideas that I have not been able to prove that are right due tomy limited knowledge, but I think that there are correct).Summary of the main ideas: This paper establishes a theoretical correspondence between BCNN with many channels and GP andpropsoes a Monte Carlo method to estimate the GP corresponding to a NN architecture. It is a very strong and completepaper since its gives theoretical contents and experiments content. I think that it is a really good result that shouldbe read by anyone interested in Neural Network and GP equivalences, and that Machine Learning in general needs these kindof papers that establish this complicated equivalences.Related to: The work by Lee and G. Matthews (2018) regarding equivalence between Deep Neural Networks and GPs and theConvolutional Neural Network framework.Strengths:Theoretical content, Experiments and methodology content (even a Monte Carlo approach) makes it a very complete paper.Having been able to establish complicated and necessary equivalences.Weaknesses:Very difficult for newcomers or non expert technical readers.Does this submission add value to the ICLR community? : Yes, it adds, and a lot.Quality:Is this submission technically sound?: Yes it is, it is a necessary step in GP-NN equivalence research.Are claims well supported by theoretical analysis or experimental results?: Yes, quite sure.Is this a complete piece of work or work in progress?: Complete piece of work.Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?: Yes, they are.Clarity:Is the submission clearly written?: Yes, but I suggest giving formal introductions to some concepts in the introductionand include a figure with the ideas given or the equivalences.Is it well organized?: Yes, although sometimes section feel a little but put one after the another. More cohesion would beadded if they are introduce before.Does it adequately inform the reader?: Yes.Originality:Are the tasks or methods new?: The monte carlo is new, the other methods not but the task of the equivalence is new.Is the work a novel combination of well-known techniques?: It is kind of a combination, but the proposed ideas are new, it is very theoretical.Is it clear how this work differs from previous contributions?: Yes, authors bother in explaining it clearly.Is related work adequately cited?: Yes, this is a huge positive point of the paper.Significance:Are the results important?: From my point of view, yes they are.Are others likely to use the ideas or build on them?: I think so, because the topic is hot right now.Does the submission address a difficult task in a better way than previous work?: It is a new task.Does it advance the state of the art in a demonstrable way?: Yes, clearly.Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?: Yes, the theoretical approach is sound.Arguments for acceptance: It is a paper that provides theory, methodology and experiments regarding a very difficult and challenging task that add value to the community and makes progress in the area of the equivalence between NN and GPs.Arguments against acceptance: I do not have.Typos:-&gt; Define the channel concept in introduction.-&gt; Put in bold best results of the experiments.-&gt; Why not put "deep" in the title?-&gt; In the introduction, introduce formally a CNN. (brief)-&gt; Define the many channel limit.-&gt; Put a figure with the equivalences and with the contents of the paper explaining a bit. The authors frame continual learning as a meta-learning problem that balances catastrophic forgetting against the capacity to learn new tasks. They propose an algorithm (MER) that combines a meta-learner (Reptile) with experience replay for continual learning. MER is evaluated on variants of MNIST (Permutated, Rotations, Many) and Omniglot against GEM and EWC. It is further tested in two reinforcement learning environments, Catcher and FlappyBird. In all cases, MER exhibits significant gains in terms of average retained accuracy.Pro'sThe paper is well structured and generally well written. The argument is both easy to follow and persuasive. In particular, the proposed framework for trading off catastrophic forgetting against positive transfer is enlightening and should be of interest to the community. While the idea of aligning gradients across tasks has been proposed before (Lopez-Paz &amp; Ranzato, 2017), the authors make a non-trivial connection to Reptile that allows them to achieve the same goal in a surprisingly simple algorithm. That the algorithm does not require tasks to be identified makes it widely applicable and reported results are convincing. The authors have taken considerable care to tease out various effects, such as how MER responds to the degree of non-stationarity in the data, as well as the buffer size.  Im particularly impressed that MER can achieve such high retention rates using only a buffer size of 200. Given that multiple batches are sampled from the buffer for every input from the current task, Im surprised MER doesnt suffer from overfitting. How does the train-test accuracy gap change as the buffer size varies?The paper is further strengthened by empirically verifying that MER indeed does lead to a gradient alignment across tasks, and by an ablation study delineating the contribution from the ER strategy and the contribution from including Reptile. Notably, just using ER outperforms previous methods, and for a sufficient large buffer size, ER is almost equivalent to MER. This is not surprising given that, in practice, the difference between MER and ER is an additional decay rate ( \gamma) applied to gradients from previous batches. Con'sI would welcome a more thorough ablation study to measure the difference between ER and MER. In particular, how sensitive is MER is to changes in \gamma? And could ER + an adaptive optimizer (e.g. Adam) emulate the effect of \gamma and perform on par with MER. Similarly, given that DQN already uses ER,  it would be valuable to report how a DQN with reservoir sampling performs.I am not entirely convinced though that MER maximizes for forward transfer. It turns continual learning into multi-task learning and if the new task is sufficiently different from previous tasks, MERs ability to learn the current task would be impaired. The paper only reports average retained accuracy, so the empirical support for the claim is ambiguous.The FlappyBird experiment could be improved. As tasks are defined by making the gap between pipes smaller, a good policy for task t is a good policy for task t-1 as well, so the trade-off between backward and forward transfer that motivates MER does not arise. Further, since the baseline DQN never finds a good policy, it is essentially a pseudo-random baseline. I suspect the only reason DQN+MER learns to play the game is because it keeps "easy" experiences with a lot of signal in the buffer for a longer period of time. That both the baseline and MER+DQN seems to unlearn from tasks 5 and 6 suggests further calibration might be needed. This paper proposes a dynamical neural network for sparse coding where all the interactions terms are learned.  In previous approaches (Rozell et al.) some weights were tied to the others.  Here the network consists of feedforward, lateral, and feedback weights, all of which have their own learning rule.  The authors show that the learned weights converge to the desired solution for solving the sparse coding objective.  This seems like a nice piece of work, an original approach that solves a problem that was never really fully resolved in previous work, and it brings things one step closer to both neurobiological plausibility and hardware implementation.Other comments:What exactly is being shown in Figure 2 is still not clear to me. It would be nice to see some other evaluations, for example sparsity vs. MSE tradeoff (this is reflected in the objective function in part but it would be nice to see the tradeoff).  There is recent work from Mitya Chklovskii's group on "similarity matching" that also addresses the problem of developing a fully local learning rule.  The authors should incorporate a discussion of this in their final paper. Language is hierarchically structured: smaller units (e.g., noun phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the different units of an LSTM can learn to track information at different time scales, the standard architecture does not impose this sort of strict hierarchy. This paper proposes to add this constraint to the system by ordering the units; a vector of "master" input and forget gates ensures that when a given unit is reset all of the units that follow it in the ordering are also reset.Strengths:* The paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly.* The evaluation tasks are very sensible. It's good that the model is shown to obtain good perplexity and slightly improve over an LSTM baseline; it's not the state of the art, but that's not the point of the paper (in fact, I would emphasize that even more than the authors do). The unsupervised parse evaluation (Table 2) is the heart of the paper, in my opinion (and should probably be emphasized more) -- the results from the second layer are quite impressive.* The (mildly) better performance than LSTMs on long-distance dependencies, and (mildly) worse performance on local dependencies, in the Marvin &amp; Linzen dataset, is interesting (and merits additional analysis).Weaknesses:* The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused. I am not sure that neural networks with latent syntactic structures can really address the seemingly very fundamental question mentioned in the first paragraph (whether syntax is related to "an underlying mechanism of human cognition") - I would suggest eliminating this part. At the same time, the authors might want to add another motivation for studying architectures that discover latent structure (as opposed to being given that structure) - this setting corresponds more closely to human language acquisition, where children aren't given annotated parse trees.* The authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the LSTM is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences.* There is limited analysis of the model. Why does the second layer show better unsupervised parsing performance than the third layer? (Could this be related to syntactic vs. semantic/discourse units I mention in the previous bullet?) Why is the model better at ADJP boundaries than NP boundaries? It would have been more useful to report less experiments but analyze the results of each experiment in greater depth.* In this vein, I am not sure it's useful to include WSJ10 in Table 2, which is busy as it is. These sentences are clearly too easy, as the right branching baseline shows, and require additional POS tagging.* I found it difficult to read Figure A.2: could you help us understand what we should take away from it? * It is not entirely clear why the model needs both unit-specific forget/input gates and the "master" forget/input gates, and there is no discussion of this issue. Have you tried using only the "master" gates?Minor notes:* RNNGs are described as having an explicit bias to model syntactic structure; this is an arguably confusing use of the word "bias", in that the architecture has a hard constraint enforcing syntactic structures (bias implies a soft constraint).* There are some language issues: agreement errors (e.g. "have" in the sentence that starts with "Developing" in the introduction), typos ("A order should exist", "co-occurance"), determiner issues ("values in [the] master forget gate", "when the overlap exists") - I would suggest going through and copy editing the paper.* "cummax" seems like a better choice of name for cumulative maximum than "cumax".* It may be helpful to remind the reader of the update equation for c_t in a standard LSTM.* Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?* It's not clear if the results in Table 2 reflect the best seed out of five (as the title of the column "max" indicates) or the average (as the caption says). The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) end-to-end always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training.  Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. The proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering&), I envision this paper to generate important impacts for practitioners pursuing those ideas. Additionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. I therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify &amp; improve in revision:1.Eqn (7) assumes noise-free case. The author stated The zero-noise assumption is for simplicity of the proofs. Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less simpler way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTAs performance under noise?2.Section 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA?3.The writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, the x-axes denotes is the indices of layers should remove is. Please make sure more proofreading will be done. This paper proposes a (CNNs) architecture for encoding and decoding images for compressed sensing. In standard compressed sensing (CS), encoding usually is linear and corresponds to multiplying by a fat matrix that is iid gaussian. The decoding is performed with a recovery algorithm that tries to explain the linear measurements but also promotes sparsity. Standard decoding algorithms include Lasso (i.e. l1 regularization and a MSE constraint) or iterative algorithms that promote sparsity by construction. This paper instead proposes a joint framework to learn a measurement matrix Phi and a decoder which is another CNN in a data-driven way. The proposed architecture is novel and interesting.  I particularly liked the theoretical motivation of the used MSE loss by maximizing mutual information. The use of parallel convolutions is also neat and can significantly accelerate inference, which can be useful for some applications. The empirical performance is very good and matches or outperforms previous state of the art reconstruction algorithms D-AMP and Learned D-Amp. On comparisons with prior/concurrent work: The paper is essentially a CNN autoencoder architecture but specifically designed for compressed sensing problems. There is vast literature on CNN autoencoders including (Jiang 2017 and Shi 2017) paper cited by the authors. I think it is fine to not compare against those since they divide the images into small blocks and hence have are a fundamentally different approach. This is fine even if block-reconstruction methods outperform this paper, in my opinion: new ideas should be allowed to be published even if they do not beat SOTA, as long as they have clearly novel ideas. It is important however to discuss these differences as the authors have done in page 2.  Specific comments: 1. It would be interesting to see a comparison to D-Amp and LDAmp for different number of measurements or for different SNRs (i.e. when y = Phi x+ noise ). I suspect each method will be better for a different regime?2. The paper: `The Sparse Recovery Autoencoder' (SRA) by Wu et al. https://arxiv.org/abs/1806.10175is related in that it learns both the sensing matrix and a decoder and is also focused on compressed sensing, but for non-image data. The authors should discuss the differences in architecture and training. 3. Building on the SRA paper, it is possible that the learned Phi matrix is used but then reconstruction is done with l1-minimization. How does that perform for the matrices learned with DeepSSRR?4. Why is Figure 1 going from right to left? This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method.pros:- The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution.- The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years.- The large literature on the subject has been well covered in the introduction.- The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point.- The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. remarks:- part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that?- part 4.2 : the consistency part is too condensed and not clear enough.- experiments : what about computation time?- More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets?minor comments:- part 1: 'but also IN weakly-supervised learning'- part 2. related work : post- precessing --&gt; post-processing- part 2. related work : it is proven THAT the minimal number of U sets...- part 2. related work : In fact, these two are fairly different --&gt; not clear, did you mean 'Actually, ..' ?- part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing.- part 5.1 Analysis of moving ... closer: ... is exactly THE same as before.- part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' Summary: The authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation.  Further details:While the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed:- How does the natural extension of UU learning extend beyond the binary setting? - As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. - In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. - In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the "arbitrary binary classifier" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. Minor issues: -At the bottom of page 3 the authors state, " In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen &amp; Williamson (2018). " It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. - In the first sentence of Section 3.1 "imagining" is mistyped as "imaging."- What does "classifier-calibrated" mean in Section 3.1? - In Section 3.1, "That is why by choosing a model G, g = arg mingG R(g) is changed as the target to which" was a bit unclear at first. The phrase "is changed as the target to which" was confusing because of the phrasing. Upon second read, the meaning was clear. - In the introduction it was stated "impossibility is a proof by contradiction, and the possibility is a proof by construction." It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed.- In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. - Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. Summary:This paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images. The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:-Increasing batch size (8x) and model size (2x)-Splitting noise z in multiple chunks, and injecting it in multiple layers of the generator-Sampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled. This seems to be used only at test-time and is used to control variety-fidelity tradeoff. The generator is encouraged to be smooth using an orthogonal regularization term.In addition, the paper proposes practical recipes for characterizing collapse in GANs. In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse. In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs. Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.Strengths:-Proposed techniques are intuitive and very well motivated-One of the big pluses of this work is that authors try to "quantify" each proposed technique with training speed and/or performance improvement. This is really a good practice.-Detailed analysis for detecting collapse and improving stability in large-scale GAN-Probably no need to mention that, but results are quite impressiveWeaknesses:-Computational budget required is massive. The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.Comments/Questions:-Can you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance? -It is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs. Providing such analysis would be also helpful for the community.-How do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?Overall recommendation:The paper is well written, ideas are well motivated/justified and results are very compelling. This is a good paper and I higly recommend acceptance. The paper presents algorithms for optimization using sign-SGD when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates. They also run optimization experiments on synthetic data. Additionally, they demonstrate superiority of the algorithm in the number of oracle calls for black box adversarial attacks for MNIST and CIFAR-10. The provided algorithm has optimal iteration complexity from a theoretical viewpoint. The paper was, overall very well written and sufficient experiment were presented. The math also seems correct. However, I think they should have explained the motivation for the need of developing such an algorithm better. Section 3 can be improved. I think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent. However, the ideas and the estimators are not novel. They show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm. This paper proposes to feed the representations of various external "teacher" neural networks of a particular example as inputs to various layers of a student network. The idea is quite intriguing and performs very well empirically, and the paper is also well written.  While I view the performance experiments as extremely thorough, I believe the paper could possibly use some additional ablation-style experiments just to verify the method actually operates as one intuitively thinks it should.   Other Comments:- Did you verify that in Table 3, the p_w values for the teachers trained on the more-relevant C10/C100 dataset are higher than the p_w value for the teacher trained on the SVHN data?  It would be interesting to see the plots of these p_w over the course of training (similar to Fig 1c) to verify this method actually operates as one intuitively believes it should.- Integrating the teacher-network representations into various hidden layers of the student network might also be considered some form of neural architecture search (NAS)  (by including parts of the teacher network into the student architecture). See for example the DARTS paper: https://arxiv.org/abs/1806.09055which similarly employs mixtures of potential connections.  Under this NAS perspective, the dependence loss subsequently distills the optimal architecture network back into the student network architecture.Have you verified that this method is not just doing NAS, by for example, providing a small student network with a few teacher networks that haven't been trained at all? (i.e. should not permit any knowledge flow)- Have the authors considered training the teacher networks jointly with the student? This could be viewed as teachers learning how to improve their knowledge flow (although might require large amounts of memory depending on the size of the teacher networks).- Suppose we have an L-layer student network and T M-layer teacher networks.Does this imply we have to consider O(L*M*T) additional weight matrices Q?Can you comment on the memory requirements?- The teacher-student setup should be made more clear in Tables 1 and 2 captions (took me some time to comprehend).- The second and third paragraphs are redundant given the Related Work section that appears later on. I would like to see these redundancies minimized and the freed up  space used to include more results from the Appendix in the main text. The paper describes a new learning framework, based on generativeadversarial imitation learning (GAIL), that is able to learn sub-taskspolicies from unsegmented demonstrations. In particular, it followsthe ideas presented in InfoGAIL, that depends on a latent variable,and extend them to include a sequence of latent variables representingthe sequence of different subtasks. The proposed approach uses apre-training step, based on a variational auto-encoder (VAE), toestimate latent variable sequences. The paper is well written andrelates the approach with the Options framework. It also shows,experimentally, its performance against current state-of-the-artalgorithms.  Although the authors claim in the appendix that the approach isrelatively independent on the dimensionality of the context variable,this statement needs further evidence. The approach is similar to HMMswhere the number f hidden states or latent variables can make adifference in the performance of the system.Also, it seems that the learned contexts do not necessarily correspondto meaningful sub-tasks, as shown in the circle-world. In this sense,it is not only relevant to determine the "right" size of the contextvariable, but also how to ensure a meaningful sub-task segmentation. Training for Faster Adversarial Robustness verification via inducing RELU stabilityAs I am familiar yet not an expert on adversarial training and robustess, my review will focus mainly on the overall soundness of the manuscript. I also only went superficially into the quantitative results.Summary:The authors are interested in the problem of verifying neural networks models trained to be robust against adversarial attacks. The focus is on networks with relu activations and adversarial perturbations within an epsilon l1-ball around each input, and the verification problem consists in proving the network performs as intended for all possible perturbations (infinitely many)The review on verification is clear. Elements that affect verification time are introduced and well explained in main text or appendix from both intuitive and theoretical perspective: l1 penalty, weight pruning, relu stability. These can be summ\arized as : you want few neurons, and you want them to operate in the same regime for all inputs, both to avoid branching. Relu stability is apparently a new concept and the proposed regularization approximately enforces relu stability.The approximation [itself using the novel improved interval arithmetic] based bounds on unit activations propagated through the network seems not to scale well with depths (more units are mis-labelled as relu unstable, hence wrongly regularized if I understand correctly). The authors acknowledge and document this fact but I would like to hear more discussion on this feature and on the trade-off that still make this approach worthwhile for deeper networks.This regularization does not help performance but only paves the way for a faster verification, for this reason the term co-design is used.The rest of the manuscript is a thorough empirical analysis of the effect of the penalties/regularizations on the network and ultimately on the verification time, keeping an eye on not deteriorating the performance of the network.How much regularization can be added seems to be indeed an empirical question since networks are over-parametrized in the first place with no clear way to a priori quantify task or model complexity.The devil is in the details and in practice implementation seems not straightforward with a complex optimization with varying learning rates and different regularizations applied at different time along the way. But this seems to be the case for most deep learning paper.The authors claim and provide evidence to be able to verify network well beyond the scope of what was achievable before due to the obtained speed-ups, which is a notable feature.Overall, this manuscript is well structured, thorough and pleasant to read, and I recommend it to be accepted for publication at ICLR TL;DR. Significant contribution to meta-learning by incorporating latent metrics on labels.* SummaryThe manuscript builds on the observation that using structured information from the labels space improves learning accuracy. The proposed method --CAML-- is an instance of MAML (Finn et al., 2017), where an additional embedding is used to characterize the dissimilarity among labels.While quite natural, the proposed method is supported by a clever metric learning step. The classes are first represented by centroids and an optimal mapping $\phi$ is then learnt by maximizing a clustering entropy (similarly to what is performed in a K-means-flavored algorithm, though this connection is not made in the manuscript). A conditional batch normalization (Dumoulin et al., 2017) is then used to model how closeness (in the embedding space $f_\phi$) among labels is taken into account at the meta-learning level.Existing literature is well acknowledged and I find the numerical experiments to be convincing. In my opinion, a clear accept.* Minor issues- I would suggest adding a footnote explaining why Table 1 reports confidence intervals and not just standard deviations. How are constructed those intervals?- Section 3.2 bears ambiguity as the manuscript reads "We first define centroids [...]" depending on $f_\phi$ which is then defined as the argument of the minim of the entropy term. What appears as a circular definition is merely the effect of loose writing yet I am afraid it would confuse readers. I would suggest to rewrite this part, maybe using a pseudo-code to better make the point that $f_\phi$ is learnt. The paper proposes a control architecture for learning task oriented whole body behaviors  in simulated humanoid robots bootstrapped with motion capture data.The authors use a hierarchical approach, where the low-level controllers are trained to follow motion captured data whereas the high-level control combines them. The topic of the paper is interesting and the language is understandable. The paper discusses and compares different ways to achieve such a higher level control.It probably wont be useful for real robots, but will be possibly useful for computer graphics.I suspect that code will not be published anytime soon, and I am afraid it will be hard to reproduce without. There is a solid software engineering involved and the system has many parameters.  The related work section (or lack thereof) can be improved. What is the advantage of this work over the multi-skill integration in Peng et al 2018? Please explain explicitly in the paper.The end-to-end approach seems a bit too weak to me. The video shows more artifacts than other similar papers, (cf. Heess et al. 2017). Whats the detail of the training for the end-to-end baseline?Are the environments randomized in each rollout? If not then this would need an ablation study which ablates memory/vision to prove its claim of integrating vision and memory. How much is the memory used in the tasks where nothing needs to be memorized?Is there any noise in the simulations?One weakness is that the low-level controllers are not adapted any further. That is probably why the fragments outperformed the transition policies etc., because the higher level policy has more flexibility.Overall, from the perspective of deep learning, I think the paper is novel and provides some insights into different approaches to the problem. The reviewer finds that the proposed method interesting. The model is very clean, and the implication in causal inference is significant. The writing is also clean and clear. The reviewer has several concerns:1) the algorithm seems not very scalable. In the two subproblems, there is one solved by a large number of parallel SDRs. SDR is quite expensive, and for each column in the data matrix one has to solve an SDR in each iteration. This is too much for large scale recommender systems. In fact, in the experiment 1 on MovieLens, the algorithm was only tested on a not-so-large dataset and run 5 iterations. The reviewer feels that more scenarios should be tested (e.g., more iterations, various sizes of dataset, etc.). Fixing the number of iterations also sounds a bit funny since it is more intuitive to stop the algorithm using some validation set or when the algorithm converges under a certain criterion.2) The algorithm works with *probability* of binary data. This is quite hard to estimate in practice. For example, people ``'likes'' a movie for only once. It is hard to tell what is the probability of generating this ````"like". It seems that the experiment part of this paper did not clearly state how to obtain the probability that the algorithm needs.3) The proposed method is a special nonnegative matrix factorization, which could be unidentifiable. How to circumvent such situation? Since identifiability of NMF affects interpretability a lot. Summary:Standard CNN models for MNIST, CIFAR10 and ImageNet are vulnerable with regardto (adversarial) rotation and translation of images.The paper experimentally examines different ways of formulating attacks(gradient descent, grid search and sampling) and defenses(random augmentation, worst-case out of sample robust training,aggregated classification) for this class of image transformation.The main results are:- Gradient descent is not effective at generating worst-case rotations /translations due to nonconcavity of the adversarial objective- Grid search is very effective due to low parameter space- Sampling and pick the worst is also effective and cheap, for similar reasons- L infinity ball pixel perturbation robustness is orthogonal to the examinedtransformations and does not provide good defense mechanism- Just augmenting data with random translation / rotations is not a strongdefense- Using a worst-case out of sample of 10 for training with an approximation ofa robust optimization objective combined with an aggregated result forclassification is a stronger defenseRecommendation:The paper presents a comprehensive study of a relevant class of adversarialimage perturbations for state-of-the-art neural network models.The results are a useful pointer towards future research directions and forbuilding more robust systems in practice.I recommend to accept the paper.Strong points:- The paper is well written, has clear structure and is technically easy tounderstand.- The question of padding and cropping comes up naturally and is then answered.Open questions (things that could potentially be of interest when added):- Loss landscapes look like most of the nonconcavity is along the translationparameter. Any idea why?- What mechanisms within CNN models do or do not learn (generalize) rotationand translation from provided data (including augmentation)?Specific:- Page 2: perturbrbations (Typo)- Page 3: witho (Typo)- Page 3: Constrained optimization problems typically written asmax_{...} \mathcal L(x', y) s.t. x' = T(...)(s.t. for subject to instead of for) but that's matter of taste I guess- Page 4: first order -&gt; first-order (consistency)- Page 4: tyipcally (Typo)- Page 4: occurs most common(ly)I am not sufficiently knowledgable about the previous literature to ensure thatthe claimed novelty of the paper is truly as novel. This is a very interesting paper that achieves something that seems initially impossible: to learn to reconstruct clear images from only seeing noisy or blurry images. The paper builds on the closely related prior work AmbientGAN which shows that it is possible to learn the *distribution* of uncorrupted samples using only corrupted samples, again a very surprising finding. However, AmbientGAN does not try to reconstruct a single image, only to to learn the clear image distribution. The key idea that makes this is possible is knowledge of the statistics of the corruption process: the generator tries to create images that *after they have been corrupted* they look indistinguishable from real corrupted images. This surprisingly works and provably recovers the true distribution under a very wide set of corruption distributions, but tells us nothing about reconstructing an actual image from measurements. Given access to a generative model for clear images, an image can be reconstructed from measurements by maximizing the likelihood term. This method (CS-GAN) was introduced by Bora et al. in 2017. Therefore one approach to solve the problem that this paper tackles is to first use AmbientGAN to get a generative model for clear images and then use CS-GAN using the learned GAN. If I understand correctly, this is the 'Conditional AmbientGAN' approach that is used as a baseline. This is a sensible approach given prior work. However, the authors show that their method ('Unpaired Supervision') performs significantly better compared to the Conditional AmbientGAN baseline. This is very surprising and interesting to me. Please discuss this a bit more ? As far as I understand the proposed method is a merging of AmbientGAN and CS-GAN, but much better than the naive separation. Could you give a bit more intuition on why ?I would like to add also that the authors can use their approach to learn a better AmbientGAN. After getting their denoised images, these can be used to train a new AmbientGAN, with cleaner images as input , which should be even better no ?In the appendix where is the proposed method in fig 5- 8 ?Does the proposed method outperform Deep Image Prior ? This paper proposes a new approach to use more informative signals (than only class labels), specifically, regions humans deem important on images, to improve deep convolutional neural networks. They collected a large dataset by implementing a game on clickme.ai and showed that using this information results in both i) improved classification accuracy and ii) more interpretable features. I think this is good work and should be accepted. The main contribution is three fold: i) a publicly available dataset that many researchers can use, ii) a network module to incorporate this human information that might be inserted into many networks to improve performance, and iii) some insights on the effect of such human supervision and the relation between features that humans deem important to those that neural nets deem important. Some suggestions on how to improve the paper:1. I find Sections 3 &amp; 4 hard to track - some missing details and notation issues. Several variables are introduced without detailing the proper dimensions, e.g., the global feature attention vector g (which is shown in the figure actually). The relation between U and u_k isn't clear. Also, it will help to put a one-sentence summary of what this module does at the beginning of Section 3, like the last half-sentence in the caption of Figure 3. I was quite lost until I see that. Some more intuition is needed, on W_expand and W_shrink; maybe moving some of the "neuroscience motivation" paragraph up into the main text will help. Bold letters are used to denote many different things - in  Section 4 as a set of layers, in other places a matrix/tensor, and even an operation (F). 2. Is there any explanation on why you add the regularization term to every layer in a network? This setup seems to make it easy to explain what happens in Figure 4. One interesting observation is that after your regularization, the GALA features with ClickMe maps exhibit minimal variation across layers (those shown). But without this supervision the features are highly different. What does this mean? Is this caused entirely by the regularization? Or there's something else going on, e.g., this is evidence suggesting that with proper supervision like human attention regions, one might be able to use a much shallower network to achieve the same performance as a very deep one?3. Using a set of 10 images to compute the correlation between ClickMe and Clicktionary maps isn't ideal - this is even less than the number of categories among the images. I'm also not entirely convinced that "game outcomes from the first and second half are roughly equal" says much about humans not using a neural net-specific strategy, since you can't rule out the case that they learned to play the game very quickly (in the first 10 of the total 380 rounds). 4. Title - this paper sound more "human feedback" to me than "humans-in-the-loop", because the loop has only 1 iteration.  Because you are collecting feedback from humans but not yet giving anything back to them. Maybe change the title? This work presents an extension of the MAML framework for "learning to learn." This extension changes the space in which "inner-loop" gradient steps are taken to adapt the model to a new task, and also introduces stochasticity. The authors validate their proposed method with regression experiments in a toy setting and few-shot classification experiments on mini- and tiered-Imagenet. The latter are well known and competitive benchmarks in few-shot learning.The primary innovations that distinguish this work from previous gradient-based approaches to meta-learning (namely MAML) are that (i) the initial set of parameters is data-dependent and drawn from a generative distribution; and (ii) the adaptation of model parameters proceeds in a lower-dimensional latent space rather than in the higher-dimensional parameter space. Specifically, model parameters are generated from a distribution parameterized by an adapted latent code at each adaptation step. I find both of these innovations novel.The experimental results, in which LEO outperforms the state of the art on two benchmarks derived from ImageNet by "comfortable margins," and the ablation study demonstrate convincingly that these innovations are also significant. I also found the curvature analysis and embedding visualization illuminating of the model's function. My one suggestion would be to test the model on realistic data from beyond the image domain, perhaps on something sequential like language (consider the few-shot PTB setting from Vinyals et al. (2016)). I'm aware anecdotally that MAML struggles with adapting RNNs and I wonder if LEO overcomes that weakness.The paper is clearly written and I had little difficulty in following the algorithmic details, although I'm sure it helped to be familiar with the convoluted meta-learning and inner-/outer- loop frameworks. I recommend it for publication.Pros:- Natural, novel extension to gradient-based meta-learning- state of the art results on two competitive few-shot benchmarks- good analysis- clear writingCons:- realistic, high-dim data is only from the image domainMinor questions for the authors:- Relation Networks are computationally intensive, although in few-shot learning the sets encoded are fairly small. Can you discuss the computational cost and training time of the full framework?- What happens empirically when you generate parameters for more than just the output layer in, eg, your convolutional networks?- What happens if you try to learn networks from scratch through the meta-learning process rather than pre-training and fine-tuning them? Some of the methods you compare against do so, to my understanding. This paper gives the first nonvacuous generalization bounds formeaningful Imagenet models.  These bounds are given in terms of thebit length of compressions of learned models together with a methodfor taking into account symmetries of the uncompressed parameters.These bounds are nonvacuous only when the compressed models are small--- on the order of 500 Kilobytes.  State of the art compressed modelsof this size achieve Imagenet accuracies slightly better than Alexnet,16% error for top 5, and this paper reports a nonvacuousgeneralization guarantees of 89% error for top 5.  While there isstill a large gap between the actual generalization and the guarantee,this would still be a significant accomplishment.I have one major concern.  The generalization bound involves adding anempirical loss and a regularization term computed from a KLdivergence.  I am convinced that the authors have correctly handlesthe KL divergence term.  But the paper does not contain sufficientdetail to determine if the authors correctly handle the empirical lossterm.  It is NOT correct to use the training loss of the(deterministic) compressed model.  The generalization bound requiresthat the training loss be measured under the parameter noise of theposterior distribution.  The paper needs to be clear that this hasbeen done. The comments in Appendix B on noise robustness aredisturbing in this regard.If the training loss  has been calculated correctly in the bound,the results are significant.Assuming correctness, I would comment that the Catoni bound, while sqeakingout all available tightness, is very opaque.  I might be good toconsider the more transparent bounds, claimed to be essentially thesame, given in McAllester's tutorial.  If the more transparent boundsachieve equivalent numerical results, they would make the nature ofthe bounds clearer.Another comment involves a largely ignored detail in (Dzuigaite andRoy 17). Their bounds become vacuous if they center their Gaussianprior at zero.  Instead they center the prior on the initial value ofthe parameters.  This yields a dramatic improvement in the bound.  Inthe context of the present paper, this suggests a modification of theprior distribution on the compressed model.  We represent the model byfirst selecting the r code values.  I think a distribution could bedefined on the code book that would improve its log probability, but Iwill ignore that.  Given the r code values we can define adistribution over the possible compressed representations of a weightw_i in terms of a prior on w_i defined in terms of its initial value.This gives a probability distribution over the compressedrepresentation.  Using log probability of the compressedrepresentation should then be a significant improvement on the firstterm in (8).  This shift in the prior on compressed models has noeffect on the second term of (8) so things should only get better. The paper describes a VAE-based approach to semi-supervised learningof dependency parsing. The encoder in the VAE is a neural edge-factoredparser allowing inference using Eisner's dynamic programming algorithms.The decoder generates sentences left-to-right, at each point conditioningon head-modifier dependencies specified by the tree. A key technical step is to develop a method for "differentiable" sampling/parsing,using a modification of the dynamic program, and the Gumbel-max trick.I thought this was an excellent paper - very clear, an important problem, a very useful set of techniques and results. I would stronglyrecommend acceptance.Some comments:* I do wonder how well this approach would work with orders of magnitudemore unlabeled data. The amount of unlabeled data used is quite small.* Similarly, I wonder how well the approach works as the amount ofunlabeled data is decreased (or increased, for that matter). It shouldbe possible to provide graphs showing this.* Are there natural generalizations to multi-lingual data, for examplesettings where supervised data is only available for languages otherthan the language of interest?* It would be interesting to see an analysis of accuracy improvementson different dependency labels. The "root" case is in some sense justone of the labels (nsubj, dobj, prep, etc.) that could be analyzed.* I wonder also if this method would be particularly helpful in domain transfer, for example from Wall Street Journal text toWikipedia or Web data in general. The improvements could be moredramatic in this case - that kind of effect has been seen with ELMO for example. I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so Ill focus on them:K-ary dependenciesFunctions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that youre explicitly modelling higher-order interactions that improves performance? Or is it that youre doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? These two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance thats the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? SGD approaches:I think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I dont follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \pi-SGD to converge, but we arent provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions dont seem to be specific to \pi-SGD - any SGD algorithm with slightly biased gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isnt evaluated so were left with theory that doesnt provide guidance and isnt evaluated.Summary:There are two ways to read this paper: 1. Janossy pooling as a framework &amp; proposed pooling approach implemented in one of the two ways discussed above. 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.I liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that Im arguing for its acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. [Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, andAlexander Smola. Deep Sets[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets This  seems like a very interesting concept, creating adversarial agents for each class that essentially compete with each other.  It seems like this might be a very promising method for arguing for even more abstract classes like "circus" vs "zoo" I wise more had been said about why the Honest Advocate outperformed the standard Advocate on the MIMIC dataset.  The authors state:"Advocates can effectively compete to generate higher quality evidence, though this effect waslargely localized to a few class-pairs (e.g. shirts v.s. pullovers). "Does it do this on things that are essentially very similar?  Overall, I think this is a great idea. I have been looking for some similar work and consider this work to be similar in the multi-generative aspect: "MEGAN: Mixture of Experts of Generative Adversarial Networks for Multi-modal Image Generation" - Park, Yoo, Bahng, Choo and Park, IJCAI 2018, but I cannot find similar work using the generative experts as collective adversaries for discrimination.The paper is clear and well written.  Improvements for the paper would be going into more detail about why the method works.  It would have been great to have seen a data set on which the method performs poorly - that would give additional insight into its strengths and weaknesses. The Authors of this paper investigate Neuro-Symbolic methods in the context of learning a SAT solver generalized to the Circuit-SAT problem. Using a reinforcement learning  inspired approach to demonstrate a framework that is capable of (unsupervised) learning, by means of an end-to-end differentiable training procedure. Their formulation incorporates the solving of a given SAT problem into the architecture, meaning the algorithm is trained to produce a solution if a given problem is satisfiable. This is in contrast to previous similar work by (Selsam et al. 2018), where the framework was trained as a SAT classifier. Their results outline the performance increase over the previous work (Selsam et al. 2018) on finding a given solution for a SAT problem, on in-sample and out-sample results.Neg: Figure descriptions are not very clearWhen it comes to comparing the results, they do use a prepossessing step for their algorithm which they do not incorporate into the resultsPros:Clear outline of the data sets used for benchmarks.Good Literature review, expressing in-depth knowledge of the current state of the art formulation for same/similar tasks Extensive background section, that explains the theoretical concepts and their architecture used well.Clear outline of the Solver, where the individual parts/networks are explained and justified in detailVery well outlined argumentation for approaching this particular problem by the proposed method/The experimental results as well are easy to follow and show promising results for the proposed frameworkThe proposed method as well is novel and outperforms similar algorithms in the experimental evaluation.The paper is very well written, proposes a novel Neuro-Symbolic  approach to the classical SAT problem, and demonstrates promising results. Paper summary:The paper proposes to predict bouncing behavior from visual data. The model has two main components: (1) Physics Interface Module, which predicts the output trajectory from a given incoming trajectory and the physical properties of the contact surface. (2) Visual Interface Module, which predicts the surface properties from a single image and the impact location. A new dataset called Bounce Dataset is proposed for this task.Paper strengths:- The paper tackles an interesting and important problem.- The data has been collected in various real scenes.- The idea of training the physics part of the network with synthetic data and later fine-tuning it with real images is interesting.- The experiments are thorough and well-thought-out.Paper weaknesses:- It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.- It is not clear how the evaluation is performed. For instance, the length of the groundtruth and predicted trajectories might be different. How is the difference computed?- The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.- Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?- The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.- How is the bounce location encoded in VIM?- I don't see any statistics about the objects being used for data collection. That should be added to the paper. Summary: the paper introduces a new way of fine-tuning neural networks. Instead of re-training the whole model or fine-tuning the last few layers, the authors propose to fine-tune a small set of model patches that affect the network at different layers. The results show that this way of fine-tuning is superior to above mentioned typical ways either in accuracy or in the number of tuned parameters in three different settings: transfer learning, multi-task learning and domain adaptation.Quality: the introduced way of fine-tuning is interesting alternative to the typical last layer re-training. I like that the authors present an intuition behind their approach and justify it by an illustrative example. The experiments are fair, assuming the authors explain the choice of hyper-parameters during the revision.Clarity: in general the paper is well-written. The discussion of multi-task and domain adaptation parts can be improved though.Originality: the contributions are novel to my best knowledge.Significance: high, I believe the paper may facilitate a further developments in the area.I ask the authors to address the following during the rebuttal stage:* explain the choice of the hyper-parameters of RMSProp (paragraph under Table 1).* fix Figure 3, it's impossible to read in the paper-printed version* explain how the average number of parameters per model in computed in Tables 4 and 5. E.g. 700K params/model in the first column of Table 4 is misleading - I suppose the shared parameters are not taken into account. The same holds for 0 in the second column, etc.* add a proper discussion for domain adaptation part. The simple "The results are shown in Table 5" is not enough. * consider leaving the discussion of cost-efficient model cascades out. The presented details are too condensed and do not add value to the paper.* explain how different resolutions are managed by the same model in the domain adaptation experiments. This paper presents an interesting analysis of metamerism and a model capable of rapidly producing metamers of value for experimental psychophysics and other domains.Overall I found this work to be well written and executed and the experiments thorough. Specific points on positives and negatives of the work follow:Positives:- The paper shows a solid understanding of the literature in this domain and presents a strong motivation- The problem itself is addressed at a deep level with many nuanced (but important) considerations discussed- Ultimately the results of the model seem convincing in particular with the accompanying psychophysical experimentsNegatives:- (Maybe not a negative, but a question) At the extreme tradeoff between intrinsic structure and texture, the notion of a metamer seems somewhat obscured. At what point is a metamer no longer a metamer?- (Also not necessarily a negative) Exercising SSIM is a valid decision given it's widespread use. I am curious if MS-SSIM, IW-SSIM or other metrics make any significant difference. This paper presents an interesting conceptual advance connecting causality, disentangled representation learning, invariant representations and robust classification.The authors propose a Counterfactual Generative Network (CGN), which is basically "modular" generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism, i.e. independent factors in the structural causal model of the data. In the context of generating natural images like those comprising the ImageNet dataset, the CGN once trained can be used to generate high-quality counterfactual images with direct control over of factors of variations determining the content of an image shape, texture, and background. These generated samples obtained by independently and uniformly sampling over factors of variation can be used to train a classifier to achieve out-of-domain robustness. The authors show indeed show in simulation that this procedure works as a data augmentation procedure that increases out-of-domain robustness while only marginally degrades the overall accuracy. As the authors explain, this can be thought of as a generalization of "domain randomization".Additionally, CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting.The authors also carry out extensive ablation studies that quantify the contribution of the different training costs for CGN to the overall quality (measures as Inception Score) of the generated counterfactual images. This is first and foremost an "idea paper" putting forth a very interesting conceptual proposal. This is then empirically validated on out-of-distribution classification tasks in different versions of colored MNIST, and a coarse grained subsed of ImageNet.A natural question for the authors is whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios. Another natural question would be to quantify the effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples.Lastly, it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting, i.e. how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively. The authors focus on the selection problem of k statistically significant features discriminating 2 probability distributions accessible via samples. They propose a non-parametric approach under the PSI (post selection inference) umbrella using MMD (maximum mean discrepancy) as a discrepancy measure between probability distributions. The idea is to apply (asymptotically) normal MMD estimators, rephrase the top-k selection problem as a linear constraint, and reduce the problem to Lee et al., 2016. The efficiency of the approach is illustrated on toy examples and in GAN (generative adversarial network) context. The technique complements the PSI-based independence testing approach recently proposed by Yamada et al., 2018. The submission is a well-organized, clearly written, nice contribution; it can be relevant to the machine learning community.Below I enlist a few suggestions to improve the manuscript:-Section 1: The notion of characteristic kernel (kernel when MMD is metric) has not been defined, but it was referred to. 'Due to the mean embeddings in RKHS, all moment information is stored.': This sentence is somewhat vague.-Section 1: 'MMD can be computed in closed form'. This is rarely the case (except for e.g. Gaussian distributions with Gaussian or polynomial kernels). I assume that the authors wanted refer to the estimation of MMD.-Section 1: 'K nearest neighbor approaches (Poczos &amp; Schneider, 2011)'. The citation to this specific estimator can go under alpha-divergences. The Wasserstein metric could also be mentioned.-Section 3.1: k is used to denote the number of selected features and also the kernel used in MMD. I suggest using different notations.-Theorem 1: '\Phi is the CDF...'. There is no \Phi in the theorem.-Section 3.2: The existence of MMD (mean embedding) requires certain assumptions: E_{x\sim p}\sqrt{k(x,x)} &lt; \infty, E_{x\sim q}\sqrt{k(x,x)} &lt; \infty.-Section 3.2.: block estimator: 'B_1 and B_2 are finite'. 'fixed'?-Section 3.2.: MMD_{inc}:    i) 'S_{n,k}': k looks superfluous.   ii) 'l': it has not been introduced (cardinality of D).-Section 3.3: typo: 'covraiance' (2x)-Section 3.3: Fan et al. 2013: The citation can go to \citep{}.  -Theorem 2:    i)'c' is left undefined.   ii)Comma is missing before 'where'.   iii)\xrightarrow{d} (Theorem 2, Corollary 3-4): Given that 'd' also denotes dimension in the submission, I suggest using a different notation for convergence in distribution.-At the introduction of block-MMD the block size (B) was fixed, while in the experiments (e.g. Figure 3) it is growing with the sample size (B=\sqrt{n}). The assumption on B should be clearly stated.-Section 5.1: (b) mean shift: comma is missing before 'where'.-References:    i) Abbreviations and names in the titles should be capitalized (such as cramer, wasserstein, hilbert-schmidt, gan, nash).    ii) Scholkopf should be Sch\{"o}lkopf (in the ALT 2005 work).   iii) 'Exact post-selection inference, with application to the lasso': All the authors are listed; 'et al.' is not needed. Paper formalizes the gradient estimation problem in a black-box setting, and provs the equivalence of least Squares with NES. It then improves on state of the art by using priors coupled with a bandit optimization technique.The paper is well written. The idea of using priors to improve adversarial gradient attacks is an enticing idea. The results seem convincing.Comments:- I missed how data dependent prior is factored into the algorithms 1-3. Is it by the choice of d? I suggest a clearer explanation.- In fig 4, I was confused that the loss of the methods is increasing. it took me a minute to realize this is the maximized adversarial loss, and thus higher is better. you may want to spell this out for clarity. I typically associate lower loss with better algorithms.- I am confused by Fig 4c. If I am comparing g to g*, I do expect a high cosine similarity. cos = 1 is the best. Why is correlation so small? and why is it 0 for NES? You may also want to offer additional insight in the text explaining 4c. Minor comments:- Is table one misplaced?- The symbol for "boundary of set U" may be confused with a partial derivative symbol- first paragraph of 2.4: "our estimator a sufficiently". something missing?- "It is the actions g_t (equal to v_t) which..." refering to g_t as actions is confusing. Although may be technically correct in bandit setting- Further explain the need for the projection of algorithm 3, line 7.- Fig 4: refer to true gradient as g*Caveat: Although I am well versed in bandits, I am not familiar with adversarial training and neural network literature. There is a chance I may have misevaluated central concepts of the paper. The authors analyze the learning dynamics in deep neural networks and identify an intriguing phenomenon that reflects what in biological learning is known as critical period: a relatively short time window early in post-natal development where organisms become particularly sensitive to particular changes in experience. The importance of critical periods in biology is due to the fact that specific types of perturbations to the input statistic can cause deficits in performance which can be permanent in the sense that later training cannot rescue them.The authors did a great job illustrating the parallelism between critical periods in biological neural systems and the analogous phenomenon in artificial deep neural networks. Essentially, they showed that blurring the input samples of the cifar10 dataset during the initial phase of training had an effect that is very reminiscent of the result of sensory deprivation during the critical periods of visual learning in mammals, resulting in a long-term impairments in visual object recognition that persists even if blurring is removed later in training. The authors go as far as characterizing the effects of the length of the "sensory deprivation" window and its onset during training, and comparing the results to classic neuroscience monocular deprivation experiments in kittens, pointing out very striking phenomenological similarities.Next, the authors establish a connection between critical periods in deep neural networks and the amount of information that the weights of the trained model contain about the task by looking at the Fisher Information Matrix (FIM). With this method they obtain a host of interesting insights. One insight is that there are two phases in learning: an initial one where the trace of the FIM grows together with a rapid increase in classification accuracy, and a second one where accuracy keeps slightly increasing, but Fisher Information trace globally decreases. They then go into detail and look at how this quantity evolves within individual layers of the deep learning architecture, revealing that the deficit caused by the blurring perturbation during the early epochs training is accompanied by larger FIM trace in the last layers of the architecture at the expense of the intermediate layers.Besides the fact that deep neural network exhibit critical periods, another important result of this work is the demonstration that pretraining, if done inappropriately can actually be deleterious to the performance of the network.This paper is insightful, and interesting. The conceptual and experimental part of the paper is very clearly presented, and the methodology is very appropriate to tease apart some of the mechanisms underlying the basic phenomenological observations. Here are some detailed questions meant to elucidate some points that are still unclear.- Presumably, early training on blurred images prevents the initial conv filters from learning to discriminate high-frequency components (first of all, is this true?). The crucial phenomenon pointed out by the authors is that, even after removing the blur, the lower convolutions aren't able to recover and learn the high-frequency components. In fact, the high FIM trace in the latest layers could be due to the fact that they're trying to compensate for the lack of appropriate low-level feature extractors by composing low-frequency filters so as "build" high-frequency ones. If this makes sense, one would assume that freezing the last layers and only maintaining plasticity in the lower ones could be a way of "reopening" the critical period. Is that indeed the case?- The authors show that their main results are robust to changes in the learning rate annealing schedule. However, it is not clear how changing the optimizer might affect the presence of the critical period. What would happen for instance using Adam or another optimization procedure that relies on the normalization of the gradient?- On a related note, the authors point out the importance of forgetting, in particular as the main mechanism behind the second learning phase. They also point out that the deficit in learning the task after sensory deprivation is accompanied by large FIM trace in the last layers. What would happen in the presence of a standard regularizer like weight decay? Assuming that large FIM trace in the last layers is correlated with large weighs, that might mitigate the negative effect of early sensory deprivation.- In neuroscience the opening of the critical period window if thought to be mechanistically mediated by the maturation of inhibition. Is that view compatible with the results presented in this paper? This is sort of complementary to the FIM analysis, since is mostly about net average input to a neuron, i.e. about the information contained in the activations, rather than the weights. The authors propose a novel method for learning graph convolutional networks. The core idea is to use the Lanczos algorithm to obtain a low-rank approximation of the graph Laplacian. The authors propose two ways to include the Lanczos algorithm. First, as a preprocessing step where the algorithm is applied once on the input graph and the resulting approximation is fixed during learning. Second, by including a differentiable version of the algorithm into an end-to-end trainable model. The proposed method is novel and achieves good results on a set of experiments. The authors discuss related work in a thorough and meaningful manner. There is not much to criticize. This is a very good paper. The almost 10 pages are perhaps a bit excessive considering there was an (informal) 8 page limit. It might make sense to provide a more accessible discussion of the method and Theorem 1, and move some more detailed/technical parts in pages 4, 5, and 6 to an appendix. The paper describes a new open synthetic dataset for serial crystallography generated by a simulator. Three methods are proposed and implemented to demonstrate the classification of these diffraction images. The results from these methods are compared and clearly show the ones achieve high performance. The article structure is clear and is well written. The experiments are carried out in a professional way and statistical analysis is shown. It will be better if the authors can demonstrate how the models obtained from training the synthetic data perform in real scenario. Please also add some discussion on how good the synthetic data simulate the real data. Some image comparison between the synthetic data and real data should be analysed. This paper tests a number of untrained sentence representation models - based on random embedding projections, randomly-initialized LSTMs, and echo state networks - and compares the outputs of these models against influential trained sentence encoders (SkipThought, InferSent) on transfer and probing tasks. The paper finds that using the trained encoders yields only marginal improvement over the fully untrained models.I think this is a strong paper, with a valuable contribution. The paper sheds important light on weaknesses of current methods of sentence encoding, as well as weaknesses of the standard evaluations used for sentence representation models - specifically, on currently-available metrics, most of the performance achievements observed in sentence encoders can apparently be accomplished without any encoder training at all, casting doubt on the capacity of these encoders - or existing downstream tasks - to tap into meaningful information about language. The paper establishes stronger and more appropriate baselines for sentence encoders, which I believe will be valuable for assessment of sentence representation models moving forward. The paper is clearly written and well-organized, and to my knowledge the contribution is novel. I appreciate the care that has been taken to implement fair and well-controlled comparisons between models. Overall, I am happy with this paper, and I would like to see it accepted. Additional comments:-A useful addition to the reported results would be confidence intervals of some kind, to get a sense of the extent to which the small improvements for the trained encoders are statistically significant.-I wonder about how the embedding projection method would compare to simply training higher-dimensional word embeddings from the start. Do we expect substantial differences between these two options? This paper discusses the optimization of robot structures, combined with their controllers. The authors propose a schemebased on a graph representation of the robot structure, and a graph-neural-network as controllers. The experiments showthat the proposed scheme is able to produce walking and swimming robots in simulation. The results in this paper are impressive, and the paper seems free of technical errors. The main criticism I have is that I found the paper harder to read. In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be. This makes the contribution of this paper in terms of the methodhard to judge. Please include further description of the ES cost function and algorithm in the main body of the paper.The second point is that the proposed approach seems to modify a few things from the ES baseline. The efficacy of the separate modifications should be tested. Therefore I would like to see experiments with the ES cost function, but withinclusion of the pruning step, and experiments with the AF-function but without the pruning step. This paper performs an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate. Based on these outcomes they reformulate the functions (interpreted as differential equations) to add a decay-behaviour in the forget gates, finally called DecayNet-LSTM.The theoretical analysis in this paper is very welcome and goes beyond observations which we made in the past, i.e., we often saw similar behavior in our experiments and as the authors also state in Section 5, there have been previous observations and approaches. In 2016, I have seen an idea called Random Activation Preservation (RAP) (https://ieeexplore.ieee.org/abstract/document/7487778 ) which just randomly "resets" the gates. However, they only show empirical outcomes, not a sophisticated analysis as in this paper.In the experiments it is shown, that the DecayNet-LSTM performs similarly, or sometimes better than simple LSTM on standard tasks. On more difficult tasks, such as Perm-SeqMNIST, a state-of-the-art performance is achieved.Minor comments:Please note, it should be Long Short-Term Memory (with hyphen between short and term)You call the contribution DecayNet; And in the paper sometimes refer to it as DecayNet-LSTM; Maybe there could also be a DecayNet-GRU, ... If you, instead of "reformulation", would clearly write that DecayNet is an addition to the LSTM architecture, it might be more clear. This paper presents a thorough and systematic study of the effect of pre-training over various NLP tasks on the GLUE multi-task learning evaluation suite, including an examination of the effect of language model-based pre-training using ELMo. The main conclusion is that both single-task and LM-based pre-training helps in most situations, but the gain is often not large, and not consistent across all GLUE tasks.This paper represents an impressive amount of experimentation. The study and the experimental results will be useful and interesting to the community. The result that some tasks' performance are negatively correlated with each other is surprising. The paper is clearly written. One clarification question I have is about what the "Single-task" pre-training means. The paper seems to suggest that it consists of pre-training a model on the same task on which it is later evaluated. I'm confused by what this means, and how this is different from just training on that task. Noticing that widely used latent code interpolations for exploring the generative capabilities of VAEs and GANs have distribution mismatch problems, this paper proposes to utilize monotone transport map to exactly eliminate the distribution mismatch between modified interpolated codes and a prior distribution, assuming I.I.D. code components and a L1 code distance. More precisely, a transformation of the latent space operation is learnt with the objective that the distribution of the transformed variable match the prior distribution used in training the generative models. Optimal transport is used as a measure to minimize the two distributions. By restricting the class of cost functions used in the optimal transport formulation, the solution to the optimal transport problem (and hence the transformation function) has been shown to take a simple form (closed form in cases where cdf has a analytical form). Experiments on CIFAR-10, LLD-icon, LSUN, CelebA datasets show that, the minimally modified interpolated codes for several different interpolations produce samples with higher Inception Scores and better visual effects under an improved Wasserstein GAN than the original interpolated codes.This paper is well written, the studied problem is highly important, and the approach presented has potentially wide applications. However, there are some concerns about the experimental evaluations,1. Although the quantitative evaluations for 2-point and 4-point interpolations are important, it is hard to assess these interpolations in a semantically meaningful way. Extensive quantitative (FID and IS) and qualitative evaluations should be conducted for analogy interpolations. For example, adding glasses, adding mustache, and many others. It is much easier to assess the quality of the generated images from the minimally modified interpolated code for this category in a meaningful way.2. Another concern is that how big the effect of the transformation function inducing on the latent space operations will be. For example, a linear interpolation is no longer linear after getting transformed. So, are there transformations that drastically transform the original latent space operations? In that case, will the transformed variable make any sense with respect to the original latent space operations? Extensive experiments for analogy interpolations are required to answer these questions.3. Experiments have been shown only on GAN architectures, however, the framework can be easily extended to VAEs. Experiments on VAEs will be informative.Minor:Section 1.1, in the second paragraph, (SLERP) should be moved a correct position.Figure 2: it's better to use a different color for midpoint linear other than blueProblem 1, f* ---&gt; f*: This paper1) extends an argument for the GP behaviour of deep, infinitely-wide fully-connected networks to convolutional and residual deep neural networks with infinitely many channels and2) provides a computationally tractable approach to compute the corresponding GP kernel. This kernel has few hyper-parameters, and achieves state-of-the-art results on the MNIST dataset. While point (1) is a relatively straightforward adaptation of Lee et. al (2017) and Matthews et al. (2018) to a different network structure, point (2) is original and non-trivial. All in all, I think this paper makes a significant contribution that I believe will spark interesting follow-up work (hinted at in the last section of the paper).Questions:- In my understanding, the kernels of Section 3 do not require the weight matrices W to share the same values across rows. Accordingly, their performance cannot necessarily be explained by properties of convolutional filters (in particular translation invariance). Can the authors comment on that?- What would be the performance of a parametric CNN trained with SGD that matches the architecture (# layers) &amp; the squared loss function of ResNet GP? The only point of comparison is Chen &amp; al. (2018), which I suppose optimizes a log loss? Specifically, I would like to understand the impact of the loss function and of the number of layers on the relative performance of the two approaches.The paper is clear and easy to follow. A few suggestions:- I recommend turning the argument in section 2.2 into a formal, self-contained theorem that states a result on A_L, defined in eq. 17 (which I would move to the main text). This would make the precise claim easier to understand.- I suggest including a more thorough discussion of the results. Table 1 is only introduced in the related work section.- If space is a concern, I would move part of Section 2.2 outside of the main text, since it mostly follows Lee et al. &amp; Matthews et alSmall questions/comments:- Eqs 1 and 2: b_j should be multiplied by the all-ones vector, just like in (5) and (6).- Below eq. 5: "while the *elements of the* feature maps themselves display..."- Paragraph above eq. 7: "in order to achieve an output suitable for *binary* classification or *univariate* regression"- Paragraph above eq. 7: "if we only need the covariance at *certain* locations in the outputs..."- Algorithm 1: you might want to add a loop over g for clarity The authors studied an interesting problem of unsupervised domain adaptation when the source and the target domains have disjoin labels spaces. The paper proposed a novel feature transfer network, that optimizes domain adversarial loss and domain separation loss.Strengths:1) The proposed approach on Feature Transfer Network was novel and interesting.2) The paper was very well written with a good analysis of various choices.3) Extensive empirical analysis on multi-class settings with a traditional MNIST dataset and a real-world face recognition dataset. Weakness:1) Practical considerations addressing feature reconstruction loss needs more explanation.Comments:The technical contribution of the paper was sound and novel. The paper considered existing work and in a good way generalizes and extends into disjoint label spaces. It was easy to read and follow, most parts of the paper including the Appendix make it a good contribution. However, the reviewer has the following suggestions" 1. Under the practical considerations for preventing the mode collapse via feature reconstruction, how is the reference network trained? In the Equation(6) for feature reconstruction, the f_ref term maps the source and target domain examples to new feature space. What do you mean by references network trained on the label data? Please clarify.2. Under the practical considerations for replacing the verification loss, it is said that "Our theoretical analysis suggests to use a verificationthe loss that compares the similarity between a pair of images" - Can you please cite the references to make it easier for the reader to follow. I like the idea of the paper and I believe it addressing a very relevant problem. While the authors provide a good formalization of the problem and convincing demonstration of the generalization bound, the evaluation could have been better by including some more challenging experiments to really prove the point of the paper. It is surely good to present the toy example with the MNIST dataset but the ethnicity domain is less difficult than what the authors claim. This is also pretty evident from the results presented (e.g., in Table 3). The proposed approach provides maybe slightly better results than the state of the art but the results do not seem to be statistically significant. This is probable also due to the fact that the problem itself is made simpler by the cropped faces, no background, etc. I would have preferred to see an application domain where the improvement would be more substantial. Nevertheless, I think the theoretical presentation is good and I believe the manuscript has very good potential. This paper proposes a new sequence to sequence model where attention is treated as a latent variable, and derive novel inference procedures for this model. The approach obtains significant improvements in machine translation and morphological inflection generation tasks. An approximation is also used to make hard attention more efficient by reducing the number of softmaxes that have to be computed.  Strengths:- Novel, principled sequence to sequence model.- Strong experimental results in machine translation and morphological inflection.Weaknesses:- Connections can be made with previous closely related architectures.- Further ablation experiments could be included. The derivation of the model would be more clear if it is first derived without attention feeding: The assumption that output is dependent only on the current attention variable is then valid. The Markov assumption on the attention variable should also be stated as an assumption, rather than an approximation: Given that assumption, as far as I can tell the (posterior) inference procedure that is derived is exact: It is indeed equivalent to the using the forward computation of the classic forward-backward algorithm for HMMs to do inference. The models overall distribution can then be defined in a somewhat different way than the authors presentation, which I think makes more clear what the model is doing:p(y | x) = \sum_a \prod_{t=1}^n p(y_t | y_{&lt;t}, x, a_t) p(a_t | y_{&lt;t}, x_ a_{t-1}).  The equations derived in the paper for computing the prior and posterior attention is then just a dynamic program for computing this distribution, and is equivalent to using the forward algorithm, which in this context is: \alpha_t(a) = p(a_t = a, y_{&lt;=t}) = p(y_t | s_t, a_t =a) \sum_{a} \alpha_{t-1}(a) p(a_t = a | s_t, a_{t-1} = a) The only substantial difference in the inference procedure is then that the posterior attention probability is fed into the decoder RNN, which means that the independence assumptions are not strictly valid any more, even though the structural assumptions are still encoded through the way inference is done. [1] recently proposed a model with a similar factorization, although that model did not feed the attention distribution, and performed EM-like inference with the forward-backward algorithm, while this model is effectively computing forward probabilities and performing inference through automatic differentiation.The Prior-Joint variant, though its definition is not as clear as it should be, seems to be assuming that the attention distribution at each time step is independent of the previous attention (similar to the way standard soft attention is computed) - the equations then reduce to a (neural) version of IBM alignment model 1, similar to another recently proposed model [2]. These papers can be seen as concurrent work, and this paper provides important insights, but it would strengthen rather than weaken the paper to make these connections clear. The results clearly show the advantages of the proposed approach over soft and sparse attention baselines. However, the difference in BLEU score between the variants of the prior or posterior attention models is very small across all translation datasets, so to make claims about which of the variants are better, at a minimum statistical significance testing should be done. Given that the Prior-Joint model performs competitively, is it computationally more efficient that the full model? The main missing experiment is not doing attention feeding at all. The other experiment that is not included (as I understood it) is to compute prior and posterior attention, but feed the prior attention rather than the posterior attention. The paper is mostly written very clearly, there are just a few typos and grammatical errors in sections 4.2 and 4.3. Overall, I really like this paper and would like to see it accepted, although I hope that a revised version would make the assumptions the model is making clearer and make connections to related models clearer.  [1] Neural Hidden Markov Model for Machine Translation, Wang et al, ACL 2018. [2] Hard Non-Monotonic Attention for Character-Level Transduction, Wu, Shapiro and Cotterell, EMNLP 2018. This paper introduces a generative model for question answering.  Instead of modeling p(a|q,c), the authors propose to model p(q,a|c), factorized as p(a|c) * p(q|a,c).  This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working.                                                                                                                                                            Reactions:                                                                                           - Section 2.1: Is there a bias problem here, where you're only ever training with the correct answer?  Oh, I see you covered that in section 2.6.  Great.- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?- Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?- Section 3.1 (and section 2.6): Can you back up your claim of "modeling more complex dependencies" in the generative case?  Is that really what's going on?  How can we know?  What does "modeling more complex dependencies" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.- Section 3.3: Your goal here seems similar to the goal of Clark and Gardner (2018), trying to correctly calibrate confidence scores in the face of SQuAD-like data, and similar to the goals of adding unanswerable questions in SQuAD 2.0.  I know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.                                                                                                     Clarity issues:                                                                                      - Bottom of page 2, "sum with a vector of size d" - it's not clear to me what this means.            - Top of page 3, "Answer Encoder", something is off with the sentence "For each word representation" - Section 2.5, "we first embed words independently of the question" - did you mean "of the _context_"?- Section 2.5.2 - it's not clear to me how that particular bias mechanism "allows the model to easily filter out parts of the context which are irrelevant to the question".  The bias mechanism is independent of the question.- Section 2.7 - when you said "beam search", I was expecting a beam over the question words, or something.  I suppose a two-step beam search is still a beam search, it just conjured the wrong image for me, and I wonder if there's another way you can describe it that better evokes what you're actually doing.- Section 3.1 - "and are results..." - missing "competitive with"?                                   - Last sentence: "we believe their is" -&gt; "we believe there is" Summary:This paper proposes the deep weight prior: the idea is to elicit a prior on an auxiliary dataset and then use that prior over the CNN filters to jump start inference for a data set of interest.  Both explicit and implicit priors are considered, with the latter having the benefit of increased flexibility but having the drawback of a lack of a parametric form to plug in to the ELBO.  The authors address this last point by extending the ELBO appropriately.  Experiments are performed testing the priors ability to capture trained filters (Figure 1), provide a good initialization (Figure 2), improve sample efficiency (Figure 3), improve training speed (Figure 4).  Pros:I like this paper: it is a intuitive idea, and the experiments explore exactly what one would hope to gain from the prior (i.e. better initialization, improved sample efficiency).  I find the paper clearly written and to have a logical flow.  Furthermore, I think eliciting priors---while so crucial in more traditional Bayesian modeling---has been mostly overlooked by the Bayesian ML community, and this paper clearly shows that there are gains to be had from a fairly straightforward procedure.  Cons:The only potential issue with the paper is the use of the implicit prior, as it complicates variational inference, requiring the extension to the ELBO described in Section 3.2.  As far as I can tell, all experiments use the implicit priors.  I would have liked to have seen an experiment using a parametric prior (eg Gaussian) that shows what gains the implicit prior provides.  Or is it simply a matter of memory efficiency?  Other comments:-- Nice first sentence in the introduction!  I like how its a general statement but immediately focuses the readers attention to the papers topic.-- While it doesnt say so explicitly, the paper seems to imply it is the first to use implicit priors.  Some previous work that uses some form of implicit prior includes:Runs a chain to refine the prior: Alex Lamb et al. "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models." Advances in Neural Information Processing Systems. 2017.Optimizes a NN implicit prior based on an invariance objective: Eric Nalisnick and Padhraic Smyth. "Learning priors for invariance." International Conference on Artificial Intelligence and Statistics. 2018.Defines implicit priors over functions through samplers: Chao Ma, Yingzhen Li, and José Miguel Hernández-Lobato. "Variational Implicit Processes." arXiv preprint arXiv:1806.02390 (2018).Evaluation:  I recommend this paper for acceptance.  It is a sensible idea with pointed experimental validation. The paper takes a good step toward developing more structured representations by exploring the use of quaternions in recurrent neural networks.  The idea is motivated by the observation that in many cases there are local relationships among elements of a vector that should be explicitly represented.  This is also the idea behind capsules - to have each "unit" output a vector of parameters to be operated upon rather than a single number.   Here the authors show that by incorporating quaternions into the representations used by RNNs or LSTMs, one achieves better performance at speech recognition tasks using fewer parameters.The quaternionic representation of the spectrogram chosen here seems a bit arbitrary.  Why are these the attributes to be packaged together?  its not obvious.  Shouldn't this be learned? Author proposes general framework to use gradient descent to learn a preconditioner related to inverse of the Hessian, or the inverse of Fisher Information matrix, where the inverse may take a particular form, ie, Kronecker-factored form like in KFAC. I have tracked down the implementation of this method by author from earlier paper Li 2018 and verified that it works and speeds up convergence of convolutional networks in terms of number of iterations needed. In particular, Kronecker Factored preconditioner using approach in the paper worked better in terms of wall-clock time on MNIST LeNet5, comparing against an existing PyTorch implementation of KFAC from César Laurent.Some comments on the paper:Section 2The key seems to be equation 8. The author provides loss function, the minimum is what is achieved by inverse of the Hessian. Given the importance of the formula, it feels like proof should be included (perhaps in Appendix).Justification of the criterion is relegated to earlier work in Li (https://arxiv.org/pdf/1512.04202.pdf), but I failed to fully grasp the motivation. There are simpler criteria being introduced, such as criterion 1, equation 17, which simply minimizes the difference between predicted gradient delta and observed, why not use that criterion?The justification is given that using inverse Hessian may "amplify noise", which I don't buy. When using SGD to solve least-square regression, dividing by Hessian does not have a problem of amplifying noise, so why is this a concern here?Section 3The paper should make it clear that empirical Fisher matrix is used, unlike "unbiased estimate of true Fisher" which used in many natural gradient papers.Section 4Is "Lie group" used anywhere in the derivations? It seems the same algebra holds even without that assumption. The motivation for using "natural gradient for learning Q" seems to come from Amari. I have not read that paper, how important it is to use the "natural" gradient for learning Q? What if we use regular gradient descent for Q?Section 7Figure 1 showed that Fisher-type criterion didn't work for toy problem, it would be more informative if it used square root of Fisher-type criterion. The square root comes out of regret-analysis (ie, AdaGrad uses square root of gradient covariance) The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. * pros: - the paper is well written. - the idea is simple but BRILLIANT. - the used techniques are good (especially to learn word clusters). - the experimental results  (speed up softmax at test time) are impressive. * cons: - the model is not end-to-end because word clusters are not continuous. But it not an important factor. - it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time.- it would be better if the authors show some clusters for both input examples and corresponding word clusters. Sequential Monte Carlo (SMC) has since its inception some 25 years ago proved to be a powerful and generally applicable tool. The authors of this paper continue this development in a very interesting and natural way by showing how SMC can be used to solve challenging planning problems. This is a enabled by reformulating the planning problem as an inference problem via the recent trend referred to as "control as inference". While there is unfortunately no real world experiments, the simulations clearly illustrate the potential of the approach.While the idea of viewing control as inference is far from new the idea of using SMC in this context is clearly novel as far as I can see. Well, there has been some work along the same general topic before, see e.g.Andrieu, C., Doucet, A., Singh, S.S., and Tadic, V.B. (2004). Particle methods for change detection, system identification, and contol. Proceedings of the IEEE, 92(3), 423438.However, the particular construction proposed in this paper is refreshingly novel and interesting. Hence, I view the specific idea put fourth in this paper as highly novel. The general idea of viewing control as inference goes far back and there are very nice dual relationships between LQG and the Kalman filter established and exploited long time ago.The authors interprets "control as inference" as viewing the planning problem as a simulation exercise where we aim to approximate the distribution of optimal future trajectories. A bit more specifically, the SMC-based planning proposed in the paper stochastically explores the most promising trajectories in the tree and randomly removes (via the resampling operation) the less promising branches. Importantly there are convergence guarantees via the use of SMC. The idea is significant in that it opens up for the use of the by now strong SMC body of methods and analysis when it comes to challenging and intractable planning problems. I foresee many interesting developments to follow in the direction layed out by this paper. When it comes to your SMC algorithm you will suffer from path degeneracy (as all SMC algorithms does, see e.g. Figure 1 in https://arxiv.org/pdf/1307.3180.pdf) and if h is large I think this can be a problem for you. However, this can easily be fixed via backward simulation. For an overview of backward simulation see Lindsten, F. and Schon, T. "Backward simulation methods for Monte Carlo statistical inference". Foundations and Trends in Machine Learning, 6(1):1-143, 2013.I am positive to this paper (clearly reveled by my score as well), but there are of course a few issues as well:1. There are no theoretical results on the properties of the proposed approach. However, given the large body of literature when it comes to the analysis of SMC methods I would expect that you can provide some results via the nice bridge that you have identified.2. Would this be possible to implement in a real-world setting with real-time requirements?3. A very detailed question when it comes to Figure 5.2 (right-most plot), why is the performance of your method significantly degraded towards the end? It does recover indeed, but I still find this huge dip quite surprising.Minor details:* The initial references when it comes to SMC are wrong. The first papers are:N.J. Gordon, D. Salmond and A.F.M. Smith, Novel approach to nonlinear/non-Gaussian Bayesian state estimation, IEE Proc. F, 1993L. Stewart, P. McCarty, The use of Bayesian Belief Networks to fuse continuous and discrete information for target recognition and discrete information for target recognition, tracking, and situation assessment, in Proc. SPIE Signal Processing, Sensor Fusion and Target Recognition,, vol. 1699, pp. 177-185, 1992. G. Kitagawa, Monte Carlo filter and smoother for non-Gaussian nonlinear state-space models, JCGS, 1996 * When it comes to the topic of learning a good proposal for SMC with the use of variational inference the authors provide a reference to Gu et al. (2015) which is indeed interesting and relevant in this respect. However, on this hot and interesting topic there has recently been several related papers published and I would like to mention:C. A. Naesseth, S. W. Linderman, R. Ranganath, D. M. Blei, Variational Sequential Monte Carlo. Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, Lanzarote, Spain, April 2018.C. J. Maddison, D. Lawson, G. Tucker, N. Heess, M. Norouzi, A. Mnih, A. Doucet, and Y. Whye Teh. Filtering variational objectives. In Advances in Neural Information Processing Systems, 2017.T. A. Le, M. Igl, T. Jin, T. Rainforth, and F. Wood. AutoEncoding Sequential Monte Carlo. arXiv:1705.10306, May 2017.I would like to end by saying that I really like your idea and the way in which you have developed it. I have a feeling that this will inspire quite a lot of work in this direction. The paper challenges recent claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. Along the road, it presents a couple of nice results that I find quite interesting and I believe they provide useful insights. Finally it presents a simple modification to the cross-entropy loss, which the authors refer to as differential training, that alleviates the problem for the case of linear model and linearly separable data.CONS:I find the paper useful and interesting mainly because of its insightful results rather than the final algorithm. The algorithm is evaluated in a very limited setting (linear model, synthetic data, binary classification); it is not clear if similar benefits would carry over to nonlinear models such as deep networks. In fact, I strongly encourage the authors to do a generalization comparison by comparing the **test accuracy** obtained by their modified cross-entropy against: 1. Vanilla cross-entropy as well as 2. A deep model large margin loss function (e.g. as in "Large Margin Deep Networks for Classification" by Elsayed). Of course on a realistic architecture and non-synthetic datasets (e.g. CIFAR-10).PROS:Putting the algorithm aside, I find the theorems interesting. In particular, Theorem 3 shows that some earlier claims about cross-entropy's ability to attain large margin (in the linearly separable case) is misleading (due to neglecting a bias term). This is important as it changes the faith of the community in cross-entropy and more importantly creates hope for constructing new loss functions with improved margin.I also find the connection between the dimension of the subspace that contains the points and quality of margin obtained by cross-entropy insightful. In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary.Overall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. I only have a few questions and remarks:* Whats the random attack baseline in these tasks? In computer vision its often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.* Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also fooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores.* Are you planning to release the code? Will it be part of CleverHans or Foolbox?Overall, I find this work to be a really exciting advance on discrete adversarial attacks. The authors provide a natural definition of adversarial examples for natural language transduction (meaning-preserving on source side while meaning-destroying on target side) and a human judgment task to measure it. They then investigate three different ways of generating adversarial examples and show that a metric based on character n-gram overlap (chrF) has a stronger correlation with human judgment. Finally, they show that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting.Overall this is a strong paper. It is incredibly  well structured, the problem studied is highly interesting and the proposed meaning-preserving criteria and human judgement will be useful to anyone interested in adversarial attacks for natural language. While the studied attack methods are fairly primitive, the empirical results are still interesting.Comments---------------I wish the authors would include experiments with CharSwap where OOV is not forced as I'm not sure the assumption that OOV is more meaning-destroying in the target side is necessarily true (one could also argue that since the models are already trained with OOV words, they may be more robust to OOV words than in-vocabulary words in the wrong context).It would be nice to add correlation for each type of constraint as well to Table 2. The result would be even stronger if the experiment was replicated in the opposite direction or for another language pair as well.I don't understand why the adversarial output in the second example in table 4 has a RDchrF of zero (the word July is completely dropped).From Table 6 it looks like random sampling is actually slightly better than adversarial training in terms of robustness to CharSwap attacks in the Transformer model. Moreover, the benefit of adversarial rather than random sampling is quite small in the LSTM model as well. This could be made more clear in the text.It would be interesting to see how adversarial training with the CharSwap method fares against the unconstrained and kNN attacks in table 6. This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP). ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others. The embeddings of each band are then projected into the same size. This resulted in lowering the number of parameters. Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities. While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus. Further analyses showed that ADP gained performance across all word frequency ranges.Overall, the paper was well-written and the experiments supported the claim. The paper was very clear on its contribution. The variable-size input of this paper was novel as far as I know. However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax. The weight sharing was also needed further investigation and experimental data on sharing different parts.The experiments compared several models with different input levels (characters, BPE, and words). The perplexities of the proposed approach were competitive with the character model with an advantage on the training time. However, the runtimes were a bit strange. For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4). The runtime of ADP seemed to lose in term of scaling as well to BPE. Perhaps, the training time was an artifact of multi-GPU training. Questions:1. I am curious about what would you get if you use ADP on BPE vocab set?2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization? This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.The article is well written and I find the contribution simple, but interesting. It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space. I understand that for two matrices A and B we have rank(AB) &lt;= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?ReferencesJoulin, A., Cissé, M., Grangier, D. and Jégou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310). Summary:In this paper the authors tackle the problem of alignment between input tokens and output acoustic features. The key contribution of this paper is replacing the attention mechanism of the Tacotron 2 with an explicit representation of token durations. The attention mechanism is vulnerable to issues such as pauses, repetitions, and skips, and hence using durations directly takes care of such issues. The challenge lies in obtaining the durations. The authors propose different methods toward that end. First, they introduce a duration predictor in the Tacotron 2 model architecture which utilizes the encoder features to predict the durations. This duration predictor may be supervised if target durations are available. The authors train HMM-based aligners to obtain target durations for this method. They also introduce a fine-grained variational autoencoder (FVAE) which is a conditional VAE that model the alignment between phonemes and acoustic features to extract token-level features which are in turn used by the duration predictor to predict the durations. The FVAE may be semi-supervised or unsupervised.The proposed non-attentive Tacotron model achieves similar naturalness scores to Tacotron 2 which is very close to ground truth naturalness. Additionally, the authors evaluate the proposed model for robustness. They propose two metrics which measure the severity of alignment issues in synthesized speech. The proposed model outperforms the attention-based Tacotron 2 for multiple datasets. The authors also demonstrate the ability to control the pace of utterances by modulating predicted durations. Finally, the authors demonstrate the ability of the semi-supervised and unsupervised models to achieve similar results to the supervised model by utilizing the FVAE duration model.Pros:1. The use of speech recognition-based metrics for measuring robustness is very interesting. More widespread adaptation of robustness as a criteria in addition to MOS scores is required. These objective metrics which can be computed for large datasets can be very helpful.2. The duration modeller working in the supervised, semi-supervised, and unsupervised setting is another good feature of this model. Cons:1. The connection between the duration model and the full model is not very clear in the text. Why does the duration model have a section separate from the Model section, instead of a subsection? A suggestion would be to at least mention early in section 2 that ground truth target durations are not required since there is a semi-supervised duration modeller. There is a delay of 4-5 pages before it is clear that phoneme durations are not a requirement to use the non-attentive tacotron.2. In section 5.4, the experiment setup is not very clear. Do you mean that out of the 66 speakers in the full dataset, 10 speakers' durations are removed (for the semi-supervised experiment)? The paper identifies two atomic problems, respectively in fields of ML (MNIST classification) and quantum mechanics (measuring a single photon), and brings them together in a simplified setup that uses a single photon emitted according to the spatial distribution of images to classify MNIST/Fashion-MNIST. The introduction of quantum mechanics into the problem is through a trainable computational model of a beam splitter/phase shifter mechanism, aka a rotation in a high dimensional complex space, that's allowed to alter the photon's state before hitting the measurement device. The paper shows that using this overly simplified (and claimed to be physically feasible) quantum computer, which acts as the representation learning layer, improves classification accuracy over any other representation learning method that doesn't use quantum computing. The major take-away is an accessible demonstration of how an elementary quantum computer might work for ML, and what may be possible with actual qubits.Strengths:* The paper sets out to use two textbook problems in ML and quantum mechanics to introduce a textbook problem at the intersection, and does a fairly good job at analyzing the problem extensively. Given that the overall problem is of broad interest to the representation learning community, the solid execution of the paper is itself a good argument for acceptance.* Accessible explanation of quantum states, the measurement process, and the building blocks of quantum computing.Comment:* The paper analyzes a single problem where no classical representation learning method can improve accuracy due to the fact that there is a single photon. It would be very beneficial to allude to other setups, perhaps in a related work section, to provide broader context and help the reader understand better the significance of the problem. This paper deals with tree-like structure embedding with box embedding on the lattice (poset). This paper is well-motivated and well-presented. Though there is a limitation on data structure, this paper still presents a novel idea in this area. This method also achieved promising results in experiments. Thus, I would like to recommend to accept this paper. Summary: The authors observe that, while effective, contrastive learning unavoidably introduces some bias depending on the choice of augmentations the algorithm is made invariant to, and that deteriorates performance depending on the task. The authors corroborate this hypothesis with experiments with the MoCo baseline and proceed to propose a modification to the usual contrastive learning setup: learning a shared representation and multiple projection heads, each invariant to a different augmentation type. They empirically show the effectiveness of the proposed solution on several tasks, including few-shot learning and data corruption datasets.Great:* The structured approach to understanding which augmentations help and how to tackle the issue of choosing is addressing a significant issue in the contrastive learning literature. The authors thoroughly motivate their approach. While it's overall clear in the literature that contrastive learning is extremely successful at building generalizable representations, the choice of which augmentations to use is often arbitrary. The authors show empirically how making a model invariant to specific augmentations is  detrimental to some tasks (e.g. adding rotation invariance degrades 100-category ImageNet accuracy). * Extensive evaluation on several different tasks, showing consistent improvement on all and highlighting the flexibility of the proposed approach. The authors show results on a coarse-grained task (ImageNet 100 and iNaturalist 2019), fine-grained (CUB-100 and VGG Flowers), an augmentation-specific task (ObjectNet, real-word objects with different views and rotations) and a robustness task (ImageNet-C).Questions:* The proposed method suggests that the best performance is achieved by choosing more and more augmentations. Do the authors believe there is a limit after which adding more augmentation heads makes the task impossible for contrastive learning, by asking the joint embedding space to account for too much flexibility? * In practice, even when using the proposed method, one still has to choose a relatively small set of possible augmentations. In light of their work, do the authors have thoughts on how should someone make that choice?Overall:The paper is proposing a well empirically motivated modification to current contrastive learning methods, the methods and results are presented clearly, the experiments are extensively described. A clear accept. The authors conducted a comprehensive set of experiments on choices of learning rate schedules for re-training/fine-tuning during iterative or after 1-shot pruning of deep convnets.  Empirically, they reported that high learning rate (LR) is particularly helpful in recovering generalization performance of the resultant sparse model.  The results are purely empirical, well-documented observations from well-designed experiments, which is of practical value in practice of network compression, and the consistent, somewhat surprising observation raises interesting questions.  Notably, this work has brought to attention an important but often overlooked aspect of network pruning: there exist complex interactions between the dynamics of optimization and sparsification, and as a consequence, it is only fair to compare two sparsification techniques when each of them are put in the _best_ optimization setup, respectively.  I have a few comments that I wish the authors would address here, discuss in revision or note for future work:(1) Why is large LR helpful in recovering the accuracy of sparse nets?  There is little information provided in these experimental results to shed light on this question.  There has been loss landscape studies of sparse nets during training (such as arxiv:1906.10732, arxiv:1912.05671)--perhaps these could be applied to study the problem.  If the high LR's role were to knock the solution out of bad local minima, then does adding noise to gradients or smaller batch size achieve similar effect at the initial phase of re-training?  (2) Given a fixed re-training flop budget, after a pruning operation on the network, both (a) weight value rewinding (as in the Lottery Ticket Hypothesis training), (b) re-training LR schedule (as in this work) might be potentially helpful.  How does weight value rewinding interact with LR?  (3) For the random pruning results in Sec. 4, do fine-grain unstructured pruning methods present the same results?  (4) Does the result generalize to transformer models?  What about optimizers?  Does Adam present a same story as SGDM? Page 5, line1 of the 3rd paragraph of Sec. 3.2: typo "reachs" This paper proposes a method termed RoutIng Diverse Experts (RIDE) for reducing both the bias and the variance of a long-tailed classifier. Specifically, RIDE consists of three crucial components: 1) a shared architecture for multiple experts; 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. Experiments are conducted on three long-tailed benchmark datasets, i.e., CIFAR100-LT, ImageNet-LT, and iNaturalist. Satisfactory classification results of long-tailed visual recognition are observed.Paper strengths:- The problem, i.e., long-tailed visual recognition, is practical, important and challenging in computer vision and deserves further studies.- The proposed method has good motivations and sounds reasonable.- The experimental results of the proposed RIDE are significantly better than the results of previous work, which shows the effectiveness of RIDE.- The paper is well written and easy to follow.- Analyses and ablation studies are sufficient and could bring new insights of long-tailed recognition.Paper weaknesses:- There are typos and grammar mistakes in this paper. For example, in the caption of Fig. 2, "ImaeNet-LT and BBN" should be "ImageNet-LT and BBN". The authors should carefully proofread the final version.- Some references have inconsistent reference formats, e.g, "In Proceedings of the IEEE conference on computer visionand pattern recognition" of [Xie et al., 2017] vs. "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition" of [Zhou et al., 2020] vs. "CVPR09" of [Deng et al., 2009]. In addition, some other references lack detailed information, e.g., [He and Garcia, 2009] lacks page, volume information. Some references also have capital issues, e.g., "The inaturalist species classification" -> "The iNaturalist species classification" of [Van Horn et al., 2018], and "Bbn" -> "BBN" of [Zhou et al., 2020]. Pros:- The paper proposed a novel self-supervised learning method to embed graphs to vector space. Different from previous methods, the method proposed a global-semantic learning strategy to encourage the embeddings to form a hierarchical clustering structure.  Both the embedding network and the hierarchical structure can be jointly learned.- Authors have provided extensive and convincing comparison results and numerical analysis to show the effectiveness of the method.- The paper is well-organized and clearly written. To the best of my knowledge, the proposed method is technically feasible.Cons:- The number of prototypes is determined by RPCL and can not be adjusted in training. Interpretable ML approaches are very important to advance a wide variety of fields in healthcare and science. This work makes a significant contribution in this direction by successfully applying SENNs to multi-variate non-linear time series data. The method which the authors named GVAR is generic enough and can be applied to many different problems involving multi-variate time series data thereby making the paper quite impactful and significant. The authors look into GC relationships between variables including the sign of the relationship which they can successfully infer using their approach. The authors provide an array of favorable results across multiple simulated non-linear dynamical systems which corroborate their findings convincingly and also compare their results to a comprehensive set of related and SOTA approaches. There are no obvious inconsistencies or errors that I can see in the paper to the best of my knowledge. The paper is well written and conveys the information clearly enough. An exciting next step is to apply this approach to real data, could be a great addition to this paper but not necessary for publication.   This paper proposes AdaSpeech, a Transformer-based TTS architecture derived from FastSpeech, but multi-speaker, and focussed on the task of low-resource, robust, and low-dimensional speaker adaptation. The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder. I don't have a clear intuition for exactly what kind of effect this would have on the phoneme embeddings and their mapping to spectral features, given that there are several non-linearities involved, but it certainly is a strong restriction. A global acoustic embedding conditions the decoder in addition to speaker embeddings, in the hopes of accounting for recording conditions, and, I suppose, timbre, which should then be disentangled from the linguistic information from the text in the decoder during pretraining and adaptable to new recordings at fine-tuning/inference. There is also a phoneme-level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme-level idiosyncrasies of the speaker, although this isn't clear to me. However notice that, if I'm not mistaken, these acoustic embeddings are used zero-shot; it is *only* the speaker embedding that is the input to fine-tuning, and this only via the normalization parameters. (Both the normalization parameters *and* the speaker embedding itself are fine-tuned.) I am not sure what the speaker-embedding is left to do with all this acoustic-level input, but OK. The authors assert in section 2.2 that zero-shot is not enough, but they do not cite a paper that does exactly what they did. This would be useful, or, even more welcome, an ablation study with the acoustic embeddings but not the speaker embedding. Looking at Figure 4b just underscores this point for me. The result is that, within the margin of error, this method is just as good in terms of speaker similarity as fine-tuning the entire decoder. Having listened to the examples they gave, I do find that there are speakers for which it is clearly not as good, but this is not reflected in the evaluator's results.Overall, this is very exciting work, as it not only promises space-efficient voice cloning, but, in doing so, suggests better disentanglement of speaker and phoneme properties in multi-speaker synthesis. This paper proposes diff pruning, an alternative paradigm for parameter-efficient transfer learning of pre-trained models. Similar to adapters, diff pruning leaves the body of the pre-trained model unchanged. Rather than inserting additional task-specific parameters into the pre-trained model, diff pruning adds reparameterizes the parameters of the transferred model $\theta_\tau$ by adding a diff vector $\delta_\tau$ to them: $\theta_\tau = \theta_{\text{pretrained}} + \delta_\tau$. Parameter efficiency is achieved by regularizing $\theta_\tau$ to be sparse. The authors achieve this by using a relaxed mask vector to approximate the $L_0$ norm. They also propose a way to control for a specific sparsity rate via projection onto the $L_0$ ball after training and to enforce group sparsity that takes the model's structure into account. The approach is evaluated on the GLUE benchmark where it achieves competitive performance to full fine-tuning a BERT Large model and adapters while being more parameter-efficient than both of them.Pros:1. The proposed method is intuitive and the different modelling choices are principled and well motivated.2. The method achieves strong results. It is competitive with full fine-tuning and more parameter-efficient than adapters, the prevalent approach for parameter-efficient transfer learning.3. The authors show how to effectively control the sparsity rate and incorporating structure via group sparsity brings further gains.4. The authors conduct extensive analyses, which pre-empted many of my questions, such as the variation across different sparsity masks, sparsity patterns across different tasks, etc. Overall, the analyses shed additional light on the characteristics and preferences of different tasks in transfer learning.Cons:1. The approach is potentially more complicated than the baseline, so it is important that the authors open-source their code.2. The diff vector is distributed over the entire set of parameters of the model rather than focused in a few layers. This makes it potentially harder to combine the diff vectors from different tasks as can be done with adapters (see e.g. https://arxiv.org/abs/2005.00247) and to compose multiple diff vectors. Questions:1. Does a visualization of diff vectors of different tasks (such as using t-SNE) reveal any interesting patterns?2. Are there any transfer settings that the addition of task-specific parameters can model but inserting layer-specific transformations via adapters cannot (or vice versa)? Adapters have been used to transfer across modalities such as languages (see e.g. Pfeiffer et al. (2020), https://arxiv.org/abs/2005.00052) and I am wondering whether the same would be possible by adding task-specific parameters.3. How long does your approach take to converge in comparison to the baselines? How much longer do you need to fine-tune with non-zero masks for magnitude pruning for sparsity control? What is the performance benefit of this further fine-tuning? The paper provides generalization bounds for seemingly complex neural networkson the basis of much simpler ones. That is a good idea and something that is currently very relevant I think and the approach seems to be the natural one to take. Essentially the bounds proved, bound the out of sample errorwith a form of in sample error, average difference in predictions between complex and simple network, and complexity term for the simple network in terms of Rademacher complexity. The authors provide a general framework which can be applied and show a particular way of using it and use recent results (Bartlett et al) to provide interesting application of the framework.In terms of the actual bound achieved there are a few things I feel should be discussed more.In Lemma 1.1.The in-sample error. First it is not the in-sample classification error but the sort of a margin error that essentially is never zero for any prediction. Usually margin errors have alinear penalty on the wrong side of the margin and zero on the correct side of the margin.Second, there is a factor 2 in front of it. Normally, and in uniform convergence bounds, there are no constants in front of the in-sample error. (The other constants are of no concern).The authors state that their work can be applied to  generalization bound of Arora et a. 18 that only worked for a compressed network but not the original one. Given the above comments about the actual bound achievable it is not clear to me what exactly one would get out of "distilling" the construction in Arora et al, but it seems it does not become the same bound as for the compressed network as shown by Arora et al. Comments on that would be appreciated.Another small question: What does the early distillation phase on page 4 means (below lemma 1.3)?I like the experimental setup, particularly using gradient descent to try and find a network to distill to.Overall, I think the paper is well written including the proof sketches that make me believe the statements are actually provable (I did not rigorously check)Overall, I think this is an interesting paper and should be accepted.  The authors introduce an interesting approach to handling hard "confident" samples in learning with label noises. At the heart of the proposed approach is an interactive method that jointly refines the classifier and the samples. The confident samples are initialized by utilizing the memorization effect of deep networks. Then, a classifier is learned from such samples. During the learning process, hard confident data are selected progressively by looking at the classification results, which further better select confident samples. Experimental results show that the proposed method achieves state of the art.Pros:1. The task studied here could be of interest to a large number of readers in the community.2. The overall idea is quite interesting and intuitively makes sense. I like the fact that the core idea of the proposed approach finds its root in physics. 3. The results are very promising, in spite of the simple nature.4. The manuscript is overall well-written and easy to follow. Cons:1. The iterative nature is good and bad. It seems to me that sometimes it requires many rounds in the inner loop, as shown in Fig. 3, for example, the number is up to 20 to 30. Please show some numbers in terms of running time and compare them with the baselines. 2. I might be missing something here but, what about the not-so-confident samples? Any scheme to take care of them?Minor issues1. The fonts in Fig. 3 should be enlarged.2. It would be better if the author could elaborate more on its connection with the momentum in physics. Summary: Well written paper with solid experiments on an extension of two prior works. This is likely of interest, good quality and I recommend to accept the paper at ICLR. There are no extensions that I would propose to include in this version.Quality: Good quality, well written paper, easy to follow and sufficiently detailed. Content is on-topic for ICLR and of interest to a general audience. Clarity: The paper is very well written, all details on the model, the training procedures, experiments are included. This paper is well polished and was easy to read and follow. The main assumptions are stated early on, problem definition is well stated, and the goal of the experiments are clearly stated before going into their discussion. The Appendix provides additional results and details. Nice paper to read, thanks for putting in the effort. Originality: This paper is a combination of two tasks, combining two prior works to one system. The original part is the research question on whether the two tasks should share a common feature space and whether the results improve by the network model. So this is an interesting paper, I would assume there is quite an audience that is interested in this topic. Without doubt the model is well constructed and trained, so there is also value in the construction. I have not seen the task of (few shot) recognition and visual reconstruction seen so far. This paper is a good extension of HoloGAN and has some novel points.- Conditional version of HoloGAN. This is a simple extension but useful and serves the purpose. - Combination of view synthesis and recognition. The flow of the architecture is well explained and leads to empirical improvements over each task in separation. More architecture choices would be possible, an evaluation of different backbones is included but not of other network combinations.- Experimental results are sufficient, on established dataset, there is no novelty in the application. Significance: For both tasks (view synthesis and reconstruction) there are stronger models. The authors claim that other models could be combined in their setup, I agree, but the empirical results are below state-of-the-art. But this is definitely a step in the right direction and I believe there is an interested audience for this finding and it is likely that the construction inspires future work. There are some extensions that would go beyond the paper, such as more challenging data, images with more than one object, and combination with even more vision tasks. This paper sets up a spatiotemporal neural network, claiming it hasn't been done before. It may however be similar to a spatiotemporal graph network (using a regular lattice graph connectivity structure). The authors should clarify this. The paper is well written however and sound otherwise. Some edits are required:- Page 2, ln 1: 'hidden states'? Language needs to be improved here. - Section 2, ln 6: Zhao et al (2015) rather. - Section 3 refers to 'markers'. In spatial statistics these are marks. I suggest sticking to the terminology of the field? - Page 2, ln -4: cannot rather than can not - Where is Figure 1 referred to in the text? - Page 3, ln -4: What is 'firing'? - Section 4.1, ln 1: fix the language- Section 4.1: be consisten: 2(a) or 2 (a) - not both- Page 6, lns -7, -8: spaces are missing- Caption of Figure 4 needs editing. - Figure 5: remove the names from the graphics at the top - put into captions. - There are many capitals missing in the references and journal names that should be in full. - Park (2019) has an et al in it? - Is there a published version of Mozer (2017)? # SummaryThis paper introduces a novel VAE-based model with the aim to improve unsupervised disentangling of latent factors in visual data.  This model differs from previous disentangling models in that it takes short (2-frame) videos as input instead of static images.  The model is equipped with a Laplace prior over the dynamics of the video to help it align its representation to axes of sparse temporal dynamics.  The intuition is that the temporal dynamics of natural visual stimuli vary sparsely according to some choice of factors, and that choice of factors is exactly what disentangled should refer to.  The authors show that their model achieves better disentangling than previous static-image methods according to a number of metrics.# Pros* The interpretation of disentangling as a basis in which the distribution of temporal dynamics of video is sparse is valuable.  Previous approaches to disentangling have been plagued by non-identifiability, and this new interpretation of disentangling is a natural and operational solution.  I fully agree with the authors that temporal information is essential for representation-learning, and hope this paper can help accelerate the field in that direction.* The paper is clear and well-written.* The experiments are very thorough in terms of comparisons to previous models and evaluation with previous metrics.* The application of the Mean Correlation Coefficient (MCC) as a metric for disentangling is good --- I think it is simpler and clearer than many existing disentangling metrics.* The latent embedding plots are a nice way to visualize latent representations.  While they dont show what effects non-matched generative factors have on the latent coordinates, they offer valuable information about the latent embedding that is complementary to the commonly used latent traversals.# ConsMy biggest suggestion is to include an ablation study.  Aside from PCL (which isnt variational so lives in a different world), there are no existing disentangling models on videos to fairly compare the authors model to.  Consequently, it is very important to perform ablation experiments.  More specifically, after reading the paper I have a burning question:  How important is the Laplace prior over transitions?  In other words, can the model work with just a KL regularization between the posteriors for consecutive timesteps?  I think its quite possible the answer is yes --- simply having a KL regularization instead of the Laplace prior should disentangle better than static-image VAEs because the diagonal posterior will be pressured to align with the transition dynamics.  So I wouldnt be surprised if the model does quite well with just a KL instead of the Laplace, and that would be a simpler model with two fewer hyperparameters.  So please do this experiment --- regardless of the outcome, the results will be very valuable for readers considering using your model.Aside from that, I have only a couple minor suggestions:* Figure 2 is a notationally confusing.  You use z subscripts to indicate time in the lower part of the figure, but in the top part of the figure z subscripts indicate component index.  Maybe make the component index a superscript, or at least make the z in the bottom part of the figure bold so the boldness distinguishes vector from scalar.* Perhaps make a note that the MCC metric doesnt work well for discrete latents like shape in dSprites.  For example, in Figure 4 the SlowVAE permutes the shape ordering (e.g. as compared to PCL) and gets a low MCC score for that, but that low score is a drawback of the metric, not the model.  This isnt unique to MCC --- most other metrics (except MIG) would fall for this too.  But perhaps for discrete latents (ones where we dont care about the ordering of a discrete set of values), MCC can take the highest over all permutations.  I dont think its necessary to do this, but perhaps you can add a sentence that mentions it so readers understand why the shape score is low.# Conclusion:Overall, I recommend this paper to be accepted.  It is very topical, since disentangling has recently been receiving increasing attention, and by incorporating temporal cues into disentangling in VAEs this paper could help steer the field in a productive new direction.  I only hope the authors do the suggested ablation experiment (I think practitioners would appreciate those results). Paper summary:The paper provides a reformulation of the fundamental operations in Euclidean space that are used in neural networks for the Poincaré ball model of hyperbolic space (and thus hyperbolic space generally). The papers reformulation differs from previous reformulations (Ganea et al. 2018) in several ways. For multinomial logistic regression, the excess n-1 degrees of freedom available in previous formulations when defining a Poincaré hyperplane via a point on the hyperplane and a normal vector are eliminated by using a canonical choice of normal vector along with a scalar quantity corresponding to the distance to the hyperplane from the origin. Fully connected (FC) neural network layers are also reformulated in a way that keeps with the interpretation of an affine transformation as returning a point whose individual coordinates are distances to a set of different hyperplanes. On the other hand, the previous reformulation directly used Möbius matrix-vector multiplication, which does not this same interpretation. The paper then gives hyperbolic reformulations of further types of neural network operations in Euclidean space, namely split/concatenation (less computationally intensive than that in previous work), convolution (not present in previous work). Turning its focus to attention models, the paper proves a theorem regarding the equivalence of various hyperbolic midpoints proposed in previous work. Finally, the paper carries out experiments testing each part of its reformulation on appropriate datasets.------------------------------------------Strengths and weaknesses:The paper gave clear motivations for each of part of its hyperbolic reformulation and explained how it differed from previous reformulations. The paper was well-written, and the authors performed regular sanity checks, such as re-deriving the Euclidean case for c -> 0. The experiments were appropriate and demonstrated that the reformulation works as well as previous reformulations (possibly better, with regard to stability). I liked the paper a lot, and I think it will definitely be of interest to people working on non-Euclidean deep learning. Im assigning a score of 8 (a very good conference paper), and I think that the paper is more or less ready for publication as is. Ive included a few questions below (to help my own understanding), as well as some typos I spotted whilst reading the paper.------------------------------------------Questions and clarification requests:1) Which of Spivak (1979), Petersen et al. (2006), and Andrews & Hopper (2010) were you working from when writing the brief summary of Riemannian Geometry on page 2? Some of the definitions given like An n-dimensional manifold M is an n-dimensional topological space that can be linearly approximated to an n-dimensional real space at any point x  M and g_{x} is a positive definite symmetric matrix defined on T_{x}M struck me as unusual (and possibly imprecise).2) In equation 4, you write q as the exponential of r_{k}[a_{k}], even though a_{k}  T_{q}M, not T_{0}M. Is this correct?3) At the beginning of section 3, you write The core concept is re-generalization of èa,xéb type equations with no increase in the number of parameters, which has the potential to replace any affine transformation in a shared manner. What did you mean by in a shared manner?4) At the end of section 4.1, you write In particular, our parameter-reduced approach obtains the same level of performance as a conventional hyperbolic MLR in a more stable training, as can be seen from the relatively narrower confidence intervals. Do you think your approach is genuinely more stable? If so, do you have any intuition as to why performance was more stable?------------------------------------------Typos and minor edits:- Section 1, paragraph 4, sentence 1  Despite such progresses -> Despite such progress- Section 2, Poincaré hyperplane  Ganea et al. 2018 certainly discusses Poincaré hyperplanes, but hyperplanes in Riemannian geometry have been studied a very long time before this.- Section 3.3, paragraph 1, sentence 5  approaches to the -> approaches the- Section 3.3, last paragraph, last sentence  treats every inputs fairly -> treats every input fairly- Section 4.2, paragraph 1, sentence 2  better to rephrase this- Section 4.2, paragraph 2, sentence 2  our models achieved the almost the same performance as Set Transformers -> our models achieved almost the same performance as Set Transformers- Section 4.3, paragraph 1, sentence 1  we experimented the convolutional sequence to sequence modelling task -> we experimented with the convolutional sequence-to-sequence modelling task- Table 2  on the Euclidean space -> on Euclidean space- Section 4.3, paragraph 1, sentence 2  the hyperbolic version of which have already been verified -> the hyperbolic version of which has already been verified- Section 4.3, paragraph 1, sentence 6  Note that the inputs for the sigmoid functions in Gated Linear Units are logarithmic mapped just as hyperbolic Gated Recurrent Units proposed by -> Note that the inputs for the sigmoid functions in Gated Linear Units are logarithmically mapped just like hyperbolic Gated Recurrent Units proposed by- Conclusion, final sentence  for the future researches -> for future research / for future researchers This work focuses on single-sided decentralized federated learning. The authors propose a pushsum-based algorithm to relax the symmetric matrix assumption, which leads to a flexibledecentralized training on a directed network graph. The authors analyze the regret bound, as well asrun some numerical experiments to demonstrate the correctness of the proposed algorithm.***Strengths***:Overall the paper is well written.1. The proposed method bridges a gap between existing decentralized federated learning algorithmsand real single-sided social networks. Another example I can recall is that sharing in the data marketis single-sided. The motivation is sound. As far as I know, this is the first paper in the FL communitytalks about this setting.2. The authors design a novel algorithm. Its theoretical results of the convergence rate connect"online learning" and "asymmetric graphs", which is novel in federated learning.3. The experimental design is excellent (including many settings). The code is very readable andwell-documented. I believe this helps the popularity of this proposed algorithm.***Weakness***:I can understand that this work focuses on the algorithm rather than provides a privacy guarantee.So it would be great if the authors provide some intuitions or discussions about how to addressprivacy concerns.I prefer to demonstrate the proposed algorithm on more challenging datasets, but since this workemphasizes the convergence analysis, it should be OK for me.The authors only mention an important work in the related works section: Notably, Zhao et al.(2019) shares a similar problem definition and theoretical result as our paper. However, single-sidedcommunication is not allowed in their setting, restricting their results. I suggest the authors discussmore about it and distinguish the contributions in theory analysis.Related works:Stochastic gradient push for distributed deep learning (ICML 2019) should be discussed.In section 4.4, more privacy-related works should be mentioned.Overall RatingSince the theory in this work is sound and the experimental design and code implementation arealso excellent, I incline to strongly support the acceptance of this work. The authors show that gradients from backpropagation are lognormally distributed empirically across several popular architectures like (Bert, ResNet18, MobileNetV2, VGG16, DenseNet121) over widely used datasets (CoLa, MRPC, ImageNet and CIFAR100). Using this result, they propose schemes for gradient quantization and gradient pruning that are theoretically principled and outperform existing methods in the literature. I was very impressed by the principled nature of the authors' approach. Proving the lognormal distribution using a KS test and using it elegantly to find analytical formulations for optimal bit division in quantization and threshold parameter for sparsity in gradient pruning was masterful. * In the appendix, the KS test shows that logLaplace distribution is also a good fit for the gradient distribution. Can the authors provide any intuition as to why a log normal might be better? * Clearly for pruning, logNormal distribution seems to work better than logLaplace. Itd be interesting to see the results of using LogLaplace distribution for quantization and verify that it yields worse results there too.* Can the authors elaborate on time/memory complexity, and how well their methods made improvements for training time? The paper proposes 2 new benchmarks for Rapid Task Solving (RTS), that evaluate RL agents on the ability to memorize past experiences and learn to plan to solve new tasks in different environments rapidly. The paper also proposes Episodic Planning Networks (EPN), an RL method that replaces a weighted sum and multi-layer peceptrons in memory networks with self-attention.  The proposed EPNs proved to significantly outperform baseline methods with memory and an LSTM method without memory. The paper formulates RTS as an extension of meta-reinforcement learning framework, where in addition to optimizing over a distribution of tasks, the objective is to also optimize over a distribution over environments. I think the work proposed in the paper is an important step towards developing general agents that learn to adapt quickly. To that end, it would be beneficial for the authors to release their code to benchmark new and older methods.I have a few clarification questions.How is the reward defined for each task in the 2 benchmarks? Is it 1 for reaching the goal and 0 otherwise?Given that the 2 domains are defined in terms of relations between entities (connections between symbols in the Memory&Planning game and neighborhoods in the other) how would relational and symbolic RL methods perfom compared to the baselines and the proposed EPN? I believe these would provide a fairer baseline performance compared to the LSTM used.   Summary Taking multilingual NMT (MNMT) into account, this work, investigates better model optimization alternative, that is in part can be attributed as a multi-task optimization problem. MNMT's are quite beneficial from different perspectives (improving low-resource languages, efficiency, etc). However, their inherently multi-task nature requires more focus on how to gist out the best possible learning for each of the languages pairs. With a potential impact on the optimization of other multi-task models, this work asks how model the similarity between model gradients is crucial in multi-task settings, and how to best optimize MNMT models focusing on the typologically similarity of languages. By analyzing the geometry of the NMT model objective function, authors indicate that computing similarity along gradient provides information on the relationship between languages and the overall model performance. Authors argue the analysis of the gradient helps to identify the point of limitation in multi-task learning, which the work aims to address, by focusing the parameter updates for tasks that are similar or close in terms of geometrical alignment (also known as Gradient Vaccine /GradVac/). Experimental results are provided from multilingual tasks involving 10^9 magnitude model training examples and several languages pairs. Mathematical proof and theoretical details of the proposed optimization approach GradVac are detailed in comparison with previous approach (such as Gradient Surgery). Experimental results shows the proposed GradVac to contribute for the improvement of model performance. These findings underline the importance of taking into account language proximity for a better optimization approach and model improvements in general. Pros / Reason for the Score After my assessment of the proposed approach and the visible advantage of GradVac, I am voting for an accept score. Below my points of the pros and cons of this work. It's my hope authors will address the cons and the questions raised in the rebuttal period.  - This work raises an important question of optimization in a multi-task model, particularly for multilingual NMT models where an optimization approach is quite rare and recent progress in MNMT mainly focuses on improving performance. Hence the findings in this work, can provide further insight on how to best optimize an MNMT model and potentially set a new standard training mechanism for future works in MNMT. - From the experimental results, particularly its quite interesting to see how the proposed approach (GradVac) improves the high-resource languages (on the left side of Figure 6 (b)). I think in massive MNMT models while there is huge gain (naturally) for low-resource cases, the high-resource pairs tends to degrade. This work shows an interesting mechanism to address performance degradation for certain pairs in an MNMT model and to maintain an improvement trend  for all of the language pairs involved. Cons and Questions - Considering the language similarity, this work focused on typologically similarity (that deals with the characteristics of the language structure), is there any consideration for genetic similarity, or any other similarity measure between languages the authors considered? Or why is the typological similarity the primary/only choice for this work? - As in Yu et al. 2020, where the PCGrad approach is used to project the gradient of task i to the plane of task j, was there any motivation behind not to adapt or asses this approach in MNMT before going / do the authors have any comment why this approach does lag behind from the GradVacc.? - Perhaps this is related to the assumption PCGrad is not fit a positive gradient similarities - a case in this work?- One of the advantages of MNMT model is efficiency (as also mentioned in this work), however, when we deal with model training or even inference the paper does not mention the complexity that can be introduced by the application of GradVac, can the author provide the details on this? - Page (P) 1: mentions one of the motivations for the work is to investigate ways to optimize the single language-agnostic objective for training an MNMT model that leverages training data of multiple pairs - if this work is aiming at optimizing based on task relatedness - did it consider for instance training MNMT models that are language family specific and see how that relatedness correlates with the approach in this work and the baseline MNMT models (such as Any->En or En->Any)?- What is the impact of training only two MNMT models Any->En and En->Any, why not Any<>Any? Wouldn't this make a lot more sense from the point of having multiple tasks (in terms of observing different language characteristics both at the encoder and decoder side of the model)?Similarly, the Any<>Any that is employed (shown in Figure 2.) gradient similarities correlates positively with model quality. In other words authors clearly demonstrated the En>Any direction gradient similarity is quite low with respect to Any>En, in my understanding using Any<>Any model throughout the experiment makes more sense by constructing a real multi-tasking MNMT model, where we can also see the proposed approaches effectiveness. - Not sure if I am missing it, if it correct that we do not have comparison of the proposed optimization approaches with other optimizations from the results in Figure 6? At least with PCGrad ?Comments- In an ideal case, I would go for evaluating a multilingual model that is not English centric to properly construct a real multilingual model. I understand the experimental design here, specifically this is the data (En<>Any) in general or available in-house for the authors. Yet, with recent progresses in multilingual NMT and zero-shot NMT approaches, its becomes realistic now to incrementally augment data for the non English pairs (can leverage monolingual data of the Any languages too), hence, resulting in more pairs. Such a setting of Any-Any can even further reflect how the optimization is beneficial. - please re-arrange the figures, I see discussion about Figure 5 while there is Figure 3 and 4 beforehand - if possible.  This paper proposes an iterative method that jointly estimates viewpoints, light directions, depth, and albedo from single images, by projecting intermediate renderings to the nautral image manifold. Intuitively, the method works by generating, with pre-trained GANs, multiple views of the same object under different lightings, and then inferring 3D shapes from those variants. The key idea is to use pre-trained 2D GANs to make such data generation photorealistic. The authors also demonstrate 3D edits, such as 3D rotation and relighting, that one can perform after running their model.I like this paper because(1) it presents the novel idea of "generating", by GAN inversion, photorealistic multi-view, multi-light data of the given real object, from which the 3D shape can then be estimated;(2) extensive evaluations were performed to demonstrate the high quality achieved; and(3) one can perform 3D edits, such as 3D rotation and relighting, on top of the model outputs. Having explicit 3D understanding for relighting makes a lot of sense to me, and this paper presents a new angle of doing so by GAN inversion. In terms of drawbacks, this paper would benefit from the following experiments or clarifications:(1) How crucial is the size of the dataset for GAN pretraining? For example, if the dataset is small, not covering many of the face poses, I imagine the GAN projections may not always look realistic, thereby causing the shape estimation to degrade. Such failure cases or studies should be shown so that the reader understands what impact the dataset bias or size has on the final results. An example would be a plot of shape reconstruction error w.r.t. the dataset size or face pose coverage.(2) How robust is the algorithm to the shape initialization (ellipsoid shape)? More importantly, what about its location -- what if the "off-the-shelf scene parsing model" fails so that the ellipsoid is placed off the main object? If the model fails because of bad initializations, what do the failure modes look like? Are the shapes completely garbage, or something that looks like the initialization? (3) Why is the viewing direction parameterized in R6 (I presume the start and end XYZs)? Shouldn't it be in S2, just like the light direction? If they are in R6, then results on zooming in/out should be included. Otherwise, there seems to be no point in defining them in R6. This paper proposes an active learning and active search approach that targets samples for rare classes in very large unlabeled datasets with highly imbalanced class distributions. This is a common scenario in real-world applications, where these rare situations can be critical to accurately categorize - ie endangered species. The authors propose an approach that targets these rare cases while reducing the number of overall examples sampled, and that scales with the amount of labeled data as opposed to the amount of unlabeled data which allows them to consider datasets up to billions of examples.Pros:This is a well-motivated task, there is a clear need for this type of method as access to unlabeled data increasesTheir method scales effectively to very large sets of unlabeled data, eg matching baseline performance with only 0.1% of the unlabeled data sampled on their proprietary 10 billion image dataset.The method leads to impressive computational speedups in active learning selection rounds, eg 4000x speedup in selection round time on ImageNet.Cons - I am glad to see that they compare performance to public datasets to help with this, but some of their biggest claims are not reproducible because they are shown with proprietary data.Their model does require an initial pass through an embedding model for all the data, which would be very slow and expensive for 10 billion images. This embedding model seems to need to encode the rare classes appropriately and be able to cluster them together sufficiently for the neighbors of a rare class to be likely to be of the same class for this method to work well. In the low data regime it is not always guaranteed that you will have sufficient training data to build an embedding that handles rare categories sensibly, particularly in fine-grained scenarios. The authors could do a better job analyzing the impact of their chosen embedding function on the efficacy of their method. They discuss this a bit in section 4.4, but it would be awesome to see some concept of performance of the method for each concept included in Figure 3 (unsure how best to do this). Summary: In the submitted paper, the authors study high-quality prediction intervals (PIs). The paper proposes a novel design of loss functions to generate PIs and conditional coverage estimates. The theoretical justification for using the conditional coverage error (in Ca-module) is presented and the numerical experiments with promising results are provided on multiple benchmark datasets.Pros:- The high-quality and reliable PI becomes more critical than ever as machine learning models have been used in the real-world decision-making process. This paper considers this important topic and provides a simple yet principled solution.- The paper is well organized and theoretical results are well explained.- Numerical experiment results on multiple synthetic and real datasets justify the practical advantages of the proposed algorithm.Suggestion:- Although the Bayesian framework focuses on the parameter uncertainty, as the authors mentioned in Section 6, it can be applied to generate the PIs. (Note that the posterior predictive distribution can be directly derived from the posterior distribution). A comparison study with Bayesian methods will help readers understand the advantages (or disadvantages) of the proposed method.I vote for acceptance.  This paper studies a very interesting new problem of assessing unrolled models in a broader context using NAS methods. LISTA-style unrolling has been popular for deep learning-based inverse problems. But it is quantitatively unclear how good the unrolled models are, among all possible model variations. To fill in this gap, the authors first define a proper search space based on the varying connections and neurons from the unrolled LISTA backbone architecture. NAS is then exploited as the tool to find the best subsect of architecture from the large space. This method is very intuitive yet solidly done. Since NAS itself is not always stable, the authors present a number of efforts and discussions to support the reliability and reproducibility of their observations. Sec 2.3 shows their diligence in trying out multiple NAS algorithms; avoiding using weight sharing; and averaging the top architectures to smooth out random fluctuations. I appreciate those valuable careful efforts. The experimental study in Section 3 is very comprehensive and clearly laid out. Table 1 gives a great gist of their main findings. The authors compare with two natural baselines, LFISTA and Dense-LISTA. They prove NAS can indeed find superior architectures than those hand-crafted in all the different settings. The overall take-home point seems like no big surprise at first glance: unrolling provides a reasonably good architecture; yet if there are abundant training data, more complicated architectures can be discovered by varying connections (varying neurons seems not helpful). But, a closer look at those experiments reveals many new finer-grained observations that really intrigue me. For example, it is interesting to see the denser the better is not the simple right claim in unrolling; and especially unrolling-based models are advantageous under data-limited training. Another good surprise is to see that all top architectures unanimously adopt soft thresholding as the only neuron type for all layers. The most interesting part of this paper is the "open the box" section 4 where the authors discuss the common characteristics of top architectures. Figure 2 is a highly interesting visualization and shows the top-50 architectures are indeed consistent on most connections.  It is very promising if we can understand further (1) why the later the denser - could that imply/be interpreted as switching to another higher-order algorithm in later iterations? and (2) why the last layer is particularly all-connected?I think bringing NAS to learning based algorithm design is a very novel and interesting direction. This paper can potentially turn into a seminal work here to inspire followers. The paper is also well written, and the logic is very clear to follow. Section 5 concludes with a thoughtful discussion on what this work may connect and point to for the entire unrolling field. I have a few suggestions and critiques for the authors to address:-Can we have a weighted version of Figure 2, say the higher-ranked architectures from the top-50 will get more weights when summed up; and in this way would we be able to observe more consensus of what relatively better models (if weighed more) tend to prefer?-LFISTA is a principled way to add more connections to LISTA using momentum. In this paper, the authors claimed their LWA to be better than momentum-based averaging in NAS. I wonder what will happen if replacing the momentum connection in LFISTA with the LWA way: will LFISTA performance improve or degrade?-I remain to be skeptical about the last LADMM example. It is rather unclear to me WHY the learned connectivity patterns from LISTA should transfer to LADMM? Perhaps, that implies some general idea of how dense connectivity could be properly injected in a sparse regression task. I would request to see comparison results from (1) adding dense connections only in the latter half layers; and (2) adding random connections, but with the same total connected percentage as the LISTA pattern. If either 1 or 2 can perform on par or better than the transferred LISTA pattern, the authors should consider tuning their claims to be more specific about what really matters here, than just saying vaguely about transfer.  In this submission a routing problem is studied. In the considered model with each edge of the given graph a congestion function is associated that specifies the congestion depending on the current load of the edge. Then cars have to be routed through the network where each car has a source and a destination and one aims at choosing a path from the source to the destination with the smallest total congestion. However, the congestion functions of the edges are a priori unknown and hence one cannot trivially use a shortest path algorithm. Instead one gains information about the congestion functions only by routing the cars. When a car is routed one observes for each edge on its path the current congestion up to some random additive term. These observations can then be used for future routing decisions.The main result of the submission is an algorithm that achieves a cumulative sublinear regret of O(|E|t^{2/3}) where the regret is defined as the difference between the expected path length chosen by the algorithm and the length of the shortest path. Some experiments are also conducted with this algorithm but the focus of the submission is clearly on the theoretical results. The algorithm itself is non-trivial but also not too surprising. Due to the random noise one needs enough samples to estimate the congestion on an edge. The difficulty is that the congestion depends on the current load, i.e., one needs enough samples close to the current load (here one uses that the congestion functions are assumed to be Lipschitz continuous). The algorithm cleverly and adaptively partitions the samples of an edge into buckets where each bucket represents a certain range of loads. Then there are two factors that determine the precision of the congestion estimation: the range of the buckets and the number of samples per bucket. The more samples one has in a certain range the smaller can the ranges of the buckets be. Basically the algorithm splits a bucket if it contains already enough samples.I find the results interesting. The proposed algorithm is natural and its analysis is non-trivial. I am not completely sure if the model is very realistic though. It is assumed that the congestion functions are unknown (which makes sense). However, if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges. It is not clear to me why this makes sense. It might also be the other way round: While the congestion functions are more or less known from historic data, the current load is unknown to the drivers. In my opinion the authors could discuss the model in more detail. This paper proposes to use input convex neural networks (ICNN) to capture a complex relationship between control inputs and system dynamics, and then use trained ICNN to form a model predictive control (MPC) problem for control tasks.The paper is well-written and bridges the gap between neural networks and MPC.The main contribution of this paper is to use ICNN for learning system dynamics. ICNN is a neural network that only contains non-negative weights. Thanks to this constraint, ICNN is convex with respect to an input, therefore MPC problem with an ICNN model and additional convex constraints on control inputs is a convex optimization problem.While it is not easy to solve such a convex problem, it has a global optimum, and a gradient descent algorithm will eventually reach such a point. It should also be noted that a convex problem has a robustness with respect to an initial starting point and an ICNN model itself as well. The latter is pretty important, since training ICNN (or NN) is a non-convex optimization, so the parameters in trained ICNN (or NN) model can vary depending on the initial random weights and learning rates, etc. Since a convex MPC has some robustness (or margin) over an error or deviation in system dynamics, while non-convex MPC does not, using ICNN can also stabilize the control inputs in MPC.Overall, I believe that using ICNN to from convex MPC is a sample-efficient, non-intrusive way of constructing a controller with unknown dynamics. Below are some minor suggestions to improve this paper.-- Page 18, there is Fig.??. Please fix this.-- In experiments, could you compare the result with a conventional end-to-end RL approach? I know this is not a main point of this paper, but it can be more compelling. This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers This work builds on a sum(top k) identity to derive a pathwise differentiable sampler of 'unimodal row stochastic' matrices. The Plackett-Luce family has a tractable density (an improvement over previous works) and is (as developed here) efficient to sample. [OpenReview did not save my draft, so I now attempt to recover it from memory.]Questions:- How much of the improvement is attributable to the lower dimension of the parameterization? (e.g. all Sinkhorn varients have N^2 params; this has N params) Is there any reduction in gradient variance due to using fewer gumbel samples?- More details needed on the kNN loss (uniform vs inv distance wt? which one?); and the experiment overall: what k got used in the end?- The temperature setting is basically a bias-variance tradeoff (see Fig 5). How non-discrete are the permutation-like matrices ultimately used in the experiments? While the gradients are unbiased for the relaxed sort operator, they are still biased if our final model is a true sort. Would be nice to quantify this difference, or at least mention it.Quality:Good quality; approach is well-founded and more efficient than extant solutions. Fairly detailed summaries of experiments in appendices (except kNN). Neat way to reduce the parameter count from N^2 to N.I have not thoroughly evaluated the proofs in appendix.Clarity:The approach is presented well, existing techniques are compared in both prose and as baselines. Appendix provides code for maximal clarity. Originality:First approach I've seen that reduces parameter count for permutation matrices like this. And with tractable density. Very neat and original approach.Significance:More scalable than existing approaches (e.g: only need N gumbel samples instead of N^2), yields better results.I look forward to seeing this integrated into future work, as envisioned (e.g. beam search) Summary:========The paper presents rates of convergence for estimating nonparametric functions in Besovspaces using deep NNs with ReLu activations. The authors show that deep Relu networks,unlike linear smoothers, can achieve minimax optimality. Moreover, they show that in arestricted class of functions called mixed Besov spaces, there is significantly milderdependence on dimensionality. Even more interestingly, the Relu network is able toadapt to the smoothness of the problem.While I am not too well versed on the background material, my educated guess is that theresults are interesting and relevant, and that the analysis is technically sound.Detailed Comments:==================My main criticism is that the total rate of convergence (estimation error + approximationerror) has not been presented in a transparent way. The estimation error takes the formof many similar results in nonparametric statistics, but the approximation error isgiven in terms of the parameters of the network, which depends opaquely on the dimensionand other smoothness parameters. It is not clear which of these terms dominate, andconsequently, how the parameters W, L etc. should be chosen so as to balance them.While the mixed Besov spaces enables better bounds, the condition appears quite strong.In fact, the lower bound is better than for traditional Holder/Sobolev classes. Can youplease comment on how th m-Besov space compares to Holder/Sobolev classes? Also, canyou similiarly define mixed Holder/Sobolev spaces where traditional linear smoothersmight achieve minimax optimal results?Minor:- Defn of Holder class: you can make this hold for integral beta if you define m to bethe smallest integer less than beta (e.g. beta=7, m=6). Imo, this is standard in mosttexts I have seen.- The authors claim that the approximation error does not depend on the dimensionality  needs clarification, since N clearly depends on the dimension. If I understand  correctly, the approximation error is in fact becoming smaller with d for m-Besov  spaces (since N is increasing with d), and what the authors meant was that the  exponential dependnence on d has now been eliminated. Is this correct?Other- On page 4, what does the curly arrow notation mean?- Given the technical nature of the paper, the authors have done a good job with the  presentation. However, in some places the discussion is very equation driven. For e.g.  in the 2nd half of page 4, it might help to explain many of the quantities presented in  plain words.Confidence: I am reasonably familiar with the nonparametric regression literature, butnot very versed on the deep learning theory literature. I did not read the proofs indetail. The paper proposes a simple but effective method for controlling the location of objects in image generation using generative adversarial networks. Experiments on MNIST and CLEVR are toy examples but illustrate that the model is indeed performing as expected. The experiments on COCO produce results that while containing obvious artefacts are producing output consistent with the input control signal (i.e., bounding boxes). It would however have been interesting to see more varied bounding box locations for the same caption.In short, the paper makes an interesting addition to image generation works and likely to be incorporated into future image generation and inpainting methods. The paper analyses the data collected from 6005 neurons in a mouse brain. Visual stimuli are presented and the responses of the neurons recorded. In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli. Results show a decent correlation between neuron responses and trained network. Moreover, the rotational equivariant architecture beats a standard CNN with similar number of feature maps. The analysis and discussion of the results is interesting. Overall, the methodological approach is good.I have trouble understanding the plot in Figure 4, it also does not print well and is barely readable on paper.I have a small problem Figure 6 where "optimal" response-maps are presented. From my understanding, many of those feature maps are not looking similar to feature maps that are usually considered. Given the limited data available and the non-perfect modeling of neurons, the computed optimal response-map might include features that are not present in the dataset. Therefore, it would be interesting to compare those results with the stimuli used to gather the data. E.g. for a subset of neurons, one could pick the stimulus that created the maximum response and compare that to what the stimulus with the maximum response of the trained neuron was. It might be useful to include the average correlation of the neurons belong to each of the 16 groups(if there are any meaningful differences), especially as the cut-off of "correlation 0.2 on the validation set" is rather low.Note: I am not an expert in the neural-computation literature, I am adapting the confidence rating accordingly. This paper presents a study of the community detection problem via graph neural networks. The presented results open the possibility that neural networks are able to discover the optimal algorithm for a given task. This is rather convincingly demonstrated on the example of the stochastic block model, where the optimal performance is known (for 2 symmetric groups) or strongly conjectured (for more groups). The method is rather computationally demanding, and also somewhat unrealistic in the aspect that the training examples might not be available, but for a pioneering study of this kind this is well acceptable.Despite my overall very positive opinion, I found a couple of claims that are misleading and overall hurt the quality of the paper, and I would strongly suggest to the authors to adjust these claims:** The method is claimed to "even improve upon current computational thresholds in hard regimes." This is misleading, because (as correctly stated in the body of the paper) the computational threshold to which the paper refers apply in the limit of large graph sizes whereas the observed improvements are for finite sizes. It is shown here that for finite sizes the present method is better than belief propagation. But this clearly does not imply that it improves the conjectured computational thresholds that are asymptotic. At best this is an interesting hypothesis for future work, not more. ** The energy landscape is analyzed "under certain simplifications and assumptions". Conclusions state "an interesting transition from rugged to simple as the size of the graphs increase under appropriate concentration conditions." This is very vague. It would be great if the paper could offer intuitive explanation of there simplifications and assumptions that is between these unclear remarks and the full statement of the theorem and the proof that I did not find simple to understand. For instance state the intuition on in which region of parameters are those results true and in which they are not. ** "multilinear fully connected neural networks whose landscape is well understood (Kawaguchi, 2016)." this is in my opinion grossly overstated. While surely that paper presents interesting results, they are set in a regime that lets a lot to be still understood about landscape of fully connected neural networks. It is restricted to specific activation functions, and the results for non-linear networks rely on unjustified simplifications, the sample complexity trade-off is not considered, etc. Misprint: Page 2: cetain -&gt; certain. This paper introduces an AST-based encoding for programming code andshows the effectivness of the encoding in two different task of codesummarization:1. Extreme code summarization - predicting (generating) function name from function body (Java)2. Code captioning - generating a natural language sentence for a (short) snippet of code (C#)Pros:- Simple idea of encoding syntactic structure of the program through random paths in ASTs- Thorough evaluation of the technique on multiple datasets and using multiple baselines- Better results than previously published baselines- Two new datasets (based on Java code present in github) that will be made available- The encoding is used in two different tasks which also involve two different languagesCons:- Some of the details of the implementation/design are not clear (see some clarifying questions below)- More stats on the collected datasets would have been nice- Personally, I'm not convinced "extreme code summarization"is a great task for code understanding (see more comments below)Overall, I enjoyed reading this paper and I think the authors did agreat job explaining the technique, comparing it with other baselines,building new datasets, etc.I have several clarifying questions/points (in no particular order):* Can you provide some intuition on why random paths in the AST encode  the "meaning" of the code? And perhaps qualitatively compare it with  recording some other properties from the tree that preserve its  structure more?* When you perform the encoding of the function body, one sample in a  training step contains all the k (k = 200) paths and all the 2*k  terminals (end of Section 2)? Or one path at a time (Section 3.2)?  I'm guessing is the latter, but not entirely sure. Figure 3 could  improve to make it clear.* Can you explain how you came up with k = 200? I think providing some  stats on the dataset could be helpful to understand this number.* The results for the baselines - do you train across all projects?  (As you point out, ConvAttention trained separately, curious whether  it makes a difference for the 2 datasets med and large not present  in the original paper).* I'm not sure I understand parts of the ablation study. In particular  for point 1., it seems that instead of the AST, only the terminal  nodes are used. Do you still use 200 random pairs of terminal? Is  this equivalent to a (randomly shuffled) subset of the tokens in the  program? Also could you explain why you do the ablation study on the  validation set of the medium dataset? In fact, the caption of Table  3 says it's done on the dev set. This part was a bit confusing.* I would have liked to see more details on the datasets introduced,  in particular wrt metrics that are relevant for training the model  you describe (e.g., stats on the ASTs, stats on the number of random  paths in ASTs, code length in tokens, etc.)* I'm not convinced that the task of "extreme code summarization" is a  meaningful task. My main problem with it is that the performance of  a human on this task would not be that great. On one hand humans  (I'm referring to "programming humans" :) ) have no problem in  coming up with a name for a function body; however, I'm not  convinced they could predict the "gold" standard. Or, another way of  thinking about this, if you have 3 humans who provided names for the  same function, my guess it that there will be a wide degree of  (dis)agreement. Some of the examples provided in the supplementary  material can serve as confirmation bias to my thought :): Fig 7. I  claim "choose random prime" and "generate prime number" are  semantically close, however, the precision and recall for this  example are both low. All this being said, I understand that it's a  task for which data can be generated fairly quickly to feed the  (beast) NN and that helps pushing the needle in understanding code  semantics.* It would be nice to see "exact match" as one of the metrics (it is  probably low, judging by F1 scores, but good to be reported).* Most likely the following paper could be cited in the related work:Neural Code Comprehension: A Learnable Representation of Code Semanticshttps://arxiv.org/abs/1806.07336https://nips.cc/Conferences/2018/Schedule?showEvent=11359Page 5 first phrase at the top, perhaps zi is a typo and it issupposed to be z1? The idea is really interesting. One only need to train and maintain one single model, but use it in different platforms of different computational power.And according to the experiment results of COCO detection, the S-version models are much better than original versions (eg. faster-0.25x, from 24.6 to 30.0) . The improvement is huge to me. However the authors do not explain any deep reasons.And for classification, there are slightly performance drop instead of a large improvement which is also hard to understand. For detection, experiments on depth-wise convolution based models (such as mobilenet and shufflenet) are suggested to make this work more solid and meaningful. I really liked this paper and believe it could be useful to many practitioners of NLP, conversational ML and sequential learning who may find themselves somewhat lost in the ever-expanding field of dynamic neural networks.Although the format of the paper is seemingly unusual (it may feel like reading a survey at first), the authors propose a concise and pedagogical presentation of Jordan Networks, LSTM, Neural Stacks and Neural RAMs while drawing connections between these different model families.The cornerstone of the analysis of the paper resides in the taxonomy presented in Figure 5 which, I believe, should be presented on the front page of the paper. The taxonomy is justified by a thorough theoretical analysis which may be found in appendix.The authors put the taxonomy to use on synthetic and real data sets. Although the data set taxonomy is less novel it is indeed insightful to go back to a classification of grammatical complexity and structure so as to enable a clearer thinking about sequential learning tasks. An analysis of sentiment analysis and question answering task is conducted which relates the properties of sequences in those datasets to the neural network taxonomy the authors devised. In each experiment, the choice of NN recommended by the taxonomy gives the best performance among the other elements presented in the taxonomy.Strength:o) The paper is thorough and the appendix presents all experiments in detail. o) The taxonomy is clearly a novel valuable contribution. o) The survey aspect of the paper is also a strength as it consolidates the reader's understanding of the families of dynamic NNs under consideration.Weaknesses:o) The taxonomy presented in the paper relies on an analysis of what the architectures can do, not what they can learn. I believe the authors should acknowledge that the presence of Long Range Dependence in sequences is still hard to capture by dynamic neural networks (in particular RNNs) and that alternate analysis have been proposed to understand the impact of the presence of such Long Range Dependence in the data on sequential learning. I believe that mentioning this issue along with older (<a href="http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf)" target="_blank" rel="nofollow">http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf)</a> and more recent (e.g. http://proceedings.mlr.press/v84/belletti18a/belletti18a.pdf and https://arxiv.org/pdf/1803.00144.pdf) papers on the topic is necessary for the paper to present a holistic view of the matter at hand.o) The arguments given in 5.2 are not most convincing and could benefit from a more thorough exposition, in particular for the sentiment analysis task. It is not clear enough in my view that it is true that "since the goal is to classify the emotional tone as either 1 or 0, the specific contents of the text are not very important here". One could argue that a single word in a sentence can change its meaning and sentiment.o) The written could be more polished.As a practitioner using RNNs daily I find this paper exciting as an attempt to conceptualize both data set properties and dynamic neural network families. I believe that the authors should address the shortcomings I think hinder the paper's arguments and exposition of pre-existing work on the analysis of dynamic neural networks. [Overview]In this paper, the authors proposed to use dynamic sparse computation graph for reducing the computation memory and time cost in deep neural network (DNN). This method is applicable in both DNN training and inference. Unlike most of previous work that focusing on the reduction of computation during the inference time, this new method propose a dynamic computation graph by pruning the activations on the fly during the training of inference, which is an interesting and novel exploration. In the experiments, the authors performed extensive experiments to demonstrate the effectiveness of the proposed method compared with several baseline methods and original models. It is clear to me that this method helps to reduce the memory cost and computation cost for both DNN training and inference.[Strengthes]1. This paper addresses the computational burden in both memory and time from a novel angle than previous network pruning methods. It can be applied to reduce the computation in both network training and inference, but also preserve the representation ability of the network.2. To endow the network compression in training and inference, the authors proposed to mute the low-activated neurons so that the computations merely happened on those selected neurons. 3. For the selection, the authors proposed a simple but efficient dimension reduction methods, random sparse projection, to project the original activations and weights into a lower-dimensional space and compute the approximated response map in such a lower dimension space, which the selection is based on.4. The authors performed comprehensive experiments to demonstrate the effectiveness the proposed method for network compression. Those results are insightful and solid.[Questions]1. Is the sparsity of each layer the same across the whole network? It would be nice if the authors could perform some ablation studies on varied sparsity in different layers, maybe just with some heuristic methods, e.g., decreasing the sparsity from lower layer to upper layers. As the authors mentioned, higher sparsity causes a larger degradation on deeper network. I am curious that whether there are some better way to set the sparsity.2. During the training of the network, how the activation evolve? It would be interesting to show how the selected activation changes across the training time for the same training sample.  This might provide some insights on when the activations begin to converge to a stable state, and how it varies layer by layer. 3. Following the above questions, is there any stage that the sparsity can be fixed without further computation for selection. In generally, the training proceeds for a number of epochs. It would be nice if we can observe some convergence on the selected activations and then we can suspend the selection for saving the computation burden.[Conclusion]This paper present an interesting and novel approach for network pruning in both training and inference. Unlike most of the previous work, it pruning the activations in each layer though a dimension reduction strategy. From the experiments, this method achieved an obvious improvement for reducing the computation memory and time cost in training and inference stages. I think this paper has prompted a new direction of efficient deep neural network. As a paper on how to prioritize the use of neurons in a memory this is an excellent paper with important results. I am confused by the second part of the paper an attached GAN of unlimited size. It may start out small but there is nothing to limit its size over increased learning. It seems to me in the end it becomes the dominate structure. You start the abstract with "able to learn from a stream of data over an undefined period of time". I think it would be an improvement if you can move this from an undefined time/memory size to a limited size for the GAN and then see how far that takes you. This is an exciting paper with a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied. The authors demonstrate the reliability of their method on a large dataset of acoustic instruments and report human evaluation metrics. I expect their proposed method of preprocessing audio to become standard practice.Why didn't you train a WaveNet on the high-resolution instantaneous frequency representations? In addition to conditioning on the notes, this seems like it would be the right fair comparison. I'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?Small comments:- Figure 1 &amp; 2: label the x-axis as time. Makes it a lot easier to understand.- I appreciate the plethora of metrics. The inception score you propose is interesting. Very cool that number of statistically-different bins tracks human eval!- sentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar. - Figure 5 is low-res. Please fix. All the other figures are beautiful - nice work! The paper proposes a method for learning embedding of hierarchies. Specifically, the paper builds on a a geometrically inspired embedding method using box representations. The key contribution of the paper is facilitating optimization of these models by gradient based methods, which eventually leads to improved accuracy on relevant benchmark data (on par or beyond SOTA). The observation is that when two boxes are disjoint in the model but have overlap in the ground truth, no gradient can flow to the model to correct the problem (which is happens in case of sparse-data.To alleviate the above problem, the paper proposes smoothing the model. That is, transforming the original model constructed from indicator functions (hence difficult to optimize by gradient based method) to a smooth differentiable function by diffusing the landscape. The diffusion process corresponds to convincing the objective function with the Gaussian kernel.I find the idea of converting such combinatorial problems to differentiable, specially when gradient methods can succeed in optimizing them afterward, very fascinating. I believe this paper is taking a theoretically sound path to construct the differentiable form of the originally non-differentiable problem. As the authors find, the smoothed function leads to improved performance against SOTA on relevant benchmark data such as WordNet hypernymy, Flick caption entailment and MovieLnes market basket data.One downside of the current submission is that the details of optimization are now provided at all. What algorithm do you use to optimize the objective function? What are the hyper parameters? What value of sigma (for diffusion) do you use [or maybe you use the continuation method to gradually anneal sigma from large toward zero?). These are important details that I ask the authors to include.Also, I think some graphical illustration of the embedding would be very helpful, perhaps something like Figure 2 of "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures". I hope such illustration is added to the submission. This paper proposes a soft relaxation of the box lattice (BL) model of Vilnis et al. 2018 and applies it to several graph prediction tasks. Results are comparable to the BL model on existing artificially-balanced data but significantly better on more natural unbalanced data with a large number of negatives. The paper assumes some familiarity with the problem domain and existing works (there is not a lot of exposition for an unfamilar reader), but should be of strong interest to anyone working on embeddings or graph prediction.The paper is well-written, with clear explanations of the desired properties of the model and a concise set of experiments that are easy to follow. The strongest result is that on unbalanced WordNet, while the Flickr and MovieLens results are a little less clear but do show that this technique does not cause any loss in performance.A few points of feedback:- Missing citation / comparison: https://arxiv.org/pdf/1804.01882.pdf (Ganea et al. 2018) is an alternative way of generalizing order embeddings. They also report very high numbers on WordNet, though I'm not sure they are directly comparable.- The Gaussian relaxation (Eq. (2) and (3)) defines a particular length scale, \sigma. It's not clear if this is also implicit in the softplus derivation (by analogy with Eq. (4), should we assume that it approximates the \sigma = 1 case?). What effect does this have on the embedding space? Without it, it would seem that the normal BL model is scale invariant, which might be a desirable property for representing hierarchical data.- The main thrust of section 5.2 is that smoothed box embeddings retain better performance with increasing numbers of negatives. Could you include the ratio of positive / negative examples on the Flickr dataset, and some measure of the distribution of P(A|B) values on MovieLens to get a sense of how these datasets compare?- Flickr data: what is the encoder model that produces the embeddings here, and how does it handle unseen captions? (Why would we expect the smoothed box model to handle unseen captions better?)- There's a strong emphasis on how smoothing makes training easier. Do you have any metrics to directly support this, such as variance under random restarts?- In the abstract and introduction, it's easy to gloss over "inspired by" and assume that the actual model is a Gaussian convolution. Could be more direct here that it's a softplus approximation. Pros:This paper - Proposes a method for producing visual explanations for deep neural network outputs, - Improves quality of the guided backprop approach for strided layers by converting stride 2 layers to stride 1 and resampling inputs (improving on a longstanding difficulty with such approaches), - Shows fairly rigorous experimentation demonstrating the applicability and properties of the proposed approach, and - Releases a new synthetic dataset and benchmark for visual explanation methods.Although producing visual explanations is a task fraught with difficulty for many reasons, including that explanations for complex decisions may not necessarily be communicable via one or a small number of saliency maps over the image pixels, this paper strives valiantly in this admittedly difficult direction.The experimentation is fairly rigorous, which is a welcome departure from and improvement on the norm for this type of paper. I hope such more quantitative evaluation will become more common in papers evaluating visual explanations.Cons:What about features that are very important but not linearly predictive on their own? This approach (and many others) would not work in that case; recognizing this, extending the an8Flower dataset to include such images and labels may be motivating for the field. For example, flowers where the class is determined not by a specific single color or feature (thorns or spots) but by the combination. In these cases, its not clear what the right answer would even be in the form of a saliency map, so the first task for researchers would be to determine in what format the answer should even be provided! So: less a benchmark than a motivating open question.Smaller notes:I found the presentation of the stride 1 resampling approach a little confusing. When performing the backward pass through the network from, say, layer 20, is the approach followed at every stride 2 layer on the way back? If so, I dont think I saw this mentioned. If not, wouldnt artifacts be introduced and compounded at any stride 2 layer during the backward pass? This paper presents a novel technique  (layer-adaptive magnitude based pruning, or LAMP) for pruning neural network weights (pruning can be beneficial in terms of overfitting prevention as well as other practical considerations).  LAMP evaluates weights in each layer in terms of the ratio of the magnitude of the weight to the sum of magnitudes of all surviving weights in the layer. The weight which evaluates as least important across all layers is pruned and then the process is repeated until the desired sparsity is achieved. The method is motivated theoretically as minimizing the distortion in the input/output mapping implemented by the weights of the layer. Experimental results on several benchmarks are presented.Pros:The experimental results are strong,  with LAMP consistently winning vs. competing techniques on 4 benchmarks problems. The method is elegant, requiring no hyperparameter tuning and minimal computation. The theoretical justification (mapping-distortion-minimization) makes a lot of sense. Cons:My only objection is that all of the experiments seem to be done on image datasets. It seems possible that deep learning networks applied to non-image data might not do as well under LAMP as they do for images. 'Under diverse datasets' in the abstract seems like an exaggeration. Further comments:I found a couple of typos.P3 while such unstrutured pruning -> while such unstructured pruningP5 Global on every layers -> on every layer Summary:The present paper introduces a new approach, deep orthogonal networks for unconfounded treatments (DONUT), that allows to estimate (average) treatment effects exploiting an orthogonality property implied by the classical unconfoundedness assumption. The authors propose a regularization framework based on the orthogonality constraint and prove that a resulting estimator is doubly robust, asymptotically normal and with efficient variance. They supply multiple simulations to demonstrate their theoretical claims and to show state-of-the-art performance of their estimator.Recommendation:Clear accept. In summary, I am convinced that the this paper would be a valuable addition to this year's conference. It considers a novel approach to improve average treatment effect estimation on observational data using combining classical causal inference assumptions and predictive power of deep learning.Strong points: - The authors propose a new methodology that seems theoretically solid and that has an implementable estimator for ATE estimation, exploiting a necessary condition implied by a standard causal inference assumption. - The article is well written and easy to read.Weak points: - The code for their simulations is not accessible (broken/incorrect url?). - A discussion about the impact of the hyperparameters, especially the orthogonality regularization parameter $\lambda$, would give more insight into the importance of the contribution of the regularization term.Questions/Issues: - The provided url to access the anonymized code did not work (at least for me), would it be possible to fix this or to provide the complete code as supplementary zip file? - How sensitive are the results to the hyperparameter choices, especially the $\lambda$ parameter? - The orthogonality constraint is implied by the unconfoundedness assumption, but it is not a sufficient condition for unconfoundedness. Have the authors studied the behaviour of their method and its performance in (simulated) cases where unconfoundedness (4) does not hold but the orthogonality constraint (5) holds? - The authors mention the R-learner (Nie \& Wager, 2017) which uses the notion of Neyman orthogonality and use the R decomposition to propose an estimator of treatment effects. Would it be possible to add this method to the list of compared methods? - Since the authors theoretically compare their estimator to the IPW estimator, it would be interesting to add this to the experiments to confirm the theoretical results. - For the ACIC datasets, how does DONUT compare to BART, which is known to perform well on these data (Dorie et al., 2018)?Minor comments (that did not impact the score): - p. 2/3: equations (2) and (4) are the same. - p. 5: yield by our $>>$ yielded by ourReferences: - Dorie V, Hill J, Shalit U, Scott M, Cervone D. Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition. Statistical Science. 2019;34(1):43--68. - Nie X, Wager S. Quasi-oracle estimation of heterogeneous treatment effects. arXiv preprint arXiv:1712.04912, 2017. This paper presents an interesting idea of using random graphs to represent relational structures amongst contextual samples and between contextual samples and target samples. Besides, the authors propose a regularization objective to alleviate catastrophic forgetting. The novelty mainly comes from the use of graph structure to preserve the memory of previous tasks. The results on public datasets are encouraging. Also, the authors show some correspondence between the learned graphs and underlying (clustering) structure of data. It would be great if the authors can also extend this algorithms to other real-world datasets, e.g., capturing the climate or environmental changes over time, and provide more interpretation of learned graph structrure.  This paper proposes an analysis of the benefits when using counterfactually augmented data (CAD) for classification purposes. The proposed framework is causal inference and DAGs. They focus on the problem of spurious correlations and how they affect out-domain generalization.They study a simple scenario with linear models and show that in this scenario, when using CAD,  spurious correlations are no longer used by classifiers. These qualitative insights as well as framing the problem in a causal model are the main contribution of the paper. This is not a minor contribution: up to now the discussion about the benefits of CAD where only supported by empirical findings. Framing the problem, even if considering simplified models, is a critical step.I appreciate the dual causal model that is studied (causal and "anticausal"), not limiting the analysis to the most common interpretation. They show that both models mean to the same conclusions. The paper will be more informative with a discussion about the noise model in the case of the noisy proxy (pg. 4). Is this the most probable noise model? Are there alternatives?The paper presents a large series of experiments to support the proposed hypothesis. Experiments are well designed and results are convincing. The paper opens a full line of research, especially if we consider the generalization of these results to other related domains. A method is presented to modify a music recording so that it sounds like it was performed by a different (set of) instrument(s). This task is referred to as "music translation". To this end, an autoencoder model is constructed, where the decoder is autoregressive (WaveNet-style) and domain-specific, and the encoder is shared across all domains and trained with an adversarial "domain confusion loss". The latter helps the encoder to produce a domain-agnostic intermediate representation of the audio.Based on the provided samples, the translation is often imperfect: the original timbre often "leaks" into the output. This is most clearly audible when translating piano to strings: the percussive onsets of the piano (due to the hammers hitting the strings) are also present in the translated audio, even though instruments like the violin and the cello are not supposed to produce percussive onsets. This gives the result an unusual sound, which can be interesting from an artistic point of view, but it is undesirable in the context of the original goal of the paper.Nevertheless, the results are quite impressive and for some combinations of instruments/styles it works surprisingly well. The question of whether the approach is equivalent to pitch estimation followed by rendering with a different instrument is also addressed in the paper, which I appreciate.The paper is well written and the related work section is comprehensive. The experimental evaluation is thorough and extensive as well (although a few potentially interesting experiments seemingly didn't make the cut, see other comments). I also like that the authors went through the trouble of doing some experiments on a publicly available dataset, to facilitate reproduction and future comparison experiments.Other comments:* "autoregressive" should be one word everywhere* In section 2 it is stated that attempts to use a unified decoder with style/instrument conditioning all failed. I'm curious about what was tried specifically, it would be nice to discuss this.* The same goes for experiments based on VQ-VAE, the paper simply states that they were not able to get this working, but not what experiments were run to come to this conclusion.* The authors went through the trouble to modify the nv-wavenet inference kernels to support their modified architecture, which I appreciate -- will the modified kernels be made available as well?* The audio augmentation by pitch shifting is a surprising ingredient (but according to the authors it is also crucial). Some more insight as to why this is so important (rather than simply stating that it is important) would be a welcome addition.* Section 3.2: "out off tune" should read "out of tune".* The formulation on p.7, 2nd paragraph is a bit confusing: "AMT freelancers tended to choose the same domain as the source, regardless of the real source and the presentation order." Does that mean they got it right every time? I suspect that is not what it means, but that is how I read it initially.* I don't quite understand the point of the semantic blending experiments. As a baseline, the same kind of blending in the raw audio space should be done, I suspect it would probably be hard to hear the difference. This is how cross-fading is already done in practice, and it isn't clear to me why this method would yield better results in that respect. The paper is strong enough without them so these could probably be left out. The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. Overall I really enjoy reading the paper. The assumptions to aid the proof are very natural and much softer than the existing literature. As far as Im concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. The presentation of the paper is intuitive and easy to follow. Ive also checked all the proof and think its brilliantly and elegantly written. My only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesnt influence my recommendation to accept the paper.Minor issues:I think its better to formally define bad local valley somewhere in the paper. From what I read, the definition of bad local valley is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. In proof number 4 (of Theorem 3.3), the statement should be any *principle* submatrices of negative semi-definite matrices are also NSD, and its not true otherwise. But this typo doesnt influence the proof. Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your bad local valley. It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations? This paper studies auto-encoders under several assumptions: (a) the auto-encoder's layers are fully connected, with random weights, (b) the auto-encoder is weight-tied, (c) the dimensions of the layers go to infinity with fixed ratios. The main contribution of the paper is to point out that this model of random autoencoder can be elegantly and rigorously analysed with one-dimensional equations. The idea is original and will probably lead to new directions of research. Already the first applications that the paper suggests are exciting.The paper does a good job in justifying assumptions (a), (b) and (c) in the introduction. It is convincing in the fact that this point of view may bring practical insights on training initialization for real-world autoencoders. Thus my opinion is that this paper brings original and significant ideas in the field.One flaw of this paper is that the writing might be clearer. For instance when presenting the technical theorem (Theorem 1), it would be useful to have an intuitive explanation for the theorem and the state-evolution-like equations. However, I believe that there are some easy fixes that would greatly improve the clarity of the exposition. Here is a list of suggestions: - In Section 2.1, a large number of notations are introduced. It would help a lot if the authors made a graphical representation of these. For instance, a diagram where every linearity / non-linearity is a box, and the different variables $x_l$, $\hat{x}_l$ appear would help a lot. - Section 2.2 is rather technical. The authors could try to give some more intuition of what's happening. For instance, they could spend more time after the theorem explaining what $\tau_l, \gamma_l$ and $\rho_l$ mean. They could also introduce the notation S_sig and S_var early and this section (and not in Section 3), because it helps interpreting the parameters. It would also help if they could write a heuristic derivation of the state-evolution-like equations. From the paper, the only way the reader can understand the intuition behind those complicated equations is to look at the proof of Theorem 1 (which is rather technical). - In Section 3.1, I did not understand the difference between interpretations 1 and 2. Could the authors clarify? - In Section 3.4, I did not understand the sentence: "In particular, near the phase transition of \gamma, S_sig/S_var = \Omega(\beta^{1.5}". If one uses the \Omega notation, it means that some parameter is converging to something. What is the parameter? As a consequence, I did not understand this paragraph. - In Section 3.5, the authors should make clear from the beginning why they are running those specific simulations. What hypothesis are they trying to check? I finally concluded that they are running simulations to check if the hypothesis they make in the first paragraph are true. They also want to compare with some other criteria in the literature, named EOC, that also gives insights about the trainability of the network. However, they could explicitly say in the beginning of the second paragraph that this is the goal.- In a similar spirit, the authors should end Section 3.5 with a clear conclusion on whether or not the framework enables us to predict the trainability of the autoencoder. Minor edits / remarks: - Typo: last but one paragraph of the introduction: "whose analysis is typically more straighforwards" -&gt; "straightforward".- At the end of Section 3.2: what can be proved about the behavior of \gamma / \sqrt{\rho}? It is obviously a central quantity and the authors do not say what happens in the phases where \gamma and \rho go to infinity for instance. Is it because it is hard to analyse? This paper presents a method that learns to reproduce 'block towers' from a given image. A perception model, a physics engine model, and a rendering engine are first trained together on pairs of images.The perception model predicts a representation of the scene decomposed into objects;  the physics engine predicts the object representation of a scene from an initial object representation; the rendering engine predicts an image given an object representation.Each training pair of images is made of the first image of a sequence when introducing an object into a scene, and of the last image of the sequence, after simulating the object's motion with a physics engine. The 3 parts of the pipeline (perception, physics, rendering) are trained together on this data.To validate the learned pipeline, it is used to recreate scenes from reference images, by trying to introduce objects in an empty scene until the given scene can be reproduced. It outperforms a related pipeline that lacks a scene representation based on objects.This is a very interesting paper, with new ideas:- The object-based scene representation makes a lot of sense, compared to the abstract representation used in recent work. - The training procedure, based on observing the result of an action, is interesting as the examples are easy to collect (except for the fact that the ground truth segmentation of the images is used as input, see below).However, there are several things that are swept 'under the carpet' in my opinion, and this should be fixed if the paper is accepted.* the input images are given in the form of a set of images, one image corresponding to the object segmentation. This is mentioned only once (briefly) in the middle of the paragraph for Section 2.1, while this should be mentioned in the introduction, as this makes the perception part easier. There is actually a comment in the discussion section and the authors promised to clarify this aspect, which should indeed be more detailed. For example, do the segments correspond to the full objects, or only the visible parts?* The training procedure is explained only in Section 4.1. Before reaching this part, the method remained very mysterious to me. The text in Section 4.1 should be moved much earlier in the paper, probably between current sections 2.3 and 2.4, and briefly explained in the introduction as well.This training procedure is in fact fully supervised - which is fine with me: Supervision makes learning 'safer'. What is nice here is that the training examples can be collected easily - even if the system was not running in a simulation.* if I understand correctly the planning procedure, it proceeds as follows:- sampling 'actions' that introduce 1 object at a time (?)- for each sampled action, predicting the scene representation after the action is performed, by simulating it with the learned pipeline, - keeping the action that generates a scene representation close to the scene representation computed for the goal image of the scene.- performing the selected action in a simulator, and iterate until the number of performed actions is the same as the number of objects (which is assumed to be known).-&gt; how do you compare the scene representation of the goal image and the predicted one before the scene is complete? Don't you need some robust distance instead of the MSE?-&gt; are the actions really sampled randomly?  How many actions do you need to sample for the examples given in the paper?I also have one question about the rendering engine:  Why using the weighted average of the object images? Why not using the intensity of the object with the smallest predicted depth?  It should generate sharper images. Does using the weighted average make the convergence easier? Significance:This article is a useful contribution to transfer learning for tasks where there is not enough data available, showing a modest improvement over the other methods that employ transfer learning in the classifier space.Novelty:The main contribution of this paper is the improvement of weak classifiers when there is not enough data for a class by combining the weak classifiers with the most relevant strong classifiers. This method finds k closest strong classifiers to the weak classifier and then combines the weak classifier with existing classifiers without creating new classifiers or networks from scratch.Potential Impact:The approach presented in this paper is well-evaluated in computer vision, but potentially useful in many other settings.Technical Quality:The technical content of the paper appears to be correct.Presentation/Clarity :The paper is generally well-written and structured clearly. While this method is a clear winner on Few classes, it is not performing as well in Medium classes, as shown in Table 1. An explanation about this issue could strengthen the paper.Reproducibility:The paper describes all the algorithms in full detail and provides enough information for an expert reader to reproduce its results. I would suggest the authors release their code on GitHub or other sites to help other researchers reproduce their results. #### Summary:The paper proposes a new simple, yet powerful and alternative method of editing the semantic attributes of images generated using pre-trained GAN models as well as a pre-trained regressors. The approach allows for the manipulation of single or multiple various image attributes, while preserving the identity of the original image in contrast to the baseline method of Shen et. al 2019. The method focuses on the manipulation of the latent space, in contrast to the popular image space editing methods. The paper is easy to read and understand. The authors have presented their method and analysis which are clear and understandable, supported quite well with the provided examples and figures.#### Strengths:Experiments are conducted on various datasets and compared to various methods - supervised and unsupervisedTheir method is able to consistently preserve the image content (in case of scenes) and identity (in case of faces) Showing inversion results along the transformation shows the robustness and tractable nature of their method.  #### Weakness:It is not entirely clear how the local transformation are discovered for different z values independently, or rather what the difference is in discovering the global transformation vs discovering the local transformation#### Questions to the Authors:1. In comparison with the Voynov & Babenko model, how many of the directions that their model was able to find did you look at?2. What is the accuracy / results of the performance metric of the different used pre-trained regressors? Given that the perceptual losses were used while training, the regressors performance values could shed some light in this direction.   I have to admit straight away that this paper is far from my field of expertise (computer vision, generative networks). I have not worked with CAD models, and I am not in expert in reinforcement/imitation learning. My review is thus written from the "educated outsider" viewpoint.From that viewpoint, the paper is very strong. It introduces a meaningful problem (CAD modeling sequence reconstruction), motivates the need for a new task and the direction of research, describes the full task and the compromises it makes (face extrudes rather than sketch extrudes are considered in the simplified task). The paper then introduces a new training/test dataset and an environment for training agents, and evaluates a reasonable set of agents rather extensively.As a slight criticism, I found that too many details are moved from Section 5 to the supmat. E.g. what is meant by MLP with"an auto-regressive connection between the two target faces" was completely unclear before I looked into the supmat (and the phrase did not refer me to supmat either).Otherwise, the paper is well-written and has nice visualizations (though they crash my PDF viewer) that aid understanding.Overall, I really enjoyed reading the paper, and I am not able to identify any flaws. As far as I can judge based on my limited knowledge, the paper (together with associated dataset and environment) is likely to spur new research and to be impactful. I therefore give it a strong rating, but I cannot exclude that I have missed some flaws that would be identifiable by an expert. This paper shows a formulation of regularized Markov Decision Processes (MDPs), which is slightly different from that of Geist et al. (2019). Then, the authors propose a novel inverse reinforcement learning under regularized MDPs. One of the contributions is that policy regularization considered here is more general than that of Yang et al. (2019). This paper is written very well and is of publishing quality. I think it is sufficiently significant to be accepted. Still, I have the following questions. 1. The proposed method is based on the relationship between imitation learning and statistical divergence minimization. If my understanding is correct, Bregman divergence plays a role in generalizing generalized adversarial imitation learning. However, as the authors mentioned in Section 6, Bregman divergence does not include f-divergence, which is also studied in imitation learning. Would you discuss the connection to the formulation using f-divergence in more detail?2. I am interested in the relationship between the proposed method and Lee et al. (NeurIPS2018). Is the proposed method nearly the same as Lee et al. (2018) when Tsallis entropy is selected as regularization? If not, does the proposed method outperform Lee et al. (2018) in the MuJoCo control tasks? 3. The authors claim that the solutions provided by Geist et al. (2019) are intractable in the Introduction. However, it is shown that the reward baseline term in Corollary 1 is intractable except for some well-studied setups. Does it imply that the proposed method faces the same difficulty when applied with arbitrary policy regularization?4. The experimental results shown in Figure 3 is interesting, but I have a few concerns. In some cases, the averaged Bregman divergence of RAIRL-NSM (\lambda = 1) was larger than that of Random. Would you show the example of the learned policy for the readers understanding? Besides, is the same policy regularization used in Behavior Cloning? Finally, are exp, cos, and sin the meaningful regularizer? 5. To derive the practical algorithms, the authors consider the same form of the policy regularization used by Yang et al. (2019), which is given by - \lambda E[\phi(\pi(a))]. Is it possible to derive the algorithm in which the regularizer is given by \Omega(\pi)? The authors find 2 issues with Adversarial Imitation Learning-style algorithms: I) implicit bias in the reward functions and II) despite abilities of coping with little data, high interaction with the environment is required. The authors suggest "Discriminator-Actor-Critic" - an off-policy Reinforcement Learning reducing complexity up to 10 and being unbiased, hence very flexible. Several standard tasks, a robotic, and a VR task are used to show-case the effectiveness by a working implementation in TensorFlow Eager.The paper is well written, and there is practically no criticism. This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos, Lei Xu, J. Zico Kolter:Input Convex Neural Networks. ICML 2017: 146-155Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. This paper introduces a novel meta-learning approach to unsupervised representation learning where an update rule for a base model (i.e., an MLP) is meta-learned using a supervised meta-objective (i.e., a few-shot linear regression from the learned representation to classification GTs). Unlike previous approaches, it meta-learns an update rule by directly optimizing the utility of the unsupervised representation using the meta-objective. In the phase of unsupervised representation learning, the learned update rule is used for optimizing a base model without using any other base model objective. Experimental evaluations on few-shot classification demonstrate its generalization performance over different base architectures, datasets, and even domains.  +  Novel and interesting formulation of meta-learning by learning an unsupervised update rule for representation learning. +  Technically sound, and well organized overall with details documented in appendixes. +  Clearly written overall with helpful schematic illustrations and, in particular, a good survey of related work. + Good generalization performance over different (larger and deeper) base models, activation functions, datasets, and even a different modality (text classification).-  Motivations are not very clear in some parts. E.g., the reason for learning backward weights (V), and the choice of meta-objective.  - Experimental evaluation is limited to few-shot classification, which is very close to the meta-learning objective used in this paper. - The result of text classification is interesting, but not so informative given no further analysis. E.g., why domain mismatch does not occur in this case?I enjoyed reading this paper, and happy to recommend it as a clear accept paper. The idea of meta-learning update networks looks a promising direction worth exploring, indeed. I hope the authors to clarify the things I mentioned above. Experimental results are enough considering the space limit, but not great. Since the current evaluation task is quite similar to the meta-objective, evaluations on more diverse tasks would strengthen this paper. Finally, this paper aims at unsupervised representation learning, but its not clear from the current title, which is somewhat misleading. I think that's quite an important feature of this paper, so I highly recommend the authors to consider a more informative title, e.g., `Learning Rules for Unsupervised Representation Learning or else. The paper describes unsupervised learning as a meta-learning problem: the observation is that unsupervised learning rules are effectively supervised by the quality of the representations that they yield relative to subsequent later semi-supervised (or RL) learning. The learning-to-learning algorithm allows for learning network architecture parameters, and also 'network-in-networks' that determine the unsupervised learning signal based on pre and post activations. Quality The proposed algorithm is well defined, and it is compared against relevant competing algorithms on relevant problems. The results show that the algorithm is competitive with other approaches like VAE (very slightly outperforms).ClarityThe paper is well written and clearly structured. The section 5.4 is a bit hard to understand, with very very small images. OriginalityThere is an extensive literature on meta-learning, which is expanded upon in Appendix A. The main innovation in this work is the parametric update rule for outer loop updates, which does have some similarity to the old work by Bengio in 1990 and 1992. Significance- pros clear and seemingly state-of-the-art results, intuitive approach, -cons only very modestly better than other methods. I would like to get a feel for why VAE is so good tbh (though the authors show that VAE has a problem with objective function mismatch).One comment: the update rule takes as inputs pre and post activity and a backpropagated error; it seems natural to also use the local gradient of the neuron's transfer function here, as many three or four factor learning rules do. This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The authors argue that binarising RNNs is due to a covariate shift, and address it with stochastic quantised weights and batch normalisation.The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs, with almost no loss in test performance.Based on the more efficient RNN cell, the authors furthermore describe a more efficient hardware implementation, compared to an implementation of the full-precision RNN.The core message I took away from this work is: One can get away with stochastic binarised weights in a forward pass by compensating for it with batch normalisation.Strengths:- substantial number of experiments (6 datasets), different domains- surprisingly simple methodological fix - substantial literature review- it has been argued that char-level / pixel-level RNNs present somewhat artificial tasks  even better that the authors test for a more realistic RNN application (Reading Comprehension) with an actually previously published model.Weaknesses:- little understanding is provided into _why_ covariance shift occurs/ why batch normalisation is so useful. The method works, but the authors could elaborate more on this, given that this is the core argument motivating the chosen method.- some statements are too bold/vague , e.g. page 3: a binary/ternary model that can perform all temporal tasks- unclear: by adapting a probabilistic formulation / sampling quantised weights, some variance is introduced. Does it matter for predictions (which should now also be stochastic)? How large is this variance? Even if negligible, it is not obvious and should be addressed.Other Questions / Comments-  How dependent is the method on the batch size chosen? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var. What happens at batch size 1? Are predictions of poorer for smaller batches?- Section 2, second line  detail: case w_{i,j}=0 is not covered- equation (5): total probability mass does not add up to 1- a direct comparison with models from previous work would have been interesting, where these previous methods also rely on batch normalisation- as I understand, the main contribution is in the inference (forward pass), not in training. It is somewhat misleading when the authors speak about the proposed training algorithm or we introduced a training algorithm- unclear: last sentence before section 6. The paper proposes a method for learning regression models through evolutionaryalgorithms that promise to be more interpretable than other models whileachieving similar or higher performance. The authors evaluate their approach on99 datasets from OpenML, demonstrating very promising performance.The authors take a very interesting approach to modeling regression problems byconstructing complex algebraic expressions from simple building blocks withgenetic programming. In particular, they aim to keep the constructed expressionas small as possible to be able to interpret it easier. The evaluation isthorough and convincing, demonstrating very good results.The presented results show that the new method beats the performance of existingmethods; however, as only very limited hyperparameter tuning for the othermethods was performed, it is unclear to what extent this will hold true ingeneral. As the main focus of the paper is on the increased interpretability ofthe learned models, this is only a minor flaw though.The interpretability of the final models is measured in terms of their size.While this is a reasonable proxy that is easy to measure, the question remainsto what extent the models are really interpretable by humans. This is definitelysomething that should be explored in future work, as a small-size model does notnecessarily imply that humans can understand it easily, especially as thegenerated algebraic expressions can be complex even for small trees.The description of the proposed method could be improved; in particular it wasunclear to this reviewer why the features needed to be differentiable and whatthe benefit of this was (i.e. why was this the most appropriate way of adjustingweights).In summary, the paper should be accepted. This paper proposes an efficient algorithm to learn  neural embedding models with a dot-product structure over very large corpora. The main method is to reformulate the objective function in terms of generalized Gramiam matrices, and maintain estimates of those matrices in the training process. The algorithm uses less time and achieves significantly better quality than sampling based methods. 1. About the experiments, it seems the sample size for sampling based experiments is not discussed. The number of noise samples have a large influence on the performance of the models. In figure 2, different sampling strategies are discussed. It would be cool if we can also see how the sampling size affects the estimation error. 2. If we just look at the sampling based methods, in figure 2a, uniform samplings Gramian estimates is the worst. But the MAP of uniform sampling on validation set for all three datasets are not the worst. Do you have any comments?3. wheter an edge -&gt; whether an edge. Summary:The paper considers the problem of learning from logged bandit feedback, and focuses on the problem of the ratio of the target policy and the logged policy (the basis of algorithms such as inverse propensity scoring). The paper proposes a surrogate policy to replace the logged policy with known parametrization, with a policy obtained by maximum likelihood estimation on the observed data. The authors present theoretical arguments that the variance of the value function estimate is reduced. Empirical experiments show that the surrogate policy can be used to improve IPS and POEM, and also works when the logging policy is unknown.The paper analyses an important and interesting problem which is critical to many practical applications today. The proposed solution is modular, and the empirical experiments point to its usefulness. The theoretical analysis, while not fully explaining the proposed approach, provides comfort that there is reduced variance when using the maximum likelihood surrogate.Overall comments:- page 3, Section 3: It is unclear why the assumption that we know the logging policy, as well as its optimal parameter is a sensible one. In particular, the first paragraph seems to indicate that the surrogate policy some somehow the same parameterization and $\hat{\beta}$ is in the same space as $\beta^*$, and just a different parameter. On one hand the authors seem to indicate that they know everything about the logging. On the other hand they seem to want to claim that not knowing the logging policy is ok. What happens when there is a model mismatch between the logging policy and the surrogate policy? Please expand on these two assumptions.- page 4, Section 3.1: It might be useful to have a toy example which exactly matches the requirements of Theorem 3.9, such that you can present empirical intuition about the terms in (3.13). In particular: what is the effect of assuming a deterministic reward? How does (3.14) grow? Why is the reduction of MSE greater than $\xi(n)$?- Theorem 3.9: Please present the result that MLIPS is asympotically unbiased explicitly. Furthermore, the current proof of this main theorem should be structured better, so that it can be properly checked.Minor issues/typos:- page 3, above (3.1): In specific, we --&gt; In particular, we- Figure 1: the legend is very confusing, making it totally unclear what the text is talking about. Please match text, caption and legend.- Section 4.3: please say that the data is the multilabel datasets of Swaminathan and Joachims in Table 1. This paper explores the idea of having a VAE modelling the probability distribution of the real segmentations, in order to quantify the quality of the predicted segmentation (using another network). The paper refines this idea by applying regression over two parameters. The overall idea is interesting and novel to the best of my knowledge. Experimental results look convincing.The paper does a good job at presenting the motivation, reads well in general, and it is well written (except the paragraph Entropy Uncertainty in Sec. 4.2 which contains several typos).Some comments:S(F(X); ¸) looks good enough as an estimator. It would be good to see how it does by itself, reporting that as an ablation experiment, assessing how important it is to carry out the second step (fitting a, b).In the last paragraph of Sec. 2, I am not sure what it is meant by "Variational autoencoder(VAE) (Kingma &amp; Welling, 2013), compared with AE, has stronger representation capability and can also serve as a generative model". No doubt about the latter point, but not sure about the former.Sec. 3.3 is somewhat confusing, for example: what is E in eq. 9 should be L? Summary:This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.- Quality: Several interesting differences to prior work. Well-done experiments- Clarity: Clean derivation, easy to understand. Some details could be spelled out better- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to "neural processes" work, but this happened roughly at the same time- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensiveInteresting about this work:- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether   an inference network of this simple structure (no correlations, sum combination   of datapoints, same network for each class) can deliver a good approximation to   the true posterior.- Different to previous work, task-specific inference is done only on the weights of   single-layer head models (logistic regression models, with shared features).   Highly encouraging that this is sufficient for state-of-the-art few-shot classification   performance. The authors could be more clear about this point.- Simple and efficient amortized inference model, which along with the neural   network features, is learned on all data jointly- Optimization criterion is based on predictive distributions on train/test splits, not   on the log marginal likelihood. Has some odd consequences (question below),   but clearly works better for few-shot classificationExperiments:- 5.1: Convincing results, in particular given the simplicity of the model setup and   the inference network. But some important points are not explained:   - Which of the competitors (if any) use the same restricted model setup (inference      only on the top-layer weights)? Clearly, MAML does not, right? Please state this      explicitly.   - For Versa, you use k_c training and 15 test points per task update during      training. Do competitors without train/test split also get k_c + 15 points, or      only k_c points? The former would be fair, the latter not so much.- 5.2: This seems a challenging problem, and both your numbers and reconstructions   look better than the competitor. I cannot say more, based on the very brief   explanations provided here.   The main paper does not really state what the model or the likelihood is. From   F.4 in the Appendix, this model does not have the form of your classification   models, but psi is input at the bottom of the network. Also, the final layer has   sigmoid activation. What likelihood do you use?   One observation: If you used the same "inference on final layer weights" setup   here, and Gaussian likelihood, you could compute the posterior over psi in closed   form, no amortization needed. Would this setup apply to your problem?Further questions:- Confused about the input to the inference network. Real Bayesian inference would   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in   then?   Please do improve the description of the inference network, this is a major   novelty of this paper, and even the appendix is only understandable by reading   other work as well. Be clear how it depends on theta (I think nothing is lost by   feeding in the h_theta(x)).- The learning criterion based on predictive distributions on train/test splits seem   to work better than ELBO-like criteria, for few-shot classification.   But there are some worrying aspects. The marginal likelihood has an Occam's   razor argument to prevent overfitting. Why would your criterion prevent overfitting?   And it is quite worrying that the prior p(psi | theta) drops out of the method   entirely. Can you comment more on that?Small:- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more   general notation early on, if you do not do it later on. This is confusing Summary:This paper is built on the top of DNC model. Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing. Authors propose changes in the network architecture to solve all these three issues. With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC. The improvements are also seen in more realistic bAbI tasks.Major Comments:The paper is well written and easy to follow. The proposed improvements seem to result in very clear improvements. The proposed improvements also improve the convergence of the model. I do not have any major concerns about the paper. I think that contributions of the paper are good enough to accept the paper.I also appreciate that the authors have submitted the code to reproduce the results.I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications? This paper studies a Mixed Integer Linear Programming (MILP) approach to verifying the robustness of neural networks with ReLU activations. The main contribution of the paper is a progressive bound tightening approach that results in significantly faster MILP solving. This in turn allows for verifying the robustness of larger networks than previously studied, and even larger datasets such as CIFAR-10.This paper is a solid contribution and should be accepted to ICLR. It is quite well-written, addresses an important problem using a principled method, and achieves strong experimental results that were previously elusive, despite the large body of work in adversarial learning. In particular, the paper has the following strengths:- Clarity: the paper is well-written and easy to read. Tables, figures and pseudocode are nice and easy to understand.- Methodology: the authors take care of a number of bottlenecks in the scalability of MIP solvers for the verification problem. This is the standard approach in the Operations Research (OR) community, and I am really glad to see it in an ICLR submission!- Results: the efficiency of the MIP on the tightened model, and the improvements in the bounds on the adversarial error as compared to very recent methods from the literature are both very strong points in favor of the paper.I do not have any further questions for the authors - good job! This paper considers augmenting the cross-entropy objective with "complement" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. The paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting.One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved. # Positive aspects of this submission- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.- The methodology in Sections 4 and 5 is very thorough and useful.- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.# Criticism- In Section 3, even if the "l1 + projection" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).Similarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.Given that you also study "o-greedy-hit" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency. The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines.Overall, it is a high-quality paper. There are only some minor comments for improving the paper:½Page 6, there is a typo. for node v by employing &  should be for node u½Page 6, Both GAT and GaAN has   should be  Both GAT and GaAN have½In section 5.1, it will be great if authors can explain more what are the association events and communication events with more details in these two evaluation datasets. This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck.These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident).The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier.This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks.[1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf[2] https://arxiv.org/abs/1312.6114[3] https://arxiv.org/abs/1801.02774 This paper provides a visualization framework to understand the generative neural network in GAN models. To achieve this, they first find a group of interpretable units and then quantify the causal effect of interpretable units. Finally, the contextual relationship between these units and their surrounding is examined by inserting the discovered object concepts into new images. Extensive experiments are presented and a video is provided.Overall, I think this paper is very valuable and well-written. The experiments clearly show the questions proposed in the introduction are answered. Two concerns are as follows.Cons:1) The visualization seems to be very heuristic. What I want to know is the theoretical interpretation of the visualization. For example, the Class Activation Maps (CAM) can be directly calculated by the output values of softmax function. How about the visual class for the generative neural networks?2) I am also very curious, how is the rate of finding the correct sets of units for a particular visual class? This paper presents a PAC-Bayesian framework that bounds the generalization error of the learned model. While PAC-Bayesian bounds have been studied before, the focus of this paper is to study how different conditions in the network (e.g. behavior of activations) generalize from training set to the distribution. This is important since prior work have not been able to handle this issue properly and as a consequence, previous bounds are either on the networks with perturbed weights or with unrealistic assumptions on the behavior of the network for any input in the domain.I think the paper could have been written more clearly. I had a hard time following the arguments in the paper. For example, I had to start reading from the Appendix to understand what is going on and found the appendix more helpful than the main text. Moreover, the constraints should be discussed more clearly and verified through experiments.I see Constraint 2 as a major shortcoming of the paper. The promise of the paper was to avoid making assumptions on the input domain (one of the drawbacks in Neyshabur et al 2018) but the constraint 2 is on any input in the domain. In my view, this makes the result less interesting.Finally, as authors mention themselves, I think conditions in Theorem F.1 (the label should be 4.1 since it is in Section 4) could be improved with more work. More specifically, it seems that the condition on the pre-activation value can be improved by rebalancing using the positive homogeneity of ReLU activations.Overall, while I find the motivation and the approach interesting, I think this is not a complete piece of work and it can be improved significantly.===========Update: Authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.  I recommend accepting this paper. The paper proposes a method to check if a given point is a stationary point or not (if not, it provides a descent direction), and then classify stationary points as either local min or second-order stationary. The method works for a specific non-differentiable loss.  In the worst case, there can be exponentially many flat directions to check (2^L), but usually this is no the case.Overall, I'm impressed. The analysis seems solid, and a lot of clever ideas are used to get around issues (such as exponential number of regions, and non-convex QPs that cannot be solved by the S-procedure or simple tricks). A wide-variety of techniques are used: non-smooth analysis, recent analysis of non-convex QPs, copositive optimization.The writing is clear and makes most arguments easy to follow.There are some limitations:(1) the technical details are hard to follow, and most are in a lengthy appendix, which I did not check(2) there was no discussion of robustness. If I find a direction eta for which the directional derivative is zero, what do you mean by "zero"? This is implemented on a computer, so we don't really expect to find a directional derivative that is exactly zero.  I would have liked to see some discussions with epsilons, and give me a guarantee of an epsilon-SOSP or some kind of notion.  In the experiments, this isn't discussed (though another issue is touched on a little bit: you wanted to find real stationary points to test, but you don't have exactly stationary points, but rather can get arbitrarily close).  To make this practical, I think you need a robust theory.(3) The numerical simulations mainly provided some evidence that there are usually not too many flat directions, but don't convince us that this is a useful technique on a real problem.  The discussion about possible loss functions at the end was a bit opaque.  Furthermore, if you can't find a dataset/loss, then why is this technique useful?The paper is interesting and novel enough that despite the limitations, I am supportive of publishing it. It introduces new ideas that I find refreshing. The technique many not ever make it into the state-of-the-art algorithms, but I think the paper has intellectual value regardless of practical value.In short, quality = high, clarity=high, originality=very high, and significance=hard-to-predict This paper proposes fine-tuning an out-of-distribution detector using an Outlier Exposure (OE) dataset. The novelty is in proposing a model-specific rather than dataset-specific fine-tuning. Their modifications are referred to as Outlier Exposure. OE includes the choice of an OE dataset for fine-tuning and a regularization term evaluated on the OE dataset. It is a comprehensive study that explores multiple datasets and improves dataset-specific baselines.Suggestions and clarification requests:- The structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works. I suggest moving the details of model-specific OE regularization terms to section 3 and review the details of the baseline models. Then present the other set of novelties in proposing OE datasets in a new section before presenting the results. Clearly presenting two sets of novelties in this work and then the results. If constrained in space, I suggest squeezing the discussion, conclusion, and 4.1.- In the related work section Radford et al., 2016 is references when mentioning GAN. Why not the original reference for GAN?- Maybe define BPP, BPC, and BPW in the paragraphs on PixelCNN++ and language modeling or add a reference.- Numbers in Table 3 column MSP should match the numbers in Table 1, right? Or am I missing something? Summary:The authors propose to apply the Deep Variational Information Bottleneck (VIB) method of [1] on discriminator networks in various adversarial-learning-based scenarios. They propose a way to adaptively update the value for the bêta hyper-parameter to respect the constraint on I(X,Z). Their technique is shown to stabilize/allow training when P_g and P_data do not overlap, similarly to WGAN and gradient-penalty based approaches, by essentially pushing their representation distributions (p_z) to overlap with the mutual information bottleneck. It can also be considered as an adaptive version of instance noise, which serves the same goal. The method is evaluated on different adversarial learning setup (imitation learning, inverse reinforcement learning and GANs), where it compares positively to most related methods. Best results for classical adversarial learning for image generation are however obtained when combining the proposed VIB with gradient penalty (which outperforms by itself the VGAN in this case).Pros :- This paper brings a good amount of evidence of the benefits to use the VIB formulation to adversarial learning by first showing the effect of such approach on a toy example, and then applying it to more complex scenarios, where it also boosts performance. The numerous experiments and analyses have great value and are a necessity as this paper mostly applies the VIB to new learning challenges. - The proposition of a principled way of adaptively varying the value of Bêta to actually respect more closely the constraint I(X,Z) &lt; I_c, which to my knowledge [1] does not perform, is definitely appealing and seems to work better than fixed Bêtas and does also bring the KL divergence to the desired I_c.- The technique is fairly simple to implement and can be combined with other stabilization techniques such as gradient penalties on the discriminator.Cons:- In my view, the novelty of the approach is somewhat limited, as it seems like a straightforward application of the VIB from [1] for discriminators in adversarial learning, with the difference of using an adaptive Bêta.- I think the Bêta-VAE [2] paper is definitely related to this paper and to the paper on which it is based [1] and should thus be cited as the authors use a similar regularization technique, albeit from a different perspective, that restricts I(X,Z) in an auto-encoding task.- I think the content of batches used to regularize E(z|x) w.r.t. to the KL divergence should be clarified, as the description of p^tilde being a mixture of the target distribution and the generator (Section 4) can let the implementation details be ambiguous. I think batches containing samples from both distributions can cause problems as the expectation of the KL divergence on a batch can be low even if the samples from both distributions are projected into different parts of the manifold. This makes me think batches are separated? Either way, this should be more clearly stated in the text.- The last results for  the traditional GAN+VIB show that in this case, gradient penalty (GP) alone outperforms the proposed VGAN, and that both can be combined for best results. I thus wonder if the results in all other experiments could show similar trends if GP had been tested in these cases as well. In the imitation learning task, authors compare with instance noise, but not with GP, which for me are both related to VIB in what they try to accomplish. Was GP tested in Imitation Learning/Inverse RL ? Was it better? Could it still be combined with VIB for better results? - In the saliency map of Figure 5, Im unclear as to what the colors represent (especially on the GAIL side). I doubt that this is simply due to the colormap used, but this colormap should be presented.Overall, I think this is an interesting and relevant paper that I am very likely to suggest to peers working on adversarial learning, and should therefore be presented. I think the limited novelty is counterbalanced by the quality of empirical analysis. Some clarity issues and missing citations should be easy to correct. I appreciate the comparison and combination with a competitive method (Gradient Penalty) in Section 5.3, but I wish similar results were present in the other experiments, in order to inform readers if, in these cases as well, combining VIB with GP leads to the best performance.[1] Deep Variational Information Bottleneck, (Alemi et al. 2017)[2] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al. 2017) The paper describes a method to improve reinforcement learning for task with sparse rewards signals.The basic idea is to select the best episodes from the system's experience, and learn to imitate them step by step as the system evolves, aiming at providing a less sparse learning signal.The math works out to a gradient that is of similar form as a policy gradient, which makes it easy to interpolate both of them. The resulting training procedure is a policy gradient that gets additional reinforcement of the system's best runs.The experiments show the validity especially for the most extreme case (episodic rewards), while, as expected, for the other extreme of dense rewards, the method's effect is not consistently positive.The paper then critiques its own method and identifies a critical weakness: the reliance on good exploration. I like that a lot. The paper goes on to suggest an extension to address this by training an ensemble, and shows the effectiveness of this for a number of tasks. However, I feel that the description of this extension is less clear than that of the core idea, and introduces too many new ideas and concepts in a too condensed text.The paper seems a significant in that it provides a notable improvement for sparse-rewards tasks, which are a common sub-class of real-world problems.My background is not RL. While I am quite confident in my understanding of the paper's math, I am not 100% familiar with the typical benchmark sets. Hence, I cannot judge whether the results include good baselines, or whether the task selection is biased. I can also not judge the completeness of the related work, and how novel the work is. For these questions, I hope that the other reviewers can provide more information.Pros: - intuitive idea for a common problem - solution elegantly has the form of a modified policy gradient - convincing experimental results - self-critique of core idea, and extension to address its main weakness - nicely written text, does not leave a lot of questionsCons: - while the core idea is nicely motivated and described and good to follow, Section 2.3 feels very dense and too short.Overall, I find the core idea quite intuitive and elegant. The paper's background, motivation, and core method are well-written and, with some effort, quite readable for someone who is not an RL expert. I found that several questions I had during reading were preempted promptly and addressed. However, the description of the secondary method (Section 2.3) is too dense.To me, the paper solidly meets the threshold of publication. Since I have no good comparison to other papers, I rate it a "clear accept" (8).Minor points:I noticed a few superfluous "the", please double-check.In Table 1, please use the same exponent for directly comparable numbers, e.g. instead of "1.8e5 4.4e4", say "18e4 4.4e4". Or best just print the full numbers without exponent, I think you have the space.When reading Table 1, I could bnot immediately line up "PPO" and "Self-imitation" in the caption with the table columns. It took a while to infer that PPO refers to \nu=0, and SI to \nu=0.8. Can you add PPO and SI to the table headings?You define p as "the masking probability", but it is not clear whether that is the probability for keeping a "1" in the mask,or for masking out the value. I can only guess from the results. I suggest to rephrase as "the probability of retaining a reward". Also, how about using plain words in Table 1's heading, such as "Noisy rewards\nSuppressing 10% of rewards", so that one can understand the table without having to search for its description in the text? Overall impression: I think that this is a well written interesting paper with strong results. One thing Id have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? Ive suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. Abstract:We demonstrate its effectiveness on a number of challenging tasks. -&gt; be more specific.The term single-timestep optimization is not very clear. Can this be clarified?they are more widely applicable in the sparse or episodic reward settings -&gt; it is likely important to mention that they are agnostic to horizon of the task.Related works: Guided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. we learn shaped, dense rewards-&gt; too early in the paper for this to make sense. can provide some contexttSection 2.2:fully decides the expected return -&gt; clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit.Small typos in appendix 5.1 (r should be replaced by the density ratio)The update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from self experiences. How is the priority list threshold and size chosen?(Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards?Appendices are very clear and very informative while being succinct!I would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm What is psi in appendix 5.3? The algorithm remains a bit unclear without this clarificationExperiments. Only 1 question to answer in this section is labelled? Put 2) and 3) appropriately. Can a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes?Can the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. This paper proposes a new approach to learning quantized deep neural networks, which overcome some of the drawbacks of previous methods, namely the lack of understanding of why straight-through gradient works and its optimization instability. The core of the proposal is the use of quantization-encouraging regularization, and the derivation of the corresponding proximity operators. Building on that core, the rest of the approach is reasonably standard, based on stochastic proximal gradient descent, with a homotopy scheme.The experiments on benchmark datasets provide clear evidence that the proposed method doesn't suffer from the drawbacks of  straight-through gradient, does contributing to the state-of-the-art of this class of methods. The paper argues that identifying weak supervision signals may require domain expert knowledge and creativity. Therefore, the authors propose an interactive weak supervision solution, in which the annotators assess the quality of automatically identified labelling functions. The proposed solution shows superior performance on a number of classification benchmarks after some minimum number of iterations.Strengths:1. The paper studies an important and novel problem in the domain of weak supervision2. The proposed solution outperforms the baselines3. The authors show some promising results by running experiments with real human annotators.4. The paper is well written and easy to follow.Weaknesses:1. A large body of work on weak supervision has been completely ignored by the authors. These are some relevant papers:- Using weak supervision beyond classification (for ranking): https://dl.acm.org/doi/10.1145/3077136.3080832- Learning from multiple weak supervision signals: https://dl.acm.org/doi/10.1145/3209978.3210041- Some theoretical justifications for weak supervision training (for ranking): https://dl.acm.org/doi/10.1145/3234944.32349682. Fidelity-weighted learning (ICLR 2018) was proposed to automatically identify the quality of weak supervision signals. It is different from the proposed solution in the sense that a small set of labeled instances are used to assess the quality of weak supervision signals as opposed to labeling the weak supervision functions themselves. However, the goal is the same and some comparison (or at least some discussions) are necessary for the paper, I believe.3. There is no discussion on going beyond classification tasks and even some extreme classification scenarios.4. I think there should be some difficulty estimation associated with each weak supervision annotation function. Some functions may not be easily assessed by human annotators and this may influence the model. I think this can be addressed in future work. Summary:-------------This work presents first of a kind logic-based framework that relates contrastive (minimally absent) and abductive (minimally present) explanations and shows that abductive explanations are essentially minimal hitting sets of contrastive explanations. +ve:-----A much needed formal framework and proofs that relate the different types of explanations. With many different types of explanations proposed in the XAI literature, this line of work improves are understanding of the overall space & taxonomy of explanations. Also the relationship between types of explanations may helps us enumerate/compute other types of explanations based on the computation of one type. Suggestions:-------------------It would be good to comment a bit on the overall steps needed to convert any ML problem into the proposed framework - costs of binarizing feature values, etc. before applying the proposed ideas for local or global explanations.  This paper proposed a transfer approach for reinforcement learning. The proposed approach leverages a policy pre-trained via Never Give Up (NGU) approach, and can facilitate learning challenging RL tasks including the ones with sparse reward. This paper presents many strong pieces of evidence that this approach can be used to tackle challenging RL problems including hard exploration and multi-task learning. Strengths* Necessity of transferring behavior is well motivated and experiment showed its strength in challenging RL benchmarks.* Figure 2 is useful to have intuition on how the proposed transfer method can be useful* Table 1 set a strong unsupervised RL performance for Atari Suite* Ablation study in Figure 3 is thought-provoking. It is interesting that the significant gain of pretraining come only when both exploitation and exploration method are used jointly.* Figure 4 provides a useful intuition that more pretraining is beneficial for transfer to hard exploration task.Weaknesses* The "flights" technique is not described in detail in the main text. I managed to find the detail in Appendix A, but the pointer does not exist in the main text.* The paper claim "coverage" as the desired objective for RL pretraining and tried to support this claim by showing the transfer performance after pretraining via Never Give Up (NGU). I am convinced that NGU is a good pre-training objective but, it is not clear whether a more general claim for "coverage" is supported as well. It is not clear whether NGU is optimizing "coverage" well, and the relation between "coverage" and transfer performance is not studied. Comment / Questions to author* "but little research has been conducted towards leveraging the acquired knowledge once the agent is exposed to extrinsic reward": I'm not sure whether I agree with this description. My understanding is that (Burda et al., 2018) studied a setting where an intrinsic reward is jointly used with extrinsic rewards. The only difference with this work is that the previous work did not study a setting with a clear separation between "pre-training" and "transfer".* Is there a difference between "ez-greedy with expended action set A+ (using pre-trained policy)" vs "the proposed transfer method (exploitation + exploration)"?* I'm curious about the comparison between CPT vs joint training with extrinsic reward. How authors would compare CPT vs joint training?RecommendationI recommend accepting this paper because this paper presented strong evidence that unsupervised pre-training and transfer may be a powerful approach to solve many challenging RL problems. I believe this observation is likely to catalyze future research of the related approaches, and the proposed method itself may be used for different domains to improve the capability of RL in general. ### SummaryThis paper introduces a novel application of normalizing flows to speech synthesis, allowing direct optimization of spectrogram log-likelihoods which results in more natural variation at inference compared to L1/L2 losses that model the mean. This setup also allows more control over non-textual information and interpolation between samples and styles.### Recommendation**Accept**The idea is good and experiments are good. There are some concerns about the clarity of the paper but those can be worked on.### Positives1. The paper introduces a novel architecture and demonstrates improved output variation and more controllability, which is an important current issue for TTS research.1. Many experiments investigating controllability are described, as well as some ablation studies for the model architecture in the appendix.### Negatives1. **Figure 1**: Please provide more informative captions. In the text, timbre and F0 are mentioned but it is not clear how that is relevant in the images. It is not clear what the colors mean in 1b, and if 1b is supposed to show a separation between male and female speakers the colors make it worse. Finally, are the points in 1b cluster centers or a single random sample per speaker?1. **Figure 2**: Would it be possible to make it clearer that there is an autoregressive dependency in the Attention and Decoder blocks due to the LSTM cell memory? The way the figure is currently drawn makes it seem as if each attention/decoder block can be computed in parallel from only the inputs from the previous flow iteration. 1. In section 2.2 NN and f are described as acting on the latent variable frames $z_t$, but Figure 2 applies the NN and flow on the mel spectrogram frames x in order to produce z. Similarly, there is text saying "we take the mel-spectrograms and pass them through the inverse steps of flow" and "Inference, [...], is simply a matter of sampling z values [...] and running them through the network" but Figure 2 marks the block as "Step of Flow" rather than "Inverse Step of Flow". It would be good to smooth out the consistency.1. More discussion about the end of sequence prediction would be appreciated. As the entire sequence of z must be used for inference, I assume there is some constant max inference length z used to obtain a final x, and the end of sequence prediction only happens to the final x rather than at each step of the flow? How well does this model adapt to inference samples that are much longer than any of the inputs seen during training?1. **3.3.2 Interpolation Between Samples**: I don't understand why there was a need to sample z to find z_h and z_s. Is it not possible to take z_h and z_s from a random training example for the speaker? Or does that mean there is a correlation between the latent space z and the __content__ that is being spoken? If the latter, I would like to see more discussion on that.1. **Table 2**: I assume bold means closest to ground truth? It's really hard to know how to interpret this table and I don't think it supports saying that FTA Posterior is more effective. FTA did not capture the increase in pitch mean from expressive -> high pitch, nor the decrease in std from expressive -> surprised. A better visualization may be to express this table as a bar graph (3 separate groups of 3 bars each) and conclude FTA Posterior is able to produce much more variation than Tacotron 2 GST?### Misc#### Abstract1. "varation" -> "variation"1. spell out IAF1. "We provide results on speech variation" etc. sounds weak. eg. "Flowtron produces output with far more natural variation compared with Tacotron 2 and enables interpolation over time between samples and style transfer between seen and unseen speakers in ways that are either impossible or inefficient to achieve with prior works."#### 1 Introduction1. "Their assumption is that variable-length embeddings are not robust to text and speaker perturbations" citation needed1. "Flowtron learns an invertible function that maps a distribution over mel-spectrograms to a latent z-space parameterized by a spherical Gaussian." mention IAF and cite Kingma et al., 2016 here instead of the previous paragraph. 1. "Finally, although VAEs and GANs provide a latent embedding that can be manipulated, they may be difficult to train, are limited to approximate latent variable prediction" While the IAF approach allows for direct optimization of log-likelihood, the latent variable encoding part is still approximate, just like in a VAE. The original Kingma paper even states that it is an approximate posterior. #### 2.2 Invertible Transformations1. $f^{-1}$ wouldn't be applied to $z_t$. Maybe $f^{-1}(x_t)$ or $f^{-1}\left(f(z_t)\right)$.#### 3.1. Training Setup1. "progressively adding steps of flow on the last step of flow has learned to attend to text" on->once?#### 3.4.2 Seen Speaker with Unseen Style1. "Flowtron succeeds in transferring not only the somber timbre, the low F0 and the long pausesassociated with the narrative style" -> "not only the somber timbre, but also"#### Appendix1. IAF is known to be quite inefficient. Is there any noticeable impacts on training or inference time? Or is this not a problem due to only using 2 layers of flow? It would be great if the ablation study in the appendix regarding layers of flow also covers this, but this is quite optional. **Summary**Federated learning takes advantage of the fact that private user data does not need to be transferred and shared across devices or servers. This makes FL particularly attractive for the user verification scenario, where privacy-sensitive biometric data are used to train verification models. One crucial hurdle in this scenario is that per device, only positive data are present, potentially turning the device-wise training objective ill-posed (all embedding are likely to collapse to a single point). As a way to introduce negative examples, FEDAWS has been developed and presented at ICML 2020. This paper recognizes a crucial security risk in the FEDAWS system, that embeddings of user data are transferred to the server, and proposes a more secure training methodology, FEDUV, that involves the error-correcting codes. FEDUV enjoys stronger security guarantees while showing comparable ROC curves as FEDAWS at nearly identical computational costs (though not entirely sure about the computational cost bit ;) ).**Pros**The motivation is spot on. Having to see any form of negative samples is the itchy point of the FL-based user verification system. FEDUV magically solves this issue by pre-defining a unique prototype vector for each user, which are not shared across users and are by design far apart from each other (this is the crucial trick!) by employing a technique in error-correcting codes (ECC). As a result, each user's endeavour to get closer to the own prototype vector ensures the maximisation of distance from the others' prototype vectors. Three experiments that are quite close to real-world scenarios (speaker, face, and handwriting-based verification) show that the performance of FEDUV is comparable to FEDAWS, the state of the art framework from ICML 2020 with weaker security guarantees.Writing is nearly flawless. Highly enjoyable paper.**Cons**No major cons. Perhaps explain in a bit more depth on the BCH code to illustrate (at least a high-level, hand-wavy description) how it assigns the codes in a distance-maximizing manner. Section 2.3 only explains the desiderata for BCH, rather than *how* BCH achieves it. Please also confirm that FEDUV spends nearly identical computational cost as FEDAWS. Somehow I got this from the paper, but have not found a solid reference that confirms this (if not, please explain, too).Nits: Please add grid lines and row titles (training set, test set with known users, test set with unknown users) in Figure 2 plots. Baslines --> Baselines. Flatten the last part of Section 1 as paragraphs rather than itemize? Yu et al. 2020 (FEDAWS) is an ICML paper, not arXiv - please fix the reference.**Key reasons for the rating**I don't find any major rationale to reject this paper. However, its novelty is also eclipsed by the Yu et al. 2020 (FEDAWS) paper. Though I really like this paper, I believe the best scores should be reserved for more innovative papers. The authors propose a method that allows training of UV methods without sharing any user (exemplar or class) embeddings with the server or other uses. Models are trained using gradient averaging on the server, so any leakage through that is not addressed in this work. The paper shows experimental results on speaker identification, face and handwriting verification tasks. The authors argue that this is the first work that considers secure training in a federated setup, with neither raw inputs nor exemplar or class embeddings being shared with the server or other users.#### Pros* The paper is clearly written and the derivations are sound (for the most part, see questions below). * The idea appears to be novel and a significant delta compared to the SoTa in terms of security and the novelty of a secure embedding learning protocol in the federated setup were only (one) positive classes are available for training.* The experimental results are promising albeit can't compete with existing less secure methods. #### Cons- Clarity of experiments  - Especially for the face verification task the code length seems to play a major role. Any discussion giving an understanding of this would be appreciated. Specifically, how and why does $d_{min}$ affect the accuracy. Bottom of page 5 mentions that increasing the code-words and presumably $d_{min}$ increases the performance, but no reasoning is provided.  - Additional insights of how the baselines (softmax, FedAws) were trained and what the emedding sizes are would be helpful. Is the embedding size ~64 in all cases?#### Questions & Comments- The assumption of $||z|| = \sqrt{c}$ should be put into context. What are the practical applications for this assumption. Is it merely there for the math to work out?- The theorems show that $l_{neg}$ is redundant for when $l_{pos}=0$, however, it is not clear to me that minimizing $l_{pos}$ also corresponds to minimizing $l_{neg}$. In practice, $l_{pos}$ will likely never reach $0$ and a negative loss term could have a significant contribution to the loss surface.- Page 6 mentions that increasing $l_r$ reduces the minimum distance of the code for a given code length. Why is this the case? Is it because $r_u$ is sampled by the clients and no guarantees can be made? A more detailed discussion would be helpful.This work proposes a new idea that allows training embeddings for verification with only positive classes in a federated setting, while ensuring security. Some areas could be clarified in the paper, especially why it is sufficient to proof the redundancy of the negative loss term only for the global minimum of when $l_{pos}=0$. Assuming the authors can provide a satisfying explanation, I recommend accepting this work. The paper proposes a new motivation for designing the proximal function of adaptive algorithms. This leads to a new class of adaptive algorithms that achieves marginal optimality and converges faster than existing algorithms in the long term. The claims are substantiated empirically for a wide variety of deep learning tasks as well as empirically. I have no complaints about the comprehensive empirical evaluations and the quality of the theoretical results (if correct). I did not have time to go through all the proofs in the supplementary materials.I find that the intuition and motivation for the proposed class of methods is generally clear. It seems, however, that this method (as described in Section 3) is a bit ad hoc as the authors are trying to examine a *one-step increment* of the regret. Given from what we know of momentum methods in which the exploitation of longer-term memory may be beneficial, perhaps the authors can comment on whether looking at longer step windows of the regret would be beneficial. Furthermore, the reviewer is wondering whether there's any benefit to going beyond the use of the *diagonal* proximal function. Would a non-diagonal and more dense proximal term be beneficial?Corollary 4.1 seems overly conservative because the authors use \tau, the max of the \tau_{m_i}'s to control the overall regret bound. Also, it would be good to avoid the use of the O notation here to show the dependence of the regret (or at least discuss it) on the other parameters of the problem (e.g., the diameter of the problem).  The paper is easy to read and technically sound. It is original and presents an interesting perspective on how to understand the activations of ReLU networks. This could be pave the way for other contributions in areas such as adversarial attacks, pruning and especially generalization theory. The algorithm presented is efficient to the point that it can be used for large networks and datasets. And the insights on the differences between a FCN and a CNN are quite significant. It is also surprising that by the large number of linear regions of such a network, that such a high generalization can be achieved for FCNs. I however believe that the paper would be stronger with the addition of a small experiment, namely, comparing the results from MNIST using Fashion MNIST. Clearly MNIST is too simple, but CIFAR is already too complex. Is F-MNIST an intermediate point? would it still manage to generalize or is the generalization behavior just an outlier for very simple datasets such as MNIST.You also mention that the generalization capabilities are not explained by test points falling into the same linear regions as the training samples, but, is it possible that there are overlaps? or that the testing regions are subsets of the training ones? At least for FCNs it seems that your generalization results would indicate this is the case. Furthermore, are neighboring regions smooth on the class label? This would be significant for pruning. Have you considered estimating the volume of the regions around the training data points? This could also provide more insights into the different behavior of FCNs and CNNs.In general, I find the paper very interesting and it could have impact on other significant areas of research in DNNs.  This paper provides an method for computing an upper bound for the spectral norm of the linear transformation induced by a convolutional layer. An upper bound was first introduced as a heuristic by Miyato et al, but they did not prove any bounds. The authors use the exact computation of singular values of a convolutional layer in Sedghi, Gupta and Long to prove that the Miyato heuristic is indeed an upper bound. They further generalize Miyato's method to find 3 additional heuristics, all of which are proved to be upper bounds, and then show empirically that the minimum of these bounds gives a much tighter bound, often very close to the exact value. The bounds are significantly faster to compute than exact spectral norms, both in complexity and in practice.The authors show that their bounds can be used for regularization, and produce results similar to the spectral projection method in Sedghi et al on CIFAR-10. They also show that spectral bounds enhance the robustness of CNNs against adversarial attacks - previous papers were unable to compute these spectral norms so remained restricted to fully connected networks. By extending the results of Singla & Feizi to convolutional nets, the authors are able to improve the state of the art significantly.Strengths:The upper bounds are formally proved, and the proofs are easy to follow. The upper bounds are easy to compute - the methods overcomes the dependence on n^2 in the exact methods.Weaknesses:The upper bounds only bound the spectral norm, and do not provide any information about the remaining singular values. The comparison with Sedghi's CIFAR-10 used a baseline that was weaker, Sedghi et al improved CIFAR-10 accuracy from 93.8 to 94.7. I would like Section 4.3 to be explained a bit better, so that I can understand the significance of the results more clearly. ### SummaryThis paper analyzes the bias of models in pool-based active learning settings where the sampling procedure is probabilistic (non-deterministic). It proposes two unbiased estimators of the population risk that weight the loss for each sampled data point. Empirical experiments demonstrate that these unbiased estimators work well for active learning settings with under-parameterized models, but are less effective for over-parameterized models. For the latter case, the authors provide meaningful insights into why biased estimators of population risk may actually be beneficial for over-parameterized models.### Reasons for score**Strong points**1. The authors identify a common problem in active learning, namely the problem of non-IID data. The authors also propose theoretically-sound solutions to addressing the active learning bias.2. The authors experientally demonstrate their estimators in both linear regression and a deep learning experiment, showing how their estimators are beneficial in one case but not the other.3. The paper explains why the proposed estimators may have lower variance than the standard estimator for population risk.**Room for improvement and/or more clarity**1. The paper mentions the "quality" of the "acquisition proposal" several times (e.g., p.4, p.6, Figure 2, etc.). However, it is unclear what is meant by "quality." What makes for a "good" acquisition proposal?2. The paper only discusses the setting where the active learning sampling procedure is probabilistic (non-deterministic), with non-zero support over all remaining data points in the pool. However, many active learning procedures proposed in the literature are deterministic (e.g., choosing the K examples with highest loss, or the greedy core-set approach proposed by Sener & Savarese (2018)). Could the authors provide more context for bias in these deterministic active learning strategies?3. While I did not spend time reading the appendix in detail, it seems that points for the BNN were selected under $\tilde{R}$ (the biased sub-sample empirical risk). How do the results compare if the points were selected under $R_{SURE}$, which is a more relevant setting given that $R_{SURE}$ is the proposed estimator?4. Other than running active learning twice (once using $\tilde{R}$ and once using $R_{SURE}$), are there methods to determine when one would be preferred over the other, especially in deep learning settings?### Minor comments /  typos / formatting concerns(these did not affect my score)1. Please use vector graphics for figures. For example, save the figures as SVG or PDF files (instead of JPG or PNG) and then include them in the LaTeX file. In particular, the legend in Figure 2a is difficult to read.2. Figure 2: Does "shading is one standard deviation" mean +/- 1 std (for a total of 2 std), or +/- 0.5 std (for a total of 1 std)? Please clarify.3. In Equation (5), why not factor out the $1/N$ in front of the summation?4. The authors write,> "Using our corrective weights recovers the ideal line." (p.6)    Unless I am reading the graph wrong, this sentence is slightly misleading. It seems like the linear regression model trained using the corrective weights much more closely approximates the ideal line, but doesn't recover the ideal line exactly.5. In general, the paper is written in an "opinionated" or imprecise manner which detracts from the main points of the paper. Words like "surprising" and "greatly" are unnecessary. Phrases such as "better variance" should be replaced with more precise terminology, such as "lower variance". ### SummaryThe paper proposes an approach for building group-equivariant self-attention networks. The authors use group-invariant positional encodings, and thus the output of a self-attention layer is equivariant under the actions from the considered group. They empirically demonstrate that group-equivariant self-attention networks have an improvement over their non-equivariant counterparts.### Pros1. I enjoyed reading the paper. It has a clear title. The abstract, the intro, and the related work are well-written and give a clear understanding of the content of the paper. The technical sections are coherent and well-structured.2. The authors propose a general formulation of group-equivariant self-attention networks and rigorously prove their statements.3. The experiments demonstrate the improvements achieved by the added group-equivariance in image classification on several datasets.### ConsI did not find any major weaknesses in the presented paper. I did find some minor issues which, however, do not change the general impression and my decision.1. While the authors claim that the proposed formulation is general, they only consider 2 compact groups. A group of reflections and a discrete group of rotations. The authors also claim that current computational constraints restrict the possibility of training models with big vicinities until they converge. This issue can be especially important if a group of dilations (scaling group) is considered. The current scale-equivariant models of Bekkers 2020, Sosnovik et al. 2020, Worrall and Welling 2019, Romero et al. 2020 require one to process images with big filters. Is an implementation of a scale-equivariant self-attention network feasible given the current constraints? A discussion of these limitations would make the contribution of the current paper more clear.2. The authors use long equations with a very nested structure. Reading these equations is complicated given the current notation. The main source of confusion comes from repetitive sequences of brackets. A minor rethinking of the notation may improve the readability. For example, using square brackets when a function is an argument i.e. $\alpha(f)(i, j) \rightarrow \alpha[f](i, j)$It is a well-written paper. The theory is coherent. The text is easy-to-follow. The experimental section demonstrates a thorough evaluation of the proposed method.    *  Summary of the paper.  The paper presents a modification of the empirical risk minimization (ERM) framework in order to obtain more robust or fairer results, using tilted objective. The paper exhibit several properties of the tilted empirical risk minimization (TERM) algorithm applied to classification and regression, TERM exhibits a trade-off between average loss and max loss for instance and this trade-off can be handled with a parameter t that control how robust one wants to be or how fair one wants to be. The claims are supported by a number of numerical illustrations that show the practical efficiency of the method for diverse tasks.   * Strong points: the article is entirely reproducible as the author furnishes a well documented code. The figures and experiments made in the article provide a good illustration of the algorithm and a few theoretical lemma come to help understand the algorithm better.     * Weak points: Use of 80% outliers in Table 1 without further explications. I think there is too much material, you made a lot of different experiments and it would have been great to have less experiments but more explications. There is no theoretical risk bound that would give us the efficiency of TERM, for instance in a corrupted setting or with respect to a fairness loss.      *  Recommendation.    I vote for accept. The algorithm seems efficient and easy to implement and it allows the user a broad choice of different extensions of the basic learning framework in particular to fairness and robustness in classification and regression.**Most of my review will be about the robustness part of the article because it is a subject I am familiar with.**  *  Questions:    * It is very weird that TERM works in Table 1 even when there are 80% outliers , as my understanding of outliers is that they must be in minority. I would say that if 80% of the points are outliers, maybe the 20% points are in fact the outliers and I don't see a practical application where the inliers would be in minority. It seems counterintuitive. Can you explain that ? This is even more counterintuitive in classification where the error is smaller when the corruption is 80% than when it is 40%. I am not sure what is the task and what it is that your algorithm do in the 80% noise context. All the definitions of noise/outliers that I know of suppose that the proportion of outliers is smaller than 50%. In fact I think the problem with 80% outliers is theoretically impossible except when doing list decoding (see for instance the article "List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians" by Diakonikolas, Kane and Stewart).        * Why did you not compare your algorithms with scikit-learn algorithms or scikit-learn-contrib algorithms ? I am thinking about HuberRegressor, RANSAC, TheilSenRegressor. To go further you could also test algorithms from scikit-lego for fairness or scikit-learn-extra for robust classification. To compare to a second party and not only your algorithms vs your algorithms.        * Why did you only corrupt the labels in a robust classification task ? At least in the regression experiment, you could have corrupted the features and it works well. This is a very interesting property of your algorithm, there are not a lot of robust regression and classification algorithms that are robust to outliers in the feature space ! Is it a misunderstanding on my part or do you algorithm really work when feature space is corrupted ? I tested your algorithm on drug experiment with outliers in feature space and it worked. Huber and L1 did not work in this context.        * Is the TERM problem convex when t<0 ? As it is, there is no reason that your algorithm will always converge to a global minimum.  * Additional Feedback.    * The 80% noise in Table 1 really bugged me when I read your article, maybe you may want to explain more or to remove it as it can cause misunderstandings I think.    * When comparing robust methods, (Table 1), the authors did not compare their methods to the mainstream algorithms like RANSAC or Theil-Sen regression. It would have been interesting to do so.    * The classification task considered by the authors in Table 1 (CIFAR-10) is not easy to interpret. What is an outlier for a neural network ? For a linear classifier, an outlier is readily defined for a very non-linear classifier; this is not so easy because most neural networks will be so nonlinear that the introduction of outliers in the training dataset will not change the performances of the algorithm.    * It would have been interesting to compare TERM to other robust classification algorithms on an easier dataset in low dimension to exhibit the comparative performances similarly to what is done for regression.    * You talk of "minimax" in a way that is unusual. Most of the time, at least in theoretical ML, minimax algorithms refer to algorithms which attain the optimal rate of convergence (see Section 14.1 in "A Probabilistic Theory of Pattern Recognition" by Devroye, Gyorfi and Lugosi). I think this is not what you mean, what you used I call it minmax. I don't know if yours is a common use of the term and if this is the case, sorry for this comment and don't take it into account.    * Typo: Section 5.3 inn -> in.    * The fact that the objective is strongly convex when t>0 is fairly important (this proves the convergence), I think that you should include the whole proof and not an abridged version of it (see proof Lemma 3).                 The paper presents a dataset for autoformalization (semanticdisambiguation) of informal Latex STEM documents. It is based on theconsiderable amount of work that has been done in the last decade onflexiformal (semi-formal) language formats and tools such as OMDoc,OpenMath, sTeX and LaTeXML. The SMGloM glossary and the MiKoMHrepository are used as parallel sources, and the MMT system connectinga number of formal systems and foundations is used for dataaugmentation.These are still relatively small datasets, so custom pretraining ofGPT-2 is done on the full arxiv corpus. The pretrained model is thenfine-tuned on the smaller training data.  Multiple evaluation metricsthat are meaningful in the semantic setting are defined - some of themsimilar to those used in Wang et al 18 and Wang et al 20.The final success rate of 47.2% of test data predicted correctly looksvery good and is comparable with the results of Wang18/Wang20 on thesynthetic data obtained by informalizing Mizar.My overall impression is that this is an important step in theautoformalization program [1]. It has involved a lot of work and broughtin a range of important tools developed recently.Some detailed remarks:p5: Disamiguation ==> spell checkp5: def 4.1 "We call S  L fully disambiguated"==>I would not call the text fully disambiguated without types ofvariables. In systems with subtypes (e.g., Mizar, possibly also otherPAs with typeclasses) the meaning and provability of a statement(e.g., "forall x exists y st x = y *_complex y") will change dependingon whether the quantification is over complex, real, rational, integeror natural numbers.p7: Question: S_F = S_sTEX means exact string equality or after white space normalization, etc? If so can you say what are exactly the normalizations and what is the success rate before and after them?- Would larger GPT models help?- Would unsupervised learning like in Wang 20 be useful in some context here? The unsupervised methods seem to have improved a lot recently.References:[1] Cezary Kaliszyk, Josef Urban, Jirí Vyskocil, Herman Geuvers:Developing Corpus-Based Translation Methods between Informal and Formal Mathematics: Project Description. CICM 2014: 435-439 The paper aims to get a better understanding of differences between wide and deep neural networks through an empirical evaluation. It does it through a similarity analysis, and an analysis of which errors wide and deep residual networks do.The paper is well written and should not be difficult to reproduce. Most of the questions I got while reading where addressed in the paper, in an extensive evaluation. The most interesting and somewhat surprising finding is that even though two networks with different number of parameters and layers but with the same accuracy make very different mistakes, and there is a pattern to it. The weakest part is the similarity analysis, which does not seem to reveal much new. It has already been known that deep networks have layers which do not contribute significantly to final accuracy, can be proved or even forced to learn more useful representation. Would be interesting to apply the similarity analysis in a network with reinitialized layers. Also, the choice of specific similarity function and it's benefits/drawbacks are not discussed. Could the same analysis be achieved with a simpler similarity function? Is it computationally efficient to compute?Overall, there are definitely valuable contributions in the paper, so I propose lower score only due to the unclear choice of similarity function, as described above. This paper proposes the better utilization of strong data augmentations for contrastive loss functions in unsupervised learning. In Moco set up, typically, weaker augmentations such as color jittering, cropping is applied to construct positive pairs from the same image. In this study, by proposing a modified objective, the authors leverage stronger data augmentations to construct more challenging positives and negatives pairs to improve the quality of the representations. The paper delivers a novel objective together with leveraging existing strong augmentations to improve downstream performance. The authors can find my questions/concerns listed below.1. The paper is overall well-written, however, it is disappointing to see many typos grammar mistakes throughout the paper. Some examples are in "Thus we proposed the CLSA (Contrastive Learning with Stronger Augementations)", "to train an unsupervised representation", "The contrastive learning (Hadsell et al. (2006)) is a popular self-supervised idea".2. In section 3.1, the authors mention that the keys in the memory bank is managed with first in first out method. Is it not supposed to be first in last out? I would like to see some clarification on this.3.  The numerator in Equation 3 should be z_i' vs. z_i not z_k.4. The authors claim that in He et al. an input image is resized and cropped to 224×224 pixels. It should be "an image is first cropped from an input image and resized to 224x224 pixels."5. In the experiments section, the authors list other methods including MoCo, SimCLR, MoCo-v2, BYOL and compare to what they propose. As a baseline, it would be nice to directly use the stronger augmentations in MoCo-v2 objective and perform comparison to their method. Throughout the paper, the authors claim that strong augmentations hurt the learned representations due to distorted images. It would be meaningful to show this experimentally as well.6. The authors explain that they choose a strong transformation randomly from the given 14 transformations and repeat it 5 times to strongly augment an image. Is the sampling done without replacement? In other words, do the authors choose 5 unique transformations with the corresponding magnitude and apply those transformations to a single image?7. I like how the authors point the similarity of their objective to knowledge distillation. In this case, strong augmentations are assigned probability of being a positive pair from the positive pair constructed with weak augmentations. It helps to understand the full picture for the proposed method.8. Finally, I think the figure 3 is confusing rather than being helpful. Both weak and strong augmentations go to the memory bank and it looks like two distributions come out of nowhere in the figure. It would be more clear to point out that there is distribution of the representations from the strong augmentations and weak augmentations and they supervise the assignment for strong augmentations given predictions on the weak augmentations. This paper aims to improve the robustness of DARTS, and proposes to add an auxiliary skip connection branch to the mixOp in cells. The authors also analyze the effect of auxiliary branch on residual block from the view of gradient flow. Additionally, this paper refers to the theory of the work [1] and demonstrate that the auxiliary branch is able to reduce the dependence of the convergence of network weight on $\beta_{skip}$ so as to disentangle the two-fold role of skip connection: as an auxiliary connection to stabilize the supernet training, and as a candidate operation to build the final network. Moreover, extensive experiments on multiple search spaces and datasets are conducted, showing the effectiveness of this method. They also illustrate the accuracy landscape w.r.t the architecture parameters to demonstrate that the auxiliary branch is able to smoothen the landscape, making DARTS- less sensitive to the perturbation than DARTSI have two questions: 1). I notice that the authors adopt decay strategy on the weight of auxiliary branch ($\beta$), and claim that their method is insensitive to the type of decay policy. I think the authors find an interesting phenomenon of skip connection, and I wonder how $\beta$ affects the searching procedure? And can the effect of auxiliary branch holds or how long can the effect holds after $\beta$ decays to 0. Specifically, suppose we search for 50 epochs with auxiliary branch, then we remove the auxiliary branch and search for another 50 epochs, will DARTS collapse?2). I agree with the authors point that skip connection plays two-fold roles. However, I wonder with the auxiliary skip connection suppress the architecture parameter of skip connection ($\beta_{skip}$), so that no skip connections are chosen in the final model?In general, I think this paper propose a simple but efficient method to alleviate the performance collapse of DARTS, the strengths and weaknesses are listed as follows:Strengths:1). The authors propose a simple but efficient indicator-free method to prevent skip connection from dominating the superNet. They also demonstrate the effectiveness of auxiliary branch from the view of gradient flow and the convergence of network weight. 2). Extensive experiments on multiple search spaces and datasets show the effectiveness of the method.3). The method can combine with DARTS variants to further improve the performance.Weaknesses:1). I am sort of concerned that the auxiliary skip connection may suppress the weight of original skip connection. And I hope the authors will have further analysis.2). I wonder what is the meaning of DARTS-since the method actually add an auxiliary skip connection stead of removing any connections or operations. I strongly suggest the authors change the name into regDARTS (regularized), gradDARTS (graduated) etc.[1] Zhou, Pan, et al. "Theory-inspired path-regularized differential network architecture search." arXiv preprint arXiv:2006.16537 (2020). This paper proposes PABI (PAC-Bayesian Informativeness?), a way of measuring and predicting the usefulness of incidental supervision signal for a downstream classification task. In particular, when labeled data is only available in noisy or partial form, or over a different domain than the target test domain, this data may still be used to improve a classifier, but its unclear how to tell which forms of incidental supervision will be most useful. Having a measure which allows us to compare different types of such supervision enables us to make intelligent tradeoffs.PABI is proposed as a very general framework. The most general form of the measure, dealing with updates to the concept class prior, seems that it could capture any kind of incidental supervision. However, this means most of the work is in understanding how to apply and approximate it. This paper provides several such methods, particularly focusing on inductive learning (from constraints or partial/noisy gold labels) or transductive learning (from complete gold labels on different input domains). Mathematical developments of PABI are given for these cases, and experiments show that PABI is nicely positively correlated with the relative improvement that comes with various methods for integrating incidental supervision signal (including one which is developed as a side note by the authors).Computing PABI may be challenging in some cases. In the case of transductive learning, it seems that a model needs to be trained on the incidental signal, although this is better than the combinatorial explosion of jointly trained models that would be required to test relative improvements directly. However, its not clear if efficient approximations for PABI will be feasible in all cases. This and other questions about the breadth of application of PABI are left for future work.### StrengthsI think this paper is very well-motivated, situates itself well with respect to previous work, and presents clear advantages. Having a unified framework for comparing the utility of different kinds of incidental supervision signals seems potentially very useful, especially these days when incidental supervision of various sorts is instrumental in state-of-the-art models. It is also extremely relevant for data annotation and task design, which often has to make tradeoffs between these factors (i.e., noise versus partial annotation or dataset size).There is a lot of content in this paper, including mathematical developments, algorithms, and experimental results. While I did not carefully check the proofs in the appendix, and I am not familiar with PAC-Bayesian theory or the associated literature, the paper seems technically sound to me.### WeaknessesWhile the generality of the proposed PABI framework is great and improves over existing work, I think this paper could be scoped more carefully and the scope could be clarified better.As proposed, the PABI framework seems very generalwhich is good. But the paper only shows how to realize the framework in a couple specific cases, for inductive and transductive learning independently. This is still more general than previous work, but from the first few pages of the paper I was expecting something even more general.* It seems to me that the combination of inductive and transductive learning may be possible using something close to the paper's proposed methods , but this isnt addressed by the paper except a glancing mention in Footnote 6.* It also is not clear to me from the papers text whether something close to the PABI framework can apply in broader settings like language modeling style pretraining, where the input-output format of the incidental supervision signal is different than that of the target task. In particular, it seems that in this case the approximation method proposed for transductive learning would indeed have to reduce to training a combined model. Related issues were finally mentioned briefly in the last paragraph of the paper, and something along these lines appears in appendix A.3, but I think a more up-front clarification of the limitations is warranted.More broadly, the question in the back of my head when I began reading the paper was if this would help explain why and when language model pretraining (and other more flexible related-task pretraining) works well. The paper points to related work in this area, such as Gururangan et al 2020 (Dont Stop Pretraining), leading me to think this paper would shed light on the issue, but in the end the issue was not mentioned and seems perhaps out of scope.This is fine. All I would ask of the authors is to be more explicit about the limitations of PABI (or the proposed realizations of it) from the beginning, laying out the scope of this work and stating the limitations outright instead of only pointing to the appendix. It seems to me like PABI is more of a foundational framework which is ideal for future work to build into, rather than already being a general solution in itself. I think it would be best to pitch the paper this way.### RecommendationAccept. Important problem, lots of solid content, clear benefits over previous work and directions for the future. Great work.### More comments/questionsI think the point of the formulation in Section 2.2 can be made a bit more explicit. It seems like the point is for applying PABI to partial labels. If thats true (or theres more to it) then might as well just say it there, or at least give this case as a motivating example.Regarding the cross-domain results: why are the incidental supervision sets so small? It seems that there is a ton more incidental supervision available for NER, and in both cases the incidental supervision data is even smaller than the test set. Why not use more? It seems to me that the use case here is when a large amount of incidental supervision is available anyway. It also seems like the low-data setting is not totally fair to the vocabulary overlap baseline.### Typos, style, etc.When describing your experiments, I think its worth mentioning that they are on English text.Figure 3: I dont understand which numbers correspond to which model in the caption. This would be much easier to read in a table.* P. 7: somethings wrong with twitter(Strauss et al., 2016)* P. 7: The FitzGerald et al 2018 dataset is called QA-SRL Bank 2.0.* P. 7: servers -> serves* P. 7: the lower bound for is *SummaryThis paper provides a novel method of learning density models for sets of points by modelling the sets as samples from a point process, approximated with normalizing flows.  A point process gives a probability to a set of points, not assuming that the points are independent of one another. The authors describe the CONFET method, which uses continuous normalizing flows to map from a uniform point process with a learned transformation. The authors describe a method to tractably compute the exact trace of their transformation, allowing it to scale to high dimensions and numbers of points. Experiments show state-of-the-art performance on benchmarks.*PositivesThe paper is generally well-written and motivated well.The exact computation of the trace in this setting is a key aspect to the tractability of this work, and is a good contribution.The experimental results seem convincing that the overall method is an advance in the state-of-the-art of modelling point datasets.The appendices are discursive and comprehensive*QuestionsHow does this method perform on tasks such as modelling point clouds?*RecommendationOverall I recommend to strongly accept this paper.  The trace-computation method is a nice fundamental contribution, which is used to present a compelling set of experimental results.*Minor pointsTypo in paragraph 1 of section 6.2: 'studies are in D' should be 'appendix D'Typo in paragraph 2 of section 2, 'Symmetry requirement comes from' should be 'the symmetry requirement...'Figure 5 might be better off in a different colour scheme than green/red for those with colourblindness In this paper, the authors analyze the convergence of a proximal gradient descent ascent (GDA) method when applied to non-convex strongly concave functions. To establish convergence results, the authors show that proximal-GDA admits a novel Lyapunov function that monotonically decreases at every iteration. Along with KL-parametrized local geometry, the Lyapunov function was used to establish the convergence of decision variables to a critical point. Moreover, the rate of convergence of the algorithm was computed for various ranges of KL-parameter. Pros:The paper studies an interesting and relevant problem in a vibrant field of research. The convergence analysis of proximal-GDA are detailed and well presented (covering function value convergence, variable convergence, function value convergence rates, and variable value convergence rates). To show the results, the authors provided a novel Lyapunov function and analyzed the convergence using KL local geometry.The paper is very clear, concise and neatly written (almost free of typos).The related material are referenced and well-discussed in the paper. The authors clearly positioned their work in the related field and discussed their contributions in comparison to other similar works.Cons:The paper lacks any experimental results. Demonstrating the different convergence rates on critical points with various KL-parameter can be good experiment.Minor Comments:1.Definition 2: Should it be $h(z)$ instead of $f(z)$?2.Equation (19) in the Appendix, and P Third Inequality should be =.3.Page 14: upper bound on distance of subgradient set to 0, second inequality should be =.4.Appendix F: Theorem 3, Function missing a ``'c'5.Equation (36): $d_{t-1}$ instead of $t_{t-1}$.6.Last expression of Page 19: I think 1/2 is missing in the left hand-side.  This paper studied the convergence properties of the proximal-GDA algorithm for solving nonconvex-strongly-concave optimization problems. It develops a novel and comprehensive theoretical understanding of the variable convergence and rates of the algorithm under the KL geometry by identifying an important and intrinsic Lyapunov function. Specifically, the paper considers a regularized nonconvex-strongly-concave optimization problem, with one convex regularizer and another possibly nonconvex and lower-semicontinuous regularizer. This problem formulation generalizes many existing differentiable minimax problems. Then, under standard conditions in Assumption 1, the authors identified a novel and important Lyapunov function H(z) and showed that this function monotonically decreases throughout the optimization process, although the minmax objective function value may oscillate. Based on this new characterization of Lyapunov function, the authors proved that every limit point of the algorithm is a critical point of the minmax problem. Moreover, under the general KL geometry of the Lyapunov function, they formally proved that proximal-GDA converges to a single critical point. This is the first variable convergence result in nonconvex minmax optimization. Besides, they also characterized the dependence of the variable and function value convergence rates on the parameterization of the KL geometry.Overall I believe this is a novel and important work in minimax optimization. In particular, the Lyapunov function is a very powerful tool for studying the convergence properties of the two variable sequences of proximal-GDA. It conveniently simplifies the analysis of minmax optimization into the analysis of min optimization via Proposition 2. Also, the monotonic decreasing property of the Lyapunov function further enables analyzing this algorithm under the general KL geometry, and obtain a fundamental variable convergence (rates) result(s). These technical tools develop a new framework for analyzing minmax optimization algorithms and can potentially used to study other minmax algorithms. Below are some of my minor comments:In Def.2, the {h_\Omega < f(z) < h_\Omega + \lambda}, should be h(z).The proof of Theorem 2 assumes that H is a KL function. As H is essentially the objective function regularized by some quadratic terms, is it sufficient to assume \Phi+g is KL?If possible, I suggest do a simple experiment to verify the monotonic decreasing property of the Lyapunov function. ##########################################################################Summary: this paper extends the lottery ticket hypothesis to life-long learning. The paper proposes a top-down and bottom-up approach to network pruning and shows that the bottom-up pruning reaches SOTA performance on several datasets while reducing the network size to a few percent of the full model size. The paper also shows higher performance against SOTA for class-incremental learning. ##########################################################################Reasons for score: To the best of my knowledge, this paper brings novel contributions to the community.  The approach is sound, well-explained, and evaluated rigorously.  The Open Questions section presents open questions but these do not represent a blocker to publication in my opinion.##########################################################################Pros: The paper has the following advantages:- Novelty: to the best of my knowledge, the paper brings a novel contribution to the problem of LTH for life-long learning- Clarity: the paper is well-structured, clear, and easy to read- Rigor: the work presented in the paper is rigorous. An ablation study is included and several in-depth analysis are presented. Due diligence has been done on experimental setup. The appendix contains numerous experimentation details (providing code would be even better)- Impact: the paper brings a significant contribution to the literature by beating or reaching SOTA on lifelong learning.There are several open questions in the Rebuttal section, which, according to me, should not challenge my score.  However, I am interested in the opinion of the authors and other reviewers on these questions.##########################################################################Cons: I do not see major limitations to this paper, apart from the code not being released, which reduces the reproducibility of the paper.  This section contains only minor editorial recommendations.The first sentence of the abstract should read "The lottery ticket analysis states that..." instead of "demonstrates that..." since a hypothesis cannot "demonstrate" something.  It is a minor detail but since this is the first sentence of the paper, it has a significant impact on the reader's impression of the paper.Typo page 3 in "can be trained same well in isolation" (this phrase does not make sense)Typo page 4 in "Why we need beyond top-down pruning" (this phrase does not make sense)Graphs on Figure 3 (and in the Appendix) are hard to read for small values of remaining weights.  Many scaling the x-axis differently would help.Typo page 6 in "learning the rest three tasks" (does not make sense)Page 6, the sentence "Therefore, the bottom-up lifelong pruning debuts, ..." does not make senseThe Related work section is put at the end of the paper, which can make sense (some paper do this regularly).  However, for this particular paper, I would tend to think that moving it up in the paper (close to the beginning) could make sense too.##########################################################################Open questionsMy understanding when reading page 4 section "Curriculum schedule" is that TD pruning requires the knowledge of the number of tasks.  Is that correct?  How would it extend to an unlimited or unknown number of tasks?The rewinding point approach seems to require maintaining the full model in parallel to the optimized one.  If that is true, it seems to defeat the purpose of optimizing the model.  Am I missing something?  Also, in the real world, this could have memory implications that could make the approach less practical.The ticket size seems not to have a theoretical upper bound in this approach.  Is this correct?Results seem out of noise for most experiments, but it would be nice to have confidence intervals, in particular for the claim that TR-BU outperform the dense model by 0.52% (page 6)From Figure 4, it seems that the ticket sizes seem to converge for TR-BU and TD as the number of tasks grows.  Is that what is happening?  Any theoretical analysis of this?######################################################################### Summary: This work presents a method for online learning of an assistive typing user interface (XT2) with implicit user feedback. User inputs for such an assistive typing interface are assumed to be in the form of eye gaze or handwritten characters. However, the implicit human feedback is assumed to be backspaces typed on a keyboard. Backspaces are used to delete words predicted by the assistive typing interface based on the users input. The online learning of such an interface to improve its assistive performance and adapt to the user over time is framed as a contextual bandit problem. A reward prediction network is trained to predict the use of backspaces (implicit feedback) by the user. This reward prediction network combined with the default interface policy using Bayes theorem is used to update the policy of the typing interface. The experimental results with two user studies reveal that the presented method performs better than a non-adaptive default interface, stimulates user co-adaptation to the interface, and offline learning accelerates online learning.########################Pros:- The paper is well-written and easy to follow.- Strong experimental results to support the idea of the work.- Offline pretraining of the reward model reduces the burden on online user interactions needed for the interface to be improved.- The presented technique is applicable to any form of user inputs and the authors test their approach with two different forms of inputs (eye gaze and handwritten characters).- Use of backspaces to train an interface with RL is a novel idea.########################Cons:- The implicit feedback is assumed to be perfect and available via a keyboard. How realistic is this assumption? What happens if the feedback is noisy? Is the availability of an independent backspace action being true to the nature of the interface being truly assistive?- It was unclear to me while reading the paper if the implicit feedback (backspaces) are provided via a keyboard or are also predicted with the users input (such as with gaze tracking). I assumed its the former with subtle hints in the paper and Fig 1. It would be helpful to make this explicit for the reader in Section 2.1 (as part of assumption 1).- This approach does not model the temporal effects of learning. Practically, modeling the problem as an MDP would be more realistic versus contextual bandits.########################Reason for score: This simple approach is presented with clarity and supported with well-reported experiments (including several ablative analyses). Some minor issues in the writing could be improved but overall the idea is well-presented and well-evaluated.########################Questions during rebuttal:- Sec 4.1: Is the user study conducted with the interface type variable being varied within-subjects (i.e. each user uses both default and X2T)? From the experimental results, it seems this is the case but it is not explicitly stated. If this is so, is the order in which users attempt to use the two interfaces (default and X2T) counterbalanced?- Please address other questions raised as part of the Cons section and other feedback.########################Some typos and other feedback:- Section 2,  paragraph 1, sentence 2: & relies on an assistive typing interface to infer the users intended action from available inputs & -> & relies on an assistive typing interface, where the users intended action is inferred from available inputs & - Algorithm 1: Consider defining what p_user(x) is. Are you making any assumptions on such a model of human user inputs (such as random sampling as suggested in Sec 4.3)? - Consider citing a recent work on learning from imperfect implicit user feedback such as facial expressions):(Cui, Y., Zhang, Q., Allievi, A., Stone, P., Niekum, S., & Knox, W. B. (2020). The EMPATHIC Framework for Task Learning from Implicit Human Feedback. Conference on Robot Learning (CoRL), 2020.- Even though compared to prior work, this work does not assume access to ground truth action labels from the user provided to the interface, it does assume access to ground truth backspace actions. It would be beneficial to emphasize this in paragraph 1 of Section 3.- It only becomes clear to me by Section 3 what is meant by handwriting as an input and how it can be an assistive input modality. A reference to Appendix Figure 5 early on the introduction, along with highlighting this can be an easier mode of user input versus typing on a keyboard, would be helpful. - Section 4, paragraph 1: References to the subsection numbers can be made when stating the evaluation questions. For example: Q1 (Sec 4.1): Does X2T improve with use and learn to outperform a non-adaptive interface? The questions are well-framed and very clear though!- Section 42: One of the insights presented for the presented method is that the XT2 interface can learn to automatically overcome calibration issues with the gaze tracker, thus the interface adapts to the mis-calibrations over time without the need for recalibration, even though external conditions would require a recalibration for better eye gaze prediction. This should be highlighted in the introduction as well.- Section 4.3: Isnt p_LM(u) conditioned on the preceding characters of the text seen so far? Would p_LM(u|t) be a better representation?- Section 4.4, last paragraph: Consider reversing the order of the first two results presented (they are in opposite order to the questions posed in the previous paragraph). The topic of the paper is interesting and important. The padding strategy seems to be a small but largely overlooked aspect in CNN learning. While many papers attempt to reduce/improve the effect of position in images. This paper gives a different perspective on the padding patterns that essentially causes these position sensitivity. The paper is easy to follow: the hypothesis at the beginning are important and interesting. And the hypothesis are quite clearly verified with experiments.The Hypothesis raised in this paper are important factors for training networks for various tasks. especially for H5 and H2. And the paper gives useful conclusions based on experiments.   H4 is a bit unnecessary as it is common knowledge that as layer goes deeper, it has a larger receptive field thus even boarder information will be included in the center regions. The current analysis are based on CIFAR, it would be more convincing if these hypothesis are further validated on larger datasets such as imagenet or coco (for segmentation). A minor suggestion, it would be more informative if the author in related work/analysis compare with the positioning methods used in transformers.  ## Paper summaryThis paper addresses the problem of constraining adversarial image perturbations to be similar to some natural class of perturbations. This would allow natural perturbations to be treated with the same rigor (e.g. quantifiable attack and defense strengths) as standard adversarial perturbations. Standard adversarial perturbations are arbitrary perturbations of an image that are within a certain $\mathcal{l}_p$-ball around the datapoint in pixel space. They are otherwise unconstrained and therefore appear as unstructured noise. Instead, the paper proposes to train a conditional autoencoder to generate perturbed versions of clean images from pairs of clean and perturbed images. The autoencoder can then be used to generate new perturbations that are similar to the training data. Adversarial versions of these perturbations can then be defined in the latent space of the autoencoder, rather than in pixel space. The paper provides theoretical arguments that perturbations generated in this way are close to the perturbations used for training. Using these learned adversarial perturbations, models can then be trained that are robust to them.## Arguments for acceptance* The paper provides a method for generating adversarial perturbations that are similar to natural perturbations, such as lighting changes. This has great promise for studying and improving the robustness of vision models to natural perturbations.* The paper provides a convincing motivation and thorough theoretical justification for the proposed method.* The theoretical section of the paper is clear and well written.## Arguments against acceptance* The experimental section only mentions the bare minimum of methodological details and results, and instead defers most experiments to the extremely extensive appendix. For example, the experiments on the multi-illumination dataset, while very intriguing, are barely intelligible without reading the appendix. Perhaps a venue allowing longer articles (e.g. a journal) would have been more appropriate.* The method is evaluated only on CIFAR and the multi-illumination dataset. It is unclear if the method scales to larger datasets such as ImageNet-C.## Conclusion and suggestionsThis paper provides a well motivated and generally applicable method for generating adversarial examples that are similar to natural image perturbations. This work will be of interest to many ICLR attendees, so I recommend acceptance.Suggestions for improvement:* Move material from the appendix to the experimental section, e.g. more detailed descriptions of the experimental methods.* Repeat experiments on ImageNet-C.* Discuss computational cost/scalability of the method in the main paper. # General statementsThis paper has a special flavour, in the sense that it provides new light on a very established training method for energy-based models: contrastive divergence. Its core contribution is to provide a theoretically grounded understanding of CD as it is widely used, avoiding the common assumption that this algorithm stems out of a simplifying assumption.This is done through a connection between CD and adversarial training. On their way, the authors show how some minor corrections suggested by their interepretation may dramatically improve performance of CD, at least on their toy example.Since CD is a widely accepted method, I feel that the deliberate choice of restricting their experiments on toy data is legitimate.All in all, I would say that the paper is a very nice read, and its english usage is good, as well as the references that are appropriate.I think that it is appropriate for presentation at ICLR, since it may stimulate new research on CD.# Detailed commentsBelow are some minor comments in chronological order## Introduction* "Thus, Our": uppercase## Toy example* In figure 4, you probably mean "from left to right"* To be extra sure, are you effectively disabling gradient recording when computing \tilde{x} as I assume you do ? I'm asking because \tilde{x} actually appears as a function of x, parameterized by \theta, i.e. as \tilde{x}_\theta(x), since it involves the transition kernel q_theta for its computation. As you write below eq. (17), you are considering that the kernel q as kept fixed, explaining such a choice. however, and if I'm not mistaken, it should not be too difficult with autograd mechanics to include this dependency in the updates. Did you try it ? Did it break the algorithm ? * I would appreciate more steps in your derivations (22) and (24): I don't follow easily the transitions to lines 2 and 4 of each.* The neural net used for the toy data looks impressively large (8 layers of FC+leakyReLU with 512 hidden size). Was it really necessary ? The paper proposes a method of choosing variables for branching in branch and bound approaches to MIP solving. The approach is based on reinforcement learning.The paper presents an adequate overview of previous approaches to the problem. There is not a lot of detail about how these approaches work but the overview of the techniques given allows the reader to see how the proposed approach differs from this earlier work and motivates the technique.The MDP formulation used appears not to be novel, but the paper presents a novel way to use reinforcement learning to find good strategies.The paper makes a clear argument about why simply choosing the same variable as strong branch would is not the best variable selection strategy. This is supported by later experimental results. I am not sufficiently close to the field to know whether this is a novel argument or accepted fact. In any case, it seems worth stating here as it illustrates a significant fault with many existing alternative approaches.There are quite a lot of decisions made in the design of the algorithm. While it is clear what has been done, I am unclear on why many of these choices have been made. For example, what is it about NS-ES that makes the authors think it is suited to this task? Is there some feature of the problem that means this is an appropriate method? Ditto the novelty metric. Is is clear how to compute the metric, but there is a lack of argument or intution on why this is an appropriate way to measure the distance between two sets of polytopes.The benchmarks used in the testing are sensible. It is always easy to raise questions about whether testing could be on a larger set of problems, but those presented here seem suitable for a conference paper. The competing methods evaluated against are appropriate and fair. The setting does not seem biased towards any of the methods.The novel approach appears significantly to beat other learning approaches on two of the problem types. The new approach seems about the same as the other approaches on the facility location problems. It would be interesting to understand what it is about this problem that gives different results.In table 1, wins are defined by number of nodes visited. In table 2, the time taken is used instead. Either method could be justified, but to change between experiments without good justification looks dubious.A side benefit of the paper is that it results in determining branching rules which seem to perform well while not (intentionally) imitating strong branching. There is some investigation of why this is and why the technique works. Further research could build on this and the result may lead to further work on manually constructed branching rules.There is a discussion section in the appendices. This seems very odd.The paper is very well written - the language is easy to understand and the arguments being made are clear. Summary: The authors propose a simple recurrent network as a model of spatial navigation in the MEC/Hippocampal network. This model assumes that grid cells only regularly receive egocentric movement information, an important aspect for understanding the origin of these functional cell types in-vitro. Overall, this article should be of interest for any ICLR members interested in biological  spatial navigation. Strong Points: As stated above, the model is an intuitive explanation of how allocentric-egocentric transformations might be performed. While the authors use a back propagation approach to training, the separated loss functions (eq 10-12) for each layer of the network mean that learning could be performed by predictive contrastive coding, as recent research suggests biological networks may be doing.The learned receptive fields show many of the more nuanced aspects of grid cells found in experimental studies, such as discretized angle relative to the environment. The investigation of error accumulation as a function of time steps since encoding allows additional comparisons to the literature. These give additional confidence that the model is biologically plausible, at some level of abstraction.Weak Points: As a non-mathematician, it's unclear to me what the implications of the lie algebra presented in the beginning of section 2 are. My understanding is that this makes grid activities the product of two separable matrices (displacement, and rotation). While this is biologically plausible, perhaps the authors could explain any additional reasons why this is of importance.Additional Comments: A possibly interesting future experiment would be investigating effects on error correction by only having place cells at a handful of spatial scales (A.2), especially if there is a "block" structure in u. This paper presents Hopper, a method that performs multi-hop reasoning to address the problem of object permanence in videos.### Strengths:- Very good results in the Snitch Localization task. Good and sensible baselines.- Good ablations, experiments and visualizations. Also good justification of results both for Hopper and for baselines.- They introduce a method that is modular, and clearly separates several stages of processing (while still making everything end-to-end trainable). This provides good understanding of the system, and makes ablations and comparisons with baselines easier (for example, baselines can work a the per-module level).- The multi-hop transformer reasoning module they introduce is intuitive, and it is very well executed.- Overall well written and good positioning among state of the art literature.- Contribution of a new CATER-h dataset that corrects some biases of CATER for the specific task the paper is solving. Good explanation of the need for this dataset, both by analyzing the dataset statistics, and by looking at the results of the baselines on the two datasets.### Weaknesses:- A lot of supervision and limited dataset.- Object annotation class and bounding boxes are available.- Fixed set of objects and attributes, with no variation across samples (synthetic data).- The tracker learns based on the class. If there are two objects belonging to the same class, it will probably not distinguish them (see Figure 15a). Probably humans could watch a video and follow the snitch even if all objects looked exactly the same, just by following their movements, and this method is, _by construction_, not doing this. Therefore, Figure 15a is not just some very hard case, but a case that the system as it stands cannot solve. This implies that the tracking done by the system is a very weak form of tracking, and it is more a relabeling of objects. Actually, to solve the task the system only needs to know 1) where the snitch was before and 2) where did the container move (just reidentify by classification). While this can be improved, the main contribution of the paper is in the step after this, so it is OK to leave it for future work.- In some cases the method feels very specific for the presented task and dataset. This is acceptable to some extent because it is a hard problem and it doesn't have to scale right away, but I am concerned it is too specific about this formulation and cannot be used. There are a lot of specific intuitions for this specific problem and setting (not interesting for any other case). See for example page 6. More specifically, an example is the heuristic for computing the occluder, which would fail if the snitch has moved between frames, and it is visible but somewhere else (and this case is not even generalizing to a slightly different task, but a potential situation in the current dataset). While these heuristics are optional and the authors present ablations, the feeling is general for the whole method. - Some parts of the method could use more clarity:- Figure 3 is confusing, as it does not exactly follow Algorithm 1. It would be convenient to use the same names as in the rest of the paper for the layers and the inputs/outputs.- Why the attentional feature-based gating (line 11 algorithm 1) is necessary? Why doesn't Transformer_f directly output the final U_update?- Why two transformers are necessary, actually? The masking could be applied at the beginning of the first one and just use one. Note this is not the same as the "hopper-transformer" in the baselines.- t is computed with a softargmax. However, it is used in places that require it being a hard number, for example in line 2 in the algorithm. This implies these places have to detach the gradient from t. Apart from line 2, is there any other place that requires this? What about lines 12 (Masking) and 5 (Extract). My understanding of the paper tells me that at least the Masking() module requires a hard t. In what case is the gradient actually propagated through t?- Teacher forcing: the paper argues that hop 2 should focus on the first occluder. What about hop 3, what is it supposed to predict?- For line 14 in the algorithm, are the attentions of the heads averaged? - About the 5 steps: Looking at the examples (eg Figure 15b) it looks like there are much more than just 5 key steps to follow. However, Hopper never uses H > 5 in the shown examples (while H=5 is actually the minimum number of hops that are allowed to the system). Why is that? It looks like as soon as it has a chance, the system tries to predict the last frame available, even if it is not the most convenient (it can always get later, to the last frame). This could make sense for CATER, but the shown examples are for CATERh . This is important because the main point of Hopper is that it can select where to attend. I would appreciate some intuitive explanation and statistics of the number of steps Hopper takes.- Unclear what the model is _really_ learning. While it is not learning temporal biases, there are potentially a lot of other biases it can be exploiting. It would be very interesting to have analyses to answer this question. Otherwise it is hard to believe the models are really "solving" this task.### Additional comments and questions:- When an object is occluded or contained by another one, the goal is to predict the position of the first object, or the position of the occluder?- Is the 1 fps chosen for any specific reason? The solution to the problem is actually different depending on the sampling rate. For example, it would be possible that in between frames there is a key move that we do not see.- "Humans realize object permanence by identifying key or critical frames where objects become hidden", and similar sentences throughout the paper. Citation?- I think the third sentence in the introduction refers to the first one. It is confusing because it reads as it is referring to the second one instead. I would move the second one to later when the authors talk about object permanence (line 20).- If the key point of the paper is the multi-hop transformer, an interesting comparison would be the same system without DETR, and instead using only the true class label and bounding box (remove steps A and B). At least to see how much these are a bottleneck, or how important they really are.- Analyses on learned attributes. The classes are combinations of 4 attributes. Why not learning them separately? Is the model creating different represenations for each specific combination? Would it generalize to a new object or to a new combination of attributes not seen during training?- Will the code for the paper and the cater-h dataset be released?### Final recommendationOverall, I believe this paper should be clearly accepted to ICLR, as the strengths outweigh the weaknesses. This paper is not the final solution to the object permanence task, and it has a lot of possible improvements, but it is a good step. Summary:This paper proposes a novel graph neural network-based architecture. Building upon the theoretical success of graph scattering transforms, the authors propose to learn some aspects of it providing them with more flexibility to adapt to data (recall that graph scattering transforms are built on pre-designed graph wavelet filter banks and do not learn from data). By dropping the dyadic distribution of frequencies within the wavelet bank, the proposed architecture actually learns a more suitable frequency separation among the different wavelets.Strong points:The paper is well written and technically solid.The idea of flexibilizing scattering transforms while retaining key theoretical contributions is a notable contribution. In particular, the idea of learning the dyadic nature of the frequency bands in the wavelet bank is a key aspect, since the frequencies in graphs are rarely evenly spaced, and thus the ability to partition the space of frequencies in other form than a dyadic one is bound to bring substantial improvements.The numerical experiments in the paper deal with new biochemical datasets which are much more interesting and practically useful than the de-facto benchmarks of semi-supervised learning in citation networks. This is a welcome change to see actually useful applications of graph neural networks.Weak points:No real weak points except for a common misunderstanding of the low-pass nature of filters in a graph convolutional neural network. Please, see below for details. I seriously encourage the authors to re-write parts of the abstract and introduction to avoid perpetuating the misconception that graph convolutional neural networks can only learn low-pass filters.Recommendation:This is a very interesting contribution and, provided the clarifications that I mention below, I firmly support the acceptance of this paper into ICLR.Major comment:The claim that GCNs rely on low-pass graph filters is, at the very least, misleading. As a matter of fact, even a graph convolution with an order-one polynomial can be a high-pass. To see this, assume we adopt the graph Laplacian as the matrix description of the graph. If this is the case, we know that low-eigenvalues correspond to low-frequencies and high-eigenvalues to high-frequencies (according to the notion of total variation). If this is the case, then a filter like H(L) = 0*I+1*L gives a frequency response of h(lambda) = lambda which is 0 for the zero eigenvalue, and grows for larger eigenvalues. This is an example of a high-pass filter. Likewise, assume that we use the adjacency matrix. If this is the case, then the highest real eigenvalue is the one with the lowest frequency, and depending on how far we are from that eigenvalue (measured as modulus operation in the complex plane), the higher the frequency is. So, for the sake of argument, let us assume that the adjacency matrix has real eigenvalues and is normalized by the largest eigenvalue, so that all eigenvalues are contained in [-1,1]. For this situation, eigenvalues closest to 1 will be low frequencies, and eigenvalues closest to -1 will be high frequencies. Then, a filter of the form H(A) = 0.5*I - 0.5*A gives a frequency response of h(lambda) = 0.5 * (1-lambda) which has value 0 for the lowest frequency, and value 1 for the highest possible frequency. This makes such a filter a high-pass filter. As we can see, we have just constructed two filters (one for the Laplacian and one for the adjacency) where both filters are of order one, and still high-pass filters. Of course, it becomes easier to build high-pass filters if the orders of the polynomials are higher (i.e. ChebNets). Therefore, since the coefficients of the graph convolutions are learned from data, it is very hard to argue that the resulting filters will be low pass (i.e. they need not be, there is nothing preventing an order one graph filter to be a high-pass and there is no enforcement of such a constraint in the original formulation of graph convolutional neural networks).This claim is first found in the abstract, and later repeated in the introduction. With the clarifications from the introduction, I may understand where the misinterpretation arises. Kipf's GCNs use as graph matrix a normalization of (I+A) and consider an order one polynomial with the zeroth coefficient set to zero: h*(I+A). This forces h_0 = h_1 in an order-one graph filter on the adjacency matrix, which would otherwise be written as h_0 * I + h_1 * A. By forcing h_0 = h_1, we are indeed forcing the filters to be low-pass, i.e., we always learn filters of the form h(lambda) = h*(1+lambda) where we only learn the coefficient h. However, this is a design choice of Kipf's GCNs. The general formulation of a graph convolutional neural network (see Defferrard's ChebNets, for instance), does not require h_0 = h_1 and thus can also learn high-pass filters as discussed above. Interestingly enough, even if we would consider the normalization of (I+A) as the graph matrix, but we don't force the zeroth order coefficient to be zero, we would be able to have filters of the form h_0 * I + h_1 * (I+A) which would actually allow the learned filters to be high pass (i.e. h_0 = 1, h_1 = -0.5, see above). In any case, what I mean here, is that learning low-pass filters is a self-inflicted problem by Kipf's GCNs that can be very easily avoided by just using a more general formulation of graph convolutional neural networks (for instance, Defferrard's ChebNets).In summary, I would suggest the authors to avoid any mention that graph convolutional neural networks only learn low-pass filters. Otherwise, this would perpetuate an important misunderstanding that has been around for a while now and is misleading research efforts. In other words, yes, Kipf's 'GCN' implementation of graph convolutional neural networks can only learn low-pass filters. However, the general definition of graph convolutional neural networks (suggested in Defferrard's ChebNets, and formalized in the signal processing literature regarding GNNs) by no means implies that the learned filters are low-pass. Thus, 'graph convolutional networks' are not oversmoothers, or only learn low-pass filters. Kipf's GCN does (and it can be avoided by just using a different implementation).In any case, this does not alter the contribution of the paper. I would just request the authors to correct this aspect to avoid perpetuating the misunderstanding surrounding graph convolutional neural networks.Minor comments:1) The authors suggest two learnable adaptations of graph scattering transforms. The parameter alpha in the graph matrix, and the scales. However, the authors find out that the parameter alpha does not contribute anything in training and is thus arbitrarily set to 1/2. I would suggest, then, that the authors focus on the scales as the learnable adaptation, and only mention in passing that alpha can be potentially trained, but that the numerical experiments in this paper show no improvement by doing so. I believe this would put the focus and draw attention to, probably, the main contribution of the paper.2) When referring to GCNs, the authors cite Kipf's paper, Velikovi's paper and Abu-El-Haija. The GAT architecture in Velikovic's paper is not a graph convolutional neural network, so it shouldn't be cited here (GATs are, probably, the first case of a popular graph neural network architecture that is not convolutional). Also, I believe it is unfair not to cite the two main contributions to GCNs: Bruna's 'Deep Spectral Networks' paper from ICLR 2014, and Defferrard's 'ChebNets' from NeurIPS 2016. (Omission of ChebNets would explain the misunderstanding with respect to the graph convolutional neural networks being low-pass filters). Summary: The paper presents a new way algorithm to compute the straight-through variant of the Gumbel Softmax gradient estimator. The method does not change the estimator's bias, but provably reduces its variance (with a small overhead, using Rao-blackwellization). The new estimator shows good performance on different tasks, and appears to lead to more efficient optimization for lower temperatures (lower bias).Clarity: The paper is well written.Originality: The use of Rao-blackwellization in the proposed way is, up to the best of my knowledge, novel.Pros of the paper and significance: - Relaxation-based gradient estimators are widely used, and the proposed method may their variance quite significantly.- The proposed algorithm has a clear justification from a theoretical perspective, and admits a simple implementation.- The proposed algorithm does not require additional model evaluations, and thus may lead to large reductions in variance without incurring a high computational cost.- The proposed method leads to more efficient optimization at lower temperatures (lower temperature translates to lower bias, but often higher variances).Cons: I'd say one thing that could be included are additional baselines in the experimental section. There are other estimators that may be use. For instance, you could compare against VIMCO. While this is a different type of estimator (non single evaluation, not based on relaxations), it could be interesting to see how the results compare using this estimator too.Recommendation: Accept (reasons in the "pros" list above). Summary:The paper proposed a regularizer loss as an alternative to adversarial training to improve the robustness of neural networks against adversarial attacks. The new regularizer is derived from a second-order Tyler series expansion of the loss function in the model robustness optimization problem. Clear mathematical derivation and thoughtful empirical experimental results are provided. The proposed method outperformed baseline adversarial training methods with better or on part robustness and higher standard accuracy.Pros:- The paper is really well-written and easy to understand. Both the intuition behind the method is conveyed clearly and the potential drawback of the method is discussed with solutions.- The performance of the proposed method is outstanding with a large margin compared with other baseline methods.The experiments are extensive and rigorous. - The idea is easy to implement.- There is a lot of valuable discussion and experiments presented in the supplemental materials. In fact, some might be better to be moved to the main text.Cons:- As also discussed in the challenges in Appendix E6, an expected advance of this approach would be the training efficiency. However it is not well discussed in the main text. It would make the paper an even better one with some efficiency optimization. - I have some concerns about the sampling steps in the regularization loss. Only a single sample is drawn for both eta and z. How does this sample affect the stability of the training? Some empirical analysis on the intermediate values would be nice.- The author mentioned in the appendix that batch normalization layers are removed from the SOAR experiments. Do you also remove these layers from the baseline experiments? If not, how does the removal of BN layers impact the performance?Other detail comments:- I think the clipping of the regularization loss should also be mentioned and discussed in the main text.- Missing space in conclusion: l2 attacks This paper introduces the geometry-aware framework that can be adapted to any existing weight-sharing NAS methods optimized over gradient descent. The authors focus on the aspect of optimizing the architecture parameters to overcome the criticism of weight-sharing methods. The author's method relies on the mirror descent supporting their methods with a theoretical guarantee for the fast convergence. The author also supports their methods on various datasets such as CIFAR-10, ImageNet, NAS-Bench-201 (Dong & Yang), and NAS-Bench-1Shot (Zela et al.).The paper supports their ideas not only supported by theoretical background, but outperforming results consistently with extensive empirical experiments such as CIFAR-10, ImageNet, NAS-Bench-1Shot1, and NAS-Bench-201. Furthermore, the GAEA can easily be applied to existing NAS methods which I believe making this work more valuable. Strength1. Their methods easily apply to existing NAS methods with gradient-based methods. 2. Theoretically supported methods with convergence guarantee. 3. Extensive empirical experiments on CIFAR-10, ImageNet, NAS-Bench-1Shot1, and NAS-Bench-201. Moreover, detailed experiment descriptions and fair setups that are critical in NAS comparison are provided.4. Novel perspective of view (optimization perspective) to overcoming the weight-sharing methods' criticism of recent works. 5. Reproducible code included along with the paper. Overall, I recommend clear acceptance. This paper will provide new insights/perspective to NAS algorithms which adopts weight-sharing methods. Reference1. Dong & Yang. NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search (ICLR 2020)2. Zela et al. NAS-Bench-1Shot1: Benchmarking and Dissecting One-Shot Neural Architecture Search (ICLR 2020) Summary: This paper demonstrates and analyzes the accuracy disparities induced by selective classification on subgroups. It evaluates these induced disparities across five datasets from different domains, and provides a theoretical analysis of the conditions under which selective classification may decrease accuracy or magnify disparities via evaluation of the margin distribution.Overall, this paper is a useful addition to a growing literature on the disparate impact of various machine learning techniques on subgroups. The analysis in terms of margin distribution is both appropriate to the selective classification task, and novel, as far as I am aware. The theoretical analysis clearly describes conditions under which the various observed experimental results may generally hold. The (short) section on DRO also suggests a promising way out of this problem, informed by the results and analysis in the paper. The paper could use some minor edits and clarifications, but I believe it would be a valuable addition to the conference.Note: I did not evaluate or check all of the proofs.#### Major Comments:* I found the description of the baselines confusing; it is not made clear that C and I are being defined (I had to scan to see if they were previously discussed) and a great deal of text is used to describe a baseline construction procedure which could be encapsulated in a simple algorithm (my recommendation) or set of numbered steps.* Throughout, the paper uses a definition of "average accuracy" (defined in Sec. 3) which is not actually average accuracy -- it is average accuracy on covered points. I find this to be a bit misleading, or at the very least a confusing overloading of the term, and would recommend introducing a new name for this metric and re-labeling the y-axis of the plots that use it.* The DRO section (7) feels like an afterthought, but it is actually quite important to the paper. There are several recent works which identify disparate model performance impacts on subgroups (e.g. due to model compression, differential privacy, etc.), with no clear solution. The connection to DRO suggests a potential solution, at least in practice, for the problem identified in this paper. It would be nice to, at the very least, see some of the discussion of DRO upgraded from the appendix to the main text, if not a more thorough analysis.#### Minor comments:* I did not feel that the actual process of training a selective classifier (vs. a standard classifier) was clearly explained; since this is fundamental to the analysis, it would be useful to have this clearly signposted for readers unfamiliar with selective classification (such as this reviewer).#### Typos etc.Sec. 7: "to their average accuracies Figure 5" This work makes a convincing case that we need to trace information flow (or at least, to find clusters of neurons working together) within a deep neural network in order to get the clearest picture of how the network is working.  The approach is quite simple but also quite novel in that it uses concepts from coding theory which are not widely known in the deep learning community.  The paper is well written and presents examples from deep CNNs (~20 layers) trained on ImageNet.Quality:The method is well thought-out and explained, and two different evaluations are used (MNIST and ImageNet).  It would be more impressive if the authors had included a problem from a different data modality since in principle the method is quite general.  The examples look very impressive, but my main concern is with whether the examples could have been cherry-picked, in the sense that most of the thousands of rules produced may not be useful.  Relatedly, I would like to know the reproducibility of the result.  If you train twice with different seeds, how similar are the results?  Or if you fit ExplainNN with different seeds on the same network?  And what is the danger of false positive findings?Clarity:The paper is very clear with regards to the problem setting, previous work, and the methods.  I am somewhat familiar with coding theory, having read much of MacKay's Information Theory, Inference, and Learning Algorithms, but I am by no means an expert in coding theory.  Still, I am confident that I understand the principles of the method.  However, I did not exactly follow how the authors carried out the tracing (pg 7 second paragraph.). Do you just apply ExplainN as usual and then filter for rules that (strictly/non-strictly) include Y?  This seems important to explain since the tracing, in my view, is the main contribution, given that there already exist tools for understanding the similarities of classes such as representational similarity (https://roberttlange.github.io/posts/2019/06/blog-post-3/).  Originality:I have not previously seen the idea of mining association rules for deep neural networks, although it is a simple enough idea and I would not be surprised if the idea has appeared before.  However, the application of MDL to solving the problem for binarized activations is likely to be novel.Significance:The work is promising, but to me it is not conclusive that it will make a lasting impact.  There are a number of important practical questions to be addressed that could make or break the method as a tool for the field, such as the reproducibility and usability of the method (whether most rules produced are meaningful or significant manual filtering is required).  On the other hand, even if the method does not meet practical needs, it could still be of great utility for NN methods researchers interested in investigating, say, the redundancy of specific architectures. the true degree of similarity between two trained models, convergent dynamics between alternative architectures, and the value of overparameterization.  The paper could be even more significant if the authors could comment on the generalizability to other architectures such as RNNs, GNNs or transformers.  The method itself is interesting enough and the examples sufficiently compelling (even if cherry-picked) that I would recommend the paper to almost anyone interested in neural network interpretability.Note on rating:  In the face of limited details, I am willing to give the authors the benefit of the doubt that they did not cherry-pick overly aggressively and that the examples are representative of typical outputs. If it turns out that most rules do not look like the examples, then my rating would decrease.  On the other hand, if the authors can address my concerns about cherry-picking in the response, it is possible that I would raise my rating.Pros: * important application * method is quite general * method is simple and intuitiveCons: * evaluation of method performance limited to selected examples * reproducibility not addressed * control of false positives not addressed * method for tracking across layers not well-explained This paper shows that the class of two-player markets have no satisfactory outcome in the usual sense. Players should neither escape to infinite losses nor converge to strict maxima or non-critical point. Some concrete examples are analyzed with negative results. This paper is a reminder of researchers: we should carefully model the objective functions of multiple interacting intelligent agents and the interactions between them.Weakness:It is better to describe the \alpha and \gamma in Sec. 3.3. This paper presents AVEC, a new critic loss for model-free actor-critic Reinforcement Learning algorithms. The AVEC loss can be used with any actor-critic algorithm, with PPO, TRPO and SAC being evaluated in the paper. The loss builds on the mean-squared-error, and adds a term that minimizes $E_s [f_{\\phi}(s) - \\hat{V}^{\\pi_{\\theta_k}}(s) ]$. The addition of that extra term is motivated by recent research on the stability of actor-critic algorithms, and the benefits obtained by the AVEC loss are empirically demonstrated in numerous environments, with AVEC+PPO, AVEC+SAC and AVEC+TRPO.Quality: the paper presents an interesting idea, that is simple but well-motivated, and leads to encouraging empirical results. Both the theoretical and empirical motivations are strong.Clarity: the paper flows well and is quite clear. However, an intuition for what the added term in the AVEC loss is missing. Section 4.2 motivates the added term in a mathematical way, but a few sentences explaining what the added term does, in simple terms, may help the readers understand why AVEC is a better loss than simple MSE.Originality: the contribution of this paper seems original. It builds on recent work, but the recent work identifies problems while this paper offers an original solution to these problems.Significance: the fact that AVEC provides good empirical results, and can be used as the critic loss of any actor-critic Reinforcement Learning algorithm, points at the high significance of this work. Many actor-critic implementations can easily be improved by using the AVEC loss. Another positive point is that the paper discusses how to implement the AVEC loss in algorithms that fit a neural network on batches of samples. This really helps implementing the proposed loss, that contains an expectation in an expectation and is therefore not trivial to properly implement.In general, I like this paper and recommend acceptance.A few questions/issues:- An explicit mention of the gradient of the loss, or at least a discussion of where to stop back-propagating gradients, would have been interesting. $f_{\phi}$ appears two times in the AVEC loss, and it is unclear whether the loss contributes to gradients in $f_{\phi}$ two times, or if the expectation over states is first computed (without computing any gradients), and then used as a constant in the rest of the evaluation of the loss.- As mentioned in "clarity", an intuition of what the added term of the AVEC loss does, especially since it is "inserted" in the mean-squared-error (inside the square), would help the less mathematics-savvy readers. It is not crucial to understand the paper, but the generality of the approach proposed in the paper may lead it to be used often by students, and so an intuition of why AVEC works and what it does would greatly help. Summary---This paper proposes Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS) for for goal-directed planning problems (i.e. problems where reaching a specific goal state is the objective, like traversing a maze with specified start and goal positions). The assumed setting is one where transition and reward models of the environment are not (necessarily) available, but a low-level goal-directed policy that can attempt to navigate from a given start to a given goal position, as well as value oracle that can return the success probability of the low-level policy on any given task, are available. Planning problems are modelled as AND/OR search trees, where OR nodes are labelled by a start state s and a goal state s'', and AND nodes are labelled by triples (s, s', s''). An OR node has children for every possible state, such that traversing to a child indicates the insertion of the corresponding state as an additional subgoal in between s and s', plus one extra child to indicate the choice of returning the current plan without inserting any additional subgoals. AND nodes have two children; one OR node for the first half (s, s') of the plan, and a second OR node for the second half (s', s'') of the plan. The MCTS can construct a plan by inserting subgoals such that they become easier to solve for the low-level policy by searching this tree.Strong Points---1) Interesting problem setting, way of modelling the problem, and proposed algorithm. Technically sound as far as I can tell.2) Solid and clear writing.3) Interesting results.Weak Points---The second paragraph of the Introduction describes two fundamental challenges in sequential planning; (i) assumption of reliable transition model existing, and (ii) credit assignment problem over long time horizons. Then the next paragraph basically starts out (I'm paraphrasing and probably slightly exaggerating here) that we overcome these challenges by changing our assumptions to include that we are already given a goal-directed low-level policy and a value oracle. This may leave the reader wondering "well what if I don't have them?", or "are there practical scenarios where these assumptions are realistic?". Personally I don't necessarily believe that all research has to immediately have examples of practical applicability readily available, it can still be interesting without it... but in this case, I think it shouldn't be too difficult to actually provide some example situation where these assumptions hold, and including that could make the introduction a bit more convincing. Maybe it's just me, but in particular also the word "oracle" in "value oracle" keeps me scared and wondering for a long time if we're really going to end up needing a hard, ground-truth oracle (unrealistic assumption), only to finally figure out much later in the paper that it's okay for this to just be approximated.Overall Recommendation---I recommend accept. The strong points are clear, and the weak points are quite minor.Questions for Authors---Would it be possible to clarify in more detail how the standard MCTS baseline can be implemented in terms of the Algorithm 1 pseudocode? In Section 5 it's very briefly mentioned that the only changing is restricting DC-MCTS to only expand the "right" sub-problem in line 11 (which I assume should actually refer to lines 9-10). But then G_{left} has no value, and G_{left} is also still required in line 12. Intuitively I think I get the point, that only being allowed to look at the right half of AND nodes forces the algorithm to construct plans again in the same order in which they get executed, and probably by thinking about it more deeply I could figure out how to reconstruct the correct implementation, but either way I feel like it's not super obvious so may be worth clarifying in slightly more detail.Minor Comments---- Final paragraph of Section 1, when listing all the subsequent sections, skips Section 4.- "where \varnothing is them empty" --> "where \varnothing is the empty" (second paragraph 2.1)- "MPDs" --> "MDPs" (third paragraph 2.1)- Even though it's fairly obvious already, I guess I'd prefer "maximizing" rather than "optimizing", to be 100% explicit- At the end of Section 2: "Figure 5 in Appendix 5.3".... but there is no appendix 5.3 and Figure 5 is not in an appendix.- "MDP wit a" --> "MDP with a" (first paragraph Section 3)- "free to chose" --> "free to choose" (A.4)- "procudre" --> "procedure" (B.5) This paper proposes an extension of the Differenciable Neural Computer networks (DNC)In these DNCs, the reading operation on the external memory are done by accessing a single memory block, which represents a single piece of information or knowledge. The architecture proposed in this paper aims instead to give the possibility to access multiple memory blocks at the same time. In this way, the approach of reading memory is more holistic (Chalmers, 1992). This is a desirable feature of distributed representations and, then, distributed memories. Otherwise, these memories are just similar to the classical approach of representing symbols (Fodor and Pylyshyn ,1988). This debate of what is the main charateristic of distributed representations is revitalized in Ferrone and Zanzotto (2020). It is then a needed extension of DNC.The paper is well written and results are convincing.However, there is a minor problem. There is not a direct link among equations in Section 2 and equations in Section 3. Clearly, DNC equations are extended by equations in Section 3. Are these equations linked only with the M, that is the Memory? ReferencesFodor, J. A., and Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: a critical analysis. Cognition 28, 371.Chalmers, D. J. (1992). Syntactic Transformations on Distributed Representations. Dordrecht: Springer.Ferrone, Zanzotto (2020), Symbolic, Distributed, and Distributional Representations for Natural Language Processing in the Era of Deep Learning: A Survey Summary: The paper proposes a network architecture called Matrix Shuffle-Exchange (Matrix-SE) that can learn many logical reasoning tasks on 2D data and graph. It has complexity O(n^2 log n) for 2D input of size n x n, which is much smaller than the complexity of naive attention applied to 2D data (O(n^4)). The proposed architecture is an adaptation of the Neural Shuffle-Exchange network architecture (Freivalds et al., 2019), moving from 1D to 2D data. This adaptation is done by using a Z-order iteration of the 2D input, then performing radix-4 shuffle and radix-4 exchange, instead of radix-2. This model is shown to be able to solve several hard tasks on 2D data, such as inferring algorithms on binary matrices (transpose, rotation, bitwise XOR, matrix squaring), graph operations (component labeling, triangle finding, transitivity), and solving Sudoku puzzles. The experiments show impressive results and the model's ability to generalize to test inputs of larger sizes than those in the training set.======Strengths:1. Wide variety of algorithmic and logical reasoning tasks. I enjoyed reading the experiment section, and the tasks are fun and creative.2. Impressive generalization on larger sizes. On those tasks mentioned above, the Matrix-SE model generalizes well to 2D arrays that have larger sizes than those in the training set, out-performing baselines (ResNet, Graph Isomorphism Network).3. Simple model design. The generalization from Neural Shuffle-Exchange network to Matrix-SE is natural and straightforward.4. The paper is well-written and easy to read.Weaknesses:1. Lack of theoretical characterization of the model. More concretely, what kind of operations can be represented by Matrix-SE? The theoretical motivation seems to be from the classic result that Benes networks can represent any permutation. However, it's not clear how expressive the proposed model is.2. More realistic tasks. Do the logical reasoning tasks in more realistic scenario, such as modeling social networks represented as graphs?======Overall, I vote for accepting. The proposed model is simple, can perform logical reasoning tasks on 2D data, and generalizes well beyond sizes that are in the training set.======Additional feedback and questions:- Do all the matrices in section 5.1 have binary values?- How is accuracy defined in Table 1 and 2. Does the output matrix have to match the label exactly? Or is it accuracy per element?- In Section 5.4, why is Residual-SE so much slower (9x) than Matrix-SE? I think it should only be 2x slower, because the depth of Residual-SE is twice that of the Matrix-SE. The work uses diffusion probabilistic models for conditional speech synthesis tasks, specifically to convert mel-spectrogram to the raw audio waveform. Results from the proposed approach match the state-of-the-art WaveRNN model. The paper is very well-written and it is quite easy to follow. The study of the total number of diffusion steps and two different ways (continuous and discrete) ways to feed it in the network is very interesting. It is quite relevant and important for speech synthesis tasks. Using this, authors are able to find a 6-step inference procedure that yields very competitive performance to WaveRNN while still being computationally feasible.Pros:1. Great results for the neural vocoding task2. Exhaustive study of the diffusion steps and how to feed them to the network is valuable and original.Cons:1. Study of the width/depth of the network can be more exhaustive.*Further comments*: Authors mention in the conclusion ````Wavegrad is simple to train`. What makes the authors say so? It would be great to substantiate it with evidence/comparison? A. SummarizeThis paper proposes to not only maximizes the trace of the projected covariance matrix R but also minimizes the off-diagonal element of R, which helps to recover the real principal components(eigenvectors of the covariance matrix) from data, while the other large-scale algorithms only recover the top-k subspace.Furthermore, the authors utilize the hierarchical relation between eigenvectors and design a utility function for each eigenvector. Therefore, each eigenvector serves as a player in a game and they will achieve strict-Nash Equilibrium at the end, which enables a decentralized algorithm for large-scale PCA problems. In the experiment, the authors conduct experiments on synthetic data, moderate scale data, and large scale data by resnet activation maps. The first two experiments demonstrate that the proposed algorithm is competitive with Oja's algorithm and even better under some conditions. The large scale experiment on resnet activation maps is only feasible by the proposed algorithm and demonstrate that it is a powerful tool to achieve interpretable representation. B. Strength1.  This paper is well organized and easy to follow. The explanation about why other algorithms only recover the top-k subspace but not the principal components is step-by-step. Based on this observation, the authors propose to minimize off-diagonal elements and derives the utility function, which adds a generalized Gram-Schmidt step to the gradient and can be decentralized naturally. 2. Besides the algorithm itself, the proposed method also opens new doors for other interesting future research other than recovering principle components.3. The experiments are simple yet sufficient to demonstrate the superiority of the proposed algorithm. To the best of my knowledge, the proposed algorithm is the first that dealS with a problem as large as in the resnet-200 experiment. C. Weakness:1. My only question is that the proposed algorithm focuses on recovering the real principle components and finding interpretable features. So it would be good if we can see some comparison of the lower-dimensional features in some downstream applications. For example, can we cluster the input data into meaningful clusters better than other algorithms?D. Justification of score:This is a great paper that gives a new perspective on PCA and derives a novel decentralized large-scale algorithm and will inspire a lot of further research along this line. So I vote to accept this paper. The paper under review studies the question of whether gradient descent can solve the problem of calibrating a deep neural network for separating two submanifolds of the sphere. The problem studied in the paper is very interesting and as been the subject of recent increasing interest in the machine learning community. The contribution is restricted to a simple set up and addresses the question in the finite sample regime. The framework of the analysis hinges on the Neural Tangent Kernel approximation of Jacot et al. This extremely technical paper is indimidating and I wonder how many readers will actually read the proofs. Moreover, one may wonder if the NTK approach precludes a deeper understanding of the actual performance of the network under consideration.    This paper studies the problem of universal approximation with networks of bounded width and arbitrary depth. The objective is to understand what's the minimum width necessary to approximate any function in a suitable space. The main results of the paper can be summarized as follows:(1) ReLU networks approximate functions in $L^p(\mathbb R^{d_x}, \mathbb R^{d_y})$ if and only if the width is at least $\max(d_x+1, d_y)$. This result is tight and improves upon (Kidger and Lyons, 2020), which give an upper bound of $d_x+d_y+1$.(2) The same result does *not* hold if we look at the approximation in $C(K, \mathbb R^{d_y})$, $K$ being a compact. The author(s) prove that the minimum width is 3 by giving a counterexample. (3) In order to maintain the width of $\max(d_x+1, d_y)$ in $C(K, \mathbb R^{d_y})$, it suffices to use ReLU and threshold activations. (4) An upper bound on the width of $\max(d_x+2, d_y+1)$ for approximation in $L^p(K, \mathbb R^{d_y})$ is given for a wide class of activation functions. The proofs contain two main elements of novelty: (a) The upper bounds (results (1)-(3)-(4) above) rely on what the author(s) call a 'coding scheme'. The input is mapped to a one-dimensional codeword by an encoder; the codeword is mapped to a one-dimensional target by a memorizer; and the target is mapped to vector close to the output by the decoder. The idea is that all these three maps (encoder, memorizer and decoder) can be constructed with neural networks of width $\max(d_x+1, d_y)$ by using ideas of prior results, e.g., (Hanin and Sellke, 2017). The point of the coding scheme is to decouple the input and the output dimension: the original mapping from a space of dimension $d_x$ to a space of dimension $d_y$ is broken into (i) a mapping from dimension $d_x$ to dimension 1, (ii) a mapping from dimension 1 to dimension 1, and (iii) a mapping from dimension 1 to dimension $d_y$. This is what allows to improve the bound on the width from  $d_x+d_y+1$ to $\max(d_x+1, d_y)$.(b) The counterexample on which the lower bound is based (result (2) above) comes from a topological argument. The paper is well written, the results are interesting and strong, the proof techniques are novel. Thus, I am generally positive about the submission.I have a few questions/remarks:(Q1) This is a general question. The author(s) provide a general upper bound on the width of $\max(d_x+2, d_y+1)$. Is that tight for a sub-class of activations? Any comment on how to improve it beyond ReLU?(Q2) Lemma 5. Does the set $\mathcal S$ (or, equivalently, the choice of $a_1$, $a_2$, $b_1$, $b_2$) depend on $\phi$? It looks like this is the case, and it would be better to clarify this point.(Q3) Section 5.2. It is mentioned that "by the definition of $\ell^*$ and Lemma 5, there exists a line intersecting with $\mathcal B$". How do you guarantee the intersection with $\mathcal B$? The set $\mathcal S$ in Lemma 5 is generic (and, in principle, may not give rise to an intersection). (Q4) In Lemma 6, the author(s) talk about a bounded path-connected component without defining what path-connected means.  I spotted a couple of typos:(T1) Page 6. "memorizer_{K, M}" should be "memorize_{K, M}".(T2) "differentiable at at least". This occurs several times in the main text and the appendix as well. #### SummaryThe submission tackles the problem of cross-domain few-shot learning in a setting where unlabeled data is available for the test domains.It introduces an approach called "Self Training to Adapt Representations to Unseen Problems" (STARTUP) which first pre-trains a teacher model on the (labeled) base dataset, and then distills the teacher model into a student model (initialized with the teacher parameters and a random output layer) on the base set of classes as well as the (unlabeled) target domain dataset. For the target domain examples, the loss is a combination of the KL-divergence between logits output by the student and teacher models and an additional unsupervised/self-supervised loss function (SimCLR in this instance). For test episodes, STARTUP is free to choose any applicable inference approach, and the paper opts to fit a linear classifier on top of the frozen feature extractor.The proposed approach is evaluated on BSCD-FSL using a random 20% subset of each target domain to form unlabeled datasets and the remaining 80% to form evaluation episodes. STARTUP is compared with recent cross-domain few-shot classification approaches (which do not use unlabeled target domain data) and a purely self-supervised baseline which ignores the base dataset and learns a representation from the target domain using SimCLR.The submission also investigates two hypotheses to explain why STARTUP improves performance, and concludes that its purpose is *not* to simply introduce noise during training and instead appears to emphasize the natural groupings induced by the base classifier on the target domain data.#### Strengths and weaknesses* **+** Clarity: the paper is well-written and easy to follow.* **+** Cross-domain few-shot classification is very relevant to the few-shot learning research community, and the availability of unlabeled data for the test domains is a plausible assumption.* **+** The proposed approach is sound and straightforward.* **+** The paper makes an effort at explaining why STARTUP provides a performance improvement.* **-** The paper should be more rigorous when reporting which approach performs best in a given setting.#### RecommendationI recommend acceptance. Overall the paper is clear, the problem being tackled is relevant to the research community, the proposed idea is sound, and evaluation is rigorous. My main concern has to do with the way in which best-performing approaches are reported, but thats easily fixable in a subsequent version of the paper.#### Detailed justificationI appreciate the quality of the writing. The proposed idea is straightforward and makes intuitive sense. The baselines chosen for comparison are reasonable.The main concern I have is with the way in which results are reported in Table 1. I believe the authors bolded the best-performing entry in each setting without taking the 95% confidence intervals into account. As an example, can we say that STARTUP performs significantly better than Transfer in the ChestX 5-way 1-shot/5-shot settings when their 95% confidence intervals overlap as much as they do? In my opinion the more rigorous way to determine that would be to run a 95% confidence statistical test on the difference between the means and bold all entries for which the test is inconclusive in rejecting the hypothesis that the difference in mean to the best-performing entry is zero, like is done in the Meta-Dataset paper. This doesnt change the main conclusions drawn by the paper, but should nevertheless be addressed.#### Questions1. When training the student model, would there be a benefit to compute the loss as the KL-divergence between the label distributions output by the teacher and student models instead of the cross-entropy with the true label? Have the authors investigated this?2. The proposed approach has the downside that it requires training and storing a separate student model for each target domain. Have the authors thought of more compact approaches, like domain-conditional self-training?#### Additional feedback1. The abstract mentions evaluating on a "challenging benchmark with multiple domains" but does not name it (BSCD-FSL). This is information that would be helpful to the reader.1. The insistence on "several hours of compute" in the introduction as being a drawback of current recognition systems could be toned down. To me, training a model for several hours doesnt sound unreasonable.1. The Visual Task Adaptation Benchmark (VTAB) and Big Transfer (BiT) papers would be relevant to mention in the related work section. In particular, the VTAB paper investigates various representation learning strategies (including self-supervision) when transferring between very different base and novel domains.1. The strategy to use logits on the label set of the base dataset as targets (and therefore leveraging the natural groupings induced by the base dataset classifier) reminds me in a way of Ngiam et al. (2018)s work on "Domain Adaptive Transfer Learning with Specialist Models", which uses this to inform the weighting of classes used to pre-train a target domain-aware classifier on the base dataset. This papers proposed approach differs in many ways, but the similarities would be interesting to expand upon. Authors assessed how these subnetworks can be as diverse as independently trained networks. The contribution of this paper is in proposing an approach to improve uncertainty estimation and robustness with minor changes (1 percent) to the number of parameters and compute cost. STRENGTHS: Training multiple independent subnetworks within a network, with minimal increase in number of parameters. The use of MIMO makes this approach simple, while it can be evaluated in a single forward pass.  CONCERNS:The authors claim that the benefits of using multiple predictions can be achieved for free, while their proposed model increases the number of parameters (even though by 1 percent)The paper has examined the accuracy and disagreement of the subnetworks, but a detailed evaluation on number of parameters is missing (i.e. where the 1% increase in parameters comes from).An experiment on more diverse datasets would be also helpful, such as OpenImages.  Amari's Natural Gradient has been very successfully applied for policy optimization, e.g. in a recent line of work by Schulman et al. These benefit of using these natural gradients that restrict the change in KL divergence between successive policies was more stable and faster convergence. However two distributions may differ a lot in terms of KL divergence but because of the dynamics of the MDP they may still have almost the same behavior, therefore recent work has focused on using the Wasserstein distance to measure the divergence between successive policies which naturally leads to "Wasserstein Natural Descent". In this paper the authors build upon recent work on Kernelized Wasserstein NGD by making it more widely applicable and scalable. Moreover they present a good empirical comparison between KL-NGD and Wasserstein-NGD on a combination of pedagogical toy problems and some standard RL benchmarks from OpenAI gym. Overall this paper will be a good contribution to the conference and I recommend acceptance.**Corrections and suggestion for improving presentation**1. [Citations] Page 2, third paragraph from bottom cites Schulman 2015 instead of Schulman 2017 for PPO. Page 4 last paragraph has a citation missing and Li and Zhao "Wasserstein Information Matrix" paper is cited twice. 2. I think it will be better to write down that equation (5) defines $f_u$. Also shouldn't the $1/2$ factor in equation (5) be actually $\beta / 2$ ? 3. On page 3 it is said that "the penalty only accounts for global proximity in behavior .... " in reference to equation (4), but PPO is not implemented with a single value of $\beta$, i.e. the strength of the penalty typically varies as optimization goes on. Why can't the same be done for the weight of the $W_2$ penalty ? ##########################################################################Summary: The paper proposes an algorithm to improve on a 'semi-performant' policy on a real world system from logged data of this policy driving the system (offline setting).The algorithm build a MPC based controller using as ingredients learnt models of the dynamics, reward and value function along with a clone of the semi performant policy.The algorithm mixes in an original fashion the following ingredients- soft reward weighted trajectory averaging to select the next action from sampled trajectories (PDDM).- reuse of past MPC trajectories (linear mixture with next guiding samples)- guided shooting with a cloned policy (POPLIN) - learning of ensembles to capture a notion of learnt model uncertainty (PETS)##########################################################################Reasons for score:  The combination of the aforementioned ideas to the problem of offline policy improvement is original.The paper is well written, the related work section is thorough the results  convincing. ##########################################################################Remarks: 1. The model based approach to off-line learning is a very promising avenue. The explicit separate representation of reward and dynamics provides great flexibility as shown in the adaptation to new objectives or additional constraints. 2. The paper is close to PDDM as acknowledged by the authors in the introduction sections. The ablation study is interesting and shows that the behavior cloning is the key factor in the algorithm.The reader is referred to Figure 2 and the results not discussed at all. I guess it is a space issue, but I'd like to read it. 3. In the ablation section, the author say they recover PDDM when removing the policy and the value function.I would suggest you give a more central place to this statement.The RL literature is (incredibly) messy and discussing relation (inclusion, generalization, special case) between methods when available should be highlighted more.#########################################################################Some typos: (1) sec 2.1 'aim to to' ##########################################################################Summary:This paper proposes a simple yet general approach for exploration in discrete-action problems. The proposed approach, called ez-greedy, combines randomly selected options with the well-adopted e-greedy exploration policy to achieve temporally-extended e-greedy exploration. The paper overviews the publicized exploration methods from the perspective of their inductive biases, and clearly states where the inductive bias of ez-greedy would be better suited over e-greedy. The paper reports results in tabular, linear, and deep RL settings, on numerous domains ranging from classic toy problems to Atari-57. The results are interesting, and the analysis aligns and supports nicely the narrative of the paper.    ##########################################################################Reasons for Score:Overall, I vote for accepting this paper. The idea is simple (a generalization of e-greedy) and the discussions nicely illustrate the main properties of an ideal generally-applicable exploration method. The experiments clearly show where ez-greedy exploration would be useful. Also, they show that the inductive bias of ez-greedy does not hurt much the performance in simpler dense-reward domains while more specialized algorithms suffer significantly.  ##########################################################################Pros:See "Reasons for Score" above.##########################################################################Cons:1) The results in Atari are based on a deterministic version of Atari (i.e. not using "sticky actions"). Also, in DeepSea the deterministic version of the task is used. Ideally, I would've liked to see empirical results in stochastic domains as well. More importantly, I'm not sure why only deterministic domains are used?2) The literature on action-repeats are discussed briefly. But it's hard to know how the former related works were different in their formulation and use of action-repeats. Also, could you clarify how sticky-actions are positioned w.r.t. ez-greedy (beyond that the purpose behind sticky-actions was to induce stochasticity in the environment as opposed to being used explicitly for exploration)? For instance, do sticky-actions actually improve learning performance in the same domains were ez-greedy improves performance?3) The rainbow + e-greedy vs. Rainbow + ez-greedy Median and Mean plots do not show significant findings. I think a bar-plot should be added to show per-game relative human-normalized improvements for these versions. The same should be done for R2D2 (e-greedy) vs. R2D2 + ez-greedy as well. I think what this could reveal is symmetric bars over the 57-Atari games (i.e. number of games in which ez-greedy outperforms and underperforms e-greedy are the same). Also, the extent of improvements on average is the same as shown in the Mean plot of Figure 8. To clarify, I don't see an issue with this outcome (i.e. if the bars are symmetric; meaning overall there are as many games in Atari-57 that would benefit from ez-greedy over e-greedy as there are games in which the opposite is the case). This does not go against the narrative of the paper which makes it clear that they each have an inductive bias that suits some tasks over others. But I think this should be made super clear in the results section, through such bar plots. For the same reason, I think the Mean plots should also be brought to the main text and shown next to the Median curves.   4) Why only 5 random seeds in DeepSea? I suggest showing results for 30 randoms seeds like in the other toy problems.  ##########################################################################Questions during the rebuttal period:Please address and clarify the "Cons" above.##########################################################################Minor comments:- It would be useful to replace "Rainbow" with "Rainbow (NoisyNet)" in Figure 3 so as to emphasize the difference between "Rainbow" and "Rainbow + e-greedy". Similarly, for "R2D2" it'd make it easier for the reader if the Figures show "R2D2 (e-greedy)". - Table 1: "Algorithm (@200M)": M doesn't need to be italicized (to be consistent with "Algorithm (@30B)").- It'd make it easier if "(100%)" is added to the y-axis of Median/Mean plots. **Summary:**This paper offers a critique of current exploration techniques as being overly complex and engineered to only work on specific tasks. As an alternative, the paper proposes temporally extended $ \epsilon$-greedy exploration which maintains the simplicity and generality of $ \epsilon$-greedy while offering better . More specifically, the proposed algorithm simply repeats the randomly chosen action for a random number of steps (where the number of steps comes from a specific distribution), this is a specific instantiation of the more general algorithm presented in the paper where any set of semi-markov options can be used.--------------------------------------------------------------------**Strengths:**1. Clarity. This paper is very well-written and clear, making it enjoyable to read. It sets up the shortcomings of prior methods and offers a simple solution. I also especially appreciated the clear discussion of the limitations of the proposed method.2. Strong critique of prior methods to provide motivation. It is an important observation that while many exploration methods are developed in the theory and deep RL communities, they are often inferior in practice to simple strategies like $ \epsilon$-greedy. While this is not a novel contribution, this paper really drives home the point by providing a slightly smarter variant of dithering that competes favoriable with much more complicated algorithms. This is an especially important contribution of this paper since it makes the point to the RL community that simple exploration strategies may be more effective in practice, but there is still room to innovate while maintaining simplicity and generality.3. Strong empirical results. The experiments clearly show an improvement over $ \epsilon$-greedy in small benchmark problems. Then, they demonstrate how $ \epsilon z$-greedy even improves over more complocated exploration strategies for deep RL algorithms applies to atari relative to more complicated exploration algorithms like RND (at least in the "average" case, but not on "hard exploration" games like Montezuma's revenge).   --------------------------------------------------------------------**Weaknesses:**1. The theory could be tightened. The paper would be stronger if the theorem were stated more formally (defining polynomial sample complexity) and the proof provided the specific results being used from the cited papers (maybe as lemmas in the appendix). At a more substantive level, it is not clear how exhaustive the list of desired properties of an exploration algorithm is. The paper lists three desiderata for an exploration strategy: (1) that it is simple, (2) that it is stationary, and (3) that it promotes full coverage of the state-action space. Each of these goals makes sense, but the paper does not provide any framework to explain why these are a necessary or sufficient set of properties to yield the desired behavior. Moreover, it is not clear what the tension or tradeoffs are between the properties. A more clear discussion of these issues or formal framework could go a long way toward clarifying the landscape of exploration algorithms. --------------------------------------------------------------------**Recommendation:**I reccomend accepting this paper and gave it a score of 8. I think the paper provides a clear argument for simple and general exploration strategies and that $ \epsilon z$-greedy seems to be an algorithm that achieves these goals. Moreover, I think that the paper makes an important point to the community working on exploration algorithms that the complicated algorithms being developed can often be beaten by simple strategies when considering a broad range of problems.--------------------------------------------------------------------**Additional feedback:**- One reference that I think should be included when discussing learning temporally extended representations of actions is [1].- Typo: line 2 of the last paragraph on page 1 should be "such a compromise".- The discussion of the choice of distribution over durations was somewhat abrupt. This is an interesting part of the algorithm and it would be nice if it was fleshed out a bit more. [1] Whitney, W., Agarwal, R., Cho, K., & Gupta, A. (2019). Dynamics-aware Embeddings. *arXiv preprint arXiv:1908.09357*. This paper proposes a reliable multi-view classification mechanism equipped with uncertainty, called Trusted Multi-View Classification. The goal is to dynamically assess the quality of different views for different samples to provide reliable uncertainty estimation. The idea is clear and well-motivated. The authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective, robust and reliable. Strengths: + It is interesting to conduct multi-view classification by dynamically integrating different views at an evidence level, which provides a novel and flexible way in multi-view classification.+ The way of using Dempster-Shafer theory for integrating evidences in a unified and learnable framework is quite neat. + The paper is well-written and clearly presented. + Strong and sufficient empirical results are provided.Minor comments: + It is reasonable to use the subjective logic theory to directly model uncertainty, however, beyond the advantages mentioned, it will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided. + The authors could present failure cases which may be associated with high uncertainties (ideally).+ For the results (Table 2) of the end-to-end experiments, are the data used original or being corrupted manually? Overall, the paper is very well motivated and easy to follow. The assumptions and decisions are well supported. The stepwise experiments are helpful and provide good insights to evaluate the proposed algorithm. The method seems to be of great potential in real-world (cost-sensitive) applications.  The paper gives thorough intuitions, theoretical arguments and synthetic examples that demonstrate the shortcomings of off-manifold Shapley values. It then proposes two approaches to overcome this limitation. Specifically, they enable scalable sampling from the on-manifold conditionals. The first approach is unsupervised based on VAEs and model-agnostic. The second approach builds upon the supervised model. The latter is shown to be more accurate and data efficient. Both approaches are shown to fix the limitations of off-manifold Shapley values on a real-world MNIST example. The authors provide implementation detail that will be useful for practitioners.Thanks for this a well-written, well-motivated easy-to-follow paper.Minor comments.- add to the intro a defintion of "data splicing"- formalize problem of off-manifold shapley values- It would be great if the authors released code for their experiments. The paper proposes a neural topic model derived from the perspective of optimal transport (OT). Topic embeddings are learned as part of the training process and is used to construct the cost matrix of the transport.  The cost function based on the OT distance is further improved by combining with the cross-entropy loss and by using the Sinkhorn distance to replace the OT distance.The paper is well written.  The proposed method is first explained from the perspective from the optimal transport and then developed by considering the cross-entropy loss and the Sinkhorn distance.  It also explains how the model incorporates the word embedding and introduce the topic embedding to simply the cost matrix M of the transport.  The proposed method is sound.The experiments included recent neural topic models for comparison.  The chosen test data sets include also some with short text. In general, the proposed method has been show to perform better than other methods in terms of topic coherence and topic diversity.  The experiment section also some quality analysis on the topic discovered by the proposed model.  The experimental results are convincing.The paper explains quite clearly the relationship between the proposed method and related methods. In particular, the paper seems to have adequately credited the sources of ideas during the development of the proposed model.  The novelty of the proposed method appears to be the use of optimal transport for developing neural topic model and the construction of the cost matrix M based on the cosine similarity between word embedding and topic embedding.  The novelty is sufficient.Although the clarity of the proposed method is good, the rationale for using OT distances for comparing probabilities may deserve more explanation.  The paper gives an intuition for the transport matrix P in section 2.2.  It may be better if it can also give more explanation on the role of M and what the meaning of the distance between two probability vector is.  References may also be given to other works that use OT distances for comparing probabilities.The experiment includes recent neural topic models but does not include other recent topic models not based on neural networks. It would be interesting to see how the proposed method performs compared to those models.I cannot find any indication that source code will be released.  It is suggested to do so for reproducibility and for the use of practitioners.Minor comment:- Add the data set name to the caption of Figure 4. This work introduces MUSIC, a framework for intrinsically motivated RL, where the intrinsic reward comes from maximizing the mutual information between the agent's state and the surrounding environment's state. The authors motivate and describe this approach, explain its incorporation into various training modes, exhaustively characterize its properties, and compare to numerous related past approaches. Although it is somewhat specific to particular environment domains, MUSIC offers a compelling addition to the family of intrinsically motivated RL algorithms based on concepts of mutual information.### ClarityThis paper is, for the most part, a model of clarity. The proposed algorithm follows from a clear intuition and the results present a detailed and organized characterization/validation. The authors could further improve clarity by adding a bit more exposition around the training of the mutual information estimator. This is only a minor issue, since the mutual information framework itself is well established by now. Still, it would help to more concretely describe the implementation details covered in Section 3.2, perhaps in another small section of the Appendix.### QualityThis paper is very high quality. The experiments are thorough and well organized, addressing an impressive number of (literally enumerated) questions. This also serves to demonstrates an impressive versatility of MUSIC, the proposed technique, while simultaneously enabling comparison to a **wide** range of past methods and showcasing a variety of potential uses. However, there is a lack of guidance around practical challenges facing this technique and its potential pitfalls.  For example, Campos et al. (_ICML_ 2020) describe some failure modes associated with simultaneously learning networks for estimating mutual information and using them to train policies. It would be valuable to know if MUSIC has similar (or otherwise noteworthy) failure modes.### Originality and SignificanceAs the paper describes, there is a large body of work on intrinsically motivated RL as well as intrinsic rewards derived from mutual information-based objectives. In many cases, the types of behaviors those alternative approaches would hope to encourage are the same as those learned by MUSIC. From what the paper demonstrates, it seems like MUSIC is a more successful iteration of these attempts. That is not meant to diminish its significance -- it's a very hard problem! Ostensibly, MUSIC incorporates the right inductive bias through its decomposition of the state. I expect that the field will find both the technique to be useful as well as the general insights brought about through this work. My only concern regarding the significance is whether MUSIC is only applicable to a relatively narrow set of domains. Even if we assume that the state can be cleanly decomposed into the agent/surrounding constituents, how might MUSIC handle things like partial observability or other types of uncertainty that may affect estimations of mutual information? The paper may benefit from a brief discussion around generality and, if appropriate, how future work may address any issues therein.**Pros**- Simple extension of an existing framework leading to clear and versatile improvements within a challenging problem- Paper is exceptionally clear and well-organized- Impressively thorough experimental characterization/validation- Code is provided to help improve reproducibility and external adoption**Cons**- Some gaps in practical guidance and discussion of potential pitfalls- Generality of the approach (with respect to environment/task setting) is a bit unclear This paper proposed an inter-training framework of BERT, an unsupervised training method applied between the conventional pre-training method via Masked Language Models and the fine-tuning method by using labeled data in the target task. The proposed inter-training methods are an unsupervised method by first running clustering with BoW features for the target task and then fine-tuning by training BERT over pseudo-labels generated by clustering results. This inter-training framework significantly improves prediction accuracies especially when the task is topical, i.e., the task to classify texts based on a high-level distinction related to what the text is about, and the labeled data is scarce.This paper is well-written. The motivation is reasonable and the proposed methods make sense. Experiments are comprehensive and analyses are well designed. The contribution of this paper is obviously above the ICLR borderline.I think it is better if the authors mention the computational cost of the inter-training framework in detail. While the computational cost of the clustering is negligible, the authors did not mention the computational cost of the fine-tuning in the inter-training framework. Compared with the final fine-tuning process, it is better to clarify how much additional cost we need to perform the inter-training method. Moreover, it is better if the authors mention the effect of the size of unlabeled data in the inter-training method. Is more unlabeled data improve target metrics a lot, or are hundreds of texts sufficient for the inter-training method. It is better if the authors show the plot by changing the size of the unlabeled data used for the inter-training method. Post-training quantization is an important problem, especially for industry. This paper leverages the basic building blocks and conducts a block-wise quantization. Nice results are obtained with the proposed method. The proposed method is cheap to implement and pushes the post-training quantization to 2-bit. The measurement problem of mixed precision literature raised in this paper is of insights. This problem may inspire the community to find a better measurement in future work. Extensive experiments on various methods (handcrafted and designed by NAS), various tasks (classification, detection), various configurations (different bits, latency, model size) are impressive. Various baselines are also included to make the results stronger. Questions:The block-diagonal scheme is selected according to experimental results. Is it possible to visualize the real Hessian of stage-wise settings? (it may be impossible to the full Hessian matrix for the whole network). If we see a few non-zero elements at the off-diagonal for the block-wise setting, this choice can be better motivated.  Recent literature proposed that even label smoothing improves the teacher model, it will hurt the distillation training of student models due to the information erasing. Although this idea dominated more and more literature, this paper argued that this observation is not entirely correct. In order to clarify this idea, the paper systematically discussed the correlation between knowledge distillation and label smoothing. Comprehensive experiments well support the claims in this paper, i.e. label smoothing is compatible with knowledge distillation. The correlation between label smoothing and knowledge distillation remains an open question to date, and this paper made a breakthrough regarding this question. Besides the main purpose (clarify previous ideas), this paper also provided multiple interesting empirical conclusions, e.g. a better teacher always leads to a better student by producing more informative distillation labels, the distillation itself can provide enough regularization for training and the hard-label classification loss is no more needed. To conclude, the paper overturns the previous perspective with convincing explanations, discussions, and experimental results. Several empirical discoveries are introduced, which are expected to have high impacts on the tasks of knowledge distillation. The major contributions of this paper can be concluded as:1) The paper empirically confirmed that label smoothing is well compatible with knowledge distillation, overturning previous dominant ideas. This is an important finding because it can prevent subsequent research from being misled.2) It further explained the phenomenon of relative informative erasing, which only happens on the semantically different classes. Thus previous lopsided ideas (label smoothing hurts knowledge distillation) can be well explained.3) The paper claimed that the dominating factor in knowledge distillation is the performance of the teacher and further proposed a stability metric to measure the quality of supervision. This metric is crucial in the tasks of knowledge distillation since it provides a simpler and faster way to measure distillation quality. The paper also claimed that the distillation loss itself can provide enough regularization, which also inspires me a lot.It's quite a good empirical paper and I really enjoy reading it. I think there's no significant weakness on it, so I recommend a clear acceptance for this paper. This paper presents a prototype-based method for data augmentation based on a generative model without rule/template based requirements. The generative model creates new input-output pairs from training fragments (recombination: rewrite model conditioned on multiple examples), and samples in low-density places (rare words) of the training data (resampling). Empirical results show that the in combination recombination and resampling perform on par with a recently introduced rule-based method, GECA.Experiments are conducted on two compositional generalization tasks: SCAN and sigmorphon. The paper is very clearly written and motivated, doing a good job in presenting the recent and past pertinent literature. The problem addressed is of great interest, and the two proposed contributions of resampling and recombination are likely to be useful to further research in data augmentation. The extension of prototype models to multiple examples is a promising step, but depending on the task leaves open questions. The empirical results on SCAN are strong. The results on Sigmorphon are strong in the sense of obtaining comparable accuracy to simple rule-based approach, which itself is very simple and has many incorrect examples it constructs, but do not clearly outperform it.Granting resampling is a contribution, wouldnt the proper comparison be GECA resampling with the recomb-1 or -2, since those include resampling as well? In that case the Sigmorphon performance is much closer to each other for the two methods. Taking the performance jump from 1 to 2 prototypes on SCAN as potential for further jumps, why restrict to n=2? And related, why do you think -2 outperforms -1 in some instances on Sigmorphon?   This paper considers the optimization of a wide two layers neural network (for a regression task) using averaged SGD. The authors consider the Neural Tangent Kernel (NTK) regime. The NTK is a kernel defined using the activation function and the initial distribution of the parameters of the input layer. The RKHS H associated to this NTK is assumed to contain the Bayes predictor. Based on this, the authors derive a convergence rate for the predictor constructed from the T-th iterate of averaged SGD and the Bayes predictor in terms of the L2 distance wrt the distribution of the features. By specifying this bound in terms of the decay of the eigenvalues of the integral operator in H, they obtain an explicit generalization error bound, which is optimal for the class of problems considered in the paper.It seems that the paper solves an important problem related to the training of neural nets. The paper is rather well written even if some paragraph are hard to understand for a non expert. For example, several paragraphs of the paper are dedicated to compare the obtained results with those of concurrent work. The level of technicality of these discussions makes the reading experience difficult (e.g. last paragraph of Page 6), often because the discussion happens at a step where the reader is not familiar with the results (e.g. Section 1.2). These paragraphs seem like a discussion with the authors of these concurrent works. I would suggest to gather these discussions at the end of the paper, once the reader understands the results. Moreover, a better approach in my opinion would be to explicitly state the results (mathematically) of these concurrent work. This way, the comparison will be easier.As a non expert, I believe that the results are new but I cannot be sure because I cannot compare with existing works (see my comment above). I recommend acceptation.Overall, the paper shows an important result: optimal rate for generalization bounds for 2 layers NN in the NTK regime. The result is well explained but some precision could make the paper even more insightful. For instance, why considering the NTK regime? What is the intuition behind that? How would you define mathematically "the NTK regime" ? I would also like to understand better the relaxation of the positivity of the NTK. Does it have to do with the assumption that the norm of the integral operator is greater than lambda?Moreover, Proposition A, which is fundamental in the approach, should be stated in the main paper for a better understanding. I think that the authors can move the numerical experiments to the appendix to win some space.MINOR:Check the def of excess risk, last equation of Page 3"negative dependence on" should be "inverse dependence on"", That is, "Why is Figure 1 in Section 1? Is it a mistake? It should not be placed here. Moreover it is not commented in the text. Here is the review of the article: ``Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime''.**SUMMARY**The authors show that under the Neural Tangent Kernel Regime investigated lately, averaged SGD achieves optimal rates in the attainable case.  Note that this result is not a *plug and play* based on the current kernelized-SGD litterature: the difficulty the authors achieve to overcome is the fact that they could bound the difference between the dynamics of SGD on the neural networks and on the neural tangent kernel. This is stated in Proposition A and it represents the novelty of the articleMoreover, they give an explicit representation of the capacity condition (decrease rate of the eigenvalues of the covariance matrix) in the cases where they have a smooth approximation of the ReLu.**Clarity**The paper is outstandly clear. Indeed, despite the fact that optimality in RKHS can be technical to introduce, I found the paper very clearly presented and well motivated. It was very pleasant and smooth to read. The references are also both precise and sufficient to understand well the problem.The only default of the paper is that, in my opinion, the authors do not stress enough their contribution and their novelty. Indeed, despite Proposition A and the sketch of the proof, the article consists in a *plug and play* result on averaged SGD. The authors should then stress the outline of the proof (going to a M-approximation of the Neural Tangent RKHS) and comparing the dynamics on these.**Quality and Originality**The paper is not super original, and as accustomed to this literature, there is no surprise seeing this result. However, the quality of the paper is undeniable and fills the gap between optimality of kernel methods and the NTK literature. I thank the authors to have done it very clearly.**Comments**-My main comment is about the fact that except from Proposition A, this article is a *plug and play* one. This proposition, and the full sketch of the proof should be emphasized as they consist on the true novelty of the work.-Three remarks concerning the plots :1-*Minor comment.* They should be bigger.2-*Minor comment.* Figure 1 should illustrate the fact that $\beta = 1 + \frac{1}{d-1}$. I suggest a log-log plot.3-*Intermediate comment.* I do not really see how exactly the discussion of the experimental part illustrates really the result. The discussion is fairly interesting, but I really would like to see a theoretical proposition showing that taking the two layers is better for learning.*** Conclusion***Yet the fact remains that, I would really like this article to be published when the commentaries of the reviewers will be taken into accounts.  This paper analyzed the averaged SGD for overparameterized two-layer NNs for regression problems. Particularly, they show that the averaged SGD can achieve the minimax optimal convergence rate, with the global convergence guarantee. To achieve, they propose a new parameter which captures the ``complexities of the target function and the RKHS associated with the NTK.The paper is well-written, and the result looks very interesting. I am tending to accept the paper. This paper is a theory, so experiments are a plus. If the authors can address some of my comments, I am tending to increase the score of the paper.Here are some comments about writing.I believe Assumption 1 and 2 are reasonable. But each statement is just math, it is good to write a 2~4 words to summarize each A1, A2 .. Also several math statements highly replied on the definitions, and it is hard to find them in the paper.After Assumption 1, in the next page, page 5, there is a Remark that has 4 bullets, maybe write Ai at the beginning of each bullet.Algorithm 1 requires more words and explanation, it is hard to understand this algorithm in the following sense : the size of each matrix/vector is not mentioned and hard to find them in the paper.In page 6, is it possible to simplify the statement of Theorem 1 a bit? e.g. write a simplified version here, and put the full version in appendix.In Theorem 1, what is M_0? Is that over-parameterization size? Is that polynomial in parameters or exponentially in parameters? (This is not major point of the paper, I am just curious about the bound) In page 4, Eq. (2), is it possible to consider a simple model where gamma = 0? This is quite common in previous work.In appendix, e.g. page 30, the last step of many equations use ->0. I dont follow the meaning of this notation. Is that possible to avoid it?This paper is focusing on average SGD, is there any intuition why non-average SGD wont give the similar result?I felt the following paper is highly close to this work, and should be cited and discussed more deeply. Usually optimization has two parts, one is the number of iterations, and the other is cost per iteration. This paper focused on improving the number of iterations. The following paper improved the cost per iteration, in the NTK overparameterized regime. Training (Overparametrized) Neural Networks in Near-Linear TimeJan van den Brand, Binghui Peng, Zhao Song, Omri WeinsteinMinor commentsIn page 1, second paragraph, the place cited Du et al. 2019b, Allen-Zhu et al. 2019 and Du et al. 2019a.The following two papers should also be citedZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural networks.  [This paper shows the result for recurrent neural networks. Note that RNN is  a harder case, In deep neural networks the weight matrices in different layers are different. However in RNN, the weights matrix are the same over all the layers]Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.[This paper improved the over-parameterization bound from m >= n^6 (Du et al. 2019b) to m >= n^4, where m is the width of a neural network, and n is the number of input data points.]In page 2, the first paragraph, the place cited Du et al. 2019b, Arora et al. 2019a, Weinan et al. 2019, Arora et al. 2019b, Lee et al,. 2019.The following papers should also be citedJason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. [Arora et al. paper shows a connection between neural networks with neural tangent kernel regression. This paper generalizes the Arora et al result, and shows the connection between regularized neural networks with neural tangent kernel ridge regression.]Similarly, in page 5, some citations should be added.Small typos:The last paragraph, Page 2 the key to show -> the key to showingThe third paragraph, Page 3 which enable -> which enablesThe fifth paragraph, Page 4  A stochastic gradient descent -> Stochastic gradient descentThe third paragraph Page 5 a neural networks -> a neural networkThe third paragraph Page 6 arbitrary small -> arbitrarily smallThe first paragraph Page 8 the single-layer learning -> single-layer learning Overview =The paper introduces an approach that, given a set of "basis" policies, constructs a high-level policy from the basis policies that is able to perform well in a variety of distinct (but related) tasks. Such tasks are described by MDPs with similar state-action spaces and similar dynamics, and differing only on the reward functions, all of which are built as a linear combination of common features.Given a set of policies, the paper introduces the notion of "set improving policy" as a policy that outperforms any policy in the given set on the family of considered tasks. It provides two examples of such policies (SMP and GPI) and formalizes the problem of computing a SIP with maximal worst-case performance on the set of considered tasks as a max-min problem. It then contributes an incremental algorithm for this problem. The proposed approach is tested in a grid-world environment and the DM control suite.= Positive points =The paper is very well written, with the proposed approach clearly motivated, presented and analyzed. The proposed approach is novel, to the extent of my knowledge, and analyzed both theoretically and empirically.= Negative points =My main criticism is, perhaps, some lack of detail on the experimental evaluation -- particularly in the DM control suite.= Comments =Overall, I really enjoyed reading the paper. The problem addressed -- that of building a policy that performs well in a number of related tasks from a set of "simpler" policies -- is, in my view, quite relevant for the RL community, and has potentially interesting applications in domains such as robotics.The proposed approach is, as far as I know, original and contributes to the state of the art. The paper briefly links its contributions to the existing literature on apprenticeship learning and hierarchical RL, but I would have appreciated some more discussion on these topics -- particularly, I'd like to better understand how the learned policy relates with policies taught through apprenticeship learning.Overall, the ideas in the paper are presented in a very clear and elegant manner and the results strike me as technically sound. The proposed approach focuses on building a set of "basis" policies in such a way that the policy built from them performs as well as possible in all the considered family of tasks. The method is derived from first principles, and the performance bounds provided (framed in terms of the performance of the SMP policy) are then validated empirically.Finally, the paper is evaluated in a smaller grid-world domain and in the DM control suite. One aspect that could, perhaps, be improved is concerned with the description of the empirical evaluation in the DM control suite: the paper does describe how the family of rewards for these tasks were built, but it would be good to provide some description of the types of policies that the different rewards lead to and how the policy computed by the proposed approach relates (or differs) from them. The paper considers stochastic gradient descent convergence in a distributed setting with m workers, where up to ± workers can be Byzantine, i.e. perform in an arbitrarily adversarial way. In this setting, they develop a variant of SGD which finds a second-order stationary point, prevents Byzantine workers from significantly affecting convergence, and achieves ±^2 + 1/m speedup compared with the sequential case. The main idea of the algorithm is to measure deviations of gradient updates for a certain number of rounds and detect Byzantine machines which must have a significant deviation to noticeably affect the algorithms behavior.If Im correct, Lemma 3.1 allows a much simpler proof:|\sum_{t=1}^{T-1} (¾_0 + & + ¾_{t-1}) * Delta_t|<= |\sum_{t=1}^{T-1} ¾_t| * |\sum_{t=1}^{T-1} Delta_t| + |\sum_{t=1}^{T-1} ¾_t * (Delta_1 + & + Delta_t)|The first term is estimated as a sum of independent Guassians. The second term can be estimated using Azumas inequality (its a martingale since E[¾_t] = 0 and ¾_t is independent on ¾_1, &, ¾_{t-1}, Delta_1, &, Delta_t). We can bound their norms of ¾_t by O(log (T/p)) with probability 1 - p/T.Questions:In Jin et al. (2019), dependence on d can be avoided when an additional assumption (Lipschitz stochastic gradient) is made. Can this work use this assumption? I believe that footnote 3 on page 4 talks about this assumption and argues that it may be too strong for practical applications, but I dont see a reason to not get a result with this assumption. Are there technical obstacles?Assumption 2.1, both items: the bounds are typically taken in expectation: E[||f(x_t) - _ti ||^2] <= Ã^2. Can this paper handle this kind of assumption? At the very least, one cant immediately detect byzantine machines that deviate from the mean by more than Ã.Algorithm 1, line 11: it shouldnt matter, but isnt it more natural to take an average over good_{t+1}, not good_t?Page 13, last equation: while the inequality seems to be true (since ¾_t are independent Gaussians), I dont see how it follows from Lemma 4.2.Page 14, Equation B.2: isnt this way to bound the sum too loose? Whats the intuition behind this approach? Can we get a better bound with some other approach?The following parts would benefit from an additional discussion:While its clear that ±^2 + 1/m comes from bounds on Ã_t and Delta_t, the intuitive meaning behind the ±^2 term is not clear. The reason why ±=1/sqrt(m) is a threshold is also not clear.While it may be clear from the algorithm (namely how the median behaves), I think its also worth explaining why the algorithm doesnt significantly degrade when ± is close to ½, unlike what one can expect.I believe that the following places would benefit from the further expansion:Page 13, bound on ||Delta_0 + & + Delta||^2: its better to explicitly write relation between B^t and Delta_t.Page 15, for some vector ||¸_t||: I would explain the bound.Page 15, È_t is zero except in the first coordinate: I would explain why.Page 16, right after M is introduced: I would explain the first transition.Minor issues:While assuming that smoothness constants are 1 slightly simplifies the presentation, I believe that it makes some transitions harder to understand. E.g. bound on ¸_t on page 15 would be more clear if the Hessian Lipshitz constant was in the equation.nu should be an input/parameter of Algorithm 1Page 13, proof of Claim B.2: its said that the proof is by induction, by I dont think you use induction anywhere.Page 13, Proof of Lemma 4.3: Lipschitz smoothness -> smoothness? The authors apply iterated learning - a procedure originating in CogSci analyses of how human languages might develop - to the training of neural module networks. The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures (in past work, such explicit pressures have generally been necessary). The proposed approach brings substantial improvements in systematic generalization across two datasets, SHAPES and CLEVR.Strengths:1. The approach is well-motivated, including impressive coverage of prior literature in both ML and CogSci.2. The approach brings impressive gains in an area that is one of the major weaknesses of current ML systems, namely systematic generalization. 3. In addition to the gains in accuracy, one particularly impressive benefit of this approach is the decreased amount of supervision that it requires compared to past approaches.4. The paper is generally well-written and easy to follow. Weaknesses:1. One of the motivations is to expand the use of iterated learning beyond toy datasets. While SHAPES and CLEVR may be not as toy-ish as datasets used in the past, they still are pretty toy-ish, so Im not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains.2. Though the paper in general was very clear, I found Section 3.2 to be a bit hard to follow, and that section is important as it is the part that describes the structure of the iterated learning framing. I think this section would benefit from starting each subpart with a more high-level, intuitive description of what that stage accomplished, before diving into the details.Minor comments:1. Fodor et al only has 2 authors - Fodor and Pylyshyn2. Page 3: Although, the Gumbel straight-through estimator: this use of although is usually frowned upon - better to use However3. Page 5: typo: minimzing4. In general, for the bibliography, check to see if a paper has been published at a conference or journal; if so, cite that version instead of the arXiv version. E.g., Neural machine translation by jointly learning to align and translate. was published in ICLR 2015, and Systematic generalization: what is required and can it be learned? was published at ICLR 2019. The main question this paper tackles is: can one develop sample efficient architectures for graph problems while retaining the simplicity of the message passing framework?While combining message passing with GNNs has shown to have positive empirical results,  we do not know of a general neural architecture that is  versatile enough to provably encode the solution space of a variety of graph problems such as shortest paths and minimum spanning trees. This paper introduces a theoretically principled architecture -- GCN+ -- which is attempts to make GCNs more efficient by using ideas from the subfields of sketching approximations and parallel computing. ############I vote for accepting the paper due to its novelty and the pros listed below.  ############Pros+ An interesting paper with novel contribution combining ideas from parallel computing and sketching approximations. + Major algorithmic contribution of interest to practitioners + Theoretical contributions in the form of several theorems in the paper Cons - No code provided with the submission #### Summary and contributionsThis paper proposes a generalized framework for score-based generative modeling (SBGM). The proposed method subsumes previous SBGM techniques of score matching with Langevin dynamics (SMLD aka NCSN) and denoising diffusion probabilistic modeling (DDPM) and shows how they correspond to different discretizations of Stochastic Differential Equations (SDEs). The  continuous-time SDE generalizes the idea of a finite number of perturbation kernels used by previous methods to a continuum of them. The authors propose a forward SDE that transforms the data distribution into a known noise distribution and the corresponding reverse-time SDE that converts samples from this noise distribution to the data distribution. A predictor-corrector sampling framework is studied that leads to improved performance of both NCSN and DDPM frameworks. The paper also shows the equivalence of the proposed SDE to Neural ODEs which allows exact computation of the log-likelihood using the continuous change of variables formula. Quantitative experiments on the CIFAR10 dataset show that the proposed framework leads to significant improvements over previous SBGMs. Qualitative results on the CelebA-HQ dataset demonstrate the ability of the method to scale to high resolution images.#### StrengthsThis paper makes significant technical and empirical contributions to the emerging area of score-based generative models. The generalized SDE framework subsumes recent works in this area and is also connected to Neural ODEs, enjoying exact likelihood calculation, which may be relevant to the normalizing flows and generative modeling community is general. The empirical evaluation is particularly well-done. It bridges the gap between the performance of NCSN and DDPM models leading to state-of-the-art performance. The authors also demonstrate the ability of the method to generate high quality images of human faces when trained on CelebA-HQ dataset. Preliminary experiments on class conditional generation, imputation, and image colorization demonstrate the wide applicability of the proposed method.#### WeaknessesThe paper does not suffer from any obvious weaknesses. The quantitive experiments could be strengthened by the addition of results on another dataset but the empirical evaluation is sufficient in its current state.#### Additional feedbackQuestions:- In equation 11, how is the weighting function $\lambda$ chosen?- In equation 11, apart from being able to sample from the transition kernel, it should also have a closed-form density for the evaluation of the score. Is my understanding correct?- In equation 21, should there be a discretization step-size corresponding to $\Delta t$?- In table 1 (a), why does SMLD with corrector only perform so poorly? As far as I understand it is equivalent to NCSN. Can the authors clarify if I misunderstood something? This paper generalizes a family of score-based generative models that rely on sequences of noise scalings of the data and extends them to the continuous domain, which leads to an SDE-based framework. By using score-matching, the forward SDE, which transforms data into a tractable noise distribution, can be reversed and thus used as a generative model. This is then improved by employing a two-phase algorithm with a prediction step, followed by a tunable number of correction steps. Further, reformulating the problem as a neural ODE allows for exact likelihood computations and reduces the number of required function evaluations. The framework enables unconditional, as well as conditional samples.I find the paper to be very well written and straightforward. As someone who does not have neural SDEs or Langevin Samplers as a core competence, I was able to follow all of the writeup, which is remarkable. I think the framework is nice and there is substantial novel innovation to justify accepting this paper. The experiments are convincing.A few questions and remarks:- You claim that you unify current methods into a common framework. While I see that you attempt to do this (i.e. putting the algorithms side-by-side, etc.), but in essence, you still handle VE and VP SDEs separately throughout. My suggestion would be to either really try to unify them into a single formulation, or alternatively, tone down the claims of unification, maybe just say that you show commonalities.- In Figure 2, you claim that the results are best when computation is "split" between the predictor and corrector. However, this is a very imprecise statement. An equal split would mean just 1 step of corrector, but I don't see clear evidence that that's best. Do you have numerical evidence that a 1-to-1 split is best, or what do you mean by "split"? Otherwise, you could just say that M is a tunable hyperparameter.- Also in Figure 2, it seems that there is a clear shift at some point where the samples go from low to high quality. Do you have any numerical indication (without looking at a test set, FID, etc.) of how a practicioner could notice that running for more steps would or wouldn't help?- In Table 1, what is the last row? I guess it's just employing the corrector, but maybe a label for the row would be nice.- Also in Table 1, it looks like the corrector helps for VE SDEs, but hurts for VP SDEs. Do you have an explanation for this? Maybe it's somewhere in the text, but if it is, I've missed it, so maybe point me to it.- Given that you can compute exact likelihoods, is it possible to compute the exact NLL for any real dataset, like a test dataset? This paper studies the effects of random action + observations in reinforcement learning, and proposed to use partial trajectory resampling to improve off-policy algorithms such as SAC. The proposed method performs strongly in multiple environments augmented with random delays, and beats baseline SAC and RSAC in both sample efficiency and final policy performance. Pros:(1) The methodology is sound, and the performance gaps between the proposed method and baselines are significant, especially when the tasks become harder as delay increases. (2) The experiments are comprehensive and demonstrate the potential of this algorithm. Cons:(1) The paper introduced a critical concept: partial trajectory resampling in Sect 3.1. I find that this section is generally not easy to read given the packed symbols. In addition to the equations, it would be better to add a figure illustrating the recursive sub-sampling process. (2) The paper only augmented SAC. However, in theory this paper should apply to other off policy learning algorithms such as TD3. It would be more comprehensive to try such studies. (3) Clearly the baseline SAC suffers in environments with large latencies and the learning is slow. In these scenarios I am interested in seeing an on-policy baseline such as PPO/TRPO, which generally seems to be more robust to state/action delays. Conclusion:Please address my concerns raised in the "cons" section. In this work, the authors introduce a method called LIME for imparting certain mathematical inductive biases into a model. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction. Each of these pretraining tasks is a sequence-to-sequence mapping involving 3 basic components of reasoning (Rule, Case, and Result), where two of these three components are provided as input and the third component is the target output. After pretraining on these tasks, the authors fine-tune on 3 different proof datasets, and find that the pretraining almost always improves performance, sometimes by a large margin.Strengths:1. This approach is creative and thought-provoking; pretraining is an important topic in ML nowadays, and this paper gives several interesting insights about how to structure pretraining. Therefore, publishing this paper at ICLR could help inspire others to use and develop improved variations of pretraining.2. One aspect of the pretraining that I found particularly impressive was how the authors found such clear improvements from such small amounts of pretraining. This is in stark contrast to the usually massive pretraining datasets that are used, and stands as an especially strong piece of evidence for the models usefulness.3. The experimental setup is well-motivated, drawing on a principled analysis of the problem domain. 4. The paper is overall clearly written and clearly structured.5. There were some interesting discussion points and ablation studies analyzing the approach in more detail. I particularly liked the discussion about how loading the vocabulary weights had little effect, showing that the inductive biases that were imparted were abstract in nature. It was also useful to see that LIME was more useful than other pretraining tasks, ruling out the possibility that you could get similar improvements from just any pretraining task.Weaknesses:1. Part of the papers motivation for imparting inductive bias through a dataset, rather than through an architecture, is that designing an architecture strongly requires human insight. This is true, but LIME also seems to strongly rely on human insight, so this point is not a benefit for LIME over architectural approaches. This is not a huge problem, but it does not seem like a great motivation for LIME. 2. Related to the previous point, it would be good to discuss the fact that the usefulness of LIME may be limited by the need to design the right pretraining task(s). As Table 4 shows, the nature of the pretraining task is very important; and although the authors were able to create some successful pretraining tasks for mathematical reasoning, it might be harder to create similarly useful tasks for larger-scale tasks in, e.g., language or vision. Again, this is not a huge problem, but I think it at least deserves some discussion.3. Though the goal of the approach (if I am understanding correctly) is to give inductive biases for induction, deduction, and abduction, the paper gives no direct evidence that it has done so: The authors create an approach *intended* to impart certain inductive biases, and this approach improves performance on 3 tasks that plausibly would benefit from those biases. But this result does not necessarily mean that the model has the inductive biases that were intended to be imparted; its possible that LIME imparted some other inductive biases that are also useful for mathematical reasoning but that are not related to induction, deduction, and abduction. Thus, there is a bit of a gap between the motivation and the actual experiments.4. Its not entirely clear to me that the specific tasks (Deduct, Induct, Abduct) will necessarily enforce the types of reasoning that they are intended to enforce. For instance, consider the following input/output example: {A : a, B: b, C: d+e} <s> A A + B = C -> a a + b = d + e. Such an example is intended to show deduction, but it could instead be viewed as induction (where A A + B = C is the Result, a a + b = d + e is the Rule, and the Case dictionary should be read in reverse, treating the values as keys and the keys as values). Thus, related to the previous point, I think there is some concern that the LIME tasks may not necessarily encode the intended primitives. The results show that the LIME tasks clearly encode something useful, but its not clear exactly what useful things they encode. Recommended citations: (you definitely dont need to include all of these or even any of these, but Im pointing to them just in case theyre useful):1. You already cite the GPT-3 paper (Brown et al.), But it might make sense to cite it in a second place as well, for the sentence where you say However, there is another potential advantage of pre-training--it may distill inductive biases into the model that are helpful for training on downstream tasks. Another paper you can cite for this point is this one: Can neural networks acquire a structural bias from raw linguistic data? https://arxiv.org/pdf/2007.06761.pdf2. Like your approach, the following paper also uses carefully-constructed synthetic datasets as a way to impart targeted inductive biases into a model. (However, they use these tasks for meta-training, not pre-training): Universal LInguistic Inductive Biases via Meta-Learning. https://arxiv.org/pdf/2006.16324.pdf. This paper might also be useful as an example of how you can address the last two points I listed under weaknesses, as this paper gives examples of how to test whether a model has some specific inductive biases; the paper I linked to in the previous bullet (Warstadt and Bowman) also does this. (However, adding such analyses might be more work than would be doable for a camera-ready).3. It might be good to cite Peirce when first mentioned in the intro; right now, the citation to Peirce is buried deep in the paper, after he has already been discussed at length.4. Some more potentially-relevant examples of architecturally encoding inductive biases for math: https://arxiv.org/pdf/1910.02339.pdf, https://arxiv.org/pdf/1910.06611.pdf Other comments (these are not things that have affected my assessment. Instead, they are just comments that I think might be helpful in revising):1. Note that there is another approach in ML called LIME, which could potentially cause confusion. Its completely up to you, but I would consider renaming to avoid confusion. Here is the other LIME by Ribeiro, Singh, and Guestrin: https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?casa_token=VrGSeKoqOnkAAAAA:tmzXq2uCWkUVyPdd9ytCNK4LSdRfIwsIeX4hd8EMkjnjevZ4d-rCeIIM7acIRWGtQlQemUqDlAJx-Q 2. Abstract: neural architecture should be neural architectures3. Abstract: on three large very different mathematical reasoning benchmarks should be on three very different large mathematical reasoning benchmarks4. Abstract: I did not understand what dominating the computation meant until I read the rest of the paper.The intro says It is commonly believed that the benefit of pre-training is that the model can learn world knowledge by memorizing the contents of the natural language corpus. This statement seems strong - I am more inclined to think that much of the benefit comes from learning linguistic structure, not world knowledge. So it might be safer to reword as saying One plausible explanation for the benefit of pretraining is&5. Page 3 says the BERT pretraining objective, which suggests that BERT is the objective. But BERT is a model, not an objective; the objectives are masked language modeling and next-sentence prediction.6. Table 1: The formatting of the table makes it look like the first two rows are numbers copied from Li et al. But from the prose of your paper, and from looking at Li et al, Im pretty sure that these numbers are from your own re-implementation. Is that correct? If so, it might be best to format the table different - using the citation within the body of the table gives a strong suggestion that the numbers come from Li et al., in my opinion.7. Table 4 and Table 5: In the caption, say what task these results are for, so that the table can be understood on its own.8. Please double check the references: Several of them seem to only list authors, title, and year when there is at least an arXiv version that could be listed as well. E.g., Mathematical reasoning via self-supervised skip-tree training, Enhancing sat solvers with glue variable predictions, transformers generalize to the semantics of logics. Also, where possible, cite a papers actual publication venue instead of arXiv - e.g., the Raffel et al. T5 paper appeared in JMLR, not just arXiv. Summary: Overall, I am rating this an 8 because I find the strengths compelling but think that the weaknesses in framing hold the paper back from an even higher score. I would consider increasing the score if those weaknesses were addressed, though those weaknesses are deep enough that it would be hard to properly address them in time. Summary:The paper provides an interesting (and to my knowledge, novel) approach for learning a mapping of actions and observations from one domain/character to another reasonably similar domain/character. This mainly allows the transfer of skills (i.e. policies) from one domain/character to another. Immediate use cases of this approach are in imitation learning and sim2real transfer. In order to learn these mappings, the authors use the idea of cycle-consistency in generative adversarial networks and adapt it to the imitation learning task. Importantly, this allows them to obviate the need for paired state samples across domains. The authors show that their method not only works across modalities (vision from real robot to states in simulated robot), but it also works to some extent on different character morphologies.Reasons for score:Overall, I vote for accepting this paper. To my knowledge, the method is novel and provides a viable and interesting approach for imitation learning and sim2real transfer. The experiments and the final quality are also high, however, the broader applicability to more challenging tasks and its limitations remain to be seen.Cons:- The chosen tasks are (understandably) simple, therefore the applicability of the methods to more challenging environments remain to be seen.- The limitations of the method are not discussed well. I believe some commentary on the challenges of this method (e.g. the ease of training with GANs) would be useful. Also, more discussion on the use cases of the method and where it excels existing methods is missing.- The code is not provided which hampers reproducibility. I strongly suggest providing the code if possible. Question: - How does the method handle partial observability? In the robot arm example, a single image of the robot does not contain information about the velocity and angular velocity (of the joints). I'm confused as to how the model can actually infer these or work without knowing them.- Can you spend more time explaining Figure 3 (c)? It seems strange that the "L1 error" increased when more data was available.- Related to the last question: in this case, can you pre-train G the same as F and use cycle-consistency only for H and P? A brief explanation would suffice.Fixes or suggestions:- I will have to double check the conference style guides, but having tables and algorithms intertwined with the text in a single-column publication is not pleasing to the eyes. I suggest rearranging these elements.- The plots in Figure 3 can be improved. At the very least, the distortions caused by resizing the images should be addressed. The authors present a universal medical attack method that can consistently produce adversarial examples across several medical imaging domains. The authors achieve this by developing a novel objective function that includes two terms, which they refer to as stabilized medical attack (SMA). The first term is the loss deviation term, inspired by the conventional fast gradient sign method, which enlarges the difference between CNN predictions and ground truth labels. The second term, a regularizer, is the loss stabilization term that enforces consistent predictions between the adversarial image and the Gaussian smoothed version of the adversarial image. The authors then provide an insightful interpretation of their SMA loss via KL divergence. The derivation demonstrates that perturbations consistently move towards a fixed location in the SAM objective landscape during successive iterations of gradient ascent. This method increases perturbation robustness by overcoming huge variations that result from different types of medical imaging data. The authors provide an illustrative figure (Fig.2) to demonstrate that both the variance and direction of the adversarial perturbation remain stable and consistent across multiple iterations, compared to using the deviation loss alone. The authors then perform an ablation study to demonstrate the DEV + STA loss results in a significantly greater reduction in model performance across medical imaging datasets compared to DEV loss alone. Finally, compared to the state-of-the-art adversarial methods, SMA results in the greatest reduction in performance for all datasets.Pros:This is an excellent paper. Succinct and clear, addressing a known problem with any CAD system. They provide good justification of the technique. They provide sufficient mathematical detail to follow the KL divergence derivation. Both empirical studies are sound and demonstrate the expected results. Additional examples are provided in the supplementary materials. The methods are correct. The empirical methodology is correct. The paper is generally clear.Cons:It would be helpful for the reader/audience to have an associated figure that graphically demonstrated the point made in 3.2. There is a compelling geometric intuition behind the idea expressed in the text above Sec 3.3 that would help present the contribution of adding the STA term. # Paper Summary The paper uses a combination of visual, human gaze and human motion sensors to build representations that perform better on downstream tasks such as action recognition, physics prediction and depth estimation than representations extracted from solely visual input. The paper announces the release a new data set of aligned visual images, eye gaze fixations and IMU motion readings from test subjects walking around an environment. Representations are computed using three different forms of information simultaneously. Given a visual input, the system tries to predict the location of eye gaze in image frame coordinates, whether each of 6 groups of motion detectors are active or not (head, torso, legs, etc.) and the result of a more traditional auxiliary visual pretext task. In this work, the paper uses instance discrimination where representations of augmented versions of a specific image are pushed close together in latent space and far away from augmentations of other images. Tests on diverse benchmarks show that the gaze and motion prediction improve over visual pretext tasks alone and that there is a small benefit to using both together, but it is not additive. The paper also shows the benefit of gaze and motion is present for two different visual auxiliary tasks.# Pros and Cons The paper defines a new way of obtaining self-supervised representations for a variety of core computer vision tasks which is important and highly relevant to the ICML community. The paper clearly describes prior work and situates its contributions well within this space. The paper evaluates the proposed augmented loss function on a diverse collection of standard benchmarks to test their hypothesis including scene classification, action recognition, future motion, walkable surfaces and depth estimation. Table 1 provides a clear ablation study showing that predicting gaze and motion improve downstream performance on diverse benchmarks by non-trivial amounts:  ~7% , 3.5%, 1%, 1% and reduce RMSE  on depth estimation by 2% or so.  In the first benchmark, adding either attention or movement increases results by 6%, but adding both only improves results by 7%, not 14%! This suggests that there is a lot of redundant information between gaze and motion sensors which is a surprising as superficially they seem like very different modalities. It is interesting and surprising that eye-gaze, where a person is looking in an image, is a useful feature for tasks such as depth prediction. This suggests there are features that are easy to compute (compared to interactions between robots and physical world)  that significantly improve network intuitions about what is important in images.  From table 4, it seems like Torso movement is more informative for scene classification, neck more informative for action recognition, arms for dynamics and arms for walkability. It is interesting that different parts are more or less informative for different tasks.  I am a little unsure what inference to draw about the results in table 2 (experiments on Lnce vs. Lae). I guess the main point is that the addition of gaze and motion also improves Lae results, so the effect is not specific to Lnce? It is interesting that gaze and motion are not as helpful with Lae ( 3% gain vs 7% with Lnce) but it is not clear why. Why would there be better complementarity between gaze, motion and NCE? Some thoughts on this would be interesting. # Recommendation  Accept - it is an interesting and surprising result that adding self-supervised tasks for other modalities significantly improves self-supervised representations and that gaze and motion are redundant with each other.   # Questions  How are alpha, beta and gamma set? Hmm, see from appendix A.4.1 these are 0.09, 0.01 and 0.9 respectively indicating that visual loss is the primary driver here and the others are acting more like regularizations on the representation. It is a bit surprising that the other modalities have so large an effect given their small weights in the loss function. Were any other forms of regularization used such as dropout or weight decay?  Didnt see anything about this in the appendix. Section 5.3.2 It seems surprising that gaze, a 2 dimensional quality needs a 512 D embedding. Is this some sort of one-hot encoding over a matrix of locations?  # Feedback  Section 3: feature extractor to embed a more detailed representation & I dont know if it is more detailed, but hopefully, it is more invariant, at least to the augmentations applied, ideally to all non-semantic attributes of the image. I guess it is implied that the baseline vis approach in the paper  is identical to the vis in He et al 2020, but it would be nice if this was explicitly in the caption for Table 1.    ##### Summary:This paper presents Random Kernel Attention which is based on replacing the kernel function in the Linear Attention with random projection kernels. In general, I think the method is novel and quite impactful. Nowadays, some people are still staying away from attention because of its quadratic time and space complexity. To the best of my knowledge, it is the first attention method with linear complexity that can match or even outperforms the conventional attention.##### Strengths:- The method is intuitive and interesting to me.- The results are strong. Unlike Linear Attention, the proposed RFA outperforms the original multi-head attention baseline on both LM and MT tasks. This is quite impressive. Based on my experience the Linear Attention with ELU non-linearity can bearly match the performance of the original attention mechanism.- The authors provide several in-depth analyses in the appendix. I like the experiments in C.2.- It is nice to see that the authors confess that the training is actually increased when using the RFA compared to the original Transformer. Usually, the inference time and memory usage are more important in practice.- It is great to see that the authors compare with the baselines that cache the query/key/value representations. Nowadays, some papers avoid it make their speedup look better.##### Weaknesses & suggestions:- It is not clear what D is used in the experiments. The authors just vaguely say that they don't observe the improvement by setting it great than 2d. However, it would be better to see plots at least in the appendix. Also, I wonder if it would behave differently with different d. Also, it would be great to see what exactly the number is in the experimental setup to make this paper more reproducible.- The arccos feature maps have only D-dimensional features, unlike the Gaussian feature maps which have 2D. It is not clear whether the authors use the same D for both variants or double the D of arccos to keep the feature dimensions the same.- After introducing the random projection weights, the number of parameters would increase. It would be better if the number of parameters and the inference speed are both provided in Tables 1 & 2.- The authors should clarify that the time complexity in Table 3 is based on the assumption that we have infinite number of threads or GPT/TPU cores that can be scaled up when M is increased. Otherwise, the time complexity of training the softmax model is still O(M^2) because there is a matrix multiplication between matrices of sizes M-by-M and M-by-d. ##### Questions:- Based on the experiments, it seems that the Gaussian random feature maps don't really try to approximate - How would the gating mechanism perform on the encoder side? Similar to BiLSTM, half of the dimensions can be applied in a backward manner to make it bidirectional.- Do you resample the random weights during the time? - I wonder if the authors will release their implementation. Based on my quick re-implementation, the proposed RFA doesn't really converge on some other dataset. I believe there might be some differences in how the parameters are initialized which is not clearly described in the paper. Admittedly, there is a chance that I have a bug in the code.- Based on the conclusion in C.2 that RFA is not approximating the softmax kernel, would it be better if we just trained those projection matrices instead of fixing them as random matrices? The paper presents a linear time and space attention mechanism based on random features to approximate the softmax. The paper is clearly written and easy to follow. The results are convincing: not chasing SOTA, but comparing to sensible baselines, namely [Baevski & Auli 2019] for language modeling on Wikitext-103, and [Vaswani et al. 2017] for machine translation on WMT14 EN-DE/EN-FR and IWSLT14 DE-EN.The difference between theoretical speed-up and experimental speed-up is honestly discussed, and the overhead of the random features is not swept under the rug. However, having an experimental study on the random features dimensions' impact on empirical compute time vs. approximation performance vs. end-task performance would have been a plus.I had read "Rethinking Attention with Performers" when I reviewed this paper, and I originally thought it was the same paper, but (along with notation being different) they start to differ on page 3. Where "Performers" goes with positive orthogonal random features (to improve over vanilla RFA), this paper adds a gating mechanism: this adds the possibility to learn some monotically decaying attention over older context, similar to learned receptive fields of attention (as in e.g. [Sukhbaatar et al. 2019]).Overall, this is a good paper, and I don't see why we should downplay it in light of simultaneous ("Performers" got on ArXiV on September 30th, the ICLR deadline was October 2nd) quite similar contribution that the authors took the time to discuss. (It would be even better if they could compare to it in a future version.) This paper presents empirical evidence that non-stationarity data typical in deepRL settings can affect the intermediate representation of deep neural network and affect testing performance. The paper is easy to read and the authors provide experiments to support the their observations and claims. Overall I think this is a good paper and in the following I suggest some good to have additions.(1) The examples for the supervised learning setting clearly demonstrates the impact of non-stationary data. However, given that this is inspired by the problems under DRL setting, it will be interesting to do more analysis of this effect on some DRL tasks. For example, an analysis for offline RL might be a good setting to study this effect.(2)Imitation learning algorithm like Dagger might be another good example to demonstrate the effect of nonstationarity. The data under the Dagger setting is also changing overtime and it will be interesting to see how it affects the student policy.(3) The RL experiment is mainly done in the on policy (PPO) settings. Some experiments with off policy RL setting might be useful, and the effect of the non-stationarity might be more pronounced as well. **GENERAL**The paper claims that high quality of generated samples and SOTA bpds are achievable by VAEs if the model is deep enough (deep in terms of the number of stochastic layers). The authors explain the architecture that resemblances the U-net architecture, and explain its building blocks. Interestingly, they are able to learn VAEs with up to 78 stochastic layers, and achieve SOTA bpds on CIFAR-10, ImageNet-32, ImageNet-64, FFHQ-256 (5-bit), and setting a great result on FFHQ-1024 (8bit).**Strengths:**S1: The authors are capable of training VAEs with over 45 stochastic layers (up to 78).S2: The proposed architecture does not contain any extra "tricks", it is relatively simple. This is a great plus for the paper!S3: The presented theorems are interesting additions to this rather practical paper.S4 The experiments are well performed and the ablation studies are insightful.S5: Generated images are of very high quality! Even a reflection in a glass of a generted lady is better than samples of CIFAR10 in many papers.**Deficiencies:**D1: The prior is not explained in the paper! Without this information, it is hard to properly understand what kind of problems occur during training q(z|x) and p(z).**Remarks:**R1: The proposed heuristic method for training q(z|x) reminds of the following paper:He, J., Spokoyny, D., Neubig, G., & Berg-Kirkpatrick, T. (2019). Lagging inference networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534.It would be interesting to compare at the conceptual level both heuristics.R2: It seems that the authors do not use BatchNorm. Is it correct? This would be also interesting to discuss, because in the following paper:Vahdat, A., & Kautz, J. (2020). Nvae: A deep hierarchical variational autoencoder. arXiv preprint arXiv:2007.03898.the BatchNorm is indicated as an important component for achieving a deep VAE.**Questions:**Q1: What kind of prior was used in this paper?Q2: Is BatchNorm indeed irrelevant for VAEs? **summary** the paper puts forward an idea that deep-enough VAE should perform at least as well as autoregressive models. Authors explore this in the context of image generation, and construct VAE model that is a generalisation of typical autoregressive architectures. They use several tricks to ensure stable training of very deep VAEs and show that final performance exceeds all autoregressive models. This experimentally supports their claim that very deep VAEs encompass autoregressive models.**pros**The idea of perceiving VAE architectures as strictly more powerful and potentially efficient is very appealing. Given the recent work on improving deep VAE training(like Vahdat & Kautz (2020)) this paper takes another step in this direction by effectively, as it seems from the text, removing the depth limitation for training such VAEs. The tricks used to stabilise training are pretty ad hoc, but their effectiveness, showed experimentally, is important in advancing the field.**cons*** The main criticism I have is around ablation studies that justify the proposed architecture choices and training stabilisation tricks, as well as comparison to other tricks in the literature (e.g. Vahdat & Kautz (2020)). Of course the positive result speaks for itself, but the paper would be even more convincing with some details on the exploration that led to the final model.**questions*** it would be good to clarify in the text how exactly sampled latent variables from lower layers are decoded into the images to produce Fig. 4: is the idea to pass those latents down the top-bottom path and just not add new latents in the node "+" within the topdown block?* In Section 5.2.1, it is unclear why models with 32x32 and 1024x1024 resolutions have equal number of parameters: is this because ResNet blocks used at different resolutions share parameters?* Did the authors experiment with methods of slowing down the training of the prior, other then stopping it for the first half of training? It seems that exponentially averaging prior parameters might be another way of doing it, although the exponent will become another hyperparameter.**comments*** Further investigating the relation between using NN interpolation in upsampling and having active latents in all layers would be very useful. * I particularly enjoyed the perceptional shift that the paper advocates for, i.e. that VAE and autoregressive models are not competing approaches, but rather VAE is a more general one and it encompasses the latter. ##########################################################################Summary: The paper proposes an interesting way to do imitation learning without using an adversarial framework. The proposed approach involves density estimation to learn a surrogate reward function that can be optimized via RL. The approach is motivated by recently proposed maximum occupancy entropy RL and extends this to the imitation learning setting. The authors derive an efficient policy optimization method that outperforms existing approaches for imitation learning.##########################################################################Reasons for score:  This paper provides strong theory and strong empirical results validating the theory. GAIL-like methods are notorious for their instability so having non-adversarial IL methods is a significant improvement. ##########################################################################Pros:  1. Non-adversarial approach to IL that seems well suited policy optimization with any RL algorithm.2. Significant improvement over state-of-the-art IL approaches. Also works with only one demonstration.3. Nice theoretical results showing the objective lower bounds reverse KL between expert and imitator.##########################################################################Cons:  1. Only results on mujoco tasks with state information. Most of these tasks can be solved reasonably well with just a bias toward longer episodes. It would be nice to show that the authors method qualitatively imitates a variety of behaviors rather than just being able to go really fast without falling down. Imitating something like the hopper back flip would be a nice addition (https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/). Some other kind of open ended task would also be interesting where learning from demonstrations actually makes sense.2. Omits other papers that also perform efficient reward learning, then RL for non-adversarial imitation learning: e.g. Uchibe. "Model-free deep inverse reinforcement learning by logistic regression." Neural Processing Letters, 2018.Brown et al. "Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations." CoRL, 2019. 3. Quite a few knobs that need to be tuned in terms of hyperparameters. Perhaps I missed it, but it would be nice to have better intuition for how to set these and how imitation behavior changes based on them.##########################################################################Questions/Clarifications:I'm confused about why the authors think this method will work well with hard exploration video games. I'm not sure I buy the claim that optimizing the authors bound will cause an agent to explore new levels.After equation (9) the authors say this will cause the policy to seek out areas with low randomness and update the policy to be random there, but the alternative also seems equally likely given the objective, e.g., seek out states where there is lots of randomness and try and make the policy less random there.The notation for the autoregressive section is confusing what does x = (x_1, ..., x_dim(S) + dim(A) ) =  (s,a) mean? What if S and A are inf dimensional? Also, autoregressive models are often used for time series data, but here is looks like q is conditioned on (s,a) pairs from some lexicographic ordering which does not seem to make sense since there may be no correlation between actions at far apart states. In this paper, the authors introduce a new weight sharing pattern to search for the width in a network layer. Besides, FLOPs-sensitive bins is proposed to measure the real FLOPs of a single channel at a layer and further reduce the search space. The paper proposes a locally free weight sharing mechanism where the channels in a layer are split into base channels and free channels. Compared with conventional fixed weight sharing pattern where the leftmost channels are assigned as the sub-network, the proposed locally free pattern increases more flexibility while the search space also scales at O(n). The proposed FLOPs-sensitive bins forces the layers with larger FLOPs sensitivity to have fewer channels, thus reducing the search space at a fixed FLOPs. Experimental results on several datasets show that the proposed CafeNet outperforms many other width search algorithms. The searched network experimentally achieves high performance with tiny FLOPs budgets.What I like about this paper in that: 1.The motivation and intuition are reasonable, which is to design a more flexible weight sharing pattern for network width search. 2.Experiments are sufficient, thorough and carefully designed. Experimental results can support the objective of proposed methods. The searched network achieves remarkable performance with tiny FLOPs budgets.3.The paper is well written and organized. The work is easy to follow and be reproduced.4.The proposed methods have high generality and might be used on any convolutional network.Some minor concerns or suggestion about this paper:1.The searching and training algorithms (max-max selection and min-min optimization) should be described in more detail.2.The free channels are the neighborhood of the c-th channel in this paper, but I think more channels on the right should be included in the zone. This paper explores the weight sharing schema in one-shot width search and proposes a locally free weight sharing strategy (CafeNet). By splitting each width candidate into base channels and free channels, CafeNet makes a compromise between fixed weight pattern and full freedom pattern. Such strategy can reduce the search complexity and improve the performance ranking, w.r.t. different width, in the supernet. Experiments on various tasks, including classification, detection and attribute recognition, are well provided to support the effectiveness of the proposed method. The final results are quite promising.Strengths:1) The paper is well written and easy to follow. The motivation is clearly explained by an example and the problem formulation.2) The idea of locally free weight sharing is interesting. Such a solution for the previous fixed weight sharing seems sound.3) Experiments with additional analyses are well provided.I have the following concerns and suggestions:1) Missing some relevant papers. OFA[1] and TF-NAS[2] introduce width search by dynamically choosing the channels. The authors should cite and explain the differences.2) Is there any correlation between the degree in Eq. (4) and the searched accuracy under a fixed FLOPs?3) The bin size of Eq. (9) makes me confusing. As shown in experiments, ² can be less than 1 and the second term in the right side of Eq. (9) is also less than 1. Thus, the bin size bi (i.e., number of channels in a bin) is less than 1 channel. Please explain it in detail.4) Why lambda=0 achieves the best accuracy in Fig. 2(b)? It is in conflict with the statement As shown in Fig. 2(b), our 0.5-FLOPs MobileNetV2 on CIFAR-10 improves 0.92% accuracy from lambda=0 to lambda=1.5) I suggest the authors to split 1G group in Table 1, as done in Table 11.6) In Algorithm1, both the supernet and the total number of epoch are defined as N.Although some details make me a little confusing, the experimental analyses and the intuitive solutions of locally free weight sharing in one-shot width search are quite informative and helpful to the NAS community. I suggests the authors to release their code and would like to see the authors responses. [1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han. Once-for-All: Train One Network and Specialize it for Efficient Deployment. ICLR, 2020.[2] Yibo Hu, Xiang Wu, Ran He. TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search. ECCV, 2020. This work considers optimizing a two-layer over-parameterized ReLU network with the squared loss and given a data set with arbitrary labels. It is shown that for a sufficiently large number of hidden neurons (polynomially in number of samples) gradient descent converges to a global minimum with a linear convergence rate. The proof idea is to show that a certain Gram matrix of the data, which depends also on the weights, has a lower bounded minimum eigenvalue throughout the optimization process. Then, it is shown that this property implies convergence of gradient descent.This work is very interesting. Proving convergence of gradient descent for over-parameterized networks with ReLU activations and data with arbitrary labels is a major challenge. It is surprising that the authors found a relatively concise proof in the case of two-layer networks. The insight on the connection between the spectral properties of the Gram matrix and convergence of gradient descent is nice and seems to be a very promising technique for future work. One weakness of the result is the extremely large number of hidden neurons that are required to guarantee convergence.The paper is clearly written in most parts. The statement of Lemma 3.2 and its application appear to be incorrect as mentioned in the comments. I am convinced by the authors' response and the current proof that it can be fixed by defining an event which is independent of t. Moreover, I think it would be nice to include experiments that corroborate the theoretical findings. Specifically, it would be interesting to see if in practice most of the patterns of ReLUs do not change or if there is some other phenomenon.As mentioned in the comments, it would be good to add a discussion on the assumption of non-degeneracy of the H^{infty} matrix and include a proof (or exact reference) which shows under which conditions the minimum eigenvalue is positive. The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. One question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. In (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one "the" in "minimize the the validation" in the sentence above (5)) This paper empirically investigates the effect of batch size on the convergence speed of the mini-batch stochastic gradient descent of popular deep learning models. The fact that there is a diminishing return of batch size is not very surprising and there is a well-known theory behind it, but the theory doesn't exactly tell when we will start to suffer from the diminishing return. Therefore, it is quite valuable for the community to have an empirical analysis across popular ML tasks and models. In this regard, however, It would've been even nicer if the paper covered more variety of popular ML models such as Machine Translation, Speech Recognition, (Conditional) Image Generation, etc which open source implementations are readily available. Otherwise, experiments in this paper are pretty comprehensive. The only additional experiment I would be interested in is to tune learning rate for each batch size, rather than using a base learning rate everywhere, or simple rules such as LSR or SRSR. Since the theory only gives us asymptotic form of the optimal learning rate, empirically you should be tuning the learning rate for each batch size. And this is not totally unrealistic, because you can use a fraction of computational time to do cross-validation for searching the learning rate.pros:* findings provide us useful direction for future research (that data-parallelism centered distributed training is going to hit the limit soon)* extensive experiments across 5 datasets and 6 neural network architecturescons:* experiments are a bit too much focused on image classification* error bars in figures could've provided greater confidence in robustness of findings Summary: The authors propose a new approach to encourage valid interpolation in Auto-Encoders (AE). It is based on a regularization procedure involving a critic network judging the realistic nature of reconstructed data point from its mixed latent representations by recovering the mixing coefficient. The authors show that this approach does indeed improve the quality of interpolated samples on few tasks. A synthetic tasks of lines interpolation (proposing new Mean Distance and Smoothness metric for this task), classification task (with a single-layer classifier) from the latent space representation and finally a clustering accuracy on the latent space. On the proposed regularization method seems to help significantly compared to commonly used AE architectures (Basic AE, Denoising AE, Variational AE, Adversarial AE and VQ-VAE).This paper was a very interesting read, and the work seems to be of significance for the unsupervised learning community.It was clearly written and conveys the contributions clearly and the experimental results and their interpretations seem valid.The proposed approach of a critic based regularizer is a simple but seemingly important addition that contributes to improving interpolation in AE significantly and even show impact "downstream tasks" as the authors put it.Few comments/questions come to mind:- For the critic Loss L_d in equation (1) , the authors mention that the \gamma based second term (that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor)  does not seem to be crucial in your approach but stabilized the adversarial training. Could you somehow quantify this. It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation. This comment, even though I assume it well-founded, seems a bit of a contradiction.- For the Lines synthetic data. It was chosen to use a 32x32 image size with 16 points length lines. This configuration does quantize directly the angles your measures can distinguish. Below a certain angle differences (or delta), 2 angles must have the same pixel representation, i.e. exact overlapping lines. My question is simple: What is the smallest angle you can use/distinguish or, how many exact unique lines can you have? Overall this is a good paper that deserves publications. The authors introduce a class of quasi-hyperbolic algorithms that mix SGD with SGDM (or similar with Adam) and show improved empirical results. They also prove theoretical convergence of the methods and motivate the design well. The paper is well-written and contained the necessary references. Although I did feel that the authors could have better compared their method against the recent AggMom (Aggregated Momentum: Stability Through Passive Damping by Lucas et al.). Seems like there are a few similarities there. I enjoyed reading this paper and endorse it for acceptance. The theoretical results presented and easy to follow and state the assumptions clearly. I appreciated the fact that the authors aimed to keep the paper self-contained in its theory. The numerical experiments are thorough and fair. The authors test  the algorithms on an extremely wide set of problems ranging from image recognition (including CIFAR and ImageNet), natural language processing (including the state-of-the-art machine translation model), and reinforcement learning (including MuJoCo). I have not seen such a wide comparison in any paper proposing training algorithms before. Further, the numerical experiments are well-designed and also fair. The hyperparameters are chosen carefully, and both training and validation errors are presented. I also appreciate that the authors made the code available during the reviewing phase. Out of curiosity, I ran the code on some of my workflows and found that there was some improvement in performance as well. The authors analyze the local SGD algorithm, where $K$ parallel chains of SGD are run, and the iterates are occasionally synchronized across machines by averaging. For sufficiently short intervals between synchronization, the algorithm achieves the same convergence rate in terms of the number of gradient evaluations as parallel minibatch SGD, but with the advantage that communication can be significantly reduced.The algorithm is simple and practical, and the analysis is concise and seems like it could be applicable more generally to other parallel SGD variants.I am curious about what happens for the analysis of the algorithm when $H$ becomes large. As the authors point out, when $H=T$, this is one-shot averaging which is known to converge. The authors mention not working too hard to optimize the bounds for extreme values of $H$, which is fine, but I wonder if this is possible using their analysis technique, or whether new tools would be necessary. The authors of this paper analyze a well known technique for parallel training, where each compute node locally trains a model with SGD, and once in a while the K compute nodes average their models. Local SGD, although not as widely used as mini-batch SGD, can provide some gains in terms of the cost of communication. This can be achieved by decreasing the frequency of synchronization, while locally also increasing the minibatch. To the best of my knowledge, the authors are the first to provide a complete theoretical analysis of local SGD for strongly convex functions. They prove that under strong convexity, and the bounded gradients assumption, local SGD will (in the worst case) achieve a linear speedup over vanilla SGD, as long as the parallel models are averaged frequently enough. They show that although frequent averaging is important for speedup, the overall communication cost can be lower than minibatch SGD that may require smaller batches and hence more frequent communication. The authors extend their results to the asynchronous case, where a similar convergence bound is derived. The overall theory seems to be partly inspired by the perturbed iterates framework of Mania et al., however the application is novel and interesting.The authors include some limited experimental results that validate their bounds.This is a well-written paper, that will certainly be of interest to researchers working on stochastic optimization, and distributed learning. The results are interesting and clearly stated. The proofs seem complete and correct, and are easy to follow. I have two minor comments:1) In a recent paper, Dong et al. [1] suggest that for any problem (convex or nonconvex), the largest possible batch size in minibatch SGD that allows for linear speedups will be proportional to gradient diversity, i.e., a measure of similarity between the concurrently processed gradients. For example, when all gradient are identical, there is no speedup to be extracted. This diversity term does not seem to appear in the main theorem, as one may expect. For example, the presented bounds still seem to provide speedup gains for the case where all individual n functions are identical (eg minimum grad. diversity). This should not be possible, as there are no parallel speedups to be extracted in this case. Im wondering how that fact is reflected in the presented bounds (maybe its one of the extreme parameter cases that are not covered by the main theorem).2) The authors do not provide details of their experimental setup. For example it would be useful to know what hardware they implemented their algorithms on. It seems that they run experiments for up to 1K workers. Are these individual cores, or was this the result of hyper-threading? Finally, its unclear if Fig 1 is a theoretical, or an experimental curve.[1] http://proceedings.mlr.press/v84/yin18a/yin18a.pdf I enjoyed reading this manuscript. The paper is based on a simple idea used by others as well (i.e., the image has two components, one  that encode content which is shared across domains and another one characterizing the domain specific style). The other important idea is the use of feature masks that steer the translation process without requiring semantic labels. This is similar to attention models used by others but I think it is novel when applying to this specific application domain. I was a bit disappointed by the evaluation part. The authors decided to perform ablation and to show the importance of each component using only the MNIST-Single dataset. While this is good as a toy example I would have expected to see such analysis on a more complex example, e.g., street-view translation. This is also surprising considering that it is not even present in the supplementary material. Overall, this is a solid submission with interesting ideas and good implementation. In this paper the authors propose a new variance-reduction technique to use when computing an expected loss gradient where the expectation is with respect to independent binary random variables, e.g. for training VAEs with a discrete latent space. The paper is interesting, highly relevant, simple to implement, suggests many possible extensions, and shows good results on the experiments performed. However the exposition leaves a lot to be desired.Major comments:The authors devote several pages of fairly dense mathematics to deriving the ARM estimate in section 2 (up to section 2.5). However I found it relatively easy to derive (15) directly, using elementary results such as the law of total expectation and a single 1-dimensional integral, in about 10 lines of equations. As the authors note, deriving (4) from (15) requires an extra line or two. In my opinion it would greatly improve the clarity of the paper to use a more direct and straightforward derivation (perhaps with the interesting historical account of how the authors first derived this result given in an appendix). I could understand the more lengthy derivation being helpful if it gave insight into the source of variance reduction, but I don't see this personally, and the current discussion of variance reduction does not refer to the derivation of (15) at all.The analysis of variance in section 2.6 leaves a lot to be desired. The central claim of the paper is that this method reduces variance, so it is an important section! Firstly, the variance of ARM vs AR is interesting, but the variance of ARM vs REINFORCE seems also highly relevant. Secondly, it seems like it would be very informative to look at the ratio of stdev to the mean for the ARM gradient estimate, since the true gradient is multiplied by sigmoid(phi) sigmoid(-phi) and so is very small if the probability of z = 1 is close to 0 or 1, exactly in the same regime where ARM has an advantage in variance reduction over AR. For example, it may be that learning in this regime is very difficult due to the weak gradient even if the estimate is extremely low variance. Thirdly and somewhat relatedly, in this same regime (z = 1 close to 0 or 1) the ARM gradient estimate is very often 0, meaning no learning takes place, so it seems a bit strange to argue that the new method is fantastic in the regime where it's almost always not learning! Of course, not learning is better than adding lots of spurious variance as reinforce would, but perhaps this could be made clearer. Finally, the theoretical analysis involving correlation gives very little insight and is extremely hand-wavy. A short worked example in the 1D or 2D case explicitly computing the variance of REINFORCE, AR and ARM seems like it would be highly informative.Minor comments:In the introduction, "*approximately* maximizing the marginal likelihood" might be more accurate, since as given in (28) the exact marginal likelihood is not optimized in practice, and the exact marginal likelihood is not of the form (1) but is rather the logarithm of something of the form (1).I wasn't clear why "equal in distribution" was used a few things for things that are simply equal, such as just above (5).In section 2.3, I don't see any real reason the estimates in (9) and (11) "could be highly positively correlated", other than an argument along the lines of the simple one given in section 2.6 that they're often equal and so zero.As an aside, in section 3.1, it is great not to assume conditional independence of the binary latent variables across layers, but assuming conditional independence within each layer is still very restrictive. It is reasonable for the generative distribution to have this property, since the resulting net can still be essentially "universal" by stacking enough layers, but assuming this factorization in the variational distribution is highly restrictive with hard-to-reason-about consequences for the learned generative model. I realize this is a commonly used assumption and the authors are interested in the variance reduction properties of their approach rather than the training itself, but I just mention that it would be great to see extensions of the current work that can cope tractably with correlated latent variables within each layer.In section 3.2, according to my understanding of standard terminology, "maximum likelihood inference" is a misnomer and would normally be "maximum likelihood estimation", since maximum likelihood is a method for estimating parameters whereas inference is about inferring latent variable values given parameters.In section 4, it would be great to see some plots of explicit variance estimates of the different methods, given the overall goal of the paper (unless I just missed this?), even though figure 1 gives some insight into the variance characteristics.In section 4.2, the expression log 1/K \sum_k Bernoulli... differs in the placement of log from Jang et al (2017). Which is the standard convention for this task? Summary-------This paper describes a model for musical timbre transfer.The proposed method uses constant-Q transform magnitudes as the input representation, transfers between domains (timbres) by a CycleGAN-like architecture, and resynthesizes the generated CQT representation by a modified WaveNET-like decoder. The system is evaluated by human (mechanical turk) listening studies, and the results indicate that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.High-level comments-------------------This paper is extremely well written, and the authors clearly have a great attention to detail in both the audio processing and machine learning domains.  Each of the modifications to prior work was well motivated, and the ablation study at the end, while briefly presented, provides a good sense of the contributions of each piece.I was unable to listen to the examples provided by the link in section 6, which requires a Microsoft OneDrive login to access.  However, the youtube link provided in the ICLR comments gave a reasonable sample of the results of the system.  Overall, the outputs sound compelling, and match my expectations given the reported results of the listening studies.On the quantitative side, it would have been nice to see a measurement of phase retrieval of the decoder component, which could be done in isolation from the transfer components by feeding in original CQT magnitudes.  This might help give a sense of how well the model can be expected to perform, particular as it breaks down along target timbres.  I would expect some timbres to be easier to model than others, and having a quantitative handle on that could help put the listener study in a bit more perspective.Detailed comments-----------------The paper contains numerous typos and grammatical quirks, e.g.:    - page 5: "GP can stable GAN training"    - page 7: "CQT is equivalent to pitch"The reverse-generation trick in section 3.2 was clever! This paper proposes the temporal difference variational auto-encoder framework, a sequential general model following the intuition of temporal difference learning in reinforcement learning. The idea is nice and novel, and I vote for acceptance.1. The introduction of belief state in the sequential model is smart. How incorporate such technique in such an autoregressive model is not easy.2. Fig 1 clearly explained the VAE process.3. Four experiments demonstrated the main advantages of the proposed framework, including the effectiveness of proposed belief state construction and ability to jumpy rolling-out, Other Comments and Questions:1. Typo, p(s_{t_2}|s_{t_1}) in the caption of Fig 1.2. Can this framework partially solve the exposure bias?3. The author used uniform distribution for t_2 - t1, and from the ``NOISY HARMONIC OSCILLATOR`` we can indeed see larger interval will result in worse performance. However, the author also mentioned other distortion could be investigated, so I am wondering if the larger probability mass is put on larger dt, what the performance will become.4. The code should be released. I think that it is a fundamental framework deserving further development  by other researchers. This is a paper of the verification of neural networks, i.e. check their robustness, and the main contribution here is to tackle it as a statistical problem adressed with multi-level splitting Monte Carlo approach. I found the paper well motivated and original, resulting in a publishable piece of research up to a few necessary adjustments. These concern principally notation issues and some potential improvements in the writing. Let me list below some main remarks along the text, including also some typos. * In the introduction, "the classical approach" is mentioned but to be the latter is insufficiently covered. Some more detail would be welcome. * page 2, "predict the probability": rather employ "estimate" in such context? * "linear piecewise": "piecewise linear"? * what is "an exact upper bound"? * In related work, no reference to previous work on "statistical" approaches to NN verification. Is it actually the case that this angle has never been explored so far?* I am not an expert but to me "the density of adversarial examples" calls for further explanation. * From page 3 onwards: I was truly confused by the use of [x] throughought the text (e.g. in Equation (4)). x is already present within the indicator, no need to add yet another instance of it. Here and later I suffered from what seems to be like an awkward attempts to stress dependency on variables that already appear or should otherwise appear in a less convoluted way. * In Section 4, it took me some time to understand that the considered metrics do not require actual observations but rather concern coherence properties of the NN per se. While this follows from the current framework, the paper might benefit from some more explanation in words regarding this important aspect. * In page 6, what is meant by "more perceptually similar to the datapoint"? * In the discussion: is it really "a new measure" that is introduced here? * In the appendix: the MH acronym should better be introduced, as should the notation g(x,|x') if not done elsewhere (in which case a cross-reference would be welcome). Besides this, writing "the last samples" requires disambiguation (using "respective"?). I enjoyed reading this paper which is a great example of solid computational neuroscience work.The authors trained CNNs under various biologically-motivated constraints (e.g., varying the number of units in the layers corresponding to the retina output to account for the bottleneck happening at the level of the optic nerve or varying the number of "cortical" layers to account for differences across organisms). The paper is clear, the hypotheses clearly formulated and the results are sound. The implications of the study are quite interesting suggesting that the lack of orientation selectivity in the retina would arise because of the bottleneck at the level of the optic nerve. The continuum in terms of degree of linearity/non-linearity observed across organisms at the level of the retina would arise as a byproduct of the complexity/depth of subsequent processing stages. While these results are somewhat expected this is to my knowledge the first time that it is shown empirically in an integrated computational model.Minor point: The authors should consider citing the work by Eberhardt et al (2016) which has shown that the exists an optimal depth for CNNs to predicting human category decisions during rapid visual categorization.S. Eberhardt, J. Cader &amp; T. Serre. How deep is the feature analysis underlying rapid visual categorization? Neural Information Processing Systems, 2016. This paper addresses questions about the representation of visual information in the retina. The authors create a deep neural network model of the visual system in which a single parameter (bandwidth between the retina and visual cortex parts) is sufficient to qualitatively reproduce retinal receptive fields observed across animals with different brain sizes, which have been hard to reconcile in the past. This work is an innovative application of deep neural networks to a long-standing question in visual neuroscience. While I have some questions about the analyses and conclusions, I think that the paper is interesting and of high quality.My main concern is that the authors only show single examples, without quantification, for some main results (RF structure). For example, for Fig. 2A and 2B, an orientation selectivity index should be shown for all neurons. A similar population analysis should be devised for Fig 2C, e.g. like Fig 3 in [1]Minor comments:1. Page 4: These results suggest that the key constraint ... might be the dimensionality bottleneck..: The analyses only show that the bottleneck is *sufficient* to explain the differences, but the key constraint also implies *necessity*. Either soften the claim or provide control experiments showing that alternative hypotheses (constraint on firing rate etc.) cannot explain this result in your model.2. I dont understand most of the arguments about cell types (e.g. Fig. 2F and elsewhere). In neuroscience, cell types usually refers to cells with completely different connectivity constraints, e.g. excitatory vs. inhibitory cells or somatostatin vs. parvalbumin cells. But you refer to different CNN channels as different types. This seems very different than the neuroscience definition. CNN channels just represent different feature maps, i.e. different receptive field shapes, but not fundamentally different connectivity patterns. Therefore, I also dont quite understand what you are trying to show with the weight-untying experiments (Fig. 2E/F).3. It is not clear to me what Fig. 3B and the associated paragraph are trying to show. What are the implications of the nonlinearity being due to the first or second stage? 4. Comment on Fig 3F: The center-surround RFs probably implement a whitening transform (which is linear). Whitened inputs can probably be represented more efficiently in a network trained with L2-regularization and/or SGD. This might explain why the quasi-linear retina improves separability later-on.[1] Cossell, Lee, Maria Florencia Iacaruso, Dylan R. Muir, Rachael Houlton, Elie N. Sader, Ho Ko, Sonja B. Hofer, and Thomas D. Mrsic-Flogel. Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex. Nature 518, no. 7539 (February 19, 2015): 399403. https://doi.org/10.1038/nature14182. The paper studies generalization properties of preconditioned gradient descent on linear/kernel regression problems. The main preconditioner that is studied in addition to vanilla GD is the (population) Fisher matrix (natural gradient descent or NGD), its empirical counterpart, and its interpolation with GD.The authors first consider the "ridgeless" regression setup in high-dimension, where the estimator corresponds to the limiting gradient flow iterate, and show that NGD leads to a smaller (and optimal) variance term, and can improve the bias term compared to GD particularly in the presence of strong misspecification. Among others, the authors also consider early-stopping in a non-parametric RKHS setup, showing that an appropriate interpolation between NGD and GD achieves optimal rates with a much smaller number of steps compared to GD, a difference which becomes larger for "difficult" problems (which require more weight on the Fisher preconditioner). The findings are further illustrated with simple experiments on neural networks.Overall, the paper provides a comprehensive study of the impact of preconditioning/second-order methods/natural gradient on generalization by giving a precise analysis in tractable regression settings, which illustrate conditions under which preconditioning is or is not useful for better generalization. This makes the paper a strong contribution, and I am in favor of acceptance.comments/typos:- section 3: 'population risk' -> should this be excess risk given the presence of noise? add a reference or some more details on the bias-variance decomposition?- the last sentence in section 3.2 "in the analogy..." could be clarified- end of p.5 "lower bias compare to" -> "compared to"- Prop. 6: first part with theta_P holds for any P? please specify- theorem 7: specify conditions on eta?- some comments on computational difficulties of the full preconditioner would be welcome. Would a diagonal preconditioner, as often used in deep learning, provide any (partial) benefits as in the full-matrix case presented here? I'll start with a disclaimer: I have reviewed the NIPS 2019 submission of this paper which was eventually rejected. Compared to the NIPS version, this manuscript had significantly improved in its completeness. However, the writing still can be improved for rigor, consistency, typos, completeness, and readability.Authors propose a novel variational inference method for a locally linear latent dynamical system. The key innovation is in using a structured "parent distribution" that can share the nonlinear dynamics operator in the generative model making it more powerful compared. However, this parent distribution is not usable, since it's an intractable variational posterior. Normally, this will prevent variational inference, but the authors take another step by using Laplace approximation to build a "child distribution" with a multivariate gaussian form. During the inference, the child distribution is used, but the parameters of the parent distribution can still be updated through the entropy term in the stochastic ELBO and the Laplace approximation. They use a clever trick to formulate the usual optimization in the Laplace approximation as a fixed point update rule and take one fixed point update per ADAM gradient step on the ELBO. This allows the gradient to flow through the Laplace approximation.Some of the results are very impressive, and some are harder to evaluate due to lack of proper comparison. For all examples, the forward interpolate (really forecasting with smoothed initial condition) provides a lot of information. However, it would be nice to see actual simulations from the learned LLDS for a longer period of time. For example, is the shape of the action potential accurate in the single cell example? (it should be since the 2 ms predictive r^2 shows around 80%).Except in Fig 2, the 3 other examples are only compared against GfLDS. Since GfLDS involves nonconvex optimization, it would be reasonable to also request a simple LDS as a baseline to make sure it's not an issue of GfLDS fitting.For the r^2=0.49 claim on the left to right brain prediction, how does a baseline FA or CCA model perform?Was input current ignored in the single cell voltage data? Or you somehow included the input current as observation model?As for the comment on Gaussian VIND performing better on explaining variance of the data even though it was actually count data, I think this maybe because you are measuring squared error. If you measured point process likelihood or pseudo-r^2 instead, Poisson VIND may outperform. Both your forecasting and the supplementary results figure show that Poisson VIND is definitely doing much better! (What was the sampling rate of the Guo et al data?)The supplementary material is essential for this paper. The main text is not sufficient to understand the method.This method relies on the fixed point update rule operating in a contractive regime. Authors mention in the appendix that this can be *guaranteed* throughout training by appropriate choices of hyperparameters and network architecture. This seems to be a crucial detail but is not described!!! Please add this information.There's a trial index suddenly appearing in Algorithm 1 that is not mentioned anywhere else.Is the ADAM gradient descent in Algorithm 1 just one step or multiple?MSE -&gt; MSE_k in eq 13LFADS transition function is not deterministic. (page 4)log Q_{phi,varphi} is quadratic in Z for the LLDS case. Text shouldn't be 'includes terms quadratic in Z' (misleading).regular gradient ascent update --&gt; need reference (page 4)Due to the laplace approximation step, you don't need to infer the normalization term of the parent distribution. This is not described in the methods (page 3).Eq 4 and 5 are inconsistent in notation.Eq (1-6) are not novel but text suggests that it is.Predict*ive* mean square error (page 2)Introduction can use some rewriting.arXiv papers need better citation formatting. The authors propose in this paper a series of results on the approximation capabilities of neural networks based on ReLU using quantized weights. Results include upper bounds on the depth and on the number of weights needed to reach a certain approximation level given the number of distinct weights usable. The paper is clear and as far as I know the results are both new and significant. My only negative remark is about the appendix that could be clearer. In particular, I think that figure 2 obscures the proof of Proposition 1 rather than the contrary. I think it might be much clearer to give an explicit neural network approximation of x^2 for say r=2, for instance. The proposed DADAM is a sophisticated combination of decentralized optimization and the adaptive moment estimation. DADAM enables data parallelization as well as decentralized computation, hence suitable for large scale machine learning problems. Corollary 10 shows better performance of DADAM. Besides the detailed derivations, can the authors intuitively explain the key setup which leads to this better performance?The experimental results are mainly based on sigmoid loss with simple constraints. The results will be more convincing if the authors can provide studies on more complex objective, for example, regularized loss with both L2 and L1 bounded constraints.  Th experimental results in Section 5.1 is based on \beta_1 = \beta_2 = \beta_3 = 0.9. From  the expression of \hat v_{i,t} in Section 2, this setting implies the most recent v_{i,t} plays a more important role than the historical maximum, hence ADAM is better than AMSGrad. I am curious what the results will look like if we set \beta_3 as a value smaller than 0.5. This is an interesting paper targeting adversarial learning for interfering car detection. The approach is to learn camouflage patterns, which will be rendered as a texture on 3D car models in urban and mountain scenes, that minimizes car detection scores by Mask R-CNN and YOLOv3-SPP.Different from image-based adversarial learning, this paper examines whether 3D car textures can degrade car detection quality of recent neural network object detectors. This aspect is important because the learned patterns can be used in the painting of real-world cars to avoid automatic car detection in a parking lot or on a highway.The experimental results show that the car detection performance significantly drops by learned vehicle camouflages. Major comments:- It is not clear how learned camouflage patters are different in two scenes. Ideally, we should find one single camouflage patter that can deceive the two or more object detection systems in any scenes.Minor comments:- In abstract, it is not good that you evaluated your study as "interesting". I recommend another word choice. This paper introduces a new VAE model, the latent tree VAE (LTVAE), which aims to learn models with multifaceted clustering, that is separate clusterings are enforced on different subsets of the latent features.  This is achieved using a tree-structured prior on a set of discrete "super latent variables" (Y_1,...,Y_L) that identify which cluster the datapoint falls into for each separate facet (i.e. there is a separate clustering associated with each Y_n).  The subset of the standard latent variables z then form a Gaussian mixture model (GMM) for each Y_n.   Both the structure of this setup (i.e. the associated graphical model) and the parameters (i.e. means and variances of the clusters) are learned during training.  This introduces a number of computational challenges not usually seen in for VAE training, for which, seemingly well thought through, novel schemes are introduced, most notably a message passing scheme for calculating gradients of the log marginal p(z).Overall I think this is a very good paper.  The exposition of the work is, for the most part, very good - the paper was a pleasure to read.  I think that the key idea is novel and adds something unique and useful to the literature, I thus think it is work which will be of substantial interest to the ICLR community.  The quality of the paper is also very good: algorithmic details seem to have been well thought through and the experimental evaluation is above average, both in terms of apparent performance and in the breadth of experiments considered.  I would very much like to see this work accepted to ICLR and I think that the extra use of space over 8 pages in the submission is justified.  However, I do have some questions and concerns that I would like to see addressed in the rebuttal period and I may lower my score if they are not.The key issues I would like to see addressed further discussion on are:a) There is no discussion about what is done for the encoder in the paper.  This is surely a very important consideration here as if the encoder is not expressive enough, this will impact the learned models.  For example, the dependency structures of the latent space induce particular dependencies in the posterior that must be carefully handled to avoid harming the learning (see e.g. https://arxiv.org/abs/1712.00287).b) I would like to see some numerical results for the similarity between the different clusterings that are learned.  A lot of the novelty of the work rests on being able to pick up different clusterings with the different facets.  However, the results suggest that the clusterings may actually have very significant overlap and so this should be quantified.c) The approach is presuming substantially slower than a setup where the structure is pre-fixed.  I think it is fine even if there is a big slow down, but I would like to see timing information so that the reader can assess how much higher the time cost is.d) As far as I can tell (sorry if I have made a mistake), the presented results are from single runs.  I would like to see information about the variability across different runs so that the fragility of the approach can be assessed.e) I would like to see more justification for having a dependency structure between the Y's, ideally both in motivating this choice and in experimental evaluation to check it (more generally ablation studies for different components of the algorithm would improve the paper).  Might it be possible to use this in a way the encourages the different clusterings to be distinct from one another?Other comments:1) Though the writing is generally very good, there are a few exceptions:- The second paragraph in the intro becomes a list of related work from the point where DEC is introduced.  This should be moved to the related work to improve the flow (just cite those papers at the end of the first sentence in the third paragraph) and it would be good for it to be less of a list of separate things and more something that puts the current work in the context of other approaches.- The paragraph after Eq 3 needs some rewriting- The explanations around and including equations 5 and 6 were quite poor: \pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in Eq 6 to avoid ambiguity2) The reference formatting is wrong (i.e. cite is used everywhere instead of citep)3) I thought the motivation for the approach in the intro was very good4) As the seemingly most related work, it would be good to elaborate more on the Goyal et al paper and the differences of your approach to theirs.  Is there a reason this is not used as a baseline in the experiments?5) I could not understand the step from the gradient to the gradient of the log in Eq 6.  Is this because p(y_b|z) = f(y_b) Norm(..)?6) The text in figures 2 and 3 is too small and difficult to make out.7) I think it is misleading to talk about p(z) as being a marginal likelihood and would use the term marginal prior, or just marginal, instead.8) I thought Figure 4b provided a nice demonstration.9) Is there a reason that log likelihood / ELBO scores are only provided on MNIST and only for the LTVAE / VAE?  I might be wrong, but I thought at least some of the other baselines provide this and those results presumably already exist as a side effect from calculating the clustering scores?  Relatedly, I'm aware that a previous version of this work included estimates of the normalized mutual information -- is there any reason these are no longer included?10) Did the larger dimensional latent spaced used for the qualitative results improve or worsen the performance of previous metrics?Minor points / typos- mehod -&gt; method- of generation network -&gt; of the generation network- brackets in eq 7- MoG not defined in section 4.5 The authors propose a learning scheme for the unsupervised acquisition of skills. These skills are then applied to (1) accelerate reinforcement learning to maximize a reward, (2) perform hierarchical RL, and (3) imitate an expert trajectory.The unsupervised learning of skills maximizes an information theoretic objective function. The authors condition their policy on latent variable Z, and the term skill refers to a policy conditioned on a fixed Z. The mutual information between states and skills is maximized to ensure that skills control the states, while the mutual information between actions and skills given the state is minimized, to ensure that states, not actions distinguish skills. The entropy of the mixture of policies is also maximized. Further manipulations on this objective function enable the scheme to be implemented using a soft actor-critic maximizing a pseudo reward involving a learned skill discriminator.The authors clearly position their work in relation to others, and especially point out the differences to the most similar work, namely Gregor et al 2016. These differences while seemingly minor end up providing exceptional improvement in the number of skills learned and the domains tackled.The question-answer style is somewhat unconventional. While the content comes across clearly, the flow / narrative is a bit broken.Overall, I believe that applicability of the work is very wide, touching inverse RL, hierarchical RL, imitation learning, and more. The simulational comparisons are also very useful.However, there is an issue that I'd like to see addressed:Fig 8: In a high-dimensional task, namely 111D ant navigation, DIAYN performs slightly worse than others. Incorporating a prior on useful skills makes DIAYN perform much better. Here, apart from the comparision with other state of the art RL methods, the authors should also compare to VIC. Indeed one of the key differences to VIC was the uniform prior on skills, which the authors now break albeit in a slightly different way. Thus, it is essential to also show the performance of VIC, and comment on any similarities / differences. The relation of this prior to the VIC prior should also be made clear. Further, the performance of VIC on the half cheetah hurdle should be also be shown.If the above issue is addressed, I strongly recommend that the work be presented at ICLR.Minor issues / typos:pg 1: "policy that alters that state of the environment" to "policy that alters the state of the environment"pg 3: "mutual information between skills and states, I(A; Z)" to "mutual information between skills and states, I(S; Z)"pg 4: "guaranteeing that is has maximum entropy" to "guaranteeing that it has maximum entropy"pg 4: " soft actor critic" to " soft actor critic (SAC)" since SAC is used later.pg 5: full form of VIME not introducedFig 5: would be good to also show the variance as a shaded area around the mean.pg 7: "whereas DIAYN explicitly skills that effectively partition the state space" ? SUMMARY:This work is about learning state-transition models in complex domains represented as sets of objects, their properties, ``"deictic" reference functions between sets of objects, and possible actions (or action templates). A parametric model for the actions is assumed, and these parameters act on a neural net that learns the transition model (probabilistic rule) from the current state to the next one.  It is basically this nonlinear transition model implemented by a network which makes this work different from previous models described in the literature. The relational transition model proposed is sparse, based on the assumption that actions have only ``local effects on related objects. The prediction model itself is basically a Gaussian distribution whose mean and variances are represented by neural nets. For jointly learning multiple rules, a clustering strategy is presented which assigns experience samples to transition rules. The method is applied to simulated data in the context of predicting pushing stacks of blocks on a cluttered table top.EVALUATION: The type of problems addressed in this paper is challenging and highly relevant for solving problems in the ``real'' world. Although the method proposed is in some sense a direct generalization of the work in [Pasula et al.], it still contains many novel and interesting aspects.Any single part of the model (like the use of Gaussians parametrized by functions implemented via neural nets) is somehow ``standard in deep latent variable models, but in complex real-world rule-learning problems the whole system presented  defines  certainly a big improvement over the state-of-the-art, which in my opinion has the potential to indeed advance this field of research. The authors present a setting of MARL communication where only a number of agents can broadcast messages in a shared and limited bandwidth channel. The paper is well written and easy to follow, and the authors run an extensive number of baselines to illustrate the contributions.Comments:1) It's not clear to me how do the authors tackle partial observability without the use of recurrent connections or time-steps?2) Do the agents know if they were chosen to be broadcasted at the previous timestep?3) Many times it's important to know who sent the message, do the agents share this information? This paper presents a new benchmark, CausalWorld, for studying generalization, transfer learning, and causal structure learning in RL and robotics. This is a hugely important problem, and I think this benchmark has some clear advantages over existing benchmarks. The benchmark consists of a simulated three finger robot over a bin containing blocks, within which there are 8 "families" of tasks, (a) pushing, (b) picking, (c) pick and place, (d) stacking 2 blocks, (e) stacking many blocks, (f) general rearrangement, (g) more complex multi-block stacking, and (h) building towers. More importantly, for each family of tasks, there is controllable procedural generation of goals as well as controllable factors of the environment such as object sizes, masses, frictions, colors, etc. This enables what I think is the key contribution of this paper - a procedural way to define training/evaluation splits where each split samples from different subspaces of the above controllable factors. This provides a systematic way of defining problems which require varying degrees of generalization, measuring the difficulty of such splits, and defining curricula within each split, which is critical to developing learning algorithms which are capable of this sort of generalization. While prior work (Yu et al, James et al) have defined many robotic tasks with some shared structure, one challenge is that it is difficult to say how much generalization one can expect between any two tasks which can be quite different, a problem which this benchmark takes a step towards addressing.Like the paper mentions, prior works have also used procedural generation over similar controllable factors like this paper does. In fact most physics simulators do allow varying these parameters directly. But, a simple API with a standardized interface to define these splits, as well as common splits that are used as benchmarks is still missing, and this paper takes an important step towards that. While I don't see a link to the code, I would encourage the authors to design their API in a simple and standardized way, as that is likely what would motivate people to use CausalWorld instead of manually defining train/eval splits in their own physics simulators.The main weakness I see of the paper is the experimental section. The authors train a few SOTA RL algorithms on 3 different train configurations of increasing randomization, and test on 12 different eval configurations. First in terms of clarity, I found Figure 5 difficult to interpret. I think it would be helpful if for each of the Eval protocols it was clearly described what was changing. In general I think the best way to present this would be to look at each pair of "train domain, eval domain" and the corresponding performance, with some clear description about what sort of generalization is needed. Also in terms of performance, it seems like when faced with anything more challenging than push/pick-place with limited randomization, all the SOTA algorithms fail even on the train domain (and as a result struggle on Eval domains as well). So one concern is that of the many domains presented in the benchmark, perhaps only a few are actually solvable by current RL algorithms during training. At the same time I think this indicates the challenges in learning generalizable policies, and may inspire better RL algorithms.Overall, I think this is an exciting benchmark, and would be excited to use it.  This paper presents an interesting idea to improve the softmax embedding performance with heated-up strategy. It is well-written and the proposed method is easy to implement. Several experiments on metric learning datasets demonstrate the effectiveness of the proposed method.The motivation to find a balance between the compactness and "spread-out" embedding is reasonable. The major weakness is the intermediate temperature selection, it might be a little tricky. How to generalize it to other applications?The authors claim that "heated-up" strategy produces well generalized feature, but the rationale behind is unclear. And there is no quantitative analysis to support this point. The starting temperature aims at pushing the incorrect samples to boundary samples and pushing the boundary samples to centroid samples. I would like to see the ratio of #incorrect/total and #boundary/total changed with different temperature in training process, i.e., alpha = 16, 4, 1. This experiment may help to verify the idea.As mentioned in Section 3, multiple strategies could be defined to increase the temperature. It is interesting to design a multiple heat-up strategy. Does it help to improve the learning speed? The paper introduces the concept of fluctuation-dissipation relations to stochastic gradient descent. These relations hold for certain observables in physical systems in equilibrium. In the context of SGD as a non-equilibrium process with a stationary density, they allow to quantify how far away this process is from its stationary state. One of the strengths of the paper is that it works in the discrete-time formalism and uses the master equation, as opposed to other recent works that used the continuous-time limit of SGD to derive related (yet different) results. Furthermore, the formalism does not even rely on a locally quadratic approximation of the loss function, or on any Gaussian assumptions of the SGD noise. To the best of my knowledge, all of this is very innovative. Ultimately, the authors propose a practical algorithm to adaptively lowering the learning rate based on testing fluctuation-dissipation relations.This is an interesting paper which I recommend to accept. It not only shows new theoretical results, but also conforms their validity in real-world experiments.I have only a few questions / comments:1. In Eq. 17 and others where the scalar product of theta and grad(f) occurs, is it implicitly assumed that the optimum of f is at theta=0?2. In Fig. 2, the distinction between solid and dotted curves could be made better visible.3. For completeness, it would be good to add the following citation:Stephan Mandt, Matthew D. Hoffman, and David M. Blei. "Continuous-time limit of stochastic gradient descent revisited."&nbsp;NIPS 2015 Workshop on Optimization for Machine Learning. The authors present lightweight convolutions and dynamic convolutions, two significant advances over existing depthwise convolution sequence models, and demonstrate very strong results on machine translation, language modeling, and summarization. Their results go even further than those of the Transformer paper in countering the conventional wisdom that recurrence (or another way of directly modeling long-distance dependencies) is crucial for sequence-to-sequence tasks. Some things that I noticed:- While you do cite "Depthwise Separable Convolutions for Neural Machine Translation" from Kaiser et al. (ICLR 2018), there are some missed opportunities to compare more directly to that paper (e.g., by comparing to their super-separable convolutions). Kaiser et al. somewhat slipped under the community's radar after the same group released the Transformer on arXiv a week later, but it is in some ways a more direct inspiration for your work than the Transformer paper itself.- I'd like to see more analysis of the local self-attention ablation. It's fantastic to see such a well-executed ablation study, especially one that includes this important comparison, but I'd like to understand more about the advantages and drawbacks of local self-attention compared to dynamic convolutions. (For instance, dynamic convolutions are somewhat faster at inference time in your results, but I'm unsure if this is contingent on implementation choices or if it's inherent to the architecture.)- From a systems and implementation perspective, it would be great to see some algorithm-level comparisons of parallelism and critical path length between dynamic convolutions and self-attention. My gut feeling is that dynamic convolutions significantly more amenable to parallelization on certain kinds of hardware, especially at train time, but that the caching that's possible in self-attention inference might make the approaches more comparable in terms of critical path latency at inference time; this doesn't necessarily line up with your results so far though.- You mostly focus on inference time, but you're not always as clear about that as you could be; I'd also like to see train time numbers. Fairseq is incredibly fast on both sides (perhaps instead of just saying "highly optimized" you can point to a paper or blog post?)- The nomenclature in this space makes me sad (not your fault). Other papers (particularly a series of three papers from Tao Shen at University of Technology Sydney and Tianyi Zhou at UW) have proposed architectures that are similarly intermediate between self-attention and (in their case 1x1) convolution, but have decided to call them variants of self-attention. I could easily imagine a world where one of these groups proposed exactly your approach but called it "Dynamic Local Self-Attention," or even a world where they've already done so but we can't find it among the zillions of self-attention variants proposed in the past year. Not sure if there's anything anyone can do about that, but perhaps it would be helpful to briefly cite/compare to some of the Shen/Zhou work.- I think you should have tried a language modeling dataset with longer-term dependencies, like WikiText-103. Especially if the results were slightly weaker than Transformer, that would help place dynamic convolutions in the architecture trade-off space.That last one is probably my most significant concern, and one that should be fairly easy to address. But it's already a great paper. The paper proposes a convolutional alternative to self-attention. To achieve this, the number of parameters of a typical convolution operation is first reduced by using a depth-wise approach (i.e. convolving only within each channel), and then further reduced by tying parameters across layers in a round-robin fashion. A softmax is applied to the filter weights, so that the operation computes weighted sums of its (local) input (LightConv).Because the number of parameters is dramatically reduced now, they can be replaced by the output of an input-dependent linear layer (DynamicConv), which gives the resulting operation a "local attention" flavour. The weights depend only on the current position, as opposed to the attention weights in self-attention which depend on all positions. This implies that the operation is linear in the number of positions as opposed to quadratic, which is a significant advantage in terms of scaling and computation time.In the paper, several NLP benchmarks (machine translation, language modeling) that were previously used to demonstrate the efficacy of self-attention models are tackled with models using LightConv and DynamicConv instead, and they are shown to be competitive across the board (with the number of model parameters kept approximately the same).This paper is well-written and easy to follow. The proposed approach is explained and motivated well. The experiments are thorough and the results are convincing. I especially appreciated the ablation experiment for which results are shown in Table 3, which provides some useful insights beyond the main point of the paper. The fact that a linear time approach can match the performance of self-attention based models is a very promising and somewhat surprising result.In section 5.3, I did not understand what "head band, next band, last band" refers to. I assume this is described in the anonymous paper that is cited, so I suppose this is an artifact of blind review. Still, even with the reference unmasked it might be useful to add some context here. The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the experts and agents stationary state-action distributions. The proposed method outperforms existing imitation learning approach (GAIL &amp; BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose).While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.).Nonetheless, the paper overall presents a strong submission based on novelty &amp; relevance of the proposed method and is recommended for publication. Minor issues:- Related work: improve transitions between the section about trajectory tracking and BC.- Ablation studies with less flexible probabilistic models would strengthen the experiment section further. - Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access.- A release of the code base would further strengthen the contributions of the submission.General recommendation:- The authors are encouraged to further investigate off-policy corrections for improved convergence. This paper investigates the relationship between the eigenvectors of the Hessian. This paper investigates characteristics of Hessian of the empirical losses of DNNs through comprehensive experiments. These experiments showed many important insights, 1) the top-K eigenvalues become bigger in the early stage, and decrease in later stage. 2) Bigger SGD steps and smaller batch-size leads to smaller and earlier peak of eigenvalues. 3) The sharpest direction update does not contribute to the loss value decrease in the normal step size (or bigger). From these analyses, this paper proposes to decrease the SGD step length on top-K eigenvectors for speeding up the convergence. Experimental results showed that the proposed method could converge to local minima in a fewer epoch and obtain better result, which means higher test accuracy.This paper is well-written and well-organized. Findings about eigenvalues and these relationship between the SGD step length are very impressive. Although the step length adjustment on the top-K eigenvector directions are not realistic solution for improving the current SGD-based optimization on DNNs due to heavy computational cost, I think these findings and insights are very helpful to ICLR and other ML communities. The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself. Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets. I found that this is an interesting paper, both original ideal and numerical results. Thank you for a pleasurable and informative read, I consider the writing and structure of the paper to be coherent and well written. Given an end-to-end learning of neural motifs, a great deal of time can be avoided, reducing the several intermediary steps required to detect motifs from calcium imaging. This paper may very well improve researchers efficiency, in particular when working with calcium imaging. The question remain to what extent these ideas may be useful in other imaging modalities, i.e. fMRI.My main critique would be to be more explicit about why the VAE you propose, is superior to other models in the generative modelling domain. last time i had two comments:1. the real data motifs did not look like what i'd expect motifs to look like. now that the authors have thresholded the real data motifs, they do look as i'd expect.2. i'm not a fan of VAE, and believe that simpler optimization algorithms might be profitable.  i acknowledge that SCC requires additional steps; i am not comparing to SCC. rather, i'm saying given your generative model, there are many strategies one could employ to estimate the motifs.  i realize that VAE is all the rage, and is probably fine.  in my own experiments, simpler methods often work as well or better for these types of problems.  i therefore believe this would be an interesting avenue to explore in future work. In this paper, the authors proposed a multi-domain adversarial learning approach, MULANN, to improve the classification accuracy on three datasets-DIGITS, OFFICE and CELL-in the semi-supervised DA setting. Its contributions include: i) using the H-divergence to bound both the risk across all domains and the worst-domain risk (imbalance on a specific domain); ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state-of-the-art on two standard image benchmarks, and a novel bioimage dataset, CELL. In addition, this paper has a clear logic to explain and prove the problem to be solved, and has ample experimental evidence. Above on, this paper did a meaningful work. But there are some errors of expression, so it should be checked. This is one of the best papers I reviewed so far this year (ICLR, NIPS, ICML, AISTATS), in terms of both the writing and technical novelty.Writing: the author provided sufficient context and did comprehensive literature survey, which made the paper easily accessible to a larger audience. And the flow of this paper was very smooth and I personally enjoyed reading it.Novelty: I wouldn't say this paper proposed a groundbreaking innovation, however, compared to many other submissions that are more obscure rather than inspiring to the readers, this paper presented a very natural extension to something practitioners were already very familiar with: taking an average of word vectors for a sentence and measure by cosine similarity.  Both max pooling and Jaccard distance are not something new, but the author did a great job presenting the idea and proved it's effectiveness through extensive experiments. (disclaimer: I didn't follow the sentence embedding literature recently, and I would count on other reviewers to fact check the claimed novelty of this paper by the authors)Simplicity: besides the novelty mentioned above, what I enjoyed more about this paper is it's simplicity. Not just because it's easy to understand, but also it's easy to be reproduced by practitioners.Quibbles: the authors didn't provide error bar / confidence interval to the results presented in experiment session. I'd like to know whether the difference between baselines and proposed methods were significant or not.Miscellaneous: I have to say the authors provided a very eye-catching name to this paper as well, and the content of the paper didn't disappoint me neither. Well done :) This submission presents a simple model for sentence representation based on max-pooling of word vectors. The model is motivated by fuzzy-set theory, providing both a funded pooling scheme and a similarity score between documents. The proposed approach is evaluated on sentence similarity tasks (STS) and achieves very strong performance, comparable to state-of-the-art, computationally demanding methods.Pros:+ The problem tackled by this paper is interesting and well motivated. Fast, efficient and non-parametric sentence similarity has tons of important applications (search, indexing, corpus mining).+ The proposed solution is elegant and very simple to implement. + When compared to standard sentence representation models, the proposed approach has very good performance, while being very efficient. It only requires a matrix vector product and a dimension-wise max.+ The paper is very well written and flows nicely.+ Empirical results show significant differences between different word vectors. The simplicity of this approach makes it a good test bed for research on word vectors.Cons:- Nothing much, really. - Eq. (3) is awkward, as it is a sequence of equalities, which has to be avoided. Moreover, if U is the identity, I don't think that the reader really need this Eq...I have several questions and remarks that, if answered would make the quality of the presentation better:* In infersent, the authors reported the performance of a randomly-initialized and max-pooled bi-lstm with fasttext vectors as the input lookup. This can be seen as an extreme case of the presented formalism, where the linear operator U is replaced by a complicated non linear function that is implemented by the random LSTM. Drawing that link, and maybe including this baseline in the results would be good.* Related to this previous question, several choices for U are discussed in the paper. However, only two are compared in the experiments. It would be interesting to have an experimental comparison of:- taking U = W- taking U = I- taking U as the principal directions of W- taking U as a random matrix, and comparing performance for different output dimensions. Overall, this paper is a very strong baseline paper. The presented model is elegant and efficient. I rate it as an 8 and await other reviews and the author's response. Summary : This paper develops a variant (DDIM) of an existing method (DDPM) with the goal of accelerating it greatly while still maintaining performance. The authors are working in the context of a denoising process that runs in the reverse direction to a sequence of steps that each add a small amount of Gaussian noise to the original data. The proposal is to introduce an auxiliary function that breaks the Markov assumption by leaking some information in a controlled way about the training points x0, and then use this auxiliary function as scaffolding to train the actual Markov chain of denoising functions.This paper builds on other works in the recent literature and proposes something useful and novel. They propose something relatively down-to-earth and then they methodically analyze the consequences and derive all the mathematical formulas that follow. I feel that the dose of mathematical content is just appropriate for what they set out to do. Less would be too vague, more would be excessive.The direction that they are proposing is relevant, contrary to certain other papers that involve a lot of correct equations but don't take us anywhere interesting. In my mind, this is a very good clean paper. I appreciated the authors taking the time to mention the connection with ODEs, and I thought they would mention Neural ODEs in section 7 as a nod to the recent surge of interest in ODEs in the context of Deep Learning sparked by that paper.The only bad things that I could possibly say about this paper would involve comparing it to certain other wildly creative papers, and to say that it's not as innovative or throught-provoking as those papers. And that's not a fair thing to say.I would like to ask the authors if, with their framework, there is a need to train a completely different epsilon_t for every t=1..T ? I understand these models to be U-Nets, as mentioned in the appendix. I presume that the thing that makes this reasonable is the fact that S < T, and only S different models need to be trained? This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function. They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost. Their experimental results showed competitive performance to SGD/Adam on the same network architectures. 1. Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function. While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case. An appendix with more numerical comparisons on other loss functions might also be insightful. 2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2). In Table 2 I saw some optimizers end up with much lower test accuracy. Can the authors show the convergence plots of these methods (similar to Figure 2)? 1. This is a very relevant and timely work related to robustness of deep learning models under adversarial attacks. 2. In recent literature of verifiable/certifiable networks, (linear) ReLU network has emerged as a tractable model architecture where analytically sound algorithms/understanding can be achieved. This paper adopts the same setting, but very clearly articulates the differences between this work and the other recent works (Weng et al 2018, Wong et al. 2018).  3. The primary innovation here is that the authors not only identify the locally linear regions in the loss surface but expand that region by learning essentially leading to gradient stability. 4. A very interesting observation is that the robustifying process does not really reduce the overall accuracy which is the case of many other methods. 5. The visualizations show the stability properties nicely, but a bit more explanations of those figures would help the readers quite a bit.6. While I understand some of the feasibility issues associated with other existing methods, it would be interesting to try to compare performance (if not exact performance, the at least loss/gradient surfaces etc.) with some of them.7. The adversarial scenarios need to be explained better. This paper proposes a simple and elegant approach to learning "grid-cell like" representations that uses a high-dimensional encoding of position, together with a matrix for propagating position that involves only local connections among the elements of the vector.  The vectors are also constrained to have their inner products reflect positional similarity.  The paper also shows how such a representation may be used for path planning.By stripping away the baggage and assumptions of previous approaches, I feel this paper starts to get at the essence of what drives the formation of grid cells.   It is still steps away from having direct ties to neurobiology, but is trying to get at the minimal components necessary for bringing about a grid cell like solution.  But I feel the paper also stops just a few steps short of developing a fuller theoretical understanding of what is going on.  For example the learned solution is quite Fourier like, and we know that Fourier transforms are good for representing position shift in terms of phase shift.  That would correspond to block size of two (i.e., complex numbers) in terms of this model.  So what's wrong with this solution (in terms of performance) and what is gained by having block size of six, beyond simply looking more grid like?  It would be nice to go beyond phenomenology and look at what the grid-like solution is useful for. Summary:The paper presents an analysis and numerical evaluation of stagewise SGD, ADAGRAD and Stochastic momentum methods for solving stochastic non-smooth non-convex optimization problems. Comments:I find the ideas presented in this paper very interesting. The convergence analysis seems correct and the paper is reasonably well written, and tackles an important problem. The analysis holds for ¼-weekly convex functions. This assumption is really important for the development of the algorithm and the proposed analysis. I like the fact that the authors provide two examples showing that popular objective functions in machine learning satisfy this assumption.The numerical evaluation is adequate showing the effectiveness  of the proposed stagewise algorithms.  However i have the follow suggestions/minor comments:1) It will be nice to have also some plots showing the performance of the proposed method on the ImageNet dataset. 2) Another possible nice experiment will be a comparison of the four stagewise methods (SGD,ADAGRAD,SHB,SNAG) on the same dataset. Which one behaves better? Minor Comments:1) The captions of the figures can be more informative (mention also the division by column). First column is SGD, Second column Adagrad, etc.2) Typos: Section 1, last bullet point, second line: "stagwise"Section 5, second paragraph , first line :"their their"page 8, 3 line from the bottom:  "seems, indicate"2) Missing reference.In the area of stochastic gradient methods with momentum many papers have been proposed recently for the case of convex optimization that worth to be mentioned:Gadat, Sébastien, Fabien Panloup, and Sofiane Saadane. "Stochastic heavy ball." Electronic Journal of Statistics 12.1 (2018): 461-529.Loizou, Nicolas, and Peter Richtárik. "Momentum and stochastic momentum for stochastic gradient, Newton, proximal point and subspace descent methods." arXiv preprint arXiv:1712.09677 (2017).Lan, Guanghui, and Yi Zhou. "An optimal randomized incremental gradient method." Mathematical programming (2017): 1-49.Overall, I suggest to accept this paper. The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set. This setting is further generalized to multiple sets. The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors. Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.The paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical. This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.Overall, I enjoyed reading the paper. My only concern is the experiments:1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al18 and references in there) are missing.2) Applying the model of Hartford et al18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)  Do you agree that for this reason, all the experiments on the synthetic dataset is flawed? The paper demonstrates the trade-off between accuracy and robustness of a model. The phenomenon is shown in previous works, but this work interestingly proposes a theoretical model that supports the idea. The proving technique can be particularly beneficial to developing theoretical understanding for the phenomenon. Besides, the authors also visualize the gradients and adversarial examples generated from standard and adversarially trained models, which show that these adversarially trained models are more aligned to human perception.Quality: good, clarity: good, originality: good, significance: goodPros: - The paper is fairly well written and the idea is clearly presented- To the best of my knowledge (maye I am wrong), this work is the first one that provides theoretical explanation for the tradeoff between accuracy and robustness- The visualization results supports their hypothesis that adversarially trained models percepts more like human.Suggestions:It would be interesting to see what kind of real images can fool the models and see whether the robust model made mistakes more like human. In this paper, authors propose safe policy optimization algorithms based on the Lyapunov approach to constrained Markov decision processes.The paper is very well written (a few typos here and there, please revise) and structured, and, to the best of my knowledge, it is technically sound and very detailed.It provides incremental advances, mostly from Chow et al., 2018.It fairly accounts for recent literature in the field.Experimental settings and results are fairly convincing.Minor issues:Authors should not use not-previously-described acronyms (as in the abstract: DDPG, PPO, PG, CPO) This is a very exciting proposal that deviates from the typical assumption that all future frames can be predicted with the same certainty. Instead, motivated by the benefits of discovering bottlenecks for hierarchical RL, this work attempts to predict predictable video frames  those that can be predicted with certainty, through minimising over all future frames (in forward prediction) or all the sequence (in bidirectional prediction). Additional, the paper tops this with a variational autoencoder to encode uncertainty, even within those predictable frames, as well as a GAN for pixel-level generation of future frames. The first few pages of the paper are a joy to read and convincing by default without looking at experimental evidence. I do not work myself in video prediction, but having read in the area I believe the proposal is very novel and could make a significant shift in how prediction is currently perceived. It is a paper that is easy to recommend for publication based on the formulation novelty, topped with VAEs and GANs as/when needed.Beyond the methods explanation, I found the experiment section to be poorly structured. The figures are small and difficult to follow  looking at all the figures it felt that more is actually less. Many of the evidence required to understand the method are only included in the appendices. However, having spent the time to go back and forth, I believe the experiments to be scientifically sound and convincing.I would have liked a discussion in the conclusion on the methods limitation. This reviewer believes that the approach will struggle to deal with cyclic motions. In this case the discovered bottlenecks might not be the most useful to predict, as these will correspond to future frames (not nearby though) that are visually similar to the start (in forward) or to the start/end (in bidirectional) frames. An additional loss to reward difficult-to-predict frames (though certain compared to other times) might be an interesting additional to conquer more realistic (rather than synthetic) video sequences. Summary:The paper considers online optimization with zero-order oracle. Motivated by nonstationarity of the objective function, impracticality is underlined for the two-point feedback approach. Instead, staying in the one-point setting, the proposed approach reuses the objective value from the previous round of observations, which is called as residual feedback. The variance of the corresponding proxy for the subgradient is estimated under more relaxed assumptions than existing in the literature. The proposed approach leads to smaller variance and better regret bounds. Regret bounds are proved for smooth/non-smooth convex/non-convex cases, the non-convex case being analyzed for the first time in the literature. Numerical experiments show that the practical performance of the proposed gradient estimator is better than that of the existing one-point feedback methods and is close to the performance of the one-point approach with two observations per round. The latter approach can be impractical for some applications.   Evaluation:I believe that the paper contains new interesting results on zero-order methods with one-point feedback, which are supported both theoretically and numerically. So, I suggest accepting the paper.Pros:1. New theoretical results which are significant for optimization and learning literature, as well as for applications. 2. Numerical results support theoretical findings.3. The paper is overall clearly written and motivated.Cons:1. There are several minor comments mainly on the clarity of presentation. See below.Minor comments1. Some related work seems to be missinghttp://proceedings.mlr.press/v48/hazanb16.pdf (non-convex optimization with one-point feedback)https://link.springer.com/article/10.1007%2Fs10107-014-0846-1 (non-convex stochastic optimization)http://papers.nips.cc/paper/5377-bandit-convex-optimization-towards-tight-bounds.pdfhttp://papers.nips.cc/paper/4475-stochastic-convex-optimization-with-bandit-feedback.pdf 2. Please consider writing explicitly on p.7 that Bach & Perchet (2016) use two function evaluations in each round. Also it would be nice to explain in more details, why their approach is impractical. For example, in the considered in Sect. 6.1 example, why one can not observe x_{k+1} two times (with different values w_k), and the evaluate the loss twice?3. The proof of Lemma 2.5 does not completely correspond to the statement of the Lemma. In the proof more is derived than stated in the Lemma, but under additional assumptions.4. In the first line of Appendix F, did you mean that $f_{\delta,t} \in C^{1,1}$? Also here Assumption 3.1 is used, which should be mentioned. ##########################################################################Summary:This paper proposes O-RAAC, an offline RL algorithm that minimizes the Conditional Value-at-Risk (CVaR) of the learned policy's return given  a dataset by a behavior policy. It learns a distributional critic, a VAE for imitation learning, and an actor that perturbs the VAE output to minimize the risk given by the distortion operator D (Here it uses CVaR).Several experiments were performed to show the effectiveness of O-RAAC, both with a simple 1-d driving environment and with a modified version of D4RL dataset. ##########################################################################Pros: The paper is well written, with comparisons with competing methods throughout. This makes the connection with those methods clear and easy to understand. The discussion around the design choices and the tradeoffs are especially insightful. (e.g. Huber loss over l1 or l2; VAE vs vanilla BC; etc.)The contribution to risk averse offline reinforcement learning is novel, with the use of CVaR and the idea of a perturbed version of a imitation learning actor. With the assumption that all the risks is captured within the reward. ##########################################################################Suggestions:The risk-neutral experiment in 4.3 was a bit confusing. I was not sure whether you measured the O-RAACs risk-neutral performance by setting alpha = 0, or you measured its performance with alpha=0.1, but using a risk-neutral metric. I think you did the latter after a few more reading. More clarification either in Table 2 or in 4.3 would be great. in 3.3, you said "...is a perturbation model that is optimized maximizing the actor loss (5)," This part was confusing as well, since one would normally minimize a loss. In this paper, actor loss = risk aversion, so maximizing risk aversion makes sense, but I wouldn't call it a loss.  This paper describes a role-based learning model for the DEC-POMDPs. The main contribution lies in the efficient discovery of roles from the joint action spaces and then learning a bi-level role assignment for each achievement. This is achieved in two steps. First, the joint action space is clustered into different role action spaces that reduce the action search space for each role. Second, a bi-level role assignment technique is used to learn action and roles for each agent. The technique is tested on StarCraft II micromanagement environments.For the action space reduction, the model learns action representations that can reflect the effects of actions on the environment and other agents. To this end, a deep learning model is created which predicts the effects of joint actions on the induced rewards and change in the effects. Actions generating similar effects are cluster together using K-means and are called roles action spaces. This restricts the joint action search spaces for each role. The role selector is now used to learn a bi-level hierarchical assignment to map the action-observation history of each agent. At the top-level the agents are mapped to their corresponding to roles based on a Q-value function of each role conditioned on action-observation history and at a lower-level similar Q-value function is used to find the agents action. To avoid too many concurrent selections of a single role and action by multiple agents, a global Q-value is learned from individual Q-values to ensure overall coordination between the agents. This is inspired by QMIX, previous work on multi-agent learning.Positives:1. The idea of reducing the search space by effect-based clustering appears interesting and novel.2. The technique leads to good exploration and performance in hard and super maps.3. The paper is well-written, and the technique is extensively tested on all the maps with useful ablationsMinor issues:1. Some comments/reasoning related to outlier roles and action spaces would have been helpful2. Do changes in the clustering algorithm leads to a significant difference in performance or role assignment?3. Compared to the previous approaches, the RODE-algorithm learns slower in most of the easier maps.  This paper proposes a new key-point based object detector, PolarNet, which predicts the distances between key-points and corner pairs (such as top-left and bottom-right pair or top-right and bottom-left pair) on polar coordinates. This is different from other key-point based object detectors such as FCOS which predicts distances between key-points and bounding box boundaries on cartesian coordinates.  The authors claim that the advantage of representing the offsets in the form of polar coordinates is this representation reduces the variance in the offsets, which makes learning easier.Pros:This is an interesting approach and new, to the best of my knowledge, in the context of object detection. The authors show that PolarNet outperforms other approaches such as FCOS and FoveaBox under the same backbone network, ResNet-101. The use of polar coordinates improves the performance of FCOS by more than 4% which shows the effectiveness of the polar coordinates. With a larger backbone and deformable convolution, PolarNet demonstrates state-of-the-art performance among all anchor-free detectors on the challenging COCO dataset.Cons:I am confused about the corner supervision in section 3.4.2. It seems to me that the corner supervision is to train PolarNet to predict the offsets which are in polar coordinates. But the authors also apply this to FCOS (FCOS + Centerness + Corner) and compare it with FCOS with polar coordinates (FCOS + Polar).  I dont understand the difference between them. How do the authors apply corner supervision if FCOS is predicting on cartesian coordinates (i.e. FCOS + Centerness + Corner)? How is that different from FCOS + Polar exactly? Or do I misunderstand the meaning of corner supervision? The authors also mention in the section where they introduce corner supervision that they train a regressor based on corner features. What does corner supervision mean exactly? Does it mean the authors extract features from the corners and use the features together with the key-point features when they predict the offsets? Or do they simply refer to the regression loss function?The use of IoU loss seems to a bit redundant. It seems that the $L_{corner}$ already trains the network to predict the offsets. Why do the authors still need the IoU loss? The authors should provide an ablation study to demonstrate how the IoU loss is affecting the performance of PolarNet. The papers proposes an interesting analysis that links several aspects of architectural design in Deep NNs to the spectral analysis and observed roughness. Different activations functions are considered in the study, mainly centered on deep CNN with or without skip connections (in the framework of ResNet v1 and v2). The starting point, which is not novel, actually, but relevant, is that specific types of non-linearities introduce harmonic distortions, and the effect is potentially amplified when multiple non-linearities are stacked. Theoretically, the paper shows that there is a concrete link between architectural choices in the network design and the blueshift in the frequency domain. Experimentally, the observations support the mathematical analysis. All in all, some of the conclusions regarding trainability of CNN architectures with skip connections have been already noted and do not seem greatly new, but the paper introduces a nice perspective to see this phenomenon in another light. The paper is generally well written and I appreciated reading it.The major downside I see in the current form of the manuscript is given by some aspects of the presentation. For instance, Fig. 1 is clearly misplaced (it should be in Section 4.4). Similarly, Figure 6 should be in Section 5. Moreover, abbreviations would be better used in a more uniform manner (e.g., SDFA, FDSA, SDSA). Regarding the reported experiments: are the given plots achieved by averaging over multiple runs? (only in one of the many experimental settings this information is given in the paper). Finally, the link between the left and the right sides in Figure 6 is not really straightforward, perhaps grouping together the short and the noshort results could be of help for the reader. ***Summary***I would firstly like to thank the authors for an interesting read. I enjoyed going through the submission very much.The authors propose to understand the qualitative effects of nonlinearities by studying the impact they have on the Fourier spectrum of deep neural networks. The central hypothesis is that nonlinearities with a lot of energy in their side lobes (high frequencies), lead to neural networks that have a rougher mapping and that are consequently tougher to train because the derivative landscape is also rougher. They back this hypothesis up with some mathematical arguments from the area of harmonic distortion analysis and with empirical experiments to support the qualitative predictions of this theory.***Pros***I found the submission very readable. I think the balance of text to mathematics in the main submission was about right, reserving the appendix for a more in depth discussion.I think that while the central finding that deep mappings are smoother is, in itself, not particularly novel, the chain of reasoning to get to this fact is new. I like the use of the Fourier spectrum to show this and the analysis behind how the spectra of various nonlinearities affect overall network smoothness..The choice of experiments, which sequentially back up the claims, makes for a good paper. I particularly enjoyed the results in Figure 2, which were very instructive and gave good insight into the predictions of the theory.***Cons and constructive feedback***In order from start to finish.In the abstract should differential be differentiable?I think a good paper to cite would be Avoiding Pathologies in Very Deep Networks (Duvenaud et al., 2014) who analyze deep kernels in Gaussian processes. While the underlying models are different, the kinds of qualitative results in this paper are very similar to the submission.I am concerned about the use of the Fourier spectrum to model the ReLU nonlinearity. Will there not be issues with the Gibbs phenomenon? The discontinuous gradient will mean that a spectrum exists, but reconstructions are poor.Paragraph below equation 3: uniformely -> uniformlyEquation 4: using t_j is confusing given that you use t in eqn 1. Please change to another symbolEqn 6: Please define the autocorrelation symbol in the main text.Eqn 6: Please define z versus z_jSection 3.2 discussion: I would assume that while higher order autocorrelations would broaden the spectrum they would also smooth it out. For high orders it would like Gaussian-like in shape. This would not necessarily lead to blue-shifting.Section 3.3: therfore -> thereforeSection 3.4 trivial -> triviallySection 3.5: Exponential downweighting. ResNets have combinatorially more medium length paths than short or long ones. So the average weight of a medium path is far higher than short or long ones. I would have liked to have seen a deeper analysis of this effect.Experiments: I found these very interesting. What is the motivation for only focussing on networks at initialization? I would have loved to have seen what a pertained network looks like.Are ensembles covered within the scope of this theory? They seem to have good performance but since each member is trained individually there is no smoothing of the training function, although the test loss function is smoother when all member models are combined.  The authors introduce a deep ensemble kernel learning approach as a linear-based learning combination, from a deep learning scheme, to approximate kernel functions under a Bayesian (GP) framework. Namely, a universal kernel approximation strategy is proposed from eigen-based decomposition and deep learning-based function composition. Then, a variational inference strategy is used to solve the optimization from kernel-based mappings. Two regularization strategies are studied: optimal prior covariance and isotropic covariance. Results demonstrate the benefits of the proposal.Two comments:Could you include the GP and sparse GP results to compare the predictions? Your approach can be scalable from the stochastic variational variance; however, the studied databases are small in terms of the number of samples so that the well-known GP algorithms could be compared.In most of the cases, isotropic and optimal covariance-based methods seem to achieve the same results; why?  Summarize what the paper claims to contribute. The paper introduces a deep-network-based approach to regression of responses to natural stimuli in mouse primary visual cortex. There is closely related work in the literature, but this paper achieves very good performance, partly through a new way of accounting for neurons receptive-field positions. The paper also provides a helpful analysis of prediction performance versus numbers of images and neurons used to train the model, and shows that the already excellent performance is not saturated with respect to the number of images. The work also shows that features learned by a core network generalize well across different mice. List strong and weak points of the paper.Strong points: -Empirical modelling of neural responses has a long tradition, and the results in this paper are state-of-the-art. -Thorough and insightful positioning in the recent literature. -Expert execution in terms of details of the technical work. -The method of parameterizing the receptive field location is well-motivated and effective. -The analyses are interesting and provide useful insights. Weak points: I wouldnt characterize any part of the paper as weak, but here are some minor suggestions to further strengthen it: -Say more about how the model can be used, or what insights might arise from it (there is only a short comment on inception loops). -Say more about limitations as a model of neural responses, particularly with respect to dynamics. While the method is impressive with respect to short-time-window responses, system identification methods have long been used to study temporal responses. I think a short comment on this scope limitation would help to further contextualize the paper. -An additional way to contextualize the results might be relative to the total number of neurons in L2/3 of VISp (I believe ~200K). Does this number have any significance relative to the dimension of the core-network output, or the number of recorded neurons? -Consider adding a sentence on ethics oversight regarding the animal experiments. -Consider adding a few further details of the experiments. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend that the paper be accepted. The paper addresses a long-standing problem very well. It introduces a new method that is well justified and effective. Overall, the performance is impressive, and the analyses are well done. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. What are the kernel sizes in the core?Which hyperparameters are adjusted in the hyperparameter search?Provide additional feedback with the aim to improve the paper. I was confused by the following sentences: & both readouts assume that the receptive field of each neuron is the same across features & readout has c + 7 parameters per neuron & (I only see c+6.)Fig 5 for the factorized readout & (I didnt get it until reading it four times and looking for these results in Figure 5 twice.)  The authors train a neural net to predict responses of mouse V1 L2/3 neurons to visual stimulation. The NN has a "core" that is shared between all neurons, and a neuron-specific readout. They train the core on multiple animals and find that it can generalize well: it can be used in a new animal and (with sufficient training of the readouts) achieve high performance. They also use a neat approach of constraining the readout weights (receptive field location) using the known retinotopy of V1. Finally, they show that their network outperforms task-trained ones at predicting V1 responses.This is nice work overall. I have a few suggestions:1) It might be worth considering other measures of performance, different from the normalized correlation coefficient. Recent work shows that this measure can have unintended bias, being substantially affected by trial-to-trial variability.See "The unbiased estimation of the fraction of variance explained by a model" from Pospisal and Bair (https://doi.org/10.1101/2020.10.30.361253) for details, and a potential solution.2) 2-photon imaging can have issues at detecting single spikes (see this preprint, for example: Relationship between spiking activity and simultaneously recorded fluorescence signals in transgenic mice expressing GCaMP6,  https://doi.org/10.1101/788802). So the neural dataset could in principle show more multi-spike events than single-spike ones, or have other issues. This is inevitable of course with calcium imaging, but it makes the problematic to compare with previous work that used electrical recordings. E.g., I don't think it is possible to prove better performance for this work than the prior ones, because of this difference in recording methods. A good follow-up work should try this method on electrical recordings from (say) monkey, and compare with performance from the Cadena, Yamins, Kindel, Klindt, Batty, etc. studies. Summary:This article introduces a learner-evaluator framework that incorporates the different variations of problems related to incremental learning. It also proposes a method called Continual Prototype Evolution for dealing with the most general version of the problem, incremental learning on data streams, in which the learning task is not specified. The paper presents an extensive amount of experiments indicating that in this scenario, the proposed method improves significantly on existing approaches in terms of accuracy and memory efficiency. The article is well organized, easy to read and understand.Positive aspects:- It presents an adequate coverage of the literature on continual learning in section 3 and an interesting framework organizing the area in section 2.- The method does not need information about the task being learned, being more applicable to real-world scenarios.- An extensive experimental evaluation of the proposed method and comparisons with related methods is provided.- Ablation studies indicate the most important aspects of the proposed model. Points to discuss/improve:- Momentum parameter: It took me a while to understand why the authors use a high momentum. While in gradient descent the momentum creates a tendency to keep the parameter changing in the previous directions of motion, here the momentum is supposed to make it change more slowly. There is another way of formulating Eq. 1, in which alpha works as a learning rate. Defining alpha = (1-alpha), a slow learning rate instead of high momentum, we can have Eq. 1 as: p^c = p^c + alpha(p^c - \bar{p}^c). This is easier to understand in my opinion, but authors can choose to disregard this if they prefer the current form.- The method assumes that one prototype for each class is enough. However, in certain problems, a class might need to be represented by different prototypes, indicating the different ways of being from the same class. How the model would handle these situations?- The conclusion does not discuss adequately the main findings of the article, probably due to lack of space.- Page 15 is completely black in most PDF viewers I tried. Only Chrome was able to display it correctly.Conclusion:I believe this article should be accepted as a see it presents interesting findings in the area of incremental learning on non-stationary data streams. ### Summary The authors proposed an imitation learning algorithm that utilizes the primal form of Wasserstein distance to match agents and experts state-action visitation distributions. They considered the upper bound of the primal form and devise the optimization method based on greedy coupling which makes learning suitable for sequential problems. With standardized Euclidean distance and exponential smoothing, the proposed method PWIL is shown to perform well for both MuJoCo and Door Opening Tasks and highly outperforms the baseline (DAC) for Humanoid. ### Quality Theoretical derivations are sound, and experiments are well-tuned and designed and highly support the authors claims.### Clarity The submission is clearly written overall, but I added minor comments on clarity in `Detailed Comments`### Originality The idea of submission is novel from the use of the primal form of Wasserstein distance in concatenation with greedy coupling. ### Significance I think the contribution of this work is significant in the sense that it considers the primal form of Wasserstein distance and eventually show the decrease of Wasserstein distance through experiments, whereas the existing works havent explicitly shown the performance in terms of probabilistic metrics. ### Detailed comments (p. 1, Abstract) We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning.- I agree that PWIL doesnt require neural networks and relevant gradient updates for reward learning but wasnt sure about the term offline is appropriate to describe this. PWIL involves the internal loop for reward updates during environment simulation. This is a bit confusing at the first glance since I believe offline is used when the environment interaction is not allowed, e.g., offline reinforcement learning. (My first bias was that the reward is learned before the interaction, fixed, and then used during training without any updates.) I guess therell be a better expression for it (like a non-parameter reward?) or simply ignoring the term offline is much better. - A sentence in the introduction The inferred reward function is non-stationary, like adversarial IL methods, but it is not re-evaluated as the agent interacts with the environment, therefore the reward function we define is computed offline. tries to clarify this term, but I dont understand why it is said that the reward is *not re-evaluated* since the most inner loop of **Algorithm 1** is related to the reevaluation of rewards. (p. 1, Introduction) Our method recovers expert behaviour better than existing state-of-the-art methods while being based on significantly fewer hyperparameters- In Figure 1, PWIL seems to perform comparably to DAC, not better than DAC, except for Humanoid. (p. 2, Background and Notations) $\delta$ is the discount factor- It seems like $\delta$ is used instead of $\gamma$ since $\gamma$ is used to indicate a stochastic matrix. However, since most potential readers may be familiar with the terms in RL and imitation learning, Id rather use $\gamma$ for the discount factor and a different letter for coupling. - Also, it collides with Drac distribution in the following paragraph. (p. 2, Background and Notations) from maximizing to minimizing its return- $\rightarrow$ from maximizing its return to minimizing cumulative costs?(p. 2, Background and Notations) requires the definition of a metric in the space- $\rightarrow$ requires the definition of a metric $d$ in the space (to easily link with one in the definition of Wasserstein distance)(p. 2, Method) We introduce a reward based on an upper-bound of the Wasserstein distance- $\rightarrow$ We introduce a reward based on an upper-bound of the Primal form of  Wasserstein distance(p. 3, Wasserstein distance minimization) This can be problematic if an agent learns in an online manner or in large time-horizon tasks. Thus, we introduce an upper bound to the Wasserstein distance that yields a cost we can compute online, based on a suboptimal coupling strategy.- I agree that this is a key idea of the submission, but I was wondering if it isnt available to calculate the cost even when off-policy RL is applied as was done in the experiments. (p. 3, Greedy Coupling) Eq (4)- It seems to me removing $\inf_{\pi\in\Pi}$ is possible and fits the whole equation inside the format. (p. 4, Figure 1) Note that the total cost with the greedy coupling is 7 whereas the total cost with the optimal coupling is 5.- With the definition in Eq (5), I think the multiplication of $\gamma_\pi^q[i, j]$ (which is the uniform distribution) is missing. - Also, it would be better to add detailed calculations in the Appendix for curious readers. (p.4, Greedy Coupling)  The algorithm computes the greedy coupling with a complexity $\mathcal{O}((|\mathcal{S}|+|\mathcal{A}|)D)$- Hows the complexity calculated in detail?(P.4, Experiments) As DAC is based on TD3 which is a variant of DDPG, we use a DDPG-based agent for fair comparison.- This makes me a bit confused since it was non-trivial for me to link off-policy RL with Algorithm 1 directly. One thing Im really curious about is how the reward comes from the replay buffer used in a DDPG-based agent. (P.4, Experiments) Figure 2- Hows the computational cost for overall evaluation? Specifically, I think theres no problem with the evaluation of Wasserstein distance when a small number of the expert demonstration is given, but the calculation time will hugely increase for a large number of expert data (which I believe can be in practical scenarios). This paper tackles the challenging question of how deep networks might learn to extrapolate knowledge outside the support of their training distribution. The paper contributes both with novel theoretical arguments as well as with empirical evidence collected on targeted cases. Differently from other recent approaches to the problem, the theoretical analyses presented here are non-asymptotic and provide precise information about the kind of functions that MLPs can learn in the proximity of the training region. Moreover, the authors provide compelling arguments about the need to explicitly encoding (task-specific) non-linearities in the input representation and/or in the model architecture in order to promote successful extrapolation.Overall, the paper addresses important issues and can be considered at the frontier of deep learning research. The paper is well-written and the recent literature is properly reviewed. In light of this, I think the paper would be of interest to the ICLR community. However, I would like to explicitly mention that I was not able to carefully review all the details and proofs reported in the Appendix, which is of an unusual length (almost 40 pages) for an ICLR paper.Comments for possible improvements:- The analyses reported in Appendix D.3 / C.4 regarding the extrapolation capability of MLPs with different activation functions (sin, tanh, quadratic) are relevant and should be emphasized. They could also be expanded, for example by considering some data generation tasks analyzed in the main text.- It would be very interesting to extend this analysis to other simple problems, where MLPs cannot extrapolate appropriately. I am specifically referring to the simple counting and arithmetic tasks discussed in [1], where generalization outside the training distribution was achieved by adding ad-hoc gate units to the network. I think this domain is particularly relevant here, given that arithmetics is mentioned by the authors in the opening sentence of the paper.[1]A. Trask, F. Hill, S. Reed, J. Rae, C. Dyer, and P. Blunsom, Neural Arithmetic Logic Units, in arXiv:1808.00508, 2018. # SummaryIn this paper, the authors proposed a unified seq2seq model for structured prediction tasks in NLP. They let the seq2seq model produce mixed outputs of special tokens and the original sentence. Different NLP tasks, including relation classification, entity relation extraction, NER, etc. can be converted into this seq2seq problem by adding special tokens.  The experiments show that the proposed model does better than the previous state-of-the-art, albeit with the help of multi-task and multi-dataset learning, on some of the tasks/datasets. ## Pros:1. A unified framework that allows for multi-task and multi-dataset learning. Their experiments also show that their model could benefit from multi-task, multi-dataset learning. The experiments on few-shot relation extraction show that their model could transfer knowledge from high-resource tasks to low-resource tasks.2. The formulation is neat and extensible. More difficult structured predictions tasks (in terms of structure), e.g. dependency parsing, are in principle convertible to this format, although the authors didn't try it on parsing. ## Questions:For structured prediction tasks, searching for the best output is a crucial part. However, this paper doesn't explore search strategies too much. Only in the appendix, beam search is mentioned. More concretely, I would suggest the authors try to answer the following questions:1. How much could we improve the current model purely by using better the searching strategy (the headroom)? Different from CRF, the structured prediction model is not markovian, which means we have to brute-force the best output. Is it possible to calculate such an upper-bound performance of the current model? If so, what would be the upper-bound for each task?2. In this paper, the DP alignment method is a post hoc method. What if we add such a monotonic alignment to the decoding process? To summarize, I think this is a good paper in terms of extensive experiments and convincing results, but the search strategy still needs to be explored and justified for structured prediction tasks.  ##########################################################################Summary: The paper proposed a theoretically grounded O(N) approximation of the softmax attention. The key idea is to interpret attention as a kernel function and construct the random feature projection that can reproduce this kernel. It is highly non-trivial to derive a feature mapping that can accurately approximate the softmax kernel. To better approximate the softmax kernel, the author proposed some important design choices, all of which are supported by theoretical and empirical evidences. The author showed that 1) adopting non-negative random features is very essential to the approximation and the proposed Positive Random Features (PRF) can effectively reduce the variance when the attention values are small, 2) drawing orthogonal random matrices can further reduce the variance of the approximation, 3) the final proposed Performer model runs faster, takes less memory, and has better performance than other O(N) and O(N logN) attention methods.##########################################################################Reasons for score:  The paper is very well-written and should be accepted. This is an important landmark in the research about O(N) attention. The design of the random feature mapping is reasonable and theoretical analysis is convincing. Experiments show that Performer is better than the other O(N) attention methods and also other efficient attention methods. ##########################################################################Pros:  1. The paper gives a provable O(N) approximation of the softmax attention. The method works without assumptions on the structure of the attention map (like sparsity). The theoretical proofs provide good insights on how to design a good O(N)-complexity approximation to the  attention mechanism.2. Apart from approximating the softmax attention, the proposed FAVOR+ method can be utilized to approximate other attention kernels. In fact, the author has experimented with Performer-ReLU, which outperforms Performer in some experiments. This provides the insight that softmax attention may not be the best choice.3. The author conducted very comprehensive ablation studies on different components of the proposed method. This includes: 1) effectiveness of using the positive features, 2) drawing orthogonal random samples, 3) redrawing the random samples ##########################################################################Cons:  1. From Figure 5 and also Figure 15, periodic redrawing is quite essential. However, the author has not mentioned about the implementation details on how they redraw the random samples and how to choose the period. For me, I feel that this hyper-parameter should be important because the model may need to ensure that each group of samples has been trained for a sufficient amount of time.2. Performer-ReLU has replaced the attention kernel and can sometimes be better than the softmax attention. Thus, I feel the author may also want to compare with the linear attention method in ((Katharopoulos et al., 2020) "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention".##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################Typos: (1) Page 5, after "than those from SM_{2m}(x, y)", there is an additional right bracket. ### SummaryThe paper attempts to improve the interpretability of RL agents' action selection process by (a) proposing embedded self-prediction (ESP), a model that embeds generalised value functions (GVFs) in the action-value function of the agent with a "combining" function to and (b) ESP-DQN, an extension of DQN that augments experience replay tuples with a GVF feature vector and that decomposes the model into separate combining and GVF parameters. These enable to define action-values with respect to predefined feature maps, thus providing more "resolution" into the behaviour of the policy.### Good stuff1. The idea of decomposing the policy into GVFs as a way to force explanations wrt. some features is *brilliant*, and it is well executed when combined with the contrastive explanation system.2. Sections 2-4 provide both a clear introduction to GVFs as well as a detailed and sound description of the overall framework.3. The related work section is fairly tight, but actually covers a good amount of necessary and relevant related work, which makes it easy to scan through the literature.4. The experimental section does employ mostly fairly similar environments, but it is clear in the hypotheses being tested, and it is fairly satisfactory considering what it is attempting to evaluate.### Uncategorised notes- This is more of a meta-comment, but I enjoyed reading the paragraph about manually-designed features in Section 1. I understand why the authors felt the need to write it, and it is a sad state of affairs that it is now often a requirement to argue what to many people is just reality.- I wonder if it'd be worth it to test the method on Atari, through possibly the use of MinAtar: https://github.com/kenjyoung/MinAtar -- It feels like there's a gap in difficulty (of learning and analysis) between the ToW setting and the rest of the environments, and something aking to a middle ground would probably be a good setting to add. Complex gridworlds such as BabyAI and the NetHack Learning Environment are probably also good options.### Final commentsOverall, I'm extremely happy to strongly recommend this paper towards acceptance. It is well written, it introduces a method that attempts to move forward towards solving an important problem in Reinforcement Learning, and there's a significant amount of details in the paper that would make it fairly straightforward to reproduce.### Typos#### Sec 1- RL agents explain its... -> their- directly "embed"... -> embeds- train those... -> trains#### Sec 3- Combination function update -> Combining? The manuscript is at times a little inconsistent. Summary:The authors propose a scheme for combining the trusted mathematical properties of Dual Tree Wavelet Packets with CNN-style feature extraction. Specifically, they learn simple functions of wavelet kernel outputs, rather than learning kernels themselves from scratch. The fundamental gains are a significant drop in the number of parameters, while retaining feature expressiveness and intuition.Clarity:The paper clarity is significantly above average.Quality:The paper is very clear and concise, but it might be better if it compared across other nets besides AlexNet. For example, how does it compare to nets with feature extractors in the same order of magnitude (3k parameters), vs much bigger (alexnet: 25k).Originality/Significance:I am not extremely familiar with literature related to this idea. However, I think constraining CNN filters in this way is an important area of research. Originality, Significance: This paper establishes reference points for modern LTR research. The fact that RankLib is a very popular but also a weak baseline has been exploited by too many researchers for too long. When I was reviewing other LTR papers, I often had to point out that the proposed method significantly underperforms LightGBM. This fact has been fairly well known, but apparently not widespread enough. Having an ICLR paper published on this issue will help spreading the fact, which is significant on its own.Quality: Considering the popularity of RankLib, deeper analysis of why LightGBM outperforms RankLib would've been very nice, however. Authors do mention that LightGBM has more features, but it is unclear which exact feature of LightGBM contributed to such a significant difference between the two. Understanding the reason for LightGBM's superiority could potentially help us to develop better LTR models, neural or not neural. Comparison against Catboost https://github.com/catboost/catboost would've also been useful, as it is often claimed to outperform LightGBM.Proposed DASALC framework is quite simple and uses mostly standard techniques, and this is an advantage as a reference point. Still, DASALC significantly outperforms previous neural LTR approaches. Also, although the idea of applying these standard techniques on LTR seems straightforward, but I argue that's only due to the benefit of hindsight; neural LTR has been a fairly active area of research, yet these techniques haven't yet been widely used in LTR literature. It would've been very interesting to see how these techniques improve the performance of previous neural LTR models; log1p transformation, data augmentation, and model ensembling would straightforwardly apply to other neural models as well.In summary, I believe this paper will foster more productive research by establishing the strong baseline on both decision-tree based method (although it has been known) and neural method (on which authors make good technical contributions).Clarity: The paper is quite easy to follow. This paper proposes embodied game-playing with artificial agents as a method to learn better representations of their environment. They describe a game, cache, which is a variant of hide-and-seek played in a virtual environment and a method for training an agent to play the game. They present results which demonstrate that the static representations learned through game-playing perform better than other pre-training tasks within the same virtual environment, on both virtual vision and real world vision applications. They also show that the dynamic representations are useful for completing object permanence tests inspired by developmental psychology research.This research direction (both biologically-inspired CV as a whole and specifically game playing as a pre-training task) is exciting and seems extremely promising. Assuming I understand the methods and results correctly, this seems like a clear accept. My only reservations are the complexity of the task and how the results are presented in the paper, which are relatively minor issues.The task, Cache, seems extremely complicated to implement and train, since it involves five different stages of embodied exploration/action and adversarial reinforcement learning. While all of these steps are necessary to play the game like humans (or ravens) do, it is unclear to me whether the important learning is occurring as a result of the whole process, or of a specific stage. It seems possible that a simpler task may have comparable results, and it would be an interesting direction for future research to investigate how each of these stages are contributing to learned object permanence. I'm not an expert in developmental psychology, but I know there is some research showing that smooth-motion visual signals are the main prerequisite for object permanence in chickens (https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.12796). An ablation study over game mechanics or world properties could lead to some really interesting results.Of course, the paper you've submitted already describes a huge volume of research work and doesn't need more experiments. Unfortunately, it is a bit difficult to follow as a result. You may want move some content from Fig. 3 and 4 (those figures are difficult to parse, even with my pdf zoomed in 250%) to the appendix, if only to focus the reader's attention on specific results. While arguing that your agent successfully plays the game is important, it should take a backseat to the experiments which probe the properties of your static and dynamic image representations. In this paper, the author's propose an embodied adversarial reinforcement learning agent that can play a variation of hide-and-seek called Cache. This environment is a high fidelity interactive world. The authors argue that the agents are able to learn flexible representations of their observations which encode information such as object permanence, free space and containment. The authors provide a well-written description of their game and provide well-designed visualizations to understand the interactions of the agent and the observations required for the learning problem. The authors present a concise and well-researched literature review which serves to distinguish their work as novel and well built on underlying prior work. The authors make several contributions in this paper. First, an adversarial game Cache which permits the study of representation learning in the context of interactive visual gameplay. Furthermore, they present an agent which can perform strongly on the benchmark that they create even in comparison with human players. Finally, they present a study of the static and dynamic representations learned by the agent.The authors provide an overview of the architecture for the Cache agent. It is well researched, well-reasoned and presented in a way that is easy to understand and reproduce.The authors present multiple experiments in their paper and the corresponding deep Appendices. Specifically, they attempt to understand how agents can learn to proficiently hide and seek objects with static image representations and dynamic image representationsOf particular interest are the dynamic tasks the authors present in Section 5: Experiments. The authors make allusions to study of human children and object permanence. The results are quite compelling and well presented in a way that makes them easy to understand. I would urge the authors to consider if there are any non-ablative baselines against which they may be able to compare their model. One baseline that the author's use is human volunteer comparisons, these are compelling and presented in the appendix.Figure 3 is perhaps the weakest figure in the paper. While it provides a great amount of information I would argue that the results are presented in a way that makes them less legible than if the figure was broken out into methods and results. The same could be said about Figure 4. The visualizations of the agents representations of synthetic and natural images are combined into a single image with performance on corresponding tasks. While I understand that the authors made these choices for space constraints, I would urge them to reconsider the visual hierarchy of the figures to emphasize the performance of their agents. I feel like this is a paper that would be of great interest and benefit to the ICLR community. The authors capture uncertainty in deep networks by employing stochastic activation functions but deterministic network weights.  They place a Gaussian process (GP) prior over activation functions, propose a customized GP kernel, and demonstrate that activation uncertainty leads to both more sensible extrapolations and better-calibrated interpolation uncertainties when compared to standard Bayesian neural networks (BNNs)  with weight uncertainty and deterministic activations. The observation that BNNs have too many parameters for meaningful posterior inference is widely acknowledged; modeling activation uncertainty instead of weight uncertainty is an interesting approach for alleviating this issue.  While GP based activations have been considered before, the paper does a particularly good job of executing on the idea and clearly demonstrating its advantages. The experiments are well thought out and nicely illustrate the benefits afforded by the paper's contributions.  Overall, I  enjoyed reading this paper and only have minor quibbles.1. As with any GP based approach computational scalability is a concern. While it is reassuring that only a modest number of inducing points are needed, the complexity of one epoch: $O(NM^2(D_1 &+ D_L))$ suggests that it would be challenging to scale auNN to deeper / wider networks. Section 3.4 demonstrates scalability to large data but it does not demonstrate scalability to large architectures. It would be good to include a discussion of this issue and how the authors envision scaling the approach to more standard deep learning architectures. Table 3 should include timing numbers for both standard variational BNNs and functional BNNs (even if they are not competitive in terms of performance). Having these numbers would help guide follow up work interested in replacing GPs with (potentially more scalable) parametric function approximations.2. The choice of the GP kernel seems to have a large effect on the learned activations (Figure 8). It would be interesting to consider uncertainty over GP kernels as opposed to a-priori fixing all activations to be drawn from GPs with the same kernel and potentially allow different layers or different units in a layer to prefer activations drawn from GPs with different kernels.3. I found the sentences (section 3.1, page 6, last paragraph) highlighting differences between a deepGP with an additive kernel and auNN confusing. I think the point being made is that in auNN different nodes in layer $l+1$ use a *shared* set of distinct $D_l$ functions, with each node weighting these functions differently. While in DGP there are no shared functions. From the text it almost sounds like that in auNN functions in layer $l$  (all $D_l$ of them) are the same, which doesnt seem to be the case. 4. The observation that many BNN inference techniques struggle with appropriate gap uncertainty was concurrently made by Foong et al., and Yao et al., https://arxiv.org/pdf/1906.09686.pdf  Title: HEATING UP DECISION BOUNDARIES: ISOCAPACITORY SATURATION, ADVERSARIAL SCENARIOS AND GENERALIZATION BOUNDSSummary of the paper:The idea of the paper is to introduce a new view on the geometry of the decision boundary of a classifier. Just as we may speak of the "margin" as in large margin methods and the hinge loss, the paper introduces the idea of a heat diffusion from the decision boundary - the amount of heat diffused to the data points gives a more subtle notion of stability. What a creative idea. Even cooler, the authors show that we may utilise the Feynman-Kac duality to now cast this in terms of the probability of a random walk starting at the data to hit the decision boundary. This is appealing because it differentiates between being near to an e.g. long thin decision boundary which approaches from just one side, to being completely surrounded by the decision boundary - something not accounted for by distance to the boundary.Most of the paper is a really nice and gentle discussion of this idea. I gained some appreciation for the work even though it is extremely technical under the hood, and this is due to the nice writing in the main paper. To allow this the author's had to push their main result to the end of the paper (proposition 4.1) and leave the details to the appendix, but this is a fair trade-off in my opinion.It is remarkable that in spite of the technicality involved, the authors manage to obtain proposition 4.1, an impressive first application of the new notions. The appendices are dense, but still nicely written and enjoyable even for the relatively uninitiated such as myself.Pros:This paper stands out as genuinely creative and novel. It is written with an enjoyable style which will surely motivate many theoreticians to delve into the details. I only wish I had the time and talent to do so myself.While primarily theoretical and highly novel, the authors even include some thought provoking experimental results.Cons:One might argue whether the structure of the paper is ideal, but I would counter that the paper is extremely pleasant to read, and that it makes more sense to try to lure the reader into a curious mindset rather than bash them on the head with heavy details from the outset. Nicely done.Recommendation:I strongly recommend to accept this paper. Even if the details - which I have not checked - prove to have issues, the novelty of the work makes it a must-have in the portfolio of ideas included in the upcoming ICLR. The authors propose SCOFF, a novel architectural motif, one with memory, which, as they describe, can serve as a drop-in for an LSTM or GRU within any architecture. It is inspired by the notion that when modeling a structured, dynamic environment (such as one with objects moving around), one must keep track of both declarative knowledge and procedural knowledge. They propose that these two types of knowledge be factored, creating an architecture consisting of "object files" (OF) whose evolution is governed by input, all objects, and  "schemata" which can be selectively applied to each OF.They evaluate SCOFF along several axes:(1) Does SCOFF successfully factorize knowledge into OFs and schemata? (2) Do the learned schemata have semantically meaningful interpretations?(3) Is the factorization of knowledge into object files and schemata helpful in downstream tasks?(4) Does SCOFF outperform state-of-the-art approaches?To do this, they use several video prediction tasks as well as an RL task.This is a very interesting paper, with a natural, novel motif. It is well-written -- the motif has several important attributes, and one can quickly come to understand them (though perhaps a more involved diagram, like Figure 2 but showing what parameters come into play where, might be useful). The experiments appear to be carefully done, with considerable effort, and they put forth interesting evidence towards an affirmative in each of the above four questions.While the evidence put forth is useful, I do think there is considerable follow-up work needed to really demonstrate the efficacy of this system. In the realm of video prediction, the tasks considered are fairly simple, and certainly what is of true interest is video prediction much closer to the real world. With this, there are a wide variety of techniques and benchmarks (https://arxiv.org/abs/1804.01523 and its follow-ons come to mind as useful to quickly try). Much recent work has deferred the task of obtaining a reasonable encoding from/decoding to real-world images and assumed it in order to make progress on predicting the future within a fixed encoding (https://arxiv.org/abs/1612.00222, https://arxiv.org/abs/2002.09405, https://arxiv.org/abs/1806.08047) and have with them benchmarks to which the authors' method could be adapted. Certainly for some of these, SCOFF could be a subcomponent that augments these methods. I think these more complex future predictive tasks could better stress-test the structure of how information passes through SCOFF. Related, many of the experiments rely on the encoding/decoding provided by [Van Steenkiste 2018], which showed considerable success on the environments used, and it would be interesting to see how crucial that is.I recommend acceptance. The paper is interesting, well-written, and the experiments are useful. While I look forward to more definitive demonstrations of the utility of this approach, I do think that the amount done is considerable and warrants publication, and these important follow-ups would take a great deal more effort and are for future work.A more minor comment, I think more details could be given for the RL task, both in model implementation and in exactly how the test task is specified -- apologies if I missed but I only see train environment details in the appendix.Minor, wrong "two"/"to" in "Single object with switching dynamics" experiments description. This paper investigates the attack of a spatiotemporal GNN and also proposing a method to find the weakest vertex for this attack. It investigates the strength of the suggested approach. The results are convincing.   It is well written with a few edits to make:- Page 2, ln 8: Spelling of spatiotemporal - Page 2, Section 2, par 2: These cannot... instead of These work cannot... - Page 3, par 2: equation 1 should be repositioned for flow. - Equation 6: \min etc instead of min etc.- Page 4, last paragraph: refer to the section by number instead of saying 'above'- Page 5 and further: weakness and others have the speech commas facing incorrectly outwards. These could also just be removed rather. - Page 6, par above Table 2: Refers to the 'above results' which are actually below in Table 2 - use labels and ref. - There are many unpublished references included - these should possibly be reduced - many are probably now published? - Ref Dai 2018 has et al in it?  Summary:===========The paper provides an interesting direction for improving the Graph Attention Networks. More specifically, the authors propose a self-supervised graph attention network (SuperGAT) designed for noisy graphs. They encode positive and negative edges so that SuperGAT learns more expressive attention. They focus on two characteristics that influence the effectiveness of attention and self-supervision: homophily and average degree. They show the superiority of their method (4 variations) by comparing it with many state-of-the-art methods, in 144 synthetic datasets (with varying homophily and average node degree) and 17 real-world datasets (again with various ).Reasons for score:===========Overall, I vote for accepting. I find very interesting the idea of using self-supervision to improve graph attention networks and the experiments are nicely done and convincing. The authors do an impressive work to include as much information and results as they can in the given space.Strengths:===========- The paper is about an interesting problem in the ICLR community. Graph Attention Networks have gained a lot of attention in the recent years from researchers in the field of graph and node representations with applications in node classification and link prediction. The idea of adding the advancements in recent direction of self-supervised learning to improve the learnt representations seems very promising.- The authors have done a great job in the structure and the presentation of the paper. The paper is well-written and especially the sections Experiments and Results are well-structured and contain a lot of packed details in the design and the outcome of the experiments. More specifically, Figure 4 stands out as in a very limited space contains the information for the best performed model in all 144+17 graphs!- The contributions of this work include the proposed method (GANs with self-supervision), but also an analysis for the selection of the best model depending on two important features of the graph (homophily and average degree).- The authors compare their method with all state-of-the-art methods and also four variations of their own model and exhaust their evaluation by testing the performance in 144 synthetic graphs and 17 real-world datasets (including the benchmark datasets that are usually being used in this domain).Weaknesses:===========- The proposed method uses two known graph attention mechanism as building blocks, they use negative sampling and they add cross-entropy loss for all node labels and self-supervised graph attention losses for all layers. These building blocks and mechanisms are known in the literature, and as a result the proposed method adds incremental novelty compared to the related works.- In Appendix, A.3, the description and discussion for t-SNE plots is limited or absent. It would be better to add more details to it, as for example why this is a good representation and how the representations improve or not based on the hyperparameters. Also, how the results in the subfigures differ in terms of representations. It is difficult to get any insights from these plots.Questions during rebuttal:===========- Overall, my recommendations for more analysis and insights of the results are all responded from the Appendices. I would like a comment and clarification from the authors regarding Appendix A.3 Figure 5 (t-SNE plots), even though it is not in the main paper submission.- My understanding is that the authors are going to release the code upon acceptance, is this correct? In the repository that the code will be released, it will be useful to also add links to all 17 public datasets to ease research in the field.  In this paper, the authors present a study of different aspects of language-specific model capacity for massively multilingual machine translation. To this end, language-specific behaviour is achieved via a combination of conditional computation to decide whether to use language-specific parameters or not and statically assigning experts for each languages. The language specific sub-layers are incorporated throughout the network. The training objective allow budgetary constraints on the amount of language-specific parameters. The paper does a systematic analysis on the role of language specific parameters using the proposed architecture. Based on the analysis, recommendations on design of multilingual NMT architectures are proposed and their efficacy validated experimentally. The study sheds light on the amount of language specific parameter sharing, their distribution in the network, impact of language, etc. I find the analysis presented in the paper very interesting and insightful - and distinguishes it from previous work in this area. The hypothesis are clearly stated and the experiments are well designed. The findings from the analysis are an important addition to the understanding of the role of language specific parameters in multilingual NMT. In terms of modelling, the work follows in the line of recent work on language-specific parameters for multilingual NMT. The deviation from existing work is mixing elements of conditional computation with language specific computation. I see this work more as an analysis on language-specific parameters for a particular LS-model rather than a novel architecture. It is not clear how this model would compare to other models using language specific parameters (sparsely gated mixture of experts (Lepikhin et al 2020), light-weight adapters (Bapna et al 2019)  ).Questions: - Previous work has tried to combine both language-specific and shared parameters (Wang et al 2018), rather than making a binary choice between these. Did the authors compare with such an approach? - Since a major part of the model contains shared parameters, was there a need for new set of shared parameters along with the language-specific parameters. The gating decision could have been to bypass the language-specific sublayer or not.ReferencesYining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu, and Chengqing Zong. Three strategies to improve one-to-many multilingual translation. EMNLP. 2018. This paper presents a novel class of associative memory models. The model is expressed as a network with two-body interactions (synapses) and a well-defined energy function, and it is shown to generalise and unify several existing approaches (Hopfield Networks, Dense Associative Memories and Modern Hopfield Networks). Besides its theoretical and computational properties, the model is presented as being more biologically valid/plausible than some of the existing approaches it generalizes.Overall the paper is solid, well written and highly relevant to the ICLR community. My initial recommendation is to accept. I only have a few nitpicks concerning the "biologically plausible" angle.More specifically, while I agree that the proposed model is *more* plausible than some of the other approaches discussed, its absolute level of biological plausibility remains limited. The authors recognise this point and devote a paragraph to discussing it at the end of section 2 ("For the purposes of this paper we defined biological plausibility as&"), but it seems odd to have this passage buried at the end of the mathematical derivation, rather than up front in the introduction. I suggest that this passage is moved forward to a more prominent location.Furthermore, given that this is essentially a paper on the theory of abstract associative memory systems, the emphasis given to the biological angle in the title seems eccessive, and the choice of the word "neurobiology" somewhat puzzling. In my opinion the title would be a better description of the content of the paper if the reference to biology was toned down.Finally, in the introduction: "typical synapses are not highly reliable, and a cortical synapse stores no more than one or two bits of information". While I agree with the general spirit of the observation that synapses are not typically very reliable, some source should provided to back up the quantitative statement about synaptic storage capacity. I am not a specialist on the matter, but this seems surprising in light of work showing that the capacity of hippocampal synapses can be up to 3 to 5 bits (Bartol et al 2015, Bromer et al 2018).### References Bartol Jr, T. M., Bromer, C., Kinney, J., Chirillo, M. A., Bourne, J. N., Harris, K. M., & Sejnowski, T. J. (2015). Nanoconnectomic upper bound on the variability of synaptic plasticity. Elife, 4, e10778.Bromer, C., Bartol, T. M., Bowden, J. B., Hubbard, D. D., Hanka, D. C., Gonzalez, P. V., & & Sejnowski, T. J. (2018). Long-term potentiation expands information content of hippocampal dentate gyrus synapses. Proceedings of the National Academy of Sciences, 115(10), E2410-E2418.  The paper considers the problem of representation learning of actions, i.e., learning a decomposition of action-value function in multidimensional action spaces using hypergraphs. The key idea is to represent the actions as a hypergraph and learn a representation for each hyperedge in an arbitrary hypergraph. Consequently, the architecture conduits of a neural network for each hyperedge that are then combined using a mixing function. The choice of mixing function appears to be dependent on the problem -- for instance, it could be an universal mixer such as a neural  network or it could be a simple summation. The key impact of this architecture could be in the bandit problem setting with very large action spaces. Empirical results on a few atari games (29) and simulated physical control benchmarks clearly demonstrate the superiority of the approach.The paper is written well. The problem is well motivated and sufficient details are provided. Experimental results appear convincing. The use of hypergraphs themselves are not necessarily novel but the implications in large multi-agent systems makes the paper compelling. Personally, i feel that the paper can have a good impact. I do have a few comments:First, I think the term relational inductive bias is quite a stretch for here. It really is much more than a simple inductive bias. You are constructing the full architecture. Granted that the structure serves as a bias but this is much more than an inductive bias. It really can be interpreted as a search bias as well. Moreover, relational is also misused a lot (not just in this paper). IF you claim relational, then you should demonstrate generalization. Without that one cannot claim relational. I dont think your method is generalizable to varying number of objects in the domain. I suggest calling these as structured problems more than relational domains. The atari games for instance are not even close to being relational. I am not sure I understand why a 3-hyperedge will have higher variance. Could you kindly explain this? Does this mean that you are overfitting in terms of creating newer edges that dont exist? Is it too fine-grained of a representation? Some speculation could certainly improve the paper.  In this paper, the authors present a system that exploits both natural-language instructions, and demonstrations, to learn how to perform multi-subtask tasks in a Minecraft-like environment.  The system has two objectives: First, given a state and objective, learn to generate a natural language description of the high-level subtasks to perform (based on training from human instructions and demonstrations); and second, learn a policy to actually perform the steps.  The policy network accepts both the state description and the final state of the natural language generation network as input at every step.  The system's zero-shot performance is evaluated on previously-unseen tasks with neither demonstrations nor natural-language descriptions provided.The performance of the network is compared to a number of baselines, some of which are ablated versions of the proposed system, and some of which resemble systems from the literature with similar goals.The basic idea --- predicting the high-level instructions for a goal, and then using the predicted instructions as an input to the low-level policy --- is a very creative solution to the problems of hierarchical planning, and seems to be effective.  The additional interpretability benefits are very valuable in their own right.  Overall, I think this technique is likely to have a significant impact on future work.I have some reservations about the clarity of the writing, and the breadth of the empirical comparisons:The writing is at times difficult to follow, in ways that I think could be straightforwardly addressed, e.g.:- p.5/6: I found it hard to understand what the difference was between baseline 2 (IL + Generative Language) and baseline 3 (IL + Discriminative Language)- p.6: "The model stores the past T states as input to predict the T+1 state, where (T=3)." The letter T is probably being incorrectly reused here; maybe it's something like "at time T, the states T-2, T-1, and T are used to predict state T+1", but I can't be sure.I'm also a little concerned about whether the right baselines were used in the empirical evaluation.  The "ablated" versions of the proposed system are definitely valuable for demonstrating that all of the proposed components are necessary, but I am left wondering how the system's performance compares to the state of the art.  There appears to be only one baseline (#3) that directly corresponds to related work in the literature.  I would appreciate some more direct comparisons to what can already be accomplished in this domain. Context and summary of the results:Data poisoning attacks deal with adversaries who change the training set, up to a certain degree (controlled in different ways), with the aim of lowering the "quality" of the produced model. Previously methods were designed to tolerate adversarial perturbations while preserving low risk in the produced model.This paper studies point-wise certification methods for the decision made on a test instances against data poisoning.  Namely, for a given point, we would like to know: how many changes in the training set would still lead to the same produced label. Knowing this result gives confidence about the predicted label, even if some poisoning has already happened (to a certain degree). Exhaustive search (on various cases of data and retraining) would not be possible, and a theoretically sound algorithmic method would be necessary.This paper proposes a general, simple, yet effective idea for certification in poisoning context: partition the input data into a bunch of subsets and then train sub-models on them independently; then output the majority of the predictions. It is not hard to see that if the majority vote is far by amount alpha from the 2nd majority vote, then one can certify up to alpha/2 changes in the training set. To make this proposal formal, there are some subtleties that the paper handles by using hash functions to do the partition (so that the order of the elements wont affect which subset they land). This approach is indeed reminiscent of "Bagging" (Bootstrap Aggregating), but this seems to be the first work (along with concurrent cited works) that apply this to certification under poisoning attacks. Other than studying general poisoning attacks (in which changes in the data set are measured by Hamming distance), the paper further applies the idea to the special case of label-flip attacks, which were previously studied by Rosenstein et al. as well, but this work shows how to improve their bound through the experiments on popular data sets quite strongly. Along the way, (for label flipping) there are some neat ideas also to improve the results and the efficiency. (1) The paper shows how to benefit from semi-supervised learning by each of the sub-models is still trained on the *whole* data set, while only a subset of the samples (i.e., those in a particular partition) would have the labels kept. (2) To make this scalable, the paper carefully picks the semi-supervised method in such a way that all trainings of sub-models share the same 1st step of dealing with non-labeled data (on the *whole* data set). To evaluate: Certification of decisions under poisoning is a very recent and important direction. The results of this work are convincing as they improve the bounds achieved in previous work quite strongly. The methods used here, though seemingly simple in the hindsight, are sound and might be applicable in other settings as well. The paper is written quite clearly. I, therefore, recommend acceptance.Comments:The paper mentions that "Rosenfeld et al. (2020) does not provide a provable defense for this more general case."However, it seems the latest version of this previous work of Rosenstein et al. (from Aug'20 - 2nd paragraph of section 4) actually did propose a method for "general" poisoning attacks using randomized smoothing as well, even though they only applied the idea concretely only to label flipping. So, even though it is not quite clear how their exact certification bounds would be, and also the result would be randomized certification as opposed to deterministic, which is the result of this paper, I still think this previous work on general poisoning certification shall be mentioned as well.Some related works: The exact noise models studied in this paper are actually studied long ago in classical learning papers that, unfortunately, do not get to be cited in recent works in poisoning attacks much. It is true that those works are not about certification, but the basic problem of provable learning under poisoning/noise are essentially the same. Citation [1] below defined the label flip noise model, and [2] defined the "general" poisoning model in which the adversary can change a fraction of the data. The current paper actually uses a more fine-grained version that distinguishes between add/removes, but this is the same as measuring Hamming distance up to a factor 2. Works [3,4] also initiated a computationally efficient approach to dealing with poisoning attacks in a provable way, and [6] particularly studied supervised learning and poisoning-robust SGC. Of course, I understand that here we want an extra certification on top of the defense, yet I still think the line of work on provable defenses against poisoning are closely related. Also, I think your work (and similar certified robustness papers on general poisoning) can be interpreted as "certified poisoning against *targeted* poisoning" because you certify the decisions for each individual test instance. So, they seem to complement "provable attacks" on targeted poisoning e.g., from [5] in which it is shown how to increase the error close to one for a particular test instance if the original error (without attack) is a not-so-small probability to begin with. It seems one difference between the two settings is that the attack of [5] needs a bigger poisoning budget to succeed and you certify a smaller number (naturally) but I think a comparison with the complementary line of work is needed as well.I also think that the comparison with Steinhardt et al. is better to be expanded a bit more. Both papers use the term "certified robustness" but as (correctly) mentioned in the paper, the two settings are not the same (e.g., here the certification is for a test instance rather than the model). I think it is better to bring this note out of a footnote and shed more light on this helpful comparison to prevent confusion for readers.I know that the experiments are done with image datasets, but the problem studied in this work is more general, and it would have been probably better if the treatment of the issue was also as such (e.g., the abstract mentions "bounded number of images", though it could be basically any piece of data).Citations:[1] Sloan, Robert H. "Four types of noise in data for PAC learning." Information Processing Letters, 1995.[2] Bshouty, Nader H., Nadav Eiron, and Eyal Kushilevitz. "PAC learning with nasty noise." Theoretical Computer Science 2002.[3] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. "Robust estimators in high dimensions without the computational intractability." FOCS, 2016[4] Kevin A Lai, Anup B Rao, and Santosh Vempala. "Agnostic estimation of mean and covariance." FOCS 2016.[5] Mahloujifar Saeed, Dimitrios I. Diochnos, and Mohammad Mahmoody. "The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure." AAAI 2019.[6] Diakonikolas, I., Kamath, G., Kane, D., Li, J., Steinhardt, J., & Stewart, A. "Sever: A robust meta-algorithm for stochastic optimization." ICML. 2019. ### SummaryThis paper proposes PARROT, a method for learning a policy prior from a dataset of expert state-action pairs that have been derived from multiple similar tasks. The policy prior is parameterized as a deep conditional generative model that maps a noise input and a state to an action. The latter map can be inverted, which is important to guarantee that the prior assigns nonzero probability to the full action space for all states. Given a new task, the policy prior is used to parameterize a new policy; the new policy outputs noise inputs to the policy priors invertible mapping, which in turn outputs an action in the original action space. This parameterization of the new policy leads to much more targeted exploration versus sampling actions uniformly from the original action space. Experiments are on a suite of pick-and-place robotic tasks in simulation.### Pros- The writing is overall very clear and persuasive. The introduction is especially compelling. The sections are well organized.- Related work is quite thorough, with just a few potential omissions (see Cons)- The problem setup section is very much appreciated. It is difficult to formalize or even discuss the assumptions behind an approach like this, but I thought the authors did a very good job.- The problem setting in general is very motivating. It does seem realistic to suppose that a set of expert trajectories from related tasks are available, but that they are not necessarily annotated with rewards. The authors do a good job explaining how this differs from other problem settings like LfD, meta-learning and meta-imitation learning.- The whole method is elegant. It is a sensible integration of existing techniques to address a well-motivated problem.- The experimental results are strong (though there are many opportunities for additional experiments)- The baselines are well chosen- The pseudocode in appendix A is clean and clear. All of the notation throughout the paper is too.- The paper provides a good level of detail in appendix B for reproducibility### Cons- Missing references:     -  Learning Action Representations for Reinforcement Learning Chandak et al. ICML 2019    - Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning Siegel et al. ICLR 2020    - Residual Policy Learning Silver et al. (2018) and Residual Reinforcement Learning for Robot Control Johannink et al. (2018)- The empirical results are very encouraging, but I would like to have a better understanding of when the overall approach might fail. Some experiments in this direction would include:    - Evaluate test-time performance as a function of the number of training tasks    - Deliberately bias the training tasks so that they non-uniformly sample from the set, and see how much of an effect that has on the downstream performance. For example, train only on pick tasks and test only on pick-and-place tasks, or vice versa.    - More domains beyond the single one considered- It would also be nice to see some ablations. In particular, I am curious about the extent to which parameterizing the new task-specific policy with the behavior prior is important, versus just using the behavior prior to gather data. A simple comparison would be to use the behavior prior as an exploration policy that is called epsilon% of the time during RL.- I highly encourage the authors to release code for the paper- Three random seeds is not enough### Detailed Comments- Figure 1 is visually compelling, but a little hard for me to follow. I am most thrown off by the exploration part, which says: attempt all possible tasks in a new scene. I am interpreting this to be somewhat metaphorical -- it is not as though there is actually a list of tasks, and you are attempting each one in a new scene. Also, as far as I understand, there is not a strict separation between exploration and learn a specific task via RL -- exploration is part of that RL process. Let me know if my understanding here is correct. If so, I would recommend perhaps cutting the exploration part of this figure.- Figure 2 is very helpful and clear- In the problem setup, there is discussion of fixed state and action space dimensionalities, but it is not stated that the spaces are vector spaces. Would it suffice to instead say that $\mathcal{S}$ and $\mathcal{A}$ are assumed fixed?- In the problem setup, in two places, there is an expectation over single step rewards that should really be an expectation over temporally discounted returns. - In the problem setup, it says: Note, however, that the optimality assumption is not very restrictive: every trajectory can be interpreted as an optimal trajectory for some unknown reward function. I think this is misleading. It is true that there is some reward function for every trajectory, but the whole point of the problem setup is that there is assumed to be a nontrivial distribution over MDPs, and the important question is whether the observed data are representative of this distribution.- Please indicate in the caption what the lines and shaded areas represent in Figure 5. I assume they are means and standard deviations, but other choices are plausible too.- Typos:    - Each task can considered a Markov decision processes (two typos)    - (making it expressive) is missing a period    - each coupling layer in the real NVP take as input (should be takes)    - hyparparameters    - This comparisons    - behavioural (I know this is an alternative spelling, but the paper should be internally consistent)### Questions-  How does the asymptotic performance of PARROT compare to vanilla RL? Is there a point at which RL starts to outperform PARROT in the tasks considered in this work? It looks like this happens in Pick up Baseball Cap.-  What steps would need to be taken to try PARROT in an environment with discrete actions?- How might PARROT be used in a continual/lifelong learning setting? My concern is that if the behavior prior is changing over time, all of the policies that have been parameterized using the behavior prior would also change, and would potentially need to be relearned. (This question does not necessarily need to be addressed in the submission -- I am just curious.)-  What would it take to apply PARROT in a domain where it is not possible to write down good scripted policies (like the ones in Appendix C :) )? The paper presents a learning method for the scenario of feature dependent label noise. A framework where label noise diminishes away from the decision boundary is established and a relabeling strategy based on this by relabeling highly confident points is proposed.  The method is a straight-forward adaptive method which the authors both theoretically and empirically explore in detail.# Pros- The approach is simple and easy to implement on top of existing methods- The authors support their simple method with some very nice theoretical results- Good empirical evaluation showing competitive performance to other methods- The paper is clearly written and easy to read- Citations place the work well among existing literature# Cons- The authors mention Menon et al. in their "related works" section, but dismiss it immediately as "...it does not recalibrate individual data based on their contexts, and thus are not as effective as other deep-learning -based methods in practice." A citation here is needed showing this point, or further experiments.- The related works section is very short and is essentially a list of other approaches. A more thorough discussion would help readers.- A selection of baseline methods are chosen for comparison to the proposed approach in the empirical evaluation, but the particular choices aren't discussed in any detail. Some of the methods were mentioned in the related works section, but some are not (why not?), and neither section explains some of the choices. **Summary of paper**The authors introduce a data-relabeling method that they claim is the first that both allows for data-dependent noise and is theoretically guaranteed to converge to an optimal model.The authors introduce a novel family of label noise, called Polynomial Margin Diminishing (PMD), which defines a polynomially-decreasing upper bound on the label noise as the true label probability is above some threshold and as it approaches 1. Below the threshold (when the true label probability is closer to 0.5), the noise is unbounded.The authors introduce an algorithm, "Progressive Label Correction", which iteratively flips the training labels of examples for which the model's confidence is above a threshold. The threshold decreases over time, so that only the most confident examples are flipped at first, and then the less-confident examples are flipped later.The authors prove (Theorem 1) that under the assumption of PMD label noise distribution, as well as Assumption 1 concerning the flexibility of the hypothesis class and the continuity of the true label's conditional distribution, then their Progressive Label Correction algorithm asymptotically approaches a set of corrected labels that match the true (de-noised) labels with high probability.The authors experimentally show that their algorithm consistently outperforms 5 alternatives on CIFAR-10 and CIFAR-100 datasets with various types of synthetic noise, both feature-dependent and hybrid feature-depedent/indepdendent. Finally, they show their algorithm outperforms 10 alternatives on a real-world dataset (Clothing1M) with unknown noise.**Conclusions**Quality: Overall I like this paper. It's a pretty simple algorithm to implement, and it seems to be quite effective in practice and have nice theoretical properties.Clarity: The paper is structured very well. It is easy to follow the narrative and high-level ideas. There are a few minor typos (see below for some examples). The theory is relatively easy to follow.Originality: I'm not familiar with the related work, but this appears to be original/novel from my limited perspective.Significance: This algorithm can potentially be used to improve test accuracy on any supervised learning task, so the intended audience is quite large. The improvement on both synthetic and real data seems quite large. It's hard to tell whether the results were cherry-picked at all, but they are impressive.**Minor comments**Figre 1 caption has a typo: "Red dots are the data that remain incorrect. that remain un-corrected and are closer to thedecision boundary."In Section 2.1, "illustrate the upperbound (red curve)", but the figure actually shows an orange curve (not red) for the upper bound.Figure 2 caption, "closed to 0 or 1" should be "close to 0 or 1". Also, "a equal probability" should be "an equal probability". Summary: This paper presents a novel method called Embed-SAD (as well as Input-SAD) to learn graph/node representations to disentangle structure and attribute information. Input-SAD is a simple baseline that tries to get structure-attribute disentanglements by individually processing graph structures and node attributes. For structure, the original node attibutes are replaced by out-degrees only, and passed to GNNs, while for attibutes, the node attibutes are passed to fully-connected networks. Embed-SAD is a more elaborate method to disentangle the GNN embeddings by posing two types of additional losses, i.e., the edge-reconstruction loss for structures, and the Noise-Contrastive Estimation (NCE) loss to maximize the mutual information against the structure-encoding vectors, in addition to the original loss for supervision. The paper also develops an interesting evaluation metric called SAD-Metric where node attibutes or graph structures are exclusively perturbed for each graph, and prediction for whether that perturbation is for structure or for attibutes made by the element-wise absolute differences between embedded vectors before and after the perturbation. This SAD-Metric can quantify the extent to which the obtained representation can detect which perturbation, that for structures or that for attibutes, is made for each sample graph. The experimental results also demonstrated that the structure-attibute disentanglement by Embed-SAD learning strategy actually improved the prediction performance of many off-the-shelf GNNs over many different graph- or node-level tasks.Comments:The paper is well-written and easy to follow since the intention and corresponding ideas are quite clear. The combined loss fo Embed-SAD also has ablation study results that was also informative. Basically I liked the idea and have not too much to say, but here are some  comments:- The SAD-metric would be more carefully evaluated since it includes "training" and we might be able to get zero-training error if embedding vectors are sufficiently high dimensional or we can use a strong predictor here for memorization. Even for a simple linear classifier (a "low capacity" classifier), I think it's better to replace this part by cross validation, or at least, a simple training-test split. - This study would depend on what node attibutes (i.e. "featurization") we use, and it should be more clearly described in the main body what node attibutes are used in each benchmarking experiment. In particular, we often include "structure-derived features" such as degrees (or the number of attached hydrogens in the case of molecular data) to the node attributes of GNNs, and how this practice affects the disentanglement would be informative in the paper's context. In particular, for Input-SAD.- Also, we need more information on the baselines of Table 3 for evaluating the SAD-metric. What is "Random-GCN (2017)"? (GCN with random node attibutes?) What is the intention to include this one here? Even Random-GCN had around 90% accuracy, and we need an additional baseline that clearly fails to disentangle structure and attibute information.- First, I personally felt that Input-SAD is actually not for "disentanglement" and a bit confusing, rather it's a simple baseline to see what if we input structure- and attibute- information separetely in the first place. However, to my surprise, the performance of Input-SAD was not that bad for molecular graph classification benchmark, in a sharp contrast to those for citation and coauthor network cases. Actually, Input-SAD was better than Embed-SAD for SIDER and BACE. Are there any possible explanations on this?- For eq (2), t_uv seemed undefined (though we can guess it). Do GIN, GAT, GraphSage use the edge attibutes? ##########################################################################Summary:This paper studies the problem of data augmentation that obtains new training examples by modifying existing ones. Data augmentation is popular in machine learning and artificial intelligence since it enhances the number of training examples. However, its effect on model performance remains unknown in practice. An augmentation operator (e.g. image rotation) can be either helpful or harmful. This paper introduces two novel metrics, named affinity and diversity, to quantify the effect of any given augmentation operator. The authors find that an operator with high affinity score and high diversity score leads to the best performance improvement. ##########################################################################Reasons for score: Overall, I like the idea of this paper about making the effects of augmentation operators tractable. The proposed affinity and diversity scores are great indicators to evaluate the usefulness of an arbitrary operator. The finding that higher scores are better is insightful. With that, practitioners are able to inspect a large number of operators and select the most effective ones. However, the computation of affinity and diversity scores may be very expensive. The runtime could be more than that runs an operator directly to get the performance improvement. It will be good if the authors can compare the efficiency and/or share some comments in the rebuttal.##########################################################################Pros:1. The paper proposes a novel idea of quantifying the effect of data augmentation. Specifically, the idea introduces two metrics, affinity and diversity scores, to evaluate any given augmentation operator in its effect on model performance. The experiments showed that higher the scores, better the performance improvement. The finding is novel that should be the first time in the field. 2. The paper justifies that either affinity or diversity alone does not predict model performance. They together can make the prediction deterministic. The finding suggests that it is nontrivial to quantify the effect of augmentation operators. The paper nicely leverages heat maps as shown in Figure 3(b) and 3(c) to reveal the finding. ##########################################################################Cons: I have a concern about computation efficiency. It seems it takes significant time to compute the proposed affinity and diversity scores. For a given operator, Definition 1 regarding affinity requires running the model once. Similarly, Definition 2 regarding diversity requires running the model another time. So it needs to run the model twice to get affinity and diversity scores. Empirically, we can just run the model once to calculate the actual model improvement of the operator. If so, it is inefficient to use affinity and diversity scores for the purpose of predicting model performance. ##########################################################################Questions during rebuttal: It will be good if the authors can add experiments or discussions related to efficiency in runtime. Or the authors can extend the use cases beyond model performance prediction.   This paper introduced a new hyperbolic neuron based on horocycles (hyperbolic counterparts of hyperplanes). The authors proved that these neurons in H^n are as useful as traditional neurons in R^n through theoretical arguments and demonstrated they can significantly improve learning in hyperbolic embeddings of tree datasets and MNIST/CIFAR datasets.Quality:This contribution has both theoretical and practical strengths. Theoretically, they proved that the proposed hyperbolic neurons are universal approximators (Theorem 2). Practically, they introduced a new kind of hyperbolic neuron, with its difference with existing literature clearly demonstrated through formulations and density plots. It shows supervisor performance improvements in several examples.Clarity:The language is well polished. The formulations and statements are clear and consistent. The presentation has high clarity with good intuitions through illustrations.Originality:The proposed method is mostly related to hyperbolic neural networks constructed using Mobius arithmetic operators. Their difference is demonstrated both intuitively and empirically through experiments. The relationship with previous works is clearly stated in section 2. The references are proper with page numbers mentioned.Significance:This paper establishes a new connection between hyperbolic geometry and deep learning. Therefore it should be interesting to the large group of audiences in those areas.My main concern and questions are listed as follows:Most importantly, the introduction and the theorem are based on equation (2); while the experiments are based on the Poisson layer introduced in section 4.2. I can see some inconsistency here: clearly they are different functions. Please fill this gap in the rebuttal and next version.Are there any explanations and technical arguments of the good empirical performance?Clearly, the hyper-parameter epsilon is important to maintain numerical stability. There should be some demonstrations in the main text or the  supplementary material to show the robustness to instability (e.g. by setting epsilon=0)Finally, I summarize the pros and cons as follows.Pro:- new hyperbolic deep learning- proof of representation power on H^n- strong empirical resultsCon:- missing connection between equation(2) and Poisson layerOverall, based on the above assessment measures I recommend strong acceptance.Here are more comments for the authors' revision:abstract: "MLR" is the abbrev of?Introduction: introduce notation T_p(H)After theorem 1 there have to be some remarks to explain the statement.Same for Lemma/Corollary 1.Theorem 2 is referred to before the statement.The volume element "dm" is a bit hard to readWhere are the notation h_x(y) used?Figure 8 x-axis and y-axis are not clear This paper proposes Generalized Variational Continual Learning (GVCL). It is shown that Online EWC and VCL are special cases of GVCL, along with other theoretical contributions. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL. GVCL and GVCL-F are applied to a number of continual learning tasks and demonstrate competitive performance. Although GVCL and GVCL-F do not outperform baselines, particularly in hard settings (split-mnist and mixed vision), GVCL is an original and excellent contribution. The paper is clear and well-written, the proposed algorithm is theoretically motivated and analysed, experiments are comprehensive, demonstrating the empirical performance of GVCL. I have the following comments:- It would be interesting to have VCL and Online EWC added to Figures 2 and 3.- Why is GVFL significantly worse than baselines for split-mnist (Figure 2c)?- Why is split-mnist omitted from Figure 3?- The supplementary material contains some analysis on the effect and sensitivity of the value of $\beta$ on the performance of the algorithm. This should be extended and presented in the main paper.Minor:- "the node is *effective* shut off" -> effectively The paper proposes a weight-encoded neural implicit representation for 3D shapes. The idea is to encode every shape in the network weights of its own designated small MLP network, instead of trying to learn a latent space of shapes. This leads to a really compact shape representation based on signed distance fields that could be interesting for many applications. The approach uses importance sampling to speed up training and robust losses.The paper evaluates the representation in terms of visualization efficiently, stability, and the compactness of the representation on a large dataset of shapes. The approach outperforms latent encoded shape representations such as DeepSDF in terms of compression and accuracy.I believe the idea of encoding shapes using signed distance fields and MLPs for compression is really exciting. Such implicit representations of shapes are becoming really popular at the moment and have huge potential in the future. In terms of novelty, I have seen many shape representation approaches based on MLPs lately, but none so far focussed on the compression aspect.In summary, I believe that the compression aspect of this work is really interesting and will inspire follow up works along those lines. The method achieves impressive compression ratios and can decompress/render the shapes still at high speeds. The paper introduces an unsupervised task called Augmented Temporal Contrast which associates pairs of observations separated in time using a contrastive loss. The paper uses the task in several training regimes (in online RL, pretraining and multi-task RL).Pros:- Well written and structured paper.- Interesting, general and simple to implement task.- Evaluated in several training regimes and on several environments.- Improves sample efficiency on most of the environments and setups, and improves over prior methods.- The attention maps in the paper and appendix are great and although they may be hand picked(?) examples, it highlights the issue with many approaches that can't model a goal rarely seen.Cons:- You write you use multiple seeds but I don't see anywhere details on this? Consider adding it to the tables in appendix.- In Figure 3 one agent step is 4x environment frames? I suggest to make it clear in plot or in caption. In Figure 2, environment frames is used.Comments/questions:- Wrt. 4.1, to what extent is the "small" replay for DMLab necessary over just using observations in batches/unrolls? Looking at Table 3 I can't see how large the replay is? 10k as in SAC or smaller? The paper explores a very important question in dynamical system identification of how to make recurrent neural networks (RNNs) learn both long-term and short-term dependencies without the gradient vanishing or exploding limitation. They suggest using piece-wise linear RNNs (PLRNNs) with a novel regularization technique.The paper is well written and is very thorough with the necessary theoretical foundation, numerical experiments and analysis. I think the theory and results of this paper are significant and will be relevant to further our understanding of RNNs and system identification.Major points:1) L2 weight regularization can be easily applied to any of the RNN models used in the experiments. While other weight initialization schemes were compared to the paper's proposed model (rPLRNN), none of the other RNN models had similar regularization. This will shed some light on whether it is indeed the proposed regularization that matters or the full proposed model with PLRNN and a mix of regularized and non-regularized units.2) It is not clear to me how one can choose the correct ratio of regularized vs unregularized units in the model. While the amount of regularization clearly helps in reducing training error as shown in Figure 3, increasing the ratio of regularized units in Figure S3C did not help the error past 0.1 and then larger values resulted in large increases such that the error at ratio 1 is equivalent to the error at ratio 0. Perhaps this observation is specific to the addition problem, but I feel that a discussion of the effect of this ratio on performance should be included for clarity. Additionally, the ratio of regularized units with best performance could potentially be different for different regularization amounts.Minor Point:g is not defined in equation 2 Well-written paper with strong, well-presented results. The MOS results attain or surpass the best WaveNet results. The presentation is on the whole clear. I was a bit confused by the core model description at first. In particular, the index t on the samples x_t is not a time-index, correct? Rather, it's just a step in the diffusion process? At first I thought it was a time-index, and so the model seemed very much like an AR model, leaving me very confused.Some additional things I liked about the work:Compelling contrast with existing methods (WaveNet, VAE, GANs).Use of multiple metrics in the evaluation, 5 objective metrics in addition to MOS, e.g. Tables 2 & 3, with details on the metrics provided in an Appendix.Use of multiple models as reference models, WaveNet, WaveGlow, WaveFlow, Clarinet, WaveGAN, in addition to the proposed model.Focus on unconditional generation, which AIU has not received that much attention in the communityWhere I am unsure is the originality of the work. I personally am not aware of the diffusion approach having been applied to TTS, but this is not my primary area of expertise. Obviously, if there is related work in TTS with diffusion models, this should be cited.Also, what I don't see in the Conclusion is any discussion of the weaknesses and challenges for the model going forward. The paper would be strengthened by having a more balanced conclusion.Some caveats regarding my review:I am not familiar with the specific datasets used, so cannot fully appreciate the significance of the results reported.I did not check the math in detail; the notation overall seemed clear and consistent to me (though see my first set of comments).I have a few more specific comments.Throughout the paper, "... audios ..." : "audio" is not usually used as a plural noun. E.g. "We randomly generate 1,000 audios" --> "We randomly generate 1,000 audio waveforms"?"Notably, the quality of audios ... " --> "Notably, the quality of audio ... ""Note that, the quality of ground-truth ...": nit, no comma after "that". Summary :The paper proposes a new framework for addressing the problem of adversaries in black box settings in order to improve model robustness. Leveraging classical deception frameworks used in network security, the authors propose to fool the attacker by training what they call a `luring component that is augmented to an already trained model such that the new model does not later good samples and targets the adversaries to achieve the desired result. Additionally, the proposed framework does not need access to labeled data and can be applied to any pre-trained model. Promising results are demonstrated on multiple datasets like MNIST and CIFAR 10, etc.Positives:Novelty: the paper proposes a new framework which is distinct from existing methods that mostly consider white box settings, rely on robust features or anticipate perturbations. Instead, the proposed method is applicable to black box settings, is data agnostic, and can be applied to any pre trained model. Technical sophistication:1. The intuition behind the idea is explained well. Though the concept is borrowed from network security literature, its introduction and adaptation to the ML community is valuable. 2. The authors introduce two objectives that are designed towards training the luring component, and these are intuitive as well. In order to achieve their objective, the authors introduce a new loss function called the luring loss.3. The algorithm for training the luring component also seems reasonable.Experimentation:1. Extensive comparison against baselines is detailed for characterization of luring effect as well as for metric evaluation. 2.  Results are reported on multiple datasets.Presentation:The paper is well motivated and fairly clear.Overall comments:The paper opens a new direction to adversarial machine learning research by proposing a new conceptual framework for addressing adversaries in black box settings. It offers practical value given that it is data agnostic , applicable to black box settings, and can be applied to any pre trained model.  The authors prove a theorem (thm 4.1) which describes a basis for the space of kernels in a G-steerable CNN for any compact group G.  Steerable CNNs are similar to CNNs but replace channels with G-reps and enforce an equivariance constraint on the kernels.  Though Cohen et al 2019 state the constraint, and Cohen et al 2019, Cohen & Welling 2016, and Weiler & Cesa 2019 and several other papers solve this constraint for different groups and representations, there has not been a general formulation which applies to all compact groups.  Here, solving the constraint means to construct a basis of the space of steerable kernels.  Any steerable kernel is then a linear combination of this basis and the network can then be trained by learning the coefficients.The problem of finding a basis for the space of steerable kernels is non-trivial and critical for constructing G-steerable CNNs.  Up until now this has done group by group.  The theorem proved in this work unifies such previous efforts and provides a useful method for approaching further G.  This paper is a significant contribution to the field.The appendix provides a complete and approachable background in the area as well as detailed and precise proofs.  Moreover, the effort by Cohen & Welling to frame equivariant deep learning in the proper context of representation theory is continued and extended here to good profit.  The appendix is quite verbose and the language is more casual than I am accustomed to in a mathematical text or research paper, but it serves the goal of being didactic and approachable.A practical consideration remains. Though theorem 4.1 reduces construction of a basis of steerable kernels to 1) finding Clebsch-Gordon decomposition of tensor products, 2) describing endomorphisms of irreps, and 3) describing harmonic functions, none of these problems is trivial (or even necessarily solved) for a general compact group G.  That said, Appendix E does a good job providing evidence that this can be done for many individual groups.  However, in that case, we are still back to solving the problem on a group by group basis.    Specific Additional Points:1.My opinion is that the language of physics does not add to the paper.  While Clebsch-Gordan and harmonic functions first arose in physics, they can be described in terms of representation theory.  In this way, both steerable CNN and quantum mechanics are applications of rep. theory and so it is not necessary to use physics here to describe steerable CNN.2.Page 5, the notation $[j] = dim(V_j)$ seems unusual to me. It would be better to use something more standard.  In particular, in Defn 3.5, brackets are used as parenthesis, making this more confusing.3.Page 5, the fact that input and output representations $V_{in}$ and $V_{out}$ decompose into irreps does not immediately explain how to construct a steerable kernel basis for $V_{in} \to V_{out}$ given ones between irreps $V_i \to V_j$.  Though it is not complicated, I would include this.4.Page 5, I was confused by the inclusion of $End_{G,K}(V_j)$ at first since it does not appear when working over $\mathbb{C}$ due to Shurrs lemma.  It is explained in the paper and more so in the appendix that it is necessary over $\mathbb{R}$, but it could be a bit clearer and earlier in the paper.  Namely, you could note that over $\mathbb{C}$ $End_{G,K}(V_j) = \mathbb{C}$ and give the possibilities over $\mathbb{R}$.5.Page 5, Thm 3.4 and Page 43, Thm C.7.  Given that the purpose of this paper is partially to formulate Steerable CNN in precise terms, the fact that $\delta_x$ is informally considered as in $L^2(X)$ is very imprecise.  Not only does this make the proof informal, it means the maps in the theorem are not even defined.  Can you replace $L^2(X)$ with an appropriate space of distributions in order to make the statement precise?  6.Page 6, which is zero for almost all J should be which is zero for all but finitely many J7.Appendix E, $U(1)$ is isomorphic to $SO(2)$, so it is strange to use both notations.  The difference between these subsections is whether the representations are real or complex. The paper considers Group Equivariant Convulation Neural Networks (GCNNs) which are convolutional neural networks that are equivariant wrt group symmetries of the underlying space. The equivariance requirement places constraints on the parameterization of the corresponding CNN. This work extends previous results for particular symmetries and outlines a method for obtaining these parameterizations for any compact group symmetry.Technically, the paper establishes a Wigner-Eckert Theorem for G-steerable kernels, which in turn allows any admissible kernel to be expressed using a basis of kernels thereby establishing a natural parameterization. This procedure is carried out for U(1), SO(2), SO(3) among others.The paper highlights important ideas from representation theory that can be used to obtain paramaterizations for symmetry-constrained learning models, and the mathematical methods could be of independent interest. The results obtained here are significant for any learning problem where there are inherent natural symmetries, as the authors point out this could be especially beneficial for data arising from physical processes. #### SummaryThe submission focuses on a variant of inverse reinforcement learning, where the learner knows the task reward but is unaware of hard constraints that need to be respected while completing the task. The authors provide an algorithm to recover these constraints from expert demonstrations. The proposed algorithm builds upon a recent technique (Scobee & Sastry 2020) and addresses problems with large and continuous state spaces.++++++++++++++++++++++++++++++++++#### Reasons for scoreStrengths:* The problem considered is interesting and relevant to the ICLR community.* The technique developed (Algorithm 1) is novel and well motivated.* The experiments provide adequate evidence to back the claims.* The paper is very well written and organized.Weaknesses:* Justification for the policy loss function (Equation 9) is unclear.* Comparison with prior art is lacking.* Discussion of related work is sparse and can be more detailed.Based on the above-mentioned strengths, I vote for accepting. My concerns (further detailed below) potentially can be addressed during the rebuttal phase.++++++++++++++++++++++++++++++++++#### Major Comments1. (page 2) The requirement of ability to modify the environment is listed as a limitation of prior art (Scobee & Sastry 2020). However, like the current approach, the prior art adds the constraints / modifies the environments only conceptually (and not physically). Further, both the current and prior work focus on the case of hard constraints. Please clarify this limitation of the prior art vis-à-vis proposed approach.2. (page 2) The rationale behind the objective (Equation 7) of the prior art and the proposed approach is identical. Please clarify, then, if the current algorithm is also greedy.3. (Equation 9) Please provide additional details for the inclusion of the entropy term in the policy loss function.   - The principle of maximum entropy is used to arrive at Eq. 4, the loss function of theta (since Eq. 4 uses the term derived in Eq. 2, which in turn is obtained from the maximum entropy principle). Given this, it is unclear why the entropy term is also included in Eq. 9. Is it used as a regularizer?  - Alternatively put, consider the unconstrained version of Equation 9. In this unconstrained case, the problem is analogous to MaxEnt IRL (Ziebart et al.). In MaxEnt IRL, given the reward $\theta$, the policy $\phi$ is computed by value / policy iteration and without the extra entropy term.  - Further, adding both $J$ and $H$ in the loss seem counterintuitive as they have different units. J is cumulative reward, while H is dimensionless entropy. Why is the entropy term normalized by $\beta$? How is the normalization constant chosen?4. (Section 4) While not all domains considered in the Experiments can be captured by the prior art (Scobee & Sastry 2020), the first three can be (as they have discrete state, action spaces). Please benchmark the proposed approach with prior art for these three domains. Time permitting, also consider utilizing one of the recent high-dimensional techniques (see below) as another baseline.5. (Section 6) Space permitting, please include a discussion of following related works.  - Constrained IRL for high-dimensional problems:    * Chou, Glen, Necmiye Ozay, and Dmitry Berenson. "Learning parametric constraints in high dimensions from demonstrations." Conference on Robot Learning. PMLR, 2020.     * Park, Daehyung, et al. "Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning." Conference on Robot Learning. PMLR, 2020. Notes: Extends beyond the proposed approach to consider constraints which may not be global (i.e., locally active constraints).    * Chou, Glen, Necmiye Ozay, and Dmitry Berenson. "Learning constraints from locally-optimal demonstrations under cost function uncertainty." IEEE Robotics and Automation Letters 5.2 (2020): 3682-3690.  - Inverse reward / policy learning frameworks that incorporate prior knowledge of reward / policy:    * Ramachandran, Deepak, and Eyal Amir. "Bayesian Inverse Reinforcement Learning." IJCAI. Vol. 7. 2007.     * Michini, Bernard, and Jonathan P. How. "Bayesian nonparametric inverse reinforcement learning." Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2012.    * Unhelkar, Vaibhav V., and Julie A. Shah. "Learning models of sequential decision-making with partial specification of agent behavior." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.    * Jeon, Wonseok, Seokin Seo, and Kee-Eung Kim. "A bayesian approach to generative adversarial imitation learning." Advances in Neural Information Processing Systems. 2018.  - Learning features (which can be in the form of logical constraints) for IRL:    * Choi, Jaedeug, and Kee-Eung Kim. "Bayesian nonparametric feature construction for inverse reinforcement learning." Twenty-Third International Joint Conference on Artificial Intelligence. 2013.++++++++++++++++++++++++++++++++++#### Questions for Rebuttal PhasePlease address comments 1-4.++++++++++++++++++++++++++++++++++#### Minor Comments- (typo) In the Introduction, Scobee & Sastry is used as singular noun, where in fact it is plural.- (Equation 5) Beta is missing in the log exponential term.- (Page 4, below Equation 7) The statement Notice that & essentially tries to match is ambiguous, since the gradient by itself does not try to match the two values. Please consider rephrasing to say that this matching occurs at the minima (where the gradient is zero).- (Page 5, Section 3.3) Please denote the range $[0,1]$ as $(0,1)$, since 0 and 1 are not in the range of $\zeta$.- (Equation 9) Consider distinguishing the loss functions in Equation 5 and 9 (say through superscript or subscript). Due to $L$ being overloaded, at first glance, I misunderstood the loss function in Eq 9 as a continued derivation of Eq 5.- (Section 7, typo) (2) -> Eq. (2)++++++++++++++++++++++++++++++++++ SummaryThe paper considers the cooperative MARL setting where agents get local rewards and they are interconnected as a graph where neighbors can communicate. The paper specifically considers the communication of reward sharing, that is, an agent shares (part of) its reward to its neighbors, such that each agent optimizes its local reward plus rewards from its neighbors. This motivates a bi-level optimization framework where the high-level policy decides how the rewards are shared and the low-level policy locally optimizes the shared rewards given the high-levels decision. The papers flow motivates such a framework well. The experimental results demonstrate the methods effectiveness. I think it is a strong paper (accept), but my confidence is low due to the following confusions I have. Comments/Questions 1. I have a high-level comment on the reward sharing mechanism. It seems that the proposed method does not support multi-hop sharing because rewards can only be shared to neighbors. Why is this single-hop sharing effective in the experiments? Is it because of domain-specific reasons, or its because that single-hop sharing is in principle equally effective, why?2. The derivation of (18) using taylor expansion is unclear to me. Could the authors explain it with more details?3. I dont fully understand the proof of Proposition 4.2. Specifically, does phi can be learned in a decentralized manner mean that the *optimal* phi can be based on only the local observation for each agent, instead of based on global state? Could the authors comment on the approximation error induced by the mean-field approximation? Why the proof begins with phi_i based on o_i and ends with phi_i based on global state s.4. In Equation (17) and (20), should phi^* be just phi (i.e. no * here)?5. The low-level policy is to optimize the shared rewards. My understanding is that any (single-agent) RL algorithm can be used for optimizing the shared rewards, e.g. DQN, DDPG, A2C, etc. Why would the authors choose DGN, a rather less popular RL algorithm? Have the authors tried more popular algorithms as the low-level policy?6. For fixed LToS,  how do we determine the fixed sharing weights? The paper present a new method, called LToS which enables agents to share rewards in MARL. Two levels of policies, high-level and low-level, determines rewards and optimize global objectives. Three diverse scenarios were used to test the performance of LToS compared to other baseline methods. LToS consistently outperforms other methods. In the second scenario, authors also show the need for high-level policy by introduction fixed LToS. - At the end of Introduction, the sentence LToS is easy to implement and currently realized by DDPG& can be misleading because of the word realized and the fact that authors argue that LToS is a newly proposed method. Does this mean LToS simply combines DDPG and DGN?- Do Figure 5 and 6 represent selfishness of agents when LToS is used?- Minor editorial errors in Appendix This paper studies injective ReLU networks, motivated by various applications such as generative modeling, inverse problems and compressed sensing with generative priors. The authors fully characterize injectivity of fully-connected and convolutional layers and networks. They provide layerwise and multilayer results, characterizing the stability of inverting an injective network, and using tools from differential topology to study injectivity of deep networks. They also prove a sufficient condition for injectivity, which is an end-to-end doubling of the dimension. The overall writing is clear and the use of pictorial illustration is helpful for less mathematically mature readers. Despite not being an expert in this area, I acknowledge the signifcance of exploring the properties and theories of injective (ReLU) neural networks, particularly for the study of inverse problems and generative modeling. The authors performed a comprehensive study of this subject in this paper, developing theories which allow deeper understanding of injective ReLU networks and their applications to nonlinear inverse and inference problems. Pros: - This paper established a mathematically rigorous framework to study the injectivity of fully-connected and convolutional ReLU networks. - Substantial and careful discussion of common operations in fully-connected and convolutional neural networks, such as normalization strategies and pooling operations, regarding their effects on the injectivity of networks with their presence. Cons: - Would like to see an experiment for inverse problem applications, though this is rather minor. Typos: - In **Notation.**, do you mean $ \mathcal{NN}(n, m, L, \mathbf{n}) $ instead of $ \mathcal{NN}(n, m, L, \mathbf{m}) $?- In (2), $ b_L $ instead of $ b_\ell $? Should there be $ b_{L+1} $ as well? - Should the constant $ C(W) $ appear in (6)?- Page 6, line 3: do you mean $ c \in \mathbb{R}^{N_1} \times \cdots \times \mathbb{R}^{N_p} $ instead? Summary:This paper studies the behaviour of deep neural networks in situations where simple but irrelevant correlations exist between input and output, and dominate more complex but relevant correlations. The authors conduct experiments on synthetic datasets (like coloured MNIST) and show that an invariance penalty helps the network focus on relevant correlations. Pros:- The paper studies neural network behaviour with respect to systemic biases that are likely faced by most neural networks in some form or the other. To make the study tenable, the authors make use of meaningful synthetic datasets, and propose an intuitive regularization to overcome the systemic biases. - The analysis done in the paper is very methodical, and the presentation is very clear.- The numerical simulations are comprehensive and convincing.Cons:- It would be nice to see how this would be applicable to real world datasets. The paper is interesting even without it, and I also appreciate that the authors are honest about it - so I would not hold it against the authors. But it would further strengthen the paper if some basic experiments are done on real world datasets. For instance, will one be able to find a partition on ImageNet?Comments:- Section 5.1: Minimization is spelt incorrectly.- Equation (6-7): I am not entirely sure what is happening with respect to the constraint on \theta. What does capital \theta correspond to? And if \theta itself is the result of an optimisation (argmin), then why is there another optimisation on the same \theta in the loss function?- In the text that appears before equation (3), it is mentioned that the predicted features f_\theta will be matched for the two partitions, but equation (7) matches the predicted output post softmax. Could you please clarify? This paper shows that group invariance methods across inferred partitions show better generalization in (non-)systematic distributional shifts and anomaly detection settings. It also suggests a new invariance penalty and empirically shows that it works better on three synthetic datasets viz. coloured-MNIST, COCO-on-colours, and COCO-on-places.The paper is written well and starts off by giving an intuition of why IRM-like methods are important by presenting the results of a simple experiment on coloured-MNIST (table 1). It then goes on to talk about (non-)systematic generalization before introducing the proposed method. The authors use reverse KL divergence between the group distributions as the penalty and use prior work to partition the datasets into groups. They use The results look promising across datasets, though it is slightly lower in the 'in-distribution' setting. I am happy to see that they also talk extensively about hyperparameter selection especially in the case where they assume no access to validation sets with a distributional shift.Overall, I like the work and would like to see it presented at the conference.One minor point: cite work the first time you introduce something, not later on. It can be a little confusing for the readers. I wondered if I missed something. For ex: "We find that a recently proposed method can be effective at discovering...", "IRMv1", etc. # Summary The paper introduces the Graph Information Bottleneck (GIB) which aims to learn the most-informative compressed representation $Z$ given graph $G$ with associated label $Y$. Further, it defines GIB-Subgraph which aims to learn the compressed representation as the subgraph $G_{sub}$ which maximizes the mutual information within the family of subgraphs ${\cal G}_sub$ of $G$. The paper introduces bi-level optimization objective which has the following parts: (a) optimizing the mutual information loss $L_{cls}$ between the subgraph representation $G_{sub}$ and the graph label $Y$ using the backbone GNN followed by aggregation of subgraph node embeddings $X_{sub}$ and cross-entropy loss when comparing to graph labels. (b) approximates the mutual information $L_{MI}$ between the original graph and a subgraph $I(G, G_{sub})$ using statistics network $f_{\phi}$ which uses the backbone GNN to obtain graph embeddings (using mean/sum or pooling over node embeddings) followed by MLP over concatenated embeddings of $G$ and $G_{sub}$. The procedure retrains the graph-subgraph mutual information estimator in the inner loop for each step (eqn. 10) before updating the parameters of the backbone GNN and the subgraph selection MLP and finally updating the subgraph-label MI estimator ($L_{cls}$). In order to obtain compact subgraphs, the paper introduces a regularization term $L_{con}$ closely related to graph cut. The papers shows empirically on downstream task of graph classification that adding the GIP objective improves classification accuracy. Further, on graph interpretation task, the authors show that the GIP objective improves the similarity of the retrieved subgraphs using domain-specific metrics. The authors also evaluate on graph denoising on the MUTAG dataset.# Recommendation I vote for a strong accept. This paper is well-written, makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation. # Questions to the authors - I would have liked to see in the supplementary material an example of the algorithm on a toy graph example (similar to case study A). - I wonder does the initialization have an influence on the final chosen subgraph nodes. Does $S$ (node-assignment) (always/almost always?) saturate  as mentioned on page 5? - What is the influence of the ${\cal L}_{con}$ on the size of the final chosen subgraph. A table showing the size of final subgraphs (in term of output of MLP $\theta_2$ in Figure 1) might be helpful, though this is partially addressed in Table 4. - For completeness, it would be good to provide in the supplementary material the properties of the datasets used e.g., number of graphs, mean/max/min number of nodes, edges, dimension of node features, dimension of edge features (if any), etc. - It would have been good to see plots showing the convergence of the different losses as part of the bi-level optimization iterations. - [optional] On the graph denoising experiment, it might be good to add more concrete evaluation both on larger graphs e.g. on graph families such as Power-Law, SBM as well as non-uniform edge addition.  Summary:Taylor polynomial based loss function metalearning acts as a regularizer that improves the networks adversarial attack robustness, performance, training time, and data utilization. The authors evolve weights, and so add arbitrary other factors to the loss, including adversarial robustness to learn a loss function parameterization which is more robust. They provide analysis of the attractor states under the optimization of a suite of loss functions.Quality:Writing Quality:The authors have paid attention to detail. The writing is succinct and precise throughout, and I was only able to find one typo though the first 8 pages of the manuscript. The progression between concepts is well motivated.Evaluation Quality:The evaluation methodology is novel and its results address the dynamics of training rather than static outcomes. The choice of adversarial attack robustness to demonstrate the value of optimizing the loss function for an alternate metric is a sound one. Validation acuuracy is used elsewhere to evaluate the generalization ability of the loss. Transfer of the learned loss across datsets and models is not evaluated.Result Quality:The improvements in adversarial attack robustness are large when optimizing TaylorGLO directly for that objective. The differences in attractor dynamics are dramatic (but also are less surprising). Clarity:Graphs & Tables:Figure 1, the attraction dynamics graph, is readable and quite clear. Unfortunately the entropy reduction definition of attraction in equation 17 cant be efficiently described in the caption in the same way that it is described in the text.Table 1s invariance results are clearly presented. The Welchs t-Test results generating P-value scores and the corresponding bolding is good. I would like to have seen a measure of meta-training stability or consistency in addition to or instead of accuracy, backing the claim about improved stability moving with the evolution population size. Figure 2s attack strength against testing accuracy plot is clean. Each axis meaning is clear, as and the interpretation of the result is natural. The papers writing clarity is very good. The background is comprehensible. The decompositions in section 3 are well factored. The disagreement with Blanc et al. (2020) in section 4.1 can be fleshed out in more detail, but the writing in the rest of the section is precise and succinct.Originality:One challenge with addressing the originality in the paper is understanding what novelty should be attributed here and what should be attributed to the original TaylorGLO paper.Novel evaluation methodology (attractor dynamics) are underemphasized relative to novel regularization results, and depends on the insight that the upside of the zero training error regime is that much of the continued update is all about the optimizers bias and not about the training data. Significance:One major question in this work concerns the generality of its findings. Are these attractor dynamics specific to TaylorGLO? What are ther implications for other regularizers? The methods added complexity makes the method unlikely to be used unless it can clearly differentiate itself from other regularizers. For example, label smoothing is likely to create very similar attractor dynamics to the dynamics seen in TaylorGLO. The regularization effect (output entropy penalty) is also very similar. One differentiating feature is the ability to add other objectives (like adversarial robustness) to the learned loss. Pros:The novel evaluation methodlogy which relies on the insight that all loss function changes will have a downstream impact on the gradients is a nice addition to the loss function metalearning suite. Attractor dynamics, optimizing for an alternate objective like adversarial robustness and the demonstrated flexibility of TaylorGLO to cover label smoothing, MSE, Cross-Entropy, and more are welcome contributions.Cons:The appreciation of the existing loss function metalearning literature is poor in this paper. Loss functions are commonly learned with Neural Networks! These losses are often easier to optimize and are more flexible than standard losses. They can also make non-differentiable feedback differentiable. See this metalearning survey for plenty of references. https://arxiv.org/pdf/2004.05439.pdfComparisons between taylor approximation paremeterized loss functions and neural network parameterized loss functions would have been an important comparison to see, but this side of loss function metaleanring isnt referenced. What are the attractor dynamics for those neural network learned losses? Is this different / improved? While many of these papers focus on reinforcement learning or unsupervised learning, they point to very similar improvements coming out of loss function metalearning. Ex, any of the following:Evolved Policy Gradienthttps://arxiv.org/abs/1802.04821Learning to Learn: Meta-Critic Networks for Sample Efficient Learninghttps://www.researchgate.net/publication/318029457_Learning_to_Learn_Meta-Critic_Networks_for_Sample_Efficient_LearningOnline Meta-Critic Learning for Off-Policy Actor-Critic Methodshttps://arxiv.org/abs/2003.05334Online-Within-Online Meta-Learning (learned regularization)http://papers.nips.cc/paper/9468-online-within-online-meta-learning.pdfMeta-Learning Update Rules for Unsupervised Representation Learninghttps://arxiv.org/abs/1804.00222Learning to Learn by Self-Critiquehttps://papers.nips.cc/paper/9185-learning-to-learn-by-self-critiqueThis paper doesnt focus on task generality. Many of the other metalearning loss functions papers do. Why? How general are their learned losses? Can they generalize from task to task, or does it have to be retrained every time? Why dont they discuss these issues?They dont release code for reproducibility.Notes:Ideally Figure 1s information would be shown for Bikal, MSE, and Label Smoothing as well (perhaps in the appendix) to assess whether TaylorGLOs training dynamics add anything on top of Label Smoothing (which one would expect to have the same transition to push away from the correct label in later epochs). But there the definition of zero training error itself is modified, and so their metric may not capture very similar optimization dynamics.The claim that TaylorGLO lowers confidence could be evaluated on calibration, rather than or in addition to entropy. TaylorGLO may be doing much more than regularization in practice, and the evaluiton criteria dont seem sufficient to know that more isnt happening to the model optimzed for this loss.It would be good to see the attractor dynamic graphs for label smoothing (presumably it is very similar to TaylorGLO).It appears that Theorem 4.2 basically describes label smoothing, though they dont say this.A typo on page 6! Thus, values less than zero imply that entropy is increased, values greater than zero that it is decreased, and values equal to zero imply that there is no change. The paper presents a method to find adversarial inputs for neural networks in regions where the networks can be "proven" not to admit any such adversarial examples, practically demonstrating the unsoundness of a "complete verifier" as well as an "incomplete verifier".While it was already obvious to me that "verifiers" that assume floating-point arithmetic is the same as real arithmetic are unsound, the paper is a service to the community in that it also makes this very obvious to informed outsiders who may not have already questioned the validity of robustness verification research that does not model round-off and even ignores it in its own implementation. The related work section does a good job of surveying the state of the art as it relates to floating-point soundness. The authors also took some space to discuss how their findings relate to current and future research on robustness verification, which I think is important in this case.Perhaps there could be a short discussion of challenges that different approaches face to become sound with respect to floating-point semantics. (For example, it seems particularly challenging for approaches based on duality, as the correctness of certificates depends non-trivially on closed-form solutions to optimization problems as well as associativity of addition.)The technical sections are mostly well-written, though I was not able to figure out some details. For example, it is not so clear how precisely binary search is used to find ± and ´ simultaneously. Section 4.2 is a bit dense and its presentation could probably be improved."inevitable presence of numerical error in [...] the verifier".It is not inevitable that the verifier is subject to "error". We could encode the precise floating-point semantics of the neural network as a SAT formula (and then watch the SAT solver time out, but this does give a sound and complete method). This paper is well written and introduces a novel method to learn dynamical models, incorporating prior knowledge in the form of systems of ODE.The Neural Dynamic Systems method is described in sufficient details and multiple variations are given for the handling of systems where only partial or approximate knowledge is attainable.The experiments explore three different applications of the NDS method introduced in the paper, to a simple synthetic and noiseless physical system, a simplified fusion system where the system dynamics are approximate, and to a modified Cartpole control problem.The experiments show promising results, and it seems likely that the machinery developed in this paper will find impactful applications in the natural sciences and in model-based RL.I would suggest exploring alternative RL models than the Cartpole problem, such as a robotics application, where the impact of an NDS approach might lead to more interesting results.In the related work section, I recommend adding citations to arxiv:1909.05862 and arxiv:1909.12790, which explore very different graph-based methods to tackle a similar issue of predicting the dynamics of physical systems.All in all, it is an interesting contribution to the literature of physical predictions, and I recommend it for acceptance.  Training deep learning models is becoming increasingly challenging due to a memory bottleneck that limits the size of the feature maps that can be stored. The paper presents an automatic framework (MONET) that minimizes the memory footprint for deep networks. The novelty of MONET is that it jointly optimizes over: (a) global compute graph level techniques (such as checkpointing) and (b) local techniques (such as memory-efficient implementations of individual operators). While there are several existing works that focus separately on optimizing global techniques (e.g. the work on Checkmate) or local techniques, MONET is the first to jointly optimize over global and local techniques.The memory constraints are carefully analyzed for the forward and backward passes, and expressed as a 0-1 integer program, which is then solved using the state-of-the-art solver Gurobi. The experimental evaluation confirms the theoretical gains provided by the solution of the optimization problem. In particular, the authors compare with a vanilla implementation in PyTorch, with the Checkmate-D (default Checkmate that uses global techniques), and with Checkmate-O (post-optimized to greedily run the fastest convolution algorithm). It is interesting to notice that MONET offers significant gains in all cases, even over Checkmate-O, underlying the need to perform the joint optimization in order to obtain the best schedule.I advocate for the acceptance of the paper. The memory bottleneck is an acute problem in deep learning, and the paper provides a practical solution that can alleviate this problem to some extent. I encourage this line of work and hope to see such schedule optimization become a standard option in deep learning frameworks. The paper is very well written, with a clear description of the approach and convincing experimental evaluation, while providing abundant references about related work.Minor comments, questions for the authors:The memory requirements for some of the networks shown in the experimental evaluation are still small. While I understand that the GPU memory is limited to 16GB and there was a desire to compare against vanilla implementations, I think it would be interesting to show how models that require a lot of memory for training can benefit from MONET. For example, the UNet that was used in the paper seems to be the 2D version. The 3D UNet can require up to several hundred GB without optimizations (for varying batch sizes). Would it be possible to include even a simple estimate of the possible gains when using MONET? What would be the computational tradeoff to train it on the GPU used in the paper? Objective: Find a way to parametrize equilibrium neural networks to allow setting a Lipschitz bound on the input-output mapping. Central claims: 1.There exists an extension to the parametrization proposed in Monotone Operator Equilibrium Networks (MON) that allows for explicitly setting a Lipschitz bound. 2.The resulting networks (dubbed LBEN here) can be practically trained via. unconstrained optimization, which involves computing the equilibrium using tools from convex optimization. 3.Empirically, LBEN models can achieve comparable performance with Monotone Operator Equilibrium Networks, their pre-set Lipschitz constants are tight and therefore achieve favorable accuracy-robustness tradeoff. Strong points: 1.Interesting idea with very strong theoretical backing: I found the theoretical development and analysis of LBEN models very rich and interesting. There are a bunch of novel results scattered throughout, and the analysis draws from a wide variety of disciplines, including convex optimization, dynamical systems theory and control theory. I found the paper delightful to read. The properties that LBEN models are proven to possess (well-posedness under less restrictive conditions than MON and more natural assumptions on the activation functions) are quite compelling. LBEN models also dont have any extra computational overhad over MON models. 2.Claims are well supported. Claims 1 and 2 are very well supported in theory. Claim 3 can only be confirmed for very small scale experiments. Weak points: I believe the only relative weakness of the paper is its experiments section. 1.Lack of large scale experiments: The models trained in the experiments section are quite small (80 hidden neurons for the MNIST experiments and a single convolutional layer with 40 channels for the SVHN experiments). It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small-scale experiments will (or will not) extend to larger scale experiments. 2.Need for more robustness benchmarks: It is impressive that the Lipschitz constraints achieved by LBEN appear to be tight. Given this, it would be interesting to see how LBENs accuracy-robustness tradeoff compare with other architectures designed to have tight Lipschitz constraints, such as [1]. 3.Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non-fully connected architectures. 4.Lack of description of how the Lipschitz bounds of the networks are computed: This critique is self-explanatory. Decision: I think this paper is well worthy of acceptance just based on the quality and richness of its theoretical development and analysis of LBEN. Id encourage the authors to, if possible, strengthen the experimental results in directions including (but certainly not limited to) the ones listed above.  Other questions to authors: 1.I was wondering why you didnt include experiments involving larger neural networks. What are the limitations (if any) that kept you from trying out larger networks? 2.Could you describe how you computed the Lipschitz constant? Given how notoriously difficult it is to compute bounds on the Lipschitz constants of neural networks, I think this section requires more elaboration.  Possible typos and minor glitches in writing: 1.Section 3.2, fist paragraph, first sentence: Should the phrase equilibrium network be plural? 2.D^{+} used in Condition 1 is used before its defined in Condition 2. 3.Just below equation (7): I think theres a typo in On the other size, [&]. 4.In Section 4.1, \epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \epsilon when (11) is discussed. 5.Section 4.2, in paragraph Computing an equilibrium, first sentence: Do you think theres a grammar error in this sentence? I might also have mis-parsed the sentence. 6.Section 5, second sentence: There are two thes in a row. [1] Anil, Cem, James Lucas, and Roger Grosse. "Sorting out lipschitz function approximation." International Conference on Machine Learning. 2019. **GENERAL**The goal of the paper is to model the marginal over latents in VAEs in such a way to minimize the mismatch with the aggregated posterior. The paper proposes a new class of marginal distributions over the latent space that is a product of two experts: the first expert is a non-trainable probability distribution, and the second expert is an unnormalized probability distribution parameterized using neural networks. Since training a product of experts requires to apply an approximate inference (e.g., MCMC sampling), the authors propose to use the likelihood ratio trick. Eventually, a VAE is trained in two stages. First, they assume the marginal over z's to be simply the non-trainable distribution, and the VAE is trained. At the second stage, they propose to train the second expert (i.e., the binary classifier that distinguishes z ~ q(z) and z ~ p(z)) in order to obtain the final NCP that better matches the aggregated posterior. Further, the idea is extended to hierarchical VAEs, and a separate binary classifier is trained per each stochastic level.**Strengths:**S1: The idea is very interesting and allows to enrich the marginal distribution over z's in the VAE framework.S2: The paper is well positioned in current trends in generative modeling. I find the combination of energy-based models and VAEs as a very appealing research direction.S3: The proposed two-stage learning procedure is very logical and seem to be efficient. Its simplicity increases its reproducibility.S4: Obviously, introducing an energy-based component results in intractability of the likelihood function due to the partition function. However, the FID scores and the quality of generated images are very convincing.**Deficiencies:**D1: In this review, I kept using "a non-trainable expert" or "a non-trainable marginal" for the base prior (the term used by the authors). However, I am not quite sure whether the base prior is non-trainable. I was unable to find any information about it in the paper. Therefore, I assumed it is a standard Gaussian after inspecting Figure 1. It would be easier for a reader, if this information was included in the text.**Remarks:**R1: The Appendix B.1 is closely related to the following papers:- Rezende, D. J., & Viola, F. (2018). Taming vaes. arXiv preprint arXiv:1810.00597.- Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.- Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial Intelli- gence and Statistics, pp. 12141223, 2018.It is maybe worth to mention these papers there. Otherwise, the text may sound as a new contribution.R2: Section 3, first paragraph: The authors stated that: "Recently, energy-based models have shown promising results in representing complex distributions". This statement is very misleading, because the energy-based models (e.g., Boltzmann machines) have been used in ML for over 30 years, e.g.:- Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive science, 9(1), 147-169.- Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines. Parallel distributed processing: Explorations in the microstructure of cognition, 1(282-317), 2.- Salakhutdinov, R., & Hinton, G. (2009, April). Deep boltzmann machines. In Artificial intelligence and statistics (pp. 448-455).- Larochelle, H., & Bengio, Y. (2008, July). Classification using discriminative restricted Boltzmann machines. In Proceedings of the 25th international conference on Machine learning (pp. 536-543).The word "recently" suggests that this is a new invention that is simply not true.R3: The proposed prior (NCP) could be seen as a specific form of a product of experts (e.g., Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 177). In my opinion, it is an interesting connection.R4: I wonder whether it is feasible to use some sort of importance sampling (e.g., Annealed Importance Sampling, Salakhutdinov, R., & Murray, I. (2008, July). On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learning (pp. 872-879).) or other procedure (e.g., Perturb-and-MAP, Hazan, T., & Jaakkola, T. (2012, June). On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Conference on International Conference on Machine Learning (pp. 1667-1674).) to estimate the partition function. This paper studies the problem of learning to play a Nash equilibrium in two-player, zero-sum Markov games.  This is a longstanding problem, with many algorithms proposed but relatively few theoretical convergence guarantees, and most of those either for quite restricted settings or with strong assumptions.  This is in stark contrast to the stateless setting of Normal Form Games, where we have many strong theoretical convergence guarantees.  The main algorithm is a version of the classic fictitious play algorithm.  Like prior adaptations of fictitious play to Markov Games, it operates on the Q-values, but a key novelty (at least in the stateful setting; similar ideas were recently applied in a special case of normal form games by Swenson and Poor 2019) is the use of a particular form of regularization in the best response process.  The main result is that as long as the game satisfies Lipschitz and Concentratability properties for each player when the other plays optimally and the policy updates are sufficiently accurate then play converges to a Nash equilibrium.I like this paper quite a bit.  It tackles a hard problem  and makes solid progress.  I think the algorithm and analysis are both nice contributions and definitely intend to study the latter further as I think aspects of it may be useful in other settings.  Overall the presentation, while dense, is clear.  However, I believe there are a few issues that could use additional discussion: 1) Why does the uniqueness, or lack thereof, of the Nash equilibrium not matter to the results?  Quite a bit of prior work has had caveats when they are not unique.  The results seem to hold if the assumptions are true for at least one equilibrium, presumably because of the minimax properties in a zero-sum setting, but Im not quite clear how this interacts with the assumptions.  For example, if one but not all the equilibra cause the game to satisfy Assumption 4.2 and 4.3 what causes the guarantees to still hold even if initially play gravitates toward some equilibrium where they do not?2) Im not quite clear how to interpret the convergence guarantee in Theorem 4.5.  The text after the theorem talks about the policy sequence converging to a neighborhood, while the theorem itself is about the averages across the sequence of policies.  It would help to have some more detailed discussion of exactly what sort of convergence behavior we should expect.3) Im intrigued by the observation at the end that this algorithm is Hannan consistent under stronger assumptions.  There has been some work recent work exploring connections between regret minimization and RL and it would be worth discussing a bit how this observation relates to that literature, e.g.: @inproceedings{hennes2020neural,  title={Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients},  author={Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R{\'e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du{\`e}{\~n}ez-Guzm{\'a}n, Edgar and others},  booktitle={Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},  pages={492--501},  year={2020}} Summary: This paper does theoretical analysis about self supervised learning (SSL), esp. those methods using contrastive learning. It zooms into one-layer and two-layer networks and proves contrastive learning can converge to weights corresponding to largest eigenvector of a covariance matrix. Experiments on synthetic and real datasets are consistent with theoretical conclusions.Reasons for score: The covariance operator sheds light to the black-box learning process of contrastive learning. Approximating the data distribution with a hierarchical latent tree model is an interesting technique. This work may inspire practical tools to improve SSL. Issues:1. My major concern is the analysis about BYOL. The authors of BYOL disagree with the conclusion in this paper that BatchNorm provides implicit contrastive objective for BYOL, in a newly released paper "BYOL works even without batch statistics". I'd ask the authors, does this invalidate your analysis about BYOL? Or does the GroupNorm + weight standardization also provide implicit contrastive objective, similar as BN? 2. To what degree the validity of the theoretical analysis depends on the type of the loss? Say if I change the InfoNCE loss to some other loss, will that invalidate the whole analysis?3. A typo in page 4: "we setup the following..." => "we set up the following..." This paper discusses many issues in the data poisoning literature. They call into question the real world applicability of data poisoning attacks and discuss the short coming in the literature. They present several issues in the comparison of such attacks. Notably they show that many attributes of a data poisoning attack assumed to be irrelevant to the success of the attack are in fact statistically significant, such as a dataset size. Finally they present a new benchmark which attempts to standardize the comparison between data poisoning attacks.  I think this paper will benefit the data poisoning literature by illustrating its short comings and presenting a path forward. ----------------------------Suggestions: in the 'Clean attacks are sometimes dirty ' section the authors present two examples, but a human study could be set up in order to establish how many 'clean' attacks are actually identifiable. A small Mechanical turk study would greatly increase the strength of this argument and should not be financially prohibitive. The section on 'Performance is not invariant to dataset size ' convinces the reader that this attribute thought to be irrelevant to the success of the attack is relevant, but proposes no mechanism by which the data set size could effect the attack. A discussion section attempting to provide an answer to this question would be interesting, but is perhaps out of scope.  The section on data augmentation does not mention if the augmentations are also applied to the poisoned inputs. If the clean data is augmented but the poisoned data is not, it seems expected that the success of the attack would drop.  The paper is a good idea. It is based on the notion of following the same line of reasoning as the prior work of MH-GAN. However, it makes the crucial advance of allowing the use of gradient information for the far more efficient HMC-type samplers.The paper is able to do much of the sampling in the latent space, which enables the use of gradient-based sampling. One nitpick would be that I suspect there is a latent assumption that the mapping of z -> x is injective (but need not be bijective like in normalizing flows). However, this is not explicitly stated at all in the paper. Paper summary:Building on previous work on neural operators, the paper introduces the Fourier neural operator, which uses a convolution operator defined in Fourier space in place of the usual kernel integral operator. Each step of the neural operator then amounts to applying a Fourier transform to a vector (or rather, a set of vectors on a mesh), performing a linear transform (learnt parameters in this model) on the transformed vector, before performing an inverse Fourier transform on the result, recombining it with a linear map of the original vector, and passing the total result through a non-linearity. The Fourier neural operator is by construction (like all neural operators) a map between function spaces, and invariance to discretization follows immediately from the nature of a Fourier transform (just project onto the usual basis). If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization. Experiments demonstrate that the Fourier neural operator significantly outperforms other neural operators and other deep learning methods on Burgers equation, Darcy Flow, and Navier Stokes, and that that it is also significantly faster than traditional PDE solvers.------------------------------------------Strengths and weaknesses:Much of the theoretical legwork for this paper, namely, neural operators, was already carried out in previous papers (Li et al.). The remaining theoretical work, namely writing down the Fourier integral operator and analysing the discrete case, was succinctly explained. The subsequent experimentation was extremely thorough (e.g. demonstrating that activation functions help in recovering high frequency modes) and, of course, the results were very impressive. I liked the paper a lot, and its definitely a big step-forward in neural operators. Im assigning a score of 8 (a very good conference paper), and I think that the paper is more or less ready for publication as is. Ive included a few questions below (to help my own understanding), as well as some typos I spotted whilst reading the paper.------------------------------------------Questions and clarification requests:1)Section 4, The Discrete Case and the FFT  could you explain the definition of bounds in the definition of Z_{k_{max}}?2)Section 4, Parametrizations of R, sentence 2  could you explain the definition R_{\phi}? At present I cant see how the function signature of R matches the definition given.------------------------------------------Typos and minor edits:- Page 3, bullet point 3  solving Bayesian inference problem -> solving Bayesian inference problems- Section 1, final paragraph, sentence 2 - approximate function with any boundary conditions -> approximate functions with any boundary conditions- Section 4, The discrete case and the FFT, final paragraph, last sentence - all the task that we consider -> all the tasks that we consider- Section 4, Parametrizations of R, last sentence - while neural networks have the worse performance -> while neural networks have the worst performance- Section 4, final sentence  Generally, we have found using FFTs to be very efficient, however a uniform discretization if required. -> Generally, we have found using FFTs to be very efficient. However, a uniform discretization is required.- Section 5, final paragraph, sentence 2  FNO takes 0.005s to evaluate a single instances while the traditional solver -> FNO takes 0.005s to evaluate a single instance while the traditional solver- Section 6, final sentence  Traditional Fourier methods work only with periodic boundary conditions, however, our Fourier neural operator does not have this limitation. -> Traditional Fourier methods work only with periodic boundary conditions. However, our Fourier neural operator does not have this limitation. The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments. A state distribution is fitted during the interaction with an environment and the probability of the current state is used as a virtual reward. The parameters or sufficient statistics are also applied to the policy. The motivation is clear and verified. It is generally a good paper.It is surprising that the exploration is achieved in the long term even minimizing state entropy. Is that possible the exploration events are from the 'unstable' environment? What if there are some patterns underlying the exploration events but only part of the 'unstable' environment? Is that OK to totally rely on unexpected events from the environment to explore the environment? Is that possible to add some exploration strategy in the developed model?  This paper proposes an improvement of the standard NP by using a mixture distribution \q_{\phi}, semi-implicit variational inference, and max pooling to capture the multimodel structure of the posterior distribution. Replacing one normal Gaussian distribution with a mixture (of Gaussians, normally) is a widely-adopted idea in latent variable models including NP; the adopted semi-implicit variational inference was originally developed in Yin and Zhou ICML 2018, and no further improvement on this inference method is proposed in this manuscript; max pooling is one of three commonly used pooling methods, i.e., max, min, and mean pooling. using one of them to replace another is simple but the explanation of the reason why max pooling is better is interesting and profound. So, the improvement is weak although it is shown to be effective by the empirical study. More importantly, the authors have investigated the posterior contraction of NP. It is interesting. The relationship between the two parts of the objective function of NP has been discussed related to the posterior contraction, both parts have contributed to the contraction apart from their classical explanation on reconstruction and regularization. To my best knowledge, it is the first work to discuss the posterior contraction of NP. It is a classical property in Bayesian and this link will enable further theoretical analysis for NP.  Summary:The paper meticulously builds a theoretical framework for Lie-algebra convolutional layers and then goes on to show how CNNs, GCNs and FC layers are a special case of L-convs. The paper also demonstrates how the underlying generators can be learnt from data and provide convincing supporting experimental results. The proposed L-conv layers also use much fewer parameters compared to previous works. Key strengths:The theoretical framework developed in this paper, starting from Lie groups and equivariance and invariance definitions is very elegant and convincing. I checked the maths at each step and am convinced that it is correct, to the best of my knowledge. I did need to refer to Hall 2015 though. Intuitively as well as mathematically, it makes sense to me. The comparison to MSR (Zhou 2020) seems fair to me. The experiments, though limited, in the main paper, are quite convincing. The experiments are cleverly constructed and provide enough justification to support the utility of L-conv layers in comparison to CNN and FC layers. Questions:For Figure 2: For CIFAR100 and FashionMNIST, CNN seems to do better on "rotated+scrambled" compared to "rotated". What is the reason behind that? This is not seen in any other method or dataset.  Suggestions for improvements:1. The paper inherently assumes familiarity with Lie groups/Lie algebra or even exp/log of matrices which I am familiar with, but not all readers will be. Therefore, instead of citing Hall 2015, it would be good to cite Sections within the textbook. This will aid uptake of an important mathematical sub-field. 2. There is no mention of accompanying code in the manuscript. Would the authors consider making it available upon acceptance? It would help further research in this area. 3. The section on linear regression (3.1) seems to occur again in an expanded form in the supplementary material (Sec C). The derivation in the supplementary material was a little bit clearer. 4. It would be worthwhile checking the paper for typos. A few that I noted: larest, wight, "a too many"5. Figure 1 is not easy to interpret. A substantial caption would be beneficial. It's also not referenced in the text. Overall comments:The main contribution of this paper is the development of the theoretical framework for Lie-algebra convolutions. The paper does so very convincingly and I regard this as an important contribution to the area of deep learning. This may open the door to the field using the correct inductive biases for many problems in vision, speech and physics. I enjoyed reading this paper, including the supplementary material that makes a start on imposing orthogonality during regularization for complex-valued neural networks.  In this paper, the authors propose a new test to measure a text models multitask accuracy based on 57 different tasks in Humanities, Social Science and STEM subjects. The authors experiment with the GPT-3 models of various sizes and UnifiedQA, and the results suggest that the size of the model may be one of the important factors in achieving higher performance on all the tasks.This is an interesting work that tries to tackle a very important problem of how successful are large language models across multiple tasks. It would help the paper to have a more thorough discussion of the results. Based on the reported results, larger GPT-3 models perform better but a smaller UnifiedQA model outperforms GPT-2 by a substantial margin, why is this? In addition, it would be interesting to see a comparison of GPT to BERT and XLNet. Such a comparison would first of all show if the idea generalizes to other types of language models. Second, it would emphasize the advantages/disadvantages of autoencoder-based vs autoregressive models and could potentially provide additional insights on how important these differences are for different tasks.Given the authors provide additional discussion, I would like to see this paper and the associated dataset to be presented at the conference. This work constructs non-asymptotic confidence intervals for off-policy evaluation. This is achieved by assuming that the reward at any given time only depends on the state action pair, leveraging that assumed structure to define the difference between the empirical and estimated bellman residual operators as a Martingale difference sequence. This, in turn, then allows the authors to apply a Hoeffding-like concentration inequality which applies to Hilbert spaces. The authors then provide a derivation of the confidence bounds by considering the divergence between policies. The work improves on the rate of prior work from $O(n^{-\frac{1}{4}})$ to $O(n^{-\frac{1}{2}})$ and allows for estimation without the need of global optimality via the dual formulation, both of which are very nice additions to the literature. Experimental evaluation backs up the authors claims, showing very strong performance with respect to prior art. I found this paper to be very well written and presented, with impressively thorough theoretical results and good empirical validation. A couple of minor questions:(1) Performance of the proposed method when the functions dont lie in an RKHS. It appears that the formulation in appendix E provides a bound which uses Rademacher complexity and doesnt rely on an RKHS. Can the authors provide intuition around how much worse we would expect this to be in practice? (2) Proposition G.1 makes a case for the necessity of assuming a smoothness condition in the absence of an independence between transition pairs. Under a milder condition on transition pair independence, e.g. a mixing condition, are similar bounds to those presented in the current work attainable? **Summary and Key Contributions:**This paper proposes the use of task embeddings for a novel conditional attention mechanism and various task-conditioned modules. The paper also proposes the use of an entropy-based multi-task uncertainty sampling to automatically determine how to sample the data for training the MT architecture. The paper demonstrates the effectiveness of the proposed architecture and sampling through multiple comprehensive experiments and ablations/analyses.**Positives:*** The paper proposes several modifications to the transformer architecture that enable leveraging a learned task-specific embedding, namely conditional attention, conditional alignment, conditional layer norm and conditional adapter. * The paper proposes the use of the entropy-based MT-Uncertainty Sampling to choose which training examples to use, in a manner that accounts for some of the subtleties (that were perhaps not immediately obvious, but clearly visible in hindsight), such as accounting for the differing numbers of classes in each task.* Most of the modifications proposed are clean, well motivated and make a lot of intuitive sense. The overall approach also seems to work well without the need for extensive hyperparameter tuning.* Strong, thorough experimental validation of the proposed techniques. Evaluations on GLUE, Super-GLUE show the proposed method often outperforming (or at least performing comparably to) several strong baselines, both MT-based and single model/fine-tuned, and with fewer parameters. The paper's proposed approach sets a new state-of-the-art performance on 3 tasks (WNUT-2017, SciTail, SNLI).* Good ablations of the utility of the different task embedding-grounded modules, on the performance of MT-Uncertainty sampling compared to other methods, how MT-Uncertainty sampling tends to choose most difficult tasks first while also avoiding catastrophic forgetting, and of transferring to new tasks or progressively adding in additional tasks.**Suggestions:*** The paper claims to attain 2.2% higher performance compared to a full fine-tuned BERT large model (abstract line 16). However, it seems like the paper compares a CA-MTL-RoBERTa-Base model to a BERT-base baseline. This is a somewhat unfair comparison, and it might be better to either clarify or to compare the baseline to the CA-MTL-BERT-Base model (which based on Table 2 would still be a rather impressive 1.3% better while making the comparison fairer).* While I understand that space is limited, Section 2.1 might have benefited from explaining FiLM, its use here and some intuition, to help keep the paper more self-contained.**Questions/Clarifications:*** (A5 Table 11) On SciTail, it appears that Random Init performs almost as well as STS-B in the zero-shot setting, which is rather surprising. Do you have any intuition/analysis as to why this might be the case?* (line 104-105) How exactly was the block matrix A_n obtained? And does $\bigoplus$ represent the diag operator?* (line 95) Does $p_i(\mathbf{y_i}|\mathbf{x_i},\mathbf{z_i})$ represent the model prediction score, i.e., $f_{\phi(\mathbf{z_i}), \theta_i}(\mathbf{x_i})$?* In Table 5, the total data used was 66.3% (or 64.6%). How was the amount of data to be used determined? Was a stopping point determined based on the GLUE dev set?* The paper refers to "assemble methods" (line 267, Table 4 left header, line 657). Was it perhaps meant to say "ensemble"? Summary: The paper proposes to parameterize learning rate (LR) schedule with an explicit mapping formulation. This learnable structure allowed the proposed meta-trained MLR-SNet to achieve good LR schedules. For validation, the proposed method is evaluated on both image and text classification benchmark with various network architectures and datasets, as well as transfer the learned network for new task or architectures. Justification of rating: The paper solve a practical problem that is not handled in the existing literature. Despite the straightforwardness of the proposed approach and the methodology, this work has the potential to bring high impact to the research community. Strengths:+ This work proposed to parameterize the LR schedule with a MLR-SNet.  The results shows it is more flexible and general than the hand-picked LR schedule.+ The meta-learned approach allow the learned model to be applied to unseen data.+ The paper provide comprehensive experiment to validate the efficacy of the proposed model. Comments:- Experiment on Penn Treebank shows the convergence of the proposed MLR-SNet is slower than SGD and Adam. The paper argue that it predicts LR according to training dynamics by minimizing the validation loss. Please provide more details why is this a more intelligent way to employ validation set. Is this also observed in any other dataset?- This work transfer the learned LR schedules on CIFAR-10 with ResNet-18 to several other datasets. Has the author  try to transfer MLR-SNet learned with other source dataset? How will the model trained with different model/dataset behave when transferred to new datasets or networks. It might be interesting to explore if the certain type of network architecture (more complex or simple) would learned a more generic model.- In the "Formulation of MLR-SNet", it states that the input $h_{t-1}$ and the training loss are preprocessed by a fully-connected layer $W_1$ with ReLu activation function. Please describe the purpose of this layer.  Summary: This paper introduces the free algebra, a classical mathematical concept as a generic tool to represent sequential data of arbitrary length. The proposed method has attractive theoretical property, such as preserving universality of static feature mapping, and convergence in the continuous setting. The author further proposes using stacked rank-1 projection of the free algebra as an approximation to the sequence representation in order to make it computationally feasible neural networks layers. The author illustrated the flexibility and effectiveness of the proposed method by combining the NN implementation with FCN to benchmark on multivariate time series classification problem, and GP-VAE model to benchmark on sequential data imputation problem. The proposed methods shows improved results over previous state-of-the-art. Significance: This paper provides the community an extension of the universal approximation theorem of NN on sequential data, as well as a generic method to transform static feature maps into sequence features. The experiment shows the proposed method and its implementation is flexible and effective in both discriminative and generative problems. Some questions/feedback: 1. In Proposition 2.3, while the rank-1 projection makes the method computationally feasible, taking sum over all non-contiguous subsequences of x cannot be too cheap? Would the author add analysis on computation complexity here? 2. In the experiment section, as the main motivation of stacking Seq2Tens layers is to mitigate the limitation of the representation power, how does different number of stacked Seq2Tens layers change the model performance?  Clarity: While the paper is highly technical, the author did a good job explaining the idea, concepts and objectives. Originality: I am not aware of any other work explore free algebra and its usage on sequential data representation.  ---- Summary ----This paper proposes a multi-agent learning algorithm where after each policy improvement step, a small meta-game is analysed to propose a corrected step. The paper performs experiments in various MARL environments to test the approach.---- Reasons for score ----I recommend accepting this paper. The paper addresses the important problem of the non-convergence of independent learning in MARL. The algorithm proposed is novel and well justified, and the experiments show that it yields an improvement over independent learners.---- Pros ----1. The fundamental idea of the paper - taking independent policy improvement steps, and then analysing a local metagame to decide how to update policies - is interesting and novel.2. The practical algorithm this leads to helps with the problems of independent learning in MARL, without a large increase in algorithm complexity.3. The experiments demonstrate the advantages of the algorithm, in a variety of domains and with a fair comparison to the IL methods MATRL is being based on and other relevant baselines.4. The paper is situated well with respect to the existing literature.---- Cons ----It seems to me that the effectiveness of the proposed method is likely to be heavily influenced by the underlying IL algorithm employed. In particular, an IL algorithm making very large policy updates is likely to need more correcting. For this reason, it would be interesting to investigate how the performance of MATRL and IL varies with a range of policy update sizes, and it would also be useful to clarify how the hyperparameters for the various experiments were selected.---- Typos and other minor comments ----1. Please clarify the meaning of the error bars in Figure 5.2. In the sentence before section 3.1, steps->step and details->detail.3. I didn't understand this in section 3.1: "We set agent is to make a monotonic improvement of its policy."4. Near the bottom of page 3, "conflict interests" -> "conflicting interests" **1. Summary and contributions: Briefly summarize the paper and its contributions** In this work, authors looked at Out of Distribution (OOD) data from the data augmentation and regularization perspective and introduced Out-of-distribution data Augmented Training (OAT) based on their theoretical analysis which demonstrated that training with OOD data can remove undesirable feature contributions in a simple Gaussian model. The authors conducted experiments on both standard learning and adversarial learning and showed the effectiveness of OAT with strong results. Experimental results also imply that a common undesirable feature space exists among diverse datasets.  ##########################################################################**2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.** Very strong experimental results, it is clear from the results that OAT is effective for both standard learning and adversarial learning. A sound theoretical analysis demonstrating that training with OOD data can remove undesirable feature contributions in a simple Gaussian model. Neat randomization test that analyzed the effect of OAT for standard learning. It verified the claim that OAT regularizes the model to learn only features with a strong correlation with class labels, even though the generalization gap is rather small.  ##########################################################################**3. Weaknesses: Explain the limitations of this work along the same axes as above.** In the middle of page 6: In particular, from the results against AA it can be seen that the effectiveness of OAT does not rely on obfuscated gradients (Athalye et al., 2018). It would be helpful to explain this a bit more.Adversarial training on OOD data could be more clearly described before the theoretical analysis. How exactly are you using the OOD data? Maybe switch 3.1 and 3.2? Then it will be easier for the readers to follow the theoretical analysis. I think it will improve readability if Equation 9 is introduced earlier in the paper. Why not introduce OAT earlier?For the rest of the writing issues, see the next section.  ##########################################################################**4. Clarity: Is the paper well written?**Typos: \Page 2: a high success rates\Page 2: a human-an ability thought to be\Ambiguity: \Title: The word remove first appeared on page 4, the middle of the paper. Even though the paper conveyed the idea in the title, consider either change the title or more explicitly introducing the ideas in page 4 in the introduction section to improve readability. \Page 3, The space X was never explicitly defined. \Page 3, itd be good to remind readers what is u.a.r\Page 8, can lead to higher performance by implementing existing data augmentation methods. Did you mean when mixed with existing data augmentation methods?\##########################################################################**5. Reasons for score**In conclusion, the ideas are very interesting and the strengths outweigh weakness by a large margin, so I would recommend accept.  Pros: originality, clarity, technical correctness.Cons: experiments need some clarifications.Calibration typically relies heavily on binning the data, both for the calibration itself  (Histogram Binning) and how to measure its quality (ECE). Thus both operations suffer from sampling issues that are a cause for both bias and variance. The idea to rather perform the analysis using cumulative distributions would seem obvious, as it has been tried on so many other problems, but I have not seen it used for calibration. They do it for both calibration and its measure:-The use of the Kolmogorov-Smirnov test to measure calibration between the target and the output distributions.-Spline fitting of the cumulative distribution to compute its derivativeAs the implementation details are far from obvious , there are several original contributions, especially in implementing the splines.I found the description both very clear and concise, switching between intuition and equations.The only part I found confusing is the second paragraph of section 4.2 (One method of calibration..). Fortunately, the next paragraph gives a very simple intuitive explanation by just stating how it is implemented.Experiments are very comprehensive and show improvements over Temp scaling and other methods. However, there are also some results that contradict previously reported experiments and need to be clarified:-Besides Temp scaling, the main other methods are borrowed from Kull et al, so one could expect some consistency. However ECE results from Table 6 look very different from Table 3 in Kull et al. I assume these are different types of ECE: confidence vs. class-wise? In Table 1, KS result for the ODIR methods of Kull et Al are much worse than Temp scaling, in particular for CIFAR-100 and Imagenet. This contradicts results reported in Table 2 (this paper) and Table 3 (Kull et al) and should be explained.-I am no expert in image classification,  but the 70% accuracy reported for CIFAR-100 seems way below current numbers, which have exceeded 80% since 2017, in particular for the proposed architectures (for instance Wide Resnet or DenseNet) https://benchmarks.ai/cifar-100.  However these numbers seem to be consistent with what is reported by Kull et al (Table 18 in https://arxiv.org/pdf/1910.12656.pdf), so I assume the issue comes from borrowing their architecture and scores. While this should not impact comparative results, it would have been more satisfactory to use baseline architectures that match the state-of-the-art.Additional experiments or discussions on the following would greatly help:-As the spline method is not always better than Temp scaling, how do they compare from a computational viewpoint? How long does the binary search over thousands of calibration examples take compared to the DNN feed-forward?-From Tables 1,2 and 6, KS error and ECE rank methods quite differently. One reason one should trust KS more is that it does not depend on binning choices, and rely on a time-proven test. But could one come up with an experiment that shows that KS is provably more reliable?  The authors present a binning-free calibration measure from a Kolmogorov-Smirnov-based test. Besides, the cumulative probability distribution is estimated using a spline-based fitting from percentiles. The approach allows correcting the probability estimation from trained deep learning models. The paper is clear and well-founded.  Summary:The paper proposes a new method for building graph convolutional neural networks. It shows, that during the building of the network, instead of stacking many layers and adding the residual connection between them, one could employ a randomly-wired architecture, that can be a more effective way to increase the capacity of the network and thus it could obtain richer representations. The proposed method is an interesting direction in the field of graph convolutional neural networks. The new method could be seen asa generalization of the residual networks and the jumping knowledge networks.=============================================================================Pros:1. The paper proposes a novel, randomly-wired architecture for building the graph convolutional neural networks. Moreover authors analyze proposed randomly-wired architectures and show that they are generalizations of ResNets.2. The authors provide the theorethical analysis of the radius of te receptive field of GCN. They show that by using randomly-wired network, together with trainable weights on the architecture edges and sequential path, the network could tune the desired size of the receptive fields to be merged to achieve an optimal configuration for the problem.3. The authors propose the MonteCarlo DropPath regularization - a novel regularization method for randomly-wired architectures, that is related to dropout, however is carried out at a higher level of abstraction.4. The authors provide a comprehensive experimental results of the proposed method - they compare various GCN architectures, created on traditional and randomly-wired way, on three representative tasks - graph regression, graph classification and node classification. Moreover they show, that randomly-wired GCNs gets better results than ResNet GCNs on almost all tested cases. Moreover the authors shows that deeper randomly-wired GCNs always provide bigger gains with respect to their shallow counterpart than ResNet GCNs.=============================================================================Cons:1. Figure 1 is not clear to me. I am not sure what the colored point cloud is about. The authors should consider rewriting a description of this figure.2. In the final version of the paper, the ablation study should be reported on all datasets (however the authors remark that, they do not report results on this version of paper due to space constraints).3. I would like to see the more extensive analysis of DropPath, e.g what are the scores for different levels of the drop probability.=============================================================================Questions during rebuttal period:1. Why the authors use different types of GCNs during the ablation study?==========================================================================================================================================================Reasons for score:  Overall, I vote for accepting this paper. The idea proposed by the authors is novel and confirmed theoretically and experimentally.My major concern is about ablation study and the clarity of one figure. Hopefully the authors can address my concern in the rebuttal period.  - Pros.   -  Learning the representation is crucial to model an efficient linear MAB. However, there is lot of room to include the characteristic of the learned representation into theoretical guarantee. This paper throws light on this interesting problem.   - The paper is well-written and the bounds look convincing. I have not gone through the detailed derivations (in the appendix), but the overall idea looks good.- Cons.   -  It would have been better if the paper could throw some light on other variants of representation learning.   -  Linear bandit is quite popular news/ad recommendation systems. However, posing hand-writing recognition on MNIST data as linear bandit seems to be unnatural. There are DNN based approaches that solve the problem with a great accuracy. It will be interesting to see how does the algorithm perform on a real data set of news/ad recommendation.   -  Assumption 2 is a quite strong assumption to make   I believe the exiting work: A Contextual-Bandit Approach to Personalized News Article Recommendation by Li et al (2010) deserves a citation in this paper. This paper presents a question answering dataset that needs up-to reasoning over three distinct modalities  text, wikipedia tables, and images of wikipedia entities. The dataset generation step consists of starting with a wiki table, finding entities in the table, followed by finding images associated with the entities and text from existing reading comprehension datasets such as Natural Questions, HotpotQA etc. Next, they generate single-modality question that can be answered from each mode. Additionally the paper also introduces a grammar for generating, compositional questions from the single-mode questions that needs reasoning across modalities, which I believe would be widely useful for creating future datasets. The grammar lets them scale fairly easily. Lastly, the questions generated from the grammar are then paraphrased by annotators. Incentives were given to workers to produce diverse questions (such as bonus if the second-paraphrase of a question was not answered by a baseline model). The dataset comes in both an open-setting as well as closed setting, in which distractors are chosen carefully (e.g. distractor images for other entities in the table or using a state-of-the-art retriever to get paragraphs).The dataset is tested on a model that uses a pretrained model for each modality. A classifier is used to predict the type of question (which is easy to do) and then each model is applied following the grammar. Following cautionary related work which show that models often take advantage of unwanted bias in the data, they also have a context-only and question-only baseline. They also did a manual analysis revealing that 86% of the questions actually need strong multi-hop reasoning. There is significant gap as expected between human performance and the correct modelStrengths1. I think this dataset would be a useful test-bed for multi-modal models and several models will be build around it.2. The grammar used for generating compositional question will also be helpful for building future datasets, so I think that is also an important contribution 3. It looks like the authors have taken sufficient caution to weed out unwanted biases from the dataset4. The paper is very clearly written. The analysis were also helpful.Weakness:1. As rightfully acknowledged in the paper, the distribution of questions is quite different from the question human (currently) ask to a system. However, ability to reason over multiple modalities is important and this dataset is important wrt that goal. This submission presents a rigorous analysis of a subset of ways in which machine learning models can fail when encountering out-of-distribution (OOD) samples (often referred to as train/test skew or as train/scoring skew in industry). As the paper notes, the topic has received a great deal of attention, particularly under other guises ("domain adaptation"). However, much of that attention has aimed at pragmatic or heuristic solutions (various tricks to design or learn "invariant" features), while our fundamental understanding of what goes wrong in OOD situations remains incomplete. This paper aims to fill those gaps in understanding by studying simplified settings, and asking the question: why does a statistical model learn to use features susceptible to shift ("spurious" features) when the task can be solved using only safe ("invariant") features. After formulating five constraints (guaranteed to hold true for easy-to-learn tasks), they go on to show that failures come in two flavors: geometric skew and statistical skew. They analyze and explain each in turn, while also providing illustrative empirical results.I like this work a lot (though I am more lukewarm on the paper itself, see below), and barring discovery of a fatal flaw during the discussion, I would advocate with some enthusiasm for its inclusion in the conference. The paper's claims are stated at the bottom of page 2 as:1. Careful design and articulation of "easy-to-learn" settings in which there are few, if any, unmeasured variables that could confound the findings (a weakness in previous work on this topic).2. Identification of two (but not the only two) distinct types of OOD failures that occur even in easy-to-learn settings, in the form of necessary and sufficient data "skews."3. Experimental evidence to illustrate and support the analyses from (2.), along with enlightening discussion.I agree with the paper's claims, though I admit that I was not previously familiar with, e.g., Sagawa 2019 or Tsipras 2019, and so cannot confidently situate this work amongst related research. I also feel my understanding may still be somewhat superficial -- I buy its arguments but don't have a particularly strong intuition yet for the two flavors of skew (particularly in non-toy settings).This work has a very strong scientific flavor (not always true of machine learning research): I would liken the restriction to carefully designed "easy-to-learn" settings to a well-designed laboratory experiment in which there are few, if any, unmeasured variables that could confound the findings. It is very elegant and satisfying to read and think about. I would anticipate that this paper will inspire a lot of follow-up work, in which other researchers adopt the "easy-to-learn" and "skew" framework and terminology and even utilize the specific experimental designs in this paper. After all, machine learning researchers love adopting intellectual frameworks and benchmarks that they can build upon rapidly.The "easy-to-learn" constraints articulated in Section 3.1 are sensible and clearly stated, and I am unable to find fault in them thus far. I agree with this statement on page 5: "any algorithm for solving OoD generalization should at the least hope to solve these easy-to-learn tasks well."The experiments were thoughtful and well-designed, and their results are presented effectively: each plot, it seems, illustrates a particular point or supports a specific argument in the paper. For example, I like how Figures 2 and 3 serve as visual summaries of the geometric and statistical skew sections, respectively. A reader (particularly a savvy one familiar with the relevant related work) could probably skip Sections 4 and 5 (three pages total!) and still get the high level idea simply by skimming the plots and reading the captions of those two figures.The largest weakness I perceive concerns the clarity and accessibility of the writing: for example, the connection drawn in Section 4 to the work on norms in over-parameterized neural nets is very interesting, but I'm not sure the text fully succeeds in further connecting it to OOD settings. In particular, certain details of the ongoing discussion of majority and minority groups aren't entirely clear (to me, at least)...are minority group samples available during training, just in smaller number? In that case, what is the OOD "shift" -- the prevalence of the minority group at test time?Likewise, I'm not sure I really connected with the takeaway in Section 5 -- is it that early in the optimization, the "spurious" weights get updated repeatedly by an amount proportional to the spurious correlation, and that it then takes a long time to undo these updates, if at all? The statistical skew section is definitely more abstract and perhaps a little harder to connect to practical settings, vs. geometric skew where the bridge is the previous work on "norms."My recommendation is to accept this submission, and at the moment, I am willing to advocate for it. However, it is entirely possible I am missing (or misunderstanding) key details, and so I am eager to discuss with the other reviewers. The paper analyses the generalization properties of deep neural networks for classification tasks. The authors focus on the Neural Tangent Random Features (NTRF) class of functions to investigate these generalization properties.In particular, the authors study the convergence of gradient descent (and its stochastic version). They establish that the training loss decreases until a certain level of the order of the best they can expect in the NTRF class. Then, the authors establish interesting generalization results with respect to the 0-1 loss. This problem has been tackled recently in many papers. However, to establish similar results as those proved in this paper, the previous papers required a number of neurons per hidden layer to be polynomial with respect to the number of samples n. The authors shows here that a logarithmic (to a certain power) is actually sufficient when working with the NRTF class. Such results have been obtained recently in Ji and Telgarski (2020) but only for shallow network. In this paper, the results are generalized for deep neural networks. The paper is clearly written and the comparison with the literature is complete and well-organized. The authors have worked hard to present, in a concise way and clearly their results. I appreciate it. The paper is fluid and nice to read. Here are some remarks that might improve the presentation of the paper:- It would be nice to recall the function $\sigma$ in Section 2- In my opinion the remark: " Moreover, while Ji and Telgarsky (2020)essentially required all training data to be separable by a function in the NTRF function class with aconstant margin, our result does not require such data separation assumptions, and allows the NTRFfunction class to misclassify a small proportion of the training data points",is a big advantage of your analysis. Probably, you should spotlight it more clearly.- Page 12 in the proof of Theorem 3.4. In the last inequality of the page, could you add that it holds because $-\ell'(x) \geq \ell(x)$ for all $x \in \mathbb R^d$. - Proposition 4.2: you use two notations for the proportion of data not satisfying the margin assumption. You should replace all the $\sigma$ by $\rho$. - Page 18: "where the equation follows from the fact that" --> $W^{0}$ should be $W^{t}$- Before Equation (C.7). Could you add parenthesis around the product with  respect to $l$. Otherwise it is not very clear. - Could you explain a bit more Assumption 4.1. Is it a strong assumption ? Do you have any insight wether it is verified in practice ? To summarize, I enjoyed to review this paper. The results are clear and interesting. The paper deserves to be accepted for publication. Since the publication of P-GNN (You et al., 2019), it has become clear to the graph ML community that node positional information can be effectively leveraged for link prediction and pairwise node classification tasks.This paper introduces SMP, a novel stochastic message passing approach that preserves both permutation-equivariance (common to GNN models) and node proximities. Extensive experimental results show that SMP not only achieves competitive performance on many common graph ML datasets, but also succeeds to combine together the expressiveness of a standard GNN with P-GNN  (without incurring the scalability problem of P-GNN).I thoroughly enjoyed reading this paper, both for the insights and the technical soundness. I have very few remarks about the paper, as I believe that i) SMP is an ingenious idea, ii) the experimental setting is adequate, iii) the quality of the writeup is high, iv) and the results appear to be reproducible.If I had to nitpick, I'd say that part of Table 7 (currently in the Appendix) belongs to the main paper, as I was convinced about the superior runtime performance of SMP only after reading those numbers. If the avg running time for an SMP epoch was significantly larger than the one for GAT (for instance), I would have considered SMP yet another specialized model.Instead, given that the GPU consumption (both in terms of computation and memory) is similar to any other GNN model, I believe SMP could be adopted as a more flexible graph ML method (which would avoid having to choose a method given the target task, e.g., node classification vs. link prediction).One remark about L278: SMP cannot be considered anymore SotA for ogbl-ppa. The current SotA is more than 10 points above the performance achieved by SMP. Summary:This paper addresses the problem of distributional shift in transfer learning from multiple training domains. The authors propose Risk Extrapolation (REx), which is a novel approach for out-of-distribution generalization when the new test domain for which we do not even have the covariate matrix. Thorough empirical experiments show that REx significantly outperforms state-of-the-art.Pros:- This is a highly quality paper with strong theoretical and empirical results. - The paper is clearly written and easy to understand.- Based on the thorough literature review, this idea of this work is original.- The results of this work are highly significant and of interest to the domain adaptation and transfer learning community.Cons:- Although I understand the page limit, most of the major parts of the paper (especially the theoretical aspects) can be found in the appendix. As a person who is not very familiar with the literature on distributional shift from multiple domains, I did appreciate having this thorough overview; however, the detailed discussions of the contributions of the paper might be overlooked if (when) located in the appendix. Specifically, there is  only half a page on introducing the proposed methods for REx  in Sec. 3.1.Minor comment(s):- Reference Peter Bühlmann. Invariance, causality and robustness, 2018a. is duplicated. The work describes an extreme classification strategy which leverages the computational efficiency of multiclass (and multilabel) efficiency on small label sets (circa 10K) composed with the near-orthogonality of random sparse embeddings; while exploiting inherent parallelism of repeated independent instantiations of this primitive technique to mitigate statistical issues.  The use of fixed near-orthogonal label embeddings is elegantly motivated, the computational properties of the technique are favorable (especially, amenability to inverted indexing), and the statistical performance is competitive.Inference only gets passing treatment in the entire exposition, without any supporting rationale.   For instance, this reviewer was surprised that inference involves summing over predicted probabilities, rather than summing logs of predicted probabilities.  Essentially there are a collection of K independent predictors that we are trying to ensemble, a problem that has received lots of attention in the literature.  Making these connections would both help the reader and also potentially improve the technique. ### SummaryThe generalization ability of networks with zero training error has been heavily studied.  This paper extends beyond generalization to test sets to study the network's robustness to adversarial examples.  The paper provides two theoretical contributions demonstrating that a very low training error can indicate poor robustness under reasonable conditions.  They illustrate this with experiments using label noise, demonstrating that adversarially robust networks spurn overfitting on incorrectly labelled data.  They additionally experimentally demonstrate that unusual training examples, even if correctly labelled, are unlikely to be correctly predicted by adversarially robust networks.### SignificanceThe generalization properties of neural networks and adversarial robustness are two very fast-moving areas of machine learning.  This paper does a nice job revealing some properties of overfit networks.  These properties are intuitive (at least, I would have assumed them), but I have not seen them so nicely laid out, and it is important to not have to assume.  It does a great job of filling in these holes with evidence, and so I find it quite significant.### OriginalityTo my knowledge, the work is original.### QualityThe experiments are quite well designed and performed.  I find the second theoretical contribution too quickly discussed, and the "unusual examples" experiment insufficiently emphasized, but otherwise it is quite a good paper.  Graphs and figures are meaningful and well explained.  Theoretical results nicely support portions of the paper that might otherwise be criticized as anecdotal.### ClarityVery clearly written. This work analyses the interaction between data-augmentation strategies such as MixUp and model ensembles with regards to calibration performance. The authors note how strategies such as mixup and label smoothing, which reduce a single model's over-confidence, lead to degradation in calibration performance when such models are combined as an ensemble. Specifically, all techniques, taken individually, improve calibration by reducing overconfidence. However, in combination they lead to under-confident models and, therefore, worse calibration. Based on this analysis, the author's provide a simple technique which yields SOTA calibration performance on CIFAR-10, CIFAR-10-C, CIFAR-100 and CIFAR-100-C and ImageNet. The authors propose to dynamically enable and disable MixUp based on whether the model is over/under confident on a particular class, as judged on a validation dataset. I think this work provides useful insight and a simple and effective solution. Additionally, it is clearly written and very easy and pleasant to read.The authors may find this concurrent work on ensemble calibration to be relevant: https://openreview.net/forum?id=wTWLfuDkvKpThe only question is have is why the authors think that models are overconfident on hard examples/classes and properly calibrated on easy ones. My intuition would be the opposite - that models are overconfident in areas where there are too few training examples and/or areas where there is no data uncertainty. If there is no data uncertainty then overconfidence would not be a problem. So mainly the issue is due to data sparsity in areas of non-zero data uncertainty. Would be good to expand the discussion.  Summary:The paper proposes variational invariant learning (VIL) as a framework for probabilistic inference that jointly models domain invariance and uncertainty. The approach exploits variational Bayesian approximation in both feature encoding and classifier layers in order to facilitate domain generalization and invariance. The paper evaluates VIL on benchmarks for cross-domain visual object classification.Positives:The paper presents their methodology and definitions very clearly. Comparisons to state-of-the-art methods are well presented. The ablation study is very clear to provide a thorough understanding of the approach. Overall I like this paper.Concerns:The authors state: "the Bayesian approach has not yet been explored in domain generalization" or "this is the first work to adopt variational Bayes to domain generalization". I may not strongly agree on these statements in their current form. There has been some previous work on extending variational autoencoding frameworks for domain generalization from several perspectives (e.g, adversarial inference). This type of variational inference based models ideally exploit a Bayesian approach for domain generalization as well. I would expect the authors to rephrase their claims in a more specific manner to make this distinction. Regarding this concern, one example to consider for instance is "The Variational Fair Autoencoder" (Louizos et al., ICLR 2016) approach for invariant representation learning with variational autoencoders for domain generalization. Summary and Contributions: Inspired by the mixture of experts, authors propose an image clustering algorithm using a mixture of contrastive experts where, each of the conditional models is an expert in discriminating a subset of instances based on contrastive learning.  To this end they  use a gating function to partition an unlabeled dataset into subsets according to the latent semantics and discriminative distinct, where the gating function performs a soft partitioning of the datasetbased on the cosine similarity between the image embeddings and the gating prototypes. The authors carry out experiments on four widely adopted natural image datasets to evaluate the performance of the method in these tasks and compare it to competing methods and baselines.Correctness and Clarity: The paper is well-written, with informative figures and tables. The paper presents the idea in a clear and straight-forward manner, and is solidly built on top of the current literature. Authors convincingly tested the method with multiple SOTA and baseline and the results look correct to me.  Reproducibility: The details of the experiments, implementation, and the public datasets are included in the paper. Thanks also for sharing the code.Additional Feedback and Suggestions:  Since the goal of the paper is image clustering, providing some visual results is appreciated. Also, I am curious to see the performance of the method when we have large number of clusters in our dataset e.g. ImageNet. Decision: The idea of using  a scalable variant of the Expectation-Maximization (EM) algorithm to help with the nontrivial inference and learning problems caused by the latent variables seems interesting to me.  And overall, the technical novelty together with the fine evaluation are good enough for ICLR, in my opinion. This work introduces a new algorithm FLoP for theorem proving using reinforcement learning, and tests it on a new evaluation dataset.  FLoP gives direction to a tableau based theorem prover by learning a state machine using curriculum learning applied to a prototype proof.  The authors state that this RL technique has not been previously applied to theorem proving.  I cannot judge this statement but if true it would seem enough novelty to justify publication.   They find that the technique works best on highly structured problems such as proving simple arithmetic statements in unary or binary arithmetic, and less well for problems which benefit from searching through databases of heterogeneous statements.One could debate whether this deserves the name of "reasoning by analogy".   I suspect it should be called "reasoning by imitation".   To my mind, the term analogy suggests a reasoning process in which some features are extracted from the proof, and then the proof strategies which work for these feature values are selected out of a large set of possibilities including many with different feature values.  I quote from the authors' description at the beginning of section 6 of what they show: "In this highly structured dataset FLoP is capable of extracting a general proof pattern from one or two proofs and successfully generalizing to related proofs of arbitrary length."  This does not sound like analogy as I defined it, rather it sounds like imitating the prototype. Still, since the comparison with other techniques is encouraging, and since the paper is clearly written and gives a very extensive survey of comparable works, I found it enlightening and would recommend to accept it. The main contribution is a new attention module called deformable attention module. Like deformable convolution, it adds a translation term into the expression of the transformer, allowing a sparse spatial sampling. The resulting model is very interesting in terms of convergence and complexity compared to the original DETR. A Multi-scale deformable attention module is also proposed. it needs to add a scale function in the attention module equation. Experiments shows that it increases the AP detection rate on MSCOCO compared to FasterR-CNN and DETR. Contributions are clearly state and validated. The complexity study is very interesting and shows the interest of deformable attention module.Figure 1 presents a general view of the model. Since the deformable attention module is the core of the contribution, it should be interesting to add a figure dedicated to this component. Combined with eq.2, it will give a better understanding of the method. It seems that the Axial-DeepLab paper presented in ECCV-2020 misses in the references. This paper proposes a simple strategy for attention modules that also reduce complexity. Results clearly show that deformable DETR provides better AP than DETR for less training-epochs. Moreover, the convergence is better than for Faster R-CNN (FPN). One of the concerns with deformable convolution is that the computation speed is slower than classical convolution. The same drawback appears with deformable attention modules. Fps decrease from 26 to 19 compared to DETR. It should be interesting to also report fps in the state of the art comparison table 3.  Summary:This paper proposes Deformable DETR with multi-scale deformable attention modules to solve the problems of DETR: slow convergence and limited feature spatial resolution.  In particular, it has faster convergence and achieves better performance(especially on small objects) than DETR.Reasons for score:Overall, I vote for accepting. I like this paper because it solves the main problems suffered by DETR. My major concern is about the clarity of the paper and some additional ablation studies (see cons below). Hopefully the authors can address my concern in the rebuttal period.Pros:1.This paper solves the main problems suffered by DETR: slow convergence and limited feature spatial resolution. In my opinion, it makes DETR more practical.2.The proposed Deformable DETR can obtain multi-scale features without a huge cost. In this way, it can be optimized easily and detect objects precisely, especially small objects.3.This paper also introduces some improvements and variants to boost the performance of Deformable DETR.Cons:1.In the experiments, focal loss is used for bounding box classification. What is the reason for this choice? In addition, the number of object queries is increased from 100 to 300. Why? In the test, how to choose 100 objects from 300 objects? 2.From Table 1, we can find DETR (500 epochs) has better performance than Deformable DETR on large objects (61.1 vs. 58.0), though the overall performance of Deformable DETR is better. Why?3.In the Table 1, there is only DETR-DC5+ (50 epochs). Could you provide DETR-DC5+ (500 epochs) ?Questions during rebuttal period:Please address and clarify the cons above Comment: Summary: This paper presents a technique for more expressive neural ordinary differential equations (NODE) flows. Instead of learning a fixed set of parameters $\theta$ that governs the ODE dynamic, the proposed approach learns dynamic parameters that evolve over time. The authors propose two variants of the methods: $\textit{open-loop}$ and $\textit{closed-loop}$. The former only maps the initial observation as the controller whereas $\theta$ in the later model follows another NODE $g$. Model performance is demonstrated on several tasks. The model is shown to solve the well-known "crossing curves" problem, on which NODE fails. Also, it's demonstrated that the presented technique yields sharper images compared to VAEs and also better at classification and interpolation. Overall Score: I recommend an accept. - First of all, the presented N-CODE method solves one of the most crucial limitations of NODEs in a principled manner. The model is also shown to outperform the vanilla Augmented NODE method.- The results are very impressive. Both the tables and generated images/interpolations are of high-quality, especially given VAEs don't excel at this task.- The method has certain overlaps with the control theory and maximum principle, deep generative models, and neural nets with adaptive weights. So I believe such connections would open new research avenues.Cons: I would be happy if the below are addressed:- Did you investigate how the model performance changes as f grows? I speculate that learning the parameters of a neural network via another neural network(s) is a very challenging problem, and would like to see this is verified or not. Also, I cannot see the architecture of f in your experiments (looking at Table A.1).- Connection with control theory can be made clear. As such, there is very little reference to Pontryagin's maximum principle and the link is not visible (at least to me).- Did you test vanilla NODE on experiments 5.3 and 5.4? The virtue of N-CODE is obvious on the toy problem (as expected) and somewhat significant on the classification task. I'm wondering when NODE is latent (as in 5.3-5.4), is the improvement significant?Additional comment: Is Figure-4 caption correct? I think this a very good contribution to ICLR given the topic and the quality of the submission (originality, contribution to the stare of the art, experimental evidence, etc) although the study might need to be supported in a more theoretical framework to make it worth of an oral presentation (I would recommend a poster or short presentation) Some of the strong points of the submission are summarized as follows:1.Studies in the interpretability of the results of deep learning models is a very important aspect, as well as the robustness of the obtained models in a variety of circumstances and under adversarial attacks. 2.A sufficient introduction and motivations sections, but I would suggest introducing the state of the art at the beginning of the paper as it would help to get a better grasp of how the works builds upon previous work.3.The state of the art (despite the previous comment) contextualizes the subject matter in a succinct but comprehensive manner. Although there are certain aspects that could be improved, such as including a table outlining in a clearer manner the contributions of the authors in this context.4.The experimental design is good, showing a careful analysis to validate the proposal and several ablation studies to assess the validity of the authors' hypotheses5.The foundations for the method are presented in great detail in a formalized manner and provides sufficient elements (i.e. examples) to assess the validity of the proposed approach.However, there are certain things that in my opinion could be improved:1.The authors make a very interesting contribution that leverages knowledge from several research areas and thus, sometimes the contributions with regards to the state of the art are difficult to follow. I would suggest making a table summarizing the main features of some previous works so the readers can better grasp the limitations of those previous works and understand better the improvements in each of the areas outlined in this research2.The organization of the paper is confusing, some effort should be given at creating a clearer layout that makes the paper easier to read and follow the flow of ideas.3.Future work could be further elaborated and discussion in specific domains (medical imaging, for instance) could be further discussed.4.The abstract mentions that the proposed work can be used as a blueprint for assessing the out of distribution performance of deep learning models, but this aspect is not sufficiently explored in my opinion. The paper describes how to learn cost benefit tradeoffs associated with the experts actions. The proposed approach integrates counterfactual reasoning into batch inverse reinforcement learning and offers a sensble framework for defining reward fuctions and tentatively explain how domain expert think and act. The framework is developed for those cases where active interaction with the system under study, i.e., experimentation, is not possible, which is very often the case in healtcare.The paper estimates the effects of different decisions by exploiting the concept of counterfactual to accommodate settings where the policy applied by the expert depends on histories of observations rather than just current states of the system under consideration.----------------------------------------------------------------------------------------------------------------------Reason for Score:Overall, I vote for accepting. I like the methodological framework, it is well structured and convincing, even if the basic assumprtions required to make the proposed approach working are strict. Indeed, in many situations it is not clear whether these assumptions are satisfied or not. Furthermore, assumptions can not be tested. The problem tackled is extremely challenging from the theorectical point of view and to the best of my knowledge this is the first paper which tries to explain sequential decisions through counterfactual reasoning and to tackle the batch IRL problem in partially-observable environments.----------------------------------------------------------------------------------------------------------------------Pros.1) tackles a very relevant problem, both theoretically and practically.2) well structured and written.3) methods sound and convincing.3) numerial experiments are well designed and results seem to confirm the effectiveness of the proposed appproach.----------------------------------------------------------------------------------------------------------------------Cons.1) at page 3 when introducing the value function of a policy, the choice of V is not that clever because early in the paper you let V to be the volume of tumor.2) please provide quantitative description of how accuracy is computed in numerical experiments3) the explanation components of the proposed approach should be further developed, I see potential there but at the current stage it limits to the weights and not for example tries to investigate whether these weights change during time4) I would like to see how the proposed approach scales with the number of covariates of patients.----------------------------------------------------------------------------------------------------------------------Questions during rebuttal period: Please address and clarify the cons above ----------------------------------------------------------------------------------------------------------------------I found no typos.---------------------------------------------------------------------------------------------------------------------- This paper proposes a way of speeding up non-local aggregation on graph convolutional neural networks based on sorting the nodes into an ordering, and performing a 1-D convolution on this resulting ordering. This algorithm has the advantage of being asymptotically faster than other non-local aggregation schemes, and the paper demonstrates that empirically it can do at least as well as some of the other methods.As someone only vaguely familiar with GCNs, but reasonably familiar with graph algorithms, the effectiveness of such global sorting schemes based on a single score is rather surprising to me. I also find the experimental results convincing, both in the prediction performances, and in efficiency. The paper is also well written, so I'd like to recommend its acceptance. The paper establishes a separation result between a convolutional network (CNN) and the fully connected network (FC).Specifically, the authors show that a polynomial-size of CNN trained by gradient descent with a polynomial number of iterations can learn functions that depend only on a small pattern of $k$-consecutive bits of the input,where $k = O(\log n)$ and $n$ is the input dimension. On the other hand, they show that for FC, gradient descent fails to learn a $k=O(\log n)$-parity function unless the network size is $\Omega( n^{\log n})$. Hence, the separation result in the computational aspect is proven. An experiment is conducted to support the theoretical results.I believe this paper makes a significant contribution. The writing is good and the proof is short and concise. The authors first discuss other possible explanations for the observation that CNN performs better than FC in practice and argue that parameter efficiency and weight sharing might not be the main reason for the superiority of CNN. Then, in the rest of the paper, the authors show that the key is that CNN is able to exploit the locality in the data.Q: It appears that in the analysis of the positive result regarding CNN, the training is in the neural tangent kernel regime, as you have to bound the deviation from the initial point and that you show a randomly initialized CNN already works well for the $k$-pattern problem. Does it suggest that the kernel method can learn the $k$-parity function? What is the catch here? I saw the sparsity here but I am hoping the authors can explain the intuition behind their results more. It is well-known that neural networks (NN) perform very well in various areas and in particular if one looks at computer vision convolutional neural networks perform very well. Although convolutional neural networks (CNN) are limited in their architecture (since they only allow nearest-neighbour connections) compared to fully-connected NNs (FCNN), their superiority in performance is unclear. In this paper they answer the following fundamental question: can one formally show that CNNs are better than FCNNs for a specific learning task? In this direction they answer in the affirmative.  In particular, more than just giving an example, they show that an interesting property called locality, instead of other parameters like parameter and efficiency weight sharing is the reason for its superior performance. In order to exhibit their separation, they consider the class which they denote k-patterns which is is simply a k-junta, i.e., f is a function on n-bits but when defining f it only depends on an arbitrary function of the k out of the n inputs bits, which are fixed when defining f, but unknown to a learner. In this paper, the authors consider such k-patterns which are very local simply by definition and show that for these class of functions, training a CNN with  gradient descent can learn these functions in polynomial time (for k = O(log n)) however gradient descent fails to learn these functions when training a FCNN. The proof of the lower bound for FCNN uses the well-known statistical query framework which shows that using gradient descent to learn k-parities requires time n^k (and in their application n^ log n). In order to prove the upper bound for CNN, they combine various techniques that were introduced in many recent works (in a non-trivial way).Overall, I think this paper resolves an important and interesting question. Given that most of machine learning is well understood in the non-rigorous setting, giving a rigorous separation between CNN and FCNN is an important problem and they have resolved it. If I had to be nit-picky, I would say the concept class they give this separation for is well-suited for their separation and doesn't really say much more than that, but I think it's already interesting that one can show such a separation theoretically. ##########################################################################Summary:The paper proposes a novel learning framework for robust (against adversarial attacks) fine-tuning of pre-trained language models, that is based on information theoretic arguments. It introduces two regularization mechanisms and investigates their efficacy on various tasks.##########################################################################Reasons for score: Overall I vote for accept. The approach is novel, interesting and well presented. The theoretical results seem to be sound. It also seems to outperform competitors in the field of adversarial language models. Concerning the experiments some questions remain, but I hope that the authors will address them in the rebuttal.##########################################################################Pros: 1. The idea is interesting and well formulated. The theoretical results seem to be correct to me.2. The approach is tested on several standard datasets used in adversarial language models. It seems to outperform previous approaches.3. The paper is well written and clearly structured##########################################################################Cons: 1. In my view the experiments seem to show a tendency towards a slightly worse performance on the more difficult tasks in comparison to the competitor methods. Thus, the better overall performance on the ANLI data could be driven by the easier tasks.  2. I couldn't find a clear description of the "global representation" Z. A more explicit description would be helpful.##########################################################################Questions during rebuttal period: Please address and clarify the cons above #########################################################################Minor comments: 1. Is definition 3.1. a standard definition or is it introduced by the authors? 2. Page 4, definition 3 contains an incomplete sentence ("The q(x')....").3. Page 6, "Evaluation Metrics": it should be stated witch argument is maximized.4. Page 16, Lemma A1: in the Proof of the lemma I think that all instances of Y_i should be replaced with T_i. In formula (13), in the rightmost term the token index n should be i-1.5. Page 17, formula (33): H(Y|Y) should probably be H(T|Y). Same goes for equ. (36).6. Squares might be missing in formulas (37) to (43).7. A reference to formula (44) would be nice.   I think this a very good contribution to ICLR given the topic and the quality of the submission (originality, contribution to the stare of the art, experimental evidence, etc)  Some of the strong points of the submission are summarized as follows:1.The paper tackles a very interesting subject, questioning the conventional wisdom the CE is superior to MSE loss in a wide range of machine learning problems. 2.Very good introduction and motivations sections. The hypothesis, as well as the main motivations are discussed succinctly but in a very logical manner including historical aspects leading to the current state of affairs (in terms of the manner in which models are trained) that might be helpful for interested readers not sufficiently familiar with the aspects discussed in the article.3.The state of the art (despite the previous comment) contextualizes the subject matter in a succinct but comprehensive manner. 4.I have read people making similar claims in other forums and articles, but the authors here provide a very thorough and careful experimental design, which helps to validate their hypothesis. However, it would be interesting to see how these experiments generalize to problems (in particular in computer vision) where datasets are noisier or where the image quality/resolution are lower.5.The experimental design is good, showing a careful analysis to validate the proposal and several ablation studies to confirm that the hypothesis holds for various machine learning domains, as well as several datasets.6.The foundations for the method are presented in great detail in a formalized manner and provides sufficient to assess the validity of the proposed experimental design.However, there are certain things that in my opinion could be improved:1.Future work could be further elaborated and discussion in specific domains (medical imaging, for instance) could be further discussed. *********Summary Of The Manuscript:*********The manuscript addresses the problem of Video Recognition one of the applications of Computer Vision. Due to various complex temporal dynamics of video data (Camera Motion, Speed, etc.), to capture the vast information, the author presents a novel Temporal Adaptive Module (TAM) for generating kernels based on the temporal feature maps. In addition, these feature maps are a combination of local and global features and as an exemplar, the author presents an architecture - TANet by incorporating their temporal operator. Together with a variety of experiments on standard benchmark for Video Recognition: Kinetics - 400 and Something-Something, the author showcases that for the task of Video Recognition compared to existing temporal operators, TAM's performance is fairly consistent and better and archives State-of-the-art with similar complexity in their exemplar architecture - TANet. *********Strength Of The Manuscript:*********++ Novelty- The task formulation is concise, convincing, and novel. A seemingly reasonable approach has been proposed in this manuscript for the task of Video Recognition. Compared to the existing baseline and recent approaches, the proposed architecture - TANet achieves SOTA results. - To the best of my knowledge, the incorporation of two branches - Local and Global branch makes the whole operator efficient and flexible for adaptation in the frameworks by stacking them to capture more complex information. Thus the concept of the TAM is convincing to capture complex temporal information.- In addition, the exemplar showcased by the authors - TANet has been created by incorporating TAM in the existing 2-Dimensional CNNs to capture vast information which proves that the proposed module/operator is flexible enough and can be adapted to different frameworks/architectures for better performance. ++ Clarity- The manuscript is written in an excellent way to provide a brief insight into TAM. Especially Subsection 3.2 and 3.2 provide a good in-depth description of how the local and global branch works effectively in a joint manner to capture the short and long-term complex temporal information. - The manuscript also clearly describes the improvements and adequately contextualizes the contributions in such a way that it makes a good starting point for a novice reader. ++ Evaluation- The experiments are sufficient and convincing. This new operator and exemplar TANet shows improved performance in nearly all cases on the datasets - The experimental evaluations demonstrate the effectiveness of the proposed architecture and showcase its practical value.- Also, an ablation analysis demonstrates to gain an understanding of where the performance benefits have been obtained such as receptive fields and parameter choices. *********Weakness Of The Manuscript:*********Overall, currently at this stage, this is a very good and strong manuscript in my entire batch. I like the simplicity and wide applicability of the proposed operator, especially the incorporation of local and global branches and adaptive aggregation. Thus I do not have any major weakness issues after reading the manuscript several times. Detailed literature review, a complete overview of each component, and detailed experiments and ablation studies helps to give a good insight into the manuscript. I found this paper pretty solid and have not able to found concerns relating to the proposed work. *********Justification Of The Review:*********Overall, happy with the current version of the manuscript. As mentioned earlier, I like the simplicity and wide applicability of the proposed module, and the architecture and setup details are provided in such a manner that it is very easy to convert into code in some timeframe. Detailed literature review, a complete overview of each component, and detailed experiments and ablation studies help to understand the author's work. Finally, I think the paper is pretty solid and thus I prefer to give a rating of 8 currently.  This paper identifies the inherent problem of over-squashing that exists in popular information propagation mechanism in GNN. The hidden dimension of a node vector is fixed while the amount of information need to be preserved can grow exponentially (vs linearly in RNN decoder) with the increase of the depth of the networks. This problem becomes critical when modeling long range interaction is required. Then, the paper provides a simple and intuitive solution which is shown to be effective on both synthetic and multiple real-world datasets.Strength:- The over-squashing problem is important and inherent in (most of if not all) existing information propagation mechanisms in GNN. Considering the prevalence of GNN, clearly identifying and analyzing the problem can significantly contribute to the community and potentially open a new direction of research.- The paper provides theoretically analysis on the bottleneck, and further analyzes the relationship of required depth and the hidden dimension, showing the simply increasing the hidden dimension will not solve the problem.- The paper provides a simple yet effective solution to mitigate the problem, supported by extensive empirical results on multiple datasets.Minor comments:- Though the baseline methods are relatively well-known, it is will be still be easier to follow if citations are provided when baseline approaches first appear, e.g., mentioning GGNN (Li et al 2016) and GAT (Velickovic et al 2018) at Page 4. These information is not available until Page 11.- In Section 4.1, in GAT soft-attention is used, thus the model adaptively combines information from adjacent edges, rather than only consider half of them (which may require hard-attention). This paper focuses on adversarial learning. It improves the robustness while keeping the accuracy. To achieve this point, the authors find that adversarial data should have unequal importance, which naturally brings geometry-award instance-reweighted adversarial training (GAIRAT).Pros:1. The paper has strong novelty in philosophy level. The common belief is that robustness and accuracy hurt each other. However, this paper shows that the robustness can be improved while keeping accuracy. As far as I know, this point has never been explored before.2. The paper is well motivated and easy to follow. First, the authors use Figure 1 to illustrate the GAIRAT, which explicitly gives larger weights on the losses of adversarial data. The authors use two toy examples in Figure 3 to explain GAIRAT more. Second, the whole logic of this paper is easy to follow. For example, after explaining motivations of GAIRAT, we can clearly see the objective function of GAIRAT and its realization.3. The paper is sufficiently justified in experiments. For example, PGD-200 has been used to verify the robustness of GAIRAT. From my personal opinion, this result is quite strong. Moreover, the authors upgrade their method by incorporing FAT and verify the robustness of GAIR-FAT.Cons:1.In the top right panel of Figure 10, the SVHN experiments have a period of increasing robustness training error for GAIRAT. Could you explain this? 2.Although authors show that model capacity is not enough in adversarial training, how large the DNN should be enough? What do you think? This paper challenges the common belief of the inherent tradeoff between robustness and accuracy.Instead of recent methods improving accuracy while maintaining robustness, this paper proposes a geometry aware instance reweighed adversarial training (GAIRAT) method to improve robustness while maintaining accuracy. Pros:1 The direction---improving robustness while maintaining accuracy---is novel and interesting. Specifically, several papers are challenging the inherent tradeoff, e.g., using more data [1], utilizing early stopped PGD [2], and incorporating dropout [3]. This paper still challenges the inherent tradeoff. However, different from [2,3] improving accuracy while maintaining robustness, this paper goes the other direction. To my knowledge, this is the first paper to explore this direction. [1] Understanding and Mitigating the Tradeoff Between Robustness and Accuracy, ICML 2020[2] Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020[3] A closer Look at Accuracy vs. Robustness, NeurIPS 20202 This paper has made two conceptual improvements. a) This paper explicitly argues that the overparameterized networks that have enough model capacity in standard training suffer from the insufficiency in adversarial training (though many studies have already shown AT needs the large model). b) This paper argues that under limited model capacity, adversarial data should have unequal importance. Unequal data's treatment was explored in the traditional ML methods several years ago, but it is rare in deep learning at this moment. 3 The proposed GAIRAT method is effective, indeed increasing robustness while retaining accuracy. The experiments are comprehensive over different network structures, datasets and attack methods. The experiments in the appendix provide much useful information. Cons:1.The design of weight assignment function in Section 3.3 seems heuristic. Would you explain some principles on assigning instance dependent weights? 2.In Figure 4, the GAIRAT method can relieve undesirable robust overfitting. Would you explain more about this? For example, why the robust overfitting exists in standard adversarial training? how/why your GAIRAT methods relieve it? This paper proposes a new approach to learn the dynamics of density evolution of objects from aggregated data. The basic idea is to derive a closed-form Wasserstein distance between the empirical data distribution and the predicted distribution generated from the weak form of Fokker Planck Equation (FPE). Based on this measure, an objective function is developed to learn the underlying drift coefficients and the discriminator simultaneously using neural networks. Some theoretical results are provided and numerical experiments are carried out to illustrate the effectiveness of the proposed method.The paper is overall well written and solves an important problem. I have only a few minor questions.1. How prediction is made based on the estimated model? One step ahead or multiple-step ahead prediction? Please provide more details on this issue.2. Some notations are unclear in Theorems 1-1. In Theorem 1, what are the definitions of $N$ and $x^{(k)}$? Do you generate data for N times? In Theorem 2, I  assume that $n$ stands for $n$ steps ahead from t_{m_0}? Summary: The authors propose a novel combination of VAEs and Flow models, where the decoder is modelled through a conditional flow taking as input a local representation of the size of the input image and a global representation output by the encoder. The authors evaluate the proposed method on density estimation, quality of generations and linear probing on a variety of datasets and show improvement over state of the art.Great:* Conceptually simple method that seems to work quite well in practice, for this class of models. * The linear probing experiment is quite convincing in justifying the use of global and local characterizations of the learned representations. So are the interpolations.Could be improved:* Its not clear to what extent each of the proposed refinements to Glow (reorganization, different splits, fine-grained multi-scale architecture) improves Glows performance. The authors propose a novel combination of known methods, evaluate it extensively and show considerable improvements over current state of the art. A clear accept. The idea of learning representations from video rather than single images is an appealing one with many favorable properties to allow a system to get direct signal on appearance of objects under various natural transformations (occlusion, lighting, etc). Combining instance discrimination ideas of loss based on unlabelled images for which it is known whether they are similar or not, with the idea of curating the images from video is hypothesized to yield learned representations that capture properties enabling improved performance across a variety of single image tasks. The authors create a dataset based on video with positive pairs for noise contrastive estimation, conduct fairly comprehensive experiments and promise to make their newly constructed dataset available. The experiments showcase this type of learned representation outperform alternatives not based on videos on a variety of tasks. Quality : this seems like a solid paper offering a good intuitive idea with well supported experimental section to show case its relevance. Clarity : the paper is quite clearly written for the most part. The dataset section 3.1 seems to have an omitted paragraph, please see point 1 below. Section 3.2 is quite lean and does not stand on its own, but relies heavily on previous work omitting much of the essence. I would recommend spending a bit more time on ensuring it is more rigorously written. See for example my comment 2 below. Originality : the paper is modestly original. It combines two existing ideas - that of using discrimination loss for unsupervised learning of image features, and that of using video based data to allow for rich example of the same data that takes into account real world type transformations. Thus, it is hard to claim more than moderate originality. However,  Significance : the improvements over existing baselines are solid, though I would not categorize them as dramatic. Given the originality is also solid, the overall significance is moderate. Comments:1. The dataset generation section is strange, did you omit too much? "We use the following fast and automated procedure to generate the images in our dataset. Using this procedure,..." it almost seems like a few sentences were dropped between the first and second sentences. While the information exists in the appendix, a sentence or two seem to be warranted in the main test. 2. "Gradients flow through the positive pairs" - at this point in the text you have only introduced a loss. The sentence warrants the question of gradients with respect to what? for rigor, and clarity of exposition, this intuition related statement should come after you talk of what is the parameterized aspect of eq (1) wrt which gradients are taken (so after eq 2 is introduced and the idea that the feature representations are captured through learned method, and in particular some reference to the NN you are using. This makes it a bit more complete as a description of the method and presented in a more methodical order. 3. Why would you not remove the option of choosing an anchor and positive as the same image? seems easy to avoid4. how do you know if the video doesn't contain a shift to a different scene, with different content, thus making the positive actually very different? 5. Since the representation you provide is supposed to learn representations that are somehow more informed about natural transformations (occlusion, lighting) - is there an experiment you can conceive of to test this specific hypothesis? i think it would both be interesting, and also give insight on whether this is indeed what is being learned. This might also make this representation useful for other types of tasks that are not looked at in the paper that I would encourage the authors to explore.  Summary:Paper presents a novel encoder-decoder framework for invertible dimensionality reduction. It is composed of multiple stages of homeomorphic embedding, sparse representation, linear compression and an inverse (reconstruction) process to learn invertible non-linear representations. Proposed idea is indeed novel and interesting actualization of geometry preserving dimension reduction shown in Figure 1. Strengths:Proposed methods combines multiple ideas work on structure-preserving manifold learning, invertible and distance-preserving sparse representation learning.Each of the steps above are achieved by NN structure and novel loss functions that impose orthogonality, sparsity and isometry constraints and so on.Empirical results on synthetic and real-world datasets support the approach and shown efficacy of the method. Ablation studies on adding different components show need of each aspect.Weakness: Invertible mapping learned is computed explicitly but can also be learned end-to-end during training. Is there a reason why the prior is preferred. Paper doesn't not address convergence aspect of the training and how it affects empirical results. Sparsity count (s) for representation is still heuristic and choice is not obvious. Can the RIP property provide a lower bound for choosing s.Recommendation:Paper is a clear accept as it introduces a novel method achieve Figure 1 using NN. It is well written and technically sound and qualitative results demonstrate effectiveness of the technique in terms of SOTA results. The paper presents theoretical analysis of MDPs with execution delay together with an algorithm that achieves better performance on the task than the baselines. The main theoretical result highlights the need for non-stationary Markov policies that is different from standard MDPs that can be solved using stationary Markov policies.*Quality*The authors conducted a solid theoretical study of MDPs with the execution delay. The presented claims showcase why the existing approach based on augmenting the state space is not feasible for large delays. The suggested algorithm is a based on a simple idea to estimate the state of MDP m steps in the future, but it seems to work quite well when the MDP is not too stochastic. Overall, this paper is a solid study of the problem with lots of potential research directions for the future work.*Clarity*The paper is well-written in general. *Originality*To my best knowledge, the results are new and the need for non-stationary policies is a novel highlight.*Significance*rather significant, execution delay is a common issue in practice and the paper lays foundations for analysis of MDPs with execution delay.Pros* Theoretical analysis of ED-MDPs that guides the presented algorithm* Great results on Tabular Maze and Physical domain problemsCons* No analysis on how the stochasticity of environment affects the performance of Delayed-Q* Atari results use the simulator to predict the future state This paper proposed a novel method which to quantify the reliability of DNN-driven hypotheses in a statistical hypothesis testing framework. Naive statistical testings are not appropriate for the DNN-driven hypotheses, where the hypotheses are selected by looking at the data(i.e. The selection bias exists). To address this problem, the authors developed a novel homotopy method under the Selective-Inference(SI) framework, which can derive the exact sampling distribution of the DNN-driven hypotheses. In this paper,  the authors mainly focus on DNNs which consist of affine operations, max-operations, and piecewise-linear activation. As described by Lee et al. (2016), the main idea of SI is to make the inference conditional on the selection event. Specifically to the DNN-driven hypotheses, the authors proposed a novel method that consists of two steps, 1) Adding extra conditioning to make the problem traceable. 2) Combining multiple over-conditioning cases by homotopy method to solve the over-conditioning problem. The experimental results on both synthetic and real-world datasets illustrate the proposed method can successfully control the FP error rate.Considering that there are more and more interests in the research on neural representation learning, the problem that this work is trying to solve is pretty important. The SI framework has been well studied on the lasso and other problems. To the best of my knowledge, this is the first work that deploys SI to test DNN representation driven hypotheses. Although the authors only demonstrate it on simple DNNs, I can see the potentials of this method to apply on more practical and complex DNNs. The community may benefit from it on understanding neural representations.Pros:- The problem that this paper is trying to solve, is clearly defined and is essential in understanding the representations DNN learned.- This proposed SI algorithm based on the homotopy method can derive the exact conditional sampling distribution of DNN-driven hypotheses in an efficient way, and it is proved to be effective in practical by experimental results. It also shows its novelty in solving the problem.- The authors provided comprehensive supplementary materials to help readers to understand the proposed method as well as to reproduce the experiments.- The structure of the paper is well designed, and the writing is clear.Cons:- While this paper claims that the proposed method can quantify the reliability of neural network representation-driven hypotheses, there could be more examples (more realistic models on more tasks) to demonstrate the method's effectiveness in more scenarios. Currently, the examples in this paper are almost all using basic NN components on image inputs. The performances are not obvious if the networks contain parts like residual connections or recurrent structures, either the inputs are sentences rather than images.- Related to the previous point, the boundary between where this method can be applied and can not be applied is not so clear. It would be better if the authors could give such guidance for people to use this method. SUMMARY:The authors propose a method that "explains" molecular properties based on molecular fragments and call it Molecular Evolution. The subgraph "explanations" then are used to explore larger swaths of chemical space.PROS:- As far as the reviewer notes, this approach is novel in the (now increasingly crowded) set of alternatives for molecular generative models.- The authors have a model that compares favorably to the baselines- The authors use a very relevant set of optimization parameters for the multiobjective task.- The paper is well explained.CONS:- The reviewer believes that there is much more to explain why a molecule is better for a task than identifying a subgraph. This should be made clear in the manuscript as materials scientists want to know for example quantum properties of the fragment(s) and how they influence the given property to provide a valuable explanation. This work identifies a new empirical phenomenon in the training dynamics of deep nets: when trained with full-batch GD, the curvature of the train loss increases up to a critical value of 2/(step size), at which point it plateaus for the remainder of training.This phenomenon is demonstrated robustly for networks trained with MSE loss, across various architectures and datasets, and a slightly weaker version of this holds for cross-entropy loss as well.This work contributes to our understanding of deep network dynamics -- it is a precise and apparently robust phenomenon that was surprisingly not noticed before (perhaps because of the requirement of GD vs SGD). In terms of impact: This work will be instructive for DL optimization theory, since it points out that certain assumptions which are usually made in theoretical works (e.g. step size << curvature) are far from true in practice -- moreover, it guides theory towards more realistic assumptions.It may also have later impact in practice, by leading to a better understanding of the interaction between optimization algorithm, step size, and architecture. Thus I recommend acceptance.Weaknesses and desired clarifications:- It should be mentioned more prominently that these results are primarily for networks trained with MSE loss -- and that a similar but weaker phenomena holds for cross-entropy loss. - Why is the main example in Section 3 given for a non-standard network for CIFAR-10? A 2-layer MLP with ELU activation. Why not a standard network with standard activation? (VGG-11 or ResNet-18, etc).- The distinction between SGD and GD seems crucial for this phenomenon, so more discussion would be good. In particular, as noted in the related works, some papers using SGD claim an opposite effect. This is especially important to clarify since SGD is most often used in practice. If time allows, experiments with increasing batch size could shed light on the importance of GD vs SGD.- The Related Works is currently written as an account of what previous works do *not* do, as opposed to what they do. It would help contextualize this work to relate it to prior works which are consistent (or inconsistent) with this phenomena -- especially works studying the Hessian of deep nets. Some of the mechanisms proposed in prior works (eg Lewkowycz et al 2020 and works on deep linear networks) may also be helpful to understand the phenomena in this work.Comments which do not affect the score:- I wonder if you have measured the 2nd eigenvalue during training as well? In particular, after the 1st eigenvalue has saturated at 2/eta, does the 2nd eigenvalue also "progressively sharpen" up to 2/eta ? (And so on for later eigenvals).- I am glad to see the experiments on deep linear networks, it suggests that it may be possible to theoretically understand this phenomenon in such simple settings. This would be a nice topic for future work. Summary:This submission numerically shows that during exploring the neural network landscape,  GD flow keeps increasing the sharpness.  As a result, GD with a fixed learning rate will exhibit two phases during the dynamics.  Denote by $\eta$ the fixed learning rate.  In the first phase, GD follows closely to the GD flow, and it finally converges to a region where the sharpness is roughly $2/\eta$.  Then, it transits into the second phase during which the sharpness hovers right at or above $2/\eta$. In the second phase, GD cannot increase the sharpness anymore due to the dynamical stability constraint. Thus, the authors name it the Edge of Stability phase.  What is interesting is that in the edge of stability phase, the loss is still decreasing steadily although not monotonically. Pros:I enjoy reading this submission. It is clearly written and the numerical evaluation is also sufficient.  To my best of knowledge, the observation that the edge of stability happens during the whole late phase of GD dynamics is new.  It reveals a very complicated dynamical behavior of GD for training neural networks, which has not been systematically investigated before.   Thus, I think this submission made a very important and original contribution to the understanding of GD dynamics  in deep learning. Cons:The relationship with the previous study on the dynamical stability of (S)GD is not sufficient discussed. In my opinion, just saying ''previous works have argued that the stability properties of optimization algorithms could potentially serve as a form of implicit bias in deep learning'' is obviously not precise and enough. A large number of numerical results in [1,2] already showed that the edge of stability happens for the convergent solutions, which implies that the edge of stability must happen at least in the very late phase of GD dynamics.   The new finds of this submission are that the edge of stability actually holds for a large portion of GD dynamics, which is very unexpected.   The authors should explicitly mention that the edge of stability was already observed in these previous works. Giving the right credit to the right references does not harm the contribution of this submission.  Especially, the jargon "Edge of stability" was first used in [1], and the authors even did not mention it. [1] Giladi, Niv, et al. "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?." arXiv preprint arXiv:1909.12340 (2019).[2] Wu, Lei, Chao Ma, and E. Weinan. "How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective." Advances in Neural Information Processing Systems. 2018. This paper presents a new pooling layer that is based on the Lifting Scheme from signal processing. It motivates this approach with the desire for reversible pooling functions for certain tasks. The benefits of this reversibility are demonstrated on a semantic segmentation task. As a drop-in replacement for the pooling lawyer in various neural-network backbones, it also outperforms many other pooling layers on classification tasks (ImageNet).I thought this paper had great collection of analyses: flexibility (choice of pooling band), effectiveness across kernal sizes, generalizability across various backbones, and robustness to corruptions and perturbations.*I recommend to accept*. While this may be just another pooling layer, it seems a quite well motivated pooling layer coming from a particular need for reversibility.Some highlights:- very clear writing- very well situated in historical and contemporary literature- exactly the experiments that I would want to see- the observation that the sub-band that represents vertical details constributes more to classification accuracy than other sub-bands; curious to know whether this holds outside of VGG13 on CIFAR-100.Some points for improvement:- the presentation of the lifting scheme and the use of the LL/HL/HH/HH notation is perhaps a little non-intuitive for anyone without previous exposure; you could make it clearer that not all bands would be necessarily used during pooling, but that the information would be retained for reversing the operation.- on p.7 you say "sift-invariance"; I think you mean "shift-invariance"- Instead of saying how you believe your findings will stimulate people to think about problems ("These findings may stimulate researchers to rethink", "We believe such findings will stimulate one to think"), it would be better to perhaps make a claim that needs to be evaluated in the future, or to point out exactly what is left unknown or surprising by your findings.- Figure 7 is not very clear. Think about people with colour blindness. Also, consider two separate graphs side-by-side or one above the other. It isn't clear what the shaded red area is meant to indicate and how it relates to what is "redistributed." I see what you're trying to show, and I actually just think it is a quite difficult thing to visualize, so maybe Figure 7 is the best you can get to, but I would brainstorm some more on this.- p 12. "Visualiztion"- p 12. the closing quotation marks around "high frequency" and "low frequency" go the wrong direction- Figure 9 caption: the hyphenation in LiftUpPool should be customized; it breaks the word at a weird spot- Figure 8 consider using various line types instead of colors in order to better accomodate people with color blindness- Throughout: inconsistent hyphenation "downsizing" vs "down-sizing", "up-sampling" vs "upsampling"- Bibliography: I think you need to force capitalization in some of the titles. See e.g. "pytorch" and "Mobilenetv2"----------------Question: When you "combine all the sub-bands by summing them up", do you literally just add up the values from the corresponding indices across each sub-band so that you're still reducing the dimensionality? I am a little surprised that this doesn't *reduce* performance. Can you say more about this? Why does this work?Question: Why did you only compare against three baseline pooling methods for the corruptions and perturbations instead of the full gamut as in Table 4?Question: do you have error bars for your experiments? Or did you run them only once each? For which results did you run your own experiments vs reporting numbers from previous literature (particularily in Table 4)?----------------My main uncertainty (why I am not giving this a 5 for confidence) is that I cannot be sure this hasn't been proposed in the past, but it is hard to prove a negative. I can say is that this does appear novel to me.I am also uncertain about the evaluation on the semantic segmentation task. I am familiar with this problem and the evaluations seem reasonable, but I cannot be sure whether the choices of comparator methods are the strongest alternatives. SUMMARY: The authors present an elegant Markov-Chain Monte Carlo (MCMC) method to carry out the task of generating molecular structures that satisfy several objectives.PROS: - The work is well written, concise and easy to follow- The methodology is competitive with other approaches that are state-of-the-art in the optimization of single properties (such as GA-D) and show that they outperform them in most cases.- The references that it cites are balanced.- The multiobjective optimization is based on biological objectivesCONS:- I see no major cons with this work. #### SummaryThis paper is about a comparison of methods for out of distribution detection on image classification. The authors compare along three different lines: Irrelevant inputs, novel classes, and domain shift. The results show that cosine similarity consistently outperforms other methods across all sub-tasks.The comparison made in this work is novel, as most OOD detection methods are evaluated on standard academic datasets (CIFAR10/100, SVHN, etc), which could hide issues that are presented in more realistic scenarios.#### Reasons for ScoreI believe that this paper touches a subject of importance to practitioners and researchers, the performance of OOD detection methods in more realistic settings and with three different sub-tasks inside the OOD detection task.The evaluations are made correctly, in a variety of datasets, ensuring the robustness of the conclusions that were made.My only concerns with this paper are the lack of comparisons with ensembles, which could change which method works best.#### Pros- The evaluation is comprehensive, with a variety of datasets, properly selected, ensuring robust results and conclusions. There is also a variety of tasks inside out of distribution detection (new inputs, novel classes, and domain shift).- The evaluation produces strong conclusions, with one method (cosine similarity) clearly outperforming the rest. This provides good evidence for practitioners to select OOD detection methods in the future.- The evaluation is made with a practical point of view, motivated by real-world examples, that differ from purely academic benchmarks. This is very useful for practitioners.- A nice appendix that provides detailed results that can be used for future comparison.#### Cons- I believe there is a missing reference that is very relevant for this work, namely Ovadia et al. "Can You Trust Your Models Uncertainty? EvaluatingPredictive Uncertainty Under Dataset Shift" NeurIPS 2019. Many methods evaluated in Ovadia et al. like ones relying on variational inference are not evaluated in this work, so I suggest to take a look and include additional baselines. Note that Ovadia et al. also defines corruption methods for evaluating dataset shift.- I think there are missing methods to compare, for example ensembles. The paper mentions that MC-Dropout "is an approximation of BNNs that is computationally more efficient than en ensemble of networks". I do not think this is true, ensembles and MC-Dropout are equally slow in predicting a forward pass, but an ensemble usually performs better in OOD detection and produces higher quality uncertainty (see Lakshminarayanan et al. 2017, Figure 3 and Ovadia et al. 2019). If MC-Dropout is used for comparison, then Ensembles should also be used since generally ensembles outperform MC-Dropout in terms of uncertainty quality. Note that this might change which method works better, or even create new baselines, as ensembling can be combined with cosine similarity OOD detection.#### Questions for Rebuttal PeriodWhy were ensembles not considered as one of the evaluated methods?#### Minor Issues- In Section 3.1, please mention on which dataset are models pre-trained, I assume it is on ImageNet, but this needs to be clearly specified.- Please mention in the Appendix that number of forward passes used in MC-Dropout- I think method should be separated into two groups, one where OOD detection capabilities are given by the quality of uncertainty (like Bayesian NNs, MC-Dropout, Ensembles, etc), and other methods where an score is produced specifically for OOD detection and are not direct measures of uncertainty (not probabilities). This could shed a light on why methods work so differently.- Sections 2.2 and 3.3.1 could be rewritten to clearly describe the OOD score produced by each method, for example in some cases the ID score is describe, how is the OOD score derived in these cases? This is important since an OOD score is used in Section 3.3.- For probability-based methods like MC-Dropout, entropy of the output probabilities can also be used as an OOD score.- In Figure 3, the x labels for Maha (Sum) are too close to each other and might hinder reading, I recommend to rewrite as 25K/50K/75K or rotate them a bit.- For more clarity and a self-contained work, the authors could add a summary description of each method in mathematical terms to the appendix. Adversarial training is a principled approach towards robust neural networks against adversarial attacks, but it is extremely computing intensive. This work tackles the problem by leveraging the general distributed training method, and addressing the problems of direct application by several effective innovations. Strength:+ The proposed method is practical.+ The proposed DAT can deal with both labeled data (supervised learning) and partial unlabeled data (semi-supervised).+ I like the idea to use gradient quantization/compression.+ Make theorectical conribution by convergence analysis for DAT with LALR and gradient quantization.Additional comments and questions:1. One conclusion is the paper can speed up by 3 times with 6 times resource. Please comment on what's the typical speedup of distributed training with n times of resources. Do you think you can further speedup?2. In formula (2), additional regularization term by lamba is used, making it different from the direct generation of (1) into multiple workers. Why we must introduce the regularization term in DAT?3. Please provide more details about how to measure communication time please? any profiler used?4. In table 2, additional unlabeled data can improve accuray. Why?5. Which deep learning framework is used to support gradient quantization? ### Summary of my understandingThe authors propose a method of function fitting for differential equations. They premise that a model $F$ is the additive combination of $F_p$ and $F_a$, which denote physics and augmenting parts, respectively. The functional form of the physics part, $F_p$, is given in accordance with prior knowledge, whereas the augmenting part, $F_a$, is modeled by neural nets. The proposed method follows the principle of least action of $F_a$, and the authors suggest solving a constrained optimization problem via a method of Lagrange multipliers. They show numerical results on three PDE/ODE-governed systems.### EvaluationI really enjoyed reading the paper, which is well written. The motivation is clearly presented. The related work can be more detailed given the recent active studies on physics + ML but seems sufficient from the viewpoint of ODE/PDE fitting. The proposed method is simple yet reasonable, and the experiments are enough supportive to see the superiority of the method. The ablation study in Appendix F is also very interesting. Possible improvement, which the authors would be aware of, can be found in the lack of experiments on real-world datasets. (I understand the difficulty of finding illustrative real-world examples in this kind of problem, but I couldn't stop pointing out it.) Overall, I think this is certainly sound work. This paper characterizes the implicit bias of gradient flow of two-layer ReLU networks on orthogonally separable data trained on the logistic loss. The problem of characterizing the implicit bias of gradient descent on neural networks is an important one, and while the authors do make fairly strong assumptions on the data (data corresponding to the different labels lie in separate orthants), the proof is novel, interesting and non-trivial. The proofs are carefully carried out and seemed as far as I could verify.A few questions:1. Is it possible to characterize what the outer weights (a's) converge to? If yes, I would suggest that the authors include this either in the main theorem, or as a comment after the theorem.2. Does a similar result hold if the network also has bias variables? 3. Can linearly separable data be made into orthogonally separable data (by appropriate pre-processing) and by also training a bias term?4. How are the lambda_j's chosen in the near zero initialization? The current description of choosing lambda_j on page 2 is quite vague.5. I would also urge the authors to add the additional assumption about the positive and negative examples spanning the entire space in Section 2 along with the other assumptions.  (a) This belongs to the literature of implicit bias/inductive bias, which hasgained a great deal of attention among theoretical enthusiasts with anoptimization leaning.(b) The paper is carefully laid out and argued, and is at a nice levelof clarity and precision.(c) The mathematical argumentation seems to me correct; however I haven't checked line-by-line.(d) The situation being studied is very very specialand doesn't much correspond to the big kahunadeep learning. Nevertheless the intellectual clarityof this special case is quite appealing.(e) The implied conclusion seems rather specialas well. From one viewpoint it says that if you start from the get-go with perfect separation of a particular strong form,then the future evolution of the training can never spoil things. This is a very weak statement,but I suppose if we can't get results here in a very special casethat we can understand well, then the general situation is truly hopeless.Specific Comments.(1) Why is this max-margin if your constraints only consider one class. It seems to be more of a finding a minimum-norm vector aligned with all training data of that class. Its unclear why the concept of separating margins comes in.(2) Theory III: Dynamics and Generalization in Deep Networks by Banburski et al. also considers general deep relu networks and shows that the resulting margins are max-marginrequiring only separability, not orthogonal separability. In addition, that paper uses traditional DE methods rather than relying on lesser-known extremal sector techniques. Can you discuss or highlight why the simpler example in this paper might lead to insights not found in the other paper.(3) While the paper says that it is not directly applicable to deep nets, it draws motivation from the popularity of that literature. In that spirit, to justify such an evocation, can you show at least one experiment on a non-synthetic dataset such as MNIST/CIFAR/etc (perhaps even simplified with hand-engineered preprocessed features and subsetted to two-classes) that would support the potential connection to deep learning?(4) Can you provide any evidence why datasets would become orthogonally separated? Is there some featureengineering procedure that tends to produce orthogonal separation?(5) In Figure 1, variance is strange: shows one big outlier, but the plotted projection shows two roughly-equal-magitude directions of variation.(6) It is unclear how Definition 2 relates to strict extremal directions as defined by the sign patterns.(7) G should be clarified: What G is and what it represents should be explained to make the results more insightful 16m 50sType a message This work applies information bottleneck as a way to compress the pre-trained representation so that only meaningful features are employed for the target task. It is applied for the number of GLUE tasks especially focusing on low resource settings and show consistent gains over previously known strong baselines, e.g., Mixout and L2-of-difference. This work also demonstrates that the learned model has generalization capacity so that the tuned model works on out-of-domain data.# Pros* An elegant solution to the fine tuning settings especially for the low-resource settings.* Experiments are performed extensively on various tasks and demonstrates its effectiveness in generalization for out-of-domain settings.* Interesting analysis of the experimental results.# Cons* Basic idea is already demonstrated by Li and Eisner (2019), and I was not very surprised by this results.# DetailsIt is a very sophisticated way of avoiding overfitting especially when the data size is limited, and it might have an impact of broader application when exploiting pre-trained models. Thus, I'd recommend acceptance for this submission. Pros:- I think the paper is exceptionally well-written and the figures are very carefully designed. Applaud!- Thank you for proper train/test/validation splits! Glad there are varying degrees of difficulty with proper held-out sequences.- I very much appreciate proper comparison to other methods. Very thorough.- Less important, but the model also performs better at these two tasks than any other approach. ( I say this because I believe the field shouldn't always require SoA if there is a significant technical advancement.)Cons:- The authors site "over-smoothing" for why their convolution operator performs better, but provide no direct evidence that this is the case. It needs to be noted that this is either a hypothesis, or more concrete evaluation of this needs to be performed to make this claim.- Are there any replicates for standard error and ablation studies?- Table 3 BLAST comparison is weak. JackHMMER or HMMER based tools are more appropriate than BLAST.Neutral:- What defines a hydrogen bond? This definition is clear to me in secondary structure, but seems more loose in tertiary structure.- In your figures, it looks like only carbons, oxygens, and nitrogens are defined. What about hydrogens? If hydrogens aren't parameterized, how do you define hydrogen bonds? This may be good to clarify.- In Table 2, does the modification of the architecture change the number of parameters?- Definition of a "ball query" might be helpful.- Are there any sequences with post-translational modifications in the dataset? If so, how are those handled? The authors proposed a novel method for regression problems with outliers. The main idea is to first propose a mixed-integer optimization problem for the regression problem and then and the optimization procedure of finding the solutiuon of the problem differentiable, and the objective function of the problem are also be rephrased as a differentiable function. Based on this, an end-to-end learning approach can be established.Pros:1. The motivation of the paper is very clearly stated in the text, and the sketch of the theorems make the paper easy to understand. 2. The experimental part is good and it proved the efficiency of the proposed method.3. The idea of converting a mixed-integer programming to a differentiable function is elegent. Cons:1. The authors says that they are going to somehow relax the one-to-one matching constraints, however, in the main text we can see that the model is still based on strict one-to-one matching constraints. In the experiments, for synthetic data, every generated data is in fact an one-to-one matching. For other datasets, through they are not strictly one-to-one, but they are close to one-to-one.2. Theorem 1 is a trival result due to total uni-modular, and it is proved many years ago, maybe it would be better to simple give a citation there.  ##########################################################################Summary:In this work, authors implement and tune 14 DG algorithms and compare them across 6 datasets with 3 model selection criteria, and they find (i) a careful implementation of ERM outperforms the SoTAs. (ii) no competitor can outperform ERM by more than one point (iii) model selection matters for DG.##########################################################################Reasons for score:I believe "the well-tuned ERM outperforms many SoTA DG methods" may not surprise many researchers in this area, but I would very much like to see a paper delivering this message clearly. Thus, I recommend an acceptance. ##########################################################################Pros:1. For quite a while, ERM (when carefully implemented and tuned) being the SoTA in DG is "elephant in the room". At least, we should admit that, quite often, the "improvements" claimed by those DA methods are gone when switching from a weaker backbone (e.g., ResNet-18) to a stonger one (e.g., ResNet-50). A high-standard testing protocol is a very important contribution in DG research.2. This work brings an open-sourced software for replicating the existing methods, and comparing the newly proposed ones in a consistent and realistic setting.##########################################################################Cons:1. Domain-Net, as a much larger scale and more challenging dataset, could be considered. 2. Can you elaborate the last sentence of Claim 2, i.e.,  "our advice to DG practitioners is to use CORALwith a hyperparameter search distribution that allows ERM-like behavior".##########################################################################A typo: (Page 6) "Table 5.2" shows that using a ResNet-50 neural network architecture. I think it should be "Table 4" The paper proposes a new approach for abstract reasoning and explores it in the context of the RPM task. In contrast to other competing approaches, the authors seek to build into the model as few assumptions as possible to keep the model general and not specific to the specific problem or to particular annotations or supervision signals. The general capability that they seek to incorporate into the model is the ability to effectively compare and contrast candidates in tasks that require choosing the best fit. The task is important, presented carefully and is well-motivated and the paper is clear and easy to follow. The related work section covers the necessary backgrounds including both visual reasoning and general and the RPM task in particular. It also presents the existing methods and discuss their disadvantages compared to the new approach - mainly their stronger reliance on supervision to learn good, semantic or disentangled features that will help most in addressing the task, or particular assumptions other approaches make about the specific properties or structure of the RPM task that may not hold in others or more general cases. The task is also clearly presented and the authors explain how it consists of two main sub-tasks: (1) identifying the rule that links the existing rows or columns, and (2) comparing the candidate answers to choose the best fit. They propose network modules to solve each of these tasks correspondingly: a rule-contrast module, and a choice-contrast module, to complete each of these sub-tasks. For both  sub-stack they use clustering to find contrast and similarities between elements. This idea is simple, nice and quite novel I believe in these contexts. Experiments are performed over two datasets of RAVEN and PGM, and achieve 5.77% improvement on average over state-of-the-art, and larger improvement in scarcer-data regimes, which the authors particularly focus on. They provide useful information about the datasets, baselines and implementation details and experiment settings, along with an ablation study to give further insight into the benefits of particular aspects of the model.A particular comment that I have is that it seems that the model considers each answer by replicating the board k times for the k candidates, and considering each such completion alternative. While working for the RPM case, it wont scale to problems where theres a larger number of candidates, potentially even not-bounded. Restructuring the network to be able to preprocess as much as it can about the board so to reduce the amount of answer-dependent computation may be very useful to make the approach more general and efficient. Im also looking forward to hearing about further applications of this idea as discussed at the end of the paper in the general VQA task or on other abstract reasoning, for instance such as the Abstraction and Reasoning Challenge.Overall, Great work! Summary--------The authors considers the problem in Few-shot classification and addresses the need for uncertainty management (calibrated output uncertainty, robustness to input noise, and out-of-episode detection) while maintaining high accuracy. To this end the authors propose a novel approach based on Gaussian process classification with Polya-gamma augmentation, one-vs-each Softmax posterior approximation and with a novel cosine *similarity* kernel (in composition with deep kernels). The latent variables from the Poly-gamma augmentation and latent GP function are Gibbs sampled and GP hyperparameters (including the parameters of the NN making up the deep kernel) are optimized using gradient decent based on the samples. The approach is validated in comprehensive comparative empirical experiments involving multiple datasets, and is demonstrated to be top performing in both accuracy and uncertainty management. Strong points-------------1. Well written paper, addressing an important problem in FSC with a well motivated and promising novel approach, filled with technical and methodology detail for completeness.2. The approach combine high accuracy with calibrated output probabilities.3. The performance is consistently strong at both robustness to input noise and out-of-episode detection.4. The experiments are extensive w.r.t. the competitive approaches and have wide coverage given the different data sets used.Weak points-------------Nothing obvious to me.Reason for score----------------A well written paper, with several well motivated and empirically validated contributions, on an important topic. The paper seem to be technically correct and well placed in the literature. Especially the contributions on uncertainty quantification (both benchmarks and the proposed method) i believe are valuable and important for the FSC field, as well as for ICLR at large.Minor comments--------------It would be better if Figure 2 can be made larger. Maybe by sharing the legend with Figure 3 and shortening one or two sentences slightly to make room? Summary:The authors prove several statements about the expressiveness of different classes of graph neural nets (GNNs): conventional message passing networks, linear GNNs (LGNN) and folklore GNNs (FGNN). The novel theoretical contributions include analysis of expressiveness of FGNNs that use tensors of arbitrary order in terms of comparison to the Weisfeiler-Lehman tests; a characterization of the functions that these classes of networks can approximate; universality of FGNN as the tensor order goes to infinity. The results are based on a general Stone-Weierstrass-like theorem for equivariant functions. Prior universality results can be recovered as special cases. The authors have a simple experiment that show in a limited setting that a practical implementation agrees with the theory.Strengths:-The paper and appendix are very well written and relatively well understandable for me, unfamiliar with universality proofs. Particular examples of clear writing include: the statement of Theorem 4 is clearly explained below the theorem; providing Example 16 directly after Corollary 15 aids exposition; using example 17 to motivate prop 18. -The authors use a very general statement (Thm 20) to derive their results, making them generally applicable.-The authors derive a substantial number of expressiveness results from the general theory.Weaknesses & suggestions for improvement:-The main paper only sets up the problem and states the main results, while all theoretical contributions are done in the appendix. The main paper would be more self-contained if some more intuition for the proofs was given in the main paper.-The experiments seem to not compare to LGNN. Adding this comparison would help making an empirical argument for why FGNN is best. Recommendation:Although I am not very familiar with the field of universality proofs, the paper appears to me to be a very solid contribution to the field and I recommend publication. Minor points / suggestions:-In several instances, the authors write a compact without a noun. Is this conventional language?-Below Eq 2, there is an F without subscript. Is this the same for all layers?-In App C.3, in the second line of the equation, are there suffices missing on the left-hand side?-Sec 4.3, first sentence, the set invariant, missing of -Sec 4.3, typo To clarify the meaning of theses statements-App D.3: typo as every function satisfy should be satisfies-Example 16, typo is able to learn function should be plural-App D.3, proof of Corollary 15: define when a class separates a set around clearly separates X_\mathcal{F}. Presumably means same as separates points?-Eq 17, Define S \cdot \mathcal{F}, presumably as the scalar-vector product of the outputs of the functions? -Corollary 19, assumption 3, what does pairwise distinct coordinates mean?-App D.3, typo For an equivariant function, for any missing $f$-Example 21, \mathcal{C}_{eq} should be \mathcal{C}_E? Happens later more.-Lemma 24, define R{X_1, .., X_p] as polynomials The paper studies the effect of padding on artefacts in CNN feature maps and performance on image classification and object detection. It convincingly makes the case that these artefacts have a significant detrimental effect on task performance, e.g. leading to blind spots / missed detections of small objects near the image border. It also studies the effect of uneven padding in downsampling layers, where the padding may only affect some sides of the image and not others, depending on the image size. A condition is presented for when this does / does not occur. The effect of different padding methods is also studied from the perspective of foveation by computing the number of paths from an input pixel to the output. A number of practical recommendations are given.The paper is well written. It contains lots of details that are relevant to CNN architecture design, especially when the appendix is taken into account. Proposed fixes are simple and produce a very significant improvement in performance on imagenet classification and object detection, so are likely to be adopted by practitioners. The paper states:"It is evident that the 1-pixel border variations in the second map are caused by the padding mecha-nism in use. This mechanism pads the output of the previous layer with a 1-pixel 0-valued border inorder to maintain the size of the feature map after applying a 3x3 convolutional kernel. The maps inthe first layer are not impacted because the input we feed is zero valued. Subsequent layers, however,are increasingly impacted by the padding, as preceding bias terms do not warrant 0-valued input."It would be interesting to know if batchnorm or some other kind of normalization might mitigate this issue, because if the feature map is constant but non-zero, normalization will make it all zero. Of course this will not hold for non-zero (natural) inputs, but it would still be interesting to see a discussion on the effect of (batch) normalization on padding artefacts.TyposSection 4: "To serves"Section 5: RseNet The knowledge distillation (KD) approach is a two-step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model. A key theoretical question about KD is whether and how much this two-step approach can improve on the one-step approach that trains the student model directly on the labeled data. This paper casts KD as a semiparametric inference problem by treating the optimal student model as the parameter of primary interest and the true class probabilities as the nuisance parameter. Building on the semiparametric framework, the paper makes two contributions: 1) develops theoretical guarantees for the vanilla KD algorithm; 2) proposes improved KD by using a first-order bias-corrected loss and a sample splitting procedure.Overall, I find the paper novel, well-written, and thought-provoking. It bridges the two directions of KD and semiparametric inference, allowing for the possibility of borrowing theoretical and methodological tools from semiparametric inference to analyze and improve the KD approach.On the other hand, I think the paper somewhat oversells the semiparametric inference idea, since the theory presented in the paper (Theorem 1) does not have much semiparametric flavor. The central questions in semiparametric inference are the information bounds and semiparametric efficiency for the target parameter. Although Theorem 1 is useful, it does not directly address these key questions. For example, what is the best possible performance for the student model? In many classical semiparametric inference problems, we are able to construct a semiparametrically efficient estimator. Would this be possible for KD?More comments:1) The paper seems to treat $f_0$ as infinite dimensional (page 3, 3rd paragraph of Section 3). However, classical semiparametric inference largely exploits the finite or low dimensionality of the target parameter to answer the questions mentioned above. In the KD framework, I still think the finite-dimensional case is more interesting and would provide more fundamental insights.2) There is a notable difference between the settings of KD and classical semiparametric inference: the target parameter $f_0$ in KD is ancillary in the sense that the data generating model depends only on the nuisance parameter $p_0$. Would this affect the applicability of semiparametric inference techniques?3) Page 4, line 11, student should be teacher. SummaryThe paper uses RNN trained by risk seeking RL objective to predict mathematical expression that generated the target dataset. In the second step placeholders for constants in sampled expression are optimized again by gradient optimizer to maximize the reward function. The decoder RNN uses heuristics that make it easier to generate valid math expressions.Strong points+ The paper is clearly written and potentially relevant to larger audience.+ The proposed method is relatively easy to implement and it leads to good empirical results.RecommendationI recommend acceptance of this paper since the proposed technique is highly competitive, easy to implement and well presented.Questions* How does compute used by GP to get results in Tab 1 compare to policy gradient based methods? My perspective is that all the techniques should find the correct solution in the limit with enough computational resources (under assumption that they can escape local optima). Therefore making sure that all the methods had roughly similar budgets is important.* In figure 2E mean reward for standard PG is going up even after 2M training steps, would it be possible to run the experiment longer to see where the max would be once mean reward converges? (Similarly in figure 8 in tasks Nguyen 3,5 and 8 mean of standard GP is still going up.) It is great to see that risk seeking PG converges much faster on these tasks, however knowing what is the limit for standard PG would be also interesting.* In section 3.1 you say that generated expressions had to be between 4 and 30 symbols long, however tasks 8 and 11 (sqrt(x) and x^y) have shorter descriptions. What am I missing?Possible improvements* Using the set of problems from AI Feynman would make empirical evaluation stronger. However the same can be said for AI Feynman system and Nguyen dataset.* Beyond scope of this paper: At the moment a new sequential model is learned for each task and it learns about that task only through the reward function. What if the model can "see" the dataset first by reading it through n-dimensional CNN (or RNN) that would produce "dataset embedding" that will be later used to condition the sampling RNN. In the same way that image captioning model is conditioned on image embedding. Generating infinite training dataset (with random expressions) should be trivial, that is one advantage over image captioning with limited data.Typosstandard standard -> standard This paper presents a novel approach to the problem of symbolic regression, where the goal is to learn relationships between variables in the form of mathematical expressions. This is clearly a very relevant task towards constructing explainable AI systems.The proposed approach in this paper is based on the generation of mathematical expression with recurrent neural networks, by exploiting background knowledge about the form of the expressions to impose constraints on the generated examples.The RNN is trained by maximizing a risk-seeking policy gradient that aims to increase best-case performance. The key idea here is to increase the reward of the top-epsilon fraction of samples from the distribution, without taking into account the samples that fall below such threshold.The technique is sound and novel, and it provides a significant contribution to this research area. The positioning of the paper with respect to the state-of-the-art in the field is highly accurate.A very solid experimental evaluation is carried out on several benchmarks, comparing the proposed approach with state-of-the-art systems for the same task, including commercial software such as Eureqa and Wolfram. The analysis includes an ablation study and experiments conducted with different amounts of training data. The proposed methodology is shown to perform better than all the competitors.Overall, I consider the paper to be a strong contribution for ICLR.* In the experiments with different levels of noise in data, why was Gaussian noise added to the dependent variable only, and not also to the independent variables?- Pag. 4, "is not allowed.While" -> "is not allowed. While"- Pag. 5, "but in practice has high variance" -> "but in practice it has high variance" Aim to improve the interpretability and the accuracy of the neural network, this paper takes a step further on the integration of NN with a decision tree. It will replace the final linear layer of the NN with a decision tree induced by pre-trained model weights. It takes advantage of both hard and soft decision trees and designs suitable tree supervision loss thereon. Extensive experiments verify the design choice of the proposed components. On both small-scale and large-scale datasets, it beats the decision tree counterparts. Also, on the aspects of generalization and interpretability, it shows the strength compared to NN.This work is a good try to combine the two techniques NN and decision tree. It finally makes the combination to achieve comparable accuracy with the NN and also enjoy the benefit in the aspects of generalization and interpretability. Recent SOTA of capsule networks which are based on the NN backbone and this work are both achieved comparable performance with NN. They show a promising direction for studying representation learning. Researchers can delve deeper based on this work to further exploit how to integrate decision tree into NN and the characteristics of the combination (e.g. adversarial examples). With the decision tree, we can visualize the decision process the bring the benefits of interpretability. The paper proposes to label the decision nodes with WordNet and show the applications of zero-shot generalization, high-level concepts, dataset debugging, and improved human trust. There are lots to do on the aspects. Also, the zero-shot and high-level concept experiments are really intriguing. Using the pre-trained model weights to construct the tree and the proposed tree losses to train can help the generalization in such a significant way, though the performance would depend on the accuracy of the superclasses labeling and the agglomerative clustering. Where the benefits come from? The method is only used the same information as the NN and the tree is also constructed based on the pre-trained weights. Does the way of making hierarchy decisions help here? If you do not enforce the second term of the equation (3), will the phenomenon be the same?Overall, the paper is very easy to follow and the figures really help understanding. Extensive experiments help to know the performance, effectiveness of the proposed components, and also its unusual applications.----------------------------Some concerns and comments are listed below:Will you update the weights of the intermediate nodes? On large-scale datasets, the paper currently only tests using the EfficientNet. The reviewer wonders if the author can use more advanced backbones to see the performance changes.The reviewer is unsure of the specific way to label the decision nodes. Will you use the wordvec provided in the wordnet and compare it with the decision nodes' feature? Since your structure is different from the WordNet, how do you match the classes with the nodes? # SummaryThis paper studies the relationship between extractability of features from pre-trained representations and how much a fine-tuned model uses that feature. The extractability of features is measured by the minimum description length of a probing classifier trained to detect the feature from the pre-trained representations (using the online code version of Voita and Titov). The degree to which a fine-tuned model uses the feature is measured by the amount of evidence required for a model to tease apart spurious from non-spurious features (called "target" features). Evidence here means examples where a spurious feature occurs but a non-spurious feature does not occur. When there are many such examples (high spurious-only rate), it is easier for a model to reject the spurious feature and learn to rely on the target feature. The "degree to which a fine-tuned model uses a feature" is defined as the minimal spurious-only rate at which the model can accomplish the task. The paper has two kinds of experiments, on synthetic and more natural data. The synthetic data are sequences of symbols where the task is to identify simple properties like occurrence or repetition of symbols. The experiments are set up such that varying rates of spurious-only examples are presented during training, providing increasing amounts of evidence against the spurious feature (presence of the symbol 2) and in favor of the target feature. The target feature is identical to the label, that is, it is 1 when the example corresponds to the label and 0 otherwise. The paper reports extractability of the spurious and target features via the MDL of a probing classifier. The metric of interest is the relative MDL, where higher means the feature is more extractable. When the features are more extractable, less evidence is required for the model to reject spurious features. With less extractable features, more evidence is required. The natural language examples are made with acceptability judgements of examples generated by grammars for three linguistic phenomena (subject-verb agreement, negative polarity items, and filler gap dependencies). Here again the setting is similar, modulu a tweak on how to calculate extractability. The main result here is high (negative) correlation between extractability and evidence required for rejecting the spurious feature. # Main comments1. This paper fills an important gap in the NLP interpretability literature that has recently been a cause of concern in the community. On the one hand, probing classifiers tell us something about the existence (and more recently the extractability) of properties in pre-trained models' representations. But they do not tell us whether a model uses those properties. One the other hand, many challenge sets and test suites tell us whether a model can successfully perform a task requiring some linguistic property. The paper aims to connect these two aspects, and it does so quite convincingly, although I have some reservations below. 2. The experimental setup is well designed. The use of synthetic data allows a fairly clean setup where spurious and non-spurious features are distinct and simple. The experiments of training with increasing amount of spurious-only examples are instructive. 3. The natural language examples are important as they go beyond synthetic data and closer to a naturalistic scenario. However, these are still templatic sentences and synthetic in a sense. I wouldn't call these naturalistic examples. Ideally, experiments on naturally occurring data would be more convincing. Or, at the very least a discussion of this issue should be made. 4. The paper makes use of recent advances in interpretability work, including information-theoretic probing, and draws connections to a broad range of related work. 5. The assumption that the extent to which a model uses a feature can be measured by the spurious-only error rate (at some spurious-only occurrence rate) is questionable in my opinion. In a very clean setting like the synthetic data, I could maybe accept it. But, "using" is in fact a causal concept, while a causal mechanism has not been demonstrated. The paper alludes to this point in the discussion, but I think the discussion around this point should be expanded, and the strong claims should be rephrased or modulated.  # Questions and other comments1. The paper makes the assumption that the target feature t and the label are the same. I am not convinced about the "without loss of generality" claim. In practice, it is not easy to isolate a feature t that is identical with the label. How would this assumption affect the generalization of the approach to more realistic scenarios? 2. The task is a binary classification task. The features holding is also binary, that is either a feature holds (1) or not (0). But, suppose the label is 0, then the t feature is also 0, meaning it does not hold. This seems contrary to what is meant. This could be a confusion on my part. ## Synthetic data3. Why is MDL computed by training a classifier to distinguish s-only from neither, and not from some other part of S? 4. Footnote 3 is concerning - Aren't MDLs higher than a uniform code meaningless? 5. The classifier is not so simple (LSTM + 1-layer MLP). Why is that? How does the identity of the classifier affects the results? 6. The both-error subplot in figure 2 shows a slight increase in error rate with large s-only rate. Does this mean that the model has (falsely) learned to reject the example when s is in it? That is, it has learned another spurious feature, just in the other direction, instead of learning to rely on t. 7. A similar pattern is found in the t-only error subplot. There, even with high s-only rate, the models don't classify t-only examples correctly. I wonder why this plot is different from the s-only error plot, as this shows a directional behavior. Some discussion of this would be useful. 8. There seems to be a stark contrast between the contains-1 feature and the other three, both in terms of MDL and in figure 2. Is it possible to show a more gradual behavior between the two extremes? ## Natural language examples9. Why are the training sets so small? How does this affect MDL numbers and their validity? Apropos footnote 1. 10. Why exactly is it hard to generate t-only examples? The appendix is indeed helpful in making sure the MDL(t) calculation method is legit, but more clarification around this issue would be good.  11. Here, s-only error is used as "the use of the spurious feature", but this is only one aspect in which a model may make use of s. It may be that a model makes more complicated use of s, when s is found in combination with t. The discussion touches upon this point by acknowledging that the work does not establish a causal relationship between extractability and feature use. I'd go even further and say that "feature use" should be defined in causal terms. 12. What is the performance (F-score) for determining s-rate*? is that the performance on s-only examples? on other examples? Why the shift to F-score now?13. Discuss y-axis differences in figure 3c. BERT needs much less evidence than (some cases of) GloVe and T5. How does that impact the analysis?14. The term "learning curves" for figure 4 is confusing: those aren't results during training, right? They are results after training, each time with a different rate of s-only examples.  This submission takes inspiration from work on deep learning architectures for visual tasks in order to make targeted model changes to deep reinforcement learning models. The authors show that by including dense connections (concatenating the state or state-action pair to the input of each hidden layer of the network) they are able to successfully train deeper networks. The main idea behind the work is simple but effective, and their model surpasses the most of the presented benchmarks. The paper is also well written and presents a thorough set of experiments, making it a good submission for ICLR. Positives: * The main idea behind the architecture is fairly simple and the explanation is grounded in previous architectures (specifically densenet), making the experiments quite easy to understand.* The authors evaluate their method on a diver set of tasks, and their model outperforms the benchmark for the majority of tested conditionsConcerns and Questions:* The results comparing the ResNet style architecture with the DenseNet style architecture are interesting, particularly because the ResNet architecture does not see the same benefit. An explanation of how to interpret this result would be helpful to readers (ie why does a residual connection in this setting not help with DPI?).* It is unclear why the authors chose to use a 4 layer D2RL. It looks like an experiment was done in 6b varying the number of layers, but perhaps introducing this earlier (ie as a direct comparison to Figure 2) would make this choice more clear. The authors propose a novel attention-based GNN called MAGNA. The main contribution consists in considerably increasing the receptive field by considering a multi-hop neighborhood instead of the standard one hop. The technical challenge consists in obtaining attention scores for all relevant nodes in an efficient way. MAGNA solves this by using a diffusion-based technique combined with a geometric distribution. The authors show that the latter further allows for approximations, and also give interesting theoretical insights (e.g., show a relation to page rank). The paper is overall well written, related work is considered adequately, the idea is interesting, and the results are convincing. The evaluation comprises several different datasets, domains, tasks, and competitive baselines. The ablation studies give interesting insights in the effects of different parameter choices. Altogether, I suggest to accept the paper.----------------------------------------------Smaller comments:- p.3: "degenerate categorical distribution with 1 category": I think this should be explained.- p.3: "effectively creating attention shortcuts between nodes that are not connected (Figure 1).": I assume you mean "directly connected" since the whole approach seems to consider only connected nodes?- p.4: "as well as good model generalization.": How does the diffusion process ensure good model generalization?- p.5: Footnote 2: ??- 4.2. Baselines: The paragraph mentions few what is not in the table, so maybe you can just drop it and use the space for more descriptions.- 4.2. Results.: The last sentence is unclear to me. Natural images may lie on the disjoint multi manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of Generative Adversarial Networks (GANs) and variational autoencoders (VAEs).The paper proposes QMM, a new generative modeling scheme that inherits the multi-generator scheme but involves an essential regularizer enforcing the encoder compatibility. QMM considers generic manifold structure by the generalizable equivalence relation between data, thereby taking the quotient of this relation and driving the manifold structure of untrained data.Clarity: The paper is well written and easy to follow.Novelty:  Inheriting multiple generators scheme for generate model training has been proposed in many previous works. There are some recent work for applying multiple generators to VAEs or GANs, e.g., Pan et al., Latent Dirichlet allocation based generative adversarial networks, Neural Networks, 2020,  etc.   A major difference in this paper is that the additional regularizer enforcing the encoder compatibility and the quotient of the plausible equivalence relation.  Overview =The paper proposes SUNRISE, an approach to reinforcement learning that leverages ensembles of agents to build more robust RL updates. SUNRISE comprises a number of similar agents (in the paper, SAC agents) that perform parallel updates. Sample transitions for which there is larger variability (across the ensemble) in the estimates of the next-step Q-values are down-weighted in the computation of the loss, thus potentially rendering the learned Q-function more robust to noise.The proposed approach is then combined with bootstrapping masks and UCB exploration, and is shown to outperform a number of state-of-the-art approaches in several benchmark domains from the RL literature.= Positive points =The paper is clearly written. Additionally, the proposed approach is sensible and the empirical evaluation is, in my perspective, quite comprehensive: SUNRISE is evaluated in a broad collection of domains in the RL literature.= Negative points =The paper would benefit, in my opinion, from additional discussion regarding: (a) the impact of the use of bootstrap with random initialization; and (b) the computational complexity of SUNRISE (even if the paper does briefly discuss the latter in Section 5.2)= Comments =I quite enjoyed reading the paper. The problem addressed is a relevant problem in RL, and the approach proposed in the paper is, in my opinion, simultaneously simple and sensible. The paper provides a solid empirical evaluation, covering a broad range of domains and comparing with multiple state of the art approaches from the literature. The results show that SUNRISE compares favorably -- in terms of performance -- with several of these other methods in multiple domains.There are, however, two aspects that I would like to see discussed at greater length. On one hand, the paper proposes the use of bootstrapping masks and random initialization to induce variety in the ensemble. While the paper introduces both bootstrapping and UCB exploration as a "useful complement", it seems to me that this is quite central to the performance of the algorithm. Is this correct? In fact, without this device, the agents in the ensemble would essentially train from the same replay buffer, so variability would only come from the initialization. It is a pity that this particular element isn't included in the ablation study, for I would like to gain a clearer understanding on how critical this device is for the performance of the algorithm.One other aspect that I would like to see discussed is regarding the computational complexity of the proposed approach. The paper remarks that SUNRISE is more computationally efficient than competing methods such as POPLIN and PETS, and being an ensemble method, I expect it to be naturally heavier than non-ensemble approaches such as standard SAC. However, I would like to understand how much more computation such a method involves. In particular the computation of the Bellman weights requires multiple passes through the critic network, as does the UCB exploration policy, and I was wondering how much more computation this entails.In spite of the above aspects, I again remark that I quite enjoyed the paper. What is the goal of the paper?Investigating stability of fine-tuning BERT. What has been done before?Finetuning BERT exhibit a large training instability (Devlin et al., 2019; Dodge et al., 2020) i.e. training the same model with multiple random seeds can result in a large variance of the task performance.Few methods have been proposed to solve the observed instability (Phang et al., 2018; Lee et al., 2020), however without providing a sufficient understanding of why fine-tuning is prone to such failure.This work tries to answer the question : Why is fine-tuning prone to failures and how can we improve its stability?Another line of work investigates optimization difficulties of pre-training transformer-based language models (Xiong et al., 2020; Liu et al., 2020). Both works focus on pre-training and thus orthogonal to this work.What are the contributions of the paper?Investigating two common hypotheses for fine-tuning instability: catastrophic forgetting and small size of the fine-tuning datasets and demonstrating that both hypotheses fail to explain fine-tuning instability.Investigating fine-tuning failures on datasets from the popular GLUE benchmark and show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties, characterized by vanishing gradients, and (2) differences in generalization, characterized by a large variance of development set accuracy for runs with almost equivalent training loss.Presented a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches.What are the key techniques/experiments used to investigate this task?To investigate catastrophic forgetting - comparing language modeling perplexity for failed and successful fine-tuning runs of BERT.To investigate the effect of small size of the fine-tuning datasets - compare fine-tuning using  downsampled data sets for 3 epochs (standard) vs. more epochs.Visualizing gradient norms of different layers for a failed and successful run of BERT fine-tuning. Loss surface visualizations of failed and successful runs when fine-tuning BERTVisualizing development accuracy vs. training loss at the end of the training for all BERT models fine-tuned for the paperFine-tuning performance of (a) BERT, (b) RoBERTa, (c) ALBERT for different learning rates ± with and without bias correction (BC)What are the main results?Does catastrophic forgetting cause fine-tuning instability? No.Catastrophic forgetting occurs for both failed and successful models in the top layers of the network,  except for a much smaller increase in perplexity in case of successful models. Catastrophic forgetting typically requires that the model at least successfully learns how to perform the new task. However, this is not the case for the failed fine-tuning runs.Do small training datasets cause fine-tuning instability? No.Training on less data does indeed affect the fine-tuning variance. However, when one simply trains for as many iterations as on the full training dataset, one almost completely recovers the original variance of the fine-tuning performance.Observed instability is caused by optimization difficulties that lead to vanishing gradient : Development accuracy of failed fine-tuning runs is less or equal to that of the majority classifier, but also the training loss on the fine-tuning task is trivial. This suggests that the observed fine-tuning failure is rather an optimization problem causing catastrophic forgetting in the top layers of the pre-trained model.differences in generalization : Training on fewer samples affects the generalization of the model, leading to a worse development set performance on all three tasks. The role of training dataset size per se is orthogonal to fine-tuning stability. What is crucial is rather the number of training iterations.A simple but hard to beat baseline for fine-tuning bert (better standard deviation, better mean, and competitive maximum performance) Use small learning rates with bias correction to avoid vanishing gradients early in training. " Increase the number of iterations considerably and train to (almost) zero training loss.StrengthsThe approach is well motivated and well-placed in the literature. Paper claims look correct technically and are experimentally rigorous.Paper is easy and clear to read.Authors have made an attempt to make their findings reproducible.Findings apply not only to the widely used BERT model but also to more recent pre-trained models such as RoBERTa and ALBERT. These findings should benefit others as fine tuning BERT based models is a very common practice now.WeaknessesThe role of generalization has not been discussed rigorously.Different sets of GLUE datasets were used for different experiments without any explanation for the selection." Summary: This paper investigates the robustness problem of computer vision model. To study the model robustness in a controlled setting, the author introduces three new robustness benchmarks: ImageNet-R, StreetView StoreFronts and DeepFashion Remixed. Each of them address different aspects of distribution drift in the real world. The author evaluates seven popular hypotheses on model robustness in the community on the three new datasets and has found counter-example for most of them. Based on those new results, the author concluded that model robustness problem is multi-variate in nature: no single solution could handle all aspects yet. And future work should be tested on multiple datasets to prove robustness. Moreover, the author also proposes a new data augmentation method using perturbed image-to-image deep learning model to generate visually diverse augmentations.Significance: This paper is a solid work on the robustness problem. It systematically evaluated common hypotheses and successfully found counter-example on all of them except Texture Bias. The analysis is insightful and supported by the experimental results. The authors also provides three new carefully designed datasets for future work evaluation. While the study of using deep neural network to generate training image is not new, DeepAugmentation is still an innovative and practical way for data augmentation purpose.Question: On DeepFashion Remixed datasets, it seems large zoom has better result than medium zoom. Is there a good explanation for that, considering the original image has no zoom-in? Clarity: The author did a great job on explaining the idea, objective and approach.  Summary: Neural models that autoregressively generate mel spectrograms from text (or phonemes), such as Tacotron, have been used to generate high quality synthetic speech. However, they suffer from slow inference speed due to their autoregressive nature. To alleviate this, non-autoregressive models have been proposed, such as FastSpeech and Glow-TTS. The proposed model, BVAE-TTS, is yet another non-autoregressive speech synthesis model (outputting spectrograms), with two key advantages over the aforementioned models: (a) no autoregressive teacher model is required, as in FastSpeech, which simplifies training, and (b) fewer parameters are needed than in Glow-TTS, since there is no bijectivity constraint (allowing a more expressive architecture to be used). Models are compared with inference speed and MOS, and BVAE-TTS compares favorably on both both metrics when compared to Glow-TTS.Pros:1. The evaluation of the model is done well, in a clear way. LJSpeech is used, a dataset which is commonly used and easily accessible. MOS and inference speech are provided, and error bars are provided for MOS values. BVAE-TTS is compared to Glow-TTS and Tacotron 2 (one other non-autoregressive model, and one well-known AR baseline), and hyperparameters are provided. A single vocoder (pretrained WaveGlow) is used on all models, isolating the effect of the spectrogram prediction model used.2. Section 4.3, pertaining to using attention distributions to learn a duration predictor, is interesting and novel. Using positional encodings is standard and using a loss guide is unsurprising. However, while jitter and straight-through estimators are not uncommon, all of these things together make a compelling and novel approach to using attention to infer discretized durations and compensate for that train-test mismatch well. I believe that a similar technique could be used in other models as well.3. The model is an application of similar ideas from image synthesis, which is interesting, in that it demonstrates that some of those techniques work equally well for spectrogram synthesis. This sort of cross-modal result points to the strength of the method being used, which is a valuable data point for the research community.Cons:1. The biggest weakness of this paper, in my view, is that deciphering the model itself is quite difficult. Although the model bears resemblance to NVAE (for which code is released), understanding the fine details is tricky, and the paper does little to aid in that effort. In particular, understanding the exact layer inputs and outputs and parameters of the normal distributions being used is difficult, and I believe the paper would benefit significantly from a pseudocode explanation of the network. For example, I did not understand why the generative model produced both $\mu_l$ and $\Delta \mu_l$, and whether $\mu_l$ was predicted with a dense layer or was the accumulation of the prior BVAE stacks' $\Delta \mu_l$ values (and similar for $\Sigma$). I also wonder why the output of the attention layer is not provided to the encoder; perhaps there is a fundamental reason for this which I am missing, or perhaps this is simply an architecture choice.A very clear explanation of the method itself, perhaps as psuedocode for where the means and variances come from and which features they interact with and what it sampled when, would in my view make this among the top papers.Recommendation:  Accept. The paper is well written and results are strong, although I would prefer if the method itself were explained more clearly. Summary:The authors propose regularization-based pruning methods with the penalty factors uniformly increased over the training session. The first algorithm (GReg-1) sorts the filters by L1-norm and only applies the increasing regularization to the unimportant filters; the second one (GReg-2) applies the increasing regularization to all the filters. The experiments are very extensive and convincing to support the claimed contributions.Strengths:1.The idea of utilizing Hessian without knowing its values is interesting to me. Theoretical analysis in Sec. 3.3 looks sound, which should be the main theoretical contribution of this work.2.Empirical performances are promising, especially the ImageNet one in Tab. 3 and 4. 3.Good performance. On CIFAR-10 and ImageNet, they evaluate the proposed methods with popular deep neural networks, reporting encouraging performances.4.The methods are easy to implement with current deep learning tools and can work in both the structure pruning and unstructured pruning cases, which is a plus. 5.The authors present theory analyses to show this will cause the filters to separate owing to their different underlying Hessian structure, thus achieving exploiting the Hessian information without knowing their specific values. 6.Generally, this paper is well-written with sound proofs for their formulas, although there are some small problems (see the weakness below) that the authors may want to resolve.Weakness:1.In Sec. 3.3, it says h11 > h22 leads to r1 > r2, where r is defined by the new magnitude over the old one. But in Fig. 1, the plot is the normalized magnitude stddev. How these two are related to each other is not so straight to me. Could the authors give more explanation about this since it is said in the paper that Fig.1 is an empirical validation for the analysis in Sec. 3.3?2.Some typos and small glitches: importance-based one focus more -> focusesAppendix: In the last sentence of A.1 our results (Tab. R3 and R4) (I didnt find R3, R4). Please clarify it.Appendix: Footnote in Tab. 6, the references of [7] [54] seem pointing to nowhere. This paper investigates kernel ridge-less regression from a stability viewpoint by deriving its risk bounds. Using stability arguments to derive risk bounds have been widely adopting in machine learning. However, related studies on kernel ridge-less regression are still sparse. The present study fills this gap, which, in my opinion, is also one of the main contributions of the present study. Pros:1.  As mentioned above, this study presents some novel research into kernel ridge-less regression from a stability viewpoint.2. The study presented here brings some novel insights into the relationship between minimizing the norm of the ERM solution and minimizing a bound on stability and also reveals the role that the condition number of the kernel matrix plays in kernel ridge-less regression, see formula (6). 3.  The paper is well presented and well polished. The analysis conducted in this paper seems to be sound.Just one minor concern: what would happen if the boundedness assumption on the output variable, which excludes the most common Gaussian noise, is not imposed? I understand that this condition is common in learning theory but are expecting more comments.     The authors propose methodology for sharing learned differencing coefficients for estimating spatial derivatives between multiple spatio-temporal modeling tasks. They show that increased number of tasks improves learning. Additionally, the authors propose a meta-initialization procedure by which the differencing coefficients are initialized to values obtained from synthetic data. They show that this initialization procedure improves performance. It seems to me that the 'share-ability' of the spatial differencing coefficients requires that each task is operating on the same spatial manifold with same or similar measurement locations. Toward that end, the initial differencing coefficients appear to be chosen using a flat 2D topology. The appropriateness of this choice is probably responsible for the improvements offered by the proposed approach. However, if the initial topology is poorly chosen, or the different tasks have significantly different topologies one would expect poor performance or negative transfer. How to identify these degenerate cases seems to be an open problem.Aside from my rehashing the perennial question of 'when does task sharing help?' I found this paper to be well written, and a solid contribution to the field of physics-informed machine learning. Certainly, many applications include multiple different types of measurements at common locations which might be expected to transfer. In the paper, the authors propose to minimize the discrepancy between pairs of (conditional) neural axioms to align the embedding spaces of different KGs. This method is justified by the authors' study of all kinds of OWL2 properties. The author also studied the influence of margin $\lambda$ on less constrained/long-tail entities. The authors conducted experiments by adding the proposed model on top of the best models for entity alignment. The results are mixed, but the proposed model improves the SEA and RDGCN consistently. Reasons to accept:1. This paper provides a theoretic point of view of the entity alignment task, which was mostly studied in empirical methods. The idea to align the axioms by minimizing Wasserstein distance is well-justified.2. The experiment results are in favor of the intuitions.2. The method described in this paper can be in principle adapted to any previous and future EEA scoring functions. Reasons to reject:The idea of using adversarial training to align spaces, especially cross-lingual spaces, is based on the assumption of the large overlap between KGs. For KGs that are on very different domains, this method may include errors, as two heterogeneous KGs do not naturally fit in one unified space. The influence of overlap on this method is not well-studied. All of KGs used in the experiments are general domain KGs.  The paper is quite clear and is well-motivated by a practical situation of having a classifier that is a black box and that is evaluated by various metrics that one would like to assess as efficiently as possible. The paper demonstrates that the simpler task of using active learning to identify examples for labeling that would most reduce the uncertainty in metrics of interest is more effective than the more general task of using active learning to learn to predict the labels that the original classifier would give and using those results to calculate the metrics of interest.The only cons that I see are that the paper lacks some obvious explorations that I think would be quite valuable and informative:1. What is the variation in the sequences of points chosen for labeling depending on the metric that is being calculated?2. Calculating the ROC curve does not require classifier internal structure. It only requires some continuous output representing class membership rather than just a discrete indication.3. Remark 3.2: Simplicity seems an insufficient reason to have the binary classifier and Bayesian Neural Network have the same structure. This should be explored and at least a summary of performances given for variations on this.4. To sample the point that is best for multiple metrics, equation (9) is used, which calculates the sum of the mutual informations between the BNN parameters and the metrics. Is it obvious that using the sum is the best way? I wonder if using something different like the point that most increases the minimum mutual information may work better.5. In algorithm 1, step 6, note that $\mathcal{C}_{\eta}^{t-1}$ is being trained on $\mathcal{D}_l^{t-1}$.6. In algorithm 1, step 8, $\mathcal{S}_l^{t-1}$ is undefined. Do you mean to refer to $\mathcal{S}_l$ from step 7?7. In figure 1, the average metrics seem to vary quite significantly with the number of oracle queries. Since you are measuring the average, I expected the decrease to be relatively smooth. How do you account for the significant variation?8. The Gal and Ghahramani paper is listed twice in the bibliography. The paper aims at justifying the success of few shot learning methods that work based on finding a shared representation among a number of tasks. A serious theoretical challenge is that, even if we assume such a representation exists (and belongs to a predefined class of functions with controlled capacity), we would still need to assume something that connects the source tasks with the target task. Previous work has considered "i.i.d. tasks", however, the obtained bounds were not natural in the sense that we don't have the usual decrease in the error as we increase the size of the training set of the source tasks. Under a different set of assumptions, the authors show that, in a sense, one can "fully" exploit the training data from the source tasks. Multiple settings are considered, including linear least squares with a low dimensional shared representation, generalization to non-linear representation, high-dimensional (low norm) representation, and neural networks. The obtained results are interesting, and some intuitions are provided about the assumptions and the results. These discussions are sometimes short/dense, making it hard for the reader to follow the details. This is perhaps due to the page-limit.Perhaps a basic intuition for the first result (linear case with a low dimensional shared representation) is that if the source tasks are somewhat "uniform" in the low dimensional representation and the number of tasks is large enough, then in a sense they will "cover" the low dimensional space, and the learned representation will be in a sense accurate in all directions; so if the target tasks is also selected somewhat uniformly, then the learned representation will work well. Is this correct? In any case, I suggest that the authors add more of these intuitions to the paper, especially for the other problems including the neural net and the high-dimensional linear representation.It will be interesting to do some experiments on a real world data set and see the extent to which the assumptions are realistic. It will help to see the actual value of the upper bounds on a synthetic data set that adheres the assumptions.    # General statementsThis paper introduces an interesting parallel between SDEs and GANs, and pushes the analogy to its practical implications as a way to learn neural SDEs. Globally, I found the paper a very good read, although it sometimes lack the details that could be useful for a non-specialist. This could and should easily be corrected by a few sentences here and there.Although I see that everything is included in the supplementary material to actually reproduce all experiments. Still, I believe that there could be some improvements to do. In particular:  - I think that the main text / the supplementary could be augmented with a short mention regarding the network structures. even when reading the code, it is not clear how time is handled (since the nets input not only tensors like X_t or H_t, but also time). Should I understand that the raw time stamp is simply concatenated to the other input ?  - you didn't clearly mention all the tricks and experiments you tried out. It is not clear to me to what extent the performance you report depends on the network structures you picked.All in all, I recommend acceptance.## IntroductionHer are some comments along the way:* I could regret that no general background is given for the curious reader that is not already a specialist in SDE or even ODE## SDEs as GANs* please explain the "Initial condition" statement better: why is it important that there be an additional source of noise here ?* You are mentioning X and H as the (strong) solutions to your SDEs (1) and (2). Are they guaranteed to exist ? I guess the Lipschitz condition you assumed is enough for this. Is that the case ?* In the "training data" item, H_0 is a function of Y_0. i/ is this Y_0 defined as above in "initial condition" ? ii/ Is there a reason H_0 is not a function of z_0 ? iii/ It makes the decision D done on training data actually to depend on \theta, and not only on \phi (through Y_0=l_\theta(\zeta_\theta(V)). is that ok ? The item "initial condition and hidden state" does not make that point clearer to me.* Could you briefly describe gradient penalty, instead of only refering to (Gulrajani 2017) ? That would make the paper more self-contained## Efficient computation* Section 3.1 (rough adjoint equation) is harder for me. I'm ok with the adjoint equation. Then, forgive me but I'm more uncomfortable with the (W, \mathbb{W}) couple. What is meant exactly by "sampling" them ? It means drawing (s,t) and computing the related (W, \mathbb{W}) ? For each, you compute the solution to the SDE ?* now, assuming you get your a_t process. How do you actually use it to perform optimization ? Are you computing the gradient of the parameters wrt a_t and then averaging over time ? Basically, I need some more information on the general scheme to understand 3.1, assuming the adjoint equation is understood.## Experiments* The "weights" dataset is not super clear. Is the data actually a: 10xPx100 tensor, where P is the number of parameters ? (what is the value of P ?) Just to make sure: the same net is trained for all weights (what I assume), or is it a different net per weight ?## Considerations* in the "lipschitz regularisation" of your 2.3 section, you mention using gradient penalty, requiring adjoint, etc. But here, I understand that you actually didn't use these sophistications that were introduced in section 3.1 ? I think you should rephrase a bit here and there to actually better reflect these findings.## References* References are inconsistent. Sometimes abreviations, sometimes full names.* Françios-Xavier -> François-Xavier This paper describes a number of improvements over the non-autoregressive FastSpeech TTS model.One of the trends that TTS has seen since Tacotron was announced is a retreat from fully end-to-end modeling for TTS.  FastSpeech2 is another entry in that.  The text input is first converted to phones, and prosodic information can be inferred from text (in a unified system) or can be conditioned by a user.  All of these are valuable improvements for quality and customizability.  In particular, FastSpeech takes an interesting modular approach to conditioning on pitch, duration and energy for its prosody component.  It relies on ground truth predictors during training, but learns to infer these via multitask training so they are not required during inference.   The use of a dense and continuous pitch representation as a pitch spectrogram is interesting as well.  This could be a limitation for customizability as a user would be more comfortable to provide a pitch target rather than specifying a pitch spectrogram.This paper is clearly written and includes sufficient detail for reproducibility.  The evaluation is convincing especially as it relates to training and inference time.Minor comment: In Table 2 the FastSpeech2s training time number is omitted.  This is because it is not directly comparable to FastSpeech2 or the other training numbers -- it includes training a neural vocoder while the others do not.  However, since the discussion of training time as a valuable measure is already begun, I would suggest that the authors include this number, with the explanation that it is not comparable. (or include vocoder training time along with the other methods.)Comment: The one-to-many TTS problem --- where one lexical utterance can be produced in many different correct ways -- is only partly addressed here for two reasons. 1) there is variation other than duration, energy and pitch that can vary between realizations of the same utterance, voice quality, background and channel noise, and speaker effects.  2) more significantly, the one-to-many problem is only approached during model training.  During inference, the process is still deterministic given the state of the model.    The inference side of the problem is important for objective model evaluation. This is a theoretical paper. The paper studies convergence and optimality of the actor-critic with the function approximation in a single-timescale setting. The proposed single-timescale AC applies PPO for the actor and updates the critic by applying the Bellman operator to the critic one time. Despite the actor-critic coupling, the paper establishes global convergence with sublinear rates for the proposed AC method in both linear and neural network function approximation.  Strong points:(1) The paper is well-written, and the motivation is easy to follow. The paper also provides detailed literature on related works. (2) The studied AC is practical in the sense that both the actor and the critic can take function approximation and their updates work in the one-timescale. The proposed scheme could complement the previous study of the AC methods in the two-timescale.(3) The provided convergence theory could be some new insights into dealing with the coupling of the actor and the critic. The provided intuition makes sense to me although I didnt get time checking proof details. Weak points and comments:(1) Although the paper provides a general setup for the one-timescale AC method, it is worth providing some generic examples to explain the theory, e.g., the energy-based policy with direct parametrization. (2) The theory seems to be specific to the energy-based policy. Are there any other types of policies that can also be considered? If not, it would be helpful to comment on the importance of the energy-based policy in practice or from the theoretical point of view? (3) The proposed AC methods rely on the population quantity, e.g., expectation over state-action visitation probability. How practical are they? Or how is the proposed AC related to practical one-timescale AC methods mentioned in the Introduction? (4) In (2.7), the critic depends on the current policy at k+1. What do you mean by they are updated simultaneously in the Abstract?(5) It would be helpful to make a table comparing the proposed AC with others in terms of sampling assumptions, policy classes, and convergence. I believe my concerns can be addressed during reviewing. For me, the development of this paper is new, and I am more inclined to agree with the acceptance. Summary:This paper proves that the proposed single-timescale actor-critic algorithm with KL regularization converges to a globally optimal policy when appropriate function approximation is used. The authors define a specific form of actor/critic updates, which are PPO update for the actor and the single application of Bellman evaluation operator for the critic. Given such an algorithm, the authors provide an upper bound of the regret for linear and deep neural network function approximation (although there are some strong assumptions, e.g. no approximation error). Reasons for score:Overall, I vote for strong acceptance. While I am not capable of fully checking the proofs, I understand the importance of the result. Based on the mathematical rigorousness of the main text and the proof sketch, I assume that the paper is technically correct. Given that the paper is technically correct, it is the first convergence result for a single-timescale actor-critic algorithm as far as I know (and the paper claims so). This work will be very significant as it theoretically grounds the single-timescale actor-critic algorithms, where the state-of-the-art deep RL algorithms are mostly single-timescale actor-critic algorithms.PROS:* The paper (at least the main text and the proof sketch) is well written.* First result on convergence rate and global optimality of single-timescale actor-critic, which is very significant.CONS:* No further discussions based on the results for better design of an actor-critic algorithm. It would be helpful if there are more discussions about the algorithm choice, i.e. being off-policy, broken assumption, etc.Questions:* I still do not understand how this result can be applied to off-policy results, where we have $\mathbb{P}^{\pi_{\theta_{k+1}}}$ in e.g. equation 3.4 and 3.7. How can the samples be reused if $(s,a)$ are not from $\rho_{k+1}$ but from previous $\rho$s?Typo:* Specifically, after K + 1 actor updates, we are interest -> we are interested? : before eq (4.1). I think this a very good contribution to ICLR given the topic and the quality of the submission (originality, contribution to the state of the art, experimental evidence, e Some of the strong points of the submission are summarized as follows, along with some points for clarification1.Enabling training on any embedded device (SoC, FPGA, micro-controller) is one of the holy grails for edge AI and IoT. As the authors mention, this also intersects with other domains such as federating learning privacy by design systems. The authors provide and ample motivations of the importance of this work, and some of the applications edge AI might enable, as well as the current challenges.2.The state of the art (despite the previous comment) contextualizes the subject matter in a succinct but comprehensive manner. Although there are certain aspects that could be improved, such as including a table outlining in a clearer manner the contributions of the authors in this context.3.The comparison with the traditional training method is clear. However, I would like to know if the authors have made an ablation study to assess whether or not the use of batch normalization would have an effect on the accuracy of the proposed models. Do you provide experimental evidence of the lack of degradation due to the use of l1 batch normalization? These two aspects are not mentioned in the next nor provided in the supplementary sections.4.The experimental design is good, showing a careful analysis to validate the proposal and several ablation studies to assess the memory footprint reductions and its effects on the training of various models5.The foundations for the method are presented in great detail in a formalized manner and provides sufficient elements (i.e. experiments) to assess the validity of the proposed approach. Pros: - This paper is very well-written and motivated. - The train of thoughts is explained very clearly, such that I (admittedly not being  an expert in this field) was able to follow. - The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws (for $\lambda =0$) in the gradient flow is very elegant. - By adapting a modification of the gradient flow from previous works that accounts for the discrete approximation of SGD, the derived theory was able to predict the behavior of the relevant quantities during training to a remarkable accuracy.Cons: I did not find any major drawbacks of this work. Just two small questions:- Using $\elll^2$ regularization on a problem with scale symmetry does not seem to make sense, because the cost function$$ \mathcal{L}(\theta) + \lambda \|\theta\|^2 $$ will likely not have a minimizer as soon as $\lambda >0$. Reducing the magnitude of any $\theta$ that is optimal for $\mathcal{L}$ reduces the regularization, but in the limit of $\theta=0$ the loss might jump up. Thus, the costs are not lower semi-continuous.- Additionally, the scale symmetry seems to naturally lead to a discontinuous loss function $\mathcal{L}$. Is there no problem in even defining the gradient flow for such a function? Which properties of $\mathcal{L}$ do you need to derive the continuous gradient flow equations?Overall, I really enjoyed reading this paper. Since I am not an expert in the field, I cannot really judge the novelty/contribution, but aside from this aspect, I clearly recommend the acceptance of this work. ------------ There is a typo in Section 6.1 "graident"- I stumbled upon the NeurIPS 2020 paper "Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate" by Li, Lyu, and Arora. Based on the abstract, this seems to be a relevant related work.  This paper presents a function space view of 2-layer ReLU neural networks and the implicit regularization associated with full-batch gradient descent for various initializations of the weights. In past work, it was shown that for very wide neural networks, the *global* minimizer of a loss plus weight decay regularizer corresponds amounts to regularizing the total variation of the second derivative of a function (and related results in higher dimensions). This paper explores the effective regularization associated with performing gradient descent on an *unregularized* squared error loss. The resulting regularizer is akin to a weighted 2-norm of the second derivative, where the weighting function depends on the distribution of the initial weights. This result addresses an important open problem, provides interesting insights into the role of initialization and implicit bias associated with training, and is supported by nice illustrations.  The literature review is strong and covers much of the relevant literature. Much of the analysis seems to depend on the optimization occurring in the "kernel regime". It is unclear when this is or is not a reasonable model. This issue is particularly salient in light of the comparison of the results with the work of Savarese et al. Savarese considers explicit 2-norm weight decay regularization. Although the paper under review considers unregularized losses, there are multiple studies showing that gradient descent initialized near zero induces 2-norm regularization. So a natural thought is that the Savarese result would also extend (with some non-trivial technical work) to unregularized settings with gradient descent. With this in mind, I expected to see the Savarese norm as a special case of the results of the paper under review, but this is not the case. In particular, the function space regularization calculated in Savarese is NOT an RKHS norm, while the paper under review claims the function space regularization they find IS a kernel norm. I would like to see a more detailed discussion of this potential discrepancy. Section C.4 does not make this clear. The paper develops a hierarchical reinforcement learning algorithm and analyzes its behaviour in four robotic manipulation and navigation tasks. The approach is based on a two-level hierarchy, *scheduler* at the top and *worker* at the bottom. This is similar to other approaches in the literature and the algorithm uses many ideas and elements from existing algorithms. However, these ideas and elements are combined in a novel and well-justified manner. The result is an algorithm that yields good results in a range of problems. The experiments are well done. The paper is generally organised well and written clearly. Relevant literature is reviewed well. The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm, in particular, the options that are found. Section 4.4 is useful but very short. It describes only two options. Perhaps such an analysis can be added to the appendix.In the proposed algorithm, the scheduler outputs an option every K steps in the environment. It would be reasonable to question whether more flexibility would be useful here, one that allows for varying option durations.The authors state in Section 2 that they 'will use the terms goal, option, and skill interchangeably.' In the literature, these terms refer to related but different concepts. Using them interchangeably is not good scientific practice.I did not find Figure 1 particularly useful. The simple and intuitive structure of the algorithm does not come through in the figure.   The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed ("self-aware") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a "progress monitor" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction.STRENGTHS+ The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a "progress monitor" allows the method to reason over whether the navigational progress matches the instruction.+ The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture.+ The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results.WEAKNESSES- The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next.- The paper emphasizes the use of images, the visual grounding reasons over visual features.- The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal.C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, Learning to parse natural language commands to a robot control system, in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012.S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, Learning models for following natural language directions in unknown environments, in Proc. IEEE Intl Conf. on Robotics and Automation (ICRA), 2015F. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, Inferring maps and behaviorsfrom natural language instructions, in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014.- While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)`J. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. "Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots," In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017- The paper misses the large body of literature on grounded language acquisition for robotics.QUESTIONS* What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016?* Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)? + Using a generative model as the surrogate distribution for kernel two-sample test is novel+ An important and new application of deep generative models + Strong experiments on synthetic and real-world time series data sets+ Very clear writing and explanation of the idea - reply sample segments from both directions (past and future) while in the practical setting, CPD is usually sequential and in one directional- lack theoretical understanding of the limit of the neural-generator in the kernel two-sample test A new approach to choose a kernel to maximize the test power, for the kernel change-point detection. This provides an extension to the two-sample version of the problem (Gretton et al. 2012b, Sutherland et al. 2017). The difficulty is caused by that there is very limited samples from the abnormal distribution. The idea is based on choosing a surrogate distributions using generative model. The idea makes sense although there seems to be not much detail in how to choose the surrogate distribution. There is a mechanism to study the threshold. Real-data and simulation demonstrates the good performance. I think the idea is really interesting and I am impressed by the completeness of the work. In this paper, the task of performing meta-learning based on the unsupervised dataset is considered. The high-level idea is to generate 'pseudo-labels' via clustering of the given dataset using existing unsupervised learning techniques. Then the meta-learning algorithm is trained to easily discriminate between such labels. This paper seems to be tackling an important problem that has not been addressed yet to my knowledge. While the proposed method/contribution is quite simple, it possesses great potential for future applications and deeper exploration. The empirical results look strong and tried to address important aspects of the algorithm. The writing was clear and easy to follow. I especially liked how the authors tried to exploit possible pitfalls of their experimental design. Minor comments and questions:- Although the problem of interest is non-trivial and important, the proposed algorithm can be seen as just a naive combination of clustering and meta-learning. It would have been great to see some clustering algorithm that was specifically designed for this type of problem. Especially, the proposed CACTUs algorithm relies on sampling without replacement from the clustered dataset in order to enforce "balance" of the labels among the generated task. This might be leading to suboptimal results since the popularity of each cluster (i.e., how much it represents the whole dataset) is not considered. - CACTUs seems to be relying on having random scaling of the k-means algorithm in order to induce diversity on the set of partitions being generated. I am a bit skeptical about the effectiveness of such a method for diversity. If this holds, it would be interesting to see the visualization of such a concept.- Although only MAML was considered as the meta-learning algorithm, it would have been nice to consider one or more candidates to show that the proposed framework is generalizable. Still, I think the experiment is persuasive enough to expect that the algorithm would work well at practice.- Would there be a trivial generalization of the algorithm to semi-supervised learning?   This paper introduces a new dataset and method for chatbots. In contrast to previous work, this paper specifically probes how well a dialogue system can use external unstructured knowledge. Quality:Overall, this is a very high-quality paper. The dataset is developed well, the experimental setup is well thought-through and the authors perform many ablation studies to test different model variants. The main criticism I have would be that the human evaluation is rather simple (rating 1-5), I would have expected more fine-grained categories, especially ones that relate to how much knowledge the system uses (I appreciate the "Wiki F1" metric, but that is an automatic metric). As it is, the human evaluation shows that most of their contributions are not appreciated by human annotators. Further, the paper ends a bit abruptly, I would have expected a more in-depth discussion of next steps.Clarity:The description of the work is clear in most places. I particularly like the abstract and introduction, which set up the rest of the paper nicely. In some places, perhaps due to space restrictions, method descriptions are a bit too short.Originality:The paper is fairly original, especially the aspect about specifically using external knowledge. The authors could have been more clear on how the work differs from other work on non-goal directed dialogue work though (last paragraph of related work section).Significance:The dataset is really well-developed, hence I believe many working in the dialogue systems community will re-use the developed benchmark and build on this paper.More detailed comments:- Missing reference for goal-oriented dialogue datasets: Wen et al. 2017, A Network-based End-to-End Trainable Task-oriented Dialogue System, https://arxiv.org/abs/1604.04562- How does the proposed dataset differ from the Reddit and Wikipedia datasets discussed in the last paragraph of the related work section? This should be explained.- Page 3, paragraph "Conversational Flow": what is the maximum number of turns, if the minimum is 5?- Page 3, paragraph "Knowledge Retrieval": how were the top 7 articles and first 10 sentences choices made? This seems arbitrary. Also, why wasn't the whole text used?- Page 3, paragraph "Knowledge Selection and Response Generation": how do you deal with co-reference problems if you only ever select one sentence at a time? The same goes for the "Knowledge Attention" model described in Section 4.- Page 3, paragraph "Knowledge Selection and Response Generation": how often do annotators choose "no sentence selected"? It would be interesting to see more such statistics about the dataset- Section 4.2: did you run experiments for BPE encoding? Would be good to see as this is a bit of a non-standard choice.- Section 4.2: it would be good to explain the Cer et al. 2018 method directly in the paper- Section 4.2: is there a reference for knowledge dropout? Also, it would be good to show ablation results for this.- Section 5.1: why did you choose to pre-train on the Reddit data? There should be some more in-depth description of the Reddit dataset to motivate this choice.- Section 5.1: what is the setup you use for multi-task learning on SQuAD? Is it just a hard parameter sharing model, or?- Section 5.3: as stated above, the human evaluation is a little bit underwhelming, both in terms of setup and results. I'd expect a more fine-grained way of assessing conversations by humans, and also an explanation of why the retrieval performer without knowledge was assessed as being on par with the retrieval transformer memnet.- Section 5.3: I assume higher=better for the human scores? This should be made explicit.- Section 5.3: Have others used the "F1 overlap score"? If so, cite.- Section 5.3: I don't understand the argument that the human evaluation shows that humans prefer more natural responses. How does it show that?- Section 5.3: The Wiki F1 score is kind of interesting because it shows to what degree the model uses knowledge. But the side-by-side comparison with the human scores shows that humans don't necessarily prefer chatbot models that use a lot of knowledge. I'd expect this to be discussed, and suggestions for future work to be made accordingly.- Section 6: The paper ends a bit abruptly. It's be nice to suggest future areas of improvement. The paper considers the problem of adversarial examples in (mostly high-dimensional) multi-class classification problems. Although the results are not specific necessarily to very high dimensional data or two images, the paper mostly uses images as a running example, and so will I in the review. Assume that the data all lies in the unit box in R^n ([0, 1]^n). A multiclass classifier with K classes partitions the unit cube into K parts, each part corresponding to a given class. There are distributions \rho_c associated with each class and there is a bound on their density given by U_c and the fraction of examples of class c is f_c. And (eps, p) adversarial point y for some point x is such that |x - y|_p &lt;= \eps and the classifier classifies x &amp; y differently. The paper shows that under this modeling assumption adversarial examples are inevitable. The results mostly use standard (but deep) results from probability theory. The technical proofs themselves are not particular difficult (provided one has the right background). I think the overall implications are interesting, and I will recommend the paper be accepted. However, I also feel that this is a missed opportunity. To some extent the authors do try to have some high-level discussion about adversarial examples, but I think this could be expanded on more. For instance, why should it be assumed that an example that is \eps far should automatically have the same class label? Surely, being "eps"-far away is an equivalence relation, thus this would mean that all the hypercube would have to be labeled by the same class. This is clearly not the case. One plausible explanation is that if you take two points that are in two different classes, then any sequence of points that take one to the other with the property that each adjacent pair is at most \eps far away, must have the property that some intermediate mass have negligible chance of being a "natural" image. On the other hand, doesn't the fact that humans are not susceptible to most adversarial examples, imply that adversarial-example resistant classifiers exist? My own feeling is the assumption that U_c is bounded is the strongest assumption that may not hold true with real data. In any case, the paper has enough technical content to merit acceptance and I hope the open review forum will lead to a fruitful discussion about some of these questions.--Minor comments:Page 6 (just after Thm 2). Isn't the bound in Eqn. (5) true for all \ell_p norms for p \geq 2? (not just \ell_2 as the sentence says)Paras on Page 6 (just below Thm 2). It would be more pleasant if equation x could be replaced by Eq. (x) or Equation (x). Para in Sec 7 on Unbounded density: Clarify what norm you mean when you talk about \eps/2 perturbations.Thm 5: Seems odd to have a theorem about MNIST. Surely the result is a lot more general!!! This paper describes a new large scale dataset of aligned MIDI and audio from real piano performances and presents experiments using several existing state-of-the-art models for transcription, synthesis, and generation. As a result of the new dataset being nearly an order of magnitude larger than existing resources, each component model (with some additional tuning to increase capacity) yields impressive results, outperforming the current state-of-the-art on each component task. Overall, while the modeling advances here are small if any, I think this paper represents a solid case study in collecting valuble supervised data to push a set of tasks forward. The engineering is carefully done, well-motivated, and clearly described. The results are impressive on all three tasks. Finally, if the modeling ideas here do not, the dataset itself will go on to influence and support this sub-field for years to come. Comments / questions:-Is MAPS actually all produced via sequencer? Having worked with this data I can almost swear that at least a portion of it (in particular, the data used here for test) sounds like live piano performance captured on Disklavier. Possibly I'm mistaken, but this is worth a double check.-Refering to the triple of models as an auto-encoder makes me slightly uncomfortable given that they are all trained independently, directly from supervised data. -The MAESTRO-T results are less interesting than they might appear at first glance given that the transcriptions are from train. The authors do clearly acknowledge this, pointing out that val and test transcription accuracies were near train accuracy. But maybe that same argument could be used to support that the pure MAESTRO results are themselves generalizable, allowing the authors to simplify slightly by removing MAESTRO-T altogether. In short, I'm not sure MAESTRO-T results offer much over MAESTRO results, and could therefore could be omitted. The paper addresses the challenge of using neural networks to generate original and expressive piano music.  The available techniques today for audio or music generation are not able to sufficient handle the many levels at which music needs to modeled.  The result is that while individual music sounds (or notes) can be generated at one level using tools like WaveNet, they don't come together to create a coherent work of music at the higher level.  The paper proposes to address this problem by imposing a MIDI representation (piano roll) in the neural modeling of music audio that serves as an intermediate (and interpretable) representation between the analysis (music audio -&gt; MIDI) and synthesis (MIDI -&gt; music audio) in the pipeline of piano music generation.  In order to develop and validate the proposed learning architecture, the authors have created a large data set of aligned piano music (raw audio along with MIDI representation).  Using this data set for training, validation and test, the paper reports on listening tests that showed slightly less favorable results for the generated music.  A few questions and comments are as follows.  MIDI itself is a rich language with ability to drive the generation of music using rich sets of customizable sound fonts.  Given this, it is not clear that it is necessary to reproduce this function using neural network generation of sounds.  The further limitation of the proposed approach seems to be the challenge of decoding raw music audio with chords, multiple overlayed notes or multiple tracks.  MIDI as a representation can support multiple tracks, so it is not necessarily the bottleneck.  How much does the data augmentation (audio augmentation) help? This paper combines state of the art models for piano transcription, symbolic music synthesis, and waveform generation all using a shared piano-roll representation.  It also introduces a new dataset of 172 hours of aligned MIDI and audio from real performances recorded on Yamaha Disklavier pianos in the context of the piano-e-competition.  By using this shared representation and this dataset, it is able to expand the amount of time that it can coherently model music from a few seconds to a minute, necessary for truly modeling entire musical pieces.Training an existing state of the art transcription model on this data improves performance on a standard benchmark by several percentage points (depending on the specific metric used).Listening test results show that people still prefer the real recordings a plurality of the time, but that the syntheses are selected over them a fair amount.  One thing that is clear from the audio examples is that the different systems produce output with different equalization levels, which may lead to some of the listening results.  If some sort of automatic mastering were done to the outputs this might be avoided.While the novelty of the individual algorithms is relatively meager, their combination is very synergistic and makes a significant contribution to the field.  Piano music modeling is a long-standing problem that the current paper has made significant progress towards solving.The paper is very well written, but there are a few minor issues:* Eq (1) this is really the joint distribution between audio and notes, not the marginal of audio* Table 4: What do precision, recall, and f1 score mean for notes with velocity?  How close does the system have to be to the velocity to get it right?* Table 6: NLL presumably stands for Negative Log Likelihood, but this should be made explicity* Figure 2: Are the error bars the standard deviation of the mean or the standard error of the mean? Summary:The authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. By doing so, they are able to profit from the corresponding literature and propose a few methods that are variants of SGD. The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.EvaluationThis is a very good paper and I cannot but recommend its acceptance:It is clear and well written. It has the right level of balance between theory and experiments. Theoretical results are far from trivial. I haven't seen something similar.The authors's do not make overstatements: they do not claim to have solved the GAN problem, but they do report improvements which are due to a thorough analysis (see above points). These results are much appreciated. This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve. VIP have been very successful in solving min-max style problems. Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. The authors look at a simple GAN setup where both the generator and the discriminator are linear models. In this case two kinds of gradient updates can be derived. First are simultaneous updates, and the other is alternated updates. The authors show that simultaneous updates are not even bounded and diverge to infinity, whereas alternated updates are more stable and stay bounded, but need not necessarily converge. However, I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case. The second key idea is the use of extra-gradient updates. Extra-gradient updates perform an "extra" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the "extra step".  This extra-gradient method is a close approximation to Euler's method, though far more computationally efficient.  However, the extragradient step requires one to calculate gradient twice, which can be expensive in large models. For this reason, the authors suggest using gradients from past as the "extragradient" in the extragradient method. For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence.  Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation. Authors also show how one can use these ideas using other first order methods such as ADAM instead of SGD. Experiments are shown on the DCGAN architecture. On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance Summary:The paper presents a novel method for predicting organic chemical reactions, in particular, for learning (Robinson-Ingold's) ''arrow pushing" mechanisms in an end-to-end manner. Organic molecules consist of covalent bonds (that's why we can model them as molecular graphs), and organic reactions are recombinations of these bonds. As seen in organic chemistry textbooks, traditional chemists would qualitatively understand organic reactions as an alternating series of electron movements by bond breaking (bond cleavage) and bond forming (bond formation). Though now quantum chemistry calculations can give accurate quantitative predictions, these qualitative understanding of organic reactions still also gives strong foundations to consider and develop organic reactions. The proposed method tries to learn these series of bond changes directly through differentiable architectures consisting of three graph neural networks: 1) the one for determining the initial atom where electron movements start, 2) the one for representing state transitions from  the previous bond change to the next, and 3) the one for determining when the electron movements end. Experimental evaluations illustrate the quantitative improvement in final product prediction against existing methods, as well as give chemical intuitions that the proposed method can detect a class of LEFs (linear electron flows).Comment:- This study is a quite interesting contribution because many existing efforts focus on improving differentiable architecture design for graph transformation and test it using chemical reaction data without considering what is learned after all. In contrast, this paper gives the clear meaning to predict "arrow pushing" mechanism from chemical reaction data and also makes sure the limitation to LEFs that are heterolytic. Traditional graph rewrite systems or some recent methods directly borrowing ideas from NLP do not always give such clear interpretations even though it can somehow predict some outputs.- The presentation of the paper is clear and in very details, and also provides intuitive illustrative examples, and appendix details on data, implementations, and related knowledge. - The architecture is based on graph neural networks, and seem natural enough. Basically, I liked overall ideas and quite enjoyed them but several points also remained unclear though I'm not sure at all about chemical points of view.1) the state transition by eq (2)-(4) seems to assume 1-st order Markovian, but the electron flow can have longer dependence intuitively. Any hidden states are not considered and shared between these networks, but is this OK with the original chemical motivations to somehow model electron movements? The proposed masking heuristics to prevent stalling would be enough practically...? (LEF limitations might come from this or not...?)2) One thing that confuses me is the difference from approaches a couple of work described at the beginning of section 'Mechanism prediction (p.3)', i.e. Fooshee et al 2018; Kayala and Baldi, 2011, 2012; Kayala et al, 2011. I don't know much about these studies, but the paper describes as "they require expert-curated training sets, for which organic chemists have to hand-code every electron pushing step". But for "Training" (p.6) of the proposed method, it also describes "this is evaluated by using a known electron path and intermediate products extracted from training data". Does this also mean that the proposed method also needs a correct arrow pushing annotations for supervised learning?? Sounds a bit contradicting statements?3) Is it just for computational efficiency why we need to separate reactants and reagents? The reagent info M_e is only used for the network for "starting location", but it can affect any intermediate step of elementary transitions intuitively (to break through the highest energy barrier at some point of elementary transitions?). Don't we need to also pass M_e to other networks, in particular, the one for "electron movement"? The paper presents a novel end-to-end mechanistic generative model of electron flow in a particular type of chemical reaction (Linear Electron Flow reactions) . Interestingly, modeling the flow of electrons aids in the prediction of the final product of a chemical reaction over and above problems which attack this product prediction problem directly. The method is also shown to generalize well to held-out reactions (e.g. from a chemistry textbook).General Impressions+ For me the biggest selling point is that it improves performance in predicting the ultimate reaction outcome. It should do because it provides strictly more supervision, but its great that it actually does. + Because it models the reaction mechanism the model is interpretable, and its possible to enforce constraints, e.g. that dynamics are physically possible.+ Generalises outside of the dataset to textbook problems :-)+ Well-founded modeling choices and neural network architectures.- Only applies to a very particular type of reaction (heterolytic LEF). - Requires supervision on the level of electron paths. This seems to inhibit applying the model to more datasets or extending it to other types of reactions.- Furthermore the supervision extraction does not seem take advantage of symmetries noted in the section(s) about difficulty evaluating inference. - It would be nice to margin out the electron flow model and just maximize the marginal likelihood for the product prediction problem.NoveltyIm not an expert on the literature of applying machine learning to the problems of reaction {product, mechanism} prediction but the paper appears to conduct a thorough review of the relevant methods and occupy new territory in terms of the modeling strategy while improving over SOTA performance.ClarityThe writing/exposition is in general extremely clear. Nicely done. There are some suggestions/questions which I think if addressed would improve clarity.Ways to improve the paper1. Better motivate the use of machine learning on this problem. What are the limitations of the arrow-pushing models? 2. Explain more about the Linear Electron Flow reactions, especially:- Why does the work only consider heterolytic LEF reactions, what other types of LEF reactions are omitted?- Is the main blocker to extending the model on the modeling front or the difficulties of extracting ground-truth targets? It appears to be the latter but this could be made more clear. Also that seems to be a pretty severe limitation to making the algorithm more general. Could you comment on this?Questions1. Is splitting up the electron movement model into bond removal and addition steps just a matter of parameterization or is that physically how the movements work? 2. It appears that Jin et al reports Top 6/8/10 whereas this work reports Top 1/3/5 accuracy on the USPTO dataset. It would be nice if there was overlap :-). Do your Top 6/8/10 results with the WLDN model agree with the Jin et al paper?NitsSection 2.3, first paragraph ...(LEF) topology is by far the most important: Could you briefly say why? Its already noted that theyre the most common in the database. Why?Section 3.ElectionMovement, first paragraph. Observer that since LEF reactions are a single path of electrons&. Actually, its not super clear what this means from the brief description of LEF. Can you explain these reactions in slightly more detail?Section 3.ElectionMovement, second paragraph. Differently, the above distribution can be split&. Awkward phrasing. How about In contrast, the above distribution can be split&. Section 3.Training, last sentence ...minibatches of size one reaction. Slightly awkward phrasing. Maybe ...minibatches consisting of a single reaction?Section 5.2, second sentence. However, underestimates the models actual predictive accuracy&. It looks like a word accidentally got deleted here or something.Section 5.2, paragraph 4. To evaluate if our model predicts the same major project... Did you mean the same major product? In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness.The paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network.The authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach.I enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well.The proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldnt understand very well its implications on the expressiveness of proposed method against classical low-rank approach. This paper investigates the average contribution of a sequence input to the contents of memory and derives a simple scheme to maximize the information content in memory, which is essentially to write at uniformly spaced intervals. Furthermore they present an attention-based version, where the network caches all hidden states in an interval and selects the hidden state to store via attention. The paper is very well written and has a nice balance of relevant theoretic motivation and experiments. Furthermore the question that the authors are tackling --- how should we compress information into external memories --- feels important and under-explored. The fact that the resulting scheme is simple is nice, because it's easy for people to try, and it now has some motivation beyond a heuristic decision.I think this paper will have impact in opening up more comprehensive research into the reduction of redundancy in the external memories of neural networks, and also could be instantly impactful for people using DNCs and NTMs --- especially since we see the incorporation of UW / CUW can help bridge the gap (or even surpass) LSTMs for the modeling of natural data. As such I think it is a clear accept. ---Comments to the authors:The results in Figure 2 (c) I think are misleading. The NTM with an RNN controller can solve this task, the limit of 10,000 steps implies that the model may converge to some 50% value with 14 slots but I am absolutely certain that the NTM + RNN controller would converge in 10,000 steps with a careful tuning of gradient clipping and learning rate. I think this is basically a false result. Furthermore I would like to really know what the best final performance of the models are on this task once converged, it's not clear if 10,000 steps was enough.For equation (9), was it necessary to construct the attention weights in this way? How much better was it to a direct softmax query: softmax(h_{t-1}^T d_j)? If you are backpropagating through the attention then the network can shape the hidden states to facilitate the relevant attention, as well as contain the information.In the second paragraph of S2.2.2 you have "a_{t, j} is the attention score" but you should have "\alpha_{t, j} is the attention score".Table 3: just include the Transformer results in the table!? The reasoning to exclude it is not really coherent.It would have been nice (and would raise my score) to see the UW scheme operating with a large(ish) number of memory slots. ### SummaryThe authors propose GCSL (goal-conditioned supervised learning), an algorithm that bridges the gap between reinforcement learning and imitation learning. Specifically, the authors are motivated by the limitations that RL is brittle when used with sparse rewards, and that IL requires expert demonstrations. Their technique performs learning without expert demonstrations, and does not require a learned value function or reward function. The authors formulate GCSL as iterative trajectory collection, goal relabeling, and policy refinement through behavioral cloning, and formally derive performance bounds on this technique. In addition, the authors demonstrate strong goal-reaching performance and robustness improvements over current RL techniques.### Strengths- The authors proposal is strongly motivated, in that RL techniques are clearly sensitive to hyperparameters and face stability challenges, and that using demonstrations is more robust. The paper is clearly written and the authors very cleanly explain their key ideas and insights. - The authors proposed algorithm is based on a simple idea, in a very good way. Their technique is stable, requires no value functions or reward function engineering (as is common with traditional RL techniques), and their technique of relabeling generates a large number of samples, resulting in better data efficiency.- The authors main theoretical insight that iterating on data from sub-optimal agents leads to optimal behavior is both non-trivial as the authors claim, and can likely be used to inspire other similar ideas or areas of study.- The authors clearly showcase the experimental use cases of their technique by demonstrating its benefits in terms of stability to hyperparameters and in leveraging expert demonstrations. Apart from this strong set of experimental results, the authors also present detailed ablations in the supplementary pages that are very convincing.---### Weaknesses- The theory section does seem a bit contrived. Specifically, optimality emerges when using the objective proposed by the authors, even if this is not commonly used. Further, the proof assumes that trajectories are collected from a single policy and relabeling is only performed on the last timestep, whereas in practice these conditions are not met. The performance guarantee also only holds with deterministic transitions. Finally, the guarantee on the convergence behavior is a good property, but is maybe less meaningful if in practice GCSL is difficult to converge to (if the optimization is challenging). - Notably, the authors do not evaluate the generalization ability of their technique, and instead evaluate with the same train and test environment. Although this is common in RL, it would be interesting to see how GCSL performs in environments that require more sample efficiency and test generalization, like Procgen.- The authors quantify performance as the distance of the agent to the goal at the last timestep. Im not convinced this evaluation metric makes the most sense, especially when success ratio is the main metric we care about.- On door opening, do the authors have an idea for why TD3+HER performs so much worse than PPO or GCSL? Similarly, on Sawyer pushing, do the authors have a hypothesis for why PPO and TD3+HER do not learn at all? Is there potentially a qualitative analysis of this unexpected behavior that can be performed? What about GCSL makes it so much more successful on Sawyer pushing?- Im surprised that GCSL is not any more sample efficient, since PPO and broadly speaking other policy gradient techniques are generally well-understood to be greedy sample-wise. - The authors earlier in the paper hypothesized that an optimal policy would be non-Markovian, but that GCSL with a Markovian policy would outperform a time-varying one. This seems rather counter-intuitive to me, why do the authors suspect this is the case? Is the model overly exploiting instead of exploring when conditioned on the remaining horizon? Does this behavior change depending on the number of timesteps in an episode?- In Figure 4, do the on-policy methods that converge slower do so with an empirically visible better convergence guarantee? For instance, even if it takes longer, the technique is guaranteed in the long-run to arrive at an optima?- In Figure 4, the authors compare against limited-horizon relabeling as in prior work and show this method has drawbacks. Are the drawbacks due to the lack of multi-horizon relabeling, or due to having much fewer trajectories after relabeling?- The authors compare robustness to hyperparameters against TD3, but do not compare against PPO. Because PPO is generally well-understood to not require much hyperparameter tuning empirically, I would recommend including this comparison as well. - The authors claim qualitatively (in Appendix C) that despite the different objective, the learned trajectories generally take a direct path to the goal. Is there a way to quantify this quantitatively against an oracle that does take the shortest path; for example by comparing the time taken to get to the goal, or the time spent at the goal waiting for the episode to complete?---### RecommendationOverall, I vote for accepting. I think this current submission is already relatively convincing as an accept, as it is clearly written, has well explained motivations, strong experimental results, and extensive ablations in the supplementary pages. The authors idea is conceptually simple, in a good way. My main gripe is that the theoretical results are only weakly held, or less relevant in practical settings, but that is rather typical and the strong experimental results do speak for themselves. I do have a few clarifying questions on the experimental results, but am regardless confident that this paper meets the ICLR acceptance criteria. This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.Some comments below. - The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. - Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. - The authors feature datasets comprising comparatively little supervision in the target domain (634 instances being smallest case), but did they consider also including test sets with literally just a few instances? It might be interesting to explicitly encourage "few shot" learning as a measure of generalizability.- I would consider pushing Table 5 (Appendix) into the main text. #### Summary of the paper :The authors propose to improve the sample quality of autoregressive models. The authors propose to (1) - smooth the input data distribution leveraging methods that have shown success in adversarial defense, (2) recover input distribution by learning to reverse the smoothing process. The authors first demonstrate the efficiency of their method on 1d toy-problem, and extend the demonstration to more complex datasets such as MNIST, CIFAR-10 and CelebA with application such as image generation, inpaintting and denoising.#### Pros :* The idea to leverage a method previously used for adversarial defense to density estimation is interesting and novel.* The paper is well motivated through the manifold hypothesis approximation (which results in densities with high Lipschitz constants) and compounding errors.* The theory is strong #### Cons :* The experiments on denoising and inpainting are only qualitative and suffer from a lack of quantitative evaluation.#### Recommandation :The article is clear, well motivated, and have a strong theoretical grounding. Therefore I would tend to accept the article.#### Detailed comments :* The experiment on 2d synthetic datasets (especially the olympic dataset) should be discussed more thoroughly. First, it is not clear that the proposed model is generating better sample than the MADE baseline on this specific dataset. Second, the intersection between rings, in the olympic dataset, seems to be much poorly modeled with the proposed approach compared to the MADE baseline. What is the reason ? * In the section 3.2 the authors are introducing 2 different debiasing methods (either a denoising step or another autoregressive model). In the rest of the article it is not clear which of the two methods the authors are using. In addition, in the 2d toy-problem (i.e. ring and olympic) as the authors are choosing a gaussian smoothing both debiasing methods are usable. Therefore it would be interesting to show both methods and to describe thoroughly the differences (in addition, it might provide an answer to my previous point). * The authors should not mention denoising and inpainting applications if there is no quantitative assessment (at least in appendix)& For the inpainting part, the corrupted input are not even shown (which part of the image has been predicted). The denoising and inpainting experiments sounds like its been rushed& #### Typos and suggestions to improve the paper :* Minor : Both theorems are provided with nice demonstrations, then the authors should refer to the demonstration in the core text of the article (e.g. see Appendix A).* Minor : Add small arrows in Table 2 to indicate that Inception score is better when lower, and opposite for FID* Typo : page 5, section 3.3, paragraph 2 : relative > relatively* Figure3 : Right panel : What are the 3 shaded curves ? This should be shown in the legend or at least in the caption* Figure3 : Right panel: In the x-axis it should be specified Variance of q(x^{\tilde} \mid x)* Page 7 : paragraph 1 : 'Thus, it is hard to conclusively determine what is the best way of choosing q(x |x). > I  think the authors actually give the key to properly choose the noise level (i.e. variance). It seems to depend on the task : if one wants to generate  good samples, then the variance has to be set by heuristic. If one needs a good likelihood (e.g. for subsequent downstream tasks) then the variance could be optimized.* Figure 6 :  On my understanding, the part denoising is redundant with the section image generation. It is interesting to mention the denoising application, but I am not convinced of the utility of the figure 6.* Figure 7 : What is the corrupted input ? Which part of the input has been masked ?? The authors present a novel deep learning representation for jointly modelling longitudinal measurements and dynamic time-to-event analysis where there are competing risks for a given event. The authors incorporate patient-level historical data using an RNN which allows updating of individual-level (i.e. personalized) risk predictions as additional data points are collected. This method (Dynamic-DeepHit) makes no assumptions about the underlying stochastic processes. The authors further evaluate the clinical utility of these methods in terms of interpretability of variable importance and dynamic risk predictions.The work is clearly structured and clearly articulate a well-motivated research problem. It is also extremely well-placed within the historical context of previous work done in survival modelling. The authors have carried out an extensive review of the literature showing the evolution as well as the strengths and weaknesses of these methods.My main concern with this manuscript is the handling of missing data. In the context of this study, the evaluation of missing data was inadequately investigated. This is an important problem within the context of what the authors are trying to achieve. Although it may be outside the scope of the current manuscript, different assumptions regarding missing data should be investigated. For example, if missing data was correlated with a particular outcome or a particular covariate, then replacing missing values with interpolation or with the mean and mode would lead to biased estimates. This is a good review paper covering techniques proposed across many of the well-known works in this area and doing an in-depth analysis of the value each of the techniques brings. Additionally, based on these studies the paper offers insights into the best algorithms and procedures to combine to achieving good results.One recent whitepaper that has related work (not fully overlapping though), that may be worth looking by the authors is at https://arxiv.org/abs/1806.08342. It is fairly new and not very well-known so not surprising that the authors missed it.Pros- Well written paper with lots of in-depth experiments - Does well at teasing out the impact of each of the techniques and gives some intuitive explanations of why they matter.- Provides better insights into how to make training of binary neural networks faster.- As the importance of low precision networks grows, this is a valuable paper in pushing the area of research forward.Cons- A review paper, which doesn't add much new to the existing suite of techniques. Note: This is true for most review papers. This paper explores how discriminators can be designed against certain generator classes to reduce mode collapse. The strength of the paper is on establishing the sample complexity bounds for learning such distributions to show why they can be effectively learned. The work is important in understanding the behaviour of GANs. The work is original and significant. A few comments that need to be addressed are listed as below:1. I found the paper is a bit hard to follow in the beginning, due to its structure. In Section 1, it first gives introduction and then talks about the novelty of the paper; it then shows more background work followed by more introduction of the proposed work; after that, Section 1.4 talks more related work. It makes reading confusing in the beginning.2. The authors wrote that "In practice, parametric families of functions F such as multi-layer neural networks are used for approximating Lipschitz functions, so that we can empirically optimize this objective eq. (2) via gradient-based algorithms as long as distributions in the family G have parameterized samplers. (See Section 2 for more details.)" I am not sure how Section 2 gives more details.3. There are some typos and the references are not very carefully edited. For example, in Theorem 4.5, "the exists a ..." -&gt; "there exists a ..."; in reference, gan -&gt; GAN. The manuscript introduces a novel and interesting approach to weight sharing among CNNs layers, by learning linear combinations of shared weight templates. This allows parameter reduction, better sample efficiency. Furthermore, the authors propose a very simple way to inspect which layers choose similar combinations of template, as well as to push the network toward using similar combinations at each layer. This regularization term has a clear potential for computation reuse on dedicated hardware. The paper is well written, the method is interesting, the results are convincing and thoroughly conducted. I recommend acceptance.1) It would be interesting to explore how often the layer parameters converge to similar weights and how similar. To this end I suggest to plot a 2d heatmap representing the similarity matrices between every pair of layers.2) Figure 1 is not of immediate interpretability. Especially for the middle figure, what does the dotted box represent? What is the difference between weights and templates? Also its unclear which of the three options corresponds to the proposed method. I would have thought the middle one, but the text seems to indicate it is the rightmost one instead.3) How are the alphas initialized? How fast are their transitions? Do they change smoothly over training? Do they evolve rapidly and plateau to a fixed value or keep changing during training? It would be really interesting to plot their value and discuss their evolution.4) While the number of learned parameters is indeed reduced when the templates are shared among layers - which could lead to better sample efficiency - I am not sure whether the memory footprint on GPU would change (i.e., I believe that current frameworks would allocate the same kernel n-times if the same template was shared by n layers, but I am not certain). Although the potential reduction of the number of trainable parameters is an important result by itself, I wonder if what you propose would also allow to run bigger models on the same device or not, without heavy modifications of the inner machineries of pyTorch or Tensorflow.  Can you comment on this? Also note that the soft sharing regularization scheme that you propose can be of great interest for FPGA hardware implementations, that benefit a lot from module reuse. You could mention that in the paper.5) Sec 4.1, the number of layers in one group is defined as (L-4)/3. Its unclear to me where the 4 comes from. Also on page 6, k = (L-2)/3 - 2 is said to set one template per layer. I thought the two formulas would be the same in that case. What am I missing? Is it possible that one of the two formulas contain a typo (I believe that at the very least it should be either (L-2) or (L-4) in both cases)?6) Sec 4.1, I find the notation SWRN-L-w-k and SWRN-L-w confusing. My suggestion is to set k to be the total number of templates (as opposed to the number of templates *per group of layers*), which makes it much easier to relate to, and most importantly allows for an immediate comparison with the capacity of the vanilla model. As a side effect, it also makes it very easy to spot the configuration with one template per layer (SWRN-L-w-L) thus eliminating the need for an ad-hoc notation to distinguish it.7) The authors inspect how similar the template combination weights alphas are among layers. It would also be interesting to look into what is learned in the templates. CNN layers are known to learn peculiar and somewhat interpretable template matching filters. It would be really interesting to compare the filters learned by a vanilla network and its template sharing alternative. Also, I would welcome an analysis of which templates gets chosen the most at each layer in the hierarchy. It would be compelling if some kind of pattern of reuse emerged from learning.8) Sec 4.4, it is unclear to me what can be the contribution of the 1x1 initial convolution, since it will see no context and all the information at the pixel level can be represented by a binary bit. Also, are the 3x3 convolutions same convolutions? If not, how are the last feature maps upscaled to be able to predict at the original resolution?9) At the end of sec 4.4 the authors claim that the SCNN is also advantaged over a more RNN-like model. I fail to understand how to process this sentence, but I have a feeling that its incorrect to make any claims to the performance of RNN-like models as such a model was not used as a baseline in the experiments in any way. Similarly, in the conclusions I find it a bit stretched to claim that you can gain a more flexible form of behavior typically attributed to RNNs. While its true that the proposed network can in theory learn to reuse the same combination of templates, which can be mapped to a network with recursion, the results in this direction dont seem strong enough to draw any conclusion and a more in-depth comparison against RNN performance would be in order before making any claim in this direction.MINOR- Sec3: I wouldnt say the parameters are shared among layers in LSTMs, but rather among time unrolls.- One drawback of the proposed method is that the layers are constrained to have the same shape. This is not a major disadvantage, but is still a constraint that would be good to make more explicit in the description of the model.- Sec3, end of page 3: does the network reach the same accuracy as the vanilla model when k=L? Also, does the network use all the templates? How is the distribution of the alpha weights across layers in this case?- Sec3.1, the V notation makes the narrative unnecessarily heavy. I suggest to drop it and refer directly to the templates T. Also the second part of the section, with examples of templates, doesnt add much in my opinion and would be better depicted with a figure.- Sec3.1, the e^(i) notation can be confused with an exp. I suggest to replace it with the much more common 1_{i=j}.- Figure 2 depicts the relation between the LSM matrix and the topology of the network. This should be declared more clearly in the caption, in place of the ambiguous capturing implicit recurrencies. Also, the caption should explain what black/white stand for as well, and possibly quickly describe what the LSM matrix is. Also, it would be more clear that the network in the middle is equivalent to that on the right if the two were somehow connected in the figure. To this end they could, e.g., share a single LSM matrix among them. Finally, if possible try and put the LSM matrices on top of the related network, so that its clear which network they refer to. Sec 3.2 should also refer to Fig2 I believe.- Table 1: I suggest to leave the comment on the results out of the caption, since its already in the main text.- Table 2: rather than using blue, I suggest to underline the overall best results, so that its visible even if the paper is printed in B&amp;W.- Fig 3, I would specify that its better viewed in color- Discussion: I feel the discussion of Table 1 is a bit difficult to follow. It could be made easier by reporting the difference in test error against the corresponding vanilla model (e.g., improves the error rate on CIFAR10 by 0.26%, rather than reporting the performance of both models)- Fig 4, are all the stages the same and is the network in the left one such stages? If so, update the caption to make it clear please.- Fig 4, which lambda has been used? Is it the same for all stages?- Fig 5, specify that the one on the right is the target grid. Also, I believe that merging the two figures would make it easier to understand (e.g., some of the structure in the target comes from how the obstacles are placed, which requires to move back and forth from input to target several times to understand)- Sec 4.4, space permitting, I would like to see at least one sample of what kind of shortest path prediction the network can come up with.A few typos:    * End of 3.2: the closer elements -&gt; the closer the elements    * Parameter efficiency: the period before re-parametrizing should probably be a comma?    * Fig 4, illustration of stages -&gt; illustration of the stages    * End of pag7, an syntetic -&gt; a syntetic Thank you for this enjoyable paper. Summary: The authors propose a novel approach to active learning as follows. At each iteration they develop a classifier that can discriminate between the samples in the labeled and unlabeled sets; they select the top few samples that are most likely to be from the unlabeled set as per this classifier, and request the oracle to provide labels for this batch next. This simple idea is shown to have a principled basis and theoretical background, related to GANs and to previous results from the literature. They provide clear algorithms and open source code for easy verification, and public testing. They provide good experimental verification on CIFAR-10 and MNIST benchmarks. I personally look at new papers more for novel ideas and good intuition/theoretical justification than an immediate improvement in benchmark results, so I enjoyed this paper thoroughly. Results: Among other things they show that their algorithms ranks the samples to be next labeled quite differently than uncertainty sampling based approaches; that their method is at least as accurate/sample-efficient as the state of the art ; and that some previously published experimental results are incorrect(!). As the authors will probably agree I am not convinced the proposed method is better than previous algorithms in any statistically significant way, but the novel idea itself is worth publishing even if it is just as good as the state of the art. Novelty: I liked the paper very much because it provides quite an innovative new approach to look at active learning, which resembles GANs and Core set ideas in some ways, yet differs in significant ways that are critical for active learning. I've been working and publishing in related areas for a long time so I genuinely found your central idea refreshing and new.Relevance: The paper is very relevant to the ICLR community and addresses critical questions. Question:My intuition as a Bayesian is that we most need to find labels that maximize the mutual information I(y,w) where w are the weights of the neural net. In practice this corresponds to the samples x which have the maximum class uncertainty, but for which the parameters under the posterior disagree about the outcome the most, eg see discussion below equation 2 for  Bayesian Active Learning by Disagreement (BALD) in this paper https://arxiv.org/pdf/1112.5745.pdf . In essence: The above means that the labels that provide most information about the classification model are most valuable for active learning. However, your approach intuitively ignores the conditional distribution(ie py(|x)), and instead tries to make the original unconditional distribution p(x) between the labeled and unlabeled sets similar. Yet, it works beautifully. So: Why does this work? What is the intuition?  Summary ====It is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated. This paper proposes a conjecture to explain this phenomenon that the authors call The Lottery Ticket Hypothesis:  large networks that can be trained successfully contain at initialization time small sub-networks  which are defined by both connectivity and the initial weights that the authors call winning tickets  that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely.==== Detailed Review ====I have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while.  For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the right direction for the beginning of the training, as mentioned in [1] that was cited in this submission. Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously  and that is why I strongly vote for the acceptance of this paper.Though I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues:1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject. For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results. The same should be done here. For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6].2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster convergence) and about generalization (better generalization accuracy). However, there is a slight issue with how these terms are treated in the paper. First, convergence is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum  it is better to write that early stopping regularization was used in this case. Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets). Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set. As a minor note, generalization accuracy as a term is not that common and might be a bit confusing, so it is better to write test accuracy.To conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to ICLR.[1] Bengio et al. Convex neural networks. NIPS 2006.[2] Zhang et al. Understanding deep learning requires rethinking generalization. ICLR 2017.[3] Arora et al. Stronger generalization bounds for deep nets via a compression approach. ICML 2018.[4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning. Arxiv preprint 2018.[5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry. ICLR 2017.[6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. ICLR 2018.  The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such winning-tickets on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets.The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing feature of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version.1) The main weakness of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively small and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the authors decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds in general. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies.2)  While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably break the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, &? On page 2, second paragraph, the paper states: When randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch. I dont fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough juice and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers).3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow shipping each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer).4) Some things that would be interesting to try:4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all behind some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights trajectories during training to see if there is a correlation between the distance weights traveled and whether or not they end up in the winning ticket.4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning?5) Typo (should be through): we find winning tickets though a principled search process6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well) TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.* SummaryThe manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\lambda\in (0,1)$ (sampled from a $\mathrm{Beta}(\alpha,\alpha)$ distribution).A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.* Major remarks- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.* Minor issues- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability. The paper introduces a framework for computationally efficient and exactly rotation-equivariant spherical CNNs. The work most closely resembles the Fourier space method of Kondor et al., but improves on it in a number of ways: firstly, a channel-wise structure is introduced for the tensor product nonlinearities, which avoids the degree blowup of this operation while still allowing mixing between different harmonic degrees. Secondly, computational complexity of linear layers is reduced by factorizing it into three operators, two of which operate similar to depthwise-separable convolutions and one of which acts uniformly across channels. Thirdly, an optimized sparse degree mixing set is proposed, based on a minimum spanning tree. Finally, a more efficient sampling theorem is used that reduces the Nyquist rate by a factor of two compared to the ones used in previous works on spherical CNNs.The paper is very well written and the authors clearly have a thorough understanding of the noncommutative harmonic analysis involved. This does not mean the paper will be easy to understand for all readers, but for those familiar with the relevant mathematics, either from textbooks or earlier works in the spherical CNN literature, the paper is very readable. The proposed improvements make a lot of sense to me, and their computational complexity improvements are clearly stated. The performance of a network architecture that includes the new layers is tested and shown to yield competitive or state of the art performance on several benchmark problems that have been used in many previous works.Overall I think this is a very nice paper, but I have a few minor concerns and points of improvement:The degree mixing set (3.1.3) is a minimum spanning tree that minimizes a certain computational cost. This makes some sense, but it is not clear to me that this approach is optimal in any meaningful sense or necessary at all. I have personally experimented with sparse channel connectivity in planar CNNs, and found that it does not seem to matter much how exactly the channels are connected, with the main factor determining compute/accuracy being the number of connections. Full degree mixing does seem desirable, but this implies the need of a MST only if one wishes to use the same connectivity structure in multiple layers. An interesting baseline would be to do the degree mixing using a random pattern in each layer, with various sparsity levels. It may turn out that only the sparsity level but not the precise connectivity structure matters in practice. Such a finding would not diminish the paper's significance.It should be clarified in the paper that full mixing happens only across several layers (as many as the maximum path length / tree width in the MST). The question then arises whether full mixing actually happens in the considered architecture, given that it is not very deep.It would be interesting to see actual implementation details in some DL framework, as well as wallclock timings. Also, code would be much appreciated.The appendix describes a method for enforcing spatial localization of the spectral filters, but it is not clear from the paper if/how this is actually used in the network architecture that is tested.It would be nice to know why the initial convolution layers are necessary, instead of just using the generalized layers introduced in this paper in their full glory.I may have missed it, but could not figure out what L_G^(psi) refers to in 2.6. This paper is about issues that arise when applying Information Bottleneck (IB) concepts to machine learning, more precisely in deterministic supervised learning such as classification (deterministic in the sense that the target function to estimate is deterministic: it associates each example to one true label only, and not to a distribution over labels).Namely:(1) the "Information Bottleneck curve" cannot be computed with the Information Bottleneck Lagrangian approach (because of optimization landscape issues: optimization of such a piecewise-linear function with a linear penalty will always yield the same optimum whatever the slope of the penalty is [same story as L1 vs. L0]); (2) there are many solutions to the optimization of the IB Lagrangian for any given compression/performance ratio (i.e. for any given beta in the IB Lagrangian method: I(Y,T)/I(X,T)) and some of them are provably trivial; thus optimizing just the IB Lagrangian does not imply that the solution will be interesting, and better (or complementary) criteria are needed.Another point discussed also is about the successive layers of perfect classifiers (neural networks), in which I(Y,T) remains constant while I(X,T) decreases.Pros:- the paper is well written, mostly self-contained, and easy to read (for someone familiar with information theory);- all mathematical points are detailed and well explained, with sufficient introduction;- the writing is compact, the paper is dense, and given the page limit this is a good information/compression compromise;)- information bottleneck is a topic of prime interest in the community these days;- the two first problems described ((1) and (2)) are original, interesting contributions to the field, of particular interest for people interested in applying information bottleneck concepts to supervised learning;- the solution brought to the IB Lagrangian issues is simplistic though efficient (squaring I(X,T) so that it's not linear in I(X,T) anymore).Cons:- not much.Remarks:- there exist recent papers tackling the information bottleneck concept for neural networks from a variational perspective, which enables them to compute exactly the mutual informations (such as "Compressing Neural Networks using the Variational Information Bottleneck" by Dai &amp; al., ICML 2018); I have not seen these papers cited in the article, nor discussed (nor used); I feel it would be appropriate, either in the general literature section, either for discussing how to compute in practice the mutual informations (exact values vs. estimates or lower bounds as here).- at first reading, I had found the tone of the beginning of the paper (first section) a bit aggressive, though this feeling disappeared later. Maybe rephrase some expressions that might be wrongly perceived?- About multilabel classification (end of section 2): multilabel classification can still be seen as with deterministic expected outputs, if considered as a task from X to P(Y) (power set of Y, i.e. set of all possible subsets of labels).- As in practice T is constrained to belong to a particular space of functions (neural network layer with predefined architecture): how does this impact the study? For instance the T_alpha in equation (5) are not reachable anymore; the optimization space for the IB Lagrangian is different; etc. Which properties/conclusions can be kept, and which ones cannot?- What about sampling on the other part of the IB curve, the horizontal one (same I(Y,T) for various I(X,T))? Would it bring any insight, and how to do it?- A side remark about applying IB to neural networks: What about neural networks that are not a "linear" chain of layers (i.e. most networks now)? i.e. Inception, ResNet, U-nets, etc., where computational flows are parallel, sometimes keeping full information till the end. For instance in a U-net, meant for image processing, features computed at the beginning at a full pixelic resolution are communicated to the last layer. This is not an image classification task though, as predictions are made for each pixel; still, given an input image X, there is only one correct output Y, so, still in the deterministic supervised classification problem. # WeaknessesApplications are a bit unclear.It would be nice to see a better case made for spherical convolutions within the experimental section.  The experiments on SHREC17 show all three spherical methods under-performing other approaches.  It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.  Is there a task that this representation significantly outperforms other spherical methods and non-spherical methods?  Or a specific useful application where spherical methods in general outperform other approaches?  # Strengths:The method is well developed and explained.  Ability to implement in a straight-forward manner on GPU. The authors provide a clean and easily understood sufficientcondition for spurious local minima to exist in networks witha hidden layer using ReLUs or leaky ReLUs.  This condition,that there is not linear transformation with zero loss,is satisfied for almost all inputs with more examples thaninput variables.The construction is elegant.  The mathematical writing in the paper,especially describing the proof of Theorem 1, is very nice -- theyexpose the main ideas effectively.I do not know of another paper using a similar proof, but I have notstudied the proofs of the most closely related papers prior to doingthis review, so I have limited ability to vouch for this paper'stechnical novelty.The authors also show that networks using many other popularactivation functions have spurious local minima for a verysimple dataset.  All of these analysis are unified using asimple, if technical, set of conditions on activation function.Finally, the authors prove a somewhat technical theorem aboutoptima in deep linear networks, which generalizes someearlier treatments of this topic, providing an checkablecondition for global minimality.There is extensive discussion of related work.  I am not aware ofrelated work not covered by the authors.In some cases, when the authors discuss previous work, they write asif restriction to the realizable case is an assumption, when it seemsto me to be more of a constraint.  In other words, it seems harder toprove the existence of spurious minima in the realizable case.They seem to acknowledge this after their statement of their Theorem 2,which also uses a realizable dataset.Also, a few papers, including the Venturi, et al paper cited bythe authors, have analyzed whether spurious local minima existin subsets of the parameter space, including those likely tobe reached during training with different sorts of initializations.In light of this work, the authors might want to tone down claimsabout how their work shows that results about linear networks donot generalize to the non-linear case.  In particular, to maketheir construction work in the case of wide networks, theyneed an overwhelming majority of the hidden units to be "dead",which seems as it is unlikely to arise from training withcommonly used initializations.Overall, I think that this paper makes an interesting andnon-obvious contribution on a hot topic. The submission considers a disadvantage of a standard dropout-based Bayesian inference approach, namely the pessimization of model uncertainty by means of maximizing the average likelihood for every data sample. The formulation by Gal &amp; Ghahramani is improved upon two-fold: via simplified modeling of the approximating variational distribution (on kernel/bias instead of on patch level), and by using a discriminator (i.e. classifier) for providing a "synthetic" likelihood estimate. The latter relaxes the assumptions such that not every data sample needs to be explained equally well by the models.Results are demonstrated on a variety of tasks, most prominently street scene forecasting, but also digit completion and precipitation forecasting. The proposed method improves upon the state of the art, while more strongly capturing multi-modality than previous methods.To the best of my knowledge, this is the first work w.r.t. future prediction with a principled treatment of uncertainty. I find the contributions significant, well described, and the intuition behind them is conveyed convincingly. The experiments in Section 4 (and appendix) yield convincing results on a range of problems.Clarity of the submission is overall good; Sections 3.1-3.3 treat the contributions in sufficient detail. Descriptions of both generator and discriminator for street scenes (Section 3.4) are sufficiently clear, although I would like to see a more detailed description of the training process (how many iterations for each, learning rate, etc?) for better reproducability.In Section 3.4, it is not completely clear to me why the future vehicle odometry is provided as an input, in addition to past odometry and past segmentation confidences. I assume this would not be present in a real-world scenario? I also have to admit that I fail to understand Figure 4; at least I cannot see any truly significant differences, unless I heavily zoom in on screen.Small notes:- Is the 'y' on the right side of Equation (5) a typo? (should this be 'x'?)- The second to last sentence at the bottom of page 6 ("Always the comparison...") suffers from weird grammar OVERVIEW:The paper looks at the problem of self-supervised learning using consistency-enforcing approaches. Their main contributions are two-fold:1. Analysis to understand current state-of-the-art methods for self-supervised learning, namely the Mean Teacher model (MT) by Tarvainen and Valpola (2017) and the \Pi model (Laine and Aila, 2017). They show a theoretical analysis (Sec.3.1) of a simplified version of the \Pi model and show that it reaches flatter minima leading to good generalization. They show an analysis of the SDG trajectories (Sec. 3.2) that shows how these self-supervised models achieve flatter and lower minima compared to a fully supervised approach. They also provide an intuitive explanation to explore more solutions along the SGD trajectory. Finally, in Sec.3.3, they also discuss how ensembling and weight averaging help get better solutions.2. Fast-SWA, which is a tweak to the SWA procedure (Izmailov et al, 2018) that averages models in the weight space along the SGD trajectory with a cyclical learning rate.They show good performance on CIFAR-10 and CIFAR-100 with their proposed Fast-SWA.PROS:1. The paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution. The problem and experiments are very organized and explained very well.2. Exhaustive experiments, plots and tables showing very good performance on the standardized benchmark.CONS:1. The novel contribution (as I see it) is in the theoretical analysis of Sec. 3.1 &amp; A.5 and the Fast-SWA procedure. The Fast-SWA is a minor tweak to the regular SWA. The theoretical analysis is the main novelty and it is hidden away in the appendix ! Also, the results seems to be derived on the basis of Avron and Toledo and the authors' contribution relative to that is not clear. Also, what is the difference between the regular \Pi model and simplified \Pi model and how big a difference does this make in your theory ?2. Can the Fast SWA be used directly say while supervised training of ImageNet ? Or is it applicable only to self-supervised problems ? Comments on the generalizability of this contribution might help increase novelty.OVERALL:I like the thorough analysis and good results of the paper. The novelty being a little weak results in the final rating of 7.5 (rounded up to 8, subject to change depending on other reviewers). The authors present a simple algorithm based on the statistics of neural activations of deep networks to detect out-of-distribution samples. The idea is to use the existing running estimate of mean and variance within BatchNorm layers to construct feature representations that are later fed into a simple linear classifier. The authors demonstrate superior performance over the previous state-of-art in the standard evaluation setting and provide fascinating insights and empirical analysis of their method.There are several aspects of this work that I admire.- The authors evaluate the generalization of their OOD detection model through evaluation against unseen OOD samples. This critical evaluation strategy is not typical in this literature and is much needed.- The organization of the material and the depth of the discussion is of high quality. They discuss and connect the previous work, they clearly explain the idea and provide empirical results to support the design decisions, and run several experiments to evaluate their method from different angles followed by interesting discussions.- The proposed method is easy to implement and has a minimal runtime complexity with no adverse effect on the underlying classifier.- The source code is already included in the submission.My only concern is that the feature pooling strategy first averages the input spatially, then across the channels. This feature size reduction is necessary because we have to ensure the following OOD classifier does not overfit in the validation stage. However, this reduction also introduces a permutation invariance in the feature space that is not desirable in OOD sample detection. I think it would make the work more valuable if the authors also take a critical look at the possible failure cases -- a short discussion of the weaknesses and assumptions. Overall, the paper is technically sound and well-organized with sufficient coverage of the previous work. A thorough series of evaluations support the claims. It is a novel combination of existing techniques. The empirical evidence is strong and insightful. Given the simplicity of the method, I would expect a quick adoption by the community. This white paper presents the geomstats package. The package provides tools for Riemannian modelization andoptimization over manifolds. Especially, the package supports several important manifolds: hyperspheres, hyperbolic spaces, spaces of SPD matrices or Lie groups of transformations. Pros: 1. the paper shows ever use cases of machine learning with manifolds. These use cases are concrete and representative. 2. the code in the package is extensively tested. Cons:There is no discussion about the scalability of the package. Summary==This paper is well-executed and interesting. It does a good job of bridging the gap between distinct bodies of literature, and is very in touch with modern ML ideas. I like this paper and advocate that it is accepted. However, I expect that it would have higher impact if it appeared in the numerical PDE community. I encourage you to consider this conference paper to be an early version of a more comprehensive piece of work to be released to that community.My main critique is that the paper needs to do a better job of discussing prior work on data-driven methods for improving PDE solvers.==Major comments==* You need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach. * You do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?* Im surprised that you didnt impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator. ==Minor comments==* Valid iterators converge to a valid solution. However, cant there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?* In (9), why do you randomize the value of k? Wouldnt you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver? * In future work it may make sense to learn a different H_i for each step i of the iterative solver. * When introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G.  Summarization:This paper presents a framework (called FX Network) of quantizing the weights and gradients of neural networks, based on five quantization criteria proposed in literature. The proposed framework can quantize the neural network obtaining a minimal or close-to-minimal error for a pre-specified precision level.Pros:- The proposed FX network can quantize all variables including both network weights and back-propagated gradients.- Promising results have been obtained. Experimental results on CIFAR have shown that the proposed quantization framework had reduced the representational cost, computational cost, and the communication  by up to 6x, 8x, and 4x, respectively, compared to the 32-b FL baseline and related works.- The paper is well written.Cons:- The experiment results showed in Figure 3 are quite confusing: why do the curves of the test error and loss suddenly drop at epoch 100? Explanation is needed. - This proposed quantization method require to pre-train a network with high precision in advance, similarly as the student-teacher framework or knowledge distillation. Different from BN and TG, FX network requires to pre-train a 32-b floating-point network, which requires more extra computational costs. - How does the quantization method compare with strategies like parameter pruning and sharing? It is better to see a discussion with them. It is also suggested to show the improvement of the proposed framework in terms of inference time during test. The authors introduce a  novel  on-policy  temporally  consistent  exploration  strategy, named Neural  AdaptiveDropout Policy Exploration (NADPEx), for deep reinforcement learning agents. The main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration. For this, the authors use the ideas of the standard dropout for deep networks. Using the proposed  dropout transformation that is differentiable, the authors show that the KL regularizers on policy-space play an important role in stabilizing its learning. The experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed. This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works. This poses a challenge in evaluating this paper. Nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches. Even though the authors answer positively to each of their four questions in the experiments section,  it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach. This paper analyzes the implicit regularization in SGD with finite learning rates via backward error analysis. The modified flow introduced in this paper better approximates the practical behavior of SGD as it does not require vanishing learning rates and it allows to use random shuffling in stead of i.i.d sampling. The numerical experiments validates the existence of the implicit regularization and how it affects the generalization of the model trained by SGD. The difference from SDE analysis is also discussed.Reason for score:1. The paper is well organized. Specially, I enjoy reading section II. The tool of backward error analysis and the derivation of the implicit regularization in SGD flow are introduced clearly and concisely. The analysis is based on random shuffling instead of i.i.d sampling matches the practical use of SGD.2. The numerical experiments are very convincing. The consistency of SGD with larger lr and SGD with smaller lr plus explicit regularization validates the results of theoretical analysis. The numerical experiments also provide some insights into tuning hyper parameters such as learning rate and batch size. ## SummaryThe authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform. Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model.In order to achieve this, the authors had to use several tricks, some already existing, for instance taken from the GAN TTS architecture, and some novel. The three key novelties are:- differentiable monotonic attention with gaussian kernel and length prediction.- dynamic time warping for the spectrogram loss.- using both spectrogram and waveform domain discriminatorsThe authors provide a comprehensive ablation study with MOS score, although their model is under the state of the art by a significant margin.## ReviewThis paper builds on GAN TTS, and tries to make it trainable end-to-end without aligned features.The two main contributions, namely dynamic time warping and monotonic attention with gaussian kernel are both elegant, and can likely be used for many other applications related to time series with heterogeneous time scales. In particular, the time warping loss allows to accomodate both for the natural irregularities in spoken speech, as well as providing sufficient signal for the monotonic attention to work.The rest of the architecture is very similar to GAN TTS except for the spectrogram domain discriminator that was added.While the model is under the state of the art for TTS, the samples are already quite convincing. The authors conduct a thorough ablation study, both with MOS and audio samples.Overall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution. As I already mentioned, I also believe that the approach of using dynamic time warping and monotonic attention can be used for other kind of time series.## Remarks and questions to the authors- Table 1, MOS for Tacotron 2 would be very informative. All the baselines are trained on aligned data while Tacotron is a legitimate contender for EATS as it can be trained on the same data. The point of the authors is that their methods is simpler because the training is in one stage. However, given the large number of losses and components in their model, with their respective hyper-parameters to tune, I'm not entirely sold on the simplicity argument. The tacotron 2 paper reports a MOS of 4.5 but on a private dataset.- Section 3, [1] used the same simple L1 + log spectrogram loss as used here.- I was surprised by the bad performance of the transformer attention, in particular in the audio samples, the output for this model is garbage towards the end of the signal. Any clue on why this would happen?- It would be interesting to have a benchmark, in particular, can the model generate speech in real time on GPU and on CPU?[1] SING: Symbol-to-Instrument Neural Generator, Defossez et al. Neurips 2018. This paper proposes a fully end-to-end TTS system with adversarial training. The proposed method has three main advantages: 1) a simple, end-to-end pipeline with only two submodules, with a fully differentiable and efficient architecture; 2) a flexible dynamic time warping to compute the loss between the predicted (generated) and true spectrograms; 3) a good MOS performance compared with the other state-of-the-art systems with more supervision. The evaluation is done very thoroughly with a variety of ablation studies.Overall, I vote for accepting the paper. First of all, the paper is very well organized and easy to follow. Related works are well summarized, while focusing on the main differences on the proposed method. Method is explained in great detail with easy-to-follow descriptions, proper equations, and very appropriate figures. Solid evaluation is performed, and experimental results and speech samples are convincing.Pros:+ The structure of the paper allows an easy read.+ The main contributions are clearly stated and supported by the experiments.+ Major works on the similar topic are widely covered and referenced.+ Evaluation is thorough enough to support the arguments with in-depth ablation studies.+ Appendices provide useful, supplementary information.Cons:- No comparison over the computational cost nor model size is presented. It is of particular interest because the proposed model is non-autoregressive, and thus may be capable of a causal, real-time inference.- No use of widely accepted benchmark datasets. More direct comparison would be of interest.Minor comments/questions:- If I understood correctly, the training sample has no phoneme-level alignment. Instead, only a sentence-speech pair is provided. If so, how do you select the corresponding text snippet from a sentence when you randomly sample 2 seconds of audio from the training examples whose length varies from 1 to 20 seconds?- Section 2.5 on DTW is rather lengthy. DTW is a quite well-known algorithm for alignment between the two sequences, the detailed explanation on the algorithm may be omitted without the loss of readability, in my opinion.- In Section 2.1, T is used to denote the total number of output times steps of the aligner, while T in Section 2.4 denotes the number of mel-frequency frames. Are these T's identical?- The proposed aligner module doesn't seem to be very useful compared with the attention-based aligner as seen in the ablation study (Table 1): very small improvement from 3.551 to 3.559 MOS. Can you provide more explanations? This paper is built on a simple but profound observation: Frey's bits-back coding algorithm can be implemented much more elegantly when replacing arithmetic coding (AC) with asymmetric numerical systems (ANS), a much more recent development not known at the time, simply due to the fact that it encodes symbols in a stack-like fashion rather than queue-like.This simple observation makes for an elegantly written paper, with promising results on MNIST. I truly enjoyed reading it, and I'm convinced that it will spark some very interesting further work in the field of compression with latent-variable models.Having said that, I would like to point out some possible limitations of the proposed approach, which I hope the authors will be able to address/clarify:1. At the beginning of section 2.1, the authors define the symbols as chained conditionals prod_n p(s_n | s_1 ... s_n-1), which is generally permissible in AC as well as ANS, as long as the decoding order is taken into account. That is, in AC, the symbols need to be encoded starting with the first symbol in the chain (s_1), while in ANS, the symbols must be encoded starting with the last symbol in the chain, because the decoding order is inverted.In their description of BB-ANS, the authors omit the discussion of conditional chains. It is unclear to me if a conditioning of the symbols is feasible in BB-ANS due to the necessity to maintain a strict decoding order. It would be very helpful if the authors could clarify this, and update the paper accordingly, because this could present a serious limitation. For instance, the authors simply extrapolate the performance of their method to PixelVAE; however, this model is autoregressive, so a conditioning of symbols seems necessary. Similarly, in appendix A, the authors mention the work of Minnen et al. (2018), where the same situation would apply, albeit one probabilistic level higher (on encoding/decoding the latents with an autoregressive prior).2. Furthermore, in both cases (PixelVAE and Minnen et al.), the symbols (s) and latents (y) are defined as jointly conditioned on each other (i.e., computing the posterior on one element of y requires knowledge of all elements of s, and computing the likelihood on one element of s requires knowledge of all elements of y). This seems to imply that all operations pertaining to one data vector (i.e. to one image) would have to be done in a monolithic fashion, i.e.: first sample all elements of y from the stack, then encode all elements of s, and then encode all elements of y. Hence, if the goal is to compress only one image, the algorithm would never get to the point of reusing the "bits back", and the overhead of BB-ANS would be prohibitive. It seems that in the MNIST experiments, the authors avoid this problem by always encoding a large number of images at a time, such that the overhead is amortized.3. Similarly, although the compression of continuous-valued variables up to arbitrary precision is an exciting development and I do not wish to undermine the importance of this finding, it should be noted that the finer the quantization gets, the larger the potential overhead of the coding scheme will grow. In practice, this would make it necessary to encode more and more images together, in order to still benefit from the method. This would be a good point to make in the discussion.4. The authors state in the appendix that learned compression methods like Ballé et al. (2018) and Minnen et al. (2018) could be improved by using BB-ANS. The potential gain of BB-ANS for these models seems rather small, though, as the entropy of y must be larger or equal to the entropy of y conditioned on s: H[y] &gt;= H[y|s], the latter of which should represent the potential coding gain. Ballé et al. (2018), however, found that the bits used to encode the hierarchical prior (i.e. H[y]) is only a small fraction of the total bitrate, thus upper bounding the potential gains for this type of model.Overall, I think this is a well-written, important and elegant paper, and I would like to see it accepted at this conference. If the authors can satisfactorily address some of the above potential limitations, it might turn out to be even better. This is a very nice paper contributing to what I consider a relatively underexplored but potentially very promising research direction. The title of the paper in my opinion undersells the result which is not only that "deep skinny neural networks" are not universal approximators, but that the class of functions which cannot be approximated includes a set of practically relevant classifiers as illustrated by the figure on page 8. The presentation is extremely clear with helpful illustrations and toy but insightful experiments.My current rating of this paper is based on assuming that the following concerns will be addressed. I will adjust the score accordingly after authors' reply.Main:- A very similar result can be found in Theorem 7 of Beise et al.'s "On decision regions of narrow deep neural networks" from July 2018 ( https://arxiv.org/abs/1807.01194 )        Some differences:                - The other paper considers connected whereas this paper considers path-connected components (the former is more general).                - The other paper only considers multi-label classification, this paper is relevant to all classification and regression problems (the latter is more general).                - The other paper requires that the activation function is "strictly monotonic or ReLU" whereas this paper allows "uniformly approximable with one-to-one functions" activations (the latter is more general).        The result in this paper seems slightly more general but largely similar. Can you please comment on the differences/relation to the other paper?- Proof of Lemma 4:  "Thus the composition \hat{f} is also one-to-one, and therefore a homeomorphism from R^n onto its image I_{\hat{f}}". Is it not necessary that \hat{f} has a continuous inverse in order to be a homeomorphism? I do not immediately see whether the class of activation functions considered in this paper implies that this condition is satisfied. Please clarify. Minor:- Proof of Lemma 5: It seems g is assumed to be continuous at several places (e.g. "... level sets of are closed as subsets of R^n ..." seems to assume that pre-image of a closed set under g is closed, or later "This implies g(F) is a compact subset of R ..."). Perhaps you are assuming that M is a set of continuous functions and using the fact that uniform limit of continuous functions is continuous? Please clarify.- On p.4: "This is fairly immediate from the assumptions on \varphi and the fact that singular transition matrices can be approximated by non-singular ones." Is the second part of the sentence using the assumption that the input space is compact? Please clarify.- Second line in Section 5: i &lt; k should probably be i &lt; \kappa. This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.Overall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.In equation 4.1 describes the GIN update, which is proposed as the most powerful GNN.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isnt made very clear in the paper.The training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR. Pros: The paper tackles an interesting problem of generalization and transfer learning in deep networks. They start from a linear network to derive the theory, identifying phases in the learning, and relating learning rates to task structure and SNR. The theory is thorough and backed up by numerical simulations, including qualitative comparisons to nonlinear networks. The intuition behind alignment of tasks Cons: Most of the theory is developed on a linear network in an abstracted teacher/student/TA framework, where the analysis revolves around the the SVD of the weights. It's unclear to what extent the theory would generalize not only to deep, nonlinear networks (which the paper addresses empirically) but also different structures in the task that are not well approximated by the SVD. In this work, the authors propose Switchable Normalization (SN), which *learns* to switch / select different normalization algorithms (including batch normalization (BN), Instance Normalization (IN), Layer Normalization (LN)) in layers of the networks and in different applications. The idea is motivated by observations (shown in Fig 1) that, 1) different tasks tend to have applied different normalization methods; 2) some normalization methods (e.g. BN) are fragile to very small batch size.The authors propose a general form for different normalization methods, which is a Gaussian normalization and then scale and shift by scalars. Different normalization methods utilize different statistics as the mean and the variance of the Gaussian normalization. The authors further propose to learn the combination weights on mean and variance, which is w_k and w'_k in Eqn (3). To avoid duplicate computation, the authors also do some careful simplification on computing mean and variance with all of the three normalization methods.In the experiment part, the authors demonstrate the effectiveness of the proposed SN method on various kinds of tasks, including ImageNet classification, object detection and instance segmentation in COCO, semantic image parsing and video recognition. In all of the tasks tested, which also cover the common application in computer vision, SN shows superior and robust performance.Pros:+ Neat motivation;+ Extensive experiments;+ Clear illustration;Cons- There are still some experiment results missing, as the authors themselves mentioned in the Kinetics section (but the reviewer thinks it would be ready); - In Page 3 the training section and Page 4, the first paragraph, it mentioned  and ¦ (which are the weights for different normalization methods) are jointly trained and different from the previous iterative meta-learning style method. The authors attribute "In contrast, SN essentially prevents overfitting by choosing normalizers to improve both learning and generalization ability as discussed below". The reviewer does not see it is well justified and the reviewer thinks optimizing them jointly could lead to instability in the training (but it did not happen in the experiments). The authors should justify the jointly training part better.- Page 5 the final paragraph, the reviewer does not see the point there. "We would also like to acknowledge the contributions of previous work that explored spatial region (Ren et al., 2016) and conditional normalization (Perez et al., 2017). "  Please make it a bit more clear how these works are related. This manuscript describes a deep convolutional neural network forassigning proteins to subcellular compartments on the basis ofmicroscopy images.Positive points:- This is an important, well-studied problem.- The results appear to improve significantly on the state of the art.- The experimental comparison is quite extensive, including  reimplementations of four, competing state-of-the-art methods, and  lots of details about how the comparisons were carried out.- The manuscript also includes a human-computer competition, which the  computer soundly wins.- The manuscript is written very clearly.Concerns:There is not much here in the way of new machine learning methods.The authors describe a particular neural network architecture("GapNet-PL") and show empirical evidence that it performs well on aparticular dataset.  No claims are made about the generalizability ofthe particular model architecture used here to other datasets or othertasks.A significant concern is one that is common to much of the deeplearning literature these days, namely, that the manuscript fails toseparate model development from model validation. We are told onlyabout the final model that the authors propose here, with nodiscussion of how the model was arrived at.  The concern here is that,in all likelihood, the authors had to try various model topologies,training strategies, etc., before settling on this particular setup.If all of this was done on the same train/validation/test split, thenthere is a risk of overfitting.The dataset used here is not new; it was the basis for a competitioncarried out previously.  It is therefore somewhat strange that theauthors chose to report only the results from their reimplementationsof competing methods.  There is a risk that the authors'reimplementations involve some suboptimal choices, relative to themethods used by the originators of those methods.Another concern is the potential circularity of the labels.  At onepoint, we are told that "Most importantly, these labels have not beenderived from the given microscopy images, but from otherbiotechnologies such as microarrays or from literature."  However,earlier we are told that the labels come from "a large battery ofbiotechnologies and approaches, such as microarrays, confocalmicroscopy, knowledge from literature, bioinformatics predictions andadditional experimental evidence, such as western blots, or smallinterfering RNA knockdowns."  The concern is that, to the extent thatthe labels are due to bioinformatics predictions, then we may simplybe learning to re-create some other image processing tool.The manuscript contains a fair amount of biology jargon (westernblots, small interfering RNA knockdowns, antibodies, Hoechst staining,etc.) that will not be understandable to a typical ICLR reader.At the end, I think it would be instructive to show some exampleswhere the human expert and the network disagreed.Minor:p. 2: "automatic detection of malaria" -- from images of what?p. 2: Put a semicolon before "however" and a comma after.p. 2: Change "Linear Discriminant" to "linear discriminant." Also, removethe abbreviations (SVM and LDA), since they are never used again inthis manuscript.p. 5: Delete comma in "assumption, that."p. 8: "nearly perfect" -&gt; "nearly perfectly"The confusion matrices in Figure 5 should not be row normalized --just report raw counts.  Also, it would be better to order the classesso that confusable ones are nearby in the list. General:The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE.I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019.Pros:+ The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating.+ It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors.+ I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)).+ The presented results are fully convincing.Cons:- It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented.- The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique?- The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered?Neutral remark:* Another problem, next to the posterior collapse, is the hole problem (see Rezende &amp; Viola, Taming VAEs, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak &amp; Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors opinion on this matter.* Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit. In this paper, the authors propose a method for dimensionality reduction of image data. They provide a structured and deterministic function G that maps a set of parameters C to an image X = G(C). The number of parameters C is smaller than the number of free parameters in the image X, so this results in a predictive model that can be used for compression, denoising, inpainting, superresolution and other inverse problems.The structure of G is as follows: starting with a small fixed, multichannel white noise image, linearly mix the channels, truncate the negative values to zero and upsample. This process is repeated multiple times and finally the output is squashed through a sigmoid function for the output to remain in the 0..1 range.This approach makes sense and the model is indeed more principled than the one taken by Ulyanov et al. In fact, the DIP of Ulyanov et al. can hardly be considered "a model" (or a prior, for that matter), and instead should be considered "an algorithm", since it relies on the early stopping of a specific optimization algorithm. This means that we are not interested in the minimum of the cost function associated to the model, which contradicts the very concept of "cost function". If only global optimizers were available, DIP wouldn't work, showing its value is in the interplay of the "cost" function and a specific optimization algorithm. None of these problems exist with the presented approach.The exposition is clear and the presented inverse problems as well as demonstrated performance are sufficient.One thing that I missed while reading the paper is more comment on negative results. Did the authors tried any version of their model with convolutions or pooling and found it not to perform as well? Measuring the number of parameters when including pooling or convolutions can become tricky, was that part of the reason?Minor:"Regularizing by stopping early for regularization,"In this paper "large compression ratios" means little compression, which I found confusing. Brief summary:This paper presents a deep decoder model which given a target natural image and a random noise tensor learns to decode the noise tensor into the target image by a series of 1x1 convolutions, RELUs, layer wise normalizations and upsampling. The parameter of the convolution are fitted to each target image, where the source noise tensor is fixed. The method is shown to serve as a good model for natural image for a variety of image processing tasks such as denoising and compression.Pros:* an interesting model which is quite intriguing in its simplicity.* good results and good analysis of the model* mostly clear writing and presentation (few typos etc. nothing too serious).Cons and comments:* The author say explicitly that this is not a convolutional model because of the use of 1x1 convolutions. I disagree and I actually think this is important for two reasons. First, though these are 1x1 convolutions, because of the up-sampling operation and the layer wise normalizations the influence of each operation goes beyond the 1x1 support. Furthermore, and more importantly is the weight sharing scheme induced by this - using convolutions is a very natural choice for natural images (no pun intended) due to the translation invariant statistics of natural images. I doubt this would have worked so well hadn't it been modeled this way (not to mention this allows a small number of parameters).* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice. Natural images are only approximately piece-wise smooth after all.* The use of the name "batch-norm" for the layer wise normalization is both wrong and misleading. This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it's implemented with the same function) as there is no "batch".* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation. Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell)* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor? I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context). This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezumas revenge, Pitfall!, and Private eye over a factor of 2. It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning. In comparison to the existing approach, this paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The results show a significant gain in performance as well as the capability of the model to generalize composition program to unseen data effectively. The qualitative analysis shows that the proposed model not only get the correct answer but also the correct behavior that leads to the answer. [Summary] The authors propose a method of using convolutional neural networks to determine whether large boolean formulas (containing hundreds of variables and thousands of clauses) are satisfiable. The resulting models are accurate (correctly distinguishing between satisfiable and unsatisfiable formulas more between 90% and 99% of the time, depending on the dataset) while taking 10x - 100x less time than an off-the-shelf solver (Z3), offering slightly better quality on some problems and slightly worse quality on others. In addition to determining whether formulas are satisfiable, the authors propose and evaluate a method for finding satisfying assignments. They also evaluate their system on SMT benchmarks, where it also shows 10x-100x speed-ups, albeit with somewhat lower accuracy (e.g., 73% - 92% accuracy; I couldn't find baselines for these experiments).[Key Comments] Unless I'm missing something major, I'd prefer to accept this paper, since the problem appears novel and the experimental results seem very promising for a first paper on a new problem.[Pro 1] The paper seems polished and well-written. I generally found it well-motivated and easy to follow.[Pro 2] To the best of my knowledge, the problem domain (machine learning for satisfiability problems that are so large that they are difficult to solve using conventional methods) is both novel and well-motivated.[Pro 3] Algorithms seem conceptually straightforward (but might be a bit challenging to implement in practice due to the large input size), and yield excellent results. The magnitude of speed-ups reported in the paper (10x - 100x) is large enough to be exciting from a research perspective, and also seems like it should be large enough to have significant practical applications.[Pro 4] Results are evaluated on a variety of different boolean satisfiability and SMT problems.[Con 1] To improve reproducibility, it would be helpful if the authors could provide more details about their model training setup. Figure 2 is a good start, but adding details about the layer sizes, types of pooling layers used, and the model training setup would help clarify the experiments.[Con 2] It seems like a significant number of labeled training examples (i.e., examples that are already known to be satisfiable or unsatisfiable) are needed in order to train a neural network. This seems like it could present a bootstrapping problem for certain domains: it may be computationally expensive to generate ground-truth labels for training examples, but a significant number of labels are needed to train a good prediction model. I'd be very interested to see a study of how well trained models transfer across domains: how well do models trained on one domain (e.g., a synthetic problem where labeled training data is cheap to generate) transfer to a different domain (e.g., a real-world problem where training labels are expensive to compute)? However, this is a minor point for a first paper on a new problem, and I think the paper is interesting enough to merit acceptance without such an analysis. The paper presents a new model for reading and writing memory in the context of task-oriented dialogue. The model contains three main components: an encoder, a decoder, and an external KB. The external KB is in the format of an SVO triple store. The encoder encodes the dialogue history and, in doing so, writes its hidden states to memory and generates a "global memory pointer" as its last hidden state. The decoder takes as input the global memory pointer, the encoded dialogue state history, and the external KB and then generates a response using a two-step process in which it 1) generates a template response using tags to designate slots that need filling and 2) looks up the correct filler for each slot using the template+global memory pointer as a query. The authors evaluate the model on a simulated dialogue dataset (bAbI) and on a human-human dataset (Stanford Multi-domain Dialogue or SMD) as well as in a human eval. They show substantial improvements over existing models on SMD (the more interesting of the datasets) in terms of entity F1--i.e. the number of correctly-generated entities in the response. They also show improvement on bAbI specifically on cases involving OOVs. On the human evaluation, they show improvements in terms of both "appropriateness" and "human-likeness". Overall, I think this is a nice and well-motivated model. I very much appreciate the thoroughness of the evaluation (two different datasets, plus a human evaluation). The level of analysis of the model was also good, although there (inevitably) could have been more. Since it is such a complex model, I would have liked to see more thorough ablations or at least better descriptions of the baselines, in order to better understand which specific pieces of the model yield which types of gains. A few particular questions below:- You describe the auxiliary loss on the global pointer, and mention an ablation study that show that this improves performance. Maybe I am overlooking something, but I cannot find this ablation in the paper or appendix. It would be nice to see how large the effect is. - Following on the above, why no similar auxiliary losses on additional components, e.g. the template generation? Were these tried and deemed unnecessary or vice-versa (i.e. the default was no auxiliary loss and they were only added when needed)? Either way, it would be nice to better communicate the experiments/intuitions that motivated the particular architecture you arrived at.- I really appreciate that you run a human eval. But why not have humans evaluate objective "correctness" as well? It seems trivial to ask people to say whether or not the answer is correct/communicates the same information as the gold. This is, in general, a well-written paper with extensive experimentation. The authors tried to describe their architecture both with equations as well as graphically. However, I would like to mention the following: In Section 2.1 I am not sure all the symbols are clearly defined. For example, I could not locate the definitions of n, l etc. Even if they are easy to assume, I am fond of appropriate definitions. Also, I suspect that some symbols, like n, are not used consistently across the manuscript.I am also confused about the loss function. Which loss function is used when?I am missing one more figure. From Fig 2 it's not so straightforward to see how the encoder/decoder along with the shared KB work at the same time (i.e. not independently)In Section 2.3, it's not clear to me how the expected output word will be picked up from the local memory pointer. Same goes for the entity table.How can you guarantee that that position n+l+1 is a null token?What was the initial query vector and how did you initialise that? Did different initialisations had any effect on performance?If you can please provide an example of a memory position.Also, i would like to see a description of how the OOV tasks are handled.Finally, your method is a NN end-to-end one and I was wondering how do you compare not with other end-to-end approaches, but with a traditional approach, such as pydial?And some minor suggestions:Not all the abbreviations are defined. For example QRN, GMN, KVR. It would also be nice to have the references of the respective methods included in the Tables or their captions.Parts of Figs. 1&amp;2 are pixelised. It would be nice to have everything vectorised. I would prefer to see the training details (in fact, I would even be favorable of having more of those) in the main body of the manuscript, rather than in the appendix.There are some minor typos, such as "our approach that utilizing the recurrent" or "in each datasets" This paper presents a very interesting interpretation of the neural network architecture.I think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation.Also, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. So I think this work has significant value for the community.The paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.)\questionsIn the experiment when you mention about "embed solvers as a replacement to their corresponding blocks of layers", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss.In the experiment, the author mentioned that  "what happens if the algorithm is applied for multiple iterations?". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm?\minor commentsThe definition of \lambda_max below Eq(3) are not shown, thus should be added. SummaryThe authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms.The method consists of a) mining documents that refer to the same topic, b) extracting from these documents parallel sentences, c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level.Novelty: the approach is novel.Clarity: the paper is clearly written.Empirical validation: The empirical validation is solid but limited. The authors could further strengthen it by testing on low-resource language pairs (En-Ro, En-Ur).It would also be useful to report more stats about the retrieved sentences in tab. 1 (average length compared to ground truth, BLEU using as reference the translation of a SoA supervised MT method, etc.)Questions1) Sec. 3.2 is the least clear of the paper. The notation of eq. 7 is quite unclear because of the overloading (e.g., P refers to both the model and the empirical distribution).I am also unclear about this constraint about matching the topic distribution: as far as I understood, the model gets only one gradient signal for the whole document. I find then surprising that the authors managed to get any significant improvement by adding this term.Related to this term, how is it computed? Are documents translated on the fly as training proceeds? Could the authors provide more details?2) Have the authors considered matching sentences to any other sentence in the monolingual corpus as opposed to sentences in the comparable document? This paper proposes an image completion method that can deal with large-scale missing regions. The proposed method employs a co-modulation technique to bridge the gap between conditional and unconditional GANs. It can then take the advantages from both sides with the consistency offered by conditional GANs and stochasticity provided by the unconditional GANs. The paper also proposes new image quality metrics, Paired/Unpaired Inception Discriminative Scores (P-IDS/U-IDS), for measuring the image quality of the inpainted images. Experiments show that the proposed method significantly outperforms DeepFillv2, a state-of-the-art freeform image completion method, on examples with large missing regions. Overall, I like the idea and the papers results. It represents a clear progress to the image completion problem with large missing regions. **Strengths**+ The idea is novel and makes sense. It is simple yet effective to the target problem. The idea of extending modulation to co-modulation is interesting. The co-modulation architecture design allows the proposed method to explore the generation capability of the unconditional GANs while maintaining the consistency of the completed image.+ The results are excellent. The paper and the appendix's visual results show that the proposed method can inpaint images with large missing regions very well. + The paper is generally well-written. Figure 2 illustrates the basic idea very well. **Weaknesses**- Although the experiments show a good correlation of the proposed P-IDS/U-IDS metrics to the user study results, it is not clear how well the metrics reflect perceptual fidelity. Do these metrics work best for measuring the quality of inpainted images? How were the fake images generated for fitting the linear SVM? Is it possible to generalize to other image restoration problems?   - Although the main target is image completion, the paper claims that the proposed co-modulated GANs also works well on image-to-image translation. However, the paper only demonstrates the application on a simple edge2image problem, from edge images to commodity images. It would be better if the paper presents more image-to-image translation examples for validating how well the proposed method can handle image-to-image translation. In this paper, the authors propose a general approach for image completion with large-scale missing regions. The key is to combine image-conditional and modulated unconditional generative architectures via co-modulation. The presented approach has demonstrated strong performance in the image painting with large-scale missing pixels and some image-to-image translation tasks. A new metric P-IDS/U-IDS is proposed to evaluate the perceptual fidelity of inpainted images.Strength:- The idea of co-modulation is quite interesting and has demonstrated strong results in various tasks.- The paper is well-written and well-motivated.- The solution combines the best of two worlds in image-conditional and unconditional image generation.- A new metric P-IDS/U-IDS is proposed for perceptual evaluation, verified by the correlation to human preferences.Weakness:- Only one image inpainting DeepFillv2 is compared in the experiment. Other image inpainting methods such as gated convolution, partial convolution can be also evaluated.- The results in the main paper are mainly faces that have structured information. It would be good to move some outdoor results in the supplement to the main paper.  The contributions of this paper are the following: - it extends previous work on binary neural networks to the case of deep generative models which perform density estimation (VAEs and Flows) - the results show that the proposed approach works well, with the expected trade-off but closely matching the much larger real-valued networks - in order to do so, it introduces a novel technique for binarizing weight-normalized layers, which are used in these generative models - the results show the advantage of this technique, and they illustrate it is particularly important for ResNet layers.The paper is very clear (except for one element noted below). Originality is clear but not very high, as this contribution may be seen as a low-hanging fruit, albeit a useful and well-executed one.  As these kinds of architectures are becoming more and more used in various settings, the techniques used here (e.g. using ResNet layers) may be applicable more widely, increasing the significance of this work.Minor fixes suggested:* The semantics of equations 3 and 6 is not clear. I imagine the objective is to explain how the gradient on the binary weights and activations is converted into a gradient on the real-valued ones, but this should be explicited.* In page 1 (bottom line) and 4 (1st line after eqn 9), the authors cite Rezende & Mohamed 2015, Dinh et al 2017 , but they should include first in this list the Dinh et al 2014 paper which actually introduced the notion found in Flow models, albeit under a different name (NICE).* Appendix A: the authors should also show the images generated with full precision, so we can compare visually. I like this paper and I think it represents a very through-provoking and promising idea. The key aspect of this work is a (screen-space) masked encoder that learns to complete the loop with a previously trained generator. This allows for image completion, editing, collages, and essentially creates a prior for the generators training distribution. This allows the application to snap the (possibly partial) input to the manifold. While there are many encoders for GANs, I have never seen them formulated quite this way. The idea is also thoroughly compared and ablated.Okay, with that out of the way, I can specify that these comments relate to the 22-page version including the supplement. The 8-page version is too terse and I found myself longing for details and discussions that were nowhere to be found (except in the supplement). For example, Eq. 4 is the beef, but its not really discussed in any meaningful way. Yeah, ok so you added a mask? But what does that _mean_? Given that now the encoder has only a random subset of pixels at its disposal and yet it needs to produce the full latent code, it would seem that it needs to learn very flexible ways of coming up with a plausible latent. Thus I think it starts to participate in generative modeling in a much more real sense than a typical full-image encoder does. As a reader I would like to see this kind of discussions on top of the mechanical descriptions. Now, the real issue of course is the 8 page limit combined with a verbose layout, which suits many papers very well, but is a poor fit to multi-application works like this one. ICLR (and other conferences) should consider going to 10 pages with some length must be proportional to the contribution clause, because papers like this would be faster to write AND read if they were a bit longer. They would also be genuinely better. As an official reviewer I will read the supplement, but most others will not. Getting back to this paper: Obviously the authors have chosen the subset of material they felt was most compelling, and I wont argue with that, but personally I didnt like e.g. Sec 4.3 nearly as much as the discussion in A.2.1 (minus dataset rebalancing). I felt the former was unsurprising while the latter more clearly illustrated the key contribution, but thats just a personal opinion. Sec 4.4 / Fig. 7 were very interesting for me, so please let those stay :) As a summary, I find the idea clearly described, interesting, and likely to have many applications in future. Thus I recommend acceptance. Minor:- Fig 16: Are you sure your Poisson solver isnt broken? I would have expected much better results in that column.- Sec 4.4: Most of the time c is called component, but at least once its a segment instead. Should be component.- Sec 4.3: to obtained the blended image > to obtain the blended image **PAPER SUMMARY**The paper trains feedforward networks to project input images into the latent space of a pretrained GAN generator, and shows how this can be used both for various image editing tasks as well as to probe the internals of the trained generator. Experiments demonstrate that this can be used for tasks such as image composition, image completion, attribute modification, and multimodal image editing.**STRENGTHS**- The paper is well-written and easy to follow- The proposed method is both simple and effective for a variety of tasks- Experimental results (both qualitative and quantitative) are impressive and thorough- Extensive supplementary material providing additional details to aid reproduction**WEAKNESSES**- Some missing references to BiGANs / ALI**MISSING REFERENCES**One missing line of related work is that of BiGANs [1, 2] / Adversarially Learned Inference [3] in which an inference network is jointly trained with the generator and discriminator to project samples into the latent space. However the overall goal of these papers is often some kind of unsupervised feature learning, which is very different from the image editing applications presented in this submission.[1] Donahue et al, Adversarial Feature Learning, ICLR 2017[2] Donahue and Simonyan, Large Scale Adversarial Representation Learning, NeurIPS 2019[3] Dumoulin et al, Adversarially Learned Inference, ICLR 2017**OVERALL**On the whole this is a strong paper. The method is simple and effective, the results are impressive, the experiments are thorough, and the paper is very well-written and easy to follow. This is a clear accept in my view. The main idea in this paper is very nice and is as follows:Suppose we have the standard IV setup which isX = m(U) + f(Z) Y = q(U) + b(X)Where U is some unobserved confounder.Typically we are interested in the linearized version of this model. If we regress Y on X, we get a biased estimate of B(X) because U is a common unobserved cause. We can deal with this by first regressing X on Z, then regressing Y on the predicted values of X given Z. It is well known that using any other method than linear regression in this context leads to "forbidden regression" problems. So, typically people just estimate the average effect.However, with bigger data we might be interested in trying to get more precise estimates of the function b(X) and also f(Z). Typically this is done by hand-crafting a basis (interacting instruments with things, usually). The authors propose using deep networks to learn linear bases and then essentially do 2SLS on top of them (technically this is all trained together, but I am simplifying for the review).Overall, while I think there are many unanswered questions in this paper, I think it should be accepted for the community to build on it. IV is a tricky area and there is no way to show everything we need to know about a method in one paper.Below I list several questions I had about the paper which I think the authors could incorporate in various formats (or educate me on how they have already answered them and I missed it):- Assumptions requiredThe authors state that the only assumption that is required is the standard exclusion restriction. I feel like that is not necessarily correct. For example, in 2SLS if we assume underlying heterogeneity (which, we must assume in this case otherwise why would we be learning a basis for this heterogeneity?) then if it the heterogeneity is not proxied by observed variables correctly we need some additional assumptions like monotonicity to get an interpretation of the IV estimate as some kind of causal effect (e.g. a LATE). Similarly, even with no covariates if we're trying to estimate the full function b(X), don't we need some assumption about how the IV affects X in the whole support in order to guarantee that we can use the IV to learn b(X)?- Regularization parameters As I understand it, and the authors can correct me if I'm wrong, the hyperparameters for the regularizers are set by holding out some (x,z) and some (y, x) and then evaluating standard out of sample predictive loss? This is also what is done in DeepIV. This has been shown to not be an optimal way of choosing hyperparameters in Peysakhovich and Eckles (2018) which talks about "Causal Cross-validation" for adapting split-sample IV ideas from (Angrist & Krueger, 1995; Imbens et al., 1999; Hansen & Kozbur, 2014). The setting in that paper is specifically discrete instruments rather than continuous ones, so that procedure can't exactly be adapted in all the cases here, but it still may be worthwhile to mention as a future direction. Overall, hyperparameter tuning for these problems is really important and even more so when you have these deep feature generators.- Comparison to control functions / 2 stage residual inclusionCurrently the only comparison made are to other 2sls type methods where endogenous variables are changed to their predicted values. There is another way to estimate IV which is the control function/2 stage residual inclusion approach which seems to lend itself naturally to neural networks/function approximators since all you need to do is include the residual from the first stage into the second stage to get a correct estimate of b(X). It seems like it would be pretty easy to add this as a baseline to at least the high-D demand experiment.Typo:The abstract states: "In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments." I believe this should say "linear features" in the sense that you are learning the correct basis for 2SLS? The paper proposes a new discrepancy between probability distributions called the sliced Kernelized Stein Discrepancy.  It is based on the idea of computing the standard KSD on random 1 dimensional projections and then average those. Some other proposed variants are based on optimizing over the projections.The motivation behind this new discrepancy is to overcome the 'curse of dimensionality' that standard KSD suffers from. Thus the main selling point is that the sliced version has a much better behavior as the dimension increases.The authors then propose to use the new divergence for two applications: goodness of fit testing and for models learning.The robustness to increasing dimensions is illustrated through multiple experiments on both tasks and yields convincing results.Strength:- The proposed framework is neat and the experiments are thorough and convincing with many additional discussions and results in the appendix. - The proposed method is relatively easy to implement and seems to address several known limitations of KSG and in SVGD: the scaling with dimensions.Weaknesses: The paper is a bit dense with many references to the appendix. However the main idea is clearly explained  and the advantage is clear both in terms of theory and throughout the experiments.  Questions:- In 4.1.1, the distributions p and q although high dimensional, they often have independent components. This might be very advantageous for the sliced version of the algorithm, especially when using a set of orthogonal projections for the projections $r$. What happens to the Null rejection rate when more dependence between the dimensions is introduced ? What is the exact parameter choice for the multivariate-t distribution, I couldn't find this in the appendix? Minor remarks. - A discussion on the limitations of existing methods KSD and SVGD could be useful as a transition from section 2 to 3 to motivate the slicing.- Figure 1 is a bit dense especially with all the equations- The subscript notation $f_{rg}$ is sometimes confusing as f depends on $r$ and $g$ only implicitly after optimizing the objective in 5. It might be worth either mentioning where this dependence comes from or even remove it.- In the paragraph right after corollary 3.1. The authors mention a limitation of a particular version of Max Sliced KSD over the other but then refer to appendix F without really saying what this limitation is. It might be worth saying a little bit more about those limitations. This paper studies ways of adding edges to graphs to improve the result of spectral embedding / clustering. It refines existing embeddings using by measuring edges' effect on Laplacian eigenvalues, and adjusting such edges to reduce the distortions. The performance of the algorithm is justified using developments of worst-case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy.I find the approach taken by this paper quite interesting: node embeddings are now widely used to preprocess graphs into vector data more friendly to learning pipelines. Many of these methods add additional edges imperatively / via local methods, without taking the overall data set into account. Iterating this process based on the overall embedding is an interesting, but algorithmically more intensive approach. To carry this out, the authors combined a variety of interesting tools, as well as some high performance packages that they developed. I feel the novelty of this combination, as well as the gains obtained, should make this result of broad interest to those working on spectral clustering/embeddings. Summary: The paper addresses the important topic of understanding why self-supervised learning methods show very good performance when used as pretraining for fine-tuning tasks. Authors analyse in detail the difference in performance between self-supervised and supervised pretraining and propose a new method to train model which improves over standard supervised models when used as pretraining.Strengths:- I think the paper addresses a relevant problem. Understanding difference between self-supervised and supervised pretraining is relevant to advance in this field. I particularly like the amount of evidence the paper provides to back all the claims, and the originality of some of the experiments such as Figure 2.- The paper is well written and motivates the issue very well. I think it's particularly interesting to question traditional training techniques such as cross-entropy training, when the models are planed to be used for a different goal.- The insights of the transferability experiments are useful for the community as point the strengths and weaknesses of each methods. I think it's good that authors analyse many different tasks such as classification, detection and segmentation with variate datasets. - The proposed learning loss utilising the labels to produce the negatives is simple and seems to produce promising results according to Table 4 and Table 5. - The extensive supplementary materials are also useful for additional information.Weaknesses:- It would have been interesting to see whether this presented results comparing MoCo with supervised pretraining hold with other self-supervised methods such as SimCLR. Do authors have any intuition on that? Are some of those effects from the particularities of the MoCo training or can we generalise to all self-supervised methods?- Some Table references might be wrong. Section 2 refers to Table 7 and 8 which are in the supplemental (probably referring to Table 1 and Table 2).- The face landmark task is a bit outside the main story of the paper. It is introduced very late in the paper and it is not clear where the proposed loss helps. I believe authors should clarify this points.Conclusion: I believe the paper is strong enough for publication. I think it would be good for the reader if authors clarify a bit more the face landmark section and discuss a bit how would this compare to other self-supervison methods, but overall the paper is very good.  In this work, the authors illustrate an approach for learning logical rules starting from knowledge graphs. Learning logic rules is more interesting than simply performing link prediction because rules are human-readable and hence provide explainability.The approach seems interesting and the tackled problem could interest a wide audience. It does not seem extremely novel, but it seems valid to me.The paper is well-written and self-contained. Moreover, the experimental results show that the proposed approach has competitive performance comparing to other systems (even compared with systems that do not learn rules but perform only link prediction).For all these reasons I think the paper should be accepted for publication. # The main idea: Developing an unsupervised imagedenoising approach is difficult because the real world noise distribution is much more complex than the simplified AWGN assumptions. The authors address this by mapping the noisy image into a latent space in which. AWGN assumption holds, and thus any existing gaussian denoiser is applicable.  The authors claim that the network can be trained with the input image itself and does not require any pertaining in other datasets.This is a clever idea, it essentially is MAP gaussian denoising with a variational auto encoder as the likelihood model.Since it is an image-specific network it reduces the need for wide-space sampling fo the data distribution.  How fast is it?  The authors optimize with ADMM iteratively updating network weights and image estimates (which can be time consuming).  No mention is made of compute time when compared to comparable methods.The encoder and decoder network shapes and architectures are chosen without much reasoning given other than that they are standard choices.  Did the authors explore any hyper parameter sweeps (of the Unet architectures specifically) in this space to see if performance was further improved or impoverished by different choices?Strength:Can be utilized with other existing gaussian denoisers to improve their performance.Visual Results are high quality and show clear improvement for baseline methods.Weaknesses:The visual improvement for NN+DnCNN is debatable though the other results show significant improvement. (At least in the chosen examples)The following is stated as fact but is not backed up with data.It is noted that there is an expectation term in (4) that can be estimated by the Monte Carlo method and there is no need for a large amount of samples in practice. In our experiments, the sampling number is equal to 1.Please fix:On page 1 you write 'Due to the violence of the AWGN assumption I assume this is supposed to read Due to the violation.Also, on page 5 you write :"Remark 1 In our model, the basic assumption is that the latent image z is a white Gaussian per- turbations of clean image x, i.e. z|x < N (x, Ã2I). Since z = G¸2 (y), the second term in (4) can be seen as the transformed data fidelity while the first term is a regularization such that the encoder avoids from the trivial mappings, e.g. identity map or zero map." The first term, which is  1Eµ||F¸1(G¸2(y)+µ)y||2  does prevent a zero mapping, but does not on it's own prevent the identity map (it is in fact encouraging it).  However, the combination of the first, second ( 1 ||G¸2(y)x||2) and third (»R(x)) is what pushes against the degenerate identity mapping solution.  Please update this sentence to be more correct. ##########################################################################Summary: The paper provides a comprehensive analysis of the benefits of curriculum learning in different application scenarios. This includes investigating the phenomenon of implicit curricula, showing if the examples are learned in a consistent order across different architectures, and exploring the influences of explicit curricula in the standard and emulation settings. The paper empirically shows that curriculum learning has marginal benefits for standard training, but is helpful when the training time is limited or the training data is noisy.##########################################################################Reasons for score: I vote for accepting the paper. I believe the analyses presented in the paper can be valuable for the community. I like the implicit curricula experiments, showing that the difficulty of an example is somewhat independent of the training method. Other empirical observations are also interesting. In general, I think the paper provides a satisfactory answer to the question raised in the paper title (when do curricula work?) ##########################################################################Pros: This paper has extremely comprehensive evaluations, examining the influence of curriculum learning (curriculum/anti-curriculum/random-curriculum) in diverse settings (standard/limited training time budget/noisy data). The methodology for the evaluations is technically sound;The findings presented in the paper can be valuable for the community: (1) the difficulty of an example is somewhat independent of the training method; (2) curriculum learning provides little benefit for standard training, but help for limited time and noisy training;The paper is well written. It is a thoroughly enjoyable experience to read the paper.##########################################################################Cons:I found few weaknesses in the paper. I include a question below which I hope could be clarified:The learned iteration of a sample is defined by the first epoch from which the prediction remains correct for all subsequent epochs. I wonder if there is any sample that is predicted correctly in earlier steps but incorrectly in later steps (e.g.  the forgettable examples). How to handle them in the implicit curricula experiment?##########################################################################Minor comments: Page 16: two data loader  two data loaders######################################################################### Summary:This work presents a goal-conditioned RL, which estimates probability density using a classifier. Strengths:+ The problem is well explained, the logical structure seems adequate. + The paper is well written and clear. + The approach technically sounds and mathematically well-formulated.Weaknesses:- Although the reported evaluation results are competitive to baselines, it would have been even stronger if the performance is substantially improved. Do you have any insight on how better results can be achieved?  #####################Summary and contributions:The paper provides a novel approach to normalizing flows. It models a normalizing flow as the gradient map of a convex potential function.  The gradient estimation and model inversion, that are the computationally expensive part of arbitrary complex NFs, are formulated as cheap convex optimization problems. It proves that  the proposed CP-flow is a universal density approximator and also shows that it is an optimal transport (OT) map. #####################Reasons for score:I vote for clear acceptance. The idea of convex potential flow sounds very interesting and the way the computational complexities are very useful for designing more advanced NFs. The paper is very well written. Strength:1. The paper is very well written and structured and is easy to read for a wide audience. It also provides a good review of main papers in the literature2. The motivation for using CP-flow is well explained. 3. The claims are well supported by theoretical proofs and empirical studies.   4. The gradient estimation and model inversion, that are the computationally complex part of arbitrary complex NFs, are formulated as convex optimization problems that gain advantage of fast converging and cheap optimization algorithms and it also leverages the efficient Hessian-vector product computation.5. The experimental results show the proposed flow can perform competitively with much less number of parameters .#####################Additional Feedback and Questions:1. After reading the paper it is not quite clear why we do need an optimality in Monge sense (Theorem 4) and what is the point of optimal transport in this work. It is worth expanding or adding more insights to the motivation given in the Introduction by the notion of rearrangement cost. 2. Orthogonality or Lipschitzness constraints are mentioned without citation in introduction.3. What is so special about the soft-plus type nonlinearity in theorem 3, can we use other non-linearities such as the symmetric-log derived in [1] that are monotonic and differentiable by construction ?4. CP-flow looks more expressive than NAF in toy examples but why is it outperformed in density estimation? As a more insightful comparison, I suggest comparing the transport cost of the NAF with that of ICNN in section 5.2 (Figure 4) as NAF is outperforming ICNN in all the benchmarks and is universal.5. It is worth comparing the number of parameters of the optimal CP-flow in Table 1 with the available methods to have a better understanding of models flexibility. Maybe, CP-flow can achieve the SOTA if the number of parameters are normalized.6. I wonder if CP-flow can outperform the Residual flow which uses a similar gradient estimator routine if its number of parameters, in Table 2, is increased.7. Compared to RealNVP, CP-flow requires an optimization solution per each training update. Also, as noted in the paper, the computational cost of the CP-flow is less than residual flows as it saves the Spectral normalization process. So how the speed and convergence rate of CP-flow is compared against the benchmarks, assuming all are using the same hardware e.g GPUs (and cvx opt are implemented in GPU). It looks better to have a sense of it in one of the experiments. 8. I am willing to see the randomly generated sample of the CP-flow especially to compare its local/global stability against the Residual flow due to its Lipschitz constraint as discussed in [2].9. A schematic architecture of the ICNN model helps better understand it.Ref:[1] M. Karami, D. Schuurmans, Jascha Sohl-Dickstein, Daniel Duckworth, Laurent Dinh, Invertible Convolutional Flow", Advances in Neural Information Processing Systems (NeurIPS) 2019,[2] Behrmann, Jens, et al. "Understanding and mitigating exploding inverses in invertible neural networks." arXiv preprint arXiv:2006.09347 (2020)  Estimation of the size of the support of a distribution over a discreet domain is a fundamental problem. In the standard setting, this problem is theoretically well-understood with matching upper and lower bounds. The authors assume additional access to a constant approximation of the density function at each point, and then show that this can provably reduces the sample complexity. In particular, they offer matching upper and lower bounds in this setting. While the upper bound is a twist on the existing state-of-the-art method, the lower bound seem to deviate from that. One may say that the problem of density estimation is harder than estimating the size of the support of a distribution, and therefore assuming access to such oracle is not natural. However, the authors provide practical evidence that in some cases this is reasonable. In particular, the authors use a learning-based method for estimation of the density function and plug it in as the oracle for their support estimation method. The results are promising, and seem to be more robust than a previous method that assumed access to an accurate pdf oracle.It will be quite interesting to discuss the kind of structure that is present in the data sets that allows to improve over the WY method. For example, is it the case that WY method performs poorly compared to the proposed method for light-tails distributions (and not so much for heavy tailed distributions)?The paper is well written and the background work is adequately discussed. For some reason the submission format does not allow selecting/highlighting text in the pdf file. Please check this.   The paper is a theoretical analysis of two different classes of graph neural network: 1) GNN based on neighborhood aggregations (GNN) and 2) feature augmentation before MLP (GA-MLP).The author contributions may be summarized as:- exhibiting graphs/problems that may be handled by one method and not the other,- giving an upper-bound on the number of equivalent clases induceds by linear GA-MLP in term of walks in a rooted tree.- showing a gap in expressivness between GNN and GA-MLP- showing that the choice of operator in GA-MLP is crucial and may have expressive power beyond WL.Pros:+ The paper is well written.+ The theoretical analysis performed in the paper is new, non trivial, and very interesting, giving light on these two architectures.Remarks:- Section 3.1: shouldn't 2-EXP be: log log is polynomial (instead of linear)?- Section 5, corollary 1: 'poly-exponential'?Questions:- How to go beyond linear operators in section 5?Typos:- after proposition 6: in words -> on other words- few sentences without verb and missing punctuation in the appendix- many references are incomplete, e.g. On the universality of invariant networks is ICML'19, What graph neural networks cannot learn: depth vs width is ICLR'20, etc Conformal prediction (CP) allows for the selection of a set of candidate answers guaranteed to contain the correct answer with some probability. The authors propose two extensions to CP, 1. To extend validity for all admissible answers, 2. Using prediction cascades to improve computational efficiency. The authors show that their approaches provide similar guarantees on accuracy like CP but with lowered predictive efficiency and computational cost.Reasons to accept: 1.  The approaches are simple, novel, and interesting and they come with theoretical guarantees 2. The proposed methods allow for CP to be extended to some realistic tasks3. Impressive results showing lowered predictive efficiency and computational cost4. The paper was well written and the approaches and experiments seem technically soundOverall, I believe that this would be a useful paper for the community, and based on the reasons given above I would recommend acceptance.  ### 1. Brief summaryThe authors note that in classification tasks there typically exist within-class groups of similar images that are not explicitly encoded in the coarse class label -- they call this intraclass clustering. They hypothesize that the ability of DNNs to recognize these intraclass clusters without being explicitly told about that could correlate with generalization. They then proceed to verify this on a range of networks, architectures, and a large number of hyperparameter configurations. They take care to establish causality where possible. In addition, they show that the intraclass clustering can be detected with simple variance-based methods, and that it emerges early in training.### 2. Strengths* This is a super interesting question and I really like the paper overall.* I appreciate that you looked at a large hypercube of hyperparameters to establish correlation with generalization* I also like the care you put into establishing causality* The fact that you tried a simple variance based measure is also really good, especially given that it is very predictive!### 3. WeaknessesI think this paper is really good, I have nothing much to point out here. Possible a large range of architectures and scaling up to ImageNet would be useful to establish that this scales all the way to very large data, but it is very good as is!### 4. Related papers that you might like[1] You cite Stiffness: A New Perspective on Generalization in Neural Networks by Stanislav Fort, PaweB Krzysztof Nowak, Stanislaw Jastrzebski, Srini Narayanan (https://arxiv.org/abs/1901.09491) as measuring an amount of class-specific clustering. In that paper in Figure 9 they show that stiffness is aware of the super-classes of CIFAR-10 and even their super-super-classes (animals, etc.), which seems relevant here too. Though there it goes the other way round -- the training is on subclasses, generating awareness of superclasses. Your results are I'd stay stronger than that.[2] In talking about the sensitivity to early stages of training The Break-Even Point on Optimization Trajectories of Deep Neural Networks by Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, Krzysztof Geras (https://arxiv.org/abs/2002.09572 and ICLR 2020) might be relevant, where they also establish a very strong effects of the early stages of training.[3] Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel by Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel Roy, Surya Ganguli (https://arxiv.org/abs/2010.15110 and NeurIPS 2020) also shows a very strong effect of the early stages of training on a large number of DNN measures.### 5. SummaryThis paper is really good! It starts with a strong hypothesis, verifies it on a large number of experiments, is mindful of causality and generates a potentially practically useful insight into generalization. Well done! This paper proposes a visual-semantic embedding model useful for generalized zero-shot learning. The proposed model transforms an image into a label classifier, which is then used to predict the correct label in the semantic space. The paper is well constructed and easy to read. It provides a good presentation of some related work and identifies the contributions as compared to existing approaches.The experimental validation is performed on four popular public datasets and compares the performance to several state of the art approaches. The obtained performance shows similar/promising results as compared to the state of the art.From my perspective, the paper is missing some experimental analysis/comparison to some recent methods that are inductive only to samples and also some methods that are transductive for unseen class prototypes and unlabeled unseen test instances (for instance, papers mentioned in Section 4.3). First, that comparison will allow evaluating the performance of the proposed approach to more recent papers than the ones used in Section 4.2. Second, it seems that these methods, specifically the ones that are transductive for unseen class prototypes, achieve a much higher performance and it's important to evaluate the performance of the proposed method in that setting or to report on the performance loss when someone decides to use this approach in that specific setting (inductive to both unseen images and unseen semantic vectors).  A discussion that addresses the above questions/concerns could do it too. *****************************Summary: This paper focuses on self-supervised contrastive learning. Previous contrastive learning methods heavily rely on a large number of negative samples. This paper proposed a novel method with an additional margin term, and mathematically investigate the relationship among the margin term, the temperature, and the number of negative samples. The number of negative samples can be significantly reduced by tuning the margin term, while the performance remains more stable compared to previous contrastive learning methods. Furthermore, this paper proposed a MoCo-based strong baseline that can achieve comparable results with an extremely small number of negative samples.*****************************Reasons for Score: Overall, I vote for acceptance. This paper proposed equivalent rules which successfully reduce the number of negative samples for contrastive representation learning while maintaining the performance. This motivation is very novel and interesting, and I believe the findings in this paper significantly contributes to the community of contrastive representation learning. My main concern is the assumption that the prior probabilities $P^+$ and $P^-$ satisfy $P^+/P^-=e^{-m/\tau}$ (see Sec. 2.1). ***************************** Strengths: \+ This paper focused on the problem of using a large number of negative samples in self-supervised contrastive learning, which is very important but not fully investigated before.\+ This paper proposed a simple but strong baseline that can achieve comparable results with an extremely small number of negative samples. To the best of my knowledge, this is the first time to succeed with a small number of negative samples in contrastive learning. It may also inspire the applications in other self-supervised learning problems where a large number of negative samples are hard to be maintained.\+ This paper provides detailed mathematical analysis and theorems, which makes the paper theoretically strong.\+ The proposed strategy achieves significant improvement compared to the most popular self-supervised contrastive learning methods, MoCo and SimCLR, with a small number of negative samples. ***************************** Concerns & Questions: \- The Eq. (2) includes $P^+$ and $P^-$ as prior probabilities. It is not clear to me how the priors are taken into account and whether it is theoretically correct. In addition, it is not clear the assumption $P^+/P^=e^{m/\tau}$ is correct. Could more mathematical details be introduced? Why is including $P^+$ and $P^-$ called a generalized form?\* In this paper, the temperature $\tau$ is fixed when tuning $K$ and selecting $m$. It would be an interesting issue if Eq. (5) is rewritten as $\tau=\frac{m}{\log\alpha-\log K}$. I wonder whether it is possible to select $\tau$ by fixing $m$ and tuning $K$. If not, is there any insight behind this?\* In the proposed SiMo, all the negative examples are obtained from the current batch via the momentum encoder rather than the dictionary. Table 3 in the appendix shows that momentum update is important. Note that the momentum encoder in MoCo is used for the consistent dictionary of negative samples. Since the dictionary is not used in SiMo, the momentum update should have a different role in contrastive learning, perhaps like Mean Teacher. It would be great if more analysis or explanation on this can be provided.\* SiMo uses Sync BN rather than Shuffling BN. I wonder whether Sync BN causes extra computational cost or slowers the training speed. This paper studies an interesting theoretical question: are there any natural tasks that provably separate fc-nets from convnets. The main contribution of this paper is an Omega(d^2) vs O(1) separation. To prove the hardness result, the authors use (and generalize) the notion of orthogonal-equivariance introduced by Ng (2004). The current submission improves the hardness results of Ng (2004) in the following aspects:1. Ng (2004) proved an Omega(d) vs O(1) separation, while this paper provides an Omega(d^2) vs O(1) separation. This is interesting not only from a theoretical perspective, but could also be relevant to practice. In practice, the dimensionality d is always moderately large. Moreover, the labeling function employed in the hard case is natural and could indeed capture practical scenarios. 2. The hardness result by Ng (2004) does not use a fixed hard distribution, while this paper shows that there exists a universal (and in fact, natural and simple) hard distribution that is hard for any orthogonal-equivariant algorithm. Personally I find such an improvement important: in order to demonstrate the intrinsic superiority of convnets over fc-nets, it is crucial to obtain distributions that are hard for all training algorithms. 3. The authors generalize the notion of orthogonal-equivariance and propose permutation-invariance, which allows them to prove hardness results for a wider class of algorithms. In particular, separation between fc-nets and convnets trained by Adam, which is a corollary of the hardness result in this paper, is not implied by previous results. Generalizing hardness results to a larger class of algorithms is definitely interesting for a broad class of audience in the deep learning community. The lower bound is proved by using Benedek-Itais approach and carefully bounding certain covering numbers. Overall, this paper presents a set of interesting results which rigorously explain why covnets could be more sample-efficient than fc-nets. This paper is generally well-written and provides lots of intuition on why the hardness results hold. On the other hand, there are a few typos that need to be fixed (see below). Given the great importance of the topic and results in this paper, I would recommend acceptance. Minor Comments:For random variable X and Y => random variablesfor function class F,G => function classes. also missing space before Gsemi-definite positive matrix => positive semi-definite matrixConvNets(CNN): missing space( which may depend on S) : extra spacemodels below , FC-NN: extra spaceBut as noted in the introduction: remove butnamely, the standard Gaussian, => the standard Gaussian distribution This paper presents an algorithm for verification of neural networks with ReLU activations. Essentially, it takes the tightened ReLU relaxation of (Anderson et al. 2020), builds its Lagrangian dual, and then applies a column generation scheme to accommodate the explosion of decision variables. The authors present computational results showing that, due to the amenability of the methods to GPU accelaration, they can produce stronger verification bounds than comparable methods working with weaker relaxations in a modest amount more time. Moreover, they show that the method can be successfully embedded in a branch-and-bound-like framework for exact verification.I like the paper and think it makes a good contribution to the literature. The paper builds heavily on (Anderson 2020), but the algorithm approach is very different and the authors clearly had to do some work (e.g. rederiving the dual to ensure efficient inner problem solves, making convolutions+masking work on the GPU) to make everything work out. The computational results also seem compelling, though I have some potential concerns about the comparison being made.My main concern is the use of "Gurobi 1 cut" as the baseline for comparison. Given that there is a one-to-one mapping between cuts in the primal formulation (Gurobi 1 cut) and variables in the dual formulation (the new approach), I am curious why the authors did not choose symmetric generation schemes for the two. Would the solve times be significantly lower if only one cut per layer is added (as in ActiveSet), instead of one per neuron? If so, what benefit do multiple iterations of cut generation offer? Is LP incrementalism or warm-starting used, or is the second LP solved from scratch? Even with these changes, I would imagine that the ActiveSet method still runs (much?) faster than the primal approach, but it's quite possible that the bound improvement would shrink.Minor comments:* p1: The phrasing "The main bottleneck of the above approach" is ambigious (which approach?). If the approach includes (ii), then the bottleneck will be the enumeration tree, not the convex subproblems.* p2: The "which is linearly-sized" reads like it applies to the optimal solution of (2), not the formulation (2) itself.* p18: I think there are some extra primes in the text of Appendix F.2. The main aim of this paper is to increase the efficiency (in terms of wall clock time) of Transformer based models for reinforcement learning.  Previous works have shown Transformer-like models to be highly performant across a range of domains, including recent results on reinforcement learning (with gated Transformer-XLs). However a drawback of these models  (over say, LSTMs) is their relatively slower inference speeds. This is especially a problem in RL settings where Actors are typically run on CPUs (not GPUs) and send trajectories to a central learner. The problem here is two-fold: 1) slower overall training time due to high latency with the learner waiting on actors 2) slower inference post training in latency sensitive deployment settings like robotics/other control based settings. The solution proposed here (the method is referred to as "Actor-learner distillation") is to instead use LSTMs for acting and Transformers for learning. Typically the learner would send parameters to the actors every update -- as this is not possible in this hybrid approach, a distillation loss is instead suggested as a means of updating the actors (with a replay buffer). The authors successfully show that this transfers both "good policies" and the relevant inductive biases from the Transformer as well allowing fast inference as expected. Further, the "off policy"-ness of the model does not turn out to be an issue in the domains considered. The results are promising and the the writing very clear. A few points below:* One question I have is whether the authors ran an experiment replacing the LSTMs in the actors simply with smaller transformer models? For example, 2 layers instead of 4 or smaller embedding sizes? This would also allow faster inference while inherently retaining some of the inductive biases. * As a second baseline: while this wouldn't solve the issue of slower training, one could achieve faster inference of the trained model simply by distilling the final Transformer into an LSTM (and then optionally fine-tuning). Are the authors able to report numbers for these?* Further, in between the two extremes of only LSTMs acting or only Transformers one could also consider more hybrid approaches that start with a Transformer to learn a very good starting policy in a few steps and then distill this into an LSTM online similar to work here (https://arxiv.org/pdf/1806.01780.pdf). The worked linked here performs "online distillation" combined with a convex combination of model outputs which would then negate the need for a replay buffer (though would require running both models on the actor until fully switching over to the LSTM). * While the results are conclusive, the work could be improved by running on larger (or 3-dimensional) environments as considered in the original Gated Transformer paper this builds on. * Lastly, are the authors able to provide possible future directions to be considered that could boost this work further? Overall this is well written paper exploring a conceptually simple but well-implemented and important idea. As models grow in size, techniques like this will be very important to allow scaling up of RL capabilities. I would recommend accepting this paper.  This paper studies the multi-source domain adaptation problem. The authors examine the existing MDA solutions, i.e. using a domain discriminator for each source-target pair, and argue that the existing ones are likely to distribute the domain-discriminative information across multiple discriminators. By theoretically analyzing from the information regularization point, the authors present a simple yet powerful architecture called multi-source information-regularized adaptation network, MIAN.I vote to accept the paper.-This paper has a clear motivation, is very well written, and establishes the final objective step by step with the theoretical supports. -The proposed objective is simple but very powerful. I enjoy reading the analysis of advantages over the existing solutions, which is well reflected in the experiments. The reported performance is competitive, even compared to the missing reference ECCV20 (see below).-The quantitative analyses validate the effectiveness of the model design. Particularly, the analysis on variance of stochastic gradients validates the technical benefits on optimization stability. A few comments/suggestions.-A few recent MDA works are missing, including but not limited to [ref-1, ref-2, ref-3]. Although not all of them using image datasets as testbed, but I would encourage the authors to include and discuss them under the same structure. [ref-1] Hang Wang et al., Learning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation, ECCV 2020[ref-2] Chuang Lin et al., Multi-source Domain Adaptation for Visual Sentiment Classification, https://arxiv.org/abs/2001.03886[ref-3] Haotian Wang et al., Tmda: Task-specific multi-source domain adaptation via clustering embedded adversarial training. ICDM 2019.-In section 3.3, the authors argue that their frameworks filtering out domain-specific information while preserving the amount of domain-shared information. The statement and the earlier discussion are intuitively correct to me. However, it would be great to see a quantitative or qualitive study on the effective of the preserved domain-shared information and domain-specific separately.-Minors. In page 5, It bias the representation towards & -> biases This paper uses singularity analysis developed in the context of analytic combinatorics to study the relationship between the reproducing kernel Hilbert spaces of the NTK in a deep fully connected ReLU network, the Laplace kernel, and exponential power kernels. The main results are when these kernels are restricted to the unit sphere. In particular, the authors show that, as vector spaces, the RKHS on the unit sphere of the NTK for a ReLU network of any fixed depth are the same and in fact coincides with that of the Laplace kernel. The authors also compare the RKHS for exponential power kernels, demonstrating that larger powers lead to smaller Hilbert spaces. Strong Points: 1. Understanding the NTK of a deep ReLU networks is a popular and important topic. 2. The NTK is defined via non-linear recursions, meaning that obtaining quantitative results for deep networks is both technically and conceptually challenging. 3. Proving that the RKHS of the Laplace and ReLU kernels coincides as vectors spaces is a substantial result.  4. The authors are the first, to my knowledge, to apply the rather elegant singularity analysis of Flajolet-Sedgewick to study the NTK. Perhaps the closes prior work used free probability to study the spectrum of the NTK. 5. The paper is clearly written.Weak Points:1. As the authors themselves allude to in the discussion, showing that two RKHS are the same as vector spaces ignores the Hilbert spaces structure and can still lead to different inductive biases in kernel regression.2. The present work applies only to ReLU networks, while the recursive definition of the NTK is valid for rather general non-linearities. Overall, this article is a solid theoretical contribution to our understanding of the NTK. In addition to proving concrete results comparing the Laplace, NTK, and exponential power kernels, it is serves as a proof-of-concept for potentially using the tools of singularity analysis to understand neural networks.  The paper is clearly written, technically sound and innovative.### Impact:The paper is bringing an important contribution in the domain of Physics-Informed Machine Learning. It proposes an interesting and technically sounds (and not obvious) way to combine multiple successful components of recent literature in the field.### Clarity and technical soundness:The paper is clear, technically sound and manipulates tools in an advanced way.It would be useful to provide more intuition and high-level interpretation on the most technical aspects of Deep Kernel inference and on the usage of ELBO methods. That would go at the benefit of understanding for the non-specialists of Kernel-based learning who are interested in Physics Informed ML.### Results:The results are extensive, are exploiting both simulation-type data and real-world data.The benchmark used for comparison are relevant.### Applicability:It would be interesting to have a more high-level interpretation and analysis of the applicability of the method to 3D simulation data. In 4.2, it was not clear to me what N and M stand for. Could you clarify it and provide a more high-level analysis with it too? This paper studies why language model pre-training has been such an effective technique in improving downstream performance across a wide range of NLP tasks recently.  In particular, it considers language models which compute a probability distribution over the next word in a text, given the previous context.  Then, taking inspiration from recent work that shows that many downstream tasks can be reframed as sentence completion tasks, it defines a natural task as one on which a sparse linear model over the output of the true language model (next word probability distribution, conditioned on context) attains strong performance.  Theoretically, it shows that language models which are close to the true language model are guaranteed to attain strong performance on natural tasks.  Empirically, it demonstrates that several NLP tasks are natural.**Strengths**- The paper is generally quite clearly written and the claims are well-validated.- The definitions, models, and assumptions in the paper are intuitive and clear (e.g., natural task).- The analysis which builds on these definitions/models/assumptions provides meaningful theoretical insight into why language model pre-training may be so beneficial for downstream training.  It provides a nice theoretical framework for thinking about the connection between language models and downstream tasks, which future work could build on.- The empirical validation is thoughtful and relatively thorough.  Even though the results dont show that the proposed loss function and proposed conditional mean features give improvements over baselines, the empirical results show that the basic assumptions and definitions in the theoretical analysis are relatively realistic.  For example, Figure 4 validates Assumption 4.1 (log-partition function is roughly quadratic in theta), and Table 1 shows many real tasks are approximately natural.  Furthermore, when there is a gap between the empirical results and the theoretical results (e.g., validation of Lemma 4.3 at the end of Section 4), the paper makes these limitations clear, which I appreciated very much as a reader (paper does not over-claim its contributions).**Weaknesses*** Unclear if there are real practical applications to the insights from this paper.  Neither the proposed Quad loss function, nor the theoretically inspired conditional mean features, perform better than the baselines.* The current analysis doesnt apply directly to BERT, which is trained to predict masked words in a sentence, instead of the next word.  Furthermore, BERT doesnt predict these masked words using a linear softmax model over a contextual embedding for the whole sentence, which is the assumed structure for the softmax language models considered in the analysis.  (This limitation is acknowledged in the conclusion, which is good).* The paper doesnt explain why learning a linear model directly on the context embeddings f(s) performs better than using the contextual mean embeddings.  * One idea I had here: Could you define a natural task as one for which there exists a sparse linear model over the *logits* of p*( . | s) which performs well, instead of a model directly over p*( . | s)?  Due to the very flat portions of the softmax function, there can be meaningful differences between the logits corresponding to 2 different words, but the LM probabilities for those words are extremely similar (and thus, harder for a linear model to distinguish).  With this definition, a linear model of the logits is also a linear model over the context embeddings f(s) directly.* There are some points in the paper that could be made clearer.  * I think it should be discussed earlier (in intro/related work) why the paper focuses on language models which do next word prediction via linear softmax models over fixed dimensional context embeddings, and that BERT is out of scope.  * I think there should be more discussion about the implications of Proposition 2.2.  As I understand it, this result shows that any part of p_f(s) orthogonal to row-span(Phi) doesnt affect the cross-entropy of the language model (first order optimality condition would still be satisfied).  However, this doesnt necessarily imply that p_f(s) will be in span(Phi) for all contexts s.  In particular,  the architecture of the embedding model f likely constrains f in such a way that makes it impossible for p_f(s) to be in span(Phi) for all contexts s.  Furthermore, at the end of section 3 it should be better explained why the assumption that p_f(s) is in span(Phi) for all s implies that Definition 3.2 should only consider sparse models v which are in this span as well (decompose v = v_in + v_out (component of v in the span, and orthogonal to the span), v^T p = (v_in + v_out)^T p = v_in^T p).  * I found the discussion in Section 4.1 pretty confusing.  In particular the part that argued why B = O(1/alpha).Overall, I really enjoyed reading this paper, and found it to be quite insightful.  It provided me with a much more thoughtful explanation for why language model pre-training improves downstream task performance, beyond simply it helps learn good general representations of language using large amounts of unlabeled text data (my previous reasoning).  As a result, I recommend acceptance for this paper.NIT: - Grammar last sentence of Section 1.1 (&analyze the efficiency *of* &)- Proposition 2.2: Maybe write \forall s \in S instead of \forall s ~ p_L.- Section 3: ...append a prompt like This movie is (the final quotation mark is on the next line).- Equation (5).  Use sup instead of max.- Discussion in Section 4.1- I think Figure 4 should be explained in more detail (in caption and/or text).- Using capital and lower case tau in Theorem 4.2 is confusing notation.- Similarly, using bold and not-bold B in Theorem 5.2 is confusing notation.- After definition 5.1, what does Omega[w] = Omega[w] mean?- In Table 1, can you explain more explicitly (in caption and text) what subset and class words means? Also, can you add a column where a dense linear model over p_f(s) is used? This paper studies the effect of training BN parameters on training deep neural networks. The conclusion is striking: learning only BN parameters is enough when increasing the depth of the network. Authors have done extensive experiments to understand the effect of increasing the depth and width of the network. To stress the important role of BN parameters, the same number of parameters are chosen randomly and trained. Yet, it is observed BN parameters can obtain far better accuracy. Furthermore, an interesting observation is conducted on the distribution of BN parameters: when training only these parameters, a sparsity pattern is observed on the optimal parameters. While learning all parameters does not reach such a sparse pattern for BN parameters. The sparsity pattern indicates that an efficient network only needs to have a particular ground-truth connection between different units and the choice of weights is not important. This shows that random features imposed by neurons can create a very interesting function class when they are connected in a proper way. I have a question to understand the results betters: When authors are talking about optimizing only BN parameters, do they mean that they did not optimize parameters of the last linear layer in the network (the one that is connected to outputs).I have one concern about the way that the key message delivered here: BN parameters are very important and needed to be trained. Yet, the effectiveness of these parameters is tied to the randomness of the weights. Let me explain more about this. Recent studies show that a BN network with random weights (according to the standard initialization scheme for neural networks) provides more distinguishing features compare to vanilla networks (see for example "Understanding batch normalization" and "Batch Normalization Provably Avoids Rank Collapse").  This may be the reason why training only BN parameter is sufficient as the interesting features that are created by random weights. This hypothesis can be checked with a simple experiment: initialize the weights with a non-zero mean distribution (or any other non-standard initialization) then see whether still the training only BN parameters is effective. My (academic) guess is that you can easily find an initialization scheme for weights which makes the training only BN parameters ineffective. This result will provide more intuition about the coupling of the weights and BN parameters.  **Summary**The paper considers the task of estimating the gradient $\frac{d}{d\theta}\mathbb{E}_{p_\theta(z)}\left[f(z)\right]$.This is a fundamental task relevant in all fields of machine learning, e.g. policy gradients, variational inference, or any other situation with stochastic computations.The authors come up with a constructive method for deriving an estimator for this gradient for any distribution $p(z)$ based on Fourier analysis. The derived gradients include some known estimators, e.g. the Gaussian gradient identities, but also include some new estimators based on infinite series of higher order gradients of $f(z)$ for distributions such as Gamma or Laplace.The analysis also works for Dirac delta distributions or discrete distributions.They perform toy experiments to evaluate the new gradient estimators, and show that they work and seem to give better accuracy than previous pathwise gradients (but they use higher order gradients). For the infinite series based estimators, they truncated the length of the series at a certain depth (they tested up to 4th order and up to 8th order).**Strengths**Gradient estimation is fundamental, and deriving new methods is a great contribution.The paper didn't discuss this well, but what is particular about the method derived is the form of the estimator based on separating out the computation of the expected gradients of $f(z)$, and the weighting applied to it, i.e. all of the gradient estimators derived with the method have the form $\sum_k a_k(\theta)\mathbb{E}_{p(z)}\left[\frac{d^k f(z)}{dz^k}\right]$. What is neat about this, is that one only needs estimators for the expected gradients $\frac{d^k f(z)}{dz^k}$, it is not necessary to know what the sampled $z$ values were. If you contrast this to some other gradient estimators, for example, the reparameterization gradient for the standard deviation parameter of a Gaussian distribution has the form $\epsilon \nabla_z f(z)$, where it is necessary to weight the separate values of $\nabla_z f(z)$ with different multipliers, but in the current paper, they derive a method where the multiplier is the same for all samples $z$. Moreover, from the derivation it is clear that there is only one way to perform this decomposition for each distribution, and the method to perform the decomposition for any $p(z)$ is provided in this paper. This is a general fundamental result.**Weaknesses**The discussion could be improved, e.g. the above points I mentioned were not explained.I believe Lemma 3 and Corollary 3.1 are incorrect, because they ignore the singularity at the "kink" of the relu. This issue does not affect the experiments though, because relus were not used in the experiments. (This Lemma was about making the expressions based on infinite series of higher order derivatives tractable in cases where the higher order derivatives are 0.) I explain this in more detail later.The discussion around the "discrete derivative" for categorical distributions was unconvincing to me. It appears that the derivation does not require selecting a special $z^{\*}$ choice, and the derivation goes through by just summing all $z$ without computing any difference at all.From a practical point of view, probably the method will not be immediately used.**Recommendation**I recommend accepting the paper, because the theoretical result is fundamental. They performed experiments showing that the new methods work on toy problems, which I think is sufficient. Any issues I found were minor, and could be improved with a bit of revision. Probably, I will further increase my score if they adequately revise the paper.**Questions**In the experiments, what is the performance for truncation depth 1, 2, 3?I am interested in at what point the performance starts degrading. Currentlythe experiments only show 4 and 8, and both give similar performance, so itis not clear whether, for example, 1 may also work or not.In equation 6, $\alpha$ is a multi index, not an integer. What does$(i\omega)^{\alpha}$ mean?**Additional comments, suggestions, clarifications**Kink in the relu: For example, consider $f(z) = max(0, z)$ is a relu. Then consider $\mathbb{E}_{p(z)}\left[\frac{d^2 f}{dz^2}\right]$. Due to the effectively infinite second derivative at 0, I believe this expectation should be considered as $p(0)(\frac{df(0+\epsilon)}{dz} - \frac{df(0-\epsilon)}{dz}) = p(0)$, and I believe the values at the singularities will matter for the gradient estimator. As long as I have not misunderstood something, I would suggest to just remove the discussion about relus, and say that if the higher order derivatives disappear, it becomes tractable.In general, I think the discussion would be greatly improved if you emphasize the structure of the gradient estimator based on separating out the weighting for the expected gradients $a$, and computing the expected gradient $\mathbb{E}_{p(z)}\left[\frac{d^n f}{dz^n}\right]$. And contrast this to the other existing gradient estimators, e.g. reparameterization or pathwise derivatives, which often apply a different weighting for derivatives at different $z$ values. It would be good to emphasize that what you have derived is not a general form for gradient estimators, it is a particular (fairly broad) family of gradient estimators among other techniques that are not described by your derivation. I also did not find the discussion about the Dirac delta or the discrete distributions particularly insightful, so I would suggest to spend less time emphasizing these points."Furthermore, we show that the classical deterministicbackproapagation rule is a special case of stochastic backpropagationwhere the distribution is a Dirac delta, bridging the domains of neuralnetworks and probabilistic graphical models."I would not emphasize this point, as it is probably obvious to manyresearchers. The Dirac delta can be acquired by letting $\sigma \to 0$for a Gaussian distribution, which directly gives the result.In the introduction, there are some spaces before the '?' signs, which shouldbe removed.I disagree with some of the discussion in the introduction. In particularthe questions posed are already answered to some extent:"How to develop stochastic backpropagation rules similar to those of(Rezende et al., 2014) for a broader range of distributions?"Implicit reparameterization gradients work for a broad range ofdistributions. Moreover, the reparameterization based on the cdf ofa distribution always works as long as the cdf can be inverted (this isjust a computational issue, rather than a theoretical one)."What is the link between the discrete random variable case and thecontinuous case? And finally, what is the relation between stochasticbackpropagation and classical deterministic backpropagation?"I wouldn't say the paper gives a definite answer to thesequestions. It just provides another interpretation based on Fouriertransforms/characteristic functions, which is good, but I don'tbelieve the new interpretation is better than previous ones; it isjust different. I would tone down the discussion and just say thatyou provide interpretations based on Fourier analysis.sommable -> summablePerhaps the multi-index notation could be clarified by 1-2 examples, and/oremphasized by putting in a definition block. I am not sure whether it willimprove it though.The weighting function for the derivatives is outside the expectation,i.e. the weights do not depend on the sampled z position, and the zonly comes into play for computing the expectation of some gradientsof f(z). I think this structure of the gradient estimator should beemphasized more.Equation 3 claims that the weights are unique, so there is only oneway to construct the estimators that they have constructed. I thinkthis should be emphasized more. Also, instead of $\exists!$ it may bebetter to just write "there exists a unique".$a_\alpha$ is a bit difficult to read. Consider using different notationfor either $a$ or $\alpha$." Putting everything together": you have an extra space " " in the beginningof the sentence.In the Theorem 1 statement , can you write out that $a$ are theTaylor weights of $\nabla_\theta\log \varphi_\theta(\omega)$?"Plugging this expression to equation equation 5" -> there's a duplicate"equation"The "(AUEB & Lazaro-Gredilla,2015)" citation author names should be fixed(the default on google scholar is not good).The discrete derivative explanation in equations 10 and 13 is notconvincing to me. Instead of pulling out the $\varphi(z\*)$ term in eq11, you could have just kept the summation across all $z$, and itwould have lead to the gradient estimator also summing all $f(z)$. Itis not clear why the difference between $f(z)$ and $f(z\*)$ is necessaryor why it should be interpreted as a discrete derivative as the $z$ vectoris not being perturbed.Laplace is reparameterizable. What is meant by the pathwise derivativein the experiments? The work from Jankowiak and Obermeyer defines afamily of gradient estimators not a single one.For the toy problems. Rather than showing only the learning curves,it would be good to also test that the expected gradients for thenew methods and old methods are the same at some particular parametervalues (up to estimation accuracy, but you should be able to get the estimates accurate by repeating the computation many times and averaging). Also, for the experiments with truncation, probably what is more important than the variance is the gradient accuracy. So, it would be good to show both the bias and variance at different truncation depths. (Ideally, I envision a graph showing bias and variance of the gradient at a particular parameter value plotted against different truncation depth values.)Rezende et al (2014) also mention the reparameterization of the variableunder stochastic backpropagation, and this gives a different gradientfor the covariance parameters of a Gaussian compared to the stochasticbackpropagation rule derived in the present paper. Hence, it does notgeneralize stochastic backpropagation as envisioned by Rezende. Instead,it is a separate method for deriving one particular type of stochasticbackpropagation rule for any distribution. I would suggest somethinglike Fourier stochastic backpropagation, characteristic stochasticbackpropagation, Fourier expectation gradients, etc.The log characteristic function for Laplace should be$i\omega\mu - \log(1 + b^2\omega^2)$, you're missing the $\mu$ inyour equation."Our approach, in contrast generalizes stochastic backpropagation aspresented by (Rezende et al.,2014), where the derivative is explicitlytransported to the random variable"No, it doesn't generalize it as presented by Rezende. In Rezende's work,reparameterization is a subset of stochastic backpropagation (it islisted under section 3 titled stochastic backpropagation). Andreparameterization contains gradient estimators not derived by yourmethod (e.g. the $\frac{d}{d\sigma}(\cdot)$ gradient for a Gaussian).Hence, it is wrong to say that your method generalizesstochastic backpropagation. The authors present a technique to integrate combinatorial optimization sub-problems into a gradient descent based application. The approach they describe relies only on differentiation of the value of the combinatorial program (instead of the solution vector), and can be done with relatively low overhead (compared to techniques that involve modifying combinatorial algorithms to differentiable elements, or the use of differentiable linear/quadratic programming layers)They motivate and show the advantages of their approach using two natural and useful examples. The experimental results show promise, and the paper is well written, and motivated. Summary:The paper is motivated by the need for a better trade-off between the reconstruction and disentanglement performance of an autoencoder. The proposed solution is to use KL as a latent regularizer in the framework of Wassestain autoencoders, which allows for a natural interpretation of total correlation.The paper reads well, all related work and relevant background concepts are nicely integrated throughout the text. The experiments are exhaustive and the results show competitive performance wrt disentanglement while improving reconstruction/modeling of the AEs.If a dataset is of dynamical nature, how difficult would it be to extend the current version of TCWAE to dynamical systems? Do the authors have any intuition/hint on what should change to make their method applicable to dynamical setups? Significantly changing the probabilistic model or modifying only the and encoder/decoder architecture could suffice? Minor:- Consider changing the naming of the baselines either in tables or figures to make them consistent  Chen et al (2018) -> TCVAE Kim & Mnih (2018) -> factorVAE. The problem which the authors attempt to solve is unsupervised meta-learning (UML), ie. learning in an unsupervised way such a model of a dataset, as to be able to perform meta-learning (here: few-shot classification) later. I see their contribution as two-fold:1. Proposing a framework for solving UML consisting of sampling subsets $D_i$ of a full dataset $D_u$, training a generative model based on both datapoints ($x_j$) themselves and the particular subset $D_i$ and using it in a semi-supervised fashion.2. Implementing a model in this framework based on a VAE. Here, the latent variable $z$ doesn't just compress information about a datapoint (as in a classical VAE), but is also able to encode (in an abstract way) the position of this datapoint in the subset $D_i$ (ie. "task-specific label"). To be able to capture this (arguably richer than in classical VAEs) distribution, authors use a GMM to model the variational distribution.Because MLE of GMM is intractable, authors have a two stage optimization process:a) Finding a task-specific (ie. encoding info about $D_i$ "classes") parameter $\phi^*$ via EM and b) Optimizing the ELBO given $\phi^*$ as usual. During meta-testing, the $\phi^*$ parameter is estimated in a similar way using the test-time samples $x_i$ (trying to embed the new task into the learned manifold) and then latent variable $z$ is sampled conditionally based on $x_i$ and the expected value of the constructed distribution $p_{\phi^*}(y|z)$ estimated via Monte Carlo.1. While I am neither a VAE expert nor enthusiast, I consider the proposed model principled: while the two-stage optimization mechanism is not ideal (as may make it harder to optimize compared to end-to-end differentiable models), learning a single distribution describing both elements we care about: images and their placement within a dataset seem to match the problem better than previous pseudo-labels-based methods.2. I particularly like introduction of the general framework (1.) (which is not emphasized in the paper). I believe that it should be possible (not necessarily straight-forwardly) to extend the proposed model to other generative models. To make it clear, I wouldn't expect this extension from the paper under review (what I'm proposing is basically yet another paper), but the opening of this direction of research is a big plus.3. Paper is easy to understand.4. The presented results, while competitive compared to the previous UML SOTA, are only presented on somewhat toyish problems (Omniglot, mini-imagenet). While it is understandable that it'll be hard to train a Meta-GMVAE on more complex datasets (as it's only harder than classic VAEs, which are already struggling with higher-dimensional tasks), presenting the results only on small datasets (even if this is the current SOTA and other methods do it) somewhat undermines the overall motivation to UML: to be able to use vast amounts of unstructured data while building ML models.5. I am not able to comment on the novelty of the work: I am barely aware of the contemporary VAE/UML literature. I will be willing to modify my score based on other reviewers' opinions in that regard.Question/proposal:In Sec. 3.2. authors write "assuming that the modalities in prior distribution represent class-concepts of any datasets". Why would this be the case? This seems intuitive; I feel like there could be a nice theoretical argument why it would be the case.I find the model principled and new. It solves an important problem in a natural way, improving over SOTA and opening the potential for follow-up research. I weakly question the use of VAEs, which feels like it is limiting the method (making hi-dim UML impossible), but am aware that is more of a complaint against a well-established research domain than the contribution of this paper itself.Typos:1. Abstract:... from unlabeled data which can capture ...... shares the spirit of unsupervised learning in that they both seek ...2. Sec. 1:... effectiveness of our framework, we run experiments on ...3. Sec. 2:... One of the main limitations of ...4. Sec. 3.2.:... inferring isotropic Gaussian distribution, to encode ... +++Pros.  -----The observation that model parameters evolve slowly is quite inspiring for more efficient neural network training.  -----The paper proposes MONGOOSE, which is equipped with a scheduler to adaptively perform LSH updates and learnable hash functions to improve query efficiency.  -----Experiments demonstrates the effectiveness of the proposed method, and ablation studies give the readers further insights.  +++Cons.  -----The paper is overall good, but with some minors, such as Figure 5 shows P@1 and P@5 for MONGOOSE and other baselines during the training process. in Section 4.1.1.  -----Besides, there are several mathematical symbols should be explained clearly when they first appeared, such as w in definition 2.1, C1, C2 in assumption 3.1, t_r in assumption 3.2.  +++Conclusion.  -----Based on the above analysis, I would prefer to make an ACCEPT recommendation.  -----By the way, Im curious about why you named your method MONGOOSE? Could you give some reasons?  +++Suggestions.  -----Better make the mathematical symbols more clearly for readers.   The paper explores the problem of extrapolation in graph classification tasks and by leveraging Lovaszs graph limit theory, provides graph representations and related theoretical guarantees on graph size extrapolation in the context of unattributed graphs. Specifically, it is shown that the graph representations characterized by induced homomorphism densities are size-invariant under certain conditions. The theoretical claims are validated by empirical evaluation of classifiers trained on the proposed graph representations. Overall, I find the contributions of the paper solid and of interest to many researchers. Pros: The paper is well written with an appropriate focus on motivating the problem at hand. The experiments seem convincing enough to establish the implications of theoretical guarantees. Cons: I don't see any major issues in the paper. Elaborating on a few points will help improve the readability:1. I recommend elaborating on the graphon function $W'$ defined in Theorem 1 and its relation to graph topology. 2. Are the conditions of Theorem 1 violated by any large class of graph models, such as MRFs? This paper proposes a new principled approach to growing deep network architectures based on continuous relaxation of discrete structure optimization combined with a sparse subnetwork sampling scheme. It starts from a simple seed architecture and dynamically grows/prunes both the layers and filters during training. Through extensive experiments, the authors show that this method produces more efficient networks while reducing the computational cost of training, still maintaining good validation accuracy, compared to other NAS or pruning/growing methods. Strength: (+) The proposed idea of formulating the problem as a continuous relaxation of discrete structure optimization is interesting. It seems to be a more principled approach than previous NAS or separate pruning/growing approaches. (+) Extensive experimental results are provided to verify the superiority over recent other methods and also to show the performance behavior of the proposed method. The overall experimental setup is systematic and comprehensive. The experiments were done on widely used deep networks on various tasks.I only have concerns about the clarity of the notation and the representation of the figures. Specific examples are as follows: In Eq. (3), it is said that f is the operation in Eq. (1). However, I couldnt find f in Eq. (1).It should be clarified how many temperature parameters ² are in the proposed model. Only one or as many as channels and layers? If it is only one, it does not seem reasonable that all the probabilities growing or pruning channels and layers are the same.  Equation (7) seems to imply ² to be a vector, but earlier notations (e.g. in Algo 1, Equation 6, etc.) seem to present it as a scalar.Overall, there are confusing symbols, whether its a scalar or a vector. I recommend the channel and layer indicators are denoted as vectors. It seems that each channel and each layer has its unique indicator, respectively. Also, notations should include channel and layer index if they are different depending on channels and layers.Additionally, all the experimental results shown in the main manuscript are on convolutional neural networks while the abstract mentions recurrent neural networks. The appendix has some, but very little has the main manuscript. If its an important part of this manuscript, the authors should include at least a brief summary of the results. Some figures (and the text inside) are too small while containing many details, probably because of the space limit. For example, Figure 3 has many lines that are hard to analyze and texts that are not readable. In Table 1, whats the meaning of the underlines? I guess the second best results, but for RestNet-20, the method with the second-best FLOPs is SoftNet, not Provable. And the explanation about the boldface and underlines should be included.  Summary: The paper discusses the problem of graph matching (GM), which is the combinatorial (NP-hard) problem of finding a similarity between graphs, and has various applications in machine learning. More specifically, the paper proposes methods to leverage the power of deep networks to come up with an end-to-end framework that jointly learns a latent graph topology and perform GM, which they term as deep latent graph matching (DLGM).Strengths: The proposed method seems justified. The authors both explore a novel direction for GM by actively learning latent topology.  They further propose both a deterministic optimization-based approach a generative way to learn effective graph topology for matching. Regarding the empirical results of the paper, the authors report that their methods achieve state-of-the-art performance on public benchmarks, in comparison against a few peer methods (in measures of both accuracy and F1-score).  Other than that, their claims appear to be correct, and so is the empirical methodology. Relation to prior work and differences are discussed.Weaknesses/Comments: The paper was very difficult to follow (maybe it is due to the fact that I am not highly familiar with part of the field.  Nevertheless, I think that the organization of the paper can be improved).  I think there is a missing word in the second last sentence on page 2: For notational brevity, we assume d1 and d2 keep the same across convolutionallayers. Same dimensions maybe? # Overall reviewThe authors use a 3D world to explore grounded language learning, in which an agent uses RL to combine novel word-learning with stably acquired meanings to successfully identify and manipulate objects.  They show that a novel, psychologically-inspired memory mechanism is more memory-efficient than Transformers (both of which outperform plain LSTMs) and that it exhibits surprisingly robust generalization to novel action-object pairs.  The results should be of interest to many working in grounded language / multimodal representation learning, and the experiments are thorough and well-motivated. Pros:* Interesting environment for combining fast-mapping with stable language learning in a grounded task.* Novel memory architecture, shown to improve memory-efficiency.* Psychologically-motivated and thorough experimentation, demonstrating surprising level of generalization.Cons:* A figure describing the three memory architectures in addition to / instead of the plain text could have helped compare / contrast them.* More analysis of how the agent uses its memory would be welcome.# Minor comments* 4.1: "Unless stated otherwise, all experiments in this section involve the DCEM+Recons agent".  I think this means with full memory size (1024) and not the smaller one (100), but it would help to clarify.* Figure 2: while training with 3 objects and testing with more does not work, training with 5 and testing with 8 works as well as training with 8 and testing with 8.  What do the authors make of this?  Is there some kind of "threshold of diversity" beyond which the agent can generalize to more objects?  Such a threshold idea also seems consistent with the results in Figure 3.* Fast category extension: these results show that if the agents are trained to pick up different exemplars of a category, they can do so in testing.  During training, there was one exemplar from each of three distinct categories.  I was curious if the authors experimented with relaxing that to environments where more than one exemplar of a category could be present, with the agent being rewarded for picking any of the correct exemplars.  * The environments the authors use seem like they would allow for testing of mutual exclusivity phenomena (see Ghandi and Lake 2020 [https://arxiv.org/abs/1906.10197] and references therein), by providing instructions with a novel word in a setting with one unseen object (or category).  I would be curious to see if their memory architecture does better with these phenomena than existing ones.# Typographic comments* p 3, "the current visual embedding l_t": l_t should be v_t* Appendices were referenced in the text, but not included in the uploaded PDF (even though I believe this was allowed at ICLR).  These appendices seem very helpful for complete model/experiment details. This paper considers the problem of fast mapping: in a 3d environment, learning to explore an environment to learn a (new) mapping between objects and names, and then during a new phase, learning to pick up objects by their name ('pick up a dax'). This problem is challenging because the names change every time, so a model cannot simply learn connections between a shape and a static name (that persist longer than one episode).Strengths:* The paper defines the task and situates it within the context of a 3d environment, which could be useful for future work to build off of.* The paper compares several sequence models for this task, and finds that a Transformer-XL and a a 'Dual-Coding Episodic Memory (DCEM)' model both do well. The DCEM model seems novel to this reviewer at least, and it seems to function like a hybrid LSTM and Transformer. It seems that the DCEM model performs better than the transformer-XL model when the number of timesteps is limited (so the transformer-Xl model needs to rely on its recurrence mechanism to propagate information).* The paper tries several different generalization experiments involving more objects, new objects, a different evironment, and reward ablations. These generalization experiments provide us some insight as to what models learn in this setup.Weaknesses: At least to this reviewer, there are no major weaknesses (except perhaps that the dataset is a bit toy, which isn't a concern to me). However...* It's not quite clear how much the memory bank is learning as opposed to the LSTM hidden state (which is also something mentioned in the supplemental). It would be interesting to learn a probe to measure when the model learns to assign an object to a name (if this is indeed something measurable by the memory bank alone).* The reference to the 'DeepMind Lab Suite' is worded in a way that comes somewhat close to breaking anonymity... ## SummaryThe authors propose a near minimax optimal linear estimator under distribution shift. They have estimators for when there is a covariate shift (i.e. the underlying data distribution) or model shift (i.e. the distribution of the label given the features of the data).## Strengths* The authors provide bounds for both data coming from linear and non-linear generative models* The authors analyze model shift in addition to covariate shift* Experiments showing the results on simulated data## Weaknesses* No experiments for the non-linear data generation model This paper considers an algorithm for learning and using an opponent model that is only conditioned on an agent's local information (history of actions, observations, and rewards). A variational autoencoder (VAE) is trained to predict opponent observations and actions, given the agent's local information. While the opponent's observations and actions are required to train the VAE decoder, they're not needed at play time for the encoder. The authors show that with static opponents, the output of the encoder is informative, and conditioning the agent policy on local information plus encoder output outperforms just using the local information.The paper was well written, and was to easy follow. Experiments seemed appropriate to demonstrate the author's claims. I had only a few specific comments.Section 4.2 "In Sections 1 and 2, it was noted that most agent modelling methods assume access to the opponent's observations and actions both during training and execution. To eliminate this assumption ..."Weakened seems like a better choice of words than eliminated: it does still assume access to the opponent's observations during training.Section 5What is the size of the environment? Initial random placement? I'm trying to get some idea of the magnitudes of the rewards, which are based on Euclidean distance.Are the speaker-listener and double speaker-listener experiments using the OpenAI multiagent particle environments? If so, cite this (The MADDPG paper that is already cited for opponent algorithms?) Summary:The authors present a method for tackling the problem of over-smoothing in graph convolutional networks. Specifically, this is achieved by explicitly modelling a latent graph which, ideally, would be a graph which connects an observation to all other observations of the same class and no observations of a different class. In practice, there is only an uncertain picture of this latent graph as in many applications the labels must be estimated for unlabelled observations. The authors present an EM variational algorithm for approximating both this latent graph and using it to improve the estimation of a GCN. The authors demonstrate that the proposed method performs favourably on a battery of test against an array of existing methods for solving the node classification problem. Strengths:The paper tackles an important question in the GCN literature, which is how to deal with situations in which the graph is unobserved or the observed graph structure is only a fraction of the true graph. The method proposed, modelling and optimising a latent graph, makes sense and is well justified.  The authors position the paper well in terms of its contributions in relation to previous work. This includes empirical comparisons to a wide array of existing competing methods. The paper is well written and clearly describes the proposed method. Weaknesses:The main weakness of the paper is in the empirical section. Specifically, I would like to seen an expanded Table 2 to include more comparisons with existing methods. From Table 1, I am convinced that VEM-GCN achieves similar performance to existing methods, even if the magnitude of the performance increase is not very large. However, I think the main contribution of the paper in the empirical section is Table 2 and should be the main focus. It clearly demonstrates that the proposed method reduces over-smoothing relative to a vanilla GCN as expected, especially as the depth of the model increases, but I wonder what a similar comparison to the other models in Table 1 would show. Could the authors provide the results of a similar analysis, at least for the models for which this is possible?Reasons for score:I vote for accepting the paper. It is a solid, clear, and novel contribution to the literature on GCNs that directly addresses an important consideration in these models that is often overlooked. Questions for the rebuttal period:Please refer to the questions in the weaknesses section.  This paper presents a local attention + relative positional encoding type of network suited for image classification and detection tasks. The first focus of the approach is to make attention scale to (2D) images. Vanilla (global) self-attention with an input of size $n$ (e.g. n=224x224 resized ImageNet) and a context size of $m$ (m=n if global) costs $nm$ memory. They decompose their approximation in two parts, the content attention and the positional embedding (which requires global attention). For the (dense) content part, in the same vein as multiple "linear attention" approximations (e.g. Linformer, Wang et al. 2020) they make this attention $nk$ with $k$ independent of $m$ and much smaller. For the (relative, this translation equivariant) positional embedding, the space cost is still $nm$, but doesn't depend on the image, so this factorization into content + position is beneficial for larger batch sizes.Another contribution of this paper is to study the convolutional variant, so called "lambda convolutions" (strictly local relative position embedding) by setting the weights of the convolution dynamically based on the relative positional embeddings, and which can effectively reuse optimized [T|G]PU convolution kernelsThey also break down the query in "multiquery lambdas" (followed by concatenation) to reduce the computational cost (as in grouped convolutions).Finally, they construct LambdaResNets by hybridation with vanilla ResNets where they replace any (see Table 12 for full results) of the convolution layers by lambdas for a parameters/throughput/accuracy trade-off.They perform experiments on ImageNet (classification) and COCO (detection, with Mask-RCNN), which show competitive results: beating ResNets and relevant variants on size and accuracy, but not speed. LambdaResNets also beat EfficientNets on speed-accuracy on ImageNet accross the board.Some limitations of the paper and/or method include:- (minor) The related work (which is only really included in Appendix) does not discuss Zhao et al. 2020 (which is in Table 3).- A lot of the good/important content is in the Appendix (e.g. Appendix E.1 / Table 8 showing that **the positional embedding is absolutely necessary for good performance while the content part is quite optional**; or experiments on the scope size from E.2 / Table 9).- Their model can be seen (and indeed that is how they propose to implement it) as a kind of ConvNet with dynamically computed filters. Still, there is hope to recover long(-er than context size) range attention with multiple layers (as in ConvNets) and this is not studied/discussed.- (minor) Speed is problematic in the current implementation (see Table 12 and 13 in Appendix), how much of it is due to [optimized kernels for ResNets vs. einsum implementaion] vs. necessary computational cost? There is a bit of a FLOPS comparison in Tables 6, 8, 9 and accompanying text, but practice (e.g. Table 4, 12, 13 throughput) vs. theoretical complexity is not discussed.- A small caveat is that the space cost vs. global (self-) attention is only really advantageous for large batch sizes.Overall, this is a good, readable, well studied (if one considers the appendix) paper on a promising new hybrid conv/attention layer that yields small and accurate models for computer vision core tasks (classification and detection). Typo:"=This section" in 3.2 The paper proposes an important and unexplored problem in GNNs, i.e., the inconsistent distribution between the training set with test set caused by agnostic label selection bias. I believe that studying this problem is very important for generalizing GNNs on unseen test nodes. The paper first conducts an investigated experiment to show the great impact of agnostic selection bias on test performance. Moreover, the theoretical analysis is provided to identify how the label selection bias leads to the estimation bias in GNN parameters. To remove the estimation bias in parameter estimation, the paper proposes a novel DGNN framework by jointly optimizing a differentiated decorrelation regularizer (DVD) and a weighted GNNs model. The DVD regularizer is designed based on the causal view of variable decorrelation terms. I personally like the idea of analyzing variable decorrelation by the casual view. Furthermore, the paper theoretically proves that how to combine variable decorrelation terms with GNNs would be a more flexible framework for most GNNs and how to extend the theory to the multi-classification scenario. Overall, the proposed method is theoretical sound, where some basic claims are all supported by the clear and sound theoretical analysis. The paper conducts extensive experiments on four benchmark datasets with two kinds of selection bias, well showing the effectiveness of the proposed model. Basically, the paper is well motivated and well-organized. Strong points:1.The agnostic label selection bias problem in GNNs proposed by this paper is very important but seldom studied. And the paper shows the effect of label selection bias on the generalization of GNN in both experimental and theoretical way. In practice, the selection bias widely exists, I think this work may attract more attention in this direction, which makes GNNs more robust and stable in unseen environments.2.The technique of the proposed method is sound. The differentiated variable decorrelation is well motivated. This is a general framework for enhancing most existing GNNs under label selection bias setting. The idea of analysis and design model is novel, for example, analyze the estimation bias with stable learning theory, differentiated variable decorrelation in causal view, prove how to combine DVD with GNNs is more flexible, and extend the method to the multi-classification setting. I think these ideas are instructive.3.The experiment part is comprehensive and convincing. The experiments are conducted on two kinds of selection bias data, i.e., label selection bias and small sample selection bias. These two kinds of selection bias usually happen in real-world scenarios. And the results clearly show that the proposed methods make larger improvements with heavier bias.Question for rebuttal:1.In section 3.3, the variable weight \alpha is computed from Var(W^(K1), axis = 1), and \alpha_i can only be a positive value, however, in linear regression, the coefficients could also be a negative value. Hence, how to keep the \alpha computed from Var(W^(K1), axis = 1) has the same meaning of linear regression coefficients?2.Although we can find the hyperparameters for each method from the experiment part and the corresponding paper of baselines, it is better to list all the hyperparameters used in the paper in the Appendix to improve reproducibility. Very interesting and timely project. Major concern:For a well-planned model development with large enough dataset, it is recommended to separate the case and controls from the very beginning and apply the pre-processing and imputation on the training dataset only. Feature selection should also be based on the training data alone, which is not the case in this pipeline from my understanding. The way, the pipeline is described, the processing, including imputation and feature selection are performed on the full dataset, which is then passed to the modeling phase. During the modeling phase, where model training and validation and testing will be performed.  This can be cause of over-fitting since the testing dataset was to some level "seen" before the model testing. This has to be stated in the limitation of the study design. Having a team science approach with clinician scientists as part of the team is integral part of the study, which seems that this paper is all about.  Minor:Some grammatical /stylistic error. ex: While issues such as data cleaning, algorithmic fairness, and privacy and heterogeneity have import, they are beyond the scope of our software. à revise the sentence have import.Finally, I am not a software engineer and will leave that level of evaluation to my colleagues. In the context of deep learning, back-propagation is stochastic in the sample level to attain bette efficiency than full-dataset gradient descent. The authors asked that, can we further randomize the gradient compute within each single minibatch / sample with the goal to achieve strong model accuracy. In modern deep learning, training memory consumption is high due to activation caching. Thus this randomized approach can help attain strong model accuracy under memory constraints.The authors proposed a general framework for randomized auto differentiation to achieve unbiased gradient estimators. This general framework allows randomization at different granularity such as layer level and individual neuron level. It also includes conventional minibatch gradient estimators as a special case at the sample/minibatch level for randomization. The memory saving here is achieved by trading off gradient variance for activation memory saving. Empirically, the authors show that for 1) convolution nets on MNIST and CIFAR and 2) RNN on sequential-MNIST, under the same memory budget, neuron-level randomized gradient estimator can achieve higher model accuracy than conventional SGD with smaller minibatch size. Strong point: This paper is well written with novel thoughts on the fundamental aspects of auto-diff when applied to deep learning (and also to PDE as demoed in section 5.). It can also provide new options for practitioners to train models with high accuracy under memory constraints. Thus I recommend to accept this paper.I have the following comments / questions on the technical aspects. I only raise these questions up for improvement on the paper; they are not concerns on the quality of the current version. Nonetheless,  I am happy to raise the score if the authors can demonstrate results on these aspects.1. The authors mentioned about leveraging model specific structures to control/reduce gradient variance for fine-grained randomization (such as at the neuron level). Specifically, they considered using the randomized activation sparsification only for activation memory-consuming layers. I was wondering if other model structures can also help here. E.g. can we sample at the full-channel level for convolutional layers or column / row level for linear layers. Would this has a significant impact of the attained model accuracy?2. The main focus on practical implications in this paper is about high accuracy training under memory constraints. However, I was wondering how the authors think about the implication on compute, especially related to the sparse training trend. E.g. can we also make the forward compute itself also sparse and still minimally influence model accuracy?NITs: 1. First letter capitalization for figure 3 at the beginning of section 4.2. Would it be possible to provide some preliminary in appendix on the solution to the section 5 PDE example.  3. In section 3.3, the discussion on two extremal cases needs a more precision in text. Assuming the sampling on each connection (i.e. segment on paths) are independent, I think both case will have variance exponential in terms of depth. Currently it reads like only the fully connected case have exponentially large variance. The authors introduce the novel idea of producing unbiased gradient estimates by Monte Carlo sampling the paths in the autodiff linearized computational graph.Based on sampling paths it is possible to save memory due to not having to store the complete linearized computational graph (or the intermediate variables necessary to reconstruct it). Memory is the main bottleneck for reverse mode autodiff for functions with lots of floating point operations (such as a numerical integrator that performs many time steps, or a very deep neural network).The authors' idea can therefore potentially enable the gradient based optimization of objective functions with many floating point operations without check pointing and recomputing to reverse through the linearized computational graph.The tradeoff made is the introduction of (additional) variance in the gradient estimates.The basic idea is simple and elegant:The linearized computational graph of a numerical algorithm is obtained bya) having the intermediate variables of the program as verticesb) drawing directed edges from the right-hand side variables of an assignment to its left-hand side variablec) labeling the edges by the (local) partial derivatives of assignments' left-hand side with respect to their right-hand side.The derivative of an "output" y with respect to an "input" x of the function is the sum over all paths from x to y through the linearized computational graph taking the product of all the edges in the path.The sum over all paths corresponds to the expectation of a uniform distribution over the paths times the number of paths.That expectation can be Monte Carlo sampled.The authors suggest a way of producing the path sampling based on taking a chained matrix view of the computation graph (see e.g https://arxiv.org/abs/2003.05755) and injecting low rank random matrices.Due to the fact that the expectation of the product of independent random variables is equal to the product of the expectations this injection is unbiased as well if the injected matrices have the identity matrix as expectation.To take advantage of the simple idea it is in practice necessary to consider the concrete shape of the computational graph at hand in order to decide where to best randomize and save memory without letting variance increase too much.The authors present a neural network case study where they show that for some architectures the suggested approach has a better memory to variance trade off than simply choosing a smaller mini-batch size.Furthermore, they present a 2D PDE solver case study where their approach can save a lot of memory and still optimize well.I recommend to accept the paper.Remarks:I would love to see a more in depth analysis of the variance for example for simple but insightful toy examples.For exampl simple sketching with random variates v with E[vv^T] = I can be used to obtain an unbiased gradient estimate via E[gvv^T] = g, i.e. by evaluating a single forward-mode AD pass (or just a finite difference perturbation).But of course the gradient estimate has such a high variance so as to not give any advantage over finite difference methods (since with each sample / evaluation we are only capturing one direction in the input space).We are not gaining the usual benefit of reverse mode autodiff of getting information about the change of the function in all directions.In order for paths to be efficiently Monte Carlo-friendly it is probably necessary that they are correlated with other paths.In practice this will perhaps have something to do with e.g. the regularity of the PDE solution (the gradient with respect to the solution is similar to that of its neighborhood).A simple example (ODE integrator):p1 = xp2 = xfor i in range(n):p1 = p1 + h * sin(p1)p2 = p2 + h * sin(p2)y = 0.5 * (p1 + p2)The two paths in the program compute exactly the same values so leaving one path out randomly does not make any difference at all (if we correctly re-weight the estimate).Mini-batches are often like that: Independent samples from the same class give correlated computations, hence the variance is related to the variance in the data.But if two paths involve completely independent and uncorrelated computations then the variance is such that we do not gain anything.We need at least two gradient steps to incorporate the information from both paths.Since we do not systematically cycle through them but sample randomly, we are actually going to be less efficient.In terms of arguing about memory savings for machine learning applications it would be interesting to see a case study with a large scale architecture that does not fit into memory.The random matrix injection section could be clarified by moving the sentence "the expectation of a product of independent random variables is the product of their expectation" further to the front and state clearly the idea that:E[A PP^T B QQ^T C] = A E[PP^T] B E[QQ^T] C = A I B I C = A B CIn the PDE example you could clarify the notation used to properly distinguish between the continuous and the discretized solution.Also the PDE constrained optimization problem is not inherently stochastic (as can be argued for the empirical risk minimization setting in machine learning).Therefore, it is possible to use non-SGD methods with linear or even super-linear convergence rates (quasi-Newton methods).SGD with unbiased gradients has a sublinear rate of convergence.But the ideas of the paper are of course still useful even when trying to find the optimum up to machine precision in finite time.We can first use the RAD SGD approach in the early optimization and then go to the deterministic setting later in the optimization.- Page 3: eqn equation 1 -> Equation 1- Page 6: figure 5 -> Figure 5- Throughout: Perhaps clarify the meaning of batch vs mini-batch (in other papers batch can refer to full-batch)- Figure 5 (a) has blue curves but blue is not in the legend of Figure 5 (c)- Page 8: backpropogate -> backpropagate  ######################################################################1.  Paper Summary This work introduces m-coherence, a new method for understanding gradient coherence when training deep neural networks.  The authors then present theoretical properties of m-coherence (importantly scale invariance) and demonstrate that m-coherence can be computed in a computationally efficient manner.  The authors then compute m-coherence across steps/epochs when training ResNet-18 and EfficientNet on ImageNet without label noise, with 50% label noise, and 100% label noise.  The experiments demonstrate that (1) m-coherence matches intuition after an initial phase  (i.e. m-coherence is large and decreases over time for real data) (2) m-coherences increases for all datasets to a peak in the first 100 epochs before decreasing.  ######################################################################2. Strengths2.1. m-coherence is a very natural metric for understanding gradient coherence in that it is scale invariant, easy to compute, and is easy to analyze mathematically.  2.2. The description of m-coherence and its properties is presented well in comparison to recent work.   Additionally, the proofs follow almost immediately from the definitions and highlight how m-coherence is a simple, intuitive framework for understanding gradient coherence.  2.3. As m-coherence can be computed in a computationally efficient manner (both in algorithm complexity and memory usage), the authors are able to analyze gradient coherence on modern convolutional networks trained on 50,000 examples from ImageNet.  Importantly, the authors present both step-wise m-coherence and epoch-wise m-coherence, which highlights the phenomenon of m-coherences increasing for the first few steps of SGD.  This empirical finding could be an important stepping stone for theoreticians in understanding optimization and generalization for over-parameterized models.  2.4. The authors also highlight that the initial increase in m-coherence is robust across a variety of architectures, label noise settings, and initializations/minibatch constructions.  This finding is rather surprising in that it occurs even on datasets with 100% label noise.  ######################################################################3. Limitations/Questions3.1. I would have liked to see an explicit comparison of m-coherence with the other recent methods on a subset of CIFAR10.  In particular, does a similar trend arise of gradient coherence first increasing before decreasing under these other methods?  I read through the discussion in the appendix about this result being consistent across these papers, but there appear to be caveats with what samples were considered in other works.  It would be interesting to compare gradient coherence under these methods using a fixed random subset of CIFAR10 across methods (please let me know if I missed anything though in case this is already done).  3.2. (Minor) Just for some clarity, it would be interesting to understand the makeup of the 50,000 samples considered in the experiment.  How far away is the sample set from having 50 examples per class?  ImageNet also has a large number of classes of similar objects (e.g. ~100+ dog classes).  Is there indeed a higher m-coherence for such subsets of ImageNet during training or does m-coherence actually increase on these subsets as well?######################################################################4. Score and RationaleOverall, I vote for accepting this paper.  I find the contribution of m-coherence to be novel, practical step towards understanding gradient coherence for modern machine learning settings.  In particular, I feel that these findings will be of interest to theoreticians interested in the intersection of optimization and generalization.  My only criticism of the work would be that the authors should try to provide a more concrete comparison with other works on a random subset of CIFAR10 to determine whether gradient coherence phenomena are robust across all these methods.     ######################################################################5. Minor Comments5.1. I think the "overfit" label in Figure 1 is a bit confusing in the second row of Figure 1- is this just the gap between and test error? If so, I feel that just re-labelling this as the "Gen. Gap" or even just removing it would be fine.   Summary:This paper considers communication games when agents use experience replay. The agents' communication protocol may change over time, leaving outdated symbols in the replay buffer which are then trained on. This paper proposes replacing the old communication actions with up-to-date actions as the transitions are sampled, and shows that this leads to greatly improved convergence speed and higher performance plateaus.--------------------Positives:- The problem of multiagent communication and how to learn it is important and relevant to the ICLR community. The solution presented here seems like a natural fit with the problem and popular agent architectures and is well presented.- The paper is well motivated and well written.  Overall it was an enjoyable and easy read!- The experiments in Figures 4, 5, and 6 seem like great choices to show the strengths of the approach.  They're simple, well described, and well targeted.--------------------Negatives:- I feel like there's a pretty obvious question about "What happens in richer domains?" that (unless I missed it) isn't addressed in the paper - I'll expand on that in my 'Questions to clarify recommendation' section below. While the technique seems to work very well in the experiments chosen for the paper, I wish the paper touched a bit more on upcoming challenges, possible foreseen problems, and next steps.--------------------Recommendation and Justification:Overall, I feel like this was a strong paper and should be accepted. My only real negative was that I am excited to know more about what comes next.--------------------Questions to clarify recommendation:The three environments presented in the paper, if I've understood them correctly, are pretty straightforward in that 1) the speaker only has communication actions (and no environment actions), and 2) seems to only have one consistent message to communicate during the entire episode (after perhaps waiting to receive a message from others, in Hierarchical Communication). But right from the abstract onwards, I was wondering about possible problems in richer domains, where it seems like this technique could be harmful. Specifically, what if by updating the old communication action to one chosen by the current policy, we present a communication action that no longer aligns with the old environment action, which we do not update?  I felt like this was a pretty natural question, but unless I missed it, the paper doesn't mention possible problems like this.I'll ground this in an example, similar to Cooperative Communication.  Imagine a two-player gridworld where the players cannot see each other, but are rewarded for arriving at the same map location.  Similar to Bach and Stravinsky / Battle of the Sexes, each player has a different preference over locations, but being at the same location is most important. Let's call the locations Left and Right.  To enable coordination, let one player be a Speaker that can take communication actions to signal the other player as to where they should meet in that episode. While that permits greedy Speaker policies (always announce their preferred location and then go there) and greedy Listener policies (always go to their preferred location, regardless of Speaker's announcement), it would also allow the speaker to arrange a correlated equilibrium: announce a randomly chosen location in each episode and then go there, to maximize joint reward beyond any greedy Nash equilibrium policy.Here's where I see a possible failure with the technique in this paper.  Assume that the replay buffer contains an episode where the speaker emitted symbol L (for left) and then took environment actions to move to the Left location.  Later in training, using the technique presented here, we might sample this experience, update the symbol to L', and still move Left.  As described in this paper, I would expect that should work, and converge faster than by using the out-of-date symbol L.  However, it seems possible that the newer Speaker policy might prefer to move Right on that episode instead.  Updating the symbol would change it from old L to new R', but since the technique does not (and cannot, without a world model) update the environment actions, it seems like the listener and speaker would then train on this misaligned tuple of communication action R' and environment actions to move Left.  I would expect this confusing example to be much worse than training on the original example with the outdated but still aligned communication actions.More generally: how can we make sure that the updated communication actions still align in intent with the agent's environment actions that we cannot change? It seems like conditioning the communication action on the agent's environment action for that timestep might help, but would only be a partial solution: if an agent must speak now but take their first significant environment action in the future, we would have the same problem.My questions regarding this point are:- Do you agree that this could be a problem in richer environments than those presented in the paper?- If so, do you foresee an easy solution, or will this be a challenge for future work?I think the paper is strong enough as-is, and does not need an experiment in this paper to investigate richer games like this.  However, if the authors agree that the technique could fail to help or could harm convergence in richer settings than those presented in the paper to support the technique, then I think a couple of sentences about future challenges and future work are warranted.-------------------Issues and Suggestions:- Nit: Pg2, Experience Replay. The first sentence describes the agent as receiving (s_t, a_t, ..., s_t+1) at each time step. Should this be (o_t, ..., o_t+1), since the agent receives observations and not environment states?- Typo: Pg2, MADDPG.  'uses deterministic polices' --> policies- Suggestion: Pg3, Methods section and Equation 4. Equation 4 describes the tuple as containing r^e_t+1, r^m_t+1, but the text in the paragraph above only mentions r_t'+1, and the text below only indirectly clarifies what r^e and r^m are when it describes the cheap talk setting where r^m=0.  This threw me for a while when I read the equation, and scanned back up the page to try to see where r^e and r^m were defined, and they aren't. I suggest changing the sentence in the previous paragraph from "receives rewards r_t'+1" to something like "receives rewards r_t'+1 (split into an environmental reward r^e and a messaging cost r^m)..." to clarify this before the symbols are used.- Typo: Pg4, Ordered Relabelling. "may themselves by conditioned" --> "may themselves be conditioned"- Clarify: Pg4, under equation 6. The sentence "...we sample an extra o^m_t-1 in order to determine (using the other agents' policies) the new \^{o}^m_t, which allows us to relabel...". I don't understand what this sentence is trying to say. Which player is this for? The symbol \^{o}^m_t doesn't appear in equations 5 or 6, so I don't understand what sampling an extra o^m_t-1 would do, since it's to compute a symbol that doesn't connect with the equations being discussed. Maybe I'm just missing something obvious, but I spent a couple of minutes trying to figure this out, before giving up and moving on.- Nit: Pg5, Implementation. Extremely minor, but the phrasing "we can therefore only relabel..." suggests a limitation of the approach (e.g., we are only able to do this...) whereas I think you're suggesting a performance win (we can do this using only...). I feel like flipping the words to "we can therefore relabel only a single..." better communicates that.- Typo: Pg9 and 10, References. In both of the references including Pieter Abbeel, his affiliation is prefixed (OpenAI Pieter Abbeel). No other authors' affiliations are listed, so this just seems like a .bib typo. The authors present a fun and effective idea to translate a peer's message in terms of one agent's own experience. The benefit of doing so makes sense intuitively and is verified to be effective empirically.Strengths:+ The motivation is clear, and the key idea is well-presented. Paper is positioned well in relevant works of communication-aided MARL research.+ Evaluation is thorough and indicative of the authors' claims.Major Concerns:- The reviewer has yet to discover a major issue with the paper with regard to its correctness, contribution, novelty, and effectiveness. In Instance-Level and Episode-Level Pretext Tasks for FSL the authors present a novel method to take advantage of auxiliary prediction tasks and consistency regularization tasks which have had large success in Self-Supervised Learning settings to improve upon FSL approaches. Furthermore, the authors incorporated a transformer-based predictor to improve upon to be used with multiple augmentations of an instance to improve upon naive averaging of multiple predictions.Empirically, the authors demonstrate the benefits of incorporating both instance and episode-level tasks by showing significant improvements over the ProtoNet approach upon which this work builds and also achieving state-of-the-art results on several tasks with different architectural backbones. Through ablations this work demonstrates the benefit of each proposed additional loss and shows robustness to choices in pretext transformation and architectural backbone. This work is appropriately justified and explained and with satisfactory experimentation.For section 3.3, it may help to explain that the integrated FSL task is presented as an alternative to prediction averaging. As written, the rationale for this approach is only apparent in subsequent sections. Objective of the paper:  The objective of the paper is to show that the side channel information (based on timing, etc.) for image analysis software can be used to reconstruct reasonably accurate to highly accurate approximations of user images.  While this was previously known, in this paper, the authors use machine learning techniques to more efficiently "learn" how to use this side-channel information, developing generative models that allow image reconstruction.  Strong Points:1)  The use of machine learning techniques as an "attack methodology" for making use of side channel information seems to be a potentially powerful methodology.2)  The results shown here seem quite strong.  3)  The application (image analysis) is well chosen;  the side channels under consideration seems quite plausible.  4)  The code is made available.  Weak Points:1)   There are obvious counters to this approach;  once one knows that this type of software is being targeted, one can introduce various randomization (for memory accesses, timing, etc.) to lessen the power of side channel attacks.  That is, this may only be a reasonable attack because nobody has to this point considered it important to avoid side channel information being released by image analysis software.2)   It's a little unclear (to this reviewer) how much "information" the learning algorithm is seeded with.  That is, for these image sets, how similar/distinct are the underlying images?  Your examples seem to capture a lot of the original image, but it's not clear to me how to interpret what the algorithm is starting with.  (Essentially, would these results scale to much larger sets of images?  How much does working with the limited set of celebrity faces already help yield the strong results?  This is tackled somewhat in Appendix F, so the authors do seem aware of this point.)3)  It seems like you're considering all the different side channels separately?  Is there any way to combine the multiple side channels (into a mega-side-channel?) and do better?  (I apologize if I'm missing something -- I'd expect to see a result set with each side information, as you've given, but then results with all combined.)4)  This reviewer does not have good insight into why this type of side information would be useful in determining how to reconstruct images in the manner done here.  Some description or intuition as to how accessed cache lines could intuitively be used to determine an approximation to the original picture would be welcome.  (Understood you have limited space.)  5)   The paper does seem like a first step and there remains significant room for improvement (improved side channels, improved scalability, other applications).  Overall Rating:  Despite the (relatively minor) weak points, the paper seems to be a strong proof of concept that machine learning can greatly increase the risk of side channel attacks, because they can essentially automate the task of determining how to use the side channel information to obtain the desired result.  This seems to be an important message for the community, in particular as I expect this framework is not specific to image analysis software, but can be applied more generally.  Questions for Authors:I would very much like to see some sort of experiment where noise is injected in the side channel information.  One can view this as a setting where one obtains limited side channel information for some reason, or as a setting where the program is modified in some way to lessen the amount/value of side information.   Similarly, as discussed above, is there some intuition as to why the side information yields such reasonable results?  (Again, and how this relates to the limited data sets?)  And whether or not you could combine multiple types of side information naturally using this approach?Other Feedback:  Overall thee paper seems well written, and you do provide a great deal of data.   # SummaryThe paper proposes an approach to program synthesis which is done in a bottom-up fashion.In order to guide the search more effectively the bottom-up search algorithm is accompanied with a model predicting whether particular sub-expressions of a program are promising directions.As the system for program synthesis needs to be real-time capable, the proposed approach heavily relies on property signatures as introduced in (Odena & Sutton, 2020) for featurizing program inputs.The here proposed system called BUSTLE is shown to perform favorably to a set of baselines including state-of-the-art approaches to program synthesis.# Resons for ScoreGenerally speaking, I really enjoyed the reading of the paper. The paper is well structured and written as well as easy to follow.Everything is explained in sufficient detail, i.e., thorough but concise.The contribution of the paper is significant and the approach is technically sound.# ProsAlthough the methods might not be feasible to be used in practice yet, it performs reasonably well with only a few examples for describing the desired behavior of the programs to be synthesized.However, the programs that are synthesized are still quite smallish. It would be interesting if the authors could also highlight some future directions in order to make a step towards more complex scenarios.The paper provides compelling experiments which are set up in a thoughtful and rational manner rather than throwing various methods on some benchmarks.The experiments are conducted in a very systematic way, facilitating insights into the performance of the proposed method as well as how it compares to the baselines.However, a taxonomy of problems (e.g. how to quantify problem severity etc.) would maybe help to better spot differences in performance.The method itself is quite simple. The authors almost seem to apologize or vindicate. I rather consider it an advantage.The proposed approach is well placed in the context of the existing literature.# ConsSome information about the experiment setup is missing: Are all benchmark tasks of one set (either on the proposed set or SyGuS) run in a row?Is the order kept consistent? What hardware is used for conducting the experiments?# Questions during rebuttal period- Out of curiosity: If the order is the same for all the benchmarks for all methods, it seems to be the case the method being fastest to find a solution for a benchmark task varies from task to task,and the here proposed method does not always seem to be the fastest for any instance. Given the demanding real-time setting, do you think an algorithm selection approach would work here, choosing a methodon a per task basis?# Typos- p.3: "during the search, [...]" => During the search- p.6: "fixed length representation" => fixed-length representation (this is occurring at least twice)- p.8: "This provides further evidence the our model [...]" => This provides further evidence that our model [...] (occurs also in the appendix)- p.8 last but one paragraph in Section 5: "[...], while like BUSTLE, uses values produced by intermediate programs [...]" => The comma after BUSTLE is kind of irritating. Please, consider removing it or adding another comma before "like". This paper proposes to couple a GAN, an inverse graphics network, and a differentiable renderer. The authors base their work on StyleGAN, and use the observation that a specific part of the latent code corresponds to camera view-point to rapidly annotate a large amount of synthetic images with approximate camera pose. They then use these images and rough annotations to train the inverse graphics network to provide 3D and texture data. The differentiable renderer is used to synthesize 2D images from 3D, which can be compared to the input for consistency. In a second step, the authors use the inferred 3D data to disentangle the latent space of StyleGAN to allow to use it as a controllable renderer.--- Strengths ---The presented results look great. The inverse graphics network, seems to get both shape and textures mostly right. This is especially remarkable, given the small amount of supervision that was used. The controllable StyleGAN provides very plausible results. Most importantly, results  on real images are shown, and indicate the the presented approach does make progress in brining inverse graphics networks to real images.While the individual parts of this work are taken from other prior works, the specific combination of different components is novel and quite creative. Using a GAN to train an inverse graphics network is to the best of my knowledge novel. So is using an inverse graphics network to turn a GAN into a controllable renderer. There are also various key-insights that make the approach work, such as the that StyleGAN is partially  disentangled with respect to view-point, or the use of multi-view consistency when training the inverse graphics network.--- Weaknesses ---The paper is overall well written, but leaves out some technical descriptions which make it not self-contained and hard to reproduce. Specifically, I'd like to ask the authors to elaborate more on the individual loss terms in Equation (1). Especially, the 3D-related losses $L_{lap}$ and $L_{mov}$ are unclear. While many results are shown in the supplement, I would have loved to see a video that shows more results. For example, rotating the obtained 3D models, or showing interpolations of various factors for the controllable renderer. Even if the results are not perfect, a video would help to judge the overall quality of the results.--- Summary ---This paper shows an interesting pipeline with good results. It presents some non-trivial observations and effectively leverages them into a complete system that looks quite impressive.Typos:Page 3, last sentence: "conten" The authors provided a framework for inverse graphics, i.e., infer 3D mesh, light, and texture from a 2D image. They first use StyleGAN to generate realistic multi-view images, and then trained their model with a differentiable graphics renderer.Pros- Training on data generated by StyleGAN is novel. It is easy to control the view of rendered images, but they usually do not look real. Real images look real, but we usually do not know the viewpoint (most annotated datasets are either small or have limited annotation quality). The authors proposed a method to generate images of the same category of objects with the same viewpoint (Figure 2), which addresses this issue -- the viewpoints are known, and the images look real.- The experimental part is impressive. 3D reconstructions look realistic, and the authors demonstrated the effectiveness to train on StyleGAN by comparing their model to a neural network trained on PASCAL3D+.- The way the authors generate data by StyleGAN is also novel. They empirically realized some latent code in StyleGAN controls the camera viewpoint, so that they only need to choose some latent codes that well spanned over the space. It is a wise and elegant way to control the viewpoints of an object in StyleGAN.In summary, compared to previous methods, generating multi-view images from StyleGAN solves the unrealistic and unknown viewpoint issue, and the results look impressive. Hello authors,Thank you for your submission.  I very much enjoyed reading it.  I found the writing to be clear and only found one grammatical error (detailed below).  As with any black-box system like a learned optimizer, there is naturally a lot of interest in what, actually, the optimizer itself is learning, and why it has learned in the way it has.  In this effort, the authors perform interesting experiments with intriguing results for the first of those two questions.I found the experiments to be comprehensive and the figures to be adequately described.  It would be great if the authors could provide more details in the main paper on the precise process used to approximate the nonlinear dynamical system.  It could be easy for a reader to assume, without details, that a learned optimizer is "definitely" learning momentum, while missing the nuance of how those results were obtained.  (For the record I am not saying I disagree with the results, only that if more detail can be provided, it would be helpful.)The experiments are only performed with one form of learned optimizer, using an RNN with 256 GRUs.  What happens if different learned optimizer architectures are used?  Is there reason to believe that they would exhibit similar behavior?  Have the authors perhaps done any experiments with other architectures to see if the results are replicated?  Adding some extra information about this question to Section 4 could improve the paper.Overall, I think this is a good paper and should definitely be accepted.Small bits: - Can we really drop the subscript notation in (1)?  Couldn't F use other elements of g?  Or are we restricting the situation for this exploration? - You might consider switching Figure 2 and Figure 1; the background on learned optimizers meshes well with Figure 2 and so it may be more streamlined to introduce it there.  However, your call. - Page 4: "The RNN is is trained" -> "The RNN is trained" - It would be really helpful if Figure 3 was on the same page as Section 4.1.  This also applies to Figures 4 and 5. This paper proposes a robust novelty detection method ("MAW") to model the distribution of the training data in the presence of high fraction (corruption ratios up to 30\%) of outliers. The method add new features to the variational autoencoder (VAE), to detect and isolate the outlier so that the learned distribution only represent the inlier distribution:1. Uses a carefully designed dimension reduction component to extracts latent lower-dimensional features of the latent distribution.2. Model the distribution of latent representation as a mixture of Gaussian low-rank inliers and full-rank outliers, both using full covariances instead of diagonal covariances as commonly used in previous VAE-based methods for novelty detection.3. Penalizes the Wasserstein-1 distance between the data distribution and the latent distribution from the prior distribution. Under a special setting, it theoretically proves that using the Wasserstein-1 metric for regularization yields outliers-robust estimation and is suitable to the low-rank modeling of inliers, while the commonly used Kullback-Leibler (KL) divergence does not.4. Using the least absolute deviation error for reconstruction.Experiments on popular anomaly detection datasets demonstrate state-of-the-art results of MAW on standard benchmarks for novelty detection. Summary: This paper addresses when to use a single network model vs an ensemble of convolutional neural network models based on resource budgets. The authors challenge the notion that ensemble methods should only be used when resources are a non-issue. The authors compare single networks to width-equivalent and depth-equivalent ensemble methods for SVHN, cifar10, cifar100 and tiny imagenet across multiple network architectures and describe the 'Ensemble Switchover Threshold (EST)', the amount of resources beyond which ensembles provide better generalization accuracy than single models. Strengths:The authors robustly test this threshold where an ensemble method outperforms single network models. The authors consider more than just accuracy but also at inference cost and memory usage which are important parameters for deployable code. The provided demo is an impressive and straight-forward visualization tool for understanding when to use which type of model. The authors explore how performance changes across number of models in the appendix -- a question I thought of while reading the paper and did not expect to get answered. Weaknesses:There are some inevitable limitations to this type of study that make the EST hard to interpret. The authors do not include heterogenous ensemble methods or other hyper-parameterization that might vary between the three setups: single model, width and depth-equivalent. The title and majority of the abstract brag a great scope than the paper considers. It should be made clear in the title that only CNNs are considered for this study. The work done within this scope is thorough and impressive, and stands alone in its value. Questions:The authors assumption that the number of parameters is directly proportional to the resources used seems reasonable but I did wonder if there was a citation or related work to back this point up?  I am a picky reviewer in most cases. This submission is one of the highest-quality manuscripts I have reviewed. The problem setup is clear, the survey is comprehensive, and the contrastive to the prior arts is clear. I am pretty enjoyed reading this paper. The task of finding meaningful steerability within the latent space is relatively a new research area. The authors have made a fairly complete review and investigation on the related works, then propose an unsupervised (meanwhile, still providing slight user controllability with a small set of transformations) and training-free algorithm with high-diversity and high-quality (in terms of the robustness of the trajectory found by the algorithm). Besides, the authors propose the concept of the first-order and second-order dataset biases, and tackles the problem with non-linear trajectories discovery. Also, highlight that the proposed method is significantly faster than the prior arts as shown in Table 1. Experiment-wise, the authors provide a huge set of qualitative results in the appendix. Though the results are already convincing enough, I would still recommend the authors to provide some quantitative numbers for the comparisons on the diversity of latent directions against other methods (Figure 34-38). The visual differences are obvious, but a quantitative number can further showcase the universality of higher diversity. Some simple measurements like a mean over the variance of LPIPS among all dimensions [1] may be fair enough.Despite the submission is pretty strong and I pretty much has no problem with it, I ended up rating it with an 8, as the impact is relatively narrow in a specific area/task. But I am willing to see the feedback from other reviewers and give it a certain level of adjustment.[1] To avoid ambiguity, I mean ```all_vars = []for trajectory in all_trajectories:img_pairs = # interpolate the trajectorycur_var = np.var([lpips(*img_pair) for img_pairs in latent_pairs])all_vars.append(cur_var)mean_var = np.mean(all_vars)```This is just a demonstration, any implementation with a similar concept is fine.**[Minor comments]**1. While mentioning other related methods in the paper (e.g., Table 1), the references are made with plain text (without hyperlinks) and mostly using the authors' names in replacement of the method names. It is quite hard to keep tracking which reference corresponds to which method. It would be great if the authors can investigate this problem a bit.2. Typo: Frobenious => Frobenius**[Discussion]**I do not count this section as a part of the review, so it wouldn't affect my scoring if the authors do not reply. What is the main application of the dataset summarization technique (let's exclude image editing and content creation, which can be achieved with other techniques)? It is interesting to visualize these properties of the dataset, however, the real-world application of such a technique is not obvious and carefully investigated. A quick answer might be finding the dataset bias or domain shifting, and we may explicitly fix it when these problems are revealed by these dataset summarization techniques. However, the works on dataset summarization have never explicitly shown if these applications are viable with some experimental setup. I am interested in what is the authors' perspective on the applications, and whether showing these applications is an important part of the dataset summarization research direction, as the applications often influence the impact and research direction of proposed new settings and new components. This paper studies transformations in GAN latent space that map to meaningful transformations in the generated data. The main contribution is to derive closed form methods for discovering latent transformations that correspond to 1) geometric changes and 2) changes that capture principle components of model variation. The paper also contributes new methods for nonlinear latent transformations, disentangled transformations, and an application to attribute transfer.I really like this paper. The capabilities demonstrated here arent dramatically new  other methods can achieve similar effects  but this paper achieves these effects in a new way, which has its own advantages.The positives I see are:+ Simple and elegant alternative to prior work on finding latent transformations+ Nice qualitative and quantitative results+ Practical benefits including speed up, analytical transformation end points, and better disentanglementMy main criticisms are:- toward the end of the paper its a bit of a hodgepodge of ideas- the ideas and methods in Section 3 are mostly disjoint from those in Section 2- the attribute transfer application especially feels tangential, and receives minimal analysis or evaluation- the experiment in Figure 7 is not fully convincingI think everything in this paper is interesting, but the different sections dont fully cohere together. Section 2 is great and seems to tell a complete story, about closed form solutions to finding geometric transformations. Section 3 then diverges into a few different directions, which dont directly build on each other. When the circular trajectories are introduced, Im left wondering: why not use these in Section 2? For example, would it be possible to derive closed form solutions for great circles that map to target transformations $\mathbf{P}$?I do like the story about first and second order biases, which somewhat connects Sections 2 and 3. However I would like to see more connections made at the methodological level, or a clear justification for why methods from Section 2 where not used in Section 3 and vice versa.Aside from the coherence of the story, there are a few smaller things that could be improved:In Figure 4, it would be nice to show the data distribution, or the distribution from the non-transformed GAN samples. Otherwise its hard to tell if the transformations had an effect.In Section 3.1 I think an equation would help clarify things, e.g., write out the SVD and refer to the right singular vectors by an algebraic symbol. Perhaps also contrast this with the equation for PCA from Harkonen et al.The experiment in Figure 7 is not convincing to me. From the plots its hard to tell if the proposed methods are actually incurring less shift. Maybe plotting the delta from the original distribution would make the plot clearer? Most of all, I think some numerical metric should be defined to quantify the second-order biases, rather than just requiring visual inspection of the plots. It is convincing that the FID improves for the proposed methods, but not that the level of disentanglement between zoom and shift improves.Minor comments:1. Page 1: the precise same effect  I think this is an overstatement. There are substantial differences between how a single direction affects different classes. For example see Fig 4 of Jahanian et al. 2020. I agree that it is interesting that the directions have _similar_ effects across classes.2. In Eqn 3, what if the matrix is singular? Do you use the pseudoinverse? The authors studied the behavior of adversarial examples from the channel view of activations, which is very novel. They focused on the magnitude and frequency of activations and found that state-of-the-art adversarial defense (adversarial training) only addressed the magnitude issue but the frequency distribution issue remains. This provided a novel perspective for us to understand why state-of-the-art adversarial training method works to a certain extent but not so good. Then, the authors proposed a Channel-wise Activation Suppressing (CAS) to address the frequency distribution to further improve the adversarial robustness. CAS is generic, effective, and can be easily incorporated into many existing defense methods. Pros:1. The authors studied adversarial examples from a new perspective of channels in activations. Previous works focusing on activations usually assumed that each channel is of equal importance, while the authors focused on the relationship between channels. From two aspects of activation magnitude and frequency, the authors found two novel characteristics of adversarial examples: adversarial examples have higher activation magnitude and more uniformly activated channels compared to natural examples. The findings were convincingly evaluated on different neural network architectures and different training methods. This hints at a very interesting phenomenon.2. The proposed method is generic. The authors found that the activated channels are still uniform under adversarial training, that is, some redundant and low contributing channels are still activated. To suppress the redundantly activated channels, the authors proposed Channel-wise Activation Suppressing (CAS) training strategy. It dynamically learns and incorporates the channel importance (to the class prediction) into the training process. The motivation is very clear and the method is easy to follow. More importantly, CAS can be widely applied to strengthen existing adversarial training approaches since it suppresses those less important channels.3. Lots of experiments are provided to understand and evaluate the proposed methods. The experiments covered lots of aspects, including channel suppressing effect of CAS, representation learning, ablation studies, and extensive robustness evaluation on white-box and black-box attacks. The authors also tested the adaptive attacks, strongest auto-attack, and the optimization-based black-box attack, which definitely convinced me of the effectiveness of the proposed method. Overall, the paper hints at an interesting phenomenon and inspires an in-depth understanding of adversarial training. The proposed method is elegant and generic. The empirical evidence is solid and extensive. Cons:1. How does the activation threshold effect Figure 2?2. In the testing phase, the predicted class of the auxiliary classifier is used for the channel importance. Is it vulnerable to attacks? if the predicted label is incorrect, how it will affect the final performance?3. How well the auxiliary classifier works. With the limited information from the output of GAP, it is likely that the classifier performs poorly, and thus results in bad channel importance weighting.4. CAS could both improve natural acc and adversarial robustness, why CAS could achieve this both? and how is the overhead of CAS? In this paper, the authors recognized the function of unique relevance (UR) of features for optimal feature selection and augmented the existing mutual information based feature selection (MIBFS) methods by boosting unique relevance (BUR). As a result, they proposed a new criterion called MRwMR-BUR. Experimental results are provided to show that MIBFS with UR consistently outperform their unboosted conterparts in terms of peak accuracy and number of features required. Overall,  this paper has thrown new light upon MI based feature selection and the results are valuable. However, I think the paper could be improved from the following two aspects.1. To their credit, the authors have introduced the background of MI, OR, UR, and II.  However, some of the points are not made clear. For example, at the end of Sec. 3.1, they stated that "We note that the optimal feature subset S* may also contain features with OR and no UR at certain situations. For example.....",   which seems contradictory to the Proposition 1.2. The authors are suggested to  improve the organization and the presentation of the paper. The current version is not easy to follow. For example, there appears the term J_{UR}(X_i) in Eq. (5), but I do not see any explicit definition of it until I reading Appendix A.2. This paper introduces a new offline imitation learning algorithm, to be compared with BC. The main idea (and difference with simple BC) is to re-use the old learned policy as a reward function in a policy-gradient step. As an effect, the action distribution is bootstraped in a way that enforces the action near the mode of the observed demonstration.Consequently, if the demonstrations are noisy but the mode of the optimal behaviour is conserved, then it will learn a policy closer to the optimal behaviour than the demonstration itself.I think this idea is original, simple and smart. The empirical results show that it works very well on classic locomotion tasks with artificial noises (uniform/gaussian). I only regret two things:1) The first part (theoretical) of the paper, and especially section 4, was unclear to read and somehow misleading :- In section 3 assumption 3, if the noisy expert has never followed the noise, then the reward should not depend on the noise. Maybe there is an inversion of the formulation here. And I dont get at all the link between assumption 3 and its interpretation in the paragraph above.- In section 4, I dont understand the utility of equation 2) regarding the rest of the paper. Also, as it is a lower bound of the performance of BC, I dont get how it can show any limitation of this approach (I would have expected an upper bound in that perspective).- I am still not sure to understand what is the state partition (despite the trial to explain with a figure). It looks like the expert demonstration must necessary start without noise (generating states belonging to S_*) then the very first time the action is sampled from the noise the state belongs to S_e+*, after which all the states belong to S_e. 2) It would have been interesting to observe experiments with more chaotic noises (for example generated by biased learning algorithms or real-world human demonstrations). But this last point only reflects my frustration, the paper is already showing sufficient results to be impactful.