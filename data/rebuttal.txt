 Post revision update-------------------------------The authors have been very helpful and addressed many of my concerns, and I think the revised paper is a substantial improvement. I have rated the paper as a 7, although I do have some lingering concerns.Most crucially, it's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior, since, for example, both models significantly outperform humans at the tree rule. However, the authors do point out that humans perform better at these tasks than the null distribution. Still, I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ. However, I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard, at least future researchers will be able to see the full pattern of results to draw their own conclusions. ######## Post-Rebuttal Updates:After reading the authors' response and other reviewers' opinions (especially R3), I would like to downgrade my rating slightly from 7 to 6. I still think the paper makes valuable contributions, but I also think the contributions are overstated and not precisely justified. Particularly:1. I agree with R3 that the limitation of novelty should be considered from the two perspectives of "task" and "method". The task is certainly not new, which should be made clear in the paper. The newest version of the paper still claims "we define a new taskcross-supervised object detection"2. The method is indeed somewhat new, due to the use of multi-task learning and SCM, but its novelty should be clarified, and compared to all similar methods, not only some of them which are weaker. For instance, the authors did not adequately justify whether/why their method is superior to YOLO 9000, or Yang et al ([1] above). They did mention some differences in response to R3's comments and mine, but I am not convinced. Moreover, the authors did not explicitly discuss and compare those distinctions in the paper, neither quantitatively nor intuitively.3. In response to R3, The authors claim they "are the first to address this problem in situations of realistic complexity", which is not accurate. Particularly, the paper reads "While several works [...] have explored this problem before, [...] They struggle to learn under more complex and realistic scenarios, where there are multiple objects from potentially very different classes" This is not entirely true, as Yang et al. successfully evaluate on Open Images (and also VOC and COCO in their supplementary materials). The authors do not provide a convincing reason or evidence of existing methods "struggling" in realistic settings. YOLO 9000 is open-source, and could have been compared to the proposed method to confirm that claim.Accordingly, I strongly encourage the authors to refine their claims and lay more emphasis on the actually novel aspects of their method, by thoroughly comparing those novelties with all similar methods. I still hope to see this paper accepted, but cannot endorse it due to insisting on inaccurate claims. ---------- Post rebuttal ----------After discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.However, I would like to keep my original reject score. The reasons are as follows.Extending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks. The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO. With the development of computer vision techniques, it is natural to try more challenging datasets. So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.In addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO. The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't. This difference is also minor.Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper). I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019 ========Post-rebuttal========I thank the authors for answering my questions. While I am satisfied with the authors response to my concern about the "initial good architectures" assumption, I still remain unconvinced that adversarial learning can help search find new good architectures. I keep my score.I also encourage the authors to carefully re-read the f-GAN paper, which explains exactly how any f-divergence (including the KL) can be implemented for adversarial learning. Switching to any f-divergence requires only a simple change to the loss function. It also appears that the authors significantly misunderstand VAEs. The difference between GANs and VAEs is not JS-divergence vs KL-divergence. Using a KL loss for adversarial learning does not require switching to a VAE. Given the central role they play in this paper's motivation, a better understanding of these subjects is important. # Post-response updateThank you to the authors for answering some of my questions and clarifying the search and evaluation of GA-NAS. I believe my original assessment that the contributed method was complex remains accurate; while the authors note that other methods like ENAS also use an RNN controller, in my view those methods are also complex. This paper increases this complexity with a GNN and an adversarial training setup. Use of such additions require showing significant improvements over baselines like random search, which I do not believe is achieved. I thus stand by my initial rating. Thanks for your informative response addressing my comments. After the revision, the description of the method is clearer (Sec 3.2), and the experimental results are clearer (Sec 4). I'll stay with my original accept-score.=============== AFTER REBUTTAL I read the rebuttal of the authors and the other reviews. I will increase the grade to a 5.The reasons are that I think that the authors improved significantly the paper with this revision. However, I believe that the paper would benefit from experiments on a larger number of datasets, in order to better understand on which type of datasets their method shows better performance.  ##### Post-rebuttal updateDear authors, Thanks for the response. I strongly encourage you to revise the paper using languages other than flow-based generative models.From the rebuttal I could not see whether you understand my point or not: The conditional mean embedding operator defines an **integration** instead of an invertible **transformation**, which differentiate itself from any flow-based model (at least those you referenced).As for now, I cannot recommend for acceptance.-------------- **Updates**:After carefully reading comments of the other reviewers as well as the authors' response, I change my score from 6 to 5. Updates after the rebuttalI find the revised manuscript to be clear and more transparent. After reading the reviews, the response, and the extended appendix, I am increasing my score and vote to accept this paper. ------Updates after author response------I thank the authors for the response and the new experiments. In light of the clarifications and additional evaluations, I have increased my score to 7. (Update): The score has been updated after a rebuttal from the authors.  =========After reading authors comments.- Well, still the Point 1 is generally not true in my opinion. The authors cited the work of Kidger & Lyons "Universal approximation with deep narrow networks", in which universal approximation is proved for a wide class of activation functions and long narrow networks. Moreover, if you do not strive to make the network as narrow as possible, then it is easy to convert a shallow wide network into a deep narrow one (but not vice versa). Point 2 seems quite possible. Point 3 coincides with what I mentioned --- the authors proved UAP for several examples of activations, although really quite important ones.- "Results will be accessible without the need for the technicalities". Well, I personally do not understand this statement in the conference article. From the main text, the reader only sees that a relatively basic property (universal approximation) is fulfilled for a couple of previously uncovered activation functions (or for a specialized class), and it is not clear why, since possibly new ideas are not in the main text. - This work clearly has strengths and weaknesses, which, in principle, are more or less clear. On the one hand, the universality property for approximations by dynamical systems and a specific class of activation functions is proved. How important, new and interesting is it? It doesn't seem to be super strong result, because this property is relatively weak, completely expected, and has already been proven many times before in various situations (wide shallow networks, narrow deep networks, dynamic systems with some classes of activation functions). In approximation theory, people have been studying more advanced questions like approximation rates for a long time. I also do not see any significant practical use of the results. On the other hand, it is possible that the authors have developed some new method when proving their results. But in order to check this fact the reviewer should check the whole appendix. If an article is submitted to a journal, the reviewer is expected to read full article, including the Appendix. But for this conference work, as far as I understand, this is not the case, the reviewer is not required to read the Appendix. Accordingly, if the authors could not convince the reviewer of the importance of the work within the main text, then this is their problem. As for the individual points that the authors wrote about - as I have already written, part of what they wrote is quite fair, no one disputes that they have some new results. At the same time, for example, I find it difficult to agree with point 1, as I have already written.- Well, in principle, there are two ideas that the authors mentioned (about just two values for all weights and memorising data), which are relatively interesting indeed. This is not something that would be mega-important, but it looks like non-obvious facts, which is a plus.In principle, I can increase the grade for one point. -------After reading the author's responses, I'm ok to promote the rating from 5 to 6. ------------------------------------------------------------------------------------after reading the authors' response and revision, I raise my score to 6. ========= post rebuttal ========Thanks for adding the experiments for Replica - these at least seem to suggest that the approach can work on more complex scenes than shown initially in the paper. I still think the training scheme and required data is relatively specific for this approach that it is hard to see that it will generalize beyond - but that is probably fine for a research paper.  that addresses Q1for Q2 - please include some information in the main paper - not everyone will check the supplement and the paper should be self-contained. I still find the comparisons and ablations weak as the particular training setup and model are the key contribution for the paper and thus without a proper ablation it is hard to know what exactly to take away. Therefore I still remain somewhat skeptical but have raised my score somewhat. To me the paper is still borderline and thus somewhere between 5 and 6 really.  --- Post rebuttal ---I'd like to thank the authors for the response. I've updated my score in light of these additional results. Update: I appreciate the response and have adjusted my score accordingly. ============Post Rebuttal====================After reading the feedback from authors, I still have my concerns. The novelty of this paper is too limited for ICLR. I really do not think a combination of CutMix with existing saliency detection method is a novel method. Moreover, the improvement over CutMix is diminishing. These main concerns are not addressed by the authors. So, my final recommendation is still rejection. I have increased my score to reflect the revisions---------  Since most of my primary concerns are resolved, I have updated my rating based on the revised version. UPDATE:I have read the author feedback and other reviews/discussions. I have updated my rating to 8 from 7 reflect it. ### DecisionThe authors answered all my questions. My decision stays the same. --------------------------- After Rebuttal --------------------------------The authors did a good job in the rebuttal. Most of my concerns have been addressed so I am happy to raise my score to 6. Edit after rebuttal: I have read the author response and I thank the authors for their valuable insights and answers to my questions. It is exciting that this does help in improving the performance on downstream VQA2.0 task though I would have expected the results to be conducted on one of the recent state-of-the-art models instead of very old BUTD model where achieving performance gains is trivial. I would like to keep my rating as it is. ===During discussion period, I noticed import missing references of this paper as written by Nikunj Saunshi. Besides, the authors do not respond to any of the reviewers' questions. Hence I change my score to strong rejection.  *** After reading the authors' responses ***I have raised my score to a 7. I felt that some of the key questions, e.g. regarding generalization to mathematical expressions that are qualitatively different than the training data,  were answered well. This paper should not be reviewed as being methods-driven. It's about demonstrating a new way that deep learning could be transformative for engineering, by allowing engineers to screen proposed designs for stability, etc. ============================================**Post-Rebuttal Comments**==================================I appreciate the revisions and additional results presented by the authors. The authors have addressed my concerns as well as improved the clarity of the model description in the revised version of the paper. While that results are not perfect, I acknowledge that the problem of video generation is difficult and I believe such multi-stage model can motivate future methods in this direction of scalable video generation. Therefore, I would like to improve my score to 6 and would recommend acceptance of this paper. ** Post Rebuttal**The paper has been improved based on the reviews, I think this paper is now ok to accept. Final rating- I am satisfied with the author's response and updating my ratings.  *Post-Rebuttal Evaluation [FINAL]*I have thoroughly inspected the revised manuscript and read the response provided by authors. I was pleased in registering that my two requests were taken into consideration by authors who provided a solid sensitivity analysis and additional experimental evidences on ImageNet. Therefore, taking this into account, I am now convinced in raising my original score (5: Marginally below acceptance threshold) to a full acceptance (7: Good paper, accept).  ### Post rebuttalI appreciate the authors' efforts in conducting experiments to show the latency results. After reading through the rebuttal and the reviews from other reviewers, I would like to down-grade my score by 1.Specifically, I agree with R3 and R4 that it would be better if experimental results are done for MobileNets/EfficientNets to empirically demonstrate the effectiveness of the optimal convolution. Those networks present strong baselines and it would be more convincing if optimal convolution indeed outperforms those. With that said, I agree with the authors that the DARTS experiments have shown that the optimal convolution can be better than depth-wise separable convolutions. As a result, I still recommend acceptance for this paper. **After Rebuttal**I appreciate that the authors partly answered my questions and conducted experiments to show the runtime. After reading through their rebuttal and the other reviews, I will keep my original rating.  After reading the response, my concerns are not fully resolved. For example, based on "For OMP-a and OMP-b, ... The robustness of single network is terrible. ... adversarial examples created from one of these networks can be successfully reclassified by other networks", I am feeling the OMP-a and OMP-b are less effective based on a real white-box attack. Also some other concerns are not fully addressed. Thus I am keeping my rating unchanged. ------------Update after author response: I've increased my score to 5. However, I still think this work is not ready to be published at ICLR in its current form. One of the other reviewers had raised an important point about the reliance of the proposed system on clean text which the authors should consider addressing in an updated version of this work. ******* Update after reading rebuttal ********I appreciate the additional comparisons to prior work added by the authors. I've raised my score to a 6, but I still view this as a borderline paper due to concerns about clarity and impact, along with the  concerns raised by Reviewer 3. =====================Post Rebuttal: I would like to thank the authors for the new results on WebVision-mini and ImageNet-mini, this has partially addressed my concerns as Reviewer3 raised similar issues on the SoTA claim. Overall, I think this paper is well presented and the results are solid, thus updated my rating to reflect this. =-=  Comments after author discussion(Minor) concerns have been addressed.  Thank you for the revisions. ** Comments after reading the authors' rebuttal **I would like to thank the authors for their clarifications. The threat model is now clearer to me - and I think it deserves clarifications in the paper as well.First of all, as far as I understand now, there's a net distinction between backdoors and clean-label attacks. Backdoor attacks assume that the attacker controls the design phase and the training process, and releases a backdoored model (which then someone else re-uses possibly with fine tuning). Hence, defenses against backdoors aim to detect whether models have been backdoored or not, and it is reasonable to expect that the defender doesn't know the training data as well as other design choices (as the attacker released the model). In this setting, clean-label attacks do not make sense (as the attacker controls the training labels too).Clean-label attacks assume a different setup. Here the attacker only injects poisoning samples into the training set but does neither control the training process nor the training labels. Hence, clean-label attacks make sense in this setting. However, it also makes sense that the defender knows the training data (as the defender is the one that trains the algorithm, and the purpose is to either detect and remove the poisoning points or reduce their influence over training) - and hence I'm expecting the authors to do consider previous defenses that assume knowledge of the training set in their work.To summarize, I think that:(1) the authors should clarify in the title that they restrict themselves to clean-label integrity/targeted poisoning attacks.(2) the authors should clarify the threat model, and clearly distinguish poisoning availability attacks (bilevel data poisoning) vs poisoning integrity attacks. Furthermore, in the poisoning integrity/targeted family, backdoor and clean-label attacks should be distinguished and the threat models clarified (in particular, w.r.t assumptions on what the attacker/defender know and have access to).(3) the authors should revise their sentences on the complexity of data poisoning (previous clean-label targeted attacks like poison frogs are not as complex as bilevel data poisoning attacks). A fairer comparison in terms of complexity should also be considered - how faster is this new attack w.r.t. poison frogs and the other clean-label targeted attacks? (poisoning availability should not be considered here as the goal is different in that case).(4) In general, there is need to disambiguate clean-label targeted poisoning attacks from the rest, and better position this work in context. Reading the paper in its current form, it seems that the authors are also able to improve scalability of poisoning availability attacks whereas this is not the goal of this work. I'm willing to revise my score if the authors agree on making these clarifications in the paper, better highlighting the net contributions of their work and the proper context of competing approaches (which do not include backdoors and poisoning availability attacks). ### Post-rebuttal updateI thank the authors for their response -- this helps. Having read the other reviews, I am still leaning towards acceptance. --------------------Post Author ResponseThank you for addressing my concern about complexity and adding Fig. 6. It seems that it is still better than baselines in terms of complexity. ------------------Post Rebuttal:The authors' response has addressed my concerns. Based on the response and other reviewers' comment, I have updated my rating for this work. -------------------------------------After Rebuttal============My main concerns were about the similarity between SUR and URT and the lack of detail in their comparison. I also asked for a clarification on the efficiency of the method.On the first concern, they partially address it with the Coefficient characteristics, I say partially because I would have liked a more in-depth comparison of the characteristics, but technically they have addressed my question. For the second one, they now provide the training time, and the testing time could be found in Section 4.2.Overall, even though I still think that this work lies in the application side, it is interesting enough to be published at ICLR, so I have accordingly raised my score. ## After rebuttalThe authors have addressed my main concerns and I've decided to raise my score from 6 to 7. ################ Feedback to the authors' response ###############As the authors have addressed some of my main concerns and provided nice extra experimental results, I will raise my score to 6. ----The authors rebuttal and the revised version have not fully addressed my concerns. It is not surprise that partial updating outperforms pruning by a large margin, as the inference of small updating still uses the whole weights of the network. Comparing to pruning, the technical contribution of this work is limited, so I would like to keep my original rating.    Edit: After reviewing the author response, I leave my score unchanged. I believe this paper should be accepted.  Edit after rebuttal: I have read the author response and I am thankful to authors for answering my questions. I keep my rating as it is and believe that the paper should be accepted. I hope this will be widely adopted in NLP community. UPDATE: After reading through all other reviews and responses by the authors, I share the concern that the theoretical justification of the paper is lacking as the connection between the truncation error and the improved algorithm performance is not rigorously proven. Therefore, I have reduced my score. **Update after rebuttal:** The author rebuttal clarified some minor issues for me, but it did nothing to address my main concern, which is that very similar methods have been proposed before. I'm therefore keeping my score the same.  --------------------------------------------- Edit: score modified after reading the authors' reply.Edit 2: Regarding the issue of optimization getting stuck in local optima or finding global minima (moved my comment here for visibility):I thank the authors for their flexibility on this issue, but I'd like to weigh in on this saying that from my perspective those statements were actually accurate and useful, and should be kept in the paper as they existed originally. They are properly backed up by citations, unlike the comment of "getting stuck for sure in a local optimum", which is very much dependent on the optimization problem. I am willing to follow up with the other reviewers, should they consider I am mistaken.I have also increased my score after reading the author's response, and I agree that the final decision should not depend on this specific issue. If the paper is accepted, I'd like it to include the original statement, which can be very informative for some readers. ---------------------------------------------------------------------------------------------------------------------I have read the response, and the rating is not changed. -------Thank you to the authors for their response and update. I have read the response and am keeping my current rating for the paper. ----------------------------------Rating after discussion: lowering to a 6, as I share concerns with R1 about experiments and generalizability of the proposed approach. Update after author response:The author response is much appreciated. However, my two main concerns remain unaddressed. The authors may add these additional experiments/results to Table 3 and 4 in further revisions for a stronger submission.* Table 3 is the main result of the paper which claims policies learned in TextWorld (TW) environment can be transferred over to ALFRED (ALF) environment under zero-shot setting. This result alone has several weaknesses -- (1) Evaluation done on non-human goals in ALF seem to use same template as that used in TW, so it is not surprising that agents will have non-zero success rates on similar language specifications. (2) When evaluation is done on human goals (which seems to be the real test), the agent's performance is very low. Also, there are no baselines (e.g., random) provided to compare those scores against. (3) Why are the experiments only conducted in zero-shot setting? This actually brings me to the second weakness.* Since the transfer learning is happening from a pure-text TW environment to a physically simulated (with visual input) ALF environment, it is more interesting/relevant to see how the language module pre-trained on text-only TW adapts to multimodal setup in ALF. This adaptation will require further training/fine-tuning on ALF so that visual/control modules can adapt to this pre-trained language module. This experiment was attempted in Table 4, however, as pointed in my initial review, this falls short of proving any claims made in the paper because the agents in Table 4 are learned with an oracle state estimator which means there is no visual input processing during this mode. It can also be noted that the Controller is also a heuristic module with no learning. Which means the setting used in Table 4 reduces learning/evaluating in embodied ALF environment to a pure text-driven environment. Edit ===Other more confident reviewers have pointed out some concerns. I am still positive about the paper, but due to lack of rebuttal I am going to downgrade from my initial score. [update] reduced score given the lack of  ** POST REBUTTAL UPDATE**I went through the other reviews and the author rebuttals. I thank the authors for throughly addressing all of my concerns and running some baseline experiments which I think are quite interesting, and add to the completeness of the paper, including acomparison to EST and an optimal K-shot baseline. I think the idea is simple and interesting, and now the paper has enoughexperimental comparisons to be a valuable addition to the conference. I have updated my score accordingly. %%%%%%%%%%%% After rebuttal %%%%%%%%%%%%%%%%%%%%%%%%%%%%I appreciate the great efforts the authors have made in responding to the comments. Most of my comments have been well addressed, so I have increased my score. Nevertheless, another important question is in the over-parameterization setting, why the solution obtained by the convex approach has good generalization property. This question is not addressed in the rebuttal. Probably it is too much to have this in one paper and can be an interesting question for future work.  ---I thank the authors for the response, which addresses my questions. I still believe that the paper provides valuable contributions and insights. Update:Thank the authors for their responses, clarification, and additional experiments. I read through authors responses and the comments from the other reviewers. I still think this paper makes a borderline case for 1) its technical contribution on extending DPS and thereby achieving significant performance gain on a toy problem and MRI reconstruction tasks, still 2) with limited novelty and room for a more extensive experimental validation (perhaps, beyond MRI). My other concerns on clarity and significance of experiments have been addressed. I would raise my rating to marginally above acceptance threshold (borderline). ### UpdateI thank the authors for their comprehensive response. While its unfortunate they couldn't compare to any other active methods, the related work and overall clarity of the paper is significantly improved. The t-SNE plots were informative and interesting. While I have reservations about the paper's lack of comparisons, I think its publication still might be a net positive for the research community.I have updated my score.##### Other commentsLet A(X)=F^H D\circ F X. The expression A^H(Ax-Y\circ sign(A(x))) is a subgradient of 1/2|| Y - |A(X)|||^2 but A^H(|Ax|-Y) is not. I would avoid calling (9) projected gradient descent as the "gradient" isn't really a gradient."We have performed a statistical analysis on the performance gains made by A-DPS over DPS in the MRI reconstruction task, concluding that they are indeed statistically significant." It would be nice to see confidence intervals in Tables 1 and 2.#### Questions/comments that do not effect the review:Why use an LSTM/any network with memory? It seems the next sample depends on the previous samples, but not their order. The ablation study on pg 6 shows that memory helps (at low sampling rates), but I don't understand the intuition why. Could the LSTM just have more capacity?Typos:pg 2: "cells.During" spacepg 3: "However, This" capitalization UPDATE AFTER REBUTTAL:I thank the authors for their responses and appreciate the inclusion of some of the requested changes in the paper. However, the paper still misses the comparison to other adaptive methods which is the paper's greatest weakness. Therefore, I decided to keep my score at 6. Update: I've updated the score given the authors' response, see my comment below. ---------- Update on the revised manuscript ----------I have read the new version of the paper and it reads a lot better. The new expanded methods section, and the definitions for different variations of GONs makes the paper much stronger and easier to understand. I appreciate and like the new experiments that show GONs capabilities on LSUN, comparisons with VAE on ELBO. Most of my concerns have been addressed in this version. I think this paper makes an interesting and novel contribution and I will raise my score accordingly.  ==============================Edit after author response: we thank the authors for their response and providing some more empirical information. Overall I feel that this paper presents a neat idea that could be of interest to some people in the community, and I have modified my score from 6 to 7. It would be great for the authors to discuss the importance of initialization, as in particular, it seems to me that the sign of $m_2$ can never change  (from its initial value), indicating perhaps that practitioners should try initialization at either and select the better performing model. #########################################################################Edit after author response: We thank the authors for their response to my concerns and those of other reviewers. I have updated my score from 6 to 7 based on their changes. post-rebuttal: Authors pay much effort on addressing issues that I mentioned, and I appreciate that very much. My issues can be partially resoved, but the main issue is regarding the high-level thinking on semantic supervisions, which is impossible to be fully addressed in a rebuttal. Besides, I agree with AC that (a) the improvement is very limited and (b) semantic labels are hard to obtain. Overall, I raise my rating to 5 (not so good, but could be accepted), because I really appreciate authors' effort. post-rebuttal:I thank the authors for their clarification and ablation analysis. Most of my concerns are resolved. I will keep my original score and I think the paper could be accepted.  Post rebuttal: I thank the authors for their detailed response and paper revision. My concerns have been addressed, and I particularly appreciate the experiments presented Table 3, Figure 5 and Section 5.4. I am happy to maintain my rating and recommend acceptance.  **After rebuttal, I think the authors did a good job in clarifying some points and adding an example in the experiment, which is why I upgrade the score I gave. However, I don't understand why they don't compare in the experiment with some of the methods from the cited literature concerned with the same model, especially Diolaiti et al. (2005).** -------- post-rebuttal comments --------I have read the concerns raised by other reviewers as well as the author response. I still feel that this is an interesting work and its findings are of potential value to the community: There has been a considerable amount of recent work in open set recognition for deep models, and this paper calls into question the need for sophisticated techniques by showing (fairly rigorously, in my opinion) that simple strategies and the right choice of feature engineering works better. I agree with the authors that not using ImageNet pretraining is an unrealistic and unnecessary constraint  moreover, the method generalizes even when evaluated on datasets such as MNIST and SVHN, which are distributionally very different from ImageNet. Further, I think the paper acknowledges prior work appropriately and did not find any of the claims made to be unreasonable. I agree with R1's concerns about overlap with two recent papers, but found the author response to be satisfactory. Overall, I will retain my accept rating. ========Post-rebuttal comments:Thank the authors for answering my questions. I think the current version of the paper is below the bar of acceptance at ICLR and I hope the authors can incorporate the answers to make the submission stronger in the future. POST-REBUTTALThank you for your answers and incorporated modifications! I think you've succeeded in addressing my major concerns, so I'm raising my score and recommending accept as promised. -----Post feedback edit:The authors did not address the (admittedly, small) terminology issue I raised in my review. More specifically, I argued that their method in general requires transmission of the value of the threshold function, to which they replied that this is not true if the threshold function has a particular form (not required by their general theory), and then added some discussion about removal of spikes which was not what I was getting at. My point still stands: in general, for arbitrary threshold functions, transmission of the value of the function is needed for succesful decoding, and this constitutes more information than what is generally meant when discussing spike trains (which are just sets of time stamps).Moreover, I was not convinced by the authors' reply to the points raised by Rev 4. This is actually more important than the point above, since Rev 4's objections are on the substance of the method (unlike my own points, which are little more than presentation and terminology matters).Therefore, I am lowering my score.I am not convinced by the authors' reply to Ref 4's comments. Moreover, the smaller terminology issue I raised in my own comment (about the # After rebuttalThe authors did a good job of answering my concerns. I keep my original positive rating. .Update after Rebuttal:An additional experiment on real data was added which I find a valuable addition to the submission. AR2 points out a similarity with AET which I did not notice in my initial review. I feel this does limit the contribution somewhat. As the rebuttal points out there are differences between the proposed method and AET but the core idea is very similar. In my opinion there is enough difference to still recommend acceptance ---Additional comments after rebuttal--I have carefully reviewed the authors' feedback regarding their comments on how their proposed method differentiates the existing (AVT and AET). Unfortunately, it did not address my concerns about the novel technical contributions in the proposed paper. Obviously, the authors applied the "Transformation Equivariant Representations by Autoencoding Variational Transformations" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). I am not sure why in the rebuttal, the authors claimed, "Their proposed method distinguishes from AET significantly in two aspects". I would urge the authors to check both papers below, and it clearly defines the Transformation Equivariant Representations learning by Autoencoding Variational Transformations, which could be applied for various types of data. [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT)Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. I am not sure why the authors answered that "3D objects are unavailable at the testing stage." The proof of Autoencoding Variational Transformations for 3D data directly should not depend on the availability of 3D data. ----------- Post-Rebuttal Comments -----------------Thanks to the authors for the response and for updating the draft. Some of my queries were clarified. However, I still think the paper lacks the mathematical rigor required for a theoretical security/privacy paper. For instance, in the updated proof of Theorem 2, the authors consider the output of a pseudo-random generator (PRG) as uniformly random and claim information-theoretic (perfect) security. However, PRG output is not uniformly random, and one needs to consider computational security, which is standard in cryptography/security literature. Moreover, the comparison with (Bonawitz et al., 2017) and (Bell et al., 2020) seems a bit unfair since the threat model in the paper is weaker. As an example, for the same threat model in the paper, it is not clear if (Bell et al., 2020) would need the strong assumption on dropouts. For these reasons, I retain my original score.  Post-rebuttalI thank the authors for their response, addressing some of the issues/questions I had raised.After carefully reading the other reviews (and corresponding author responses), I agree with the valid criticisms raised (some of which I had overlooked initially), which unfortunately further dampened my enthusiasm for this work.As a result, I am slightly lowering my score.However, I very much appreciate the author's efforts in this very promising work, and hope that they will revise their manuscript to take into account the feedback raised in the reviews.###  EDIT: **post rebuttal. I'd like to thank the authors for their response. As scalability to high-dimensional hyperparameter spaces is presented as a key advantage of the method, direct comparisons to high-dimensional BO techniques would be needed. The fact that using trust regions could benefit HOZOG, or that HOZOG is a better strategy compared to TurBO and REMBO, should be demonstrated empirically. I am keeping my score to 4 as the current positioning of the paper would require direct comparisons with these baselines. ** EDIT: **post rebuttal (could not add a comment readable by authors). Thank you for your rebuttal, I have read it and the other reviews. I agree with the other reviewers that some more baselines for high-dimensional benchmarks are required, and was not convinced by your rebuttal to those requests. I also think the aspect of non-convexity and local optima touched by Reviewer 4 warrants some discussion in the paper. I maintain my score of 4. ** ---- After Rebuttal ----After reading other reviews and the rebuttal, I stay at my current score. Given that the paper does not contain much analysis about "why", the paper is about an empirical discovery of a mode that is critical in getting good performance. I think this kind of discovery paper is also important to share with the community. ----------------------------------Edit after reading other conversations.I do think the other reviewers make some fair points. I've adjusted my score to a 9, though, and I'd very much like to see the paper accepted. Here is my thinking on a couple points that led to this score adjustment.AnonReviewer2's SimPLE-like ablation request: If I'm understanding this correctly, the reviewer would like to see the policy model trained on reconstructions instead of the latent space (or maybe would like the world model trained to future predict in pixel space? not completely clear to me). To me, this would be a potentially insightful ablation, but I think not a dealbreaker that they do not. Two points on this:(1) If I am understanding the paper and conversations, SimPLE significantly underperforms in the metric (Atari "end performance" under certain normalizations) that the authors care about (and is a fairly established metric). I, then, don't see a *particularly* strong motivation for careful ablations of the method.(2) Pixel-based future predictions generally perform poorly, and this is I think fairly widely thought to be a strong contributor to the failure of model-based approaches. Again, it would be nice to see that happen here (and it might be insightful to see the quality of the frame predictions) but I think there is a reasonable expectation that this would work poorly. AnonReviewer1's thoughts on the value of this sort of work in ICLR: I see your point, but I personally think there's great value to this sort of work in this sort of venue. End performance on Atari has been (for better or worse) an important baseline for the field, and the creation of performant model-based approaches has been a central question for several years, now. The proposed improvements over DreamerV1 (which has seen fruitful applications in other work) might be simple, but DreamerV1 did not work well on this baseline, and this does. I think that we far too often get excited by complex new methods, often evaluated with novel and poorly understood baselines and metrics, only to drop them as time goes on. That the innovation is a simple one should make us excited to try it in other applications.I also think that their discussion of evaluation metrics is very useful -- we continually need more careful conversations about the right ways to measure success. So, in short, the paper might be a technically simple innovation, but it puts forth strong evidence that the method is useful. ---After rebuttal---The authors have partially addressed my concerns, however, I am still not quite sure why their method would be better than SimPLE. The authors' ablation study of removing image gradients does not address my main question about where the performance benefit is coming from. I am assuming that the architecture and hyperparameters that the authors use are different than SimPLE. I think one must instead replace predicting the latent state with the image as SimPLE did to see if that makes a difference in performance. Therefore, I will be keeping my review the same. (Added on 11/29/2020) **Post Rebuttal Comment**I thank the authors for sincerely replying my review comments. I think the authors answered my questions. Addtional Comments- Section 3.4: $(c_1(x[1]),...,c_D(x[D])$ $(c_1(x[1]),...,c_D(x[D]))$ (Add a right parenthesis)--------------------------------------------------------- **=========== Update to Review, after Updates to Manuscript during Rebuttal period ==========**Summary of improvements during rebuttal and remaining concerns:- Extended the literature review, with references to an unsupervised clustering method (k-shapes),  and methods that use CAE, LSTM and VAE. This is a significant improvement. The literature review still does not discuss previous losses that lead to clustering within the latent space, which is what the proposed methods here perform, and are hence closely related (see my initial comment).- Rephrased/corrected claims about performance of the method. The corresponding claims about performance is now that the methods can usually improve k-Means clustering performance, improving the baseline CAE. This is now accurate. Yet, I do not think the performance is a sufficiently strong argument unfortunately (especially given the limited evaluation, only against few and basic baselines).- Evaluation has been expanded by employing unsupervised clustering via k-Means, k-Means+PCA, and k-Shape in Table 1. However, there is still no comparison with any other more advanced method, such as SSL based, DL based, etc (as per my initial comment). I think there is a lot of improvement here, before the paper can display improvement over the current state of the art.- No improvement on the ablation study, which is currently not particularly useful, e.g. by investigating aspects of the losses and their behaviour to provide more insights into the method, or performing empirical sensitivity-analysis to meta-parameter values.- Added clarifications about some values hyper-parameters used (Adam optim with default TF2.1 learning rate for all). There seems there was no attempt to find optimal hyper-parameter configuration for each method, in order to perform fair comparisons (e.g. each method may require different weighting/learningrate). The authors seem to acknowledge the issue and defer it to future work. However, I think that without such investigation (e.g. via an empirical study and sensitivity to these hyper-parameters), we cannot be strongly confident about conclusions on the relative performance of methods, as they can be sensitive to configuration.- Provided the code of the work as supplementary material, hence reproducibility is greatly increased. Thank you.- Improved clarity of the paper by improving Fig 1 and some of the math notation problems. Overall, the work has been improved during the revision, hence I will raise my rating (from 3 to 4). However, I believe the remaining problems of limited literature review, evaluation, no sensitivity analysis to hyper-parameters (or effort thereafter to configure them for each method) or other sort of empirical analysis that would give insights to the behaviour of the losses, still keep my score relatively low. UPD:  Thank you for your answer and for addressing the points raised in the review. Still, I agree with the concerns raised by AnonReviewer4 on the small scale of the networks used in the experiments (e.g., LeNet, fc). I am still not convinced by the experiments and the excess of hardly understandable plots given to support the main bulk of the paper's claims. Therefore, I leave my rating the same. -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing some of my questions and concerns.Unfortunately, I still think that some aspects could be better analysed,it is not crystal clear to me if the improvements come from the GP or their proposed variational informationtheoretic function. I am keeping my original score. Edit: I have read the authors' response and update  manuscript. I appreciate the new experiments and some of the clarifications. However, some of my fundamental issues with this work are not addressed.The distinction with Ingraham et al feels forced. It is important to compare head-to-head on Ingraham's dataset in order to truly understand the tradeoffs between these methods and to understand the differences in performance between flexible backbone approaches and a well tuned rigid backbone approach. Why are perplexities for Ingraham et al not reported for all cases? Improving over Ingraham when only low resolution structures or incomplete structures are available is interesting, but I also question how useful this case is. When attempting to do _structure-based design_ of new proteins, how often are fully specified structures not available for those tasks? Ingraham et al presented a variant of their method based only on a flexible structure specification and it isn't clear how that contrasts with this flexible specification. It is certainly possible that this approach is better, but we need a head-to-head comparison to know.With that in mind, I'll raise my score to 5. I think this work has promise, but is not yet ready for publication in its current state. Update: Thanks for the authors' response. After reading the response and the other reviewers' comments, I think the paper needs to be further improved, and thus I will keep my score. Thank you for the additional comments and results. I have increased my score.Requiring a simulator for sampling is indeed weaker than knowing the whole dynamic model. However, it is still a strong requirement, so the applicability of the proposed method in real-world scenarios is limited. Addressed Concerns:- corrected the typos- Some figures are updated Not Addressed:- Figure 1b is not uniform as 1a (legend information is missing)- As mentioned in weaknesses: the novelty aspect of this work is missing.- Further, the authors have not answered question 1 in major comments.- Based on Fig 2 & 3, Calcium GAN generated signals are mismatching with recorded data. How one can deploy this model in real-time when model results are not identical at all?.  Thank you for your response. I have read the revised paper and the discussion with other reviewers. While I still think that the overall problematic discussed in this paper is important, I am not satisfied with the revisions. I think the paper would benefit from more technical content and clear mathematical definitions of the procedures used. I have updated my score to a reject.  [After rebuttal: Increased my score from 4 to 5. Still think the approach needs some more work to appeal to the broader ML community and in its current state would be best suited for a conference focused on Alife] Post-author response: I appreciate the extra experiment on GLoVe vs Linear on sentiment analysis, it is what I was asking for. I have raised my score in response to that, as well as the additional reporting on the results. The discussion in Bayesian terms was interesting, I think it would help. Nevertheless, I was thinking is it that it should be possible to construct embeddings that have known syntactic vs semantic properties. E.g. one could increase/decrease the context size, perhaps to extreme values in the case of models such as GLoVe. And then we could actually have a much stronger prior. If the paper is accepted, I think such an experiment would be very informative. ---The response from the authors satisfactorily addresses several points that I raised. However, I am not fully convinced that the LTH on GANs has provided significant new insights. However, based on the response I am inclined to raise my score to above acceptance threshold and tend towards acceptance. =========================================================================================Edit post Rebuttal:Thank you to the authors for their in-depth response and the effort they put into responding to this review, and my apologies for not engaging during the discussion phase. I'll respond to several points with the goal of furthering the discussion to try and avoid unfairly "having the final say" when the authors cannot respond.-"The biggest potential impact of our work is that it provides empirical evidence that lottery tickets exist in GANs"This reviewer's opinion is that this is not at all surprising. While GANs do have unusual and unique training dynamics on a number of levels, and interact interestingly with many typical building blocks, many of the aspects of neural-network based GAN models (prunability, the relationship between signal propagation and trainability, model capacity, etc) are largely indistinguishable from those of more "typical" neural nets. Nonetheless I do agree that, if one holds the lottery ticket line of work to be important or relevant (which I personally do not, but I will withhold my bias and not "legislate from the bench" here), its extension into the realm of GANs does first require verification of its existence in this regime. I apologize to the authors if it seems like they're pushing against a brick wall here because this reviewer is not a disciple of the lottery ticket hypothesis--please note that to the best of my ability this review is meant to be calibrated around this bias and instead focus on the strength of the manuscript.-Transfer between tasks: I appreciate the authors response on this point; transfer of masks between tasks (rather than pretrained weights) does indeed represent a different modality of transfer learning, and one which may be well-suited to GANs which are known to be difficult to re-train or fine-tune in many situations (although there is a growing body of work on this topic outside of the pruning/lottery ticket context).-"Lottery tickets without early weight rewinding techniques can still hold to small-scale tasks. "Yes, this is precisely the problem--lottery tickets without early weight rewinding *do not, in general, tend to hold on large scale tasks*. This again returns to my baseline issue (one of many) with the hypothesis in the first place, that it has to be modified to work on even models like VGG.-on checkerboard artifacts: This reviewer is very well calibrated to viewing GAN samples, and holds that the checkerboard artifacts are vastly more visible in the sparsified models. I would encourage the authors to, in future work,  consider why this might be (what about sparse networks leads to increased checkerboard artifacts?) rather than seek to claim that the unpruned models are also checkerboard-y.-Additional experiments: I commend the authors for repeating their experiments on a range of architectures, and agree that the improvements for the sparsified models are in support of their argumentation and conclusions.-Thank you to the authors for updating their notation and bibliography.On the whole, I still feel that this paper is borderline. While the author's responses are fairly strong in context, re-reading the manuscript, I still do not feel that the paper (which is most of what matters here) is especially strong. I am upgrading my score to around a 5.5 (which I will simply round up to a 6 on OpenReview). I think this paper is true borderline--I won't argue for its rejection, but I don't feel especially strongly in favor of it and cannot champion it. ----------Update after author response----------I thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score. Post-rebuttal:I believe the authors have done sufficient work in their revision to address my concerns. Thus I'm leaning towards acceptance and raising my score to a 7. Edit: As mentioned in my comment below, I believe the authors have done sufficient work in their revision to address my concerns and am revising my score to an acceptance. # Rating and comments after the rebuttalThe authors addressed my concerns in their feedback and the revised manuscript they have provided. In particular, I find the claimed contributions much clearer now. In my view, they have also suitably addressed the concerns raised in the other reviews. As a result, I increase my rating to 8 as I think that this work is interesting, novel and impactful. After rebuttal:I appreciate the authors' detailed response to my questions, which largely addresses my previous concerns. It's very pleasure to reviewing this interesting, innovative and well-written paper.A clear accept. -----I read a valuable author's response, and I keep my positive score. =======================Rated up after authors' clarification. =====================Post RebuttalI have read the authors' response. All my concerns are addressed properly. However, I still doubt that even the corner cases of \eta have a better performance, would there be a systematic way to find the optimal parameters reflecting the true potential of this method. Thus, I will keep my score unchanged. ----Update: I am very happy to see the new experiments that validate the implications of the Stackelberg games theory. The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi-task learning. The additional experiments in Table 2 are useful, and suggest that much of the benefit comes from the greater expressivity due to task-specific matrices. --------After author's response----------I am not fully convinced by the explanation of the motivation behind rotation matrix, in particular why it is aligning with the single-task learning, which is counter-intuitive. The authors provided more ablation studies, however, the evaluation on datasets is still quite preliminary with some questions remaining (such as why there is a discrepancy between the two versions of Rotograd on the second dataset). Therefore I am keeping my original score. ---After author responseThe authors have improved the presentation and added the necessary ablation studies. I appreciate the authors' effort. I am glad to raise the score to reflect the changes. I hope this work will inspire future research on hierarchical planning algorithms. *****************After author response:I appreciate the updates you've made to the paper to better flesh it out, including the diagrams, pseudocode, and additional ablations. I've increased my score accordingly. Regarding generalization, I fully agree that it can be difficult to show. That said, when it's advertised in the title of the paper, I expect it to be clearly shown in the paper itself. It seems that the language has been greatly toned down in the updated version. However, without entirely re-reviewing the paper, I am unable to fully recommend acceptance.Aside: I would have appreciated responses to my questions directly so that I don't need to dig through the rewritten paper to find the answers to my questions. __________After author response:I appreciate the authors response. Previous topics:1) The authors add ablations on the hard vs. soft min during the graph search, the additional results are informative, but not conclusive. Given that the overall performance is similar, the authors need to demonstrate the soft-mins benefits for each experiment and over more training seeds. 2) For generalization, the authors confirm that this method is unable to generalize to new environments, though clearly it has other benefits in terms of data efficiency and robustness of solutions. I believe these are still important benefits, though it would be useful to discuss how generalization may be achieved.3) For harder experiments, the authors note that the baselines perform poorly there and thus these tasks were not considered. Though reasonable that the baselines are unable to perform in such cases, harder experiments would show the limit of the proposed algorithm. It would be useful to see for instance how well it scales with dimensionality, how quickly the success rate falls off.New comments:a) I believe the title change away from Generalization is an improvement, though the algorithm name "WORLD MODEL AS A GRAPH" seems to not capture the novel aspects of this work. This name I believe would be more readily applied to search on the replay buffer or semi-parametric topological memory.b) R2's point that much of the robustness may be a factor of choosing states further from the wall is an interesting one. It would be interesting to examine exactly *why* the method is robust.Overall, I believe the paper is interesting and proposes some novel ideas that have benefit, it requires more thorough analysis, and thus I am leaving my score unchanged. ---**Post Rebuttal**In my review, the main concerns were (1) validity of assumptions, (2) confusing writing/notation and (3) unclear takeaway. The rebuttal appropriately addressed (1), although I am looking forward to the revision to see how this is discussed in the paper itself. I cannot really say anything about any improvements on the writing (2) without seeing the revision, but I am confident that the authors can address most of the issues pointed out by myself and other reviewers. Regarding (3), unclear takeaway, after reading the authors' response as well as the other reviews, my concerns are somewhat assuaged (partly because the assumptions were addressed better), although I am still unsure how or if the results in this paper could be expanded to realistic attention models.There are additional issues I raised during the discussion (general lack of citations in particular), however this can be fixed fairly easily for the camera ready so I am willing to give the benefit of doubt and raise my score to 6 (borderline accept) Updates: Thanks for the authors' response. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to show the superiority of the results. I retain my original decision. ## After rebuttalThank you for answering my questions! It address my concern and I revise my recommendation.- Please clarify in the next version how the checkpoint is selected (e.g. which epoch is selected?) given a product of seed and hyperparameter. Update after the revision and the author response ===I would like to thank the authors for the additional work and effort invested into improving the paper presentation. This has made me increase my score; I still have some doubts regarding the experimental setup (using target dev sets and taking this information for granted), but maybe a high-level question I posed in my review really does go beyond this work. Given that the main premise is 'quick adaptation' to new (and unseen) languages, the inclusion of at least one-two truly low-resource languages would have still been nice to link the motivation with the experimental setup. -----------------------------------------Post-Rebuttal-----------------------------------------I believe both the strong points and the weaknesses I pointed out in my review remain valid. Therefore I am keeping my score unchanged.In case of acceptance, I suggest to the authors to carefully address all the comments from my review and the other ones in the camera-ready version (especially the issues related to clarity.).---------------------------------------- =====================Edited after author response: I thank the authors for their considerate responses. Overall, my opinion remains mostly unchanged, and I share similar opinions to reviewer 3 and 4 that although the proposed idea is interesting and intriguing, the paper is not quite ready at this point. I would like to see the authors present either: 1) stronger empirical evidence for the importance of their metric or 2) a more solid theoretical foundation of the measure they propose. **Update:** The authors clarified all my questions very well. They also added an extra plot for the double descent experiment on a different architecture (ResNet). Although I feel a bit lukewarm about the added plot (in that the double descent phenomenon is only somewhat weakly reflected by their empirical measure), I've increased my confidence score from 3 to 4 to appreciate their efforts in addressing my concerns. Good luck to the authors! ***I would like to thank authors for accurate answers and a lot of work put on reworking the paper. Unfortunately, I still find my concerns about motivation for the metric valid, which together with the rather weak performance creates a problem for this paper. I highly encourage authors to continue the work and try to explain the reasons for this correlation and find justifications for usage of the metric.  Post-RebuttalI thank the authors for the detailed answer. In light of the response of the authors and the other reviews, I still recommend rejecting the paper, with a rating of 5.Some comments based on the rebuttal:I find the data in table 6 to support my point on confounding variables. The churn with data augmentation fixed on GPU is systematically higher than with random data augmentation on TPU when model initialization is random. Note that the main differences here beside the fact that data augmentation is fixed or random, is that the accuracy is lower by 1.5%-2.5%. We see again an increase in churn related to a decrease in accuracy. Just as when the data augmentation was removed altogether. The authors say that accuracy change itself isn't predictive of churn because we could make a training perfectly reproducible with lower accuracy thus leading to 0 churn, but the same argument would hold for removing a fixed data augmentation from a perfectly reproducible training. When not fixing the whole training process, two different interventions leading to equivalent accuracy decrease could lead to equivalent churn. This for instance would be a direct consequence of a binomial modelisation of the model performance as a function of test set size and model average accuracy (and by the way which models fairly well test accuracy variation for the datasets-architectures in this paper). The lower the accuracy, the higher the variance.Table 2. We boldfaced the results in table 2 with the best mean performance, which we believe is a standard practice.It is unfortunately common practice indeed, but it is a bad practice. Results that are so close that random fluctuations could explain the difference should not be considered as different. ***Post-Rebuttal***I want to thank the author for addressing my concerns. However, the authors did not address the issues of the evaluation. I think the paper can be further improved by providing more convincing results and analysis. For me, the significance of the proposed method is minimal. Thus, I will not change my score. ####################################### AFTER REBUTTALThe new baselines added make the experimental validation more convincing. Therefore, I have raised my rating to 6 (Marginally above the acceptance threshold). However, I still believe that the contribution is incremental, and I think the paper would gain in terms of novelty if it focused more on the detection of OOD data and adversarial attacks (which right now is more like a preliminary test). Update: Following the authors' clarifications and additional experimental work, I'm increasing my rating to 6. Edit: Based on the author response in terms of adding additional experiments, I'm raising my score to a 6. ##########################################################################Post-rebuttal.Thanks for a detailed response that clarified some of my questions. I think the overall quality of the paper increased and I am happy to see additional information (like additional literature and an ablation study in Section 5.4) and a somewhat improved explanation of the core idea. However, I am still hesitant to give this paper a higher rating, in part because I find Section 5.4 to be poorly written and somewhat confusing (it lacks any sort of conclusion or insight and I cannot read Figure 4 at all, the text is too small) and partly because the paper does not clearly put its results in the context of the recent model ensemble progress (and thus makes me doubt the impact of this result on the field; although it does seem to be promising compared to the mentioned baselines). -----------update-----------After the discussions below I have changed my score from a 5 to a 6.  **The score does not represent the initial review. It was updated following the discussions below.** =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Unfortunately,  I still think the results on the object representation is not convincing due to lack of comparisons with other embedding methods, and more needs to be done to study the generalizability of this method for complex non-street scenes.  I retain my original decision for these reasons. ****Update after rebuttal ****I am increasing my rating for the paper as they did all the experiments I had asked for and updated the paper accordingly.************************** =====POST-REBUTTAL COMMENTS======== Initially I had only minor comments and the authors addressed all of them. I will keep my score. --------------------------------After rebuttal:Though the paper is interesting in showing improved proposal and detection performance by using attention, excitation, and transformers, the novelty is not significant, especially when it is shown that the claimed cross-modality attention and excitation do not help much. The other reviewers also show the same concerns. I incline to degrade my score to 5.  Opinion after rebuttal:Originally, my rating was a 5. The authors have rebutted my concerns regarding the experimentation, which have gained in insight. The limited novelty and fit to the conference do remain pressing issues and it seems that this is partially shared with the other reviewers. My recommendation is therefore to extend and resubmit. ---------Update after rebuttal:The updated paper addressed my concerns regarding missing details. I appreciated the efforts of comparing with more sophisticated backbone features. However, as pointed by other reviewers, the novelty of the paper is marginal. I keep my original rating as 5. ===========================Score raised to 5 following response to the rebuttal below, then to 6 following the re-rebuttal. ---### RebuttalI thank the authors for their reply. I'm more confident this is a good paper now. ***Post rebuttal review***Having read through the rebuttal, the updated submission, and the reviews of the other reviewers I have upgraded my review from a reject to marginally below acceptance. This is for two main reasons. 1) the authors have vastly improved the presentation of the submission, which now looks a lot easier to read, 2) the authors have clarified for me, at least, what the main contribution of the work is. That said I am not entirely sure what this contribution adds to the equivariance community, hence why my recommendation still leans towards reject. As far as I am aware, solving the equivariance equations is not the large bottleneck to progress in our community. They are linear equations, and there is work back into the 80's solving them (check out people like Pietro Perona, Patrick Teo). I think more importantly we need to focus on pushing the boundaries in areas such as extension to non-Euclidean manifolds, convolution over non-compact groups, learning symmetries, etc. While this work is clearly mathematically sound and the authors have demonstrated deep knowledge of the area, it feels a little like retracing prior works. That said, if the other reviewers disagree then I don't mind this paper being accepted. Perhaps since I have worked in this area, what appears as obvious to me is not generally acknowledged and this paper may serve as a useful clarification for those wishing to dive into the literature. Edit:Having read the reviews, rebuttal and updated paper, I have decided to maintain my score of 6. ---Edit after rebuttal:I am sorry for not yet being able to support acceptance of this paper. I appreciate the authors' work during the rebuttal, and I agree that the execution of the paper improved in this version.But the issue remains that the proposed algorithm does not fundamentally lift any of the current obstacles in implementing backpropagation in the brain. Most importantly, the algorithm is very similar to exact standard backpropagation, and it still requires the same coordinated phases as backpropagation: the forward phase and the backward phase. The learning rule is non-local in time and while it does not use activity differences, that is not necessarily a good thing. For example, the connection to spike-timing-dependent plasticity becomes harder to establish. The authors verify that learning still works after important approximations are made (notably to avoid weight transport), but these approximations were already previously reported and are more or less obviously applicable here, given how close activation relaxation is to standard backpropagation.Finally, I really recommend the authors to more explicitly state and discuss the temporal non-locality of the algorithm when introducing the method (and in Algorithm 1), which remains insufficiently clear to me. ## Second ReviewThanks for taking all my comments seriously. After carefully reading other reviews and quick modifications introduced by authors, I believe this work is richer and has shown some potential towards building scalable and robust alternative to BP. Thanks for including angles as suggested, this further supports hypothesis proposed in this work. I really appreciate results using CNNs and cross entropy loss, therefore I increase my score to 7. It seems that other reviewers do not appreciate that training a network using hebbian like updates and without BP requires some nontrivial engineering tricks and theoretical considerations which are now well described in this paper (updated manuscript). It is difficult to match BP gradients, and many popular alternatives including FA, DFA, DTP, EP struggle to match BP performance when tested on complex benchmarks (Cifar, imagenet, etc) with complex architectures (CNNs, RNNs, etc). Beside this given approach is robust even when backward weights are trainable. However I agree storing backward and forward synapses challenges the bio-plausibility of approach, which I think can be handled if local representation are handled differently. Nonetheless, changing few strong words (optimal BP, optimal gradients) and derivation, supports major hypothesis proposed in this work. A better justification on non-local updates as raised by other reviewers is required to further strengthen this work. But i liked the results with activity relaxation and how close gradients are with respect to BP. Combining current approach with other bio-inspired approaches might solve some key aspects of current learning algorithm. Current approach is still heavily dependent on backprop, and partially gets closer to bioplausible approaches (mainly hebbian like update rule). Testing this on deeper architectures (Resnet, student-teacher etc) might further strengthen your work.  ## Minor commentsFigure 4 c) change angls--> anglesIn appendix, change caption of fig 5a) from MNOST to MNIST, I believe it is a typoAdd results with FA or DFA with fixed and learnable weights, will further support robustness and closeness of BP claim in this work. UPDATE-----------the author response has addressed some concerns (efficiency concerns; unclarities about some ablations studies; reason for not comparing to Edunov for EN-DE) well, and I have slightly increased my rating.I maintain that the description is needlessly obscure in places, even in the revised version (while it is technically correct that a denoising autoencoder could be described as a crossover encoder where the two parent outputs are identical, this is needlessly convoluted), and a major concern about the framing of the paper was not resolved in the response (if back-translation is self-supervised learning, then some of the claims about limitations of self-supervised learning are untrue, and the findings lose novelty. If back-translation isn't self-supervised learning, then it's misleading to imply that self-supervised learning led to a new SOTA).  ----post-update----Hi, I thank the authors to give useful feedback about several concerns. But I am still questioned about the cross-sentence operation, even the authors give an example of the different tasks, which is hard to convince me for such kind of learning can achieve such results. The 5 tasks as the author described are much harder, which somehow indicates that bilingual data is not so necessary for machine translation (but it should be aware, this is not unsupervised machine translation, the training spirit is totally different).  Random data augmentation could be very very strong. The authors are expected to give a code implementation and the trained model to give a more convincing result.  Update:  Thank the authors for the detailed feedback. I decide to keep the score.------ --------------Post response-------------------Stock market is complex and the evaluation with few statistics is unrealistic and too simple. At least the evaluation should be more fine grained than prior work. The goal of interpretability is nice, but not sure the paper gets there by only tuning few parameters.Overall, I like the idea but needs better execution of those ideas. ##### AFTER RESPONSEThank you for answering. My concern on theory is resolved, but I still think an additional baseline of regarding it as a hyper-parameter would be helpful as a comparison, having at least a few trials with mean and standard deviation for the experiments would make the conclusions stronger, and unifying the baselines is important. ----- edit after the authors' answers -----I'm not convinced by the authors' answer on the meaning of the CF-metric when condition 2 is not verified, which is the case on real data. "... but we would expect fairer decisions to have smaller CF-metric in practice" doesn't seem enough to claim that FLAP is better because its CF-metric is lower. In addition we don't have confidence interval to assess if FLAP is really better on MAE.Finally the review of reviewer1 made me realize that CF-metric and FLAP are based on the same function. I partly agree with the authors' answer, i.e., we need a counterfactual function to construct a good metric and then it is optimal to use it as well in the algorithm. However it is not fair to use that metric to claim that the algorithm is fairer. Therefore I lower my rating to 5.  **Post-rebuttal update: the score is increased from 4 to 5. See the response to the authors' comments.** Post-rebuttal feedback-------------------------------I thank the authors for their reply. I still think that this paper has the major problem of presenting results about interference that are not strong enough to be published. In particular, I agree when the author say that "the paper tries to bring conceptual clarity to this important topic, and provide a clear empirical methodology to measure interference and understand correlations to forgetting", but still I think that the overall contribution consists "just" of one (of several other possible ones) intuitive method to measure interference, without a solid motivation. To me, this paper is promising but should present more significant results to have a stronger impact. I think the options are only two: stronger theoretical motivation, larger empirical analysis especially considering deep RL. Between the two, I consider the second option the best. #####EDIT#####I agree that the author's disagreement with my second comment and thank you for the update. I change my rate to 7. %%%%% EDIT %%%%%%I am satisfied with the author's response and given the proposed changes will raise my score to a 7. ########## Edit ##########The authors have addressed all my questions. Thanks. ----- post rebuttal -----The authors addressed most of my concerns and the revision is better than before. I would like to increase my score and would recommend an acceptance. ------ After author's response:* The response of authors identifies the problem of using running loss in projected space. While one can try to get around it by projecting the loss function as well but that would be a convoluted way to solve the problem, and in any case, not a strong criticism of the presented approach. * The updated document has generalized the derivation to take general perturbations into account. * Updates Tables 1-3 resolve empirical analysis concerns of the reviewer. With these improvements, the reviewer is happy to recommend acceptance of the paper.  --------------------------------------------------------------------Edit after rebuttal and discussion.I thank the authors for extra experimentation to showcase the effectiveness of SkipW. While most reviewers here agree that the novelty is limited (that doesn't stop it from being useful), I strongly think the impact due to SkipW will be translated to the real-world. There has been some discussion on the datasets, which I agree are not extensive making the initial experimentation weak. However, the new experiments compensate to an extent and I would like to recommend a weak acceptance with a score of 6 (I am still between 6 and 7, waiting for other reviewers to pitch in).  ============================Update after going through the updated paper and discussions---------------------------------------------The paper adds better illustrations in the revised paper to explain the technique and the approach seems to be effective even though it is simple. Given the approach, 'transductive' in the title looks okay.Figure 1, Figure 3, and Figure 4 in the current version suggest that the one-class SVM/SVDD techniques [1, 2] would be important to compare against. Popular anomaly/outlier detection algorithms Isolation Forest [3], LOF [4] would also be relevant here. The paper can be strengthened with these additional baselines. RETO has at least one short-coming over the outlier detector techniques: it needs to wait some time until it can collect enough test data.[1] Lukas Ruff, Robert A Vandermeulen, Nico Görnitz, Lucas Deecke, Shoaib A Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In ICML, volume 80, pp. 43904399, 2018.[2] Lukas Ruff, Robert A Vandermeulen, et. al., Deep Semi-Supervised Anomaly Detection, ICLR 2020.[3] Liu, Fei Tony; Ting, Kai Ming; Zhou, Zhi-Hua (December 2008). "Isolation-based Anomaly Detection". ACM Transactions on Knowledge Discovery from Data[4] Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; Sander, J. (2000). LOF: Identifying Density-based Local Outliers. Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data. SIGMOD. **Post-rebuttal**The revised version removed questionable or invalid notations/assumptions/justifications, and there are some defences for previous formal justifications in the rebuttal. IMHO, the rebuttal raises further concerns about the technical quality, and the paper still requires stronger justification for acceptance. I'll lower my score instead.Specifically, I find the rebuttal generally confusing, and I disagree with various points in the rebuttal/revised version.- Condition 3.1 doesn't seem right: by definition, a classifier in  has a misclassification probability of at most  on the ID data, but Condition 3.1 states that the misclassification rate is ? Importantly, I don't see why this condition is needed as there is no justification on how it is connected to the disagreement test. In addition, "As a consequence, with very high probability 1  (1  ´)s we cannot fit a set of s random i.i.d. in-distribution points with the wrong label" is quite vauge and doesn't seem right either.- "According to the definition, there exists a class of functions F that is complex enough such that OOD(P, F) is the complement of the support of the training distribution.": I don't think the definition implies this. In addition, if OOD(P, F) is the complement of the training distribution, then ID examples not in the training distribution are included in OOD(P, F).- "If we defined OOD(P_n, F) with P_n the empirical distribution, then the reviewer would be correct and indeed this set could contain ID samples.": in theory, OOD(P, F) CAN contain ID samples, unless additional assumptions are made.- "two-sample test": while I think this is a minor issue to call the test a two-sample test, I still don't think it agrees with the standard usage of the term (which means two random samples are used).- "one may simply pick the time point with the highest validation accuracy, after training for a fixed number of epochs": I doubt that this can be called "early stopping". So I don't think the response addresses my question regarding Fig. 5 (previous Fig. 4). In addition, since "early stopping" is used for regularization, this makes it questionable whether "regularization" is indeed needed.- The test statistic is included in the main paper now, but how about the threshold? In rigorous statistical testing, the threshold can be rigorously calculated, while in this case, it is not clear how the threshold is set. ---UPDATE: After reading the replies, I have updated my score from 5 to 6. AFTER READING AUTHOR RESPONSE-----------------------------------------------------------------------------------------Thanks for the response.Ideally, you would be able to show results for ensembles of decision trees or some model that is very different from neural networks. --- Post Rebuttal ---I've read the author response. However, I do not plan on changing my score as my main concerns have not be addressed. In particular:> the framework does not so much provide a novel formulation of successor features but rather presents a specific instantiation... Thus, the claim that this is a novel formulation of successor features with a non-linear reward seem inaccurate since the original successor feature formulation already handles the case where the reward is a non-linear function of the state.The author response is> There is no guarantee that the learned state features are able to find appropriate values. In theory yes, we agree but practically we found this not to be the case if we explicitly test for it. As our environments do, where the reward is a non-linear function of the state.I understand that perhaps existing methods are not capable of learning good state features, and presenting a method for finding better state features (not the weights) would be interesting. However, the current paper simply hard-codes good features, which I do not find compelling. Feedbacks from authors are constructive to solve my puzzle and I decided to increase my rating.  #### EDIT------------I think the authors replied to some of my concerns in a convincing way, hence I raise the score to 6. Unfortunately, I think the theoretical analysis for the noise-robust loss is orthogonal to their sampling sieving approach. And the analysis for choosing $\beta$ does not dependent on their instance-dependent noise settings. We can get the same $\beta$ by very rough estimation(their approach) in instance-independent noise settings. In addition, I guess $f_x^*[y]=1$ is still problematic in the theoretical analysis since CORES2=\inf given the ideal classifier. Overall, following author's response, I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision. --------------------------------------------------------------------------------------------------------------The authors of the paper addressed carefully the concerns I raised above. As such, I am raising my score to a 6, and would like to recommend accepting this paper.  # Update after discussionThe discussion has reinforced my concerns. I cannot fully follow the logic of the paper and am not convinced the controls are useful. Therefore  I stand by my original assessment. ### After rebuttal update.The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks. I was also convinced by authors responce on paper novelty, technical contribution and (after the re-focusing) potential usefulness to the community.Thus, I am raising my rating to weak accept.### Comments on authors response (as after Nov 24 I cannot post messages, visible to authors)> 1. However, we would be thankful if you might share any prior work (paper or published practice) where the authors automatically searched for the optimal combination of regularizers for deep learning models among a large set of regularizers, as presented in this study.I am surprised with the results of the googling, but have to admit that authors are technically right and I was wrong. While it seems obvious to me, that regularization (specifically, dropout and L2 weight decay) are the hyperparameters of the deep network training, somehow papers and guides online mostly consider mostly architectural things + learning rate + (sometimes) dropout rate as a hyperparameters to optimize. https://arxiv.org/pdf/2006.12703.pdfhttps://nanonets.com/blog/hyperparameter-optimization/Anyway, I lift my objections on novelty.> 3. "Neither software, nor dataset is proposed, the paper uses existing ones." : We engineered a source code that selects the application of 13 regularizers to a neural network, which required extensive programming efforts and several additions to the AutoPytorch library, as mentioned in Section 4.1.OK, I agree.> 4. "It is well known that the regularization/augmentation/ methods need to be tuned ... He et.al. (CVPR2019)" : The suggested reference is a collection of refinements (many of which are actually not regularization techniques), which have been suggested by the deep learning community for maximizing the generalization on Imagenet. That paper only summarizes a collection of some practices, however, it does not present a method that searches for the best combination of a large set of practicesNo, we are discussing different things. I gave the He at.al as an example, that community is well aware about the fact, the regularization and augmentation (and other things) have to be highly tuned. I agree that He at.al and the current paper use completely different methods for solving the problem (manual tinkering vs. auto-search). What I disagree is that the community was not aware about importance of regularization tuning before this paper.>5.  E.g. DropOut was present in only 35% of the dataset cocktails, hence was not selected in the cocktails of the 65% of the datasets.And experiments were using fixed-size network. Of course, is the network is not wide enough for the task, the dropout might not be needed. It is also quite strong statement that "there is no universal regularization", given that L2 weight decay and dropout are widely used in a such different domains as image, text, speech processings, RL and so on. ****I would like to point out the TabNet paper https://arxiv.org/pdf/1908.07442.pdf, which claimed "beating GB methods for the tabular data". I appreciate the fact, that unlike the TabNet, RegCocktails were using a standard deep MLP and not the attention model, yet one needs to add that reference.  === update ===I have read the revised paper, and decided to update the score (see response to authors' comment).I have read the other reviewers comments, the authors responses, and the updated paper. Given that the paper now seems to be focused on tabular datasets, Most of my concerns are addressed, and therefore, Im increasing the score to weak acceptance. I would like the abstract, introduction, and section 4.1 to be further changed to make the focus on tabular dataset more clear. *Updates after reviews and authors feedback: *The updates from the author are appreciated and make the arguments of the paper clearer.After reading the other reviews and discussions, I keep my original score of "6: Marginally above acceptance threshold". **Post-Rebuttal**The authors have addressed some of my concerns during the rebuttal phase. Thanks! For me the major drawback of the current manuscript remains that 'sequential bias' is not really defined, and not really showed to exist in the datasets at hand. Only indirectly, by our new method performs better than existing methods. I think this should be improved, to increase the understanding of this topic. --- Update after discussion ---I thank the authors for considering my recommendations. I think the clarity of the updated paper is much improved, particularly the introduction. I further think that the authors have adequately addressed the concerns raised by the other reviewers and recommend accepting the paper. --------------------------------------------------------------------------------# Review UpdateI thank the authors for their thorough response and the additional experiments. Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). I would have liked to score the paper higher, but at this stage I believe the paper is still not ready to be published. The authors acknowledged in their update that the review process helped them to understand their own work better. As a result, some aspects of their approach have been changed (e.g. removing step(ii), changes to step (iii)). I believe changes to the method go beyond the scope of the discussion phase and instead justify resubmission. This would give the authors some more time to get an in depth understanding of their approach as well.## Author comments on step (ii)I thank the authors for clarifying. In their response, the authors claim that they will use a held-out data set with known density parameters. It is then possible to evaluate which models in the ensemble best estimate these density parameters. I have some issues with this claim:1. This is not made clear in the paper.2. The approach assumes that the density parameters are unknown. Adding this assumption will weaken the paper.3. The selected models will be biased toward the held-out set.## Author comments on step (iii), now step (ii)In their update, the authors change the reweighting scheme. Instead of having a model-based weight, the reweighting is now done solely on a per-sample basis. I believe this looks like the right direction to take.## Ablation studyThe ablation study is important. One possible addition would be to make a comparison for different ensemble sizes. Update: After reading the other reviews/responses, I'm keeping my score of 5, due to pervasive concerns about the narrow focus of the experiments and incremental novelty of the method. **Post-rebuttal update: the score is increased from 7 to 8.** --------------------------------------------------POST AUTHOR RESPONSE1) Thank you for adding Fig. 3. My concern about the attacks being noticeable is resolved. 3 & 4) The scope of this work is still limited to Specifically dynamic routing, which we already know are susceptible to black box attack (the community that already exists on architecture un-aware attacks) and some other attacks. As author's mention it is not trivial to apply their attack to other capsule architectures either. However, I find their various analysis informative. Therefore, I am increasing my score to 6.  ----POST AUTHOR RESPONSE --------1.) Efficiency of our Vote-Attack: It is clear that the vote attack is more time efficient than other attacks, but there is no clear motivation for this improvement. To my knowledge the attack creation time has never been a barrier to adversarial research, nor has it prevented real world adversarial attacks. As a result this focus of the paper simply confuses the reader, be spending time addressing an issue that is not important in research or practical settings.  2.) Optimizing for activations of non-capsule components of a capsule network: In the authors response they discuss the semantic meaning of the votes of capsules. This too is a bit of a red herring. Although when discussing the motivation behind capsules, the potential for semantically meaningful capsule votes is invoked, there is nothing in the training procedure that ensures that the activations of the capsules correspond directly to features that humans would find semantically meaningful. In my original review i mentioned that by not attacking the output of the capsules after the routing procedure, this attack was simply optimizing for representations extracted from a standard neural network. In this way this work is similar to the representation attacks first presented by [1] which showed the success of representation attacks on standard neural networks.  3.) The undetected attacks often resemble the target class, under the class-conditional capsule reconstructions detection:Th addition of the attack visualizations is an improvement but the authors do not specify which attacks are successful and undetected and a few of the visualized attacks do indeed resemble the target class.   4.) The additional defense mechanisms presented in Deflecting Adversarial Attacks:The authors are right to point out the scope of the paper, and it is perhaps unreasonable to expect this paper to address these defence mechanisms, but their inclusion would greatly strengthen the paper. [1] Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J Fleet. Adversarial manipulation of deeprepresentations. In ICLR, 2016. ========== after reading the authors feedback =========Thanks the authors for addressing my concerns and I am convinced that this work is very much different from prior literature. In addition, the evaluation metrics are correct in the studied problem setup. Based on that, I would like to raise my score from 5 to 6. *******************After rebuttal period: thank you for answering my questions and for updating the paper. I still think it is a good paper and would like to keep my score. POST-REBUTTAL:I thank the authors for their response. I am happy with the responses to my concerns/questions, and retain my decision of Accept. ========POST REBUTTAL=========I would like to thank the authors to answer my questions. It addressed most of my concerns. Hence I increase my score to 6. -------- Post-rebuttal edit -------The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines. I believe that the paper is strong and makes a significant technical contribution to the field of CL. Hence, I recommend acceptance and I am updating my score to reflect the same. Edit: After reviewing the author response, I will keep my score as it is. I believe the paper should be accepted.  === Post rebuttal ===1. As reviewer 2, I am also not convinced by the reason why it could not be evaluated on a MOT dataset. The explanation suggests that the proposed method is more general and it should be able to apply it to broader domains (and thus MOT could be one of them). 2. The evaluation metric still looks suspicious to me. I can imagine we can usually get better numbers if we can directly minimize the metrics we want to evaluate. However, in my opinion, data assocision usually need to do hard assginments if we want to use it in practice (unless it is a intermediate task and in that case we can do soft assignments), so I do think it is better to set a hard threshold or draw a curve of F1 v.s. threhold. In the meantime,  I think it is unfair for some other methods, like k-means++ becaue it is minimizing a different objective.I still think the method has some merits so I am at borderline, though I will not defend it if it is rejected. === Post rebuttal ===The clarification and additional experiments are greatly appreciated and answered some of the points which were not entirely clear to me. # updateThe authors have addressed a number of issues and strengthened their submission. ### After rebuttal phase ###Since the authors did not provide a revised version of the paper or addressed my comments in detail, I do not see a reason to change my initial recommendation. I thus recommend to reject the paper. ################################################Post-Rebuttal:I want to thanks the authors for their detailed responses. It addresses my concern of hyperparameter choices. However, after going through the responses and paper again, I decide to maintain my initial assessment. The main reasons are that 1.conceptually C-score is not very novel. The addition of this paper to our existing understanding is not that much. 2.practically the C-score does not improve the-state-of-art of outlier detection (comparison with traditional methods is lacking).I encourage the authors to further explore using C-score on improving outlier detection and comparing with existing traditional methods. Besides, diving a bit deeper into using C-score to analyze the dynamics of different optimizers will be very interesting.  After rebuttal: Authors' responses do not address any of my concerns, and I completely agree with other reviewers regarding lack of clarity, evaluation, and novelty. The current form of the paper is not ready to be published. I decrease my score to reject. -------------------------------------- *Update:* Following the authors' response, I upgraded my rating, but I still think there are critical issues with the paper. The most problematic point, in my opinion, is the only-marginal improvement on the test data, indicating that the suggested training method only improves the specific "failure scenarios", making it is similar to adversarial training methods used to gain adversarial robustness. However, the abstract and introduction indicates that the paper helps in debugging in fixing failures in general, which, I think should have been evident in improved test accuracy. =======================================**Post-rebuttal Comments**===============================I appreciate the revisions and additional results reported by the authors. The authors have addressed the concerns raised by me in the revision. While I agree with R1 that novelty of the method is limited, after considering the reviews collectively, I believe this paper presents very impressive results given the fact that the problem is challenging. Therefore, I would like to improve my final rating to 7. ======================================================I've read the rebuttal and the authors have addressed most of my concerns in the revised paper so I raise my score. EDIT: The authors' rebuttals have solved my concerns partially. However, there still exist some concerns about this paper.1. In the introduction, the authors indicate that nonparametric methods compute the class centroids for novel classes. However, I am not clear whether computing class centroids is the only mechanism to solve class-incremental learning. Besides, I recommend citing more papers about this interpretation.2. The category centers are usually affected by the number of samples. When the samples are scarce, the constructed centers are not accurate, which affects the performance. The authors should give more discussions about category centers.3. For the regression problem, it is better to give more implementation details and ablation analyses. Besides, whether MSE is the only loss to train regression network.Based on these concerns, I update the score to 5. Edit: I am happy with the author responses and have raised the score accordingly to 7. ----------------------After reading the rebuttals-------------------------------Thanks authors for the detailed response to my review.Unfortunately, the responses are not satisfactory for me to increase the score.Therefore, I stand on my initial score (4) as my final score.The below are the reasons for this.1. The authors failed to provide the comparison with other AutoML papers on time-series.2. Imputation/Interpolation is a basic data preprocessing step. Also, the additional complexity for the imputation is marginal (especially compared with AutoML). Therefore, excluding the comparisons with imputation is hard to be accepted.3. There are a bunch of self-supervised learning methods. It is unclear what is the motivation that the authors utilize "contrastive learning" as the self-supervised learning methods. 4. Also, motivations of many decisions in this paper are still missing. I think I am not the only person that raised this problem.5. I asked for "quantitative" analyses on computational complexity. However, I cannot find the "quantitative" analyses in the revised manuscript and rebuttals for the computational complexity. UPDATE AFTER REBUTTAL:The authors have covered most of my concerns about the paper and I think that the paper has been substantially improved. However, my biggest concern was about the experiment results and  I think the paper still lacks on validation comparison with other methods.  ======Post Rebuttal Update======I would like to thank the authors for their rebuttal, which has addressed part of my concerns. After reading the authors' rebuttal and other reviewers' comments, I'm still concerned on the weak baselines and mixed results in this paper. Unfortunately, I will keep my rating.Concrete suggestions to improve this paper in the future:(1) Strongly recommended: use ResNet50 instead of ResNet18 for the small scale experiments, in this way you get directly the numbers from the literature (e.g. BYOL on CIFAR-100);(2) Nice to have: for the expensive ImageNet experiments, it would be nice to get comparable results using the smallest comparable architecture (e.g. ResNet50) from the literature. SimCLR claims that "With 128 TPU v3 cores, it takes <1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs" and MoCo claims that "For IN-1M, we use a mini-batch size of 256 in 8 GPUs, ..., train for 200 epochs ..., taking <53 hours training ResNet-50." ============================== Update after author's response ==============================The additional experiments look good to me, it solves most of my concern, I would like to lift the score to 6. === after rebuttal ===I thank the authors for the feedback. Regarding the proposed theorem, I was hoping to see if the iterate can stay in the flat minima instead of leaving the region eventually, as the constant of the local strong convexity is inside the log's, which is not very sensitive to the value (the landscape) and might be tricky to provide useful guidance to differentiate flat and sharp minima.I decided to maintain my score, but I still recommend a weak accept of this paper. Post-rebuttal: I thanks the authors for their answers to my concerns. After looking the reviews of the other reviewers and the ongoing discussions, I have decided to keep my score. I think this paper makes a novel proposal which deserves to be published.  **post-response update**:some of my concerns about the evaluation were resolved in the author response (for example regarding the meta-validation datasets) , and I have slightly increased my rating. **Post-response update**Thanks authors for extra effort on semi-supervised experiments. I decided to increase the score to 6.  __UPDATE__Thanks for the incredibly detailed response! I've raised my score to a 8.  I do in general quite like the paper, and the responses here are thought-provoking. I'm not sure I'm totally convinced by the WSC results comparing CALM the classifier to T5 the sequence scorer. Not sure if it's an apples to apples comparison...but I'm not sure there's a straightforward setup for this, and perhaps it starts to get beyond the scope of what's being presented here.  After reading other reviews and the authors responses to all of the reviewers, I recommend this paper by acceptedextensive results show that the CALM objectives offer more signal from data than current pretraining methods.This paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model. These objectives certainly improve over the original T5 base and larges models that are used as initializations, and especially outperform the base model in the low-data regime. The authors use objectives which capture both generative and discriminative information, which some have suggested contain mutually beneficial signal but have not been unified in a single training method.Below are two paragraphs from my original review. The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly. Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model. I therefore believe that these results show merit.I still feel that the authors use of concept and commonsense is vague, when their method can be defined more clearly with more mundane terminology. In practice, the authors use nouns and verbs as their concepts, which is fine in terms of pretraining objectives, but surely does not capture the generality of concepts. The authors have somewhat clarified in this in their updated version.Finally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning. One way this can be seen is that the slopes of the T5 and CALM lines are very similar after an initial bump which T5 likely needs to calibrate to the new distribution. This makes claims of learning commonsense hard to verify, though I do agree that something relevant to solving these problems is clearly being learned.Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools? I recommend this paper for acceptance, though I encourage the authors to revise their paper to make this the focus of the story, rather than the vaguely defined notion of concept. ## Edit after rebuttalI have updated my evaluation (from 4 to 6) based on the changes made in the manuscript and the responses by the authors. See details in my response to the authors: https://openreview.net/forum?id=3q5IqUrkcF&noteId=qZzolK7E-wF **Update after author response**I appreciated the authors response to the scalability and raw computation times.  Thank you also for the additional comparison to a non-potential function method.  This will be a good comparison.  My main concerns were answered, and I still think this is a good paper. After rebuttal:The additional experiment results provided in the rebuttal stage suggests the efficiency of the proposed method, as well as the congruent and conjugacy conditions are approximately satisfied. I therefore believe this paper should be accepted. -----Nov. 30th: On a second reading of the authors exchanges with the reviewers as well as of the updated paper, I am lowering my overall score. While I still believe that the *questions* that the paper raise are very worthwhile to the community, I agree with several reviewers that the *answers* provided in the paper are insufficiently supported by a convincing formalization and by experiments.   Post rebuttalI would like to thank the authors for their valuable response. The experimental results seem strong, but technical novelty is still limited. My review score remains the same. I believe this paper can make great impact if submitted to a chemistry journal. ###  ######Update: after reading the other reviews and author response, I decided to increase my grade to 6. Update:Thank the authors for providing the additional results and updating the paper. Since the comparison to one state-of-the-art method has been added (though some other state-of-the-arts are still missing) and the benefit is shown across different settings, I increase the score from 5 to 6.(One suggestion: I would recommend you to highlight the changes in the revision with a different color, so that readers can identify the changes easily.)-------- [Post rebuttal]I've done a complete re-read of the updated manuscript. I am not convinced with the efficacy of the proposed method in solving mode drops beyond what has already been achieved in the literature. In particular, there is no guarantee that matching covariances within the feature space of a NN prevents mode dropping, as the NN (discriminator) itself must abstract away visual information to perform discrimination. The quality of writing has not improved significantly either. I am now more confident with my original assessment.  *Update after reading the authors' rebuttal:The revision of the manuscript was much improved. However, the lack of controlled experiments does not convince me. This paper proposes a new penalty to deal with mode collapse, and the authors claimed that it could be easily added to any existing GAN variants. The authors may need to provide a controlled experiment to show their claim, e.g., what happens if adding their penalty to some GAN variants, fixing the same setting. One good point from the new revision is that BuresGAN can be competitive with the state-of-the-art baselines, with some slight changes in the network architectures. I suggest the authors to test their penalty with other GAN variants to stronger support their claim.-------------------------------------------- ######## Post-Rebuttal Updates:The authors have addressed most of my concerns, and I appreciate their effort. I am not convinced that early-stopping alone can help the model learn to utilize relation information, when the target ground truth does not require the model to do so. However, I recommend accepting this paper as it introduces a new task and presents strong results.  ### Post Author ResponseI thank the authors for their hard work and I'm happy to see that the analysis has improved. Thanks you for the updated manuscript and the answers, I am reasonably satisfied.I am still not fully convinced that the model is novel enough (e.g. simply combines two well known models HNN with MAML ANIL, with the expected conclusions) to grant acceptance, however, since the other reviewers are ok with this, I am happy to increase my score and let the AC make a final decision. ----Post Rebuttal Update:The authors addressed and clarified many of my concerns, therefore I updated my score.  ----Post rebuttal update:I  read the response and commented on it. The authors clarified my questions and updated the paper accordingly. So I think my score of 7 is supported. **Post-rebuttal comments**I read the rebuttal and other reviews. I am still not convinced that there is a big difference between this method and speaker-follower of Fried et al. The speaker model cannot be used for navigation by itself but the learned distribution is used to adjust the actions taken by the follower. The only difference between these two methods is that one uses the entire trajectory and the other one uses partial trajectories up to the current time step (pre-trained on the entire trajectory though). Also, as mentioned by another reviewer, some analysis should be provided about why the generative model works better. Due to these issues, I keep my original rating.  ---Update after rebuttalAfter reading the authors' response and the other reviews, I think the paper has quite clear pros and cons.The experimental results (especially at $\epsilon=8/255$) are strong and the underlying idea of finding a good starting point for single step adversarial training makes sense to me (see reply below).On the other side, the initial (and partially current) explanation relying on randomized smoothing given by the authors is not convincing, in particular when discussing the role of the random step in the success of Fast AT. The new experiments provided in the revision which rather analyze the smoothness of the starting point found by Backward Smoothing look like a much better explanation of the success of the proposed method (note that although the overlap of terminology I don't see a direct interpretation of the smoothness of the loss function at some specifically crafted point in terms of randomized smoothing). This should be more thoroughly analyzed and commented, which would consist in a quite major update of the paper in my opinion.The current version doesn't provide an exhaustive motivation and analysis of the proposed method (in any direction, randomized smoothing or others), although the revision improves in this sense. Then, although I appreciate the good empirical results and I'm still in favor of the proposed method, I have to lower the score. I encourage the authors to clarify the weaknesses of the paper, which might result in a better understanding of what's needed for a successful fast adversarial training. ------**Update after the public discussion:**Thanks a lot to the authors for clarifying many details and providing additional experimental data. In the updated version of the paper, the authors improve the results of the baseline "Fast TRADES (2-step)" and add additionally a stronger baseline of "Fast AT (2-step)" (except on CIFAR-10 with eps=16/255 where it's missing). However, at least for eps=8/255 on CIFAR-10, CIFAR-100, Tiny ImageNet, the authors show consistent improvements over the baselines with comparable computational cost ("Fast TRADES (2-step)" and "Fast AT (2-step)"). This is an interesting result, although it's not clear to me whether it's specific to eps=8/255 or it would generalize also to higher epsilons such as eps=12/255 or eps=16/255. On the other hand, I still think that the current motivation of the method is very incomplete and it is still unclear why the proposed method should work in the first place. Perhaps, it's a good idea to further develop the additional experiments about the curvature of the loss surface at different points in the input space.Then I think there is additional work to be done in terms of understanding what the proposed method actually does (even if we don't take into account how it was motivated -- via randomized smoothing or not). In particular, it's still completely unclear to me why 2 steps of PGD with respect to the original KL divergence (i.e. Fast TRADES (2-step)) works worse than first 1 step with respect to one KL-divergence and then 1 step with respect to another KL-divergence (i.e. Backward Smoothing). This seems quite ad-hoc and requires further explanations, in my opinion. Moreover, I find it also quite puzzling that Backward Smoothing even outperforms 10-step TRADES / AT as shown in Tables 3 and 4 -- not sure about a justification behind this.Taking into account all of this, I update my score from 4/10 to 5/10. I think the paper can still be improved in various ways: both in terms of the motivation and experimental results. ********After Discussion*************I thank the authors for answering my questions and performing additional experiments. Some of my concerned are addressed during the discussion stage. Therefore, I raise my score from 4 to 5. However,  I still hold my opinions that this work does not have a strong motivation, does not help too much for standard adversarial training and has a potential trade-off problem. Post author response:After having carefully read the author's response and additional reviews, I confirm my original recommendation.    I'd like to thank the authors for addressing my comments. I've read through the other reviews and responses, as well as the revised paper. The presented method for learning "true spatiotemporal permutations" is novel, and does indeed seem to learn effective representations.What I'm not entirely sure about is how much this method manages to push the boundary of SSL. Comparing methods with different backbones is indeed tricky, and my intention was definitely not to discourage SSL works from academia. But the burden of proof should be on the new method to perform as close to an apples-to-apples comparison (in terms of backbone) to existing methods as possible. In the end, there are many many potential pretext tasks for SSL of video representations, and I do feel that in order to be publishable at a top-tier venue, they should either enable new tasks, or show clear superiority over existing methods.Regarding temporal action segmentation as a newly enabled task -- I honestly missed this section, since it's in the appendix. This should be moved to the main paper.If I could, I would be borderline on this paper. But since I can't, I'll give the authors the benefit of the doubt, and raise my rating to 6 (marginally above). ======= POST-RESPONSE UPDATE ========I appreciate the authors' response and additional experiments. Unfortunately, my criticism still stands:- **Comparison with standard models.** Based on Appendix C, it is still unclear if the class-wise disparity is a unique property of robust models. As we can see in Figure 7, standard models also have disparate accuracies between classes. More importantly, comparing the **absolute difference** between class accuracies is misleading, since robust models have overall lower robust accuracy. A better comparison would be to measure the **relative error** between classes. While it is hard to draw conclusions by just inspecting the graph, it seems that the discrepancy is significantly milder based on this metric.- **Classifier norm.** The additional experiments still do not demonstrate any causal link between classifier norm and robustness. It is thus still unclear what this metric conveys.- **Temp-PGD.** While I appreciate the effort to provide additional intuition about the attack, I still do not find the method fundamentally new when compared to other attacks optimizing combinations of logits (e.g., Carlini-Wagner, Multi-Targeted).Overall, I still think that the original observation is intriguing, yet requires a deeper and more systematic study. ======= POST-RESPONSE UPDATE ========I appreciate the author's efforts for responding my questions and providing additional results and I do find the empirical observations of class-wise properties interesting. However, I still feel that the contribution of the current form of the paper is not strong enough to reach the bar of ICLR, so I remain my previous rating. Beyond exploratory analysis, the paper would be much stronger if it can go deeper with the observed class-wise properties of robust models.  #### Post-rebuttal Update ####I thank the authors for their detailed response and edits to the paper. However, even after reading the rebuttal some of my original concerns stand:[Novelty] As I mentioned in my original review, the discovery of disparities in class-wise robustness is not new to this paper. In the rebuttal the authors mention that their finding is different from [1] because in [1] the measure of class-wise robustness is distance to the decision boundary with respect to every other class. However, this is in my view, is just an alternative and well-established measure of robustness---i.e., distance to the decision boundary and robust accuracy are fairly correlated and not fundamentally different.[Takeaways] - The authors do not perform sufficient quantitative analysis to justify the link between robustness and weight norm. Moreover, without establishing the causality of this link, it is unclear to me how this observation provides any new insight to understand robust models. - Figure 7 (in the revised manuscript) shows that a similar class-wise disparity is present even in the *standard accuracy* of *standard models*. Thus, although I find the observation of disparities in class-wise robustness interesting, I believe further investigation is needed to understand whether this is just an inherent property of the data distribution that hurts both standard and robust models or is specifically tied to robustness. - The authors' comment about improved robustness based on methods adapted from [3] is misleading. As mentioned in Appendix G (I believe there is no Appendix H), this seems to be the case only for a specific attack. The authors themselves demonstrate a different attack under which the improved robustness of vulnerable classes disappears.[Other comments] There is no evidence to suggest that untargeted attacks will find the *closest* adversarial example within an eps ball. The optimization problem for untargeted attacks is set up to maximize the loss (w.r.t. the ground truth label) and not to find the nearest misclassification. Thus, I still assert that to get a better picture of per-class robustness, the authors need to measure the targeted confusion matrix (or distance to per-class decision boundaries as in [1]).Due to these concerns, I am unable to raise my score. ** The Good NewsI am increasing my score by one, because of the following:Having read through all the conversation, I realize didn't appreciate enough the reproducibility gains from amalgamating existing environments.The added ML hyperparameter tuning experiments (Keras and sklearn) make the work more relevant for ICLR.** Remaining Issues (referencing above)Relevance to ICLR: I still do not think ICLR is the right venue for this work. For instance, not a single previous ICLR paper is cited, the most relevant work all seems to be at GECCO. For example, ABBO is a direct follow-up to work that was published at GECCO.Highlighting new tasks: I appreciate the highlighting of which benchmarks are new. It is hard to know how impactful this is though. For instance, if someone were to publish a new blackbox paper, would they necessarily use these? Of course they're nice to have, but I suspect most people will still just use Nevergrad + MuJoCo.ABBO handcrafted: The arguments provided further reinforce my belief this is useful for industry/practical settings, but not for ICLR.Plots not clear: The arrows do help. However, the results still aren't clear/easy to read. Maybe it would be better to have a big table instead of Fig 2,3,4 and then put those plots in the appendix 2-4x the size they are now.RL Experiments: I suggest changing the language of the RL experiments, the results presented for gradient based are not at all state of the art. "SOTA with grad" is completely wrong. This was pasted from a 2017 paper (indirectly from other pasting). It is possible this could be fixed with communication. For example, I think for a linear policy some of these results would be SOTA specifically for that setting. But then you shouldn't compare against TRPO-nn, because once you open that door you should also compare against TD3/SAC (two very obvious RL algorithms that get ~3x the reward you get on HalfCheetah in significantly fewer timesteps). So it should be phrased as "blackbox optimization of a linear policy for RL" and then remove the gradient stuff, it isn't relevant, unless it was run on the same architecture. Another thing is that this is quite low dimensional. It would be interesting to see how these blackbox algorithms compare for optimizing larger neural network policies, as was included in the OpenAI ES paper (Salimans 2017).Which acquisition function was used for the BO baselines? It seems the comment that it "explores the boundaries first" is highly dependent on the setup. A greedier acquisition function like UCB will probably not do that. I feel the BO comparison is a little weak, in that no recent methods have been compared (for example TuRBO, Eriksson et al NeurIPS 2019). Update after rebuttal period: Thank you for your clarifications. I have revised my score accordingly. As most concerns of mine are addressed by the rebuttal and I would like to rise my score.  ==========================================================================================Updates after rebuttal:Thanks to the authors for the reply. I have read the author response and understand that actually there are activations in the networks but just omitted from the figures. I am increasing my recommendation to 6. ---The new section 2.4 is appreciated, though it seems the paper still does not say that incomplete methods can deal with round-off error by sound overapproximation. =========== Updated Score============Of the three concerns above, the authors satisfactorily addressed the point related to technical novelty, hence I am increasing my score. **POST-DISCUSSION UPDATE**I want to thank the authors for responding to my questions, correcting my misunderstandings, and addressing some of the raised points. Overall, I still believe that the work is not yet ready to be published. One of my main concerns is that a method for context integration should be evaluated in comparison to multiple other methods for context integration (ideally on multiple predictors) in order to see which approach is particularly meaningful and why.  As mentioned in the initial review, the general idea is interesting and worthy of resubmission after the authors address the issues raised in the reviews. Thank the authors for their responses. I read through the responses from the authors and comments from the other reviewers. I would maintain my initial rating: I think this paper will benefit significantly from a major revision, either strengthening the theoretical contributions or improving empirical validations (by actually performing extensive comparisons with existing algorithms that are designed to handle negative transfer problems). =======================After reading all the review comments and rebuttals, I would like to change my score to 4. The paper is interesting, but more detailed analysis and experiments are needed to make the work more clear and convincing. =========================================Thanks for the rebuttal and revision. My first concern has been addressed. However, I still found the proposal lack empirical or theoretical proof, so I am not convinced the contribution is principle enough. I decide to keep my original score. UPDATE:Thank you for clarifying the ethics concern. However, this makes it much more difficult to assess whether I believe your method works as well as you state. After having read the rebuttal and the other reviews, I am more confident that the methodology proposed lacks connection to educational relevance and novelty for publication at this venue. My score stays the same. ---------------after rebuttal----------I've read all reviews and the rebuttal. I think overall this is a good paper and would like to keep my score. I still have some concerns regarding the paper. The lack of baseline comparisons with spatio-temporal data (as also observed by fellow reviewers). My other concern also remains - from the authors' response, it is not clear how once can clearly attribute explainability of the results from their analysis of the model. -----------------------------------Rebuttal: Thank you to the authors for considering the comments and for the changes. I am happy with the response, and have increased the score accordingly. Fig. 5 could be changed similarly to Fig. 1 for visual clarity. The authors mention in the response that both GRU-ODE-Bayes and Rubanova et al. 2019 can be used interchangeably, but there is no reference to the latter.  -----### Post-Response UpdateAfter reading the authors' response and given the changes made in the paper, I increase my rating by one point. ##########################After author feedback:Thanks for the detailed feedback from the authors. Most of my concerns have been addressed and I will keep my scores unchanged. Please add the additional information in the feedback to the final version. Update: The authors addressed part of my concerns. For the factor estimation, the proposed method relies on first order approximations while learning the posterior of the factors; however, the approximation error does not enter into the posterior.  The approximation also raises concerns regarding the convergence of the algorithm. Overall, I think the approach is promising, but some justification of the quality of the approximation is needed. Thus, I tend to keep my rating.############## ##################################################After author feedback:The authors have addressed my comments, though it is impossible to evaluate what the authors promise to do in future work. My evaluation remains unchanged.   ########################### POST REBUTTAL DECISION###########################After reading other reviews and the rebuttal, I have concerns related to the stochastic trajectories mentioned by R2. The fact that the authors confirmed that all semantic actions have the same number of steps makes me question potential overfitting. I would imagine that some actions should take less steps than others based on the objects that are being interacted with, and potential multiple trajectories when performing the same semantic action. I still think the task is interesting, but the setup seems not appropriate to claim a general concept from the current method. Therefore, I have decided to lower my score. ----------------------Update After Rebuttal----------------------------------I appreciate the response from the authors, and the authors' efforts in significantly revising the Method section. The Method section in the current version looks much better and clear. Most points from the authors' response are reasonable, and some of my concerns are indeed cleared, such as my questions regarding the use of $||\hat{\omega}-\omega^*||$ instead of $||f(\hat{\omega})-f(\omega^*)||$. However, in the revised paper, I still find a few places questionable, so I still have a few concerns which are still about the first and fourth points I raised in my original review:- In Definition 3.2 of the revised paper, I find this definition of optimal optimizer $g^*$ not fully convincing. I understand that this definition of $g^*$ naturally gives rise to the $F(\theta^*)$ as in the first line of page 5, however, since our problem is an optimization problem (i.e., to minimize $f$), I think the summation in the definition of $g^*$ should be replaced by minimization. The current definition of using a summation over different iterations could lead to problems in some scenarios. For example, imagine we have optimizer B who quickly converges to a local minimum, and optimizer B who explores the entire search space first (encountering many large $f$ values along the way) and finally converges to the global minimum. Then according to this definition, optimizer A is likely to be defined to be better than optimizer B, which is incorrect. Moreover, another problem with the current definition is that the initialization $\omega^1_g$ is not specified. I feel that for the sake of defining the optimal optimizer, the argmin over all optimizers $g\in\mathcal{G}$ should be based on the same initialization for all optimizers, i.e., $\omega^1_g$ is the same for all $g\in\mathcal{G}$. Since I imagine that for different initializations, the optimal optimizer could be different.- Equation (5) on page 4, I think the distribution of the initial point $p(\omega^1_{\theta^*})$ should appear on the Right Hand Side. Because given the optimizer $\theta^*$, the distribution of the trajectory $\mathcal{D}$ clearly depends on the initialization. This may not be a serious problem since $p(\omega^1_{\theta^*})$ could be factored into the normalization constant of $p(\theta^*|\mathcal{D})$.- Section 3.6, I still find the motivation for using the Meta-Training Set heuristic. It is stated here that the meta-training set is introduced here to improve the robustness and generalizability of solutions, which seems unrelated to the main objective of quantifying optimizer uncertainty. I think (not sure though) a better motivation could be related to uncertainty regarding the function $f$.- Although the objective of the paper is to "further" consider the uncertainty regarding the optimizer, I feel that the introduced method ONLY considers optimizer uncertainty, and hasn't dealt with $p(\omega^*|\mathcal{D},g)$ in a rigorous way. If I understand correctly, samples from $p(\omega^*|\mathcal{D},g)$ are obtained by running optimizer $g$ for multiple random initialization points (line 5 of Section 4), and I find this kind of heuristic.Overall I find this paper very interesting mainly due to the novel perspective of considering this additional source of uncertainty, so although I cannot recommend for acceptance this time, I believe it will be a valuable contribution to the community if the problem formulation can be made more rigorous. =====POST-REBUTTAL COMMENTS========I thank the detailed response from the authors. The authors addressed the novelty of this paper. The experimental results on toy tasks are convincing. However, the way this method increases Delta still seems very problematic to me and not seem robust in complex real world cases.I increased my score. ==========Post-rebuttal comments================Based on other reviews and authors response, I have decided to keep my score. I still feel this paper need more work such as experiments on real world datasets and more comparisons as pointed out in the review.  ## Post-rebuttalI thank the authors for their hard work, and for incorporating my suggestions into the paper. I believe the paper has improved. =======After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I really like the idea, but its important to evaluate the idea with respect to a downstream task to get a better idea on how to use the learned structure.  (I have updated my review to raise my reviewer score by two points after discussion with the authors; this is still a preliminary evaluation as I have not discussed the paper with the other reviewers)-- **Update**Thank you to the authors for the detailed responses and revisions. I have adjusted my score upwards. I think this is a useful exploration of an alternative means of quantifying feature importance, with intriguing results: somehow, optimizing for adversarial robustness also seems to optimize for the score on insertion/deletion games. Further exploration of that issue (or at least, elevating some of the many appendices on that issue to the main text) would, in my opinion, increase the impact of the paper. .**Update**: The authors' update is comprehensive, well-thought out, and demonstrates significant improvements to the paper; I have raised my score to reflect this [ Rebuttal / question responses are acknowledged, and also the other reviews.    I think the algorithm is sufficiently novel, and it does really well on some difficult new problems.    I see the paper as being about a new policy optimization algorithm leveraging differential dynamics, and not about sim2real.   However, the biggest limitation is pointed about by both R3 and R4, i.e., per R3: "With both algorithms, as well as the tasks, being new, its hand to establish the strength and credibility of both using one another. "  And so with this in mind, I am changing my score to a 6, i.e., marginally in favor of accept. ]  Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4. After rebuttal:I appreciate the authors for thoughtful response and additional experimental results, which are helpful for further understanding of the manuscript. Especially, the additional experiment on the recent OOD detection method addresses my concern about the evidence that the previous OOD studies are not sufficient for defending the bit-level corruption.Unfortunately, I am still not sure about the technical novelty of this paper. I agree that the paper proposed a new problem setting, but I do not think that the technical novelty is significant, given the proposed approach of just applying the data augmentation simply at a bit level, rather than at a pixel level.Due to this concern, I want to keep my rating of "4. Ok but not good enough - rejection" as it is. ------Added after the discussion period: Thank you for answering my questions and the lively online interaction. You explained a few things to me, and I could see that you understood my point about VAEs. That was a great outcome. I hope that that you will address the  points that we discussed, where showing an honest link with VAEs will be highly desirable for your future readers.  Update after rebuttal-----Thank you to the authors for addressing my concerns. I have updated my score. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. I understand the authors view that some of my queries fall outside their desired scope for the paper, however I still think the paper could benefit from such contents. ------- after the discussion period ------I would like to thank the authors for the reply, clarification, and additional experiments. Although I do like the perspectives revealed in this work, many points are still quite unclear to me. For example, the theory seems not be able to explain why whitening does not hurt testing when sample size is large, as demonstrated in the paper.  This work also does not draw connection between sample size and generalization error, which may make the claims a bit incredible. I would encourage the authors to work towards this direction and solidify the contribution. UPDATE:I find the main contribution of the work to be the empirical analysis. I personally liked this paper, but I must agree with the other reviewers that improving the theoretical results will strengthen the paper's impact. This would require a major revision not suited for a conference rebuttal, and so, for now, I have downgraded my score to a borderline accept, but I look forward to seeing a future revision of this paper. =========Post-rebuttal=========I thank the authors for their thorough response, which is well argued. Overall, yes, I agree with the authors that there are some differences between their work and prior work. I do not claim that there is zero novelty here. My concern is whether there is enough for this venue, given the high degree of similarity. As the authors do acknowledge that the hypothesis of "catastrophic forgetting leads to GAN oscillations" was introduced in other work, then the primary contribution here is replacing one continual learning method for preventing catastrophic forgetting in GAN discriminators with another, and despite the rebuttal, I still find the proposed solution to be rather ad hoc. I keep my score. ==========I would like to change my rating from 9 to 5. The paper proposes an interesting idea and does achieve good results on several datasets. However,  after reading all the comments and feedbacks, I notice that the comparisons are not convincing enough, and I have some concerns about the performance of the proposed method on more general and challenging tasks. =====Post Rebuttal======I appreciate the authors' responses and the revision of the manuscript. My point about Section 3.2 is not that the correction doesn't make sense, but that the reasoning is not quite convincing from an optimization point of view. I would suggest the authors to simply say what they have replied me instead of trying to link this part to Newton's method.I wonder why the warm up strategy is only applied to the proposed method but not others if that is effective. Isn't it an unfair comparison?I also appreciate the new experiments, but decided to retain the score unchanged. My major concern is that if the additional cost of other solvers is the main issue, then probably it would be better for the authors to directly show the training time as well so that the comparison can be straightforward. --------------**Post rebuttal**I have considered the revised paper, rebuttal and feedback+rebuttal of fellow reviewers and in the end decided to leave my score unchanged. Below is a summary of my reasoning.----The Rebuttal has addressed many concerns and the revised edition has further strengthened the paper in many ways but unfortunately lack in the empirical evaluation. It seems like the warmup of Apollo requires a [start, end] learning rate as well as the increase rate compared to the single learning rate of say SGD. It is not clear that the additional overhead of tuning these parameters could not be used to further improve the training schedule of the baseline for better performance (particularly in the CV). At the provided [[link]](https://github.com/bearpaw/pytorch-classification/blob/master/TRAINING.md) for the CV task it looks like the weight decay for CIFAR-10 with the Resnet-110 architecture is set to $10^{-4}$ (not $5\cdot 10^{-4}$) for which $\eta=0.1$ (SGD+M) was good, meaning that the used $\eta$ is not necessarily optimal for the higher weight decay. The results on the language modeling task are impressive but for the algorithm to be accepted as a particularly good algorithm for RNNs it should compare to the more elaborate SotA algorithms for this particular task (AdaQN was proposed). ----Some points that would be good to address regardless of outcome (no influence on my decision):- Does the algorithm work in the $\beta=0$ setting? SGD with momentum reverts to SGD and Adam reverts to RMSProp which both are competitive optimizers. Does that also hold for Apollo?- How are the values in Table 5 (D.3) calculated? Depending on the implementation of Apollo it looks like 3-4 parameter-sized vectors are required per update ($g$,$m$,$B$,$d$) which in the case of 4 is twice the amount of SGD with momentum, yet the memory is only a few % larger.- In algorithm 2 you should replace $\lambda$ with $\gamma$ to be consistent with the rest of the paper. After the author response:Although the authors conducted additional experiments, they have only partially addressed the first comment regarding the use of LSTM instead of cWGAN-GEP by showing LSTM's comparison against cWGAN-GEP in synthetic data generation, when the question was to measure the forecasting performance, not synthetic data generation performance. Therefore increasing the score to 5. ### Post-rebuttal updateI would like to thank the authors for their additions to the paper. I believe that the extra metrics improve the reader's ability to extract conclusions from the experiments significantly.Having said that, I believe that the extra experiments and numbers do not paint a clearer picture. For instance, in IWSLT and WMT indeed there is fairly consistent evidence that FFN Reservoir is more efficient to train than fully trainable transformers. However, for enwik8 the T Reservoir outperforms everything significantly when looking at figure 15 but judging from figure 13 we see that the best case scenario has been selected for T Reservoir (namely 32 or 48 layers). Even more importantly perhaps, the story is completely different when looking at the test set evaluation where FFN Reservoirs perform better and actually the T Reservoir performs the worst among all methods. Similarly on RoBERTa pretraining, fully trainable transformers seem to achieve the lowest validation perplexity and with the highest efficiency.To summarize, I believe that the reservoir transformers could be a useful tool for improving either the efficiency or the generalization of transformers in low-data regimes or both. However, a distillation of the experiments and conclusions is required in order for this to be shown from the paper. Namely, even with all those numbers I still cannot judge which reservoir layer will be better, in what sense it will be better (accuracy or efficiency) and why is it going to be better.Due to the above, I tend to keep my score but because the additions of the authors provide significantly more information (and in my opinion value) to the paper, I will increase my score to 5. -- EDIT:I would like to thank the authors for the nice work during the review process. I am pretty satisfied with that and I feel serene to increase my rating to acceptance. ######updateI like the experiment added in the revision. However, it is only tested on a IWSLT which is a smaller dataset and can be influence by many hyper-parameters. It's not clear what frozen layers mean for LayerDrop and it need to be clarified with more details. I didn't find clear comparison with stronger baselines on WMT in the revision.As pointed our by the authors, I think the theory mainly come from previous works. I have also read other reviews. Overall, I would like to keep my rating. I thank the authors for their detailed reviews. I have updated my score--- ---Update after the discussionsI appreciate the efforts that the authors make in their responses, some of which address my concerns and improve the quality of the paper.I have raised my rating. However, taking into account all information during the discussion phase, I stick to my original review that this paper still needs to explore more to be a mature publication. For example, if the main contribution comes from the prior encoder, as I said the contribution is limited since the usage of the encoder (or generator in the adversarial cases) in the latent space is widely discussed in previous works, such as vampprior, semi-implicit VI, doubly semi-implicit VI, etc. This also seems to make the contribution of the sliced Wasserstein part incremental. Plus, as this paper has several components, their relations need to be discussed in a more clear way. Thus, more ablation studies are needed to help this paper to present its insight in a much more clear way.Thanks again for the efforts that the authors make and I hope my reviews could help them to polish this paper to be a nice publication.  --- Update ---The authors have addressed several concerns that I had regarding the work.  While this is largely an application of a previous method, they have made some application-specific decisions in order to achieve the significant boost in performance on colorization that they saw.  While the metrics for this task are much improved, there are still some things to be desired on the qualitative results (i.e. the diversity of results tends to be in blocks, as opposed to high within-image color variability).  Nevertheless, I think the improvement from this approach my guide future work in this area.  Given the author's responses and changes made, I have amended my recommendation accordingly.  Thank the authors for addressing reviewers' comments extensively. After rebuttal, I agree with the significance of the proposed method in terms of performance improvement in this particular task. However, the technical novelty is still limited. Thus, I increased my rating to 5. Update: I really appreciate the authors' efforts to address my original concerns. I believe that this work is a nice application of transformers to image colorization. The paper is well-written and the performance of proposed transformer architecture is strong. I think that this work is above the threshold of acceptance. Update: Thanks for the additional ablation studies. I would like to keep my original evaluation which is acceptance.-------------------- After reading the responses and checking the additional experiments, I changed the score for this paper. ## Post-rebuttal responseI have read the other reviews and the authors' responses, and do not wish to change my review. The proposed model seems quite complex with somewhat unclear conceptual motivations, and does not clearly demonstrate impressive performance gains despite the complexity. I would suggest that the authors attempt to change one of these things in a later paper, either by revisiting the model design, or task choice and evaluation (to better motivate the model). ----Update after the discussion stageI appreciate the authors' responses to address my questions in the experiments. However, I agree with the concerns of the other reviewers, especially the redundancy of Theorem 1 raised by Reviewer3. After reading the reviews and the discussion, the authors seem not to provide convincing responses to this part and this raises my concerns for this paper. Thus I change my rating below the borderline. (I updated the score after the discussion.) ----- UpdateThank you for the update and response. Unfortunately, some of my concerns remain. The plots are now run with 10 runs, rather than only 5 runs in Figure 5. But, they look almost identical (in some cases, maybe they are identical?). That is not possible, unless there is a potentially invalid choice in the experimental design. If nothing else, the standard error should change. The theory itself has some utility, since it is shown that learning in embedding space is equivalent. This is not surprising, considering it is assumed that there is a one-to-one mappings, but its good to be thorough. Nonetheless, this could maybe be shown more simply, and I am not sure Lemma 2 is exactly correct.Lemma 1 is overly complex.Alternative proof:Assume pr(a | s) is pi(a | s) (i.e., action probabilities given s are defined under pi).vpi(s) = sum_a pr(a | s) qpi(s,a) = sum_a int_e pr(a, e| s) de qpi(s,a) = sum_a int_e pr(a | e, s) pr(e | s) de qpi(s,a) = sum_a int_e pr(a | e) pr(e | s) de qpi(s,a) (also using Claim 5 like they do) = sum_a int_{f^{-1}(a)} pr(e | s) de qpi(s,a) = sum_a int_{f^{-1}(a)} pr(e | phi(s)) de qpi(s,a)Lemma 2 claims to show that the gradients are equivalent, but instead it seems to show that the functions themselves are equivalent and so should maybe be stated that way. Gradients are just placed in front of everything. Further the last step replacing d_0(s) with d_0(x) seems incorrect, as s is from a discrete space and x from a continuous space. Maybe you are suggesting that d_0 is some kind of delta distribution, but then it might be better to just sum over the same set of s.I am also a bit unsure about any smoothness assumptions required. Is J_0 even differentiable in theta? The requirements on the one-to-one mappings between discrete state to continuous state make for a piecewise flat function that could be problematic for such gradients.I also appreciate that Figure 2 was added. But, it is a bit hard to interpret. More explanation is needed there. =============================UpdateWhile I agree in principle with Reviewer 1 that this paper has jarring flaws in writing and the rebuttal version does not adequately address it, I disagree that the writing warrants such a low score. I have seen worse papers with outrageous claims (e.g. try to claim significance with p=0.1) and I would not give those a 2. I would also disagree with R1 that there is no interesting result in this paper, because there is no prior work I know that even considers how distilled models generalize like their teacher.If I were to grade this paper based on different aspects, the originality and significance would be both 9's, quality a 6 due to experiment issues and careless generalization, and clarity a 3-4 due to unclear motivation in the abstract/early intro and poor differentiation from prior work in terms of experiment design and analysis.That said, the rebuttal did not change my mind that the writing probably will not be improved enough post-rebuttal, I would thus not be able to consider this a top paper despite the interesting observations. ----------------------------------------------------------------------Update after rebuttal----------------------------------------------------------------------Thanks for the feedback.Although there are some minor issues, my main concerns are addressed.  --------------UPDATE: Thanks for the clarification and the revisions to the PDF -- I will keep my score as is. ##########################################################################After the author responseAs the authors address the reviewer's concerns, I changed the rating. ----------------------------------------------------------------------Update after rebuttal----------------------------------------------------------------------Thanks for the feedback from the authors.Unfortunately, although some parts are clarified, the main issues still exist. Overall, the novelty is limited and the motivation is still not clear enough.   **************After Rebuttal:I thank the authors for their extensive answers and clarifications.Overall, I maintain my positive outlook on this work. Although theoretical justification could be improved, I think the experiments do signal that there is something interesting and valuable in this simple approach for characterizing uncertainty. **Post Rebuttal**: The authors did not provide any responses, so I decreased the scores. **AFTER AUTHOR RESPONSE**I have read the other reviewers carefully and the feedback provided by the authors. The reviewers had two major concerns: (i) Theoretical or empirical justification/proof for the following claim (and the motivation) of the paper: the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima. (ii) Lack of comparisons with newer methods from e.g. ECCV2020 etc.For the second, I feel empathy with the authors: In such a rapidly progressing field, it is difficult to integrate comparisons with new methods that are published while writing/submitting a paper. I am sure all of us have experienced similar problems.For the first issue, I disagree with the authors and agree with the other reviewers: This is an important claim that needs to be justified and I disagree with the authors's comment on the current results of the paper being a sufficient empirical evidence for the claim. An ICLR paper should have provided the necessary evidences and justifications. UPDATE - Nov 30-----------------------After looking at the revised version of the manuscript I am still concerned that the claims made in the abstract (and implied in the main text of the paper) about the match of ANNs to the brain are misleading the reader into assigning greater biological significance to the reported result than it actually holds. While the authors made slight modifications in the text and added a few sentences commenting on the issue, these changes did not constitute a change would make the reader "extremely aware that when you say "80% match" you don't mean "80% match to the brain", but "80% match to the score"". I find that a softer claim that would explicitly acknowledge that 5% of "synaptic" updates explain 80% of the predictivity score and not 80% of the match to the brain would make this work more scientifically precise and thus more valuable. I am keeping my original assessment of this paper as being borderline. ==========Edit after author comments:I've read the author comments and the updated version of the paper.Although the authors claim that they have shortened the introduction of the paper by 1 page, this doesn't actually seem to be the case in the last version that was uploaded, where the section titled "Introduction" is almost unchanged compared to the original upload (I've used the diff tool between the latest and original version).Maybe there was a misunderstanding and the authors have shortened a different part of the paper?Although it would have been nice to shorten and streamline the introduction to make the paper easier to read, it's not critical to my rating.The added ablation experiments demonstrate that each of the different attention modules proposed in the paper improve results, which I think really improves the paper. I was originally not sure whether the complexity of the model was justified, but the new experiments demonstrate that each of the components seems to be needed.I've also carefully read the rest of the paper and the author comments explaining details of the tasks under study and now feel that I have a much better understanding of what was done and how the model could be used for other tasks.I agree with R4 that the novelty of the paper might not be groundbreaking, but I believe the paper could be relevant and interesting for other researchers who want to incorporate attention mechanisms into their architectures, so I recommend accepting the paper.I've increased my rating from 6 to 7. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%I have improved my score following the improvements made by the authors. See my reply below for details. ----------------------After author's response--------------------The response addressed most of my concerns and included experiments results of trajectory visualization and run-time comparison. I think the paper would be an interesting contribution to the conference. **Update: I thanks the authors for their answers and revised version and keep my positive rating.** #### Post Rebuttal-UpdateI thank the authors for responding to my concerns. I enjoyed reading the paper and maintain my rating. +++ Updates after author response +++I want to thank the authors for their answers as well as their attempts to improve the manuscript. I have read the other reviewers' comments and the updated version of the paper. The latter improves on the clarity of the approach in terms of the formal presentation of the approach, but a concise problem definition is still missing in my opinion. I feel that the paper still needs to additionally quantify who many labels are saved compared to classic cross-validation, as the models still need to be trained from scratch or fine-tuned. Here, it is important to establish when the method is actually (guaranteed to be) sufficiently concise, such that it can be used in practice. I therefore keep my tendency to reject the current version of the paper. UPDATEFollowing the author's response and updated draft, I've raised my score from a 6 to a 7.UPDATE 2Following discussion among the reviewers and especially a summary of experimental results by Reviewer 3, I'm lowering score back to a 6. -------------------updated comments:I thank the authors to provide responses before the last time of rebuttal. Some of the questions are addressed, but it seems many points still keep disagreement. For example, the contribution is still not clear to me The training cost is increased a lot, this can not be ignored, though I understand in the industry it may be less important. However, for academic research, we should care about this. Besides, it seems that all reviewers agree with the limited contribution and unsatisfied experiments.  ==============post-rebuttal:I have read all the comments from other reviewers and replies from the authors. All the reviewers are leaning to reject this paper due to the limited novelty and unfair and incomplete experimental comparisons. The authors' reply does not address my concerns, so I keep my initial attitude towards this work.   # Update after rebuttal phaseThank you to the authors for engaging with reviewer comments. I think the paper is much clearer now, and the additional results in Figures 5 and 6 indicate that the analysis may be relevant for practical meta-learning settings. I am not sure of the necessity of the new data-generating plot for mixed linear regression in Figure 1 (my uncertainty here was resolved with words); the authors might consider using the space for putting Figure 6 (currently in the appendix) in the main paper, or for additional experiments. Two more notes:  1. It may make the paper easier to read if the appendix were part of the same PDF as the main paper and not in the supplementary material.2. While the experiments are perhaps not difficult to reproduce, code would be helpful to the community.I am increasing my rating to a 6, as I believe the paper presents an interesting result with sufficient evidence. I am not giving a higher rating as I think the paper's impact would increase substantially with experiments on actual data, either in the mixed linear or deep net setting. For future versions of the paper, I encourage the authors to consider adding such results. Update: The authors thoroughly addressed all the questions, the experiments demonstrate an improvement, the theory coincides with experiments. From my perspective, that would be useful for the neural ODE community to know more about the proposed log-signature-based technique. I increase the score.  Post-rebuttal feedback-------------------------------I thank the authors for their reply.> In contrast, our paper focuses on the following question: how can we reduce the number/accuracy of the samples the agent takes during the test phase? (Here, the test phase corresponds to the agents behaviour after the training is completed.)I agree with the authors that intrinsic motivation is different, and perhaps in my review I expressed this concern too strongly. So I thank the authors for their long and informative answer.> We believe that the reviewer refers to the problems where a possibly large multi-dimensional data such as images in games are used as input to the RL algorithm.Exactly. Experiments on high-dimensional problems would make the contribution of this paper stronger, considering the rather limited  theoretical/methodological impact that it has now. I strongly suggest the authors to work in this direction, perhaps on robotic application if possible.After the rebuttal, I still argue for rejection, although I increase my score from 3 to 4.  update after reading authors responseI raised my rating to 6 due to the clarification on assumptions in the paper. I am now more convinced of the relevance of the theories in the paper to practice. ------------UPDATE AFTER AUTHORS RESPONSE: The authors did not address my concerns. The author response and discussion in the paper for the whitening is wrong: this paper use the whitening to mean $\bf{X}\bf{X}^T=\bf{I}_n$ where n is the data size, but the whitening in machine learning means $\bf{X}^T \bf{X}=\bf{I}_d$ where d is input dimension. This is completely different: the former makes the problem trivial whereas the latter does not. The explanation with the mini-batch is also wrong: optimization problem and the optimal solution are defined for the full dataset, and not for the mini-batch. For the memorization, I am not talking about label memorization, but the fact that we can set the weight matrix to have the direction of Y at the first layer, which is done in this paper. In Figure 4, for MNIST, it has only 60% test accuracy. For CIFAR10, it has only 18% test accuracy. This demonstrates my points above. We know closed-form solutions easily for deep neural networks in this paper's setting (as explained above), but this should not work as it is using very strange models so that we can easily have closed-form solutions. Closed-form solutions have no value with 18% test accuracy for simple datasets. It is memorizing the direction of the training data and over-fitting a lot. Linear models work better. ======================Post-rebuttal comments:I want to thank the authors for providing clear answers to my questions and comments. I found the answers satisfactory so I raised my score to 6.  === Post rebuttal ===I would like to thank the authors for the very detailed response and the improvements in the manuscript. I found the ablation experiments and experiment with "No scores" particularly useful. However, I am still a bit confused about better performance of the smaller model. Maybe, more understanding could be gained if there was an additional "tiny" model that would show that when going beyond a certain size, the performance degrade. Additionally, if the overfitting is an important issue, some regularisation methods could be explored. Finally, in the light of many changes in the paper and the original request of all the reviewers to have the writing in the manuscript improved, I think the paper would benefit from another round of reviews. However, I find this method promising and if the paper is not accepted this time, I encourage the authors to re-submit a revised version. *Final Evaluation [Post-Rebuttal]*I am thankful to the authors for their clarification regarding ANOVA decomposition and I am inclined in confirming my initial score. [Rebuttal update]Thank you for your response.This alleviates some of my concerns about Theorem 1, although I feel like I'd need to see a revised version of the entire proof to make sure I understand it.On Figure 3, I'm not sure you've understood my concerns; perhaps I did not explain them clearly enough. Regularized models do no better than chance, and less-regularized models do worse than chance on test points. This is presumably because of what I mention in my review, which is that the synthetic data is basically noise. Thus the "improvement in test accuracy" isn't really an improvement, but rather that the model is no longer free to extremely overfit.On the interpretation of Dropout you provide, this differs somewhat than the message of your paper. I agree more with this interpretation, although not fully. Either way, the paper doesn't really contain strong evidence for that interpretation, which I think would be great to have.I encourage you to rethink the experimental setup somewhat and to have clear experimental support for the proposed intuitions/insights. I think this is a valuable research direction but I think that a more mature paper would have a much higher impact. Update (Nov 30th) In light of the author's responses and the other reviews I increase my score for this paper to 7: Good paper, accept. _[Edit: The authors do not give any substantive feedback to my review, except for clarifying the hash choices. It is surprising that they object so vehemently to my intuitive description of their method as a heuristic to top-k, when they themselves write "Our proposed negative sampling scheme is a proxy to topK-softmax. It selects the top-k classes via LSH [...]". Also my reading of the sampled softmax is directly from their paper, showing a comparable accuracy-time tradeoff, but I was not refuted on this and instead was given other references claiming the inferiority of that method. I have updated my recommendation to reflect these shortcomings.]_ I raise my rating based on the additional experimental results given. ### UpdateThanks to the authors for their clarifications (and to the other reviewers).  I am more comfortable with my accept recommendation now, and have updated my confidence accordingly. =======================================================================Update:I thank the authors for providing additional data, however the additional data is insufficient for me to recommend acceptance.   While the approach is certainly novel, it appears to performs worse in the relevant metrics than other methods while working on less standard networks.  As the networks are so far from standard, it is necessary to see how they (and the method) behave on commonly accepted datasets.Furthermore, AnonReviewer4 pointed out the similarities to AdderNet which I had overlooked.   Given these similarities I expect a more thorough methodological and experimental comparison to the original AdderNet. **After Rebuttal**I thank the authors for their detailed remarks and clarification. While it is encouraging to see that the authors have offered some clarifications and assurances regarding the validity of their approach, my main concern is whether the edit distance between the current work and the proposed modifications is just too high. It appears to me that there simply is too much that the authors need to modify in order to obtain an acceptable manuscript (with the other reviewers' concerns and suggestions as well). I believe that the paper can be significantly improved if the authors incorporate the comments from the current round of reviewing.  ***AFTER AUTHOR RESPONSE***I have read the comments of the other reviewers, which revealed that all reviewers identified the same major issues with the paper (novelty and evaluation). The authors did not provide a rebuttal but kindly thanked the reviewers and stated their intention for improving the paper with the reviewer comments and submitting it for a future venue. Therefore, I changed my overall rating to rejection. ### ---- Update ----The authors' response does not satisfactorily address my concerns. My main concern is that the paper does not properly evaluate alternative choices, even though a large literature on contrastive learning exists.  While in the revision one baseline was added to the appendix, the main experiment still only contains a comparison to Dreamer and ablations. Further, it appears that the proposed method fails completely when the non-markovian part is removed. This is rather concerning since learning markovian latent dynamics is important and also possible with other methods (e.g. PlaNet-RNN). As far as I can tell, the paper does not discuss this issue and does not explain why learning non-markovian dynamics is crucial.  Overall, the paper proposes an interesting method but fails to provide any insight into how the method compares with possible alternatives. I, therefore, maintain my borderline score.  After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I keep my original score. Hope to see a better version of the paper soon. Update: I thank the authors for their response. Some justifications are provided and for that I will change my score. Overall, the paper still needs work. The authors addressed my concerns in the rebuttal. I have raised my score. ====UPDATE===My concerns were partly addressed in author's response so I have raised my score to 5. **Post rebuttal**With consideration of the authors' responses to reviewer questions and revisions to the submitted work I have changed my rating to _clear accept_. ## Post rebuttalI thank the authors for their response and revised version, which has been improved notably with the inclusion of the proof ideas. My previous rating still applies. ## Update after RebuttalThanks for the detailed replies to my questions and comments. I think the paper has been improved substantially and I have increased my rating. Congratulations on the good work! ======After RebuttalThanks for the detailed reply and additional experiments. I increased my score accordingly and I hope the authors could further address following issues:- While the results in C.3 shows default $\rho$ improves over SGD on most experiments (may also add SVHN and Fashion), I can still see its sensitivity to datasets, architecture, noise level and number of accelerators as shown in Table 6, 7, 8 and Fig. 3. For example, 0.05 is not close to optimal with labe noise 20%~60% in Table 8. It is unclear whether $\rho$ is robust to other hyperparameter changes (e.g., weight decay that controls weight scales).  So an ablation study on the sensitivity of $\rho$ and further explanation would be necessary and much valuable for practitioners. - It would be also helpful if the authors can provide more details about how to get the flat minima of Fig .1 (right) when optimizing deep non-residual networks, such as $\rho$ and other hyperparameters.- Minor: Table 8 should be validation errors rather than accuracy. *** Post-Rebuttal ***Some of my concerns are addressed by the authors' rebuttal. The unfair experimental comparison has been fixed. The proposed model indeed has some merits (e.g., parameter sharing and negative sampling). However, to me, the technical novelty of the paper is incremental.  In addition, with additional large transformer networks, the proposed model only achieves limited improvements over previous methods. The authors claim that the proposed model is more effective in handling long videos. But, only results on Charades and KS are shown without extensive comparisons. Thus, I would like to keep my rating unchanged. -----REBUTTALI acknowledge having checked authors' rebuttal and the revised version of the manuscript --After receiving authors' response--I would like to thank the authors for providing additional experimental results and giving clarifications. Since my concerns are almost solved, and I updated my score from 5 to 6. This study would be a great first step to tackle the challenging problem, open-world semi-supervised learning, though there are several remaining issues (e.g., validation is hard to conduct, the number of unknown classes should be known, etc.). I vote for "weak accept." --------------------------------------------------------- Post-rebuttal comments ---------------------------------------------------------------------------I carefully read the rebuttal and other reviewer comments. The author addressed my concerns on pseudo-label quality assessment and comparison against SOTA trackers. From the experimental perspective, I am very convinced the paper did a great job now. Please incorporate these additional experiments into the paper making it more complete. That being said, similar to other reviewers, I am not very convinced about the author's reply on novelty/contribution. It's true it has not been applied in 3D, which is new. However, I am not convinced by the claims in rebuttal, such as "using physics-based dynamics models" (I think you are referring to kinematics-based instead of physics-based), "3D extrapolations" (which could induce potential problems due to the multi-modal future uncertainty), and "self-training" (which is not new). Thus, if the paper gets accepted, I strongly encourage the author to rewrite the introduction and properly reflect the core contributions.Overall I am still on the positive side. But I am fine with both decisions.   After reading other reviews and the rebuttal, I opt for acceptance. Update following answer:Thanks for your detailed answer, which confirms me in my initial assessment.------ .###############################################################################I understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD. I still don't think that technically or conceptually, there is much difference. The authors presented many fresh results during the rebuttal (which might have been better presented just as a table in the manuscript, rather than on this thread). The experiments can form a part of a resubmission of this work, that will incorporate the extensive comments presented by the current reviews **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. Overall, I appreciate the clarifications and added experiments given by the authors. My concerns about the low novelty of the presented algorithm and findings remain, however, as I find the OOD attack to be only a slight modification of existing adversarial attacks.I also appreciate that the defense solution claim has been weakened and moved to the appendix, yet these promises are still left to be validated.Lastly, I find all the many added experiments positive, but these have significantly changed the content of the initial submission at this point, which is somewhat out of scope of the ICLR rebuttal phase (see Q4 in the FAQ of the Reviewer Guide: https://iclr.cc/Conferences/2021/ReviewerGuide).For these reasons, I would keep my recommendation to reject this work for ICLR 2021, but I encourage the authors to further improve and re-submit the now extended work to some future venue.##### Update: Thanks for the authors' response. However, I am not convinced on several points, e.g., (3) - (7). Considering the other reviewers' comments, I think the paper needs to be further improved. Thus, I will keep my score. Updates: Thanks for the authors' response. Some of my queries (1st and 3rd) were clarified. However, unfortunately, I still think more needs to be done to show the superiority of the results. I retain my original decision. ==============================================================================================Thanks for the authors' response. I am still inclined to reject this paper. Compared to the existing ControlVAE, the contributions of this paper looks incremental. More experimental results might be necessary to make the paper more convincing. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft, in particular with respect to the new experiment on the Criteo dataset. Regarding the following arguments:1- *"The dependence of regret on the prior applies to any Bayesian method, in the same way that tuning the learning rate for gradient methods is necessary."* I agree that, for batch methods, the prior can be tuned to the data. Nevertheless, in an online prediction setting, the prior needs to be chosen before observing the data and once you start predicting you can't go back and change the prior for the already predicted sequence. 2- *"To the best of our knowledge, none of the literature that proposed practical (online) Bayesian methods proposed regret bounds or even studied regret for these methods"*Regarding both arguments, there is recent literature that proposes a practical nonparametric online prediction method with regret guarantees for a fixed given prior:*Lhéritier, Alix, and Frederic Cazals. "Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees." Advances in Neural Information Processing Systems. 2019.*Therefore, unfortunately, my first two concerns (theoretical guarantees on the regret and strong dependence on the prior) remain so I retain my original decision. UPDATE: I thank the authors for their detailed feedback and updated paper. I'm a bit more in favor of acceptance of the paper now.  ==========Post rebuttal==========The authors' response does not fully address my concerns. I keep the rating as it is.  __________________________________________________________________________________________________________________________________________**Updated review** following revisions:- A5: For future reference, I believe the kinetics dataset is now downloadable from a google drive folder: https://github.com/activitynet/ActivityNet/issues/28#issuecomment-602838701- A6: I am not convinced by your hypothesis in Appendix (Section E) as to why UCF-101 is so challenging. Surely if UCF-101 is not diverse enough, then a model based on stylegan should perform well on it. In-fact I would argue that it is the opposite, that the UCF-101 is very diverse for such a small dataset.  - Observe that the datasets compared against UCF-101 in Table 7 are not that diverse, there is a dataset of just faces (FaceForensics), of just sky time-lapses and of just dogs. These are all uni-modal datasets. On the other hand, compare just a subset of the 101 classes in UCF-101; Horse Riding, Military Parade, Baseball Pitch, Billiards Shot, Brushing Teeth,...  - I would argue that the limitation of your approach (and that of DVDGAN [Clark et al., 2019]) on this dataset stems from the fact that UCF-101 is a small but very diverse dataset. On average, just over 100 samples per class.Overall, the authors have adequately addressed my questions and concerns. They also appear to have done so for all the other reviewers too.My recommendation is to **accept** this work for publication to ICLR 2021.I recommend that the authors open-source their code and pretrained models.  ------Post Reponse Update-----Thanks to the authors for their updates. I have updated my score by 1 point here. I believe the exposition in the paper could still be improved at this point. In general, this work provides an interesting use of IV techniques for interpreting black box models. ---Comments after the rebuttal:Overall, the attack settings are still questionable: low stealthiness, high poisoning rate, and intervening the training process. - The authors clarified that "Neither our random backdoor technique nor the transferred BaN and c-BaN requires that", however, in 3.1 Threat Model, it is clearly stated "The dynamic backdoor attack is a training time attack, i.e., the adversary is the one who trains the backdoored DNN model. To implement our attack, we assume the adversary controls the training of the target model and has access to the training data following previous works (this is actually not true)", the authors seem misunderstood the different between "controlling the training of the model" to data poisoning.In terms of the difference to static trigger,- The authors argued that they are different to static trigger and "a static trigger cannot be used as a dynamic trigger", which I don't find it is verified somewhere. Actually the random backdoor proposed in this paper proves that a static trigger when applied to a random location still works very well, even as good as BaN and c-BaN. Whether existing triggers can already achieve a dynamic effect or not is unclear, or verified.Without knowing how hard it is to make a static trigger works in a dynamic manner, or where it will completely fail, it is hard to evaluate the contribution and novelty of the proposed attacks. In other words, it seems that a static trigger attached to random locations is good enough to achieve the dynamic purpose, which I don't think is contributive enough.Stealthiness and poisoning rate is another concern, say when you apply the BadNets to random locations, how high the poisoning rate will be to achieve 100% ASR? Higher than 20% or need 100%?Overall, the lack of thorough comparisons of the trigger properties to existing attacks is the major weekness.So my rating will stay the same. Updated review----------Given the unanimous support for acceptance amongst the reviewers, I don't think it is really necessary for me to provide a detailed update. The details of how the authors have addressed the concerns expressed in my initial review can be found in the follow up discussion. I now support acceptance without reservation. 6.  I noticed that there is a recently published NeurIPS paper (Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis), which further reduces the merit of the paper on the perspective of adapting SVR techniques to nonstandard stochastic optimization algorithms such as TDC/Greedy-GQ.  EDIT: After reading the other reviews, responses, and thinking more about the issues raised and resolved, I'm increasing my score to an 8.--- #########Edits after the rebuttal#########I have read the response and other reviewers' review/discussion. I will keep a score as 6. ################after rebuttalAfter reading the responds from the author, I keep my score at 5. ---I have read authors' response and other reviews. Some of my concerns are addressed in the response. Especially the added discussion with related work is helpful. Thus I would increase my rating to 6. EDIT: the authors have clarified that the hardware area results take into account the need to support multiple formats, which addressed my biggest issue with the paper. I have raised my score to a 7 (accept). ---- After reading answers ------I'm still not convinced by the computation of counterfactuals, I still disagree with "flipping" the private attribute, because then the naive solution of removing them from the input of the model would appear perfectly fair. This approach seems to forgot about potential proxies, that are not the private attribute, but that a model can learn on and be biases.Moreover, if I understand correctly, the same computation of counterfactuals is used for the model and the test set. This is unfair as SenSTIR is the only algorithm to use the same kind of counterfactual data than the one used for the evaluation.I think this is a crucial point, I prefer to lower my rating to 5. **Post-rebuttal update:** Thanks to the authors for engaging in the discussion and for the responses. The authors have provided a satisfying argument that with an appropriate choice of hyper parameters, the SQLoss does promote cooperative behaviors whenever all utilities are negative -- this addresses my main concern regarding the validity of the SQLoss objective. I remain skeptical of the value of the visual Coin game, because the results do not disentangle the usefulness of the cooperation objective and the clustering from GameDistill. I (and it seems, R1 and R3) had several concerns about the presentation of the material: mine in particular about lack of clarity, statements provided without motivation, and lack of details in Section 3, both for SQLoss and GameDistill. The authors have provided clarifications to some of these concerns in the response, but the new revision of the manuscript does not seem to reflect any of these changes. I would not be strongly opposed to acceptance, conditional on the visual coin game ablations and clarifications being added in the final version. Nevertheless, it is hard for me to recommend acceptance, given the number of unseen changes that still need to be made to the paper.   ___After reading other reviews, authors' comments, and checking the revised manuscript I decided to slightly improve my rating for two reasons. Firstly, my concerns were answered during discussions, secondly, I do not agree that the concerns raised by other reviewers could justify a rejection. I believe this is an exemplary empirical study presenting novel information and insights about sampling sensitivity of point cloud encoders and point cloud evaluation metrics. --------------------------------------------------------------------------------------------------------------------------------------------------I want to thank the authors for the revision and the rebuttal. Even though the proposed method is promising for meta-learning with noisy gradients, the setup is not backed up with strong arguments/facts. Moreover, I do not fully agree to the statements in the response. For instance, these arguments are not fully observable neither by experiments nor theories:1. " Specifically, the meta-learner is prone to meta-overfit, as there are only a few available samples with sampling noise, even data is clean.". There is no proof that the meta-learner is prone to meta-overfit. The 'meta-overfit' term is not defined well. What I understand is that the learned meta-parameter is prone to overfitting in this context.2. "Due to the small amounts of samples, FSL is more easily affected by data noise, especially considering that human annotators are likely to make mistakes as training meta-learner requires a large number of classes. Besides, we are not the first to propose noisy FSL, [10] propose hybrid attention-based prototypical networks for the noisy few-shot relation classification task, as human annotators are also likely to make mistakes in language tasks.". A common sense is that a few data is more managable and less vulnerable to mislabeling.3. "Our noisy labels experiments aim to verify the robustness of Eigen-Reptile to noisy labels. Denoise is not the focus of our research. At the same time, ISPL is not a denoise algorithm in the traditional sense.". The statement is vague and not aligned to the introduction, abstract, and title of this work.The theoretical part is mostly trivial as mentioned by R2, and the contribution (Algorithm) is hidden in Appendices. I acknowledge the author's response by increasing the score but I think the paper needs to be further revised.I suggest to build strong background of this topic (meta-learning + noise/label noise) and relevant experiments to show the effectiveness of the proposed approach. Furthermore, some theoretical analysis about the limit to the noise can be verified for the future direction. For experiments with noisy gradients, I suggest to compare with the baseline on meta-learning + noise, e.g. the work by Simon et al. [2]. If the focus was noisy labels then it would be better to compare with other methods for noisy labels (not necessarily in the meta-learning setup). --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, it is still felt that this work needs to better clarify some key issues and strengthen both theoretical and experimental study. In light of these, the rating is maintained as follows.  =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Most of my queries were clarified and I raised my rating accordingly. However, unfortunately, I still think a more realistic validation (e.g. on non-toy dataset) would benefit the paper.  Summary after discussion period:----------------------------------------------The authors have done a good job in toning down their claims to match what the evidence supports. After reading the other review's comments and the updated version of the paper, I feel both my comments and most of the reivewer's comments have been addressed, allowing me to recommend this paper for acceptance. ====================================================Post rebuttal:My concerns regarding the experiments are mostly addressed, though as pointed out by other reviewers, more convincing experiments under changing transition dynamics would be very helpful. I also stand by the authors explanation regarding limited resource in running RL experiments, especially for novel research directions.That said, for the same reason (pioneering research vs large-scale application), I do not agree with the authors explanation regarding the limitation of the proposed approach in assuming a known ground truth reward function. The main contribution of this submission is not in solving a specific real-world RL application problem as the cited references. As one of the first efforts in addressing the meta-RL under offline setting, I feel that setting this constraint is a limitation and should be relaxed by means of estimating the reward function. This should be addressed in future work.I raise my rating to a weak acceptance conditioned on the wording regarding "Bayes optimal" being more precisely presented, I think it is currently over-stated throughout the paper, which could be misleading to the community if published as it is.It is important to carefully reword in which sense the proposed algorithm "approximate Bayes-optimal policy", as explained in the authors' response, the algorithm is shown to qualitatively behave in a way that a Bayes optimal policy would do under this particular setup, this is far from sufficient to claim any approximation to the Bayes-optimality in a principled sense. I would like to also point out that while Bayes-optimality is generally intractable, it is possible and not uncommon for a method to start from an explicit pursue of Bayes optimal solution and specify where and how approximations are performed to overcome specific intractibilities, and further show quantitatively that such Bayes optimality is indeed achievable under well-controlled toy examples where the true Bayes optimal solution is known. The concept of Bayes optimality is in essence quantitative, rather than qualitative. UPDATE (after author response): I appreciate the authors' response. The inclusion of the hyperparameters are helpful. I also think it's an improvement that the authors added a comparison to ZSF+bal.(ground truth) to the Adult experiment.I still have a question about the experimental comparison to Hashimoto et al. (called "FWD" in this paper). Is the version of "FWD" implemented in this paper using exactly the same fairness criterion as in the Hashimoto et al. paper? If so, am I correct in saying that the "FWD" comparison in the experiments section does not directly constrain for any of the measured AR ratio, TPR ratio, or TNR ratio? The authors should clarify this in a later version.Overall, I'm willing to raise my score to a 6, but still think the paper is borderline. The paper could still use some improvement in covering related work on the problem of fairness where the protected attributes are not fully known (including the references I suggested). ========I keep the score after reading the rebuttal / author comments. I thank the authors to conduct a lot of additional experiments. Since the authors also agree that "two nearby states can have very different representations", it doesn't make sense to treat nearby states as positive pairs (and use SimCLR) to learn the representation from the beginning, which contradicts the purpose of the entire paper. If the authors cannot show other benefits of this representation learning (other than the performance boost on a 1-2 tasks), then the performance gain could be just due to other reasons that are not known. The additional experiments (e.g., Appendix F) only compares Random with RIDE-SimCLR, and I wonder what's the performance of RIDE? Overall the authors need to rethink the proposed approach in order to give a consistent story of what is a good representation for RL exploration.  ######################################Update after reading other reviews and author response:I have decided to lower my score from 6 to 5, as I agree with Reviewer 3 that more experimental analysis of the method is needed (ablations, sensitivities, etc.) given that the theoretical backing is not convincing. The authors also did not directly answer our questions. =====================Update after author response: I thank the authors for their responses. I broadly agree with the points brought up by the other reviewers, and despite some weaknesses brought up by various reviewers, this paper is a good contribution to the community. I have brought my score from 6 to 7. ===== Post-Discussion Update =====I thank the authors' efforts for responding my questions. Overall, I find the results presented in the paper interesting and worth publishing. It would be nice to extend the results to more general settings. -------------Thank you to the authors for their response. It has helped clear some questions I had in mind. I am keeping my rating. #####################################I have considered the rebuttal as well as other comments in my final recommendation. After discussions:I read the author's response and other reviews. The authors made a considerable effort to address my concerns. As a result, the paper has improved and I increased my score. Having said that, I am still not sure that the paper is ready for publication in its current form. My main concern is still the accessibility/readability of the results in this paper, which I think can be further improved for the benefit of both the community and the authors (more accessible paper => more imapct). To conclude, the results in this paper are interesting and are strong enough to warrant a publication, but the paper can really benefit from another revision.  ----Update:I do agree with the other reviewers that the paper may be difficult to read, especially for non experts. Nevertheless I still think the paper makes a nice contribution, so I will keep my rating. ---Update: The added generalization tests and performance numbers strengthen the paper, and make the aim more clear. The paper seems now mostly focused on accelerating MPS (or potentially other SPH variants). While the new result table shows speedups compared to MPS for model inference, in practice the model still is bottlenecked by NN search, and total runtime of inference+NN is not significantly different to MPS. And if speed is the aim, the appropriate baseline comparisons should be to methods which -- as this method-- use domain knowledge and solver internals to speed up the solver (e.g. Ladicky et al.).On the method side, it does seems like the proposed approach is limited by supervising specific MPS subcomponent separately, which hinders learning richer intermediate representations, applying the model to other systems, or taking significantly larger timesteps. There could be potential benefits of the specific architecture chosen, but their effect is a bit unclear; as for the comparisons shown the most significant effect is supervision on subcomponents.I will therefore keep my score. ----post-rebuttal update I thank the authors for the responses. While I still think the idea is potentially interesting and original, I could not increase the score given the fact that this manuscript is naturally incremental without theoretical justifications. Post Rebuttal:Thanks to the authors for responding. I'm still not sure if the experiments are particularly compelling. There appear to be differences amongst the baselines with regards to class balancing, and the motivating section is still weak; there are new experiments on a larger dataset, but now with a different loss (simplified EL) which is close enough to the proposed loss that this does not work very well as a motivation anymore. Apart from this, taking some of the comments from the other reviewers and the authors' responses into account, I am retaining my initial rating. --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows.  The authors have tried to answer all issues and questions raised in the reviews. At least, I can say so for the questions and issues raised by me. I therefore tend to keep my positive opinion on this paper.  After author response:See reply comment in the thread below for further score justification.Thank you for the work you have done in running the additional experiments and revising the manuscript.Two major concerns, and why I do not think a direct comparison across all things in Table 1 is warranted still:This is to do with point 1. (ii) of my initial review. The bigger handicap here is the fact that planning/search algorithms are being compared to RL algorithms. These have fundamentally different assumptions, assuming that the simulator is resettable at will effectively to anything but the starting state is different problem than what the RL agents are trying to solve. There's nothing wrong with making the kinds of assumptions that the authors have made (deterministic simulator/hard constraint/etc) but comparing it to those that do not in a single table is a bit of an apples to oranges comparison.The KG-A2C with hard constraint is not a very good baseline given that it was never designed to exploit the hard constraint in the first place. It was designed to account for the additional difficulty in having the soft constraint by filtering actions via the graph - once this is done by the hard constraint there is little separating KG-A2C from vanilla A2C (the original paper indicated that the KG input representation portion did not account for any increase in final score) and in fact some games received an increase in score when the KG/soft constraint combination was relaxed. Once again, there is nothing inherently wrong with any of these methods - just that it becomes an issue when you attempt to compare to it side by side.This being said, the handicap clarifications added in table 1 do help, though the prose still compares them all to each other directly. PUCT-RL and MC-LAVE-RL are the only two directly comparable things there. In appreciation of the authors efforts during the rebuttal phase, I will raise my score though overall still do not think this paper is ready for acceptance. I have read the author rebuttal and appreciate the changes made to clarify the distinctions and handicaps used by each of the algorithms. Additionally the exploration into ranges of the delta parameter was appreciated. =-= comments after author discussionThe authors were quite active in editing the submission, and addressing the concerns I had. I still find the paper a bit hard to follow, but none of my original concerns remain. --------------Thanks for the response and I've increased my score. I am satisfied with the response but still not convinced about the algorithmic novelty on the intention semantics built into the method, even after reading B.1.  In particular, it seems that the loss functions do not drive mu's represented by NNs to the fixed point solution of Eq (3); psi shows up in Eq (3) but does not play a role in the following development of the method.  ------------------------------------------------------------------------------------------I have read the authors rebuttal. The authors have addressed most of my concerns. The JL random projection baseline has been added and the heuristic method of reusing the eigenspace for some iterations has been explored, which I appreciate a lot. From the current experiments, the proposed method seems effective, though it will be more convincing if the method can be tested on larger models or harder datasets. I think the topic of the paper is quite interesting and the idea of bounding $M_t - \Sigma_t$ uniformly is also interesting. The theory indeed manifests the effectiveness of the proposed method. As a result, I increase my point to 7. -----After reading the author response and other reviews, most of my concerns have been addressed. I would like to increase my score to 6. It would also be interesting to compare the performance of DP-SGD and PDP-SGD when $\epsilon$ is relatively large as suggested by Figure 7. Post-discussion update: I thank the authors for their participation in the discussion. Unfortunately I find it mostly has not cleared the numerous question marks I have regarding the paper. I recommend putting more effort into clarifying the mathematical derivations and into positioning the paper correctly w.r.t. prior work on the topic. ------------Post Rebuttal----------------I thank the authors for their response. I disagree about epistemic and aleatoric uncertainty. Bootstrapped DQN (Osband et al. 2016), Randomized prior functions (Osband et al. 2019), and several other works show that to get the variance of different possible Q-functions given the data, $p(Q|D)$ or differnet possible MDPs given the data, $p(M|D)$, you need backups consistent in time, i.e. the same dropout mask is to be used for both the main Q-network and the target Q-network for the backup. This is the uncertainty that we trypically need in offline RL and is also used in Theorem A.2 (for the high probability bound which is given the data), and that's why ensembles with Q-functions consistent with themelves are typically used. By merging the target values across different dropout masks, the uncertainty is not timewise consistent. While the paper that the authors point does actually do dropout masks with Q-function at each step, it is discussed in later works including Osband et al 2016, 2018 that not being consistent over time is leading to wide uncertainty estimates.  I would recommend the authors read the discussion on Posterior sampling for RL vs Optimism and Thmpson sampling for a discussion on this.I am a bit disappointed with the rebuttal. I expected a comparison to such metods that perform timewise consistent uncertainty estimation and also to distributional RL, since the algorithm that the authors use can also be drawn similar to a set of particles of Q-functions and performing a backup using target values computed using all of the possible particles, which is essentially what, for instance, QR-DQN does in a way or even IQN does in a way. Even REM would have been fine. Without this comparison, I unfortunately cannot increase my score and I am going to retain my score. -------------------------------------------------**After Author Response**: I really appreciate the author's efforts over the course of the rebuttal period for rigorously testing their method with several new baselines in such a short period of time. For AIA attacks, the baseline numbers provided in the rebuttal are helpful but raise concerns about whether the proposed AIA attacks are working. I find it hard to believe that victim models have less private information than extracted models in 2 out of 3 datasets, and I suspect some other factors are contributing to this counterintuitive trend (like you said, maybe dark knowledge). I will stick with my stance that the AIA setting is broken since you are inferring private attributes using information from an identically distributed D_a (I think model inversion is a more valid setting to measure leakage).For adversarial attack baselines, I agree with your argument that conducting black-box attacks directly on the victim models may need minimal difference queries which can be detected on the API side. However, you are going to need several orders of magnitude more queries to do extraction in the first place (which may or may not be easy to detect). I still encourage you to run this baseline in the next version of the paper, instead of only doing black-box attacks on extracted models. These minimal difference checks may not be in place, and directly doing black-box attacks on the victim model are much easier than extracting and then constructing adversarial examples. It is good to know what additional benefit you get by doing model extraction.Overall, I have decided to raise my score to 6 (more like ~5.5-6). This is conditional on the authors performing much more rigorous hypothesis-driven testing in the next version of the paper (just like they did in the rebuttal) to really validate the hypothesis "extracting models make APIs more vulnerable to adversarial attacks". **UPDATE AFTER THE REBUTTAL:** The new material in the paper clarifies things quite a bit, especially the intuitive explanations appearing below Equation 2 and at the bottom of page 4. Thank you for adding that, I have changed my score accordingly :) POST REBUTTAL UPDATE: I am increasing my confidence in this paper from 2 to 3 - I still believe the paper can use some more clarity but enough points have been explained and updated in the draft for me to feel more confident in my evaluation.  I think the ideas in this paper are quite interesting - for this reason I continue to recommend acceptance. .### Post Author Response Period UpdateMost of my concerns have been addressed by the additional experiments and updated language in the latest revision. I believe the techniques and analysis presented here for assessing reuse could be an important step between observations and explanations for the failure of NNs to generalize systematically. I have raised my score accordingly ---------------Update:I thank the authors to give responses to my points, especially the discussion about novelty. But I still feel the success of KNN for NMT is similar for LM, that's why a lot of works study on NMT are also work on LM. Since this KNN method only targets at the decoder side, same as LM model. Therefore, I still feel not novel enough.  --------------------------------------------------Post rebuttal:I have read the authors response and appreciate the effort made to improve the paper. But I think the results still need additional work, especially from the theoretical front. So my original score is unchanged. **Post Rebuttal Update** The authors address many of the concerns, 1) [a] is properly acknowledged in the revised version and novelty is not claimed on additional label noise in the text, 2) while quantitative studies are still absent for claims on sharp vs. flat minima, qualitative results are provided for convergence to "flatter" minima  3) connections between smooth functions and generalization is discussed 4) answers and updates regarding complexity and convergence are *somewhat* convincing. Thus, I am willing to increase the score from 6 to 7 and confidence from 3 to 4 as I believe the paper provides relevant and interesting theoretical arguments. ################################################Post Rebuttal Update: the authors have well addressed my concerns, in particular (1) the additional visualization gives a good qualitative empirical evidence supporting the claim that SLN helps escaping sharp minima. (2) the search process for the hyperparameter $\sigma$ is very reasonable and makes the usage of SLN practical. I will keep my initial assessment and vote for accepting this paper. ### UPDATEThe presentation of the paper is now convincing, with the background, contributions and concepts clearly stated. I hence increased my score. However, I agree with reviewer 2 that the library is not properly tested (I understand it can be hard to test the whole computation, but unit tests could be easily provided) and that it would have a higher impact if it were more modular, so that a user could easily add the loss analysis directly in her workflow.Moreover, I also think that the paper should be rejected given that the first submission wasn't anonymized and the paper wasn't in a state of being submitted. EDIT: Thanks to the authors' for their clear responses, I'm happy to raise my score to a 7. EDIT: The response is very helpful and it addresses my concerns. I raised my score to 7 after reading the response. ---**After rebuttal**The authors' responses do not address my major concerns (the first two). I do not think the responses directly answer my questions.So, I keep my score unchanged.    ====Post-rebuttal: I am happy to accept this paper after seeing my concerns are mostly addressed. Post Rebuttal CommentI thank the authors for sincerely replying to my review comments. The authors' answers were reasonable to me. -------------------- UPDATE Nov 30 -----------------------------------------------------------------------------------I find the additional experimental work that was carried out for the revised version of the manuscript to be a step in the right direction which has provided more confidence in the claim of the paper.The presentation of results is a bit hard to follow... it's a bit hard to put finger on what is exactly the issue. One thing I would suggest is making the names of the methods a bit more telling, deeper into the paper it becomes hard to track which abbreviation stands for which method. It also messy when some methods are reffed to by a particular RL algorithm name "QD-TD3" some by just mentioning RL "CEM-RL" some just as "SAC" and this naming convention breaks when evolutionary methods are mentioned. Maybe for someone who works with these methods a lot it is easy to keep track, but not for a reader not directly involved with the QD field.Why in Table 2 the third column was changed from "Step to -5" to "Steps to -10"? How this number was picked?Figure 3, which is the main evidence for the main claim of the paper only appear in Discussion, leaving an impression that this figure is not that important and is a bit of a side-note.I sympathize with the lack of computational resources, which makes it very hard to compete, but if a 1-to-1 comparison with competitors if at all feasible, it would be worth it. If your methods is as strong as it seems it is, then it will beat the competition across the board in term of sample complexity and send a clean and powerful message that utilizing RL with QD is the way to go.At this point it might be that the main issue with this work not receiving higher scores lies not in the idea or experimental work, but in presentation. Try taking a couple of you colleagues who are not familiar with their work and observe how they read it, notice the moment when they start loosing focus. Your text jumps from one message to another making the overall narrative not as streamlined as it could. This might be reason for reviews like R3's, where, it seems, the reader gets lost and comes out without clear understanding of the outcomes of your experiments and how these support your claims.To summarize I still find the idea clever and with the new evidence I am more confident that the claim of the paper might hold in general. Since experimental evidence was my main concern and now there is more of it, I am upping my score from 4 to 6 - "Marginally above acceptance threshold". ########################### Post Rebuttal ##################I have read the other reviews and the author's responses. I thank the authors for conducting the additional experiments and integrating the feedback from the reviews. Accordingly, I am raising my score. Overall, I agree with the authors that combining QD with pg operators is novel - however, I am still not fully convinced that it is significant enough for a full paper at ICLR. This remains my primary reservation that prevented a higher score for the paper from my end. -----------------------------------UPDATE: The authors did a very good job at answering my questions and the new experimental results are very much welcomed, hence I'm updating my score from 4 to 6.Given that this is a highly empirical paper with relatively little novelty in the key idea, more comparisons would be necessary to justify increasing the score further. While I sympathize with the lack of computational resources and access to implementations, taking some extra time to implement and run those comparisons can be done. The code for RL methods with exploration bonuses (e.g. pseudocounts, RND) is accessible and these methods are not too costly to run. Methods like PBT (whose results are typically reported using large computational resources), could be implemented and compared in a regime with much more constrained resources. Updated review: Thank you for the clarification. The previous version was indeed confusing to me. I have raised my score although I think some points still need to be addressed in the revision following my previous comments as they were not fully addressed nor in the response neither in the revision:The concern with respect to previous works is not only regarding Parseval networks. There are other more recent works that use orthogonality constraints on the network. Such examples include https://ieeexplore.ieee.org/document/8877742 https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_All_You_Need_CVPR_2017_paper.pdf https://proceedings.neurips.cc/paper/2018/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.htmlAll these works show a similar observation to the one claimed in the paper that by using orthogonality (or frame-like) operators one may train a network without skip connections and get similar results. Indeed, in the paper, more observations are being made that are different than what is presented in these works but a more proper comparison should be made.This is the work the authors should look at by Mallat https://arxiv.org/pdf/1809.06367.pdf They get similar performance to ResNet with a scattering transform-based network. Indeed, also here it is not exactly the same network that the authors here are using but there are remarkable similarities and these should be well addressed. ----------------------After the rebuttal: I'd like to thank the authors for their answers, particularly for resolving the confusion about the term "contraction". I believe this is a good paper and stick to my rating of recommending its acceptance.  ----------------Following the authors' response,  we have updated our rating accordingly considering the following facts: the authors didn't make any change to the paper within the rebuttal, while they had the possibility in response to several questions. They didn't address even our simplest concerns, about the title being inappropriate. They still want to keep the title too general, while the paper considers only graph neural tangent kernel. Even if the latter is equivalent to infinitely wide multi-layer GNN, it is still a very special case. Moreover, many of the major issues raised by the other reviews were not addressed. #### **Post Rebuttal**I'm afraid the authors failed to answer my main question regarding the results and the applicability of their proposed approximation.Therefore, I decide to keep my score.--- ##### Updated reviewer #####The authors have addressed my main concerns satisfactorily, in a clear and concise matter. I have updated my recommendation to reflect this. Edit: I have updated my score based on the new experiments added during the rebuttal process.  After rebuttal:I am satisfied with the reply of the authors and I have raised my score to 7. Post-discussion:I thank the authors for responding swiftly to all of my main concerns and for providing additional experimental results. I'm happy that authors have promised standard deviations and comparisons to ResFlow in the newly updated experiments for the camera-ready version, and I have adjusted my rating accordingly. I think this paper makes a very solid contribution to the normalizing flows literature. ---Update after rebuttalI thank the authors for the response. After reading it, the revised version and the other reviews, the concerns expressed in the initial review are still valid.Then, I keep the initial score. Post-rebuttal:I am mostly satisfied with the authors' response. After reading other reviewers' comments, I shared a similar concern on the marginal contribution. However, the newly added black-box result is a good addition to the paper. Thus, I keep my original rating toward the positive side.  ------Updates after response------I thank the authors for the detailed response and the revision. I am still not completely convinced regarding the suitability for ICLR and have similar concerns to reviewer 2, but am not opposed to acceptance.  In light of this, I have increased my score to 6. ------------- Update after reading authors response -------------I thank the authors for their detailed responses, they have answered most of my concerns and I raise my score to 5. I am still not convinced about the method covering both the aleatoric and epistemic uncertainties, without any theoretical or intuitive justification, and without any discussion/clarification on that part. If indeed this is the case, then additional experiments should be included, for example for a regression task, the standard UCI datasets [1].[1] Hernandez-Lobato, J M and Adams, R P. Probabilistic ´backpropagation for scalable learning of bayesian neural networks. In ICML-15, 2015. Update after revision------------------------------I thank the authors for their work on this paper. The second reading was more pleasant. I agree with the authors that performing a user-study is an important effort, that should be encouraged. I however still believe that, if not benefitial to the user, the complexity of the method can be a drawback. I also wished that more comparisons, but especially other data modalities were investigated. I have updated my rating to reflect the improvement in the text. --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------The authors have done a reasonable job at addressing my concerns and I have increased my score from 5 to 6.  [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. I do believe most of my concerns have been addressed. However, the concern on some possibly confusing technical details remains. The authors are expected to further revise their paper to make it more self-explained. Thanks for the feedback. However, I think most of the performance gain came from behavior regularization, which is an already existing technique, rather than the proposed metric learning method (Figure3 and Figure 5). Ill keep my ratings unchanged.============ ***************************************************************************POST REBUTTAL UPDATEThe authors provided a detailed rebuttal which addressed almost all my concerns and answered most of my questions. I wil therefore update my rating from 6 (marginally above acceptance threshold) to 7 (good paper, accept). I believe this paper should be accepted. #####################################After rebuttal: (from 5 to 6)Thanks for the clarifying the points in your response. I am happy to change my score to Accept the paper as most my concerns are addressed and I believe the paper is a good fit in this conference. A comment to the authors that didn't impact my score but raised some concerns:My concern is about mentioning the first place in VQA challenge 2020, both in paper and also in rebuttal comments. The review process is double blind and pointing out to other contributions that are public and reviewers may have already known about the winner teams, may not be fair. I know that papers can be online on arXiv but pointing to another venue as part of the contributions, may reveal the identity of the authors explicitly (if reviewers already know about the challenge) **Additional comment after rebuttal**Thanks to to the sufficient answers and results, the first rating is maintained. --------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:Thanks to the authors for providing relevant details and fellow reviewers for nice discussions. Original rating is maintained.  **Update after authors' response**I am very happy to see the additional results on CIFAR-10, and the layer-wise ablations and other control experiments. To me, these results have shed a lot of light onto how the observed hashing effect can be explained. These explanations mostly confirm intuitions. Nonetheless I think it's worth reporting the empirical verification of these intuitions that tie in with earlier results reported in other papers. To me, the most interesting aspect of the work is how the hashing properties change over training (and across layers). The paper could still be improved by experiments on e.g. CIFAR-100, ImageNet and non-vision tasks, as well as more mathematically sophisticated definitions of some of the measures (e.g. average stochastic activation diameter). I personally think the results are now sufficient (and sufficiently backed up) for a publication and most of the criticism raised by the other reviewers has been addressed sufficiently (for me). I would now rate the paper as a 6.5 - but to facilitate the reviewer's discussion I will take a clear stance and have thus raised my score to 7.--- **Update after rebuttal:** I thank the authors for their detailed responses and the additional experiments. The responses addressed most of my concerns. I noticed that I had the wrong notion of redundancy ratio in my mind (I'm glad the authors now give a more formal definition of this concept as I think this would trip up many other readers). I'm also glad that the authors have clarified the difference between their results and those reported in Hanin and Rolnick (ICML, 2019). Given these, I'm happy to increase my score to a weak accept (a weak accept, because I'm still not quite sure about the significance of the results reported in this paper). -------------------------------------------------------------- "In addition, I am not sure the description would be enough to reproduce and no code seems to provide." In the beginning, I did not find the codes related to this paper, but later, the author(s) uploaded codes. Thanks. For the codes, if setting random seed program-wide in codes maybe it will be more helpful for reproducing. [Update after authors' reply]In light of the authors' reply, I have updated my review to favor acceptance. I appreciate the additional experiments. It will be up to the readers to determine how to interpret those additional results.--- * I update my score. UPDATE POST-REBUTTALMany of my questions have been answered, though I do think reviewers should explicitly note Ben-Zwi and Ronen's paper, and change their use of "incentive compatible". I think a deeper and more systematic analysis of the A matrix is also warranted, but I do now feel the paper has better scientific merit. Edit: I have read the author's response. They have responded to the questions I had, and they address many of the concerns I had. I am now raising Edit: updating score, and recommending acceptance as per my response to the author's rebuttal. =====================================================================================================The authors addressed my concerns in the discussion period and I therefore created my score to 6. ========post rebuttal review=========After reading the response from the authors and the new version of this paper, I decided to increase my score to 6. =====================Post-rebuttal comments:Thank the authors for the detailed responses and revised submission. My concerns have been adequately addressed and I raised my score to 7. Update after rebuttal:Thanks for the response! I think it resolves my concerns on novelty and evaluation. Hence I raise my score to 6. (Edit: the authors have clarified) ===============================================================================================Update: Since the authors did not give any feedback for the reviews, I retain my original decision. Thank you. ------------------------------------Update: the major concerns above have been addressed in the appendix of the updated manuscript. I'm moving my initial rating of 6 to 7. Post-Rebuttal----------I thank the authors for their response. Both of the sections are now more clear, although the authors should make an effort to polish the narrative of the paper and the clarity of exposition throughout. The discussion of epistemic versus aleatoric uncertainty in the appendix is also interesting. I have increased my score from 6 to 7.  Updates:Thanks to the authors for addressing my concerns and responding to my questions. The newly added experimental results make the paper stronger and addressed many of my concerns. I recommend this paper to be accepted. 3-It would be great if the author can what would be the advantages/ weaknesses of their approach with the references. As the proposed approach has high similarity with the previous works, the minimum requirement is reporting more experiments and compare their method with exist methods. This extra study would present how their approach affect the performance.After rebuttal:Dear Authors, Thanks for providing more details. I believe more discussion and experiments are required to present the difference of you method. As you have mentioned, one difference is in considering summation rather than maximization, so it would be required to know what would be the advantages/ weaknesses of this difference. How does it make any impact on the performance? I would increase my score considering the closed-form solution as a nice contribution and requiring more experiments and analysis on the discussed references.Thanks! ==============After rebutall:I thank the authors and appreciate addressing all my concerns. I'm glad that the additional experiments with smaller mini-batches and smaller training set size provide more supporting evidence for the method. I encourage the authors to point to these results in the main body.One more minor suggestion is regarding the sentence in Remark 3. The way I read the sentence the "equivalence of minimizing CE loss and MLE loss" is the reason we can remove the KL divergence term. But the authors' response seems to say it is for the 3rd part of the sentence. The authors' might want to rewrite that sentence to make it clear. **POST-DISCUSSION UPDATE**I believe the authors to have addressed some of my concerns and I appreciate the additional experiments demonstrating that even the close results were more than a mere statistical artifact. Some other points remain still open such as the limited focus on Trajectron in evaluations.  In summary, I believe that the paper has now surpassed the acceptance threshold and am happy to recommend its publication. UPDATE:I do not believe that Merity 18's results are not reproducible. Just look at the vast amount of work after the publication that builds on top of the results. I don't doubt that your way of evaluating the performance of language models can lead to hypothesis testing, but it is not what the field employs thus your results are not comparable to others, which is why I cannot accept this paper. If your method really does work as well as you argue it should be no issue obtaining an improvement over the actual baseline.UPDATE 2:The new results with Merity's original benchmark leads me to increase my score from 4 to 7. I appreciate the effort in reproducing Merity's results. $\textbf{Final Rating}$Based on the authors' responses during the rebuttal period, I don't believe that the paper makes a sufficient contribution for ICLR. Hence I will stick to my original score. Thanks for updating the paper in light of my earlier comments.After a long discussion with other reviewers and ACs, we concluded that the paper would require another complete review process in view of newly added proofs, which were unfortunately missing in the first round. My lowered score signals this to the ACs.I would like to highlight that I became very upset to find out that many important details were skipped and left as exercise to readers. While this would make sense for some repetitive details within a paper, it may not apply to non-trivial proof details, such as the generalization from 2 to arbitrary S or similar. UPDATE: I would like to thank the authors for their rebuttal. I have read it, however unfortunately, I am not convinced the indicated differences from previous work is sufficient to warrant publication at ICLR. I am also not completely clear about the equivariance point.  --Update after rebuttal--The reviewer thank the authors for the response. Most of the core issues have been addressed and scores have been updated accordingly. ## Post Rebuttal I thank the authors for their response. I have two responses: 1) I still don't find the answer to my Q1 convincing; in particular, the 'filtering effect of distillation' mechanism requires more rigorous discussion, and 2) with regards to the ICCV2019 paper, I think the authors may have misinterpreted my point; my point here is that one could design backdoor attacks that do not affect the attention maps substantially and was wondering if the logic would hold for such attacks. However, I agree with the authors that the points go beyond the scope of the current paper and would be interesting for potential future work. In any case, I think the paper is a good contribution to the field and would still vote for accepting the paper.  Summary after Discussion Period:-----------------------------------------------After reading the other reviewer's comments and corresponding with the authors, I have become convinced that the author's proposed regularization method is novel and effective, and would recommend this avenue of research be further explored. Yet it has also become clear to me that the author's claims on why their method works are not yet supported by evidence. Further, I don't believe the author's proposed further ablation studies would fix the theory, since such experiments don't address whether their method works by fixing problems with Laskins work (as the author's claim) or because it provides a more direct way of enforcing invariance to transformation (as I claim).So we're left with a difficult situation, the method and the experiments are good while the theory is lacking. In such a situation both acceptance or rejection seem reasonable. Yet, as per ICLR reviewer guidelines, one should answer three questions for oneself: - What is the specific question and/or problem tackled by the paper? - is the approach well motivated, including being well-placed in the literature? - Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.Since the theory is lacking and the approach is not well motivated, and since the theoretical claims haven't been rigorously supported, I feel as per ICLR guidelines the paper is not yet ready for acceptance. == comments after discussions and the paper update ==I appreciate the authors' efforts to improve the clarity and provide additional results. I believe that the proposed method is now clearly presented and the claims are properly supported by experiments. I raise the score to "accept".  #######################################Update after reading other reviews and author responses:I am happy to keep my score and support accepting this paper. I agree with Reviewer 4 that conducting a more detailed ablation study of the impact of regularizing both the policy and value function (i.e. a comparison with an algorithm like that of Kostrikov et al. (2020)) would improve the paper and hope that it will be included in the final paper.  -- EDIT: I am thankful to the authors for their insightful answer to my concerns, and for the though work in reviewing the manuscript. At this stage, I would keep my score (also in light of the other comments), but I hope that the authors find the suggestions from all the reviewers useful to re-present a more consolidated work soon. === after author response ===I would like to thank the authors for their detailed response. My major concern is the strong assumption in this paper - that the function to be approximated is a fixed-depth GNN. This assumption makes the problem less relevant to the actual size of the graph, and avoids a major challenge in size-generalization - that larger graphs are expected to require deeper operations. As pointed out in the author's response, some efforts are made to **experimentally** (1) demonstrate the effectiveness of the proposed method on a real dataset which might not be solvable by constant depth GNN, and (2) demonstrate in real datasets there is a discrepancy between the degrees of small and large graphs. I would be happy to raise my score from 4 to 5 given these experiments. However, the theory part could still be further improved so I do not further raise the score. update: Thank you to authors for their response to reviewer comments. I acknowledge I have read and reviewed their rebuttals. Some references that demonstrate size generalization is a well-known issue.ReferencesMeasuring abstract reasoning in neural networks. Barrett et al 2018.Learning TSP Requires Rethinking Generalization. Joshi et al 2020.Learning Combinatorial Optimization Algorithms over Graphs. Dai et al 2017. Update: Thanks for fixing the statement of the assumptions such that Theorem 1 can hold.   Update 4 -> 5. Some of my concerns are still not addressed in the revised version. ***Post Rebuttal***I have read authors' response and other reviewers' reviews. After reading it is still unclear to me why sparsity of the networks could affect the performance of the proposed framework. Therefore I would like to keep my original score.  ---------------------Post-rebuttal---------------------I am improving my grade a bit. I recommend the authors to dedicate some time further improving the paper clarity, especially in the matters related to my review and the other reviewers'--------------------- Author response: discussion with the authors allowed to clarify what the agent see and how they exchange information. I still find the paper a bit difficult to understand, most probably due to its novelty, but I consider that the current version is acceptable. As such, I recommend accepting this paper.  After rebuttal, I think the author have addressed most my questions. However, I agree with other reviewers opinions that the paper need further polishing and clarification, especially regarding the cooperative settings of the problems (homogeneous team, number of options etc as admitted by the authors) and associated theoretical and practical issues. Given the novelty of the idea and the current status of paper, I maintain my score. UPDATE:In the rebuttal, the authors emphasize two facts:1. There exist such IoT scenarios and real-time systems that employ multi-exit architectures (including those that employ cloud computation.).2. The slowdown attack is effective in these scenarios.However, to prove this method works in practice, it is not a simple "1 then 2", you need to show us "1 and 2". That is, you do actually deploy any system described in [1,2,3,4,5,6,7,8], and provide a feasible approach to attack with your method, and report the actual damage caused by your method, and convince the readers the damage is significantly severe compared to the efforts spent for causing the damage. Otherwise, it is only an application of PGD with a different loss function.  After the revision and the author response === I would like to thank the authors for their very elaborate response. I acknowledge that I might have asked for too many experiments in my review, but this was mostly because I really wanted to understand the various aspects of the method (as one other reviewer mentions - it has too many 'moving parts') and ensure that all the comparisons are valid by comparing to the latest work. While it is not possible (due to time and computational restrictions) to run all the required experiments, I am still happy to raise my score. While the paper might not be so impactful, it might still be a pretty nice and quick-to-compute BLI baseline for any future developments in this area.  ============================================================**Update:**A few  suggestions on the latest version:* Adding pointers from Table 1 and 2 to their full equivalents in the Appendix would be helpful.* The Appendix ablations are now thorough and exhaustive, but parsing them and digesting what they represent is a little tricky. Adding a one-line summary (much like in Section 5.`1) would be very helpful here. For example, talking about in how many pairs FIPP+IP outperforms Proc.+IP,  FIPP+IN outperforms Proc.+IN, FIPP+IP outperforms FIPP+IN.and a few comments:* The sheer number of experiments that the authors have performed in general (and in particular in the relatively short time period of this rebuttal) is impressive, and is in my experience indicative of an extremely good, well-designed and easy to iterate upon framework/code-base. If my guess is correct, I urge the authors to release their code if possible, because I believe it would greatly help anyone working in this space (or even consuming BLI's output in a downstream task).* It is also very heartening to see how much R1's suggestion helps improve the 1k case! However, because the efficiency and the lack of need of a GPU are big selling points, the drawback of adding self-supervision (10x the time, need for a GPU) might be a good caveat to add in Section 6.1 as opposed to keeping it till Appendix A.Overall, I would like to thank the authors for their very detailed and thorough response,  and for taking into account so much of all the reviewers' feedback to make the paper clearer with much more comprehensive ablations. The paper, its techniques proposed and their performance and efficiency, and the detailed experiments it conducts will be helpful for both the field of BLI and other fields relying on it. In view of this, this paper is a clear accept in my opinion, and I raise my score from 7 to 8. Edit: Upgrade from 5 to 6 after the author's response. ** Update after rebuttal **I appreciate that the authors addressed my comments. After reading the authors response and other reviews, I still believe that it is a good paper that should be accepted to the conference. #######################Update:I thank the reviewers for their responses. I appreciate the effort they put into clarifying the paper. However, I still think Section 2.1 in particular is difficult to follow. I will therefore keep my original rating. --------------------------Thank you for your response. I raised my score accordingly. Update: the authors went out of their way to address my concerns about the absence of the unbalanced class setting: they added a new datasets (SVHN), new results (table 4) and updated some of their explanations. All these additions seem satisfactory. I was also pleased with the feedback about computational cost (R3). I improved my rating. While I agree with the concerns of reviewer 4 (those I could understand), they would apply to every publication I have read about calibration, and I think the authors addressed these concerns to the best of our current knowledge. Update after the rebuttal: The authors have answered my concerns. I believe the paper should be accepted and would be a nice contribution to the current research.=============================================================== --------After author feedback, I feel that the authors did address a number of my points, including one or two that were indeed addressed in the text that I must have simply missed. Therefore, I am improving my score.The one point if any that I continue to disagree with the authors on is that variational dropout is sufficient to cover the space of explicit ensembling methods. Many of these "cost effective" ensembling approaches still continue to compare to direct explicit ensembling with the very explicit goal of having a baseline that is known to work well but removed from the computational cost concerns. I feel that this comparison should be standard practice. -- UPDATEThanks for the clear and exhaustive response.Minor, regarding the ablation study in A.1, with S=3 you get the best results, why not try with more? ---------------Post-rebuttal- Figure 1 is helpful in understanding the model. Thank you for adding that.- The additional experiments are also appreciated. This seems to indicate that it's not just the architecture but the learning rules that make the BCPNN model work well.- It would also be helpful to visualize the features learned by softmax units in RBMs and AEs and see if this results in a similar pattern of HCs encoding broad regions and MCs encoding variations within those regions.- It seems that the RBMs and AEs were not trained using any sparsity penalty. For example, Page 11 of https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf and https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf. Having a target sparsity can have a significant impact on the learned features. Higher sparsity makes the features look more like stroke like and localized, and less spread out all over the visual field (as they do in Fig 4, C and D).  - Based on the additional experiments, I will be increasing my score to 5. However, given that the main contribution of the paper is a comparative study, the paper can add value by doing a more thorough comparison against variants of AEs and RBMs that have otherwise similar properties (such as keeping the HC-MC (softmax) architecture and sparsity levels the same). Post Rebuttal: Thank you for the response. I have read the discussion with other reviewers. A small comment: while I agree that it is reasonable to keep the classifier the same for all the models (softmax with cross entropy) for a fair comparison, I disagree that the activation function for the first layer should be kept as ReLU in the KH model. In fact KH explain that this is a suboptimal choice in Fig. 4 of their original paper. Using powers of ReLUs should increase the KH accuracy. Overall, I think that this is a nice paper, and I am inclined to keep my initial score.  ===== after rebuttal =====I'd like to thank the authors for their revisions, which have significantly improved the readability of the paper, and the presentation of the results. The addition of fairness violation/accuracy tradeoffs and also (Kamiran et al. 2012) add a lot of value in putting this paper in the right context. After reading the rebuttal, I still remain unconvinced about the contributions of this paper. From a practical point of view, the performance of the proposed algorithm is on a par with (Kamiran et al. 2012), which is a baseline for demographic parity and has been improved on several times in the past 8 years. On the other hand, the main claim of the paper seems to be theoretical optimality. Unfortunately, although several previous mistakes have been corrected in the proofs, I cannot still follow the proofs of Theorems 1 and 2. New notation pops up all over the proof, and it is unclear how to follow some lines from the others. Given this, I am increasing my rating to 4 in acknowledgement of my previous misunderstanding of the definition of the statistical parity, which was cleared by the authors during the discussion period. However, I am still unable to recommend this paper in the current form for publication in the conference proceedings. =============UPDATE POST DISCUSSIONThank you for your responses.- I am more convinced about the novelty of the proposed methods.- I am still unconvinced about relabeling being the best we can do for out-of-distribution state distributions. Building learned methods that generalize more broadly or many-inner-step meta-learners could help quite a bit.- **I got concerned that a question by AnonReviewer2 "Why for in-distribution experiments, Experience Relabeling was removed?" went unanswered.** Having evaluated MIER-wR, adding MIER to the plots during rebuttal should have been very easy. Moreover, in practice, we shouldn't be able to leverage the fact that we know a task is in-distribution or out-of-distribution and choose between MIER and MIER-wR accordingly (which would also be a bit ugly). Therefore, in my opinion, putting MIER-wR as a representative of the paper instead of MIER is not valid.Since most of my concerns pre-discussion are still there and we cannot assess whether MIER does well on in-distribution (and MIER-wR is not better than MQL for OOD tasks) I, unfortunately, have to decrease my score from 5 to 4.- This prompted me to take a closer look at the actual numbers of the plots and I got more concerned. I didn't take it into account in my rating update because I should have brought this up before the discussion:    1. I feel something is off with PEARL vs MIER-wR when looking at in-distribution vs OOD in half-cheetah vel. In particular, we see PEARL does considerably better than MIER-wR on in-distribution (Fig 2), but its performance starts already much lower than MIER-wR in Fig 6 left.    2. Another concern is whether the numbers for OOD are "decent" in the sense of creating meaningful policies. Half-cheetah-vel-hard has a reward of ~-325, which is lower than MQL with 0 data for in-distribution half-cheetah-vel. This may be because the evaluated tasks are different, but raises concerns that maybe all baselines and MIER are all doing poorly. --------------------------------------------------------------------------------------------------UPDATEI read the other reviews and the author's responses. Thanks for replying to my questions, that made some things clearer!Some final comments: - "We performed a hyper-parameter search to find the best settings of the steps to take on $\phi$ and $\theta$ for each of the environments. To ensure that we dont over-fit the model when adapting for extrapolation, we use 80% of the available test-data to train the model, and use the rest for validation." - If I understand this correctly you tune the number of gradient steps on $\phi$/$\theta$ on part of the out-of-distribution adaptation data, and evaluate on the rest. That feels somewhat restrictive to me and might not always be possible - in which case your results are a bit optimistic. In the response to R1 you wrote that "the knowledge of in-distribution or out-of-distribution is not necessary to apply our method", but in this case the question is how to determine how to decide how often to adapt $\phi$, then $\theta$, before training the policy. At the very least, this should be discussed in more detail in the main part of the paper, given that this is central to your method. - Thanks for pointing me to the appendix of the paper that has the end-performance of your baselines. However I do think these should be included in your paper in order for the reader to get the full picture. There might be differences in implementation (as an example, some papers use different horizon lengths in MuJoCo) so it's not always possible to compare numbers across papers, and your paper should be self-sufficient. - Not meta-learning the initialisation of $\phi$ (but set it to zero before every inner-loop update) seems like an important detail to me - this means in Eq (2) you don't actually take the gradient w.r.t. $\phi$. It might also be an important to stabilise policy learning (because $\phi$ doesn't change too much over the course of meta-training). Even if it is an implementation detail, it should at least be mentioned in the appendix so that somebody who wants to re-implement your method can reproduce results!- Using something like [2] for good pre-adaptation exploration is a good idea, I agree! (And I agree it's out of scope, but might be worth mentioning in the paper.)I think overall the idea is promising but the paper falls short in terms of how the method is evaluated. I feel like too many things are buried in the appendix, and it remains unclear to me if MIER can, under realistic circumstances, really adapt to OoD tasks. I agree with R2 that Meta-World might be a suitable benchmark to evaluate MIER on, since  the training and tests tasks are distinct and there's a clear evaluation protocol for ML10 and ML45 (you have to adapt within 10 episodes) which would make it easier to compare to existing methods. Given the above I stand by my initial rating. ------- Update after rebuttal ---------Thanks for your response. Even though your responses clarified some of my comments, I still don't understand how Experience Relabeling can help with OOD and why your method doesn't good enough with in-dist data. As as result, I stay with my current evaluation and score. -------------------------------------Post Author ResponseThank you for providing the results and clarifying the setup. I am increasing the score to 6.  UPDATE: I read the authors' response. They say "Thus, it is unclear that generalization to unseen viewpoints shown in our results is completely explained by the similarity between the seen and unseen set,..."I agree with this comment. I don't say we can prove that the similarity fully explains generalization to unseen viewpoints. It's my conjecture. However, the authors themselves seem to recognize my conjecture could be right; at least, it cannot be eliminated. In other words, a major issue with the current manuscript is that we cannot derive a firm conclusion from the experimental results. If my conjecture is true, then the results are almost obvious and not interesting. I want to lower the score from 5 to 4, as the authors' response makes me believe my concern is valid.I think the authors tackle quite a hard problem and admire their efforts, though.  UPDATE:I have read the author feedback and other reviews & discussions. I have updated my rating from 6 to 7. The authors responded to my comments with updated analysis to further prove their claims. Post-discussion comment: I'd like to keep my rating (6: Marginally above acceptance threshold). I think R1's concerns regarding similarity between seen and unseen set are valid. As the number of seen combinations increases, the extrapolation problem becomes an interpolation problem. But in my view, that's not a weakness as long as the symmetric grid setting appropriately captures the relationship between the two tasks. I am leaning towards thinking that it does. It eliminates other confounding factors such as the relative weights of the loss terms too, and I think it is fair overall. Hopefully future work will provide more insight. Ultimately I think this submission is above threshold. ----Post-revision evaluation:The authors have modified the statements of the main theorems as well as including a more detailed comparison to previous works, which clarifies my concern. I have thus increased my score. The technical contributions bring new insight into the studying of scale separation of GDA, and enables a tight characterization of many toy examples. I believe these are solid contributions and should be valued.On the down side, I'd like to point out that the "practical implication" in this paper is a bit of stretch since the ImageNet experiments are run with RMSprop, whereas the analysis of this paper is highly specialized to GDA. Of course, studying adaptive algorithms in min-max games is exceedingly hard and well beyond the scope of this paper. What I recommend the authors is then:1. Explicitly notify the readers of the difference between RMSprop and GDA.2. Find a nontrivial but simple example where 1-GDA provides an okay baseline (say 7-layer CNNs for mixture of Gaussians or MNIST). Increase the time scale to show if it exhibits a similar behavior that a small $\tau$ gives the best result. This is directly verifying what the theory is saying, and hence feels more valuable to me. %--------------------------------%I thank the authors for clarifying my questions and concerns. The authors have included further theoretical developments in the revision, and they look satisfactory to me. Overall, I tend to accept this paper. ------After rebuttal: Thank the authors for their efforts in the rebuttal and revision. The authors' response to the Maxmin question (along with the revised discussions) sound convincing to me. I would like to keep my original evaluation and would lean towards acceptance for this paper. -------------------**Post Rebuttal**I have considered the revised article, additional reviews and rebuttal and decided to slightly raise my score but I am still in favor of rejecting the paper. Below is a summary of my reasoning.--------The authors have provided a good rebuttal and I am overall pleased with the detailed response, additional experiments and figures, and overall exhibited transparency. Unfortunately my assumption about $t_k$ seemed correct when considering the additional L-BFGS-B results, which indicate that using standard $t_k=1$ is a really strong baseline that proved difficult to beat.I would suggest finding another set of problems where $t_k=1$ is not so good for L-BFGS or consider adapting another first-order algorithm for which it is clear that the step-length needs to change between tasks and architectures. **Follow-up to author comments**I appreciate the authors' thoughtful response. While I agree on certain points, I am not convinced that the paper, as currently written, is ready for acceptance to a venue like ICLR. (That said, I also still believe the paper's core technical claims to be correct; my question is about significance.)The paper's premise is that reconstruction attacks on InstaHide deserve extensive investigation. I'd like to see justification for that premise. To be absolutely clear: I strongly believe that techniques (like InstaHide) which attempt to provide security against moderately strong attackers deserve discussion and investigation. However, I also believe that the starting point for such work should be a careful attempt to formulate security goals. I don't see how the current paper advances the important parts of that discussion. The focus on reconstruction is narrow; my score reflects that.To add just a little bit to my review: InstaHide is best viewed as a proposed lightweight alternative to multiparty computation (MPC). MPC protocols allow participants to compute on shared data in a way that reveals nothing but the final outcome of the computation (tools like FHE, to which InstaHide's authors compare it, can help achieve that goal but are not qualitatively different). MPC protocols (and InstaHide in particular) say nothing about how much is revealed about the data by the final trained model (the "ideal functionality", in the language of MPC). There is at this point a large literature showing that models themselves leak information in surprising ways (membership inference, to pick an example that received recent attention).Even if we focus on "lightweight MPC" as the end goal, the literature on data privacy suggests a wide range of more sophisticated measures of security than resistance/vulnerability to reconstruction. (That is, it's interesting and potentially important to relax the goal of full simulation that one normally aims for in MPC; but then one should spell out what the relaxed goal is, why it's sufficient for some settings, etc.) This submission reflects none of the past decades' lessons on that count. Responding to specific points: * I was not comparing to the paper of Carlini et al. I assume that this submission and the manuscript of Carlini et al are independent. * (Minor point) I'll stick by my complaint about Page 3: "it is clearly information-theoretically impossible to recover anything about [...] the private dataset." I understand and agree with the math of the rebuttal, but not with the conclusion. To spell out my original objection: Let $V=(V_1,V_2)$ be the random variable consisting of the two private images, and let $W$ be their average. It is not true that the mutual information $I(V;W)$ is 0 (which is the natural meaning of "no information about the data set"). It is not even true that $I(V_1;W)$ is 0.  Concretely, learning $W$ makes certain pairs of images much more likely than they were a priori. Whether that's ok depends on the context, what else is likely to be  known about the images, etc. My point isn't that one released image will lead to a practical attack; it is that information leaks in lots of ways. If you want to claim that leaked information isn't useful for an attack, that requires a clear notion of "useful for an attack" (and possibly a proof, though that's less important than a clear claim). ______________________________________ Updates in regards to the authors' response__________________________________________________________I appreciate the authors efforts to improve the manuscript and I think the manuscript has improved. I therefore raise my score to marginally above acceptance.In the abstract computational efficiency is highlighted but from the response it seems the approach scales as O(N^3) this is not particularly efficient as many procedures such as conventional SBM can exploit network sparsity for computational scaling. I think stating the approach as efficient is somewhat misleading based on the response explicitly made to Reviewer #4. I appreciate the added synthetic analyses and that a real network analysis is included but I am somewhat disapointed that only one real network is considered as opposed to including a series of existing networks with ground truth community structure. I thus find that the experimental validation on actual networks could be further strengthened - but it is good to see a real result.Minor comments:When stating "All of these approaches had theoretical guarantees." I believe this refers to some of the approaches discussed and not all. It would be good to clarify which approaches.estiamted -> estimatedwhich also been used -> which has also been used Post Rebuttal:I think the paper is much improved, and I'm bumping up my score accordingly. Small point - in corollary 3.2, to say 'with high probability' still requires that explicit conditions such as $|\beta|/\sqrt{d} \to \infty$ are stated. Thm 3.1 makes no claim of _high_ probability - it instead has an explicit bound, and does not impose conditions needed to make this bound 'high'. As a conseuqnce, the conditions of Thm 3.1 do not suffice for corollary 3.2. ---- ###Final Recommendation###Based on the discussions with other reviewers and AC, this paper is not ready to publish at this stage mainy due to the following reasons:1. the big claim of causality as also pointed out by R62. the writing should be significantly improved and the experiments lack details as pointed out by all reviewers3. the new problems found during discussion with the AC regarding the ablation study, and seeds, etc. In summary, this paper presents an interesting idea, but the experiments and writing in its current shape make the paper insufficient to be published at ICLR. The authors are encouraged to polish the paper in writing and experiments for future resubmissions. **Update**: Other reviewers have pointed out issues with this paper's ablation study. Additionally, it is difficult to trust the empirical results because they are based on only three runs. In light of these criticisms, I have updated my score from a 7 to a 5. I still think this idea is neat and am generally a proponent of introducing audio into work on RL, but the experiments as presented in this submission do not currently paint a complete picture. ---UPDATE: Due to concerns raised by other reviewers, and my own confusion about the computation of audio features, and clarifications on the causality stance, I have lowered my score from 8 to 7.  ### Update after discussion period ###Good idea, but the results don't clearly support the authors claims. I lowered my score. Update:I have read the rebuttal and the updated paper. I don't see my issue of relevance addressed. My score remains the same. ---I read and appreciated the response, but my overall rating is still leaning negative. --I am changing from "marginally above acceptance threshold" to "clear accept" after reading the response and thinking about the paper a bit more.  I acknowledge that the difference from previously published methods is not that large, but I still think it has value as it's getting quite close to being a practical method for generating fake training data for speech recognition. ---Update: After looking over the additional revisions and experiments, I'm bumping this to a weak accept. I agree with reviewer 3 that novelty is not the greatest, but there is a useful contribution here, and the demonstration of its effectiveness on low resource settings is valuable, since in a practical setting it is usually feasible to manually label a few examples.I'm still not convinced by the TIMIT experiments, now that I better understand them, since the F+M baseline is quite strong and very simple to run. It simply doesn't seem worthwhile to introduce all of this extra machinery for such a marginal improvement, but the experiment does serve the job of at least demonstrating an improvement over existing methods. After Response:After the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR. After rebuttal === I thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score.   ==== After rebuttal === I thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score. ==================== Dear Authors,the new version was improved greatly. Many mistakes have been corrected and most of the raised issues have been addressed. The derivation in App. B is very helpful, it wasn't clear before that you were doing a Taylor approximation and hence get twice the derivative. The analysis of the regularization hyper-parameter is very useful as well. I will adjust my rating. I do still have few issues:In Eq. (3), shouldn't the first term be E^{\beta}(s+, x, y) incl the loss for the target, rather than E(s+, x)? Then it would also be clear why beta can be negative.You still use f instead of roh in a few places, e.g. after Eq. (12) and in App. B.In App. B you also use both L and l. In App. B, better write g(\delta w) rather than g(0) in the line where you have the lim_{\delta w \rightarrow 0}. Thank you for the updates! The paper is much improved. I have raised my score. I still have some specific concerns, below:In the new Figure 1a, could you talk about how you search over optimization and initialization hyper-parameters for the local and global loss cases? I have a suspicion that the better performance of the local network may be due only to hyperparameters being better tuned for local rather than global training.re "they differ from our method in that their layer activations z_k are calculated using backpropagation-based optimization."I'm pretty sure this is not an accurate statement about the method of auxiliary coordinates? At least, it is my understanding that, due to the quadratic coupling between z_k in adjacent layers, the gradient with respect to z_k only depends on the z_{k-1 ... k+1}, and so there is no backpropagation through multiple layers in the network. Similarly for the gradient with respect to the weights in a given layer.More minor questions/comments:re: "Even at random initialization the gradient alignment S(d_L1/d_phi1, d_L2/d_phi1) is in general NOT zero-mean.  For a randomly initialized network, d_L2/d_phi1 and d_L1/d_phi1 are both zero-mean random variables (for the reason you mention - that d_L2/d_s2 is equally likely to have either sign), but they are not independent - they are both functions of phi1, and this dependency induces alignment.  Weve changed Figure to demonstrate that the initially weak alignment becomes stronger as training progresses (and phi approaches phi*), and added a derivation of the alignment result in Appendix B."Can you say more about why you would expect the gradients to be aligned at initialization? Is it just that both gradients are expected to have a non-zero projection in the phi1 direction (because the distance between two random vectors will tend to shrink if either vector is moved towards the origin)?Another thread of references that comes to mind, for biologically plausible local learning rules in machine learning, is the use of meta-learning to learn those rules. A seed paper is:Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Université de Montréal,Département dinformatique et de recherche opérationnelle, 1990.     xxxxxxxxxxxxxxIt seems that the authors provided a generic response to all the reviewers and I am not sure if they acknowledge the lack of clarity and lot of hand-wavy explanations in the paper. This issue has been raised by other reviewers too and is quite critical for becoming a good paper worthy for ICLR. Therefore, I am unable to update my score for this paper. However, I do appreciate the comparison with Moosavi-Dezfooli et al. (CVPR'17), this is a good addition as suggested by another reviewer. Thank you for the detailed reply and for updating the draft The authors have added in a sentence about the SLDS-VAE from Johnson et al and I agree that reproducing their results from the open source code is difficult. I think my concerns about similarities have been sufficiently addressed.My main concerns about the paper still stem from the complexity of the inference procedure. Although the inference section is still a bit dense, I think the restructuring helped quite a bit. I am changing my score to a 6 to reflect the authors' efforts to improve the clarity of the paper. The discussion in the comments has been helpful in better understanding the paper but there is still room for improvement in the paper itself.============= After reading the other reviews and responses, I still think the paper needs further improvement before it can published. Post rebuttal update/comment:I thank the authors for the revision and have updated the score (twice!)One genuinely perplexing result to me is that the method behaves better than random pruning, yet after selecting the salient neurons the weights can be reinitialized, as per the rebuttal:> # Initialization procedure- It is correct that the weights used to train the pruned model are possibly different from the ones used to compute the connection sensitivity. Given (variance scaled) initial weights, SNIP finds the architecturally important parameters in the network, then the pruned network is established and trained in the standard way.First, there is work which states quite the opposite (e.g. https://arxiv.org/abs/1803.03635). Please relate to it.Fundamentally, if you decouple weight pruning from initialization it also means that:- the first layer will be pruned out of connections to constant pixels (which is seen in the visualizations), this remains meaningful even after a reinitialization- the second and higher layers will be pruned somewhat randomly - even if the connections pruned were meaningful with the original weights, after the reinitialization the functions computed by the neurons in lower layers will be different, and have no relation to pruned weights. Thus the pruning will be essentially random (though possibly from a very specific random distribution). In other words - then neurons in a fully connected layer can be freely swapped, each neuron in the next layer behaves on al of them anyway we are thinking here about the uninitialized neurons, with each of them having a distribution over weights and not a particular set of sampled weights, this is valid because we will reinitialize the neurons). Because of that, I wouldn't call any particular weight/connection architecturally important and find it strange that such weights are found.I find this behavior really perplexing, but I trust that your experiments are correct. however, please, if you have the time, verify it. ===============After rebuttal ===============================After reading all reviews, considering author rebuttal and AC inputs, I believe my initial rating is a bit generous. I would like to downgrade it to 4. It has been pointed out that many recent works that are of a similar flavor, published in CVPR 2018 and ECCV 2018, have slightly better results on the same dataset. Further, the only novelty of this work is the proposed factorization and not the encoding scheme. This alone is not sufficient to merit acceptance. after rebuttal:The authors still did not address my concern about testing on only one task with only one evaluation metric. * EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. * Update after feedback: I would like to thank the authors for huge work done on improving the paper. I appreciate the tight time constrains given during the discussion phase and big steps towards more clear paper, but at the current stage I keep my opinion that the paper is not ready for publication. Also variability of concerns raised by other reviewers does not motivate acceptance.I would like to encourage the authors to make careful revision and I would be happy to see this work published. It looks very promising. Just an example of still unclear parts of the paper: the text between eq. (3) and (4). This describes the proposed method, together with theoretical discussions this is the main part of the paper. As a reader I would appreciate this part being written detailed, step by step. -------------------Revision. The rating revised to 6 after the discussion and rebuttal. Uodate: Read the rebuttal. My score remains unchanged. Revision: The authors added many references to prior work to the paper and did some additional experiments that certainly improved the quality. However, the additional results also show that the shared experience buffer doesn't have that much influence and that for the original tasks (the humanoid results in the appendix look more promising but inconclusive) the reloading variant seems to catch up relatively quickly. Reloading and distributed learning seem to lead to the largest gains but those methods already existed. That said, the IPE method does give a clear early boost. It's not clear yet whether the method can also lead to better end results. I improved my score because I think that the idea and the results are worth sharing but I'm still not very convinced of their true impact yet. UPDATE:Thanks for your response. As you mentioned, methods like [1] and [2] do perform open-ended recombination. Note that these methods perform not only texture transfer but also color transfer, while the proposed method seems to perform mostly only color transfer. As shown in Figure 6, essentially what the method does is transfer the color of the style image to the content image, sometimes with a little tweak, making the image distorted. One could say that in terms of image style transfer, the proposed method actually underperforms [1] and [2]. Hence I agree with R2 that comparison is still necessary for the submission to be more convincing and complete.------------------------------ [UPDATE]: Given the discussion with the authors, I agree that the paper outlines a potentially interesting research direction. As such, I have increased my score from 3 to 5 (and updated the review title). I still do not find the contribution of the paper significant enough to cross the ICLR bar. Update: I maintain my scores after the rebuttal discussion. Update: I am glad that the authors added updates to the experiments. I think the method could be practical due to the simplicity, therefore of interest to ICLR.The Pong case is also quite interesting, although it seems slightly "unfair" since the true reward of Pong is also sparse and DQN could do well on it. I think the problem with GAIL is that the reward could be hard to learn in high-dimensional cases, so it is hard to find good hyperparameters for GAIL on the Pong case. This shows some potential of the idea behind using simple rewards. Revision:The authors have took in the feedback from myself and the other reviewers wholeheartedly, and have clearly worked hard to improve the results, and the paper during the revision process. In addition, their code release encourages easy reproducibility of their model, which imo is needed for this work given the non-conventional nature of the model (that being said, the paper itself is well written and the authors have done sufficiently well in explaining their approach, and also the motivation behind it, as per my original review). The code is relatively clear and self-contained demonstrating their experiments on MNIST, CelebA demonstrating the use of the visual sketch model.I believe the improvements, especially given the compute resources available to the authors, warrant a strong accept of this work, so I revised my score to 9. I also believe this work will be of value to the ICLR community as it offers alternate, less explored approaches compared to methods that are typically used in this domain. I'm excited to see more in the community explore biologically inspired approaches to generative models, and I think this work along with the code base will be an important base for other researchers to use as a reference point for future work. Edit: change score to 7 in light of revisions and new experiment. == After discussion phaseBased on the rebuttal and additional experiments that clarified and resolved my questions, I change my initial rating. After rebuttal:=============Authors have addressed many topics that not only I but rev 3 address and hence I score this paper with a 7 and recommend it for publication. ****Reply to authors' rebuttal****Dear Authors,I greatly appreciate the effort you have put into the rebuttal. The changes you have made have addressed most of my concerns and I believe that the few outstanding ones can be fixed without significantly affecting the main message of the paper. I will thus be recommending acceptance of the paper.Best wishes,Rev 3 i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted. After rebuttal, I adapted the score. See below for original review.-------------------------------------------- edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.I also agree with the introduction of the Chi vector, and with the use of the term of "photometric BA", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.--------------- -------# Post-discussionI increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors. AFTER REBUTTAL:This is an overall good work, and I do think proves its point. The results on the TaxiBJ dataset (not TatxtBJ, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.----- Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences. Final Evaluation================I have gone through the other reviews as well as the author response.Firstly, I would like to thank the authors for providing detailed responses to my questions.In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.Moreover, from my understanding the analysis in David McKays book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron). As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper). Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review). In general, I feel this section could use some tighter formalism and justifications.I also remain unconvinced by the response to my issue with the claim Our experiments show that our networks can remember a large number of images and distinguish them from unseen images, where the negative images are also seen by the memorization model, so they are not unseen. The authors address this by saying 3M of the 15 M negatives have been seen. That does not seem like a small enough percentage to claim that these are unseen images.In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating. I read the other reviewers' comments as well as the rebuttal. I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper. Therefore, I do not feel confident in championing this paper. PS: I am downgrading my confidence in my evaluation.--- I have read the author's response, and I would like to stick to my rating. From the authors' response on the convergence issue, the result from [1] does not directly apply since the activation function that the authors use in this paper is relu (not linear). Having said that, authors didn't find any issues empirically.Q7: Yes, I agree that the result depends on the gradient structure of the relu activations. But my point was that, it is still a calculation that one has to carry out, and the insight we gain from the calculation seem computational: that one can regularize jacobian norm easily. True, but is that necessary? Or in other words, can we use techniques (not-so) recent  implicit regularization literature to analyze KFAC? I still think that the work is good, these are just my questions.==== 1. Re the distribution assumption, the response from the authors is not convincing. The paper you mentioned (https://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96) says that, when using BN, "quantization preserves the direction (angle) of high-dimensional vectors when W follows a Gaussian distribution", this has nothing to do with your assumption that W follows a gaussian distribution.The original question was not that "gaussian -> low quantization error -> good performance" (I think this is clear in the past 3 years) but rather "non-gaussian -> high quantization error -> bad performance?". Recent work suggests this may not always lead to bad performance (e.g. there are binary models with good performance and high quantization error). What does Figure 5 show? That quantization error is similar for analysis and simulation. Is this level of error "small"? Clearly, it depends on the number of bits. The gaussian assumption is not true for lower bit networks (the paper you referred uses 8 bits). Overall, the distribution assumption is a weakness.3. The point was about more datasets like VOC, beyond image classification. Thank you for improving the paper, I have increased my rating appropriately. Revision:Thank you for your response. &gt; In fact, our estimator in the theoretical and experimental analysis employs a continuous (ReLU) network. Though discontinuous networks are necessary for our setting (Lemma 2), we show that (continuous) ReLU networks can approximate the discontinuous network effectively (Lemma 3), hence the effectiveness of GANs is proved (Theorem 1).-That clarifies things, however I find that the discussion after lemma 2 rather missleading, if in the end the result ends up using continuous generator:"Because of the discontinuity, generative models with smooth functions, such as anadversarial generative model with kernel generators (Sinn &amp; Rawat, 2018), cannot work well withdisconnected supports."  - It is still unclear to me how the optimal value of S_f is obtained from eq (5). The author points out the work by Zhang+ (2018), but this should be clarified in the current version of the paper: What result in Zhang+(2018) do you use to get this value?- I find the experiments  not very convincing. I understand that the point is not to show that GANs are better than  other methods but it is important to be make meaningful compairisons (use comparable scores) otherwise there is little scientific value in figure 5 especially.  - As reviewer 1 mentions, lemma 3 is supposed to be one of  the main theoretical contributions of the paper, however, the proof seems very similar to the one in ([2], appendix B.1). Although the authors mention lemma 1 of [2] in the proof of lemma 3, it seems like the whole section in ([2] appendix B.1) is dedicated to show the very same result.For all these reasons I still wouldn't recommend accepting this paper.  Update:The score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now. ******************Update after author response:Thanks for the clear response and Figure 3, and nice paper. My score is updated.PS: I still think that the (tiny) error bars are obfuscated because the line connecting them is the same thickness and color. I read the authors' response. The paper should in its final version add the precise explanation of how the two states interact and how a joint state definition differs from the current one. EDIT: Updated rating after author revisions. #Update I thank the authors for addressing my questions and revising the manuscript, which clarified many of my concerns regarding this work.  Update: The authors have addressed the most pressing issues with the manuscript. I've increased my score and vote in favour of accept. The section of limitations was difficult to follow and would benefit from a more structured comparison with competing methods.  ----Post discussion.After reading some of the author responses I have decided not to update my score. I think the paper needs a bigger revision and more results to be above the acceptance threshold. ----------------Post discussion comment:I have decided not to update my score. While the theoretical analysis is interesting, I am not fully convinced about the utility of the proposed algorithm. It would be useful to have more experiments in the widely used benchmark such as atari or have better motivation explaining why faster convergence would help in the RL context as it would in supervised learning. --------------------------------------------------------------------I have read the authors' responses and almost all my concerns have been well addressed. So I increase my point to 8. **Updates after Author Response:***Complexity Measure*: My concern with the complexity measure is that it seems to me that the biggest difficulty in reinforcement learning is in actually being able to estimate accurate value functions, while proposed complexity measure really only vaguely captures how far optimal policies are from the behavior distribution. In particular, at each state, it only depends on the true values of Q for each action, and can't capture how hard it is to estimate them.Even among similarly structured MDPs, we can consider a 2-action MDP with a behavior policy that was simply uniform. We could have one extremely simple MDP that was simply composed of independent deterministic bandit problems at each state with no transitions (always remain in your starting state), which would be trivial to solve even from offline data with full support.On the other hand, we could have a much more complex MDP with meaningful stochastic transitions, random rewards and so on. If we simply match the Q values in the two MDPs, they appear to be equally complex from this measure, despite the fact that the bandit MDP is far simpler to solve.As such, I think the proposed complexity measure doesn't really reflect the real challenges of an offline RL problem, and am not sure how one would extend it to be useful.*Re misc comments and access to true behavior policies:* My thinking here was that if we had a very stochastic behavior and finite samples, there would be actions that have reasonably high probability under the true behavior policy, but we never get to see in the data and wont' be able to evaluate well. One benefit of fitting the behavior model the the empirical data is that it would focus on those actions that do appear in the dataset (in an extreme case, we could simply have Dirac deltas on the observed data points), and so could benefit by restricting the actions to those that can be evaluated better.*Overall Opinion:* In light of the empirical results mentioned in the author's response as well as the comparisons to KL regularization, I have raised my rating. I still do believe it is a very borderline paper, and would perhaps benefit from more careful analysis and focus on how the different behavior modelling choices influences offline.  -----Post Rebuttal-----I have read all feedback and especially thanks to the authors' efforts on the extra experiments. I think the author has addressed my first concern. For the second one, I would prefer to see the errorbar.I tend to keep my scores unchanged. I think the findings are interesting [share similar thoughts as R4's], while the experiments part need to be improved. Although I like the ideas and observations, I don't feel especially strongly in favor of it and cannot champion it. *****post rebuttal updates*****I want to thank the authors for responding to my questions. The additional explanations are indeed helpful for clarifying my first two questions (selection of the binary kernel and the use of E). However, I still have concerns about Table 1 (and Table 2). For example, I have a really hard time interpreting the significance of achieving a 3.2x CR with a loss of 3% (92.3 - 89.2 from VGG-7) in acc with the proposed method (although the paper argues that it's a "bearable" loss). Considering that this is the main experiment supporting the efficacy of the proposed quantization algorithm, I think the paper needs more controlled experiments to demonstrate the practical usefulness of the proposed algorithm. As a result I'm keeping my original score and hope the authors can work on the improvements for the next version.  -----------------------------------------------------------Post-Rebuttal---------------------------------------------------------The authors have fully addressed my concerns. I changed the rating to a 7.  ------Edit:Thank you for the author response. Even if you consider the story to be the same across literature (which in this case is not, since the more recent models handle spatial relations that the previous ones failed on), it's still worth doing due diligence to the recent work, especially so that the reader gets a better sense of how to position your work amongst these. Updated=====================The authors addressed some of my concern, and I appreciated that they added more experiments to support their argument.Although I still have the some consideration as R3, I will raise the rating to 6. I took a look at the revision.  I am glad to see that the authors clarified the meaning of "optimality" and added time complexity for each algorithm. The complexities of the algorithms do not seem great (3rd or 4th order polynomial of N) as they appear to be checking things exhaustively, but perhaps they are not a big issue since the decomposition algorithm is run only once and usually N is not huge. I wonder if more efficient algorithms exist.It is also nice to see that the time overhead for one training step is not huge (last column of Table 1). While I still think it is better to see more complete training curves, the provided time overhead is a proxy. I hope the authors can further improve the algorithm description, for example, the pseudo-code of Algorithm 1 is very verbal/ambiguous, and it would be better to have more implementation-friendly pseudo-code.Despite the above-mentioned flaws, I think this work is still valuable in handling the memory consumption of arbitrary computation graph in a principled manner. ============================================================================= ##### added after author response #####I appreciate the authors effort in trying to make their contributions precise and appropriate. The connection between ZO-signSGD and adversarial examples is further elaborated, which I agree is an interesting and potentially fruitful direction. I commend the authors for supplying further experiments to explain the pros and cons of the proposed algorithms. Many of the concerns in my original review were largely alleviate/addressed. As such, I have raised my original evaluation. Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result. ----Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result. Update after feedback: I would like to thank the authors for their detailed answers, it would be great to see some revisions in the paper also though (except new experimental results).Especially thank you for providing details of a training procedure which I was missing in the initial draft. I hope to see them in the paper (at least some of them).I have increased the rating to 6. Given new experimental results both on real data and forecaster comparison I would like to increase the rating to 7. However, I am not sure that this is fair to other authors who would might not be physically able to provide new experimental results due to computational constraints, please note that the experiments in this paper are rather 'light' in the standards of modern deep learning experiments and can be done within the rebuttal period.  ==================================================== EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference. (After the first revision) I have raised the score after the very detailed author response (thanks for that!), but this is also conditioned on the authors making the actual revisions promised in their response. I am still quite interested to check how well the method works in a setup with distant language pairs. UPDATEMy most serious concerns have been addressed in the revised version. ========Thank you for the detailed responses. I have updated my score from 5 to 6.  Revision: After the authors revision, I change my score since they addressed my main complaint about results using pseudogradient attacks edit: I have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers)  --------UPDATE AFTER READING THE AUTHORS' COMMENTS-----------1. Appendix F lacks explanation. So I'm going to say what I meant in details. In order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). I would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced:1) principal components of digit 0 and digit 9 differs a lot AND 2) w_0 weights components of digit 0 higher but weights those of digit 9 lower2. "But for our regularized model, the number of weights with high values is smaller compared to that of normal model ..."I'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ##############Based on the authors' response, revisions, and disucssions we have updated the review and the score.   -------------------------------------------------After reading the authors' rebuttals, they have addressed part of my concerns but I still think the current form is not below the acceptance threshold due to its weak experimental results and unclear technical details. UPDATE 2 (Nov 19, 2018): The paper has improved very substantially since the initial submission, and the authors have addressed almost all of my comments. I have therefore increased my score to an 8 and recommend acceptance.------------------------------------------------------------------------------------------------------------------------------UPDATE (Nov 16, 2018) : In light of the author response, I have increased my score to a 6.------------------------------------------------------------------------------------------------------------------------------ ****************[UPDATE]I would like to thank the authors for implementing the changes and adding a plot comparing their algorithm with dropout. While the quality of the paper has improved, I think that the connection between the perturbation level and the Hessian is quite obvious. While it is a contribution to make this connection rigorous, I believe that it is not enough for a publication. Therefore, I recommend a rejection and I hope that the authors will either extend their theoretical or empirical analysis before resubmitting to other venues. UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions. ########################I would like to commend the authors for making a significant effort in revising their manuscript. Specifically, I think adding the experiments for QSGD and Krum are an important addition. However, I still have a few major that in my opinion are significant:- The experiments for QSGD are only carried for the 1-bit version of the algorithm. It has been well observed that this is by far the least well performing variant of QSGD. That is, 4 or 8 bit QSGD seems to be significantly more accurate for a given time budget. I think the goal of the experiments should not be to compare against other 1-bit algorithms (though to be precise, 1-bit QSGD is a ternary algorithm) , but against the fastest low-communication algorithm. As such, although the authors made an effort in adding more experiments, I am still not convinced that signSGD will be faster than 4 or 8 bit QSGD. I want to also acknowledge in this comment the fact that these experiments do take time, and are not easy to run, so I commend them again for this effort.- My second comment relates to comparisons with state of the art algorithms in byzantine ML. The authors indeed did compare against Krum, however, as noted in my original review there are many works following Blanchard et al.  For example as I noted <a href="https://arxiv.org/pdf/1802.07927.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.07927.pdf</a> (the Bulyan algorithm) shows that there exist significantly stronger defense mechanisms for byzantine attacks. I think it would have been a much stronger comparison to compare with Bulyan.Overall, I think the paper has good content, and the authors significantly revised their paper according to the reviews. However, several more experiments are needed for convincing a potential reader of the main claims of the paper, i.e., that signSGD is a state of the art communication efficient and byzantine tolerant algorithm. I will increase my score from 5 to 6, and I will not oppose the paper being rejected or accepted. My personal opinion is that a resubmission for a future venue would yield a much stronger and more convincing paper assuming more extensive and thorough comparisons are added. ##### added after author response #####I appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). The novelty and experiments are somewhat limited. Thus I am lowering my score.----------------------------------------------------------------------------------------------------------------------------------------------------------------------- Final score: The authors have addressed my concerns in the rebuttal. I believe this paper tackles an interesting problem, and the experiments are good enough since this is one of the first papers that tackle this problem. So I keep the initial score. Several changes have been made to my comments, thanks for pointing out the mistakes.  ------------------------Post Author ResponseThank you for adding the ablation study and the attention based models. I enjoyed reading your work and it has answered some of the questions we wanted to explore in Capsule Networks.  Post RebuttalThanks for the author(s)' responses. The rebuttal addressed some of my questions. I have a couple of  suggestions : 1- Your proposed capsule network is not the first one that is applicable on large scale datasets like ImageNet, there are other capsule networks that are applicable on real world scenarios and also ImageNet dataset with improvements over the baseline - Dual Directed Capsule Network for Very Low Resolution Image Recognition (ICCV 2019)-  Subspace Capsule Network (AAAI 2020)please refer to them and also give intuitions about why your proposed Trans-Caps is not performing well on ImageNet. The intuition and analysis is valuable to the community.  2- To support the generalizability claim of Trans-Caps, I highly recommend reporting results on Multi-MNIST and also affNIST. Specially when you train the model on MNIST and test it on these two datasets.  Update-------I've updated by score in light of the discussion; as I said in the comments, from a purely experimental point of view there are good results, however the presentation of the paper confounds too many aspects. If the authors can address the terminology issues then it would make the work stronger.  %% AFTER REBUTTAL %%  Thank you for all the updates.I would like to thank the authors for their humility in the rebuttal and for clarifying the paper's contributions. Accordingly, I will increase my score. However, I still believe Section 3.1's contribution, and the follow-up of using this to improve classifier robustness, is useful only for a very specific type of data and it is hard to assess its value from a practical point of view. The fact that the authors were able to showcase that such counterfactual data augmentation improves classification is, although expected, useful in itself. However, performance improvement is only evident in colored MNIST, relative to GAN augmentation. Furthermore, R4 points out the important issue that the relevant causal feature is assumed to be known in the experiments. This information is normally not available and must be inferred by the classifier. The additional experiments provided by the authors during the rebuttal are welcome but they should be in the main paper rather than the appendix since this is the main setting where spurious correlations create problems. I believe the experimental section should put more weight on this setting.In light of all this, I will provide a borderline score leaning towards rejection. I encourage the authors to expand section 3 to settings that do not restrict the images to have one foreground object and a single background.  .**Post-discussions**: the authors have clarified the meaning of "invariant classifier", which is now tied to their specific toy problem, and I now believe such a classifier is indeed the minimax OOD classifier supposedly seeked by IRM. While I still think the paper does not shine on the side of clarity, my main concern about the incorrectness of the presented theorems has been answered and I believe the paper will be of interest to the community. I therefore raise my recommendation towards acceptance === EDIT: post-rebuttal ===Thanks for the additional explanations. After reading the response from the authors, we raise our score by +1. ---The authors have addressed many of my concerns in the rebuttal, and so I am increasing my rating.  ---Post-rebuttal My main concern was assessing the value of the overall contribution of the paper. The other reviewers seem to appreciate both the new environment being offered and the combination of techniques deployed in the authors' solution. If there is an audience that will appreciate this work at ICLR as seems to be indicated by those reviews, then I would increase my score to marginally above the acceptance threshold. Thank your for your response and the revisions. The revised version of the paper includes many improvements. The results presented in Figures 1, 2, & 3 are much more compelling with the additional trials. I also appreciate the inclusion of the simple MDP example in the appendix, and the editorial changes made; the paper reads much more smoothly now.In light of the updates, I have changed my score from a 6 to a 7. ==========================================After discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments. The paper looks much better than before. Thus I increased my rating. UPDATE:I've read the revised version of this paper, I think the concernings have been clarified.-------  =================================================================================================I've read the rebuttal. I updated my score but still not vote for accept. This paper is not my main research area. Very unfortunately, this paper was assigned to me. The main issue of this paper is the fair comparisons with other works. However, I don't have enough knowledges to judge this point.  So please assess this paper with other reviewers comments. ####Revision:The rebuttal does little to clarify open questions:1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.3. The authors argue in their rebuttal that "the grid" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm. This seems contradictory.  === after rebuttal ===Thanks for the authors' response. Some of my concerns have been clarified. I increased my rating from 5 to 6. EDIT: The authors have addressed my concerns and I have raised my score. ----------------------------------------AFTER REBUTTAL:Thanks for your reply to my comments. The new revision has improved clarity and provided new supporting evidences. I would like to raise my rating to 6.That being said, (as you agreed) the link from the conceptual goal to the proposed objective has mostly empirical support. Therefore I hope it may encourage future investigation on when and why the proposed objective is successful in achieving the conceptual goal. Edit: After paper additions I am changing my score to a 7. Edit: After paper additions I am changing my score to a 6. Edit: Based on the rebuttal I've changed my rating from 4 to 5. Revision:The approach of using robust features is interesting and promising, as well as the idea of training on multiple languages. Overall, the authors response addressed most of the issues, therefore I am not changing my rating. Updated Review: The authors have updated the appendix with new results, comparing against HER, and provided detailed responses to all of my concerns: thank you authors.While not all of my concerns have been addressed (see below), the new results and discussion that have been added to the paper make me much more comfortable with recommending acceptance. The formuation, while straightforward and not without limitations, has been shown in preliminary experiments to be effective. While many important details (e.g. robust baselines and ultimate performance) still need to be worked out, HPG is almost certainly going to end up being a widely used addition to the RL toolbox. Good paper, recommend acceptance.Evaluation/Clarity/Originality/Significance: 3.5/4/3/4Remaining concerns: - The poor performance of the baselines may indeed be due to lack of hindsight, but this should really be debugged and addressed by the final version of the paper.- Results throughout the paper are shown for only the first 100 evaluation steps. In many of the figures the baselines are still improving and are highly competitive... some extended results should be included in the final version of the paper (at least in the appendix).- As pointed out, it is difficult to compare the HER results directly, and it is fair to initially avoid confounding factors, but Polyak-averaging and temporal difference target clipping are important optimization tricks. I think it would strengthen the paper to optimize both the PG and DQN based methods and provide additional results to get a better idea of where things stand on these and/or possibly a more complicated set of tasks. ====Raising my score after the authors responded to my questions and added the HER results. Post-rebuttal------------------I have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.========================================= === After rebuttal ===Thanks for adding the additional experiments (particularly with fully random embeddings) and result analyses to the paper. I feel that this makes the paper stronger and have raised my score accordingly. (After reading the rebuttal, I raised the rating from 5 to 6.) Reading thread and authors response rebuttal decision:=================================================I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication. Based on the revision, I am willing to raise the score from 5 to 7.==========================================   UPD: the discussion and the edits with the authors convinced me that I may have been a bit too strict. I have changed my score from 5 to 6. ===After rebuttal: I thank the authors for addressing the comments in my review. It clarifies the questions I had about on the 2D3DS dataset (panorama vs. 3D points). Overall I feel this is a good model and have solid experiments. Therefore, I raise the score to 7. ----Edit - November 29, 2018: Increasing my rating from a 4 to a 5 after discussion with the authors. Though their insights are not unknown, I think the authors are right in the fact that this is not explicitly discussed, at least not in the peer-review research with which I am familiar. But I don't think this by itself merits an ICLR publication. REVISION:The other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first. Thanks for the updates and rebuttals from the authors. I now think including the results for HAT may not be essential for the current version of the paper. I now understand better about the main point of the paper - providing a different setting for evaluating algorithms for combatting CF, and it seems the widespread framework may not accurately reflect all aspects of the CF problems. I think showing the results for only 2 tasks are fine for other settings except for DP10-10 setting, since most of them already show CF in the given framework for 2 tasks. Maybe only for DP10-10, the authors can run multiple tasks setting, to confirm their claims about the permuted datasets. (but, I believe the vanilla FC model should show CF for multiple permuted tasks.)I have increased my rating to "6: Marginally above acceptance threshold" - it could have been much better to at least give some hints to overcome the CF for the proposed setting, but I guess giving extensive experimental comparisons could be valuable for a publication. ===================== # [Updated after author response]Thank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. In its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.------------------------------------------  -----------------------Update: Apologies for the slow response. The new version with more baselines, comparisons to CPC, discussion of NCE, and comparisons between JS and MI greatly improve the paper! I've increased my score (5 -&gt; 7) to reflect the improved clarity and experiments. Revision 2: The new comparisons with CPC are very helpful.  Most of my other comments are addressed in the response and paper revision.  I am still uncomfortable with the sentence "Our method ... compares favorably with fully-supervised learning on several classification tasks in the settings studied."  This strongly suggests to me that you are claiming to be competitive with SOTA supervised methods.  The paper does not contain supervised results for the resnet-50 architecture.  I would recommend that this sentence should either be dropped from the abstract or have the phrase "in the settings studied" replaced by "for an alexnet architecture".  If you have supervised results for resnet-50 they should be added to table 3 and the abstract could be adjusted to that.  I apologize that this is coming after the update deadline (I have been traveling).  The authors should simply consider the reaction of the community to over-claiming.  Because of the new comparisons with CPC on resnet-50 I am upping my score.  My confidence is low only because the real significance can only be judged over time.  Update: I have read the authors' response. My current rating is final. --- After rebuttal --- Still not convinced of the value of the work to the community. Will keep my score the same.  ////////////I would like to thank authors for providing detailed answers to my questions. After reading their feedback, I am now willing to change my score to accept. UPDATE AFTER REBUTTAL:I am still torn about this paper. On one hand, I still think that the topic and discourse provided by this paper is extremely important. On the other, the results - even after the revision - do not completely convince me. I might update my score after some discussion with the other reviewers.2ND UPDATE:After giving it some more thought, I find myself convinced that this paper has a contribution important enough to be accepted. I increase my score to 7. COMMENTS RELATED TO REVISION:The new analysis that has been added takes a step towards getting at the relationship to invariance. This is a positive. In general comments, the authors state as important contributions: "1) to the debate on the harmfulness/importance of selectively activated units in DNNs [1-3] by presenting concrete examples where selectivity is important for generalization, and 2) to the neuroscience community, where, although orientation selectivity has been extensively analyzed for these 60 years since [4], its functional importance in a natural environment has remained unanswered."On point 2, while this provides an example of an artificial network where orientation plays an important role, it's a stretch to generalize this to conclusions concerning functional importance in neuroscience.One point 1, I agree that this paper takes steps in the right direction, but ultimately the overall conclusions still feel as though they are a natural and implied consequence of limiting orientation selectivity. It is noted that  orientation selectivity is "not just a superficial byproduct of object recognition, but is causally indispensable for object recognition". The idea that selectivity for oriented edges is indispensible for object recognition again is a conclusion that feels as though it would be shocking if this were not true. The notion of it being a superficial byproduct of object recognition presupposes that the purpose of the system is to recognize objects. Again, it would be surprising if there were vestigial features in the network that are learned, but play no important role - especially among early layers. I think the paper might be strengthened by re-working this second point to more strongly establish the causality the authors claim.  *Update after discussion period*I remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks. [REVISION]The work is thorough and some of my minor concerns have been addressed, so I am increasing my score to 6. I cannot go beyond because of the incremental nature of the work, and the very limited applicability of the used continual learning setup from this paper. -  My concerns about figures are solved; I want to thank authors for their efforts. ##########################################################################Thanks for the response to my feedback. Unfortunately, after reading the reviews/responses from the other reviewers, I have decreased my rating to 5, due to some concerns related the technical aspects of the paper *********EDIT: Changed my score from 5 to 6 after the author response/revision. **Update**Thank you to the authors for the detailed response. Overall, I'll leave my rating as a 6. The paper is clear and proposes a promising optimization method, with moderate comparisons to prior approaches and reasonable results on ImageNet (but not CIFAR-100). The impact of these results are somewhat diluted by the overall absolute low scores in the adversarial setting, but as a general DRO optimization method, the result is interesting. Thank you for all the discussion.Comments to the latest author response (posting this as an edit since public comments are now disabled):[1] Thanks, the updated version is clearer.[2] I agree with this characterization (for example, the unconstrained perturbation might cause optimization instability). The paper still seems to contain the language about the Sagawa paper requiring a custom sampler. Overall, I agree that there are reasons to believe that the proposed optimization method is more stable, but I think the authors should be clear that they only compared to Mohri et al. experimentally (in your response to Reviewer 3, you mentioned that you extensively discussed and compared to both Mohri and Sagawa).[6] Thanks. I also agree with this. I had two points: first, the differences between your proposed algorithm and the standard algorithm might be more stark when the training is not balanced. Second, in imbalanced settings, it is more likely that the test distribution will be skewed (e.g., rare animals in iNaturalist). After response: Thanks for the clarifications. The proposed method is a modification of existing averaging schemes to schedule learning rate, but more experimental evaluations are required to determine the benefits of algorithm. I read the author feedback and decide to keep my score. ======== after discussion phase =======The main drawbacks of the paper remain after the discussion. Mainly, the analysis is too simplistic which basically ignores the new aspects of the algorithm and its comparison with well-known methods is unclear. Therefore, I keep my score.  === After rebuttal ===I am not convinced that the improved performance is because of the adversarial training. I trained a simple MLP and with the right amount of regularization it gets 42.0% f1 score on Bibtex, so I am not sure that the adversarial training is very essential here.  === after rebuttal ===The authors explain some of their model choices in the rebuttal, but I am still not convinced about the difference with Gygli et al. 2017 is significant enough. === after rebuttal ===I appreciate the response, but I still think further analysis of the model is needed to understand where the gains in performance are coming from. The claim is that this is due to the adversarial loss used, but without further ablations I feel this is too strong a claim to be making given the current evidence. ===After rebuttal:I would like to thank the authors for the response and updating the draft. They have addressed 1) the title issue and 2) adding domain adaptation baselines. Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines. However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty. The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion). [addressed in the revision][addressed in the revision][addressed by adding WSJ experiments][addressed in the revision].  [short comment added in the revision][short comment added in the revision][addressed in the revision]. [fixed in the revision] [fixed in the revision][fixed in the revision][fixed in the revision][remains in the revision] After the discussion with authors, I am happy to recommend acceptance. --In light of the extended experiments w.r.t. to 2.1 I increased my score from 5 to 6.  Overall, I still have doubts about the interpretability and complexity of the proposed method.  Complexity:  "but all the intuitions needed would come solely from training NN".    I disagree with this response.   The architecture is a mix between a tree (hard, decision-tree like error surface,  non-local) and neural network (smooth, mostly convex error surface). This also implies that the training process and its behavior will possess patterns and challenges of both approaches. Interpretability:  I think the method misses "priors" that enforce credit assignment.  Partitioning the problem in subp-roblems should be done via the tree components, whereas processing (such as image filtering) should be done in the network nodes. However,  the method does not enforce, or encourage this behavior, for instancevia constraints:   also nodes can do partitioning (because neural networks can approximate decision trees)  and edges can do processing (e.g. decisions-trees can be used for mnist).So I still believe this to be a borderline paper, however, the experiments support a more general applicability.  The response does not fully address my concerns.  Begin revision comments:-----Given the revisions to this paper, I am more confident that it will be of interest to the community. The major contributions here I see is the removal of target networks given their approach. Given this I still have concerns on clarity and still am unhappy with the lack of confidence intervals in the main experimental section. I've increased my score to 7 to reflect my increase in confidence. After going through the authors' comments and the revised version of the paper, I keep the rating as is. The paper needs a more convincing evaluation section as well as some clean up (e.g., references to figures and tables in the text) Revision: Updated my rating to acknowledge that the reproducibility issue is addressed. ********************After authors response:Thanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper. ==== Post Rebuttal ===Thanks the authors for the response. I still have concerns about this work. Please refer to my comments "Reply to the rebuttal". Therefore, I keep my score as 5. ====11/26At this time, the authors have not responded to reviews. I have read the other reviews. Given the outstanding issues, I do not recommend acceptance.12/7After reading the author's response, I have increased my score. However, baselines that establish the claim that SMC improves planning which leads to improved control are missing (such as CEM + value function). Also, targeting the posterior introduces an optimism bias that is not dealt with or discussed. Update:I appreciate the through error analysis the authors have done in the revision, which addressed my major previous concerns. I've updated my score accordingly. Updated after reading author revisions:I appreciate the clarifications, the response answered almost all of my small technical questions.  That plus the new error analysis increases my opinion about the paper, and I'm no longer concerned that the rule templates are hand-generated given their generality and small number.  I am still concerned that we don't actually know how well the methods work, because the test sets are small and the performance differences between the methods (in Table 1) are quite close.  I will raise my score one point.The authors might try to evaluate using k-fold cross-validation with the training set, to obtain more examples for evaluation. UPDATE: Given the authors' rebuttal and the clear improvements to their paper, I've increased my rating of the work.======================= After Rebuttal:I read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3. I'm happy with the revisions the authors have made, as I find that they call out the novel contributions a bit more explicitly. Specifically I see some novel work in the area of simultaneous multi-task/meta-RL and black box optimization of the policy net architectures. I don't think calling this NAS is justified; calling it bayesopt or black box opt is fair. NAS uses a neural net to propose experiments over structured graphs of computation nodes. This work appears to be simpler hyperparameter optimization. Thanks for your final answers and changes. I increased the rating of your paper to 8. ------------------------------- After Rebuttal ---------------------------------I am very satisfied with the authors' response, so I will change my vote from rejection to acceptance. %%%%%%%% After rebuttal %%%%%%%%I appreciate authors' efforts to address my comments and am satisfied with their response. I will change decision from rejection to acceptance. Post-rebuttal revision: The authors have adressed my concerns sufficiently. The paper still has issues with presentation, and weak comparisons to earlier methods. However, the field is currently rapidly developing, and comparing to earlier works is often difficult. I believe the Langevin-based prediction is a significant and clever contribution. I'm raising my score to 6.------ ** review score incremented following discussion below ** Update: I thank the authors for providing additional experiments on this part.   ==== After rebuttal ====The authors' feedback clarified some of my concerns. But my main concern about why minimizing the objective function can reduce both error and the number of parameters still remains. So I changed my rating to 4 from 3. === after rebuttal ===The authors have addressed my concerns and I have updated my score accordingly. [UPDATE]. The authors addressed my concerns stated in my review above. I think the bibliography has improved and I recommend acceptance. ---------------------------------------------------------------------------------------------------------------------------------------------------------------------Update:Thanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. However, I still found some claims made by authors problematic. For example, it reads in the abstract that "...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters...". Clearly, Li et al. 2018b has a differentiable formulation which falls under your description.Besides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. Also, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.Directly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.Therefore, I would like to keep my original rating. --------------------------Update:The authors addressed some of the weak points mentioned above adequately. The experimental evaluation was significantly improved and the results are a nice contribution. However, the theoretical contribution and the poor motivation of capsules in the graph context remain weak points. I have updated my rating accordingly.  Update:According to the revised version which addresses a lot of my concerns, I vote for marginally above acceptance threshold. I have read authors' response.  ===after rebuttal===All my concerns are addressed. I will upgrade the score. After the rebuttal:I read the authors' comments and understand more the technical results. I raised my score. But I still feel that the techniccal contribution is a bit weak. Revised: increased score after author response. AFTER REBUTTAL:I think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. ------------ I still like the overall mission of this paper and found it highly readable. However, after a more careful reading I do agree with the issues raised by the other reviewers. It seems that there is a fundamental question in the field as to a) how important meaning preservation is for adversarial attacks and b) how this should be assessed. In its current form, I don't think this paper provides satisfactory answers to these questions, but it does point at an important topic to be resolved. I want to thank the authors for addressing my concerns.  I understand that their focus was not exactly the same as in previous work, but want to thank the authors for nevertheless adding the additional motivations and extra analysis.  I believe that this will help situate this work better within this area, and also allow better comparison with other studies.I have changed my overall rating from a 6 to a 7. I have read the authors' detailed rebuttal. Thanks. --- Update ----Given the author's response and the discussion, I'm going to raise the score a little. Although there are some valid concerns, it provides a clear improvement over Allamanis et al paper, and provides an interesting approach to the task. EDIT: I think dealing with the lower bound and including plots for all 55 games pushed this over the edge. It would've been nice if there non-zero scores on Montezuma's Revenge, but I know that is a high bar for a general purpose exploration method. In general I think this approach shows great promise going forward score 6--&gt;7 -------The rebuttal and revisions addressed some of my concerns so I am increasing my score to 7 _____________________________________________________________________________________________________________________________________________I read the answers of authors. I increased my rating. (score raised from 6 to 7 after the rebuttal) ---------EDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning. I have read the rebuttal.The discussion was interesting, but I do not see a need to change my assessment.The example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this.I do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. Edit: The addition of HER experiments push this up a bit (5--&gt;6). I'm still concerned about how significant the contribution is (as it is a straightforward extension to SFs), but the empirical results are now quite strong. Edit: The authors have made a significant effort to address my concerns, and I'm updating my score to 7 from 5 in response.  ================ after rebuttal ====================I appreciate the authors' response and slightly raise the score. It is a good rebuttal and it has clarified several things. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. The explanation reminds me of the mixup data augmentation paper from last year. I also like the additional experiments which clearly show the benefits of GAN data augmentation. However, I still think it is borderline for several reasons.1. As the other reviewer has pointed out, CIFAR-10 is a bit too toy and some models (like LeNet for Figure 2) cannot really show the advantage of the method. I would suggest try ImageNet, and use more recent networks for ablation study.  2. As the other reviewer has pointed out, the compression ratio can be impractical. The compression ratio  depends on student-teacher training, which can take a relatively long time. 3. I would suggest the following experiments that may strengthen the paper. I would consider these as a plus, not necessarily related to my current evaluation. i) Try not use GAN, but use mixup (linear interpolation of samples) as data augmentation, and go through the student-teacher training.ii) Try evaluate the effect of generator structure for data augmentation. Does the generator have to be very strong? The GAN generated results did not improve supervised learning may suggest the generator is not necessarily to be strong. - Thanks for pointing our these results in previous work of Bucila et al. I think this deserves to be explicitly mentioned in your paper, because it provides direct evidence for your claim.- I think you should report the effect of data-augmentation *alone*. Also, adding more than one data-augmentation strategy would strengthen the result (especially for image data, where there are plenty of effective methods). That being said, I agree with a point raised reviewer Reviewer-2 that for other kinds of data (e.g. tabular data) there might not be effective ways for data-augmentation.- I am not sure how you compare the execution time of inception score compared to your methods. Fundamentally, inception score requires only a forward-pass in a pretrained model, which can be done with a small number of examples (e.g. 1K or 5K). Training a model from scratch would require a forward and backward pass, and probably on much more data, but I guess the model is much smaller than inception. Also, I'm not sure it's clear from the paper that you do only one epoch of training.I am going to raise my score to 6, because I think the paper has some interesting aspects to it. I still believe it's a borderline paper, especially that I'm not convinced of the effectiveness of compression score and that GANs can be substantially more effective than data-augmentation. Thank you for the clarifications. I feel that the material is now much more convincing after seeing the architectural presentation. It is illuminating to note that one can break up content and style to capture their essence as can be seen in figures 2, 3, 4 and 5 in the appendix. Fig 2 uses multiheaded attention to compute similarity between ref. embedding and randomly initialized tokens - this seems to be a new addition to the previous GST works (Skerry-Ryan et al 2018 and Wang et al 2018). Overall, This work exhibits a very high level of application - attention based seq2seq modeling with Tacotron setup, and manipulating content and style with instructive use of techniques from the formulation to the architectures used . I rule this as a clear accept ---I thank the authors for their hard work addressing issues raised by the reviewers.Authors have answered many issues pointed out (by improved performance and showing robustness to hyperparameters) and I've increased my score from 5 to 6, and support accepting the paper.  ---Edit after rebuttal: Increased score from 6 to 7. (increased score from 6 to 7) ====== Comments after author response ====Thanks for the detailed response. Having read the update and the other reviews, I have lowered my score.  I am still left unconvinced on (at least) two aspects.--> It is unclear to me as to how a method can improve upon minibatch-SGD can actually have better generalization. While nothing theoretically rules this out, it possible means that that either (A) the model architecture was sub-optimal and there is room for improvement, or (B) the optimization found a minimum that is not very good. In either case, there appears to be a different "lesson"  than the one described in the paper. This is certainly worth exploring in more detail.--> I agree with some of the other reviewers that the experimental results can be improved, specifically, the usage of just 2 replicas (now clarified in the paper) is limiting, and a more detailed analysis regarding how to tune the regularization parameters.I do think that the paper is on track towards an interesting discovery, but I would like to see a deeper/more detailed analysis to be convinced.   Update: I have read the authors' response, and have decided to keep my score as-is ====== Edit after author response ====I have read authors' responses and have decided to keep my score as is.The additional experiments haven't addressed problem formulation issues (W1, W2, W5) if the paper is positioned as more of a theoretical work; if the paper is positioned as a more of an experimental work, the baselines used (W3, W4) need to be improved with proper hparams settings. ================================================================================================After reading the author's response and the other reviews I still lean slightly towards acceptance and have therefore left my rating unchanged.While not being an expert on the subject, I find the work interesting. In case the paper gets rejected, I recommend to the authors to the feedback provided by the referees to clarify the narrative of the paper.  ======after rebuttal=========Thank you for answering my questions.  I understood these points. The authors added a new simple experiment and a code, whereas the manuscript at the current stage can improve the clarity. All things considered, I increased the rating.  ## Post-rebuttalI've read the authors' response and other reviewers' comments. The response and the updated version clarify my concerns. So I slightly increase my score. My final recommendationI remain my initial score after the discussion. Summary After Discussion Period:After corresponding to the authors and reading other reviews, my assessment hasn't changed much, which is that the paper is a good line of research but still needs improvement readability and strictness of assumptions.The authors and reviewers all point out that this work is a relaxation over some previous works, e.g. Jin et al. Yet [1] has assumptions which are relaxed further than in this paper, and show that at regret bounds are possible with weaker assumptions.The author's correctly point out their algorithm is computational efficiency while [1]'s algorithm isn't, which is a point in favor of the author's algorithm. Unfortunately, the benefits in computational efficiency were not clear to me and none of the other reviewers highlighted computational efficiency as one of the algorithm's strengths. If indeed one of the author's algorithm's main advantage over other work wasn't clear to the reviewers, then the paper still has room for improvement in readability. -------Based on the author response, the rating has been updated (see comments below). Edit:- the authors meaningfully addressed the above points. I have raised my score accordingly. # Post-discussion update The authors have significantly updated the paper during the discussion period. I have seen the changes, but they unfortunately do not substantiate the claim that the proposed methods are learning anything meaningful. In the new results in table 1, the train accuracy now is better than random, but the test accuracy on the unseen environments is worse than a model that makes uniformly random predictions. This implies that the proposed method is not learning to ignore the spurious features at all. It might seem from table 1 that C-VIRMv1 is performing well --- it has 46% accuracy on the test environment after all. However, C-VIRMv1 also has the worse performance on the train set (50% accuracy). In-fact, the test or train performance alone does not mean anything in this benchmark. The goal of the benchmark is to do well on the train set while also generalizing to an unseen environment. Doing well on test set by doing poorly on the training set is not progress. The new results in table 3 are equally troubling. The authors claim that the results in table 3 show that C-VIRM with EIIL is performing better than IRM with EIIL, but the data does not support the claim. Both IRM and C-VIRM are performing similarly; all the results are with-in error margins of each others, and the table can not be used to make any claims. I would encourage the authors to do a more systematic study of the proposed method. Investigate if the proposed methods are learning to ignore the spurious correlation at all (Table 1 suggests they are not) and only make claims that are supported by the data. In its current form, I cannot recommend this paper for acceptance.  **edit, based on revision and after discussion**.  Thanks to the authors for answering questions 2 and 3 in discussion and adding an experiment to the paper validating the tropical pruning method in a small setting.  The additional experiment provides some evidence that the tropical pruning method is working as argued, and I change my rating of the paper to 6. Update:The revisions are good. The paper is very easy to follow and most of the story is pretty clear. Theory sections are clearer as well. So I'll improve my score as the authors followed through with both mine and other reviewer's comments. There's one hitch: Pr1, the one having to do with the labels, is not well substantiated in the paper, though it gets first-class treatment in Fig 1 as well as being one of 2 main principles guiding this method. Some of this is carried over from the de-biasing work, but I have concerns that there's essentially a trade-off between hardness and label distribution depending on beta. Unfortunately, this paper does no empirical analysis on the labels in q, and I worry that readers may be mislead that something close to Fig 1 might happen in practice.==== *******************************************Final decision: I would keep my score unchanged. As for Principle 1, the authors said that upholding Principle 1 is impossible with no supervision, and they proposed to uphold Principle 1 approximately. This is acceptable to me as they build on ideas from positive-unlabeled learning.This paper is clearly written and well organized. Both empirical and theoretical analysis is provided. The feedback addressed my concerns well. ========================================================================I thank the authors for their detailed rebuttal. However, their accuracies are still very below sota. Therefore, I am inclined to stick to my original review and rating. Post-rebuttal:I appreciate the additional ablation study, but unfortunately the results did not strengthen the paper's distinction from related work. The explanation of the motives and related work comparisons only clarified differences between this paper and prior work that are either inherent to the task being addressed, or contribution unsupported by experiments. Unless the AC agrees with the authors that the paper is acceptable even without external comparisons (despite being a merge of two lines of work), I will not be changing my score. -----AFTER REBUTTALI have carefully checked the author rebuttal, and it addressed several of my concerns. I thus improve my score to this paper. =================== Post Rebuttal ======================================I would like to thank the authors for the feedback.I realize that the diffusion kernel used in this paper is different from the AR filter in the CVPR 2019 paper, but I was misled to think they are the same because of a wrong claim made by the authors. In their paper, right below Eq. (11) (of the latest version), the authors state that the limiting case of the diffusion kernel is the solution of the classical Laplacian regularization problem, which is not true. The solution of this problem is exactly the AR filter with alpha = 1, and it is not equivalent to the limiting case of the diffusion kernel. As such, I increased my rating of the paper, but I still think the contribution of the paper is limited due to the following reasons.1.There is not much novelty in using the diffusion kernel as the graph filter. The diffusion kernel has similar response function as the AR filter. If you plot the response functions and observe the functions within [-1, 1] (range of the eigenvalues), you may see they nearly overlap. Yes, the diffusion kernel filter let more high frequency signals pass, but the amount is quite small, and it does not make much difference. The theoretical claims are not insightful either. Claim 1 is obvious, and the conclusion of Claim 2 applies to other polynomial filters such as AR (AR can be expanded into a polynomial form, the CVPR 2019 paper also proposed a fast computation method similar to what used in this paper). 2.The performance of the diffusion kernel is not significantly better than other graph filters. Notice that, it is not fair to directly compare with the results in the CVPR 2019 paper, because those results were obtained without using the validation set (which normally contains 500 labeled examples as in the original GCN paper). Also, the results in the present paper are obtained by running through a wide range of k and selecting the best. Given same conditions, it is quite likely other graph filters can achieve the same performance. Also, some recent methods such as GMNN and GCNII achieved better results than the present paper. I hope the authors dont think they are treated by harsher standards. The novelty of the CVPR 2019 work is not mainly about proposing a new filter, it is about providing insights into/unifying popular semi-supervised learning frameworks such as label propagation and GCN. Especially for GCN, there was no similar analysis on GCN two years ago (an early version of the CVPR 2019 paper is online (https://openreview.net/forum?id=SygjB3AcYX) months before the SGC paper). *Update after discussion*: The authors have addressed most of my concerns, although not always satisfactorily. They made a considerable effort running additional experiments to provide with additional standard deviations of the accuracy in CUB-200, as well as provided results achieved using a ResNet-101. They also clarified some of my concerns regarding the datasets used. In some situations, the benefit of the proposed approach versus already existing methods is not clear, but in others the experimental evaluation shows clears benefits. Given this, and the fact that the paper is well written and motivated, I am increasing my score. After rebuttal:Thanks for your response. I read authors response to my question and as well as other reviewers feedback. I will keep my rating as it is.* no effect on the rating: a point on the question on the *generalized* zero-shot learning as a downstream task, is if employing this approach, improve the performance on unseen classes while is not negatively impacting the performance for seen classes.  ---- Post Discussion ----The discussion with the authors improved my understanding of how the paper fits with recent work. ### post-rebuttalThe authors have addressed most of my concerns, thus I will increase my score from 5 to 6. # Update post author response:Thanks to the authors for the response. The newly reported results (specifically, those of Appendix E.2) satisfactorily address my concerns about both the generalization of the pruning method v.s. re-training scheme results, both in terms of sparsity levels and unstructured/structured pruning (though the observation of the relationship between R-CLR and FT do not hold quite as strongly for unstructured pruning, they do hold at high enough sparsities to be interesting). Ive raised my score to a 6 as a result. ## After Rebuttal- I thank authors for considering my suggestions. I increase my score to 5. Having a quick look (I am sorry that I didn't have more time) at the new results; most results on structured pruning seem to agree with [1]; with some improvements over baselines when CLR is used when training from scratch. Results on unstructured pruning seems minimal and focuses mostly on one-shot pruning; and furthermore the baseline suggested above (i.e. scaling the entire learning_rate schedule)  is not added to the iterative pruning results. Overall, I like the direction of the paper, but I think the motivation should be improved and results should be distilled. --- After reading the authors' response ---The authors' response addressed some of my concerns. However, I could see that some of the concerns regarding novelty, relation to the prior work and experimental results are shared among several reviewers. Thus, I keep my original score and I believe that the revised manuscript would benefit from another round of reviewing. # After rebuttalI would like to thank the authors for their extensive efforts during the rebuttal. My main concerns are resolved, so I change the rating to borderline accept. I encourage the authors to update the paper with the results provided in the rebuttal, especially the explanation of the novelty and the results for "Proposed metric with existing search algorithms". After rebuttal:The authors' response addressed some of my concerns and I'd like to adjust my rating to marginally above. After rebuttal- I read the response of the authors. The response addresses most of the concerns raised in the reviews. --------------------------------------------------------------------------------------------------------------------------------------------------The author response addresses my major concern on the experimental results. Therefore, I have updated my rating from 5 to 6. ----------------------------------------------Following the author's rebuttal I think the paper has benefitted from further experiments and from further clarifications. I would like to thank the authors for carefully considering my feedback and for modifying their paper in the directions I suggested. Ultimately, like I said in my original review, I think this is a very interesting and well-motivated problem, but I still have a few doubts. In particular, the doubt about the paper's use of the term of "conjugate" remains. In their rebuttal the authors use the term approximate-conjugate prior, but I am not sure that this is satisfactory as being conjugate means you have knowledge of the form of the true posterior's closed form, which is not the case for BNNs. I have increased my score to reflect that I think the authors are moving in a promising direction and I hope that they will continue with this work. One thing I will note on the experimental side of things is that having greater variance is indeed interesting, but it may or may not be correlated with increased uncertainty and this may be interesting to investigate in a future version of this work. Updates:The author addressed my concerns about the experiments. Though the improvement is marginal and I still have some concerns, Im ok to accept the paper. Ill change my score to 6.======================== After author response:I would like to thank the authors for providing response. For the second point, I am still a bit confused about spectrum of perturbations vs log-spectrum of adversarial examples. The authors agreed that my third point was correct, i.e., the current results do not give enough insights on how to improve robustness against adversarial examples, in the real-world white box setting. Given the above reasons, I decided to keep my score. ----------------------------------------------------------------------After Rebuttal: I would like to thank the authors for answering my questions and addressing my concerns on hyperparameters. I also appreciate the authors' efforts in the additional ablation study conducted. While I still do think that the novelty of the paper is a little bit lacking, I think the experiments are carefully conducted and the empirical results seem to be encouraging. As such, I have raised my score to 6. #########################################################################POST-REBUTTAL RESPONSE:I found the author's further work on (1) the mode-collapse experiments (2) quantitative comparisons with other slicing methods interesting and convincing. I have decided to increase my score. I reiterate that I have not checked the mathematical content of this paper in detail. __post-rebuttal__The responses have been persuasive enough. I am raising my score, with an expectation that the authors will make additional textual revisions based on their responses to make it clear in the abstract and introduction that (1) authors only consider the "reconstruction-based" SSL, instead of SSL in general, and (2) address the discrepancy between the practice and the proposed framework. (I thought asking questions could make the authors revise the manuscript, but unfortunately that did not happen.) ------Update after rebuttalThanks for the author's response. Combing the author's response (though the authors didn't upload new versions to address my comments -- include more intuitions and discussions) and other reviewer's comments and discussions. I suggested this paper being marginally above the acceptance threshold. [Post rebuttal] I have increased the score of my review to 7. Below is a copy-pasta of my comments post discussion:While my original concern about how much sampling affects FSL is still not fully addressed, I think it is not a trivial question to answer and a full exploration of the topic could constitute its own paper. So although I'm not fully convinced about the motivation of this paper, I think the thorough experimental evaluation along with the strong empirical results together warrants publication. From my perspective, a particular important strength of this paper is its ablations. I'm fairly convinced that the presented implementation is likely the best way to implement the idea of cross-episode attention + distillation. I think the baseline proposed by the AC makes sense. It would be great if that could be incorporated into the final version of the paper. A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity. Results on wider datasets could corroborate this hypothesis. ===================Post Rebuttal Update:While I think the proposal is interesting, I still think the proposed methodology lacks a clear presentation of the studied (causal inference) problem and statement of its underlying assumptions, as it has also been pointed out by other reviewers. Despite some clarifications from the authors I still vote for rejection as I believe the paper requires a major revision. Post-discussion update: The authors only partially adressed my concerns in their rebuttal. The paper suffers from lack of comparisons: only 2 baselines are compared, and only on few systems. Crucially the new Navier-Stokes experiment lacks comparisons. The authors also couldn't respond to my questions about research context or scope: it's difficult to assess what this work actually claims in relation to competing methods. For a machine learning paper this is not enough.----- **Post Rebuttal Comments**: The authors improved the paper during the rebuttal but the clarity is not sufficient and the results are still puzzling. I do not fully understand how one can learn the perfect solution with only 3-4 data points. ---_Updated review:_>The updated manuscript has some substantial improvements.>> I feel the biggest problem is that the authors didn't clearly state the problem settings. If I understand correctly, in their framework the equation is fixed but unknown. The training data are several points in the domain (with parameters input) and testing data are other points. So basically, we doing interpolations. But even the PDE is unknown, they do assume some structure of the PDE, I think.>> Other PDEs frameworks are either 1. solver-type: the equation is known and fixed, they directly solve for the solutions. 2. operator-type, the equations are unknown and changing. Train on inputs-outputs for several equations, and test on others. Their setting is quite different. I guess it's the reason their performance is much better in the updated comparison. On the other hand, it's also hard to evaluate their performance since there are no fair benchmarks.> In general, I feel this paper is novel and concrete, while it's not very complete and well-presented. I agree with other reviewers that this paper is not ready to publish. ---Post rebuttal---Thank you for the detailed response. Overall, I think the proposed work provides a valuable benchmark for testing generalization ability of RL agents. However, I agree with R3 regarding the writing being dense/difficult to follow. I keep my rating unchanged (Weak Accept).---- **Acknowledgement of author rebuttal** I appreciate the detailed discussions that the authors have provided. However, my concerns still remain, and I am not convinced to improve my overall evaluation.  Updates: I would like to thank the authors for their response and the updated draft. Unfortunately, I believe the above major concerns are still valid and therefore retain my original rating. ---I was the only reviewer who happened to imagine their threat scenario had some importance.After reviewing the authors' changes and comments, I feel that the threat scenario in the revision still is insufficiently motivated/explained.  I'm downgrading to "good paper". Post Author response update----------------------------------------Based on the author's response, I will raise my score to 5.  [Post Rebuttal Comments] Authors have done a good job for addressing my concerns, especially the additional ablation studies regarding the performance of the expert assignment module. I'm updating my score accordingly for recommending acceptance. =========== after reading rebuttal ===============I do not agree with the author's response to my first comment. Considering parameterization $X = 2U$ (here I use the notation U to avoid the confusion), then the loss function becomes $L(U) = \frac{1}{2}\\|Y-2U\\| _2^2$ and the gradient flow with respect to $Z$ writes $\dot {U}= -2(2Z-Y)$. Then we have $\dot {X} = 2\dot{U} = -4(2U-Y) = -4(X-Y)$, which gives a rate $O(e^{-4t})$, which is faster than the $O(e^{-t})$ rate achieved without using this parameterization. Therefore, comparing the convergence rate in terms of gradient flow may still not be fair and valid. Based on this issue I would like to keep my score. Overall, while the subject area of the paper is exciting, unfortunately, the execution (both empirical and theoretical) is weak. I tend to remain to vote for rejection with encouragement for a more thorough empirical and theoretical investigation of the problem.---post rebuttal---After reading the authors' response, the other reviews, and the revision to the paper, I find that my comments are not sufficiently addressed. The author did not even acknowledge the existence of the prior work, REPAIR, in the revised paper. The imprecise mathematical expressions are still in the paper despite feedback from multiple reviewers. From a practical point of view, the developed algorithm is not scalable as it requires to (almost) solve the inner maximization at each iteration (based on the rebuttal), and it only works in the significantly overfitting regime (the authors are yet to show its performance in a more interesting regime). From a theoretical point of view, the applicability of the theory is also extremely limited to the perfectly overfitting regime, which does not capture the real world. In addition, I agree with AnonReviewer4 that the proofs are inscrutable.  I regret to say that despite the fact that the subject area of the paper is exciting, I am adjusting my score to 4 post rebuttal. POST-REVISIONSThanks for the revisions made to the theoretical results. I still find parts of the discussion in Appendix F to be unclear.Firstly, how do you derive eq. (5) from eq. (4)? In eq. (4), the denominators \sum_i q_i are independent of "\Phi(x_i)", but in eq. (5), they have a dependence on \Phi through z_{i,b}. I think the change in normalization important to show the invariance principle holds (as the invariance principle requires a conditioning on each value \Phi takes), but am unable to follow your derivation.Secondly, I'm not convinced that the maximizing partition for eq (5) assigns all examples with y=1 to one group, and those with y=0 to another group. Wouldn't the maximizing partition also depend on what \hat{y} evaluates to for those examples?Overall, I'm able to see what the authors are trying to get at with this example, but unfortunately the revisions aren't sufficient to address all of my concerns regarding the theoretical results. ------------------------------------------------------------Post-rebuttal-----------------------------------------------------------Given the effort of the authors of improving their manuscript, I am improving my original score. However, my evaluation is still "weak reject" for the reasons below:(1) I still fail to see clear differences between "assistance", as defined by the authors, and the other reinforcement learning-like approaches that assume that the reward function is unknown. I can see that they perhaps provide a more organized and methodological description of how that "assistance" can happen when compared to the previous works. However, the paper lacks practical advice, exactly how should I build an agent to leverage such "assistance"? I don't think their ideas are so novel that other methods couldn't be at least adapted to work in their scenario (to include some empirical evaluation in the manuscript).(2) The paper seems a little displaced to me in this conference. The paper neither provides practical and direct guidance on how to build algorithms to leverage "assistance", nor is a survey that focuses on organizing the area and discussing differences between works. Perhaps the paper would be better placed in a "Blue Sky" track. ----------------------------------------------------------- UPDATE:After extensive discussion with the authors, I'm raising my score to a 7.  I believe their revision will adequately address the concerns I've raised.I think this paper clearly identifies and illustrates the qualitative advantages of assistance, and that this is a novel and significant contribution.In particular, I do *not* believe, as other reviewers seem to, that any of the following are sufficient reasons for rejecting this work:* The wealth of prior work on variants of reward learning and assistance * The lack of a comprehensive survey or categorization of such work in this submission* The lack of further resultsAfter reading the other reviews and responses, I am more confident that this paper makes a valuable contribution, although I stand ready to be challenged by other reviewers.  This is because the authors have argued that the qualitative benefits they describe in sections 4.1/2/3 have not been available to any of the many previous works reviewers mentioned, and no reviewers disputed this.  Furthermore, I did not find the reasons provided for rejection to be very relevant to the goals of this work.  So overall, I do not believe that other reviewers have made a strong case for rejecting this work.In my mind, the best argument would seem to be simply that the contribution is insufficient.  I think this is a common criticism of papers that do not adhere to a conventional format or "type", but in this case, it seems unfair.  I believe the intellectual contribution of this paper is rather modest, but nonetheless novel and significant.  And I found this motivation for the work quite compelling (emphasis mine):> This existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. But (as we show) the assistance paradigm can enable significantly better behavior from the agents we train! **We are hoping to influence researchers to put more effort into algorithms for the assistance domain, in order to realize these qualitative benefits, instead of continuing to work in the reward learning paradigm as they have done so far.**I would encourage the authors to explain this goal in their revision, and make sure their claims about the superiority of assistance are appropriately modest.  Overall, I think the qualitative benefits of assistance presented provide a compelling argument for more work in the assistance paradigm, *given the paucity of such work*.  But I think the overall message of the paper should be: "Given these advantages, and the lack of work on assistance, there should be more work on assistance, since it seems promising and neglected", and not "Assistance is better, so why would you do reward learning?"  And my first impression of the paper was closer to the latter.  END UPDATE ### Post-rebuttal updateI thank the authors for the clarifications and discussion. However, and admitting that I may have missed something, I remain unconvinced regarding the contributions of the paper. --------------------Response after rebuttal:Thank you to the authors for their response. I appreciate the detailed answers to each of the prior works. I still do think the paper needs to be more clear in the problem and differentiation with prior work in the writing itself, which will require a non-trivial update to the paper.On the computational results + baselines side, while the authors have run the experiment and have described these qualitative behaviors, this isn't a substitute for quantitative results, especially because this is important to show when comparing with baselines. The authors say "We have updated Section 4 to be clearer about what baseline approaches would do in the environments we have tested". It's important to show evidence that this is what the baselines actually did.Based on these points, I don't think the paper is quite ready for publication yet. ## Author response updateIn light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.The main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work. The main reason not to increase the score further is that I believe the contribution still is quite narrow.  I chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work. Post author response:I appreciate the authors efforts to clarify my questions and revise their manuscript.I am satisfied with the answers given differentiating this work from Alpha Zero as well as the additional experiments performed. I would contend though that the differences with the other environments I have provided in point 1 of my initial review are not sufficient. The three dimensions given with respect to differences with TextWorld (and hold for the other envs too) are not entirely accurate. There is nothing in the framework itself that focuses on reward maximization instead of next state probability - its the same as having a chess simulator where you can either focus on predicting the next state or just have an external reward the indicates whether or not you've won the game. It is possible to generate oracle traces, etc. equivalents to this chess dataset in most of these frameworks. Overall that is to say, chess can also be framed in exactly those three terms and given that these are frameworks and not agents, you cannot say that these three dimensions hold.This being said, in appreciation of the author's efforts for the other clarifications - I will increase my score to a 5. -----------------------Edit after rebuttal: Thank you for the rebuttal and clarifying some of my questions. I have decided to keep the original score. ##########Post-Rebuttal Feedback########I appreciate the author's thorough response and I think the additional experiments on robustness/stability make the paper stronger, so I decide to raise my score to 7. For the future version of the paper, it would be great to see more comprehensive experiment results that show the improved robustness/stability in the main text. **Post-rebuttal**I read the rebuttal and the other reviews. The rebuttal addresses my concerns to some extent (writing has improved in the revised version, but it still has some issues). So I am going to keep my rating.  --I've read the authors' response and would like to maintain my original score ***After rebuttal and discussion This paper proposes a new network quantization framework. In particular, the proposed DropBits is somewhat novel. However, it lacks sufficient and accurate analysis of SRQ+DropBits.  For example,  why SRQ can reduce quantization error has not been well motivated and explained.  The definition of distribution bias is still unclear.  I think that an accurate description of terminology is crucial and required for scientific research. Hence, the paper still needs minor polishing for publishing. I would like to decrease my rating to 5.  I will keep my score. The paper has merits, but the comparison is not fair since they have different parameters with the baselines unless they have smaller parameters like ALBERT.  ===post-rebuttal:  The authors have addressed some of my concerns, but the experimental results are still missing several important baselines.   Raising my score from 4 to 5. ---### RebuttalI thank the authors for their detailed reply. I still consider the contribution to be too high-level and to cover too much ground without going into sufficient depth. I am not sure I can follow the argument about Kolmogorov complexity. Its chain rule is also only equal up to a logarithmic factor. I will keep my score the same. Modifications after discussion:Increased score by one since the revised paper clarifies the missing details on inference and also improves the motivation. ### After author responseI thank the authors for their detailed response to my comments. I do not agree with the complexity of FGW being solved in $n^4$ which should be more related to $n^3$ for the type of distance considered in the paper (see analysis in [1]), but yet the point is still sensible for considering the minibatch version. My other comments have been adressed, and I am changing my note to the score of 7.  ---## Updates:The authors have carefully responded to my comments. Their response addresses most of my concerns. I will keep my score high. I understand that the choice of the hyperparameters can be computationally heavy, but the authors have given an idea to solve this problem. It is good to find that the Power Spherical distribution also includes the Dirac distribution as a limiting case.  -----------------------------AFTER REBUTTAL-----------------------------I appreciate the authors response. Most of my trivial comments and questions have been resolved. I stand by my initial rating. This is a solid paper. The authors clearly introduce the problem and develop a clear story with a straightforward solution. The paper ends with extensive experiments. My main concern remains: the contributions of the paper to the ML community is moderate because the story is very narrow. I think the idea can be substantially extended to solving the fundamental problems in sliced Wasserstein distances but I don't object acceptance of the paper in the current form. I recommend the authors incorporate suggestions from all the reviewers and polish the language especially Section 2 to make the paper more accessible for readers outside the sliced Wasserstein community. Thank you. Post-discussion update: The authors gave a fantastic, thoughtful and exhaustive response that did clarify all of my concerns about the paper. They also updated the paper considerably (making much better), and crucially changed the title to be very accurate the contents.I like the paper now a lot, but the unimpressive results still stand. The PDE-based image classification performs ok, but also sometimes does not work very well. This would still be ok if insightful analysis of why the model improves would be provided. Unfortunately there is almost none of this, and then the contribution is more in the engineering side than science.I would not object acceptance, but I would prefer the work to be more complete in this regard first. I raise my score to 5.---- **After Author Response** I think the authors made a good effort to address the concerns and I have recommended to accept the paper. %%% After the Author Responses and Paper Updates %%%I would like to thank the authors for very seriously considering my recommendations and genuinely attempting to implement many of them, even in their proofs. I do think their updates made the paper stronger in general. A minor general note is that many of the newly edited sections have typos and could benefit from proof-reading. Following are my final remarks:After going through the paper again, I realized a step in the proofs which indicates an extra assumption that is not mentioned in the main paper. The proof in lines (14,15) of Section 7 (supplementary material) seems to assume that the exogenous variable has a fixed distribution, i.e., despite two different models inducing the same observed distribution having two different functions f_x and \tilde{f_x}, their exogenous noise \epsilon_x must have identical distributions for the proof to go through. Similar steps are used for the exogenous noise of Y as well. These steps can only be explained by the very strong assumption that the exogenous noise term of every variable has a fixed distribution across different causal models. Such an assumption is not mentioned anywhere in the paper as far as I can see - this has to be definitely addressed. Moreover, I do not think the identifiability result is very useful when obtained under such a strong assumption. Unfortunately, I cannot recommend acceptance due to this. But I encourage the authors to pursue this direction and seek out ways to relax this condition.I had brought up the point that C is not used to induce correlation within a dataset, which takes away from the essence of the practical problem they are trying to address. In order to address this, i.e., to be able to handle the confounding within each dataset, the authors added an extra condition on the effect of confounding: They assume that once p_1(x,y)=p_2(x,y), this implies p_1(x,y|c)=p_2(x,y|c) for all c. This extra assumption allows the authors to use the machinery they developed as is with this additional argument.Unfortunately, this assumption, much like the others, is also presented in between the lines. I think it will be really helpful if the authors could explicitly write down their assumptions in a theorem environment (\begin{assumption} ... \end{assumption}) and make them very explicit rather than only within the theorems or in-text: Assumption 1: ANM Assumption 2: Exponential family Assumption 3: ... etc.This relates to the implicitness of another key assumption made in the paper: \Gamma matrix is assumed to be full rank. This intuitively suggests that the experimental conditions are sufficiently different. But this is an algebraic statement and is hard to interpret. I would recommend the authors to think about how to interpret this condition, i.e., assess how it impacts the conditional distributions - exponential assumption allows them to make an algebraic assumption here, rather than probabilistic; but a probabilistic interpretation would be more intuitive.Some of the typos that I can see: "and functions the prior of distribution of C"->"and functions as the prior of distribution of C"typo in (62): "R"->"r""We generalize the identifiable result in theorem 4.3 "->"We generalize the identifiability result in theorem 4.3 "Many of the arXiv citations are actually published in various venues, please go through the bibliography and update. ==================================================================I thank the authors for their thorough comments and experimental details. I am more satisfied after reading them. I am raising my score from 4 to a 6. **Post rebuttal**I appreciate that the authors answer my questions. After reading through the rebuttal and other reviews,  I partly agree with R1's comments and I would like to downgrade my score by 1. My main concern is the novelty of this method. I disagree that the decoupling of generation from upsampling is interesting, it seems more like an engineering problem. Besides, I found that the authors changed the images in Figure 2 in Appendix A, not only changed the order, which is not consistent with their explanation in rebuttal.  POST-REBUTTAL:I thank the authors for their response. While some of my concerns have been addressed, a few key questions haven't been answered.* The contribution is limited in novelty.* The general author response mentions that reward shaping is not used in the proposed method. In that case, Sec 3.1 seems a bit pedagogical and misleading, since MDP for AL is described in detail but is not even empirically compared with the proposed method.* Regarding the ablation studies on a different classifier, while I agree that SOTA networks need to perform well, the ablation study on a different classifier was suggested to rule out the effect of the choice of SOTA classifier in the effectiveness of the proposed method. Also, the authors respond to R1 that the gap between the ensembles' performance and each single classifier's performance is small since they chose SOTA models for the individual classifiers. This is perhaps even more reason to show how ensembling works when the base models are not SOTA.* Beyond just a statement in the response, it would have been good to see some empirical comparisons between the proposed method and, say, Q-learning-based RL methods - especially in terms of actual running time complexity.* I also agree with R3 that similar ideas have been explored before (papers cited in R3's review), and it is important to compare with those methods as baselines in the experiments.* Also, the choice of the datasets used is not justified appropriately.I stay with my original decision. **UPDATE:** My major concerns were addressed in the revised version of the paper. **Update after author response:** I appreciate the authors' efforts to address my comments. The new version reads better. However, I am still not entirely convinced by the choice of the simple baselines. Since a positive rating is already given, I would keep it unchanged.  ------------**Edit: Score raised from 4 --> 5 following discussion below.** ----Update----After reading all the other reviews and ensuing discussions, I maintain my original score. If the research question is "How does the optimal policy depend on task parameters such as uncertainty and horizon?" I believe Bayes-adaptive work answers that question. If the question is "How do policies learned by meta-RL algorithms compare to Bayes-optimal policies?" then I think more empiricism is needed (since RL2 in principle can represent the Bayes-optimal policy), or a comparison of multiple methods. Edit: I have raised my score from 6 to 7 after the rebuttal. **Update to review**I have increased the score from 5 to 6. The authors have made the paper stronger with the inclusion of stronger baselines, cleaned-up presentation, and backward transfer. I am particularly excited by the new suite of benchmarks. **Update**I think the manuscript has been further improved now and I improve my score to 7. Also since I think results on public datasets in new domains other than images may be helpful for the wider research community. Keep in mind, as written below, that other reviewers more familiar with this research field may be more  able than me to judge the evaluation quality and whether the evaluation contains too substantial flaws to allow publication.  ---## Post rebuttalDuring the rebuttal, the authors failed to handle the issues that I raised. Especially, the authors did not respond to my criticisms about the strange experiments. Therefore, I stick to my initial rating. Post Rebuttal Comments: Following the discussion in rebuttal phase and after reading all the other reviews (and authors response) , I feel that the paper is not ready for submission. While authors did address some of my concerns, the evaluation strategy seems flawed and the compared methods are not representative (as pointed out by reviewer 1). As a result, I am changing my rating from 6 to 4. Thanks! -- after rebuttal --Thank the authors for their responses.  EDIT: The author response addressed some of my concerns. In particular, it confirms that the experimental results are impressive compared to many baselines. However, I would appreciate the distinction between easy and hard confident examples much more if the authors went beyond illustrative figures and defined this concept more precisely. Without a precise definition, it's difficult to verify the paper's claims about why the method performs well. Based on t-SNE visualizations, the author response offers an alternate definition of "far away from the cluster centroids." The submission would be much stronger if it developed this idea further and analyzed it quantitatively.Next, the authors suggest that methods cannot distinguish hard confident examples from mislabeled examples using the "small loss trick" alone, and that their "momentum trick" is necessary. However, they do not present a principled argument or strong evidence to support the claim. In fact, some recent methods do show a separation between these types of training data using measurable quantities, see Figure 1 of [3] and Figures 1-2 of [4].Finally, the authors claim that reinitialization helps escape bad local optima. However, I do not see how low standard deviation supports this claim.[3] Pleiss et al. Identifying Mislabeled Data using the Area Under the Margin Ranking, Neurips 2020. https://arxiv.org/abs/2001.10528[4] Swayamdipta et al. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics, EMNLP 2020. https://arxiv.org/abs/2009.10795] The authors have not prepared a rebuttal.  ---Update after author feedback: I thank the authors for their reply. The authors have addressed all of my concerns. Therefore, I increased my final rating. ----Update and final recommendation. I still recommend acceptance of the submission. The paper is well written, results stand on its own and the numbers improve in the way described. In light of the missing comparisons to other works pointed out by the fellow reviewers I have lowered my score because I think better calibrates with the significance of the work. Combination of downstream tasks is not novel but this combination I have not seen and so even bearing similarity with other approaches the paper still stands on its own. Thanks to the reviewers and authors for their responses. ------------Post Rebuttal:I thank the authors for their response. I am mostly satisfied with the authors' response to my (and other reviewers') concerns about properly citing prior works that jointly consider coupled image generation with downstream tasks and reframing the novelty of their work in juxtaposition to them. I would like to point out, however, that the authors' statement in the rebuttal "(2) we achieve bi-directional feedback while this work only implements the feedback from viewpoint estimation task to the generative network." is technically incorrect. The viewpoint estimation network in Mustikovela et al. is directly trained with images generated by the synthesis network under various viewpoints and hence it also achieves bi-directional feedback much like this current work. The authors should clearly re-frame their novelty and make this correction in the final version if accepted.Nevertheless, I do feel that this work adds to the body of literature on joint conditional synthesis coupled with downstream vision tasks by (a) showing improvements in the quality of image synthesis achieved by considering downstream tasks and (b) by showing improvements in few-shot learning versus prior methods where only features are hallucinated, and (c) considering other applications beyond viewpoint estimation. Hence its contribution is above the acceptance threshold. I will maintain my previous rating. *Updated score based on below discussion**Updated score again based on below discussion* #### Update after rebuttal:Thanks for the detailed feedback. While I still find the technical novelty is limited comparing to [Liu et. al.]. For this reason, my score remains unchanged. ===Post rebuttal===I have read the authors response and would like to thank the authors for the clarifications. I am still inclined to keep my original score. ***********************************Post rebuttal: The author has addressed most of my questions, and the SQuAD v2 test result is on par with the state-of-the-art, partially indicating the proposed method is effective. So I am happy to increase my rating and champion for the acceptance. Post-rebuttal:Authors, thank you for your feedback. The additional results around relative speed and performance have strengthened the paper. However, I still feel that the paper still needs significant polishing before final publication (figures, grammar, presentation), and that the paper is better suited for an NLP-focused conference, and so I have not updated my final score. **After rebuttal**:  I've read the authors' feedback and my score remains the same. ------------------------After discussion:The authors do not provide rebuttal. Hence, I keep the original opinion to give this paper a weak reject. UPDATE: T`he authors' response addressed all my remarks. I've increased the score. ##################################################################Final update: Authors addressed all my comments in the rebuttal making significant improvements to the revision. I've increased my score.  edit after rebuttal: The idea of the paper was interesting to me but some motivations or choices were unclear. After reading the rebuttal and other reviews, the authors have addressed most of my concerns. Therefore, I am ready to increase my score.====== _Post-rebuttal_: I do appreciate the authors addressing my comments and updating their paper. Furthermore, the authors also addresses a lot of concerns of my fellow reviewers. As this was a good paper to begin with, I am keeping my score of recommending an accept. After rebuttalAfter reading the authors comments' and other reviews, I think that this is a borderline paper that could benefit from more rigorous experimental validation. ----------------------------------------------------------------After rebuttal: the paper seems interesting but, as I already mentioned and as other reviewers pointed out, the main concerns about this paper are novelty and relevance to the ML community. Following the author response, I have updated my score to an accept. The key contribution is likely to be of interest to the ICLR community and the paper is well executed. --- *************I want to thank the authors for their detailed comments and explanations. I agree with (3) that a global floating representation is not required, but different settings of bitwidth may require additional data paths designed in the hardware (therefore a unified bitwidth for exponent and mantissa is recommended). Given revised related work and additional practical implications, I would like to champion this paper.  ---Edit: score increased from 5 to 6 =========================== Post-Rebuttal ===========================My questions are well addressed in the rebuttal. I acknowledge the novelty in the paper and the convincing empirical results. The authors have committed to include more datasets to enhance the result completeness and add relevant but missing related work. Taking all these into account, I am adjusting my final rating to Accept. ## Post rebuttal commentThe authors have successfully answered my concerns and added the necessary experiments. I think the paper should be accepted as the support set idea is an important one to improve upon the simple contrastive idea (in particular to soften the set of positives).  .**Post Author Feedback Comments**The authors have tried to address my main concern by adding an assumption on the distributions of gradients. Essentially they have assumed that the gradient distributions are sampled from a small, finite set of distributions. I don't know how realistic this assumption is, because each client can potentially have a different distribution. However, in practice, this may be approximately true. A better assumption would have been based on probability distances (TV distance, Wasserstein distance, etc.). I have increased my score to 6 After the rebuttal.The authors addressed my concerns. I have read other reviewers' comments. I decide to remain the current score. I have read the authors' response and the associated discussions, and based on that  raised my evaluation by 1 *********************************After carefully considering the rebuttal from the authors, I am going to maintain the score based on my evaluation and also the current paper draft. Though the authors have tried to addressed the comments, the paper still requires more improvements, including theoretical novelty and experimental results. Update after author response: The changes made have improved the clarity of the paper, such as making assumptions and the threat model more explicit, and the heatmap addition provides a nice qualitative insight. However, I am inclined to agree with other reviewers that the paper's contribution is incremental. Given this I am retaining my score of marginally above acceptance. --------------------------------------------------------------------------------------------------------------------------------------------------------------------------After reading the rebuttal, I think the authors have well addressed my concern, thus this paper is good to be accepted for ICLR and I raise my rating from 6 to 7.   **Post-rebuttal**> I want to thank the author for addressing my concerns. Overall, this is an exciting paper with comprehensive ablation studies and analysis. It provides an effective multi-task training method for multilingual tasks. The authors also support the method with a strong mathematical foundation. Thus, I would like to keep my positive rating.  POST-REBUTTAL UPDATES========Thanks to the authors for the response and the efforts in the updated draft. The updated paper improves writing. The response resolves a part of the queries. The viewer yet believes the page limits should not be a good excuse to squeeze necessary information into the appendix, otherwise, as AR2 suggests, it may be more proper for other venues. I raised my rating according to the author's response. == Post-rebuttal Comments ==I am raising my score from 7 to 8, as author responses addressed my comments well (especially answer to W1 and the Figure 13) than expected. I change my rating after looking at authors response.  ------------------------------------------------------------------------------------------------------UPDATE:I have read the other reviews and the author's response. Thank you for the thorough answers - this cleared a lot of things up and I understand much better how the multi-head, multi-task and random decision planes work. I'll increase my score to a 5 because I've gotten more insight now with the additional information, and think that the paper raises some interesting points. Overall, I still tend towards rejection - even with the updated version, I still find the contributions of the paper difficult to tease out and evaluate, and not all claims in the paper are sufficiently backed up / analysed by experiments. I would encourage the authors to try and centre the entire paper more clearly around *one single* central message in the future, and present all experiments in this light, making sure that every claim is sufficiently backed up empirically.  ==================I thank the authors for the additional experiments which have marginally satisfied my initial concerns; ideally, more setup can be experimented. I keep my original rating. ================ Post Rebuttal =============================================================Thank the authors for the updates.In the latest version, the algorithm flow is clearly stated in Figure 1, and now I can understand how the algorithm works. The authors also reported additional results on running time in the latest version, which are informative. Here is what I think after reading the paper again.1.This paper proposes a novel idea. Defining directions on graphs is not a well-addressed problem in current GNN models, and using the gradients of the low-frequency eigenvectors of the Laplacian to define directions seems novel and interesting to me.  2.The insight and analysis are not clear. Section 2.4 is still difficult to follow after the updates. More importantly, I am not sure about the correctness of the theorems and corollaries.        The K-walk distance is supposed to reflect the difficulty of passing information between two nodes, and a larger distance means more difficulty. In the paper the K-walk distance is defined as the average number of times that a K-step random walk from one node to hit another (formal definition given in Page 18), which really puzzles me, because frequent visits indicate ease of message passing. Did the authors confuse hitting probabilities with hitting times?  ### Post rebuttalMy previous rating still applies. If accepted, I encourage the authors to more clearly state in the final version that their method is not applicable (without additional - and arguably inelegant - random augmentation) to graphs with degenerate eigenvalues and in particular symmetric graphs. The necessity of taking the absolute value to ensure invariance to the sign of the eigenvector should also be more clearly stated.I share reviewer #1's concerns about corollaries 2.5 and 2.6. These should be clarified or removed from the final version. Nevertheless, I think the novelty of the approach justifies acceptance. It is a simple modification that may bring the expressive power of graph networks closer to that of pixel CNNs. --- Post rebuttal updateThe authors clarified and removed problems in the paper and did additional experiments. The changes to the paper are quite substantial and I cannot make a full review again.  (Upvote from 4->5) ### Post rebuttalThe authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7. Update: I read the comments of the authors and thank them for the clarifications. The additional baselines improved the paper. I raised my score to reflect that. ## Edit on second reviewI apologize again for the tone of my first review, I sincerely tried to understand the paper but I could not when I first read it. A re-read the paper and finally understood it during the review. I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations. With the new equation (1) the paper is hopefully more understandable now. I increase my grade from 3 to 5. The findings are quite interesting but I still believe that the paper is not well written: the equations are interesting but the explanations between the equations are often unclear. One has to understand each equation and be quite imaginative to finally identify the contributions of the paper (even for somebody only "very slightly" off from the research topic). I have read the author's comments and I stand with my previous rating. I believe the paper addresses an interesting problem but lacks sufficient analysis due to the realizability assumption A2 which doesn't apply for the given problem and the other reviewers feel the same way. Unlike in online learning and MAB, the memory used by the proposed Neural-linear bandit significantly deviates from the A2 assumption. These should have been analyzed empirically or theoretically to understand the impact of the past history on the regret.  *** Update after rebuttal ***I have read the rebuttal and I deeply appreciate the detailed response by the authors. I especially appreciate the introduction of a number of experiments on a simple domain that help to illustrate the main point of the paper.At the same time, I believe that some serious issues still remain unresolved. For example, my main concern remains: I believe that wrapping the problem in the embodied setting is not introducing additional insights. I must clarify that I fully agree that studying how embodiment affects cognition/behavior is an extremely important and exciting area of research. But in the present paper, the models have no chance to benefit from embodiment (since there is no prior / shared embodied experience), but rather have to solve the task despite being placed in an embodied setting.The main insight (in my opinion) is the observation that Zipfian intent distribution together with energy costs could be a good basis for zero-shot communication. I think that additional experiments that the authors introduced help to strengthen this point, although more experiments could still be beneficial (e.g. systematically varying the population size), as well as a more thorough theoretical discussion. At the same time, the limitations of the main "embodied" experiment remain (most importantly, the fact that fairly high accuracy can be achieved because of the unique "do nothing" action trajectory). In short, a large part of the paper (on embodiment) contributes relatively little in terms of its impact and conclusions that can be drawn from it, which necessarily limits the extent to which the main insightful point can be explored. The main point is truly interesting, however, which makes the paper borderline.Overall, I believe that the paper is extremely promising and I would love to see an expanded version published. I feel very torn about the decision, but at the moment, I believe that the paper is still below the acceptance threshold, although only marginally. I am happy to adjust the score up, and I regret that I can not switch it to an "accept" recommendation.As a minor aside - the newly introduced Colab Notebook does not fully run and crashes at the cell #4 (model loading), so I can not fully explore the newly introduced experiments. That being said, I think that after fixing, this resource can be very useful in the future. This minor issue did not affect my evaluation. **Post-Rebuttal**After reading the rebuttal, the updated manuscript and the other reviews, I became *less* convinced about this paper. While I remain positive about the conceptual idea of learning a unified label space. The authors did not successfully convince me in improving the understanding of the method. The pseudo-code alone does not make the  algorithm better to understand. It seems that the space T is insanely big (100x100x100 when three datasets of 100 labels are used). Also, while the authors do add a requested baseline, it is hardly compared (afaik only in the sentence that the expert human obtains 659 labels, while the learned space contains 701). Also note that the 'human expert' is a *"best-effort" mapping, which can be a good starting point.* according to the RVC dev kit. So, this might be an overclaim.Finally, please carefully proof read the paper, main typos remain.  Additional comments after the rebuttal phase: On the positive side- the authors are clarifying a few things in the rebuttal and also in the paper such as table 1 - for that table results are clearer now by updating the table and caption. On the negative side, however, I am less convinced after the rebuttal than it looked to be prior to the rebuttal. Let me give some of the most important things- claiming that the results in table 2 and 3 are statically significant is hard to believe. The authors claim that they are but it is not clear what the mean by "standard deviation in mAP for each of the datasets... is within .1mAP" in their rebuttal - what is varied to get this standard deviation? Learning a detector e.g. with different random seeds will result in much larger differences than 0.1mAP - thus claiming this it is statically significant is actually not scientifically credible to me - while I understand the arguments the authors make while standard evaluation metrics are potentially not ideal or comprehensive, I am still quite strongly unconvinced about the novel metric mEAP - that seems very specific for the setting used here and does not lent itself for easy understanding and is also dependent on some threshold that is not clear how to choose. In that sense all the experiments using that particular measure are still not convincing to me. Additionally, the authors do not make a real attempt to make this measure more accessible in any way. It is mostly stated that mAP does not work. Even though, as the authors point out, for most labels there is a "joint" label across datasets which allows to evaluate that directly at least for those joint labels (and these are the majority of classes apparently). Also in the rebuttal the authors simply defend their metric rather than to acknowledge that this is not particularly useful. As said - rather unconvincing and I am sticking with that. The rebuttal is making me even less convinced about that metric as no attempt is made to show that the metric is sensible and fair. - I strongly recommend to NOT use the term zero-shot. It is not only confusing as mentioned before but also does not really apply for most labels (the authors mention themselves that for most labels there is a corresponding label in the other dataset) - thus is more of a domain-adaptation or label-adaptation problem than really a zero-shot setting. The authors defend the usage of the term zero-shot which I do not find praticularly unconvincing. minor- the so called "zero-shot cross-dataset generalization" setting is not properly defined. It is mentioned at the beginning of sec 5 without being properly introduced what really is meant. - typo first line sec 4.1: detectpr -> detectorI really would have liked to see a strong rebuttal given the good results for the ECCV challenge and the importance of the problem. However, the rebuttal nearly caused me lowering the score. So overall the rebuttal has made me less convinced about the paper than before. Sorry to say.  Update: I think the authors did a great job of addressing my concerns, I'm happy to increase my score to 6 Updated review--------# SummaryThis work proposes an approach for model-based optimization based on learning a density function through an approximation of the normalized maximum likelihood (NML). This is done by discretizing the space and fitting distinct model parameters for each value. To lower the computational cost, the authors propose optimizing the candidates concurrently with the model parameters. Each model's distribution is encoded as a neural net outputting a scalar which is then encoded using a thermometer approach using a series of shifted sigmoid. Candidates are optimized based on the average value of the scalar of each model evaluated using parameters obtained from an exponentially weighted average of its most recent parameters.# Reason for scoreThis work proposes a reasonable approximation to an interesting estimator and demonstrate it is capable of achieving good consistent performance. This is likely to be of interest to the community and, as far as I'm aware, is sufficiently novel. Given that I see no noteworthy issues and all of my major concerns have been addressed, I don't see any reason for rejection. I strongly support acceptance.# Pros* Using estimates of the NML for model-based optimization is an interesting idea.* This work shows that the NML can be successfully approximated with a relatively coarse discretization and that both the optimization of the candidate and the various model parameters can be optimized in tandem. This suggests that this type of approach is viable and possibly warrants further investigation. update:---Overall speaking, the added GP-BO results address my  concerns, and I've updated the score from 5->6. A final update will be given later. Update after rebuttal:Thanks for the response! Combining deep generative modeling with evolutionary algorithms is a very interesting idea in general. I hope the authors can continue to improve the paper (especially on the evaluation part) and resubmit in the future.  ----------------------------------------------------------------------------------------------------Comments after rebuttal:I would like to thank the authors for their response. I am keeping my score, but I encourage the authors to resubmit after improving the things that were discussed (most importantly, using better metrics and comparing to more established baselines); I think this will make the paper much stronger. --------------------------------------**Update**: After reading the assessment of other reviewers and the referenced papers in the intrinsic reward literature, I am reassured that the methods/metrics proposed in this paper are not novel and, as pointed out by other reviewers, have been studied under other terminologies in different prior works. The analysis of these metrics' correlation with human data is still an interesting piece of result but is not significant enough to become the sole contribution of an ICLR paper. Therefore, I move my initial assessment of 6 to 4. ---Edit after rebuttal: Changed score from 5 to 6 (see below) Update ==After reading the rebuttal I have left my score unchanged. I appreciate the clarifications, but am very concerned about the result that the pixel-based models perform worse than the identitiy function in the FPA metric. When a model fails a sanity check like that, I believe the causes and consequences need to be thoroughly investigated. In its current form, the paper does not provide that. Rebuttal Update #####I thanks the authors for their responses to my questions. They were very helpful, and I think the work, when explored further, would be a great submission to a future conference. However I do share sentiments with other reviewers about following set of issues.(1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in-depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them.(2) The scope of this paper is a bit narrow where the authors only evaluated the methods on PHYRE that is fully-observable and only contains open-loop tasks with rigid objects of simple shapes in 2D space.###### === Post rebuttalHaving read the rebuttal and the reviews from other reviewers, my rating remains the same (5: Marginally below acceptance threshold). Many of the reviewers share similar concerns:(1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in-depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them.(2) The scope of this paper is a bit narrow where the authors only evaluated the methods on PHYRE that is fully-observable and only contains open-loop tasks with rigid objects of simple shapes in 2D space.(3) The conclusion that the pixel-based model does better than the object-based counterparts may be a bit controversial. The observation is very specific to the methods and environments the authors were using and may not hold when generalizing to more complicated partially-observable or 3D scenarios. Update: Thank you for the feedback and the efforts in revising the draft. Some of my questions were clarified and many existing errors have been fixed. However, I still think the novelty is limited, and more needs should be done to enrich the method with more analysis in terms of theoretical and mathematical aspects. Based on the above reasons, I do not intend to increase my rating. Update: After discussion with the authors, I am inclined to lower my score. While I find the architecture proposed by the authors to be interesting, I do not think they have done enough to motivate the connection with neural networks. I find this especially troubling since the language used by the authors continues to imply that the connection is obvious. I would encourage the authors to look into the literature on scattering networks (e.g. https://arxiv.org/abs/1203.1513) for another approach to explaining networks from first principles that, I think, does a better job of making the connection to realistic neural network architectures.  Edit:### RebuttalRemark 1:  * Figures are much clearer; * about Figure 1, I see that MNIST has been added, which is good;Remark 2: * "We can always normalize inputs such that they are distributed as N(0,1)": this is not true. At best, we can put the mean to 0 and the variance to 1. It does not mean anything in terms of distribution *shape*; * I maintain that experiments with MNIST were missing in Figure 1 in the first version of the paper. This is more important than pretended in the rebuttal, because, in the MNIST dataset, the pixels are not N(0, 1) (even when centered and normalized), so MNIST does no meet the approximation made in Annex A.2. However, the results are not too different from those obtained with the other datasets.The remark about the limitation to piecewise linear NNs has not been correctly addressed by the authors. The setup used in this paper corresponds to "Piecewise linear NNs", which is *really* limiting, since the biases are supposed to be zero. This is very different from "Piecewise *affine* NNs", which are actually widely used (biases can be non-zero). Adding the influence of the biases would break the entire computation made in this paper (or, at least, would necessitate more effort to take it into account). I assert that this limitation is not discussed in the paper, and my attempt to discuss it here has been eluded in the rebuttal.Despite the new and more fashionable figures the authors presented in the rebuttal, I am disappointed by its lack of accuracy and vagueness, especially when discussing the "piecewise linear NNs". So, I lower my rating. ---## UpdateI have read the revision and the rebuttal. I have also re-read the initial submission for comparison.In the revised version, the authors have added "(4) Tune $\lambda_1$ such that $CP_{\mathcal{D}'} > 1-\alpha$  where $\lambda_2$ and $\lambda_3$ are fixed from (3)." after (3) in Algorithm 4, which substantiates their claim about the marginal coverage guarantee. As my other questions under #1 were all in response to the apparent absence of a valid calibration procedure, with the introduction of this line in the revised version, I have no further complaints about the correctness of the procedure itself. I still strongly recommend including Algorithm 1 in the main part of the paper, as a prediction interval is rather meaningless unless the associated coverage level is also known.The biggest reason why I am keeping my score as is that after going through all the reviewing material, some of the recurring questions appear to be pointing at a larger issue with the submission.1. It is repeatedly emphasized that the proposed method "outperforms the state-of-the-art algorithms on high-quality PI generation." This is great, except that it is hard to see *what* about the method is causing this improvement in performance. Is it the $L_{CA}$ component? Is it some non-obvious differences in architecture or in hyper-parameter tuning? Why should there be such a difference in practical performance for the simultaneous training vs a "decoupled" approach, leaving aside the practical concerns such as the computational cost?Now that I have been thinking about this paper for awhile, I suspect that a great deal of the questions that the other reviewers and I have been asking are really about this need for *some* explanation for the improved performance. In my opinion, the current version does not provide enough evidence to *convince* the readers that the excellent empirical performance reported in Section 5 is an inevitable consequence of their novel method. This makes me cautious.2. Throughout the review process, I couldn't escape the sense that the authors themselves have not settled on the central message. On this point, I am with R3. There is a lack of clear messaging on whether the focus is on (a) high-quality PI generation or on (b) estimating conditional coverage or on (c) both. About 3/4 of the way into the paper in my initial reading, I received the impression that the paper was definitely about (b). However, I revised my opinion and switched to (a) after going through the experimental section. After reading the first batch of the comments posted by the authors, I thought that the paper must have been about (b) all along. The last comment posted by the authors threw me into doubt yet again, however, as it seemed to indicate (c) as the correct conclusion.In my opinion, both these issues need to be addressed before this otherwise interesting paper can be ready for publication. After author response:I disagree with the discussion on MSE. For the empirical estimator you mention, we have:$$E[(Y - \hat{P}(X))^2] = E[(Y - A(X))^2] + E[(A(X) - \hat{P}(X))^2]$$Importantly, $E[(Y - A(X))^2]$ is a fixed value regardless of what $\hat{P}$ you use. So while you cant compute $E[(A(X) - \hat{P}(X))^2]$, you can compare whether this is higher or lower for a particular $\hat{P}$ by just comparing $E[(Y - \hat{P}(X))^2]$.By the way, this is directly analogous to classification. In classification, Y | X is stochastic, it is 1 with some probability A(X) and 0 with probability 1 - A(X). Indeed, we cannot measure $E[(A(X) - \hat{P}(X))^2]$ directly - instead we estimate $E[(Y - \hat{P}(X))^2]$, but thats just off by some fixed value (which does not depend on $\hat{P}$).At a higher level, there isnt really a distinction between classification and the setting here. Let f(X) be your confidence interval, and introduce a random variable A given by A = 1 if Y \in f(X) and A = 0 if Y \not\in f(X) be a random variable, then we are precisely estimating P(A = 1 | X). This exactly corresponds to classification, where the label A is either 0 or 1, and we are estimating P(A = 1 | X).As such, its important to compare with standard baselines (e.g. the 2 stage approach). Use the neural network features instead of training the coverage estimation model from scratch in the second stage, and show the MSE and calibration error values.I still think its unclear there is much interaction between the high quality confidence interval and coverage estimation. As the author response says, setting $\lambda_2 = 0$ and turning off the Ca-module, would not affect the confidence intervals produced. After the Response by the Authors ====Thank you for the detailed reply. For the clarifications the authors made to the algorithm description, I will increase my score.The authors state "If the scientific community had waited for deep learning to prove that it could discover the true conditional distribution of outputs given inputs, we would not have had the progress we achieved in the last two decades in AI. We believe that it is important to take into consideration all sources of evidence about the usefulness of a method, and experimental evidence is at the heart of the success of the scientific method and should not be discarded because of an established cultural habit of relying on proofs of identifiability."Note that the objections I (R1), and I believe also R3 and R4, have are not about theoretical vs. experimental research and that the paper lacks proofs or identifiability results. It is perfectly fine to not have a theoretical understanding of a proposed algorithm. But the authors should be able to justify the choices they made in the algorithm design, and especially in light of the prior work. The main justification given by the authors both in the paper and in their rebuttal is that the algorithm performs well. I believe the paper needs an iteration to address these issues.The following is my detailed feedback in addition to my original review in light of the authors' response. I hope this will help the authors in improving their paper.On fully learning the causal graph: I suggest the authors examine and try to identify, in small graphs, what aspect of their method allows it to perform better than the existing methods such as JCI or allows it to go beyond the existing equivalence classes. Without such justification, I do not think the paper in its current form will influence future research.Remark on interventions having variety: This is not sufficient for exact recovery. Imagine intervening on the same node with different mechanisms over and over. This does not allow recovery outside of the local structure around the intervened node for most causal graphs. This also relates to the remark above. Full identifiability is always related to having variety in the intervention targets and not just in interventional mechanisms. This is why some of the datasets where the exact graph is recovered by the algorithm need a detailed investigation.About synthetic experiments: One explanation for full structure recovery in the synthetic experiments could be the following: The authors randomly pick one target variable to intervene on. My guess is that this randomness in the experiment design is sufficient to have diverse enough target sets for the equivalence class to shrink to a single graph. Can you verify/check this?How many interventions do you use in the synthetic experiments? How many samples are collected per intervention? Unless I am missing something, these are not provided until page 19 but then it is not clear if these numbers are kept identical throughout the experiments. x-axis is set to be # of episodes or # of steps in most experiments whereas # of samples would be more informative.About JCI comparison: I did not completely understand why the authors could not run JCI in synthetic data. They say it is due to its complexity. But JCI's complexity comes from the graph degree and not from the number of samples for a small enough state space. It would be very interesting to compare what JCI learns relative to the proposed method in these synthetic experiments. This should test my hypothesis above that the random intervention target is providing enough diversity to reduce the equivalence class to one graph, which should be detected by JCI.Inferring a Markov equivalence class from the adjacency matrix by early stopping is definitely an interesting idea and I would encourage the authors to further pursue and formalize this direction.Sample complexity: The authors mention that their method is "sample-hungry". Given that the method presents significant divergence from the standard literature on causal inference that relies on conditional independence tests, which are known to require many samples, it is especially important to clearly present the number of samples used by the method. The main paper does not present the number of samples used in the synthetic experiments. These should be made explicit.Finally, the title and abstract still state "dependency structure discovery" and learning "Bayesian networks" whereas the authors attempt to learn causal graphs from interventions. I suggest an update to the narrative to clarify the objective of the paper. ##########################################################################Update after rebuttal discussions:- In the light of the considerable overlap with [Chiu et al., 2020] pointed out by the other reviewers, I decreased my score. I have familiarized myself with it and can confirm the said overlap. However, given remaining differences, I do not find it unreasonable to consider this paper as "complementary" to [Chiu et al., 2020], *provided that the authors explicitly address the similarities in the final version*.- I consider the sum total of contributions of the paper still tilting towards being sufficient for publication. _Post-rebuttal_:I do highly appreciate the authors trying to answer all our questions and adding more details and experiments. However, after also reading through [Chiu et al.,  SIGGRAPH 2020] I do find that this paper has a large overlap with the one mentioned. Therefore, agreeing with Reviewer #2, the contribution is reduced to applying the idea to GANs. Therefore I am keeping my recommendation. ======== AFTER REBUTTAL ========I appreciate the authors' efforts on additional thorough comparison to existing works on interpretable axes discovery. From the updated manuscript, however, it is not clear what method is superior and the authors' approach appears to be a yet another method for this task rather than generalization of previous ones. Overall, I am still on the positive side since the observed findings deliver a clear profit for GAN inversion. But I am not increasing my score given that the "interpretable axes" part has become less impressive (in terms of weaker claims and conclusions) and the competing SIGGRAPH work.    _**Update after author response:** I have read the author response, but do not find that the answers really address my concerns.  I have also not really seen any improvements in the paper itself._  ================================Post rebuttal Comments: Thank the authors for the response. After reading the rebuttal, I decided to change my rating to 5 for the following reasons:* Theorem 3.3 in the original manuscript has a major flaw which is also mentioned in the review of reviewer 3. The remedied theorem is considered as a substantial change compared to the original one.* Experiments do not fully justify the superiority of the proposed method in training deep GNNs. The performance on ogbn-product degrades when the depth of GNNs goes to 15. It is not clear whether shallow subgraph samplers really help with training "deep" models. * The empirical results are not strong enough to support the theoretical claims. The best results achieved by shaDow-GNN on ogbn-arxiv and ogbn-product are 72.28 and 80.09 which rank 15th and 9th respectively on the OGB leaderboard. The results should be improved before being accepted. final recommendations : I maintain my score. I think the other reviewer summarized it best, "interesting but immature". I hope to see this work develop and published in the future. Post Rebuttal:I thank the authors for their detailed and thorough response. All my questions and concerns were addressed and I appreciate the discussion on end-to-end learning as well as the Transporter + GNN experiment. I am happy to maintain my original rating and recommend acceptance. #### Post rebuttalThank you for providing the rebuttal. The rebuttal addressed my concern on comparing to other baselines. And it's fine to keep the design choice experiments in the main paper. However a proxy evaluation of key point evaluation is still missing and it will further strengthen the paper (I don't have a clear idea for the evaluation either). I keep my original rating of 6.  ====after author response===I would like to thank the authors for the detailed response which resolves some of my concerns about the novelty of this paper. I agree that this is the first work (as far as I know) that brings NAS to unrolled algorithms. Unlike what I initially commented, 'my score is actually between weak acceptance and weak rejection', now I am happy to rate this paper as a weak acceptance. [Score updated From 6 to 5] Post response updateThanks for the response. However, my major concern is still that the technical contribution of this paper is limited.----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Post response updateI would like to thank the authors for their detailed response. The response addresses my confusion around the use of the terminology of model inversion (I would further suggest that the author use the term data reconstruction rather than than model inversion to avoid readers misunderstanding model inversion as referring to [1]). I still have concerns around the fact that differential privacy is not used as a baseline here, which would strengthen the argument made in the response that it provides orthogonal guarantees. ----------- ***During rebuttal:The authors should highlight some technical difficulties in the paper. In principle, stochastic line search under overparameterized regime does not make things harder because the stepsize is lower bounded. The difficulty of stochastic stepsize is to control the product of stepsize and gradient, while under this regime the product is separable. It seems the analysis of momentum plus this observation is enough for the analysis. It would be useful to provide some insights and challenges of the analysis. ----------Update after author response----------I thank the authors for the detailed response. Most of my concerns have been addressed, and I decide to keep my score. ----Post rebuttal commentsThanks for the responses from the authors. These responses partially solved my questions. I think that the initialization of hyper-parameters of combination weights seem to be heuristic, and it is unclear on the effects/robustness of its initialization on the optimization performance. My questions on more comparisons and more combinations with other optimizers are not well answered. I also read the other reviews and responses, I agree with other reviewers on the concerns of experiments, justifications, robustness, etc. Considering that it needs some additional works to solve all these concerns, I suggest the authors to improve the paper and submit it to one following conference.  Update: I really appreciate the response from the authors. Some of my original concerns have been addressed, and additional experiments help to show the benefits of CAM-HD, so I have increased my score to 5. But, after reading other reviews and responses, I still believe that this work needs to be compared to advanced learning rate adaptation methods. Most reviewers have pointed out the presentation and insufficient experiments, so it's better to submit the improved version to one of upcoming conferences. After Rebuttal :Sorry for the delay. I checked the comments of the other reviewers, responses, and the revised version. The authors address my concerns in the revised version. I vote for marginal acceptance. -----------------------------------------------Post-rebuttal comments:I'd like to thank the authors for adding the experiments; the paper looks stronger now but unfortunately, the results on the new experiments are not that encouraging. Considering the results and also other reviews; I'd like to keep my score as marginally below acceptance threshold.  After reading the author's response and other reviewers' reviews, I still find the novelty of this paper somewhat insufficient. Therefore, I would like to maintain my initial evaluation. --- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows.  ---------------------------------Post-Rebuttal: Thanks to the authors for the detailed response. I think the idea has potential, however (resonating with the comments of other reviewers) I still think the paper should be significantly improved for better motivation, stronger theoretical results, and more elaborate and diverse experiments that can demonstrate the effectiveness of the method.  After reading the rebuttal and the other reviews I still lean towards rejection, the main reason being that I am not convinced at this point that exponential nonlinearities are a direction worth pursuing. I furthermore find the passages on dynamical systems and lie groups to be still lacking, for the same reasons as detailed above. ---------------------------------------------------------------------------------------------------------------------------I believe that the authors have adequately addressed my concerns, and have made a good effort to improve the quality of their work and so I am raising my score.  ----------FINAL-RATINGI read the other reviews and authors' replies carefully.First of all, I would acknowledge the effort of the authors in replying to reviewers' concerns.About my points, I agree with other reviewers that the quantitative analysis is a bit limited (I think it is also the main criticism), but the introduced ablation and the new Figure 6 (numerically comparing against other SOTA methods) are convincing. The authors solved also my other concerns, and so I raise my score; I think the study of this representation is interesting, and well fit the audience of ICLR.Thanks to the author for their availability, and best of luck with their work! The revised version of the paper with additional experiments has addressed my concern on the limited advantage over prior deep 3D representation.  Fig.6 in the revised version has shown the potential of the proposed approach in generating directly usable representation in downstream applications, including CAD design and manufacturing. Hence, I would change my rating to positive.However, the paper still lacks quantitative and qualitative evaluations. I hope it could be properly addressed in the final version. ===========================================================================================================Update:Thanks to the author's response, solving my concerns on this work. I decide to increase my score to 6. Besides, I suggest supplementing the mentioned discussions and empirical comparisons between Eq. (2) and Eq. (4) in the manuscript, as least in the appendix. Update: I'd like to thank the authors for their detailed response to my comments. - I respect the argument that CUT requires a model per pair of domains hence it does not scale to 10 domains. However  presenting the problem in such an aspect ("we evaluate only on multi-domain datasets such as the AnimalFaces, Food") limits a little the prior work that one can evaluate against. In my humble opinion the authors constrain the definition of unsupervised image-to-image  translation such that their proposed approach "is the first to succeed in this task" whereas I also think CUT is an unsupervised image-to-image translation method that "succeeds in this task". If the  argument was presented in the multi-domain setup I would be willing to buy it but as it is presented currently I find the first contribution as an overstatement. - As for SEAN my point was not to compare against it but that the architectural design of this work is not far from what prior work is already doing. - Sounds good thanks for the explanation. Having said that I still think this is a solid submission with interesting findings in an under-researched problem hence I do recommend acceptance.  =======Update:Thanks for the feedback from authors. Mostly my concerns are addressed. I suggest adding the discussion on the selection of K in the draft as this is important for readers to know when facing just a collection of images. Update:- Thanks the authors for the detailed reply. It addresses some of my concerns. Overall, I am fine if this paper gets accepted, given its novelty. My major concern is still with the experiment comparisons. The authors only compare the proposed method with FUNIT, which might not be fair since FUNIT is designed for few-shot translation, dealing with novel domains or classes with a few examples. The authors should compare with MUNIT or StarGANv2, with domain information obtained by clustering or any unsupervised methods.- The authors should tune down the eye-catching statement of truly unsupervised translation since translating between dish and dog might not make sense, although this is a minor issue. ---Post-Rebuttal---Thank the authors for their response. I now agree with the authors and other reviewers that the authors' approach has its novelty (self-supervised, rendering, etc.), and the ablation study in Table 1 is reliable to prove each component is useful. Therefore, I increase my rating by 1. EDIT after rebuttal period---------------------------------------many thanks to the authors for taking into consideration my comments. I decided not to change my note because I still believe the paper lacks of a significative experimental Section.   ##########################################################################After discussion:Since the authors and other reviewers have addressed my concerns, I would like to change my score. The introduction of LRGA into the RGNN is interesting with the theoretical analysis, although the network design is incremental. I am now leaning towards borderline acceptance.  =======================After reading the authors' response:i. As shown in (new) Table 3 in the revised version, the results of using the global normalization are not better than that of using the softmax layer in the self-attention mechanism. Hence the motivation is not enough.ii. To have the faster computation, we have SGC[1], FastGCN[2]. To have powerful GNNs, we have GIN[3]. Inspired by DualGCN, we can build a new combination (e.g., SGC+GIN) together with using the vector concatenation/sum-pooling/LSTM over different layers [5] to further improve the performance and have a faster computation. That's reason why the novelty of LRGA+GNN is weak.[1] Simplifying Graph Convolutional Networks. ICML 2019. [2] Fastgcn: Fast learning with graph convolutional networks via importance sampling. ICML 2018. [3] How Powerful are Graph Neural Networks? ICLR 2019. [5] Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.I keep my score unchanged. **Post-discussion update:**The authors have addressed all my comments. It is unfortunate that a comparison with diffusion graph augmentation could not be added, but I understand the reason provided by the authors and this anyway does not significantly detract from the presented results. Since my original score already marked this as a good paper recommending acceptance, it is left unchanged. __UPDATE AFTER RESPONSE__Hello authors, thanks for your response. After reviewing the updates, I'm still unconvinced. I will raise my score to a 4, but won't be recommending acceptance. I'll provide some suggestions below, but first I didn't note any strengths in my original review which was not right! So I'll start with a list of strengths.Strengths:  - This direction of trying to understand how much value dot product self-attention adds is very interesting. Synthesizing the attention matrix, rather than computing pairwise dependencies is a cool idea.  - The experiments are on a range of tasks including machine translation, language modeling, GLUE/SuperGLUE and more.  - The performance of the random synthesizer is quite surprising, the fact that it doesn't depend on input tokens but can still achieve non-trivial performance is intriguing.  Suggestions to improve:  - I still think the paper could do a better job of reporting a more complete set of experiments/comparisons. Comparing against the variants of synthetic attention is interesting but not enough given that there are quite a few papers that investigate similar ideas. Dynamic convolutions -- Wu et al. report a range of experiments on machine translation, language modeling etc. Why not compare to them on these tasks as well? Comparing only to self-attention just isn't enough since **synthetic attention is not the first attempt to replace it**.  - It seems a bit strange that dynamic convolutions are competitive with self-attention in the original paper, but results on GLUE are so much worse. It might be worth verifying on the sequence generation tasks that results are as expected. For GLUE, Linformer has results in the original paper, why not also compare against it here?  - The paper needs some revision to clarify the motivations -- it starts out by talking about how self-attention may not be necessary, but in some of the results synthetic attention has to be combined with dot product self-attention to achieve reasonable performance. **On GLUE, looks like the deterioration from using synthetic attention alone is as large as 10 points on average.** The fact that it improves performance to use self-attention and add some parameters strategically can still be interesting I guess, but the original motivation of the paper starts to fade.  - Small note: Everywhere, that the baseline is "Transformer" that's a self-attention-only variant (V), so maybe the notation/tables could clarify that point. -----------------EDIT after rebuttal: Thank you for the response.  ****Reply to authors' rebuttal****Dear Authors,Thank you very much for all the effort you have put into the rebuttal. Based on the improved theoretical and experimental results, I have decided to increase my score from 5 to 6.Best wishes,Rev 1 [UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6. ==========after rebuttal===========My main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that.  =============== after rebuttal ====================I appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. My main concern is that the benefit of this method is unclear. The main baseline  that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. The proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. Regarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity. [Edit] I changed my rating from 4 to 5 based on the author responses.======= Thank you for the updated paper. The revised version is significantly better than the initial submission and addresses many of the points raised (most importantly, it provides quantitative comparison against existing methods).  I have updated my score based on the latest iteration of the paper After responses: I now understand the paper, and I believe it is a good contribution. ================================================  ==after rebuttal==After reading the authors rebuttal I increased the my rating to 6 as they addressed some of my doubts. I still think that the studied setting is too idealized, but it is a first step towards an analysis.  === after rebuttal ===I have carefully read the authors' response. I appreciate the explanation. After reading [1] in detail, my conclusion is still that [1] seems to be a stronger framework than the current one and easily extends to the setting with gradient penalty. Compared with Nagarajan and Kolter, the contribution of this paper seems to be minor, although technically involved. I have checked the updated pdf but haven't found the authors' rigorous "more stable" argument. Revision:Thanks for the work of the authors' and all the reviewers. I spent sometime reading the rebuttal as well as the revised paper. It addressed most of my concern. I would like to change my rating from 5 to 6. Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7 After rebuttal:It's better now. However, the revised introduction still says:  "GRU has become wildly popular in the machine learning community thanks to its performance in machine translation (Britz et al., 2017) ... LSTM has been shown to outperform GRU on neural machine translation (Britz et al., 2017).... specifically unbounded counting, come easy to LSTM networks but not to GRU networks (Weiss et al., 2018)." So better remove the first statement on Britz et al: "GRU has become wildly popular ... in machine translation (Britz et al., 2017)" because they actually show why GRU is NOT wildly popular in machine translation, as correctly justified later in the same paragraph.Pending the above revision, we'd like to increase our evaluation by 2 points, up to 6! Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes. Update after revisions: The authors performed extensive work to address my concerns. This showed that some concerns (RF appearance) were valid, and the authors removed them from the final manuscript. I raised my score accordingly. [ADDENDUM: given the author feedback and addition of the benchmark experiments requested I have updated my score.] ----Update: after all the discussion, I'm lowering my score a bit while still hoping the paper will get published. I'm satisfied with the results and the improvement of the paper. I still find it a bit surprising that the pairs of literals/leaves in the tree are a good approximation for the program itself (as shown in one of the ablation study). The discussions with the two other reviewers have shown me that my enthusiasm for the matter at hand may have clouded my judgement.Although I maintain that the paper may be helpful for practitioners, in particular because of its identifying and comparing different kinds of architectures, the other reviewers do see the lack of clear novelty, the writing and some over-simplifications as important issues. Therefore, although I still believe the paper to be a good paper, I was probably wrong in my first assessment and changed it based on the elements presented in the discussions. REVISED: I am fine accepting. The authors did make it a bit easier to read (although it is still very dense). I am also satisfied with related work and comparisons I have read the rebuttal.Regarding normalization: I think that there are at least two reasonable meanings to the word 'normalziation': in the wider sense is just means mechanism for reducing a global constant (additive normalization) and dividing by a global constant (multiplicative normalization). In this sense the constant parameters can be learnt in any way. In the narrow sense the constants have to be statistics of the data. I agree with the authors that their method is not normalization in sense 2, only in sense 1. Note that keeping the normalization in sense 1 is not trivial (why do we need these normalization operations? at least for the multiplicative ones, the network has the same expressive power without them).  I think the meaning of normalization  should be clearly explained in the claim for 'no  normalization'.Regarding additional mathematical and empirical justifications required: I think such justifications are missing in the current paper version and are not minor or easy to add. I believe the work should be re-judged after re-submission of a version addressing the problems. ---- Update since rebuttal ----I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters. I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement. More experiments based on different transformations that the authors have mentioned would make this a stronger contribution. The use of beta&gt;1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta&gt;1 (and other hyperparameters such as k in Table 1) in isolation. NOTE:  thanks for your good explanation of the Bayesian aspects of the model ...yes I agree, you have a good Bayesian model of the GAN computation , but itis still not a Bayesian model of the unsupervised inference task.  This is a somewhatminor point, and should not in anyway influence worth of the paper ... but clarificationin paper would be nice. Revision: Score updated from 6 to 7. updated score after authors revision Update following author and reviewer discussion: I agree with others regarding the weakness of the empirical comparison to pseudo-counts in particular, but still believe that the paper deserves to be accepted due to the fact that (1) some of the results are really good, and (2) this is a simple original idea that has the potential to drive further advances (hopefully addressing the empirical and theoretical limitations of the current work) ### Addendum ###After an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \phi(x) gets too close to a linear function. (Many other criticisms also remain.)The one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid "structural vanishing gradients", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from "regular" vanishing gradients (as shown by "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice" and "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal. In response to the authors' rebuttal, I have increased my ratings accordingly. I strongly encourage the authors to include those ablative study results in the work. I also strongly recommend an ablative study on importance sampling so as to provide more quantitative results, in addition to Fig. 4. Finally, I hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.================================= after author feedback =======After discussing with the authors through OpenReview, the reviewer feels that a lot of things have been clarified. The paper is interesting in its setting, and seems to be useful in different applications.  The clarity can still be improved, but this might be more of a style matter.  The analysis part is a bit heavy and overwhelming and not very insightful at this moment. Overall, the reviewer appreciate the effort for improving the readability of the paper and would like to change the recommendation to ````   accept. --------------------------------Update--------------------------------The authors have provided a detailed response to my concerns and have fixed many of them in their revised version. I verified parts of the proofs in the appendix (Theorem 3.1 and its Corollaries). I congratulate the authors on their work and recommend acceptance! *EDIT: I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that "everyone knows" what is meant when the actual claim is misleading.(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.The variance of the policy gradient estimator, subject to a baseline "phi," is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from "phi(a,s)", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:"We expect this to be the case when single actions have a large effect on the overall discountedreturn (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward)."Please see Sec. 3, "Policy Gradient Variance Decomposition" of the Mirage paper for further details.The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in "A Better Second Order Baseline" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation. The authors have updated the paper and clarified some things, and now my impression of the paper has improved. It still feels a little incremental to me, but the potential application areas of these sorts of models are quite large and therefore incremental improvements are not insignificant. This paper suggests some natural follow-up work in exploring Hellinger distance and other variations for these models.  Update after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper. Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.------ Update==========Upon reviewing the paper revision and the author comments to my and the other reviewers' comments, I will revise my suggestion to that of acceptance. As I said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified. They have also increased the rigor of their experimental results by providing variances in the plots.I think this work will be of interest to the community. The previous version of the paper was not clear enough in the motivation and uniqueness of the work. After a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work. The current paper is much better, so I would like to raise my score to 6. My revised review is: [orginality and significance]+ The paper deals with a challenging navigation problem where natural language instructions can be underspecified and the environment is complex---thus a correct reward function being extremely hard to craft. + The paper proposed to use a &lt;instruction, state&gt; discriminator D to compute a pseudo reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in an adversarial way---with expert supervised data. The idea is neat, and its effectiveness is empirically supported by extensive experimental results.  [clarity]+ The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1 and -2). The experiments are also well-documented, including training and testing details, results and analysis.   [quality]+ The paper was not clear at certain points but the authors had helpful discussions with me and the paper was revised accordingly. + The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL.  Overall, it is a good paper. Post-rebuttal update:The authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).I did spot one typo on page 4 - "exterme", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.--- ====== Update 12/12/18 ======Thanks for your notes in reply. I'll just add that if the dataset can be extended to slightly greater complexity either for this version or for submission to a subsequent venue, it would be impactful. Simple extensions could include scenes with multiple flowers and classes where the explanatory factor is tricker to uncover. For example, a dataset could be created with scenes of three flowers: two of one color and one of another color, with the class determined by the color of the lone flower. The correct explanation (the color of the lone flower) is still clear, and it would be great to see if the proposed LASSO approach (or a future approach) could correctly identify those pixels. Update: I still feel that the paper should have either strong theory, strong experiments, or some of each to be accepted, but that both are lacking. The revisions required would be too great for acceptance at this time.  Update: I thank the authors for providing clarifications and additional experiments, in particular the comparison to open-loop grasping (SOTA grasp detection method from Mousavian et al.). I still find the technical novelty of the paper limited.  [Update]Thank you for the responses and clarifications. I appreciate the additional experiments in the real world and comparisons with the open-loop policy. Novelty still remains a concern however; in using a planner as an IL expert, it isn't clear what was challenging to adopt this strategy for the grasping problem and qualifies as a significant contribution. Additionally, the experiments to study 'contact-rich' and 'different dynamics' problems is unclear; the experiments don't indicate what aspects of the proposed method address these challenges and are able to do so with vision/depth-only feedback (no tactile); also the evaluation in simulation alone is insufficient to study such scenarios. I have updated my score accordingly. # Update post author responseThanks to the authors for the detailed response. The authors have satisfactorily responded to my main criticisms of the paper (primarily about the non-standard evaluation regime, and secondarily about the motivation and strength of the theoretical results), so Im raising my score to a 7, though I do think the paper would be further improved with references to Appendix G in the main body of the paper. # After Rebuttal: Score Lowered from 7 to 6## Concerns AddressedI appreciate the effort the reviewers put into revising the paper to include the settings I suggested.I am generally pleased with the revisions the authors made to the paper (especially Section 3), and I appreciate their attention to these details.## Remaining Concern: Settings in the Main Body are Poorly-Tuned and May Overstate ResultsI am concerned by one aspect of Figure 2: the unpruned accuracies for VGG-16 and ResNet-20 are much lower than they should be. VGG-16 should get 94% accuracy on CIFAR-10 (vs. 91% in the plot), and ResNet-20 should get 92% accuracy (vs. 86% accuracy in the plot).  This is because the paper uses Adam to train all networks without any learning rate drops, whereas the typical learning rate schedules for VGG-16 and ResNet-20 use SGD with momentum with learning rate drops.This important difference raises the concern of whether the results shown in the paper will translate into fully-tuned, large-scale settings. As evidence of this concern, Appendix G does show a fully-tuned VGG-16 getting standard accuracy. In this setting, LAMP is no better than global magnitude pruning until very extreme sparsities.**I have lowered my score on the basis that the results in Figure 2 may overstate the value of LAMP in well-tuned settings. I no longer have unequivocal confidence that LAMP is an improvement that should be adopted in general. I implore the authors to replace the experiments in Figure 2 with well-tuned versions of these networks that achieve SOTA accuracies.**## Overall: Score Lowered from 7 to 6I am less confident in the method's significance in well-tuned settings, and I can no longer unequivocally trust the empirical evaluation in the paper. I still support acceptance, but only tentatively. # UpdateThough the theoretical analysis is a bit weak, I think the experiments are quite good. The code can also run without any issue, which is a significant contribution in my opinion. ===================Post Rebuttal Update:After the discussions and reading the other reviews, I lower my score by one point. I could not find the changes to the manuscript announced by the authors during the discussion, especially the additional intermediate results necessary for the theorem's proof pointed out by reviewer 1. ---------------------------------------post-rebuttalI would like to thank the authors for their efforts to improve the methods and the draft. Part of my concerns was resolved. For clean accuracy, CAT-r did provide a better trade-off. However, it is improved after the submission deadline, it can't be counted into the original contribution in theory. For the concern that the comparison to the baseline presents unfairness as the proposed method was designed for the composite attack with a larger perturbation space, I think the author agrees with my point to some extend. I decided to keep my original score deal to the remaining weakness in the paper.  --- post rebuttal update ----The authors successfully addressed my initial concerns regarding more analysis and experiments on a larger dataset. Therefore, I keep my rating weak accept. =============== Post rebuttal comments ===============First of all, I want to thank the authors for answering my questions.The clarifications confirm my previous concern about the limited technical novelty.In addition, they highlight that only three participants were involved to build the real life dataset used in the experiments.In my opinion this is not sufficient to carry out a significant evaluation.For these reasons I keep my original rating. Update after author response:I appreciate the authors' efforts to address my concerns. I still find the paper's insights lacking in the critical middle ground between theory and practice, but understand that drawing these connections is a long game. I believe the paper leads research in an important direction and so can be published despite these flaws.  # Edit after author responseI thank the authors for their detailed response and for taking into account many of my comments. One issue that remains is the disconnect between toy and natural scenarios. There is some added discussion, which is helpful, but actually sketching out and conducting experiments, including intermediary steps moving from a full toy example to the naturalistic case, could be very helpful here.  +++ updates after authors' feedback +++I  thank the reviewers for their detailed response. I still feel that more work (experimental and theoretical, as outlined in my review) is needed. I also would like to make sure that my point is not misunderstood when I said that the sparse model requires twice as many parameters to be stored (the value and the index),  compared to the dense model. Using compression algorithms, the number of bits to store the model can obviously be reduced (below the factor of two). Anyways, the deeper point that I wanted to make  was the connection between minimum description length  (number of bits to store the model) in information theory and the model complexity /capacity in statistics/machine learning: see BIC in https://en.wikipedia.org/wiki/Minimum_description_lengthHence, the bits to store a model are directly related to the model-complexity/capacity in machine-learning/statistics. Edit: Having gone through the updates and the author's replies, I am increasing my score. =============== Edit ===============After reading the authors' response to me and other reviewers, I think my concerns are sufficiently addressed.Therefore, I update my rating from 5 to 6.There is still room for improvement in terms of the writing: as raised by the other reviewers, the text is a little bit dense to read. It would be great if the authors can further refine and brush-up the flow of the paper to make it more accessible. ## After rebuttalSome of the issues are addressed. So I change my rating to 6. --------------------------I agree the comment from the author mostly for my concerns.(1) From the interpolated images, a point in the latent space seems to be matched to corresponding image in the image distribution, which means that it does not simply memorizes the images.(2) By seeing the figure 8, I think this work can be tested in image generation task, either. In final version, I strongly want to see the Pure Image generation result. Based on the comment, I changed my previous rating.  After reading the rebuttal:The authors have addressed my major concerns with regard to this paper.   I have lifted my score.  Thanks for the nice response. UPDATE: after revisions and discussion. There seems to be some interesting results presented in this paper which I think would be good to have discussed at the conference. This is conditional on further revisions of the work by the authors. ---------Revision------------I have read the author's response and other reviews. I am not changing the current review.I have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this? Update: From the perspective of a "broader ML" audience, I cannot recommend acceptance of this paper. The paper does not provide even a clear and concrete problem statement due to which it is difficult for me to appreciate the results. This is the only paper out of all ICLR2019 papers that I have reviewed / read which has such an issue. Of course for the conference, the area chair / program chairs can choose how to weigh the acceptance decisions between interest to the broader ML audience and the audience in the area of the paper. ----------------------------------------------------------------------------------------------------------------------------------  ########################################### Updated review: The authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time. I have changed my score to a 6. #########################Post rebuttal: The authors have addressed most of my concerns regarding the poor presentation of the earlier version. I have updated my score. Update:The rebuttal resolves some of my concerns. However, I still think the contribution is incremental. The current version looks too heuristic, more theoretical analysis or inspirations need to be added. * I have revised my score upwards due to the authors response to my concerns --- particularly the addition of new results on graph classification. The original review remains here, and I respond to the author's response below.  Update:  I am satisfied with the answers and have upgraded my rating. Updated to reflect author response:This paper proposes a series of metrics to use with  a collection of generative models to evaluate different approximate inference frameworks. The generative models are designed to be synthetic and not specialized to a particular task. The paper is clearly written and the motivation is very clear.While there has been work like Forestdb to maintain a collection of generative models, I don't believethere has been work to evaluate how they perform on a series of metrics. There would be great utilityin having a less ad-hoc way to evaluate inference algorithms.While the idea is sound, the work still feels a bit incomplete. The only distributions used in the experimental section seem to be Gaussians and Mixture of Gaussians. Many more families of distributions are mentioned in Section 3, and it would have been nice to show some evaluation of them considering the code is already there. In addition to distributions mentioned in Section 3, it would help if there were a few larger dimensional distributions. Often for evaluation now, many papers use a Deep Gaussian model trained to model MNIST digits. I worry that insights drawn fromthe synthetic examples won't transfer when the models are applied to real-world tasks.I would like to see described a wider variety of models, including possibly more models with discrete latent variables as much recent literature is currently exploring.The paper is a bit confusing in how it discusses distributions and models. Distributions form the ground truth we compare different trained models to.  It would been more clear for me if the explanation with supplemented with some notation to describe who will compare draws from the true data distributions to samples from each of the trained generative models. ********************After the authors' response:If the main motivation of the paper is to fix the mistakes in [14], then the paper should clearly state so, in addition to explaining why fixing is necessary. While I believe that pointing out other paper's mistakes and correcting them is important, the current state of the paper leads me to keeping my initial score and recommending to reject the paper. -------Revision---------Thank you for the response. I have not changed the original review. Summary:This paper studies the properties of applying gradient flow and gradient descent to deep linear networks on linearly separable data. For strictly decreasing loss like the logistic loss, this paper shows 1) the loss goes to 0, 2) for every layer the normalized weight matrix converges to a rank-1 matrix 3) these rank-1 matrices are aligned. For the logistic loss, this paper further shows the linear function is the maximum margin solution.Comments:This paper discovers some interesting properties of deep linear networks, namely asymptotic rank-1, and the adjacent matrix alignment effect. These discoveries are very interesting and may be useful to guide future findings for deep non-linear networks. The analysis relies on many previous results in Du et al. 2018, Arora et al. 2018 and Soudry et al. 2017  authors did a good job in combing them and developed some techniques to give very interesting results. There are two weaknesses. First, there is no convergence rate. Second, the step size assumption (Assumption 5) is unnatural. If the step size is set proportional to 1/t or 1/t^2  does this setup satisfies this assumption? Overall I think there are some interesting findings for deep linear networks and some new analysis presented, so I think this paper is above the bar.However, I don't think this is a strong theory people due to the two weakness I mentioned. Updated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method. ========Thank you for the detailed responses. Given the additional experimental results and connections to existing work, I have updated my score from 5 to 6. --------------After seeing the authors engage at least a little with the related semantic parsing literature, I've increased my score to a 7. ----I acknowledge that the authors have made improvements to the paper and have increased my score to 6This is still definitely not my area of expertise and so I am leaving my confidence score low. --- Update:I appreciate the effort put by the authors into improving the paper. The revised draft is much better than the initial one. But I agree to AnonReviewer2 in that the degree to which this paper has to be modified goes beyond what the review process (even at ICLR) assumes. It is wrong to submit a very unfinished paper and then use the review period to polish it and add results. This incurs unnecessary extra load on the review process.The added 2D results are toy-ish and somewhat confusing (I am not sure I understand what the meshgrids are and what do they tell us). Generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation. The paper should have more results on complex datasets, like for instance ImageNet or LSUN or CIFAR or so, and should have comparisons to existing VAE-GAN hybrids, like VAE-GAN. Also, since a lot of the authors motivation seems to come from psychophysics, showing some application to that might be a good way to showcase the value of the method (although this may not go well if submitting to machine learning conferences). I encourage the authors to further strengthen the paper and resubmit to another venue.----- Update:Id like to thank the authors for their thoroughness in responding to the issues I raised. I will echo my fellow reviewers in saying that I would encourage the authors to submit to another venue, given the substantial modifications made to the original submission.The updated version provides a clearer context for the proposed approach (phychophysical experimentation) and avoids mischaracterizing GAIA as a generative model.Despite more emphasis being put on mentioning the existence of bidirectional variants of GANs, I still feel that the paper does not adequately address the following question: What does GAIA offer that is not already achievable by models such as ALI, BiGAN, ALICE, and IAE, which equip GANs with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates? To be clear, I do think that the above models are inadequate for the papers intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but I believe this is a question that is likely to be raised by many readers.To answer the authors questions:- Flow-based generative models such as RealNVP relate to gaussian latent spaces in that they learn to map from the data distribution to a simple base distribution (usually a Gaussian distribution) in a way that is invertible (and which makes the computation of the Jacobians determinant tractable). The base distribution can be seen as a Gaussian latent space which has the same dimensionality as the data space.- Papers on building more flexible approximate posteriors in VAEs: in addition to the inverse autoregressive flow paper already cited in the submission, I would point the authors to Rezende and Mohameds Variational Inference with Normalizing Flows, Huang et al.s Neural Autoregressive Flows, and van den Berg et al.s Sylvester Normalizing Flows for Variational Inference.----- ================After rebuttal:Thanks the authors for clarification. I have read the author's responses to my review. The authors have sufficiently addressed my concerns. I agree with the responses and decide to change my overall rating After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances.  REVISION: thanks for the clarification. I have slightly increased my rating (to 4). edit (19.11.) ---- updated score to 5 ----Below is based on the revision---Thanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing.  ----Updated review:After reviewing the comments and the paper in more detail (whose story has evolved substantially) , I have revised my score slightly lower. While in hindsight I can see that the paper has definitely improved, the story has changed rather dramatically, and appears to be still unfolding: the paper's many new elements require further maturation, and that the utility of empowerment for reward shaping and/or regularization to evolve AIRL (i.e. the old story vs. the new story) still needs further investigation/maturation. If the paper is accepted I'm reasonably confident that the authors will be able to "finish up" and address these concerns. (typo: eq. 4 omits maximizing argument) Update (22.11)I think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. - The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.- The derivation could be a bit more rigorous.As the presentation is now much more sound, I slightly increased my rating.  Post-rebuttal update:The paper was substantially improved. New experiments using real objects have been included, this clearly demonstrates the merits of the proposed method in robotic object manipulation. edit:  the authors nicely revised the submission, I think it is a very good paper. I increased my rating.----- I extend this review based on the replies.  One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise.  First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community.  (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies.  I do hope the dataset will be made public.  This is a major reason I gave a high score.)  Second,  the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there,  the impressive performance itself is an impact to the field.  This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. Revision:The addition of new datasets and the qualitative demonstration of latent space interpolations and algebra are quite convincing. Interpolations from raster-based generative models such as the original VAE tend to be blurry and not semantic. The interpolations in this paper do a good job of demonstrating the usefulness of structure.The classification metric is reasonable, but there is no comparison with SPIRAL, and only a comparison with ablated versions of the StrokeNet agent. I see no reason why the comparison with SPIRAL was removed for this metric.Figure 11 does a good job of showing the usefulness of gradients over reinforcement learning, but should have a better x range so that one of the curves doesn't just become a vertical line, which is bad for stylistic reasons.The writing has improved, but still has stylistic and grammatical issues. A few examples, "therere", "the network could be more aware of what its exactly doing", "discriminator loss given its popularity and mightiness to achieve adversarial learning". A full enumeration would be out of scope of this review. I encourage the authors to iterate more on the writing, and get the paper proofread by more people.In summary, the paper's quality has significantly improved, but some presentation issues keep it from being a great paper. The idea presented in the paper is however interesting and timely and deserves to be shared with the wider generative models community, which makes me lean towards an accept. Revision:The authors have taken my advice and addressed my concerns wholeheartedly. It is clear to me that they have taken efforts to make notable progress during the rebuttal period. Summary of their improvements:- They have extended their methodology to handle multiple strokes- The model has been converted to a latent-space generative model (similar to Sketch-RNN, where the latent space is from a seq2seq VAE, and SPIRAL where the latent space is used by an adversarial framework)- They have ran addition experiments on a diverse set of datasets (now includes Kanji and QuickDraw), in addition to omniglot and mnist.- Newer version is better written, and I like how they are also honest to admit limitations of their model rather than hide them.I think this work is a great companion to existing work such as Sketch-RNN and SPIRAL. As mentioned in my original review, the main advantage of this is the ability to train with very limited compute resources, due to the model-based learning inspired by model-based RL work (they cited some work on world models). Taking important concepts from various different (sub) research areas and synthesizing them into this nice work should be an inspiration to the broader community. The release of their code to reproduce results of all the experiments will also facilitate future research into this exciting topic of vector-drawing models.I have revised my score to 8, since I believe this to be at least in the better half of accepted papers at ICLR based on my experience of publishing and attending the conference in the past few years. I hope the other reviewers can have some time to reevaluate the revision. EDIT: the new version of the paper is much better but still feels like a bit incomplete. I personally would prefer a more complete evaluation and discussion of the proposed method. ====Post-rebuttalI've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly. === Update after rebuttal:Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision. EDIT: I still think the compositions under consideration are the simpler ones. Still with the new experiments the coverage seems nicer. Given the authors plan to release their source code, I expect there will be an opportunity for the rest of the community to build on these, to test TRE's efficacy on more complex compositions. I updated my scores to reflect the change. This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.       =======================================================UPDATE: I thank the authors for their responses to our questions. However, I still cannot recommend acceptance, because of the issue both R3 and I mentioned: the comparison to a more traditional VAE-style encoder, which replaces the "aggregator" of this paper with max-pooling plus some number of FC layers to output a mean and variance (as I mentioned earlier, this is almost exactly PointNet with the last layer outputting mean+variance instead of a single feature vector). I definitely do not buy the assertion that VAEs cannot support variable sized point contexts. Just about any shape encoder can be converted to a VAE encoder by modifying the last layer (and training with an appropriate decoder and loss) -- this includes encoders that handle point clouds of varying size like PointNet. In the exchange with the authors, it was not clear that this point was fully appreciated. Hence, while I appreciate the many interesting aspects of this paper, and the additional results provided by the authors during the discussion session, I remain negative and am slightly lowering my rating to make this clear (if this was a journal submission I would mark it as "major revisions" and ask for additional experiments vs VAE baselines which share the bulk of the proposed architecture other than the different aggregator). ----------------------------- Post rebuttal -----------------------------I read the author's rebuttal and I greatly appreciate their efforts. The authors have done many more experiments and I would like the authors to incorporate them into their manuscript and modify their manuscript, even methods, accordingly. I think these new materials can greatly strengthen the paper.1) It seems that ZSL with the original classifier involved is quite strong (this could not happen in ZSL as there is no original classifier for the unseen classes). I would suggest that the authors further investigate this for a detailed comparison. These methods may even simplify the authors' methods, and a connection to ZSL can strengthen the paper. For instance, Changpinyo et al., 2016) showed that their method can outperform [1] and it will be interesting to have some further comparison.2) It's nice that the authors compare the shared and non-shared alpha net. I still have doubt that why non-shared alpha net won't over-fit given that there are only a few labeled data instances. A shared alpha net might be more suitable for robustness.3) There is one difference to Kang's method. Kang's first stage stopped earlier so tailed classifiers have not covered. Did the author do the same thing?4) One method that can simply trade-off the accuracy is Kang's method. I think you can tune their hyperparameter to get a higher tail accuracy. Now the question will be, what will their head accuracy be? Without having a more ground comparison among methods, my question still remains unsolved.5) Besides ImageNet-LT and PlaceNet-LT, there are several CVPR/ECCV papers that outperform Kang's paper on CIFAR, iNaturalist but do not report on these two datasets.I have increased the score to 4, but I think the paper needs significant work to incorporate my comments as well as other reviewers' comments to be ready for being published. ---I appreciate the authors' effort in responding to my comments. But the arguments in their response appear in conflict. Overall, I am still not convinced by their arguments. So I stay with my original review. Final recommendation after rebuttalThe authors gave a good rebuttal, and the current version of Fig 5 and the new Fig 6 are making the paper stronger in my opinion. However, I will stick to my previous rating as: a) the main weakness, the tradeoff in performance for few vs overall makes the contributions weak, especially since the cRT baseline is as simple as finetuning only the classifers with balanceed sampling. I would expect gains over thatb) Fig 6 further shows the marginal gains over a weight-sharing baseline and makes the basic approach questionablec) there are no experiments on real-life long tailed datasets - a note here about iNaturalist: the argument the authors make about iNat makes some sense to me, and I want to thank them for replying. But this is exactly why we should test on real long-tail datasets, ie they dont behave as the artificially created ones. After author response: I've read the response and other reviews and keep my score (weak accept). R3 says that "The second part of information maximization loss is nearly the same as that of SHOT-IM", but I don't see it as a problem. The first part seems interesting and novel, and to get strong results they need to combine it with an existing technique, which seems typical. I agree with the authors and disagree with R3: that their "method outperforms it (SHOT) in 7 out of 9 scenarios." So the positive is that it's a nice idea, simple method, and performs quite well.However, I agree with R3 that there should be more extensive ablations to understand the effect of each part, and the sensitivity analysis of lambda should be done on more datasets. Additionally, I'd like to see more detailed comparisons to related work like https://arxiv.org/abs/2006.10963 that uses batchnorm for domain adaptation. The results aren't stellar, so without a good conceptual explanation, or empirical investigation, I don't see this as a must accept. **********After rebuttalThe revised version has a better shape. In particular, I like the analytical experiment (Fig.2), which demonstrates that the proposed scheme can improve the wide dynamic range. Overall, this paper observes that BN with small variance influence quantization and proposes a protocol for training a quantized neural network combining filter pruning. Some issues still prevent it from being accepted. For example, PfQ is proposed to reduce the dynamic range. However, there is even no definition of the dynamic range in the paper which may make readers hard to understand the mechanism of PfQ.  Besides, the author claim that the weight widening the dynamic range in quantization is theoretically analyzed. But the analysis of Eqn. (14-16) is less rigorous. The readers may expect to see how weights in the case of $V_{c}^{L,\tau} \approx 0$ increase the dynamic range according to its definition compared to those weights in the other cases ($V_{c}^{L,\tau} \gg 0$).The paper proposes an effective approach of quantization, which reduces the model size and improves accuracy. I would like to increase my rating to 5. -- Post rebuttalI appreciate the response by the authors and the new experiments. I also read the other reviews and responses. I think the paper has improved in the revised version. However, I'm still concerned about the novelty, which still remains relatively incremental, as also pointed by other reviewers. I update my rating to 5. Thanks for the feedback and the revisions to the paper. I am satisfied to the response. Congratulations on the great work! Post-Rebuttal:I would like to thank the authors for the insightful rebuttal! The authors were able to address my concerns adequately and I believe that the revision improved the quality of the paper quite a bit. Therefore, I stand with my initial recommendation and due to the reasons stated above, I endorse accepting this paper. ###  --------------------------------------------------------------------------------------------------------------------------Post rebuttal:I first want to thank the authors for their response to my concerns. My concerns have been partially addressed, but some concerns still exist. For example, I am not clear about whether the simplified theoretical estimator in Definition 4.1 really captures the training of GNN. To achieve this goal, some simulations or theoretical justifications should be provided. In addition, the analysis fails to motivate some useful and meaningful algorithmic designs for GNN acceleration, which I believe is much more interesting. For these reasons, I tend to keep my score unchanged. However, I  think the authors take a very good start in studying the impact of batch sizes for GNN training, and I encourage the authors to further enhance their papers from both the theoretical and practical perspectives. For example, it would be much better to explore some new designs for accelerating GNN training based on the developed analysis (e.g., batch size adaptation). Theoretically, the authors can study a more practical and general estimator (perhaps beyond two layers). With these improvements, I believe this can be a very good paper to be published in top venues.  POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. I think the paper is stronger and should be accepted. ################################################################After author response: The authors have addressed my comment about inconsistent evaluation setups among different papers. However, I sill think novelty of the paper is limited as it is a conditional counterpart of [A]. As mentioned by other reviewers, findings of the paper are quite incremental and are in line with LostGAN-v2 although the authors use a more consistent evaluation setup. Overall, I keep my current rating.  === After rebuttal ===Thanks for the response.I believe that Reviewer2's criticism about the similarity to Park et. al isn't sufficiently addressed by the authors. Even if the hierarchical structure is different it's unclear whether this alternative structure is superior to Park et. al. There appears to be no evidence that the latent variables contain more global information relative to VHCR (Park et. al). These claims aren't tested and the results in the paper aren't comparable since the authors don't evaluate on the same datasets as Park et. al.In general, I think the claims of a superior hierarchical structure to models such as the factorized hierarchical VAE paper needed to be tested to show evidence of a more powerful representation for hier-VAE.I will keep my score. ---------------Update 21.11.2018I think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely:- The technical contributions are minor.- The theoretical discussion (in particular regarding absorbing states) is quite shallow.The merits of the paper are:- Good results due to off-policy learning- Raising awareness and providing a fix for a common pitfall I think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. Some suggestions: Section 3.1"As we discuss in detail in Section 4.2 [...]"I think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states."We will demonstrate empirically in Section 4.1 [...]"The demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)?Section 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics.Section 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how  the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. -------------Update 22.11.2018By highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. I think that the submission can get accepted and I adapted my rating accordingly.Minor:Conclusion should also squeeze in somehow that the reward biases are caused by the implementations.Typo in 4.2: "Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic]were previous hidden, [...]"  ## after feedbackSome of my concerns are addressed the feedback. Considering the interesting technical parts, I raise the score upward, to the positive side. --REVISION--I would like to thank authors for their effort to improve quality of images. In my opinion the paper is nice and I sustain my initial score. xxxxxxxxxxxxxxxxxxxAppreciate the authors' rebuttal, updated my score. Update after the author response:The author addressed some of the concerns raised in the review(Thanks for the detailed response), in particular, the comparison to cuDNN.  I still think the paper is still borderline but the results might be of interest to some of the ICLR audience.  Update after rebuttal: I'd like to thank the authors for the detailed response. It addressed most of my concerns.    The analogy with MIP makes some sense, as huge LPs are indeed being solved every day. However, an important difference is that those problems cannot be solved in other better ways till now, while for deep learning people are already successfully solving the current formulations. I still think this work will probably not lead to a major empirical improvement.     I just realize that my concern on the practical relevance is largely due to the title "Principled Deep Neural Network Training through Linear Programming". It sounds like it can provide a better "training" method, but it does not show a practical algorithm that works for CIFAR10 at this stage. The title should not sound like "look, here is a new method that can change training", but "hey, check some new theoretical progress, it may lead to future progress". I strongly suggest changing the title to something like "Reformulating DNN as a uniform LP" or "polynomial time algorithm in the input dimension", which reflects the theoretical nature.     That being said, the MIP analogy makes me think that there might be some chance that this LP formulation is useful in the future, maybe for solving some special problems that current methods fail miserably.  In addition, it does provide a solid theoretical contribution. For those reasons (and assuming the title will be changed), I increase my score. After reviewing the author response, I adjusted the rating up to focus more on novelty and less on polished results. # Update (2018-11-29)Given the substantial author feedback, I'm willing to raise my score. I thank the authors for their detailed responses and revision.- The revision to section 5 explaining the training procedure is helpful.- The revision to section 3 is also helpful. It may have helped to go even further in explaining the objectives of the memory access learning task (as in the response to Rev2) with analogs to BF, and distinguishing them from some aspects that seem like engineering details, as the former is (to me) the conceptually significant portion of the paper.- I still could not find where it is stated that the BF plots are analytic; my apology if I missed it. This is not a major issue, and I understand the choice to use the theoretical bound, but there is some discord in including an analytic curve next to empirical curves on the same plot without clearly marking it as such, as it may give a wrong impression as to what the reader is seeing (not an actual experiment, but an estimate of what an experiment would have yielded based on probabilistic concentration). - I have revised my score to 6. The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7. -----------Update:The comment of the authors clarified some misunderstandings. I now agree that the combination of DeepWalk features and GNNs can encode more/different topological information. I still think that the paper does not make this very clear and does not provide convincing examples. I have update my score accordingly. The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper. after the rebuttal:After reading the different reviews, the replies of the authors and the updated version, my opinion that the "explanations" are simply intuitions (which is related to AnonReviewer3's concern "Regarding advantages of learning a joint model as opposed to unidirectional mappings") has not been completely addressed by the authors. Fig. 4 does address this concern by illustrating their point experimentally. However, I agree with AnonReviewer3 that the justification remains unclear. Seems no response from the authors. Update 11/27/2018Thanks for the authors for updating the paper. The updated paper have more clear comparisons with other models, with more &amp; stronger experiments with the additional dataset. Also, the model is claimed to perform multi-step interaction rather than multi-step reasoning, which clearly resolves my initial concern. The analysis, especially ablations in varying number of iterations, was helpful to understand how their framework benefits. I believe these make the paper stronger along with its initial novelty in the framework. In this regard, I vote for acceptance. UPDATE:The revised version is much better in empirically demonstrating the value of the method; though, there is still some work needed. First, the artificial examples are still rather low-dimensional where independent MH is expected to perform well. Second, ESS does not help to assess the biasedness of the sample; maybe [1] can help with this. Third, NUTS might be a better baseline than standard HMC which is know to be sensitive to the stepsize/number of leapfrog steps. An effective sample size of 1 suggests that the method did not even start to sample - likely because of a bad choice of the stepsize and/or mass matrix. I would suggest using PyMC3's NUTS implementation. Finally, to show the correctness of the method, I would suggest to 1) let alpha converge to zero such that \phi will be fixed at some point, and 2) ensure that the proposal has full support under the target for any value of \phi. In this case, the sample drawn from the target will be unbiased for large enough n (same arguments as for adaptive MCMC should apply).The idea of reusing the samples from previous iterations for approximating the loss is interesting and worth exploring.[1] Jackson Gorham, Lester Mackey. "Measuring Sample Quality with Kernels", <a href="https://arxiv.org/abs/1703.01717" target="_blank" rel="nofollow">https://arxiv.org/abs/1703.01717</a> ************ Revision ***********The authors' updates include further quantitative comparisons to Fader Networks and ablation studies for the different types of losses, addressing the concerns I had in the review. Hence I have boosted up my score to 7. Update: I thank the authors for their response and additional experiments. I am increasing my score to 7. If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments.--- UPDATE: I've increased my rating based on the authors' thorough responses and the updates they've made to the paper. However, I still have a concern over the static nature of the experimental environments.===================== After Authors Response =====As developed in my response "On D_{\infty} assumption " to the reviewers, I think that the assumption that D_\infty bounded is a critical issue.That is why I am moving down my grade. edit:In light of the revised experiments and inclusion of permutation equivariant deepset layers, I'm inclined to recommend publication. However, if I could nitpick further, I think it would be nice to make some edit (or addition) to Table 1 to include permutation equivariant deepsets. Moreover, it would be nice to have some additional description of permutation equivariant layers in Section 2.1. = Revised after rebuttal =I thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to1) Provide stronger empirical results (these are not too convincing).2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators. ===========  comments after reading response ===========I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected. Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings. After reading other reviews and author comments, I have raised my rating to a 6. My main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments). However, I'm not against the paper as an interesting finding in and of itself. It would be great if the authors (or interested members of the research community) may analyze how general-purpose their proposals are (e.g., of Gaussian base distribution) and how extensive the results are on TTS benchmarks.-- --------------------------------------------Revision:I thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns.A more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018). ########RevisionI would like to thank the authors for a thoughtful revision and response. I have updated my score to a 7 and think this paper is a worthy contribution to ICLR. The new drawback section is well written and informative.  EDIT: Reviewer has considered the response by the authors. Key details of the baseline regressor are missing, such as the exact network structure used. As a result: 1) Reviewer is unable to determine if the baseline is a proper fair comparison. 2) Authors have confirmed the methods reliance on strong shape prior, but this caveat is not clearly mentioned in the paper as a requirement for the method to work. Furthermore, authors did not quantify what affect this reliance has by adding experiments on datasets with weak shape priors mentioned by reviewer. As a result, reviewer is lowering score. Reviewer encourages authors to continue this line of research, but carefully consider the feedback given to make the work stronger before publication. Revision: in light of the relevant papers brought up by AnonReviewer3 and AnonReviewer4, that have not been discussed in the paper, I modify my rating to 6. [UPDATE] the authors address my concerns in a detailed way, and the updated revision is rather robust, therefore, I decide to change my score to accept. Response to author comments:I would like to thank the authors for answering my questions and addressing the issues in their paper. I believe the edits and newly added comments improve the paper. I found the response regarding the use of your convergence bound very clear. It is a very reasonable use of the bound and now I see how you take advantage of it in your experimental work. However, I believe the description in the paper, in particular, the last two sentences of Remark 1, could still be improved and better explain how a reasonable and computationally feasible n was chosen.To clarify one of my questions, you correctly assumed that I meant to write the true label, and not the output of the network. After the rebuttal: I appreciate the authors' effort to revise the paper. The revision made clear that the data produced by the proposed generative model is not linearly separable in general while the theory (Theorem 2) still holds.I am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence. The fact that the classification algorithm motivated by the generative model can do as well as a similar-sized ConvNet does not quite support that the generative model itself is good -- getting a good classifier is still an easier task than getting a good generative model. ===================== --After reading the authors' response, I still think the way that the contributions are depicted (e.g., a justifying the effectiveness of SGD) are inaccurate/unsupported. Furthermore, although the authors' suggest that they have tested a linear classifier and observed that the data is not linearly separable, more explanations/intuitions are needed about the assumptions that are made throughout the paper.  ****Post-rebuttal update: The authors have made significant revision to their work, which sufficiently addressed all my concerns. I have upgraded my score accordingly and I am willing to support the acceptance of this paper. EDIT: Updated score after second revisions and author responses UPDATE: Thanks for the clarifications and edits. FWIW I still find the depiction of the architecture in Figure 2 to be incredibly misleading, as well as the decision to omit dependencies from the distributions p and q at the top of page 5, as well as the use in table 3 of "ELBO" to refer to a *negative* log likelihood. -----------------------------------------------------------------------------Post discussion: Following the discussion phase, the significance of these results seems to be a bit unclear to me. For instance, suppose that we are allowed to construct the sigmoidal activation. Theorem 2.1 in  the following paper https://hal.archives-ouvertes.fr/hal-01256489/document  states that for any continuous + univariate function $f$, and $\epsilon > 0$, there exists a sigmoidal activation function in $C^{\infty}$ and an associated neural network with one neuron in the hidden layer (with the aforementioned sigmoidal activation) such that $f$ can be uniformly approximated to accuracy $\epsilon$ by this neural network. Therefore the set of N points in $\mathbb{R}^{d_X}$ can be first mapped to distinct points on a line, and then be  shattered/memorized by the aforementioned one-neuron neural network. Of course, the setup in the present paper considers the sigmoidal activation to be fixed (e.g., ReLU) and this is a non-trivial difference. But still, it is not clear just how big of a difference this is.  ==== Update following rebuttal and discussion ====The AC has raised an important point that the paper does not discuss its reliance on infinite precision. While using infinite precision is not necessarily wrong, it does make the paper's result less relevant in practice, narrowing its contributions to being mostly about theoretical aspects. More importantly, by hiding this technical detail deep within the proofs, the authors missed the opportunity to discuss the difference between the number of bits used by the parameters and the number of edges (operations) necessary for memorization. It seems to me that the number of bits is linear with the number of examples, even as you can reduce the number of edges, which does have practical implications about the utilization of the memory bought by depth. Moreover, if the authors had discussed this topic, it would've been accepted more easily.Despite the above flaws, I still find value in this article, even if its contributions are mostly theoretical. =====11/26At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score.====12/7The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselineshttps://github.com/google/dopamine/tree/master/baselines) Thanks for the rebuttal. But, I am still not very convinced with the proposed results. For CIFAR-100 (0%), you get about 0.2% gain, for ImageNet (0%), you get about 0.2% loss in top-5 accuracy, and for WebVision, you get about 0.3% gain. I am not sure whether you can call these as statistically significant gains. I believe such gain/loss can be obtained with many other tweaks, such as the learning rate scheduling, as the authors have done. I believe extensive testing the proposed method on many real noisy datasets, not the synthetically generated ones, and showing the consistent gains would much strengthen the paper. But, at the current version, the only such result is Table 5, which is, again, not very convincing to me. So, I still keep my rating. ======= After reading the rebuttal, it is still unclear of how to determine the thresholds for finding incorrect labels. The authors empirically demonstrated a procedure to statistically find a threshold under the assumption that the label noise is uniform. However, theoretical guarantees are lacking. The extension to other types of label noise is also very intuitive. Although the proposed method is simple and effective, the lack of an effective method for choosing the threshold is a major concern for real-world applications. Are there some other ways to determine the threshold? For example, cross-validation method? Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly.  --I updated my score after reading other reviews and the authors' response. Post-rebuttal update: The review process has identified several issues such as missing citations and lack of clarity with respect to aims of the paper. Although the authors have failed to update the paper within the rebuttal period, their responses show an understanding of the issues that need to be addressed as well as a broad appreciation of work in EC that would be included in a final version, making it a useful resource for the wider ML community. On top of this they will include an even larger amount of empiricial data from the experiments they have already run, which is a valuable resource considering the amount of compute needed to obtain this data.---  ################Revision:I would like to thank the authors for the extensive revision, additional explanations/experiments, and pointing out extensive relevant literature on BLUE scores. The revision and comments are much appreciated. I have increased my score from 4 to 6. ### After reading author feedbackThank you for the feedback. After reading the updated paper I still believe that 6 is the right score for this paper. The method produces better results using ensemble learning. While the results seem impressive, the method to obtain them is not very novel; nonetheless, I would not have a problem with it being accepted, but I don't think it would be a loss if it were not accepted. ------------------------Post rebuttal comments:Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues:- I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic.- The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given.- The writing is still confusing. For instance, it is mentioned that "Concretely, for each state s  S, we define T as a teacher that gives an advice T(s)", while that is not true since later it is mentioned that "the teacher give advice based solely on the terminal state". These statements are contradictory, and it is not trivial at all to provide an advice for each state.  ------In light of the author's response I am changing my review from 2 to 3.  I still feel as though the paper should be rejected.  While I appreciate that there is a clear history of using GANs to target otherwise intractable objectives, I still feel like those papers are all very explicit about the fact that they are modifying the objective when they do so.  I find this paper confusing and at times erroneous.  The added appendix on the bits back argument for instance I believe is flawed."It first transmits z, which ideally would only require H(z) bits; however, since the code is designedunder p(z), the sender has to pay the penalty of KL(q(z)kp(z)) extra bits"False.  The sender is not trying to send an unconditional latent code, they are trying to send the code for a given image, z \sim q(z|x).  Under usual communication schemes this would be sent via an entropic code designed for the shared prior at the cost of the cross entropy \int q(z|x) \log p(z) and the excess bits would be KL(q(z|x) | p(z)), not Kl(q(z)|p(z)).  The appendix ends with "IAE only minimizes the extra number of bits required for transmitting x, while the VAE minimizes the total number of bits required for the transmission" but the IAE = VAE by Equation (4-6).  They are equivalent, how can one minimizing something the other doesn't?  In general the paper to me reads at times as VAE=IAE but IAE is better.  While it very well might be true that the objective trained in the paper (a joint GAN objective attempting to minimize the Jensen Shannon divergence between both  (1) the joint data density q(z,x) and the aggregated reconstruction density r(z,x) and (2) the aggregated posterior q(z) and the prior p(z)) is better than a VAE (as the experiments themselves suggest), the rhetoric of the paper suggests that the IAE referred to throughout is Equation (6).  Equation 6 is equivalent to a VAE.I think the paper would greatly benefit from a rewriting of the central story.  The paper has a good idea in it, I just feel as though it is not well presented in its current form and worry that if accepted in this form might cause more confusion than clarity.  Combined with what I view as some technical flaws especially in the appendices I still must vote for a rejection. post rebuttal: I feel that the authors have addressed some of my concerns, in particular, in terms of additional experimental results. I have raised the score to reflect this changes. Response to author comments:Unfortunately I am still significantly unclear on why RL is useful here.  The author response attempts to clarify that by pointing me to paragraph 2 of the intro, which states that RL has been used for data selection in other settings in the past.  What would help me (and I believe, the paper) more is a reason why greedy selection isn't sufficient for this particular problem.  Even just a single motivating example would be extremely helpful.  R3 mentioned similar concerns in their review, saying that the paper lacks explanation for why RL would win over non-RL for e.g. sentiment analysis.Likewise, while I appreciate the authors comparing against a stronger baseline in Figure 3, I don't know how to interpret the figure.  Why is Figure 3(b) better than Figure 3(c), and why does using RL cause that difference to arise? Revision:After the rebuttal, I increased my rating to a 6. I feel this paper could still be improved by better motivating why multi-modality is important for single tasks (for example, by plotting histograms of activations from the network). I also think that the paper by Kalayeh &amp; Shah should be presented in more details in the related work, and also be compared to in the experimental setup (for example on a small network), especially because the authors say they have experience with GMMs. The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper. FINAL UPDATE--------------------Unfortunately, I am not entirely convinced by the additional experiments that we are truly looking into the classifier instead of analyzing the generative model. I believe this to be currently the key issue that, even after the revision, needs to be addressed more thoroughly before it can be accepted for publication. ******************************************************After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring.One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.  ---- Post author feedback comment ---- I raised my rating to 7 as the paper itself is solid, main concern as another reviewer points out is it may be a bit too specialist for ICLR. If the AC decides to reject based on this fact I am ok with that as well. I think it would be helpful to add more ablation (deformation-only results for all cases) and experiments with different numbers of bases in the final version. If that's added it will strengthen the paper.    xxxxxxxxxxxxxxxxxxxI appreciate the rebuttals from the authors, updated my score, but I still believe (just like another reviewer) that this is better suited for a workshop or a conference like SIGGRAPH. --- --- After rebuttal Authors addressed most of my concerns. The paper has merit and would be of interest to the community. I am increasing my score. ===== After rebuttal ======The authors answered some of my questions but I still think it is a borderline submission. ### post rebuttal### authors addressed most of my concerns and greatly improved the manuscript and hence I am increasing my score.   ----------------------Update: In light of the authors' rebuttal I have updated my rating from 5 to 6. [Revision] Thanks for the replies. I still believe experiments on more tasks would be great but will be happy to accept this paper. [POST-rebuttal] I've read the author's response and it clarified some of the concerns. I'm increase the score accordingly. I think the ideas are of sufficient interest to the community to merit acceptance &amp; discussion, but I still miss the high resolution samples we got with the Glow paper. Responses to my concerns somewhat addressed, though simpler alternatives to uniform dequant would be nice.===== # HIGH-LEVEL ASSESSMENT (UPDATED)After reading the author rebuttal and going through the revised manuscript, I believe the authors have successfully addressed the vast majority of concerns I had about the original version of the paper. Based on the current version of the article, I lean strongly towards acceptance and have modified my score accordingly.# STATE OF PREVIOUSLY RAISED MAJOR POINTS1. In my original review, I raised issues regarding the way LEAP was motivated and derived; an opinion also voiced by Reviewer 2. I believe Section 2 of the revised manuscript has greatly improved in terms of clarity while simultaneously being more general.I apologise for the mistaken sign in  in the subsequent analysis. In hindsight, I should have definitely caught that error based on the very unintuitive conclusions that ensue!. The fact that LEAP reduces to Reptile when minimising the expected energy of the "non-augmented" gradient flow makes perfect sense and helps understand what LEAP's "place" is alongside MAML and Reptile. The authors have also extended LEAP to minimise either the length or the energy of the gradient path, rather than minimising only the energy. This possibility was loosely mentioned in the original manuscript, but not implemented. As pointed out in their rebuttal, minimising the length of the gradient path instead of the energy implicitly "normalises" the magnitude of the gradient w.r.t. the initialisation  across tasks (Eq. 8), which might make LEAP most robust against heterogeneity in the scale of task losses.The new ablation studies included in Sections B and C of the Appendix are also a great addition to study/justify empirically some of the more heuristic aspects of the paper.2. The original review also raised some concerns regarding Theorem 1 and its proof; a point also raised by Reviewer 2.The statement of Theorem 1 and, most importantly, its proof, have been almost entirely rewritten. To the best of my knowledge, I believe the revised version is correct (potential minor inconsequential caveats described below), and is now much clearer and easy to follow.3. Besides carrying out the new ablation studies, the authors have introduced two additional baselines in Section 4.2 and now report aggregated results for 10 different seeds in Section 4.3.I still believe that having included additional baselines also in Sections 4.1 and 4.3, as well as evaluating LEAP in a "less favourable" few-shot learning scenario, could have further strengthened the paper. Nevertheless, given the time (and possibly compute) constraints, the revised manuscript also improved considerably in terms of experimental results and, most importantly, already provides sufficient evidence that LEAP can outperform existing approaches when tasks are sufficiently diverse. Response to rebuttal:It is good to know that the authors have a new modified VAE posterior distribution for the stochastic model which can achieve significant gain over the deterministic model. Is this empirical and specific to this dataset? Without knowing the details, it is not clear how general this new stochastic model is.I agree that it is worthwhile to test the model using the 45 minute dataset. However, I still believe the dataset is very limiting and it is not clear how much the experimental results can apply to other large realistic datasets.My rating stays the same. ============after rebuttal============I really appreciate the authors' rebuttal, which has addressed some of my concerns.Nevertheless, I agree with the other reviewers about the main weakness of the paper. That is, why the proposed method works and what are its advantages over similar strategies, such as Mixup, AdaMixup and Manifold Mixup, are not clear. Revision:After reading the author's response, I think most of the points were well addressed and that the repulsive loss has interesting properties that should be further investigated. Also, the authors show experimentally the benefit of using PICO ver PIM which is also an interesting finding.I'm less convinced by the bounded RBF kernel, which seems a little hacky although it works well in practice. I think the saturation issues with RBF kernel is mainly due to discontinuity under the weak topology of the optimized MMD [2] and can be fixed by controlling the Lipschitz constant of the critic.Overall I feel that this paper has two interesting contributions (Repulsive loss + highlighting the difference between PICO and PIM) and I would recommend acceptance. Revision: after reading rebuttal (as well as to other reviewers), I think they addressed my concerns. I would like to keep the original score. ====update: The bound comparison added value to the paper. It strengthens my opinion that this work deserves to be published. I therefore increase my score to 7. Revision:The authors have thoroughly addressed my review and I have consequently updated my rating accordingly. Revision: the rebuttal can not address my concerns, especially the image quality assessment and the novelty of the paper parts. I will keep my original score but not make strong recommendation to accept the paper. Thanks for the clarification and fixing the notations in Theorem 1. I think the discussion of unitary RNN models makes the paper more well-rounded. I hope this work will inspire more research in this direction in the future and help us understand the dynamics of recurrent networks. I would like to keep my rating. Response to rebuttal:The authors clarified the questions. However, I maintain my rating because the contributions are limited and the paper is very poorly written. -----------------------------------post rebuttal: the authors have responded to my main questions, and I would like to increase my score, but I cannot agree with them on possile future extensions of this work, e.g. in learning from pixels. Updates:Author(s) acknowledged that they cannot get a robust analysis. Furthermore, the optimality test also requires a robust analysis. Therefore, I believe the current version is still incomplete so I changed my score. I encourage author(s) to add the robust analysis and submit to the next top machine learning conference.------------------------------------------- EDIT: Now this is a paper that makes sense! With the terminology cleared up and the algorithm fully unpacked, this approach seems quite interesting. The experimental results could always be stronger, but no longer have any holes in them. Score 3--&gt;6 Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this. Post-response comment: while I do think the approach is interesting, the utility of it is mostly demonstrated currently through empirical experiments. These experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach. Further, upon highlighting this in my review, the authors disagree and think its empirical validity is rather superior. This leaves me concerned, and after thinking about it further, I do not think this is sufficient for acceptance. Therefore, Im reducing my score to a 5.PS: the characterization of irreducible error as a product of the limitation of a linear model may be inaccurate. --REVISION--I would like to thank the authors for their response. I highly appreciate their clear explanation of both issues raised by me. I am especially thankful for the second point (about the temperature) because indeed I interpreted it as in the GLOW paper. Since both my concerns have been answered, I decided to raise the final score (+2). I have read authors' reply.  In response to authors' comprehensive reply and feedback. I upgrade my score to 6. As authors mentioned, the extension to density estimators is an original novelty of this paper, but I still have some concern that OE loss for classification is basically the same as [2]. I think it is better to clarify this in the draft.  --------UPDATE AFTER RESPONSE PERIOD:My initial read of this paper was incorrect -- the authors do indeed separate the outlier distribution used to train the detector from the outlier distribution used for evaluation. Much of these details are in Appendix A; I suggest that the authors move some of this earlier or more heavily reference Appendix A when describing the methods and introducing the results. I am not well-read in the other work in this area, but this looks like a nice advance.Based on my read of the related work section (again, having not studied the other papers), it looks like this work fills a slightly different niche from some previous work. In particular, OE is unlikely to be adversarially robust. So this might be a poor choice for finding anomalies that represent malicious behavior (e.g., network intrusion detection, adversarial examples, etc.), but good for finding natural examples from a different distribution (e.g., data entry errors).My main remaining reservation is that this work is still at the stage of empirical observation -- I hope that future work (by these authors or others) can investigate the assumptions necessary for this method to work, and even characterize how well we should expect it to work. Without a framework for understanding generalization in this context, we may see a proliferation of heuristics that succeed on benchmarks without developing the underlying principles. Update 2018-11-23: I am reducing my rating to 5 (from 6) due to the absence of author response regarding a potential revision addressing my comments/questions as well as those from other reviewersUpdate 2018-11-27: I am increasing my rating to 7 (from 5) after the authors responded to reviewers' comments and uploaded a revised version of the paper Update:I have updated my review to mention that we should accept this work as being concurrent with the two papers that are discussed below. Update: Incorporating the AC/PC decision to treat the paper as concurrent work. After reading author response and the extra experiments, I have changed my rating to 6 (from the original rating of 5).  ======================= after rebuttal =======================I appreciate the authors' efforts and am generally satisfied with the revision. I raised my score. The authors show advantage of the proposed ProxQuant over previous BinaryConnect and BinaryRelax in both theory and practice. The analysis bring insights into training quantized neural networks and should be welcomed by the community. However, I still have concerns about novelty and experiments.- The proposed ProxQuant is similar to BinaryRelax except for non-lazy vs. lazy updates. I personally like the theoretical analysis showing ProxQuant is better, although it is based on smooth assumptions. However, I am quite surprised BinaryRelax is so much worse than ProxQuant and BinaryConnect in practice (table 1). I would encourage the authors to give more unintuitive explanation.-  The training time is still long, and the experimental setting seems uncommon. I appreciate the authors' efforts on shortening the finetuning time, and provide more parameter tuning.  However, 200 epochs training full precision network and 300 epochs for finetuning is still a long time, consider previous works like BinaryConnect can train from scratch without a full precision warm start. In this long-training setting, the empirical advantage of ProxQuant over baselines is not much (less than 0.3% for cifar-10 in table 1, and comparable with Xu 2018 in table 2). After the rebuttal:1.  Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is "using soft instead of hard constraint" for BNN. 2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015].--------------------------- (I have read your author feedback and have modified my rating according to my understanding.) UPDATE (Dec 4, 2018):I have read the author response and they have addressed the specific concerns I have brought up. I am overall positive about this paper and the new changes and additions so I will slightly increase my score, though I am still concerned about the significance of the results themselves. Post-rebuttal: I have read the authors responses and I keep my score **********************************************I thank the authors for the detailed response. After reading the rebuttal and other reviewers' comments, I feel that the consensus is the impracticality of the proposed method. I therefore retain my evaluation. ===========================================================================Update after Authors' response:- I thank authors on their detailed response and clarifications. I increased my score based on their response. ######Update after revision: I have looked over the revised paper and believe the authors have addressed most issues that were raised by the reviewers, especially by describing in more depth the relation of their approach with autoregressive models and self-attention and mentioning the runtime differences of their model compared to a normal CNN.I have, therefore, raised my score to 7. #############After reading the rebuttal I confirm the initial rating. ### VerdictThank you for addressing all my concerns. This gives me the confidence to slightly increase my score. One more small thing, for future work, you might also consider incorporating clipping the metrics to the input range. ----------------------------------Edited after authors responses: I would like to thank the authors for the detailed response and the changes they have made to the paper. While I still have some concerns and questions about generalization to more complex tasks, models and labeling functions, I think the additional experiments demonstrate the value of the proposed framework and opens the door for future work to explore these questions. ------ Post Rebuttal------Thanks to the authors for the new experiments and feedback!The rebuttal addressed most of my comments and increased my score by one point. ------After author response:The authors presented new experiments on image datasets during the rebuttal, which demonstrate the flexibility of the proposed framework. However, all the experiments are conducted on simple tasks and models. It is still unclear whether and how this method can help more practical problems.Overall, I think this paper presents a nice exploration towards interactive weak supervision. I hope the authors can release their code and experiment details to encourage future work. Update: The authors have not fixed some of the errors I pointed out in my review. For example, they still refer to the k-dimensional probability simplex and did not address some of the other corrections. Since it is otherwise a promising paper, I suggest to carefully revise and resubmit. I will not however recommend a paper with errors for acceptance. ---------------------Update: The authors have adequately addressed my questions, and I am happy to maintain the rating of 7. ===== update =====No response is provided so I am maintaining the score. I hope the authors could address all the issues in a future version.  Considering the answers from the authors, I decided to not change my score.  After rebuttal:I still think this work has a interesting task setup, though it indeed has many faults (after reading the responses and other reviewers):1. It seems that IPM is not really useful in practice.2. It is also not sufficient to large occlusion, and thus there is no explanation for its advatange over `estimating accurate depth`3. Range is short and latency is high4. After reading reviewer1's comments, I think it could use the same experimental setting as the existing methods for a fair comparison. The other methods might be not properly trained with the new setting.5. It is still not clear how to emsemble several models (with different trained weights) in this work.Thus I am changin my rating to 6, and I will not fight for this work. After discussion with authors----------------------------------------The reviewer thanks the authors for clarifications. With the confusion from the typos resolved, the paper is easier to follow. The technical correctness of the paper is not in doubt any more. But two major concerns still remain: 1.  The diversity enforcement has been reported before, and more comprehensive discussion of related work would be useful.  A detailed empirical analysis would also make the paper balanced and not heavily reliant on the novelty/depth of theoretical contribution. 2.  The main claimed theoretical contribution summarized in Lemma B.1 is tedious but a derivation of an obvious statement (as sketched out in the original review). The reviewer is raising the score to encourage this work, but still holding to the recommendation of not accepting the paper in its current form.  ######Post-rebuttal####The authors have addressed some of my concerns and I have raised my score accordingly  --update---I am upgrading to 7: Good paper accept; as my concerns have been addressed by the additional experiments.The proposed method is not yet a drop in replacement for BatchNorm in general, but it can be useful in specific circumstances, i.e. small batch-size training. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------UPDATE: The author has addressed most of my concerns, but regarding the motivations and the benefits for the community, I still keep my score. ----------The reviewer appreciates the response in detail. The new figures (Figs 4-7) are helpful for demonstrating the differences between LOF and RM, while also demonstrating that their outputs can be similar or the same in many cases. The reconciliation between optimality and composability is a nice feature. Overall, the reviewer still feels positive on this work.  Updates: Thanks for the author's response. My concerns are mostly addressed. But I still believe explicit geometry modeling should be included for this task. This could be added in future works. Overall, I am positive on the submission and keep my original rating. ##########################################################################Updates are appreciatedHi, Authors,I appreciate the updates you've made to the paper and the responses to my questions.  You're quite correct that RN50 and ImageNet is sufficient to illustrate deficiencies.(I'd still want to see broader experiments for claims of some new method overcoming these deficiencies, though!  When broadening scope to other tasks, I'd expect the authors of prior methods designed for vision tasks would be okay with use of their methods as baselines if there are no methods designed specifically for those new tasks.)With this in mind, I'll update my rating to a 7. ##post-rebuttal##I share the concerns of others reviewers that this paper has a limited novelty and narrowed scope but I think the authors have addressed other issues/concerns quite well. In my opinion, the key contributions mostly come from the insight and experimental results, and less so on the heuristic itself. Thus, I would adjust my rating to 7. ----------------------------------------------------------------------------------------------------Comments after rebuttal:I would like to thank the authors for their detailed response. Many things were addressed, and I have increased my score to 5 to reflect that the paper is not far from acceptance threshold. I think the main thing missing is a discussion of the effect of the sampling of subgraphs: i.e. showing that PageRank is indeed better than choosing nodes at random, and analysing how the results change when the reduction percentage is varied (between a low value and the maximum value that fits in GPU memory). =================== Post Rebuttal Comments ===================I would like to thank the authors for their insightful rebuttal and clarifications.Most of my concerns have been properly addressed, and I very much appreciate the discussion about PNA. However, since the authors train a "SuperNet", it is not particular clear to me why one even needs to decide for a specific aggregation scheme (in contrast to PNA), e.g., by simply using the softmax function instead of the gumbel softmax formulation.Furthermore, I'm still not that convinced about the transfer learning proposal. In my opinion, a more in-depth analysis is needed (both theoretical and practical) to justify the claims made in the paper. Since GNNs do not even need to be trained in full-batch mode (i.e. via GraphSAGE, Cluster-GCN, GraphSAINT, ...), I'm not sure which benefits the proposed approach brings to the table in comparison to any other scalable methods.Therefore, my rating will stay the same. Post rebuttal Comments:Thank the authors for the detailed response. I keep my rating as 5. ##############Post Rebuttal###############My concerns are addressed by the authors. I'm keeping my original rating. **After rebuttal & discussion**I still tend to think that the paper's scope can be adjusted relatively easily (it is not too difficult to insert more disclaimers and change the title), and we can force apply the adjustment by conferring a conditional acceptance.But I'm sold on the point that there is a lack of argumentation on whether undisclosing the user-specific embedding will improve the privacy guarantee. I had taken this argument as granted, but this is indeed not so obvious, given that there exist many attacks that are applicable in this kind of scenario, as R4 has argued. It would be great if the authors could quantify the improved privacy guarantee.I'm okay with rejecting the paper then. I still like the paper quite a lot, but rejecting it will also give the authors a good chance to assimilate more points of views in the paper. Update post-rebuttal:The rebuttal clarified the motivation, but has yet to address the flaws that was associated with the choice of motivation and positioning among related work. Unfortunately that means I will not be changing my score. ---Update:- I think the idea of this paper is very interesting. The authors' responses also address most of my concerns. The remaining issues are: 1) the writing needs to be improved to make the paper easy to read; 2) I am still not so convinced with the experiments. It would be more convincing if the authors can demonstrate this on few-shot image classification to improve SOTA, for which the joint (x,y) distribution might be too big at the image level. On the other hand, I am not so familiar with this topic. If such experiments are quite normal in this area, then I am fine to increase the rating. I have read the author responses but my initial review remains unchanged.  The paper needs to be greatly improved in terms of clarity and the responses from the authors help but I think it is better off being resubmitted to another conference with heavy revisions.  Thanks for the author's detailed response.In terms of the first question, I do appreciate the value of the paper as a nice empirical study of Muzero and other similar MBRL algorithms. Meanwhile, many MBRL algorithms are not like Muzero. For example, value-based planning algorithms don't maintain an explicitly parameterized policy. Therefore the conclusions here may not apply to all cases. Making its conclusions more precisely will not undermine the value of the paper, instead, it provides readers clearer results. I do see in the revised version, the authors changed their language in the discussion about the result. But maybe clearer results themselves are better.My second concern is addressed in the updated version of the paper, with additional experiments. Cool!In terms of the third question, after reading your response, I think there is a very interesting question. When we test planning algorithms, should we give the agent a fixed model and a fixed representation or fixed algorithms learning the model and the representation? After thinking for a while, I can see the advantages and disadvantages of both cases. So I would change my mind and agree with the authors that their choice of testing is valid. But I do hope this choice being mentioned in the paper because people like me would typically consider the other one.I would like to raise my score to 5. -----------------------------------------------------------------------------------------After reading the authors response to all reviewers, I believe all questions to be sufficiently addressed. I will therefore (happily) raise my score. ## AFTER REBUTTALFirst of all, I would like to thank the authors for their detailed response: I realize that the authors spent a lot of effort to address most of my comments and questions. I have read other reviews and the responses by authors. However, I still have questions about the paper, preserving me from increasing the score.1. **Assumption 1.** Even in the updated form presented in the rebuttal, Assumption 1 is not mathematically rigorous. How can $\mathbb{E}[||g(x,k)||^2]$ be independent of $x$? Can you provide any non-trivial example? Actually, it is highly relevant to my third concern (**Misleading results**) from the weaknesses part: when we use stepsizes dependent on $m_k$ from the future, we implicitly assume that we can use any stepsizes without changing the sequence {$m_k$} (otherwise $m_k$ should depend on $x_k$). Then, if we consider SGD with arbitrary small stepsize, we will get the method that generates arbitrary close points. In these settings, for the majority of problems, the sequence {$m_k$} should be stationary (since we can take arbitrary small stepsize). But it is not stationary, at least in the experiments presented in the paper. **It is a crucial contradiction that significantly decreases the value of the results given in the paper.** In other words, this assumption **never** holds. I understand that the authors simplified the assumption to handle the non-stationarity of the noise. However, the settings considered in the paper are too simplified: they do not cover any problem.2. **"We could present the paper in terms of $\sigma_k$..."** The paper would significantly benefit from this. Moreover, without resolving the issue mentioned above, it is better to remove the whole part based on the independence of {$m_k$} on $x$.3. **"Moreover, allowing the step size choice to depend on noise level is a standard approach in almost all SGD analysis."** I agree with the authors. Still, the key difference is that typically it is assumed that **the upper bound** for the noise level is known. It significantly differs from the case when {$m_k$} is known.4. **"We have compared SGD and Adam in Figure 4 of the appendix (see Appendix A)."** Unfortunately, I have to disagree: the authors presented the behavior of $m_k$ and $\sigma_k$ for SGD and Adam in separate figures -- Figures 1 and 4. There are 10 pages between them, so there is no transparent comparison. It would be much better to see one figure for both algorithms --- this is what I meant by comparison in my review.5. I still find the proof of Remark 9 to be incomplete: in the proof of the analog of (8) for the case of Remark 9 the authors assumed that $\hat m_{k+1}^2 = \beta \hat m_k^2 + (1-\beta)||g_k||^2$ (in the proof of the last but one inequality), while in this remark it should be $\hat m_{k+1}^p = \beta \hat m_k^p + (1-\beta)||g_k||^p$. Next, what are the assumptions on $p$? Can it be any positive number? If yes, then what is the best choice of $p$? This part requires further discussion and development.6. **About my 19-th question from the list.** I meant that $D^2$ and $M^2$ could be unrelated in general. It would be interesting to see the discussion on how it influences the convergence rate. The current analysis says that the smaller $D$ is, the better the algorithm converges. It should be discussed how it relates to the empirical findings.7. The proofs of Corollaries 5, 6, and 7 were given nor in the rebuttal, neither in the paper. Actually, the authors have not updated the paper at all, although many corrections should be applied (at least grammatical errors and misprints noticed above).To conclude, the most important questions and comments from my review have not been properly addressed by the authors. Therefore, I want to keep my initial score unchanged. (Edit: the authors considered it) ___### After author response and paper revisionFirst, I would like to congratulate the authors for their amazing work during the rebuttal period.My main concern, the comparison against [1], has been perfectly addressed in appendix G (both "conceptually", by highlighting the differences between both methods, and showing the advantages of the proposed SGDP and AdamP, but also empirically on ImageNet, where a fair comparison with proper hyper-parameter tuning has been performed).The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e-4 was the best value.With all these changes, I think the paper is good and I recommend its acceptance. ###################################################################Post Discussion Score:After reading the rebuttal from the author and the comments from other reviewers, I am still not clear if the proposed update in equation (12) yields smaller norms ||w_{t+1}|| than the momentum-based update in equation (8). However, the authors have addressed all of my other concerns. I decided to increase my score for this paper from 4 to 5.  -- The authors have attempted to address this point, but with limited time were not able to train a network to a high level of performance. -- ------------------------------post rebuttal-------------------------------------------------------------Response to the authors' response----------------------Thank you for the hard work in responding.I have read other reviewers' reviews and the response from the authors. The authors have addressed most of my concerns.I believe this paper deserves acceptance. As we know, variants of efforts have been made to improve NAS's effectiveness since 2016, and a great process has been reached. Despite the high expectation and solemn devotion, NAS's effectiveness is believed to be still low. This is inconsistent with many pioneer researchers' expectations four years ago, in which NAS is expected to be another revolutionary technique similar to 2012's deep learning. Currently, there are many NAS papers published every year. But their effectivenesses are unclear due to the lack of ranking correlation analysis. Differently, this paper comprehensively analyzes the architecture rating problem, which provides a timely analysis of the current NAS's ineffectiveness caused by inaccurate architecture rating. I think this paper can attract the community's attention, encouraging the community to pay attention to the architecture rating in NAS, especially when reviewing a NAS paper. Therefore, I recommend an acceptance for this paper to promote the analysis of the NAS's architecture rating problem.I agree with R2 that Yu et al. have proposed a similar idea (I assume R2 refers to "Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, Mathieu Salzmann, Evaluating the Search Phase of Neural Architecture Search"). But the analysis in this paper is more comprehensive than Yu et al.'s article. Many findings are new (at least they are not in published papers).I agree with R4 that the authors did not form a coherent logic flow to present these empirical findings, and the paper was similar to a technique report. However, many important articles, e.g., "Designing Network Design Spaces," "Exploring Simple Siamese Representation Learning," "Is Faster R-CNN Doing Well for Pedestrian Detection?" are also technique-report-like.I appreciate R1 for his devotion to finding similar observations in his experiments. I believe these observations are important and deserve publication. I agree with R1 that removing any operation leads to a smaller search space and a higher ranking.In summary, I will keep my rating as an acceptance. Undoubtedly, I also believe the comments from other reviewers can benefit the improvement of your paper. =================================================================== updates after rebuttal period =================================1. The authors added more equations, I now understand the methodological contributions required substantial effort.  The VAE/etc. work is outside my area of expertise.2. The authors added some figures showing both real and synthetic data. I've looked through them, and the statistics carefully.  Personally, reading through all of this, I would use PCA+OASIS rather than this new technique.  I understand R^2 is higher for this technique, but looking at the data, without actual confirmation of any of the inferences. There exists data with joint calcium imaging and ephys one can use to calibrate and evaluate methods, those are the best data to use for such purposes.  In the absence of those data, we are guessing.  My guess is that the OASIS+LFADS output looked better than the CaLFADS output. R^2 is a funny metric, especially when we are trying to compare spike trains, because translations utterly break R^2, unless the output is smooth, in which case, it does not.  It looked to me that CaLFADS had a smoother output than OASIS+LFADS, which could make R^2 higher, but is not actually what I typically want when analyzing calcium imaging data. Indeed, if we were ok with smoothing, we could simply operate on the calcium imaging data itself, without worrying about spike train inference.3. I got more clarity on the differences between LFADS and CaLFADS, though again, this is outside my area of expertise.  %%%% after the rebuttal:I would like to thank for the authors for their effort to address my concerns. The manuscript is now improved, and I am raising my score to 5.  After considering other discussions/comments, overall I still think the manuscript is below the threshold.  ##########################################################################Post-rebuttal: I have increased my score to a 7. I have laid out my reasoning in response to the author's comments. ------------- UPDATE FOLLOWING DISCUSSION -------------The authors were responsive and suggested a series of updates to address the concerns raised here. I find the inclusion of a theory section valuable. Overall, though, my stance did not change: Id like to encourage the authors to pursue their idea, but I dont think the paper is ready for acceptance yet. Id recommend focusing on two aspects:(i) Simulations and numerical results are especially confusing. Please consider a major revision in light of the comments raised by different reviewers.(ii) I find the application to fMRI interesting and potentially impactful. If the authors can include such analysis, I think itd improve the paper significantly. Being a completely different experiment, it would also alleviate the concerns on numerics. (significance, effect size, stability)------------- END OF UPDATE ------------- ------------------------------------------Post rebuttal:The authors has addressed several of my concerns regarding the method's generality and some experiments. While I'm raising my score to 5, I'm still not convinced that the paper proposed a valuable contribution to the community -- comparing RL algorithms in memory or compute footprints instead of the number interactions with the environment is not meaningful, especially when a simulator is in use. There are several much simpler things one can do to tradeoff compute or memory (for example re-render observations on the fly from stored low dimensional states). Thus, I'm voting for a rejection.  ## Post RebuttalI thank the authors for their responses. The results with a larger CNN and increased batch size help show the benefit of the method further.  I still believe the presentation of the method would be considerably stronger if results were presented in a setting with larger CNNs and higher resolution. --- Response to author's answers> The target actor points up method can be used to predict multiple agents by running repeatedly, these predictions are each independent, whereas in reality future trajectories for vehicles are mutually dependent. Two cars may have independently likely trajectories, but if they intersect, they are unlikely to occur together.Yes, this is a commonly used argument used by works that predict all actors in the scene simultaneously. However, whether the claimed benefit is achieved by the proposed model is yet to be proved. The fact that the proposed model has higher prediction errors than the state-of-the-art single-actor prediction model makes me doubt whether this is true.Thank you for updating the paper with the inference time numbers. These results are really useful.> For runtimes on the same test machine, ECCO runs 684ms versus 1103ms for VectorNet.1103ms is really high a latency for any real-time robotics system. For completeness, could you also include the number of actors that are predicted by VectorNet in 1103ms? **I have updated this review after noting the authors detailed response.**This paper focuses on the problem of Neural Text Degenerationwhere text sampled from a language model can either be too repetitive and bland or too random and nonsensical. The authors focus largely on the former problem, proposing a finetuning loss that specifically incentivizes the use of tokens that have not yet been decoded in the given document. The authors test whether this improves repetition and unique token coverage with greedy decoding in open-ended generation. A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT). Similarly good results are obtained on Image Captioning with and without trigram repetition blocking. On Abstractive Summarization BeamSearch is used and again outperform MLE and UT. Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited. The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad. Multiple hyperparameter settings are shown, with some analysis on how gamma can be chosen to get a desired behavior. Finally, the authors analyze why UT may not be as effective: it penalizes gold repetitions too much and does little for other tokens.Strength:- The results are good for greedy decoding- The method is well motivated and well explained- The analysis regarding Unlikelihood Training is interestingWeaknesses- The results shown do not make proper comparisons across models, baselines, and hyperaparameters over all tasks.- Results for stochastic decoding should have been shown across tasks.- Despite citing the need for awkward rules such as trigram repetition blocking as a reason to propose ScaleGrad, trigram repetition blocking still helps significantly.- Some details are hidden away in the appendix, which I had to read thoroughly in order to fully understand the comparison.I recommend to reject this paper, because the experimental comparisons are not quite fair and because of implicit argumentation about what Greedy decoding can or should do that is never made explicit.The following two paragraphs are obsolete, because the authors shared experimental results from a larger set of experiments.> The results in Table 1which show the main metrics of interest on open-ended generationare missing two key points of comparison: ScaleGrad is only show with gamma=0.2, even though gamma=0.5 & gamma=0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings. This is despite the fact that two hyperparameter options for Unlikelihood Training are shown. In a footnote on page 6, for directed generation, the authors state Although UL was originally proposed for open-ended generation, it is applicable to directed generation. We did the same scale hyper-parameter search for UL. Details can be seen in Appendix E. However, in Appendix E two hyperparameter settings for alpha are shown, the same two as used in Table 1, but two hyperparameter settings for gamma in ScaleGrad are shown _neither of which are shown in Table 1_ nor are repetition or uniqueness numbers shown for these hyperparameters settings anywhere in the paper or the appendices. This makes me question whether the improvements shown in Table 1 hold across hyperparameter settings as the authors claim in their analysis of Figure 1.> However, Figure 1 is missing necessary data points and comparison. First of all gamma=0.2 is not shown, though at least gamma=0.1,0.3 are so it can be somewhat inferred. That is suboptimal, but this graph does not even go up to gamma=0.8, which is what is used in the Abstractive Text Summarization experiment! Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1. Luckily, Figure 1 (c) can be compared, and if cross-referenced with Table 1, shows that Unlikelihood Training does better than ScaleGrad with a higher gamma. However, Figure 1 has no data on either Unlikelihood Training or a human baseline. It really should not be necessary to go looking through Table 1, Figure 1, and Appendix F to see that Unlikelihood Training is outperforming ScaleGrad on some metrics. Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.On page 4 the authors write Following Welleck et al. (2020), we apply greedy decoding in our experiments in this section. This allows us to evaluate the modeling capability exclusively. We will get into the matter of comparison to Welleck et al. 2020, but I would like to begin by addressing whether Greedy Decoding is a neutral choice that only tests modeling capability, because it is clearly not. There is a spectrum of generation algorithms between probability maximization and straight-forward sampling. Greedy is closer to probability maximization, but it only maximizes local probabilities (Meister et al., 2020) and inevitably comes-up with lower probability outputs than Beam Search or Bound & Branch (Stahlberg & Byrne, 2019). Welleck et al., 2020 show that Greedy Decoding results in better text along their proposed metrics for open-ended generation.Since Greedy Decoding is not a neutral choice, I do not believe it is appropriate to exclude stochastic decoding baselines from the given comparisons. Stochastic decoding algorithms such as sampling, top-k sampling, and Nucleus Sampling usually do very well on repetition and uniqueness metrics. Indeed, they can be seen to outperform all the other models on Table 16 in Appendix H.In the analysis section, tables are quite limited in their coverage. In Table 6 no comparisons are made to systems that have not been trained with ScaleGrad, and these algorithms were not reported on in Table 1 so no comparison can properly be made even if the reader goes searching for the data. In Table 8, Unlikelihood Training is not included in the comparison even though it does very similarly to ScaleGrad on the same task in Table 5. Finally, Table 5 shows that trigram blocking still helps significantly on ScaleGrad trained systems. This is understandable, but disappointing since getting rid of these kind of rules is described as the reason for proposing ScaleGrad.Altogether, I feel the comparisons made in this paper are not quite convincing and the argument about why Greedy decoding, a deterministic algorithm, should even be able to match the properties of a large, noisy distribution is not properly fleshed-out.Meister, Clara, Ryan Cotterell, and Tim Vieira. "If Beam Search Is the Answer, What Was the Question?." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.Stahlberg, Felix, and Bill Byrne. "On NMT Search Errors and Model Errors: Cat Got Your Tongue?." Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. POST-REVISIONThanks for the revisions made to the paper, particularly for elaborating on the heuristic to speed-up the inner subroutine, and the clarification on the generalization bound.Also thanks for the running time numbers. Would be great if you could report them in the paper (if you haven't already).I'm revising my score for the paper to 7. ==========Post rebuttal comments==========The rebuttal has addressed my main concerns. In general, I believe the core idea of modeling substructure interactions in GNNs should be shared in front of more audience. Therefore, I have increased the rating accordingly. ### UpdateThanks to the authors for their response to the reviews, and to the other reviewers for their comments as well.  It seems that we were all confused about many of the same things.The authors have clarified some of the points raised, and I appreciate that.  I also continue to appreciate that the empirical results are promising.  At the same time, I personally remain confused about how to reason in a quantitative way -- suitable for diagnosing problems, determining hyperparameters, etc -- about the resolution tradeoffs that are described.  In other settings involving graph coarsenings, wavelets, graph Fourier transforms, etc, there is usually a more quantitative way of expressing what information is lost and retained by a given type of compression.  The discussion of coherence in the appendices says something about this, but not in a way that I would understand how to operationalize.I also appreciate that the chemistry example in the appendix exists, but I do not understand how to visually interpret the picture.My recommendation remains a weak accept, as I think it is likely worthwhile to put the empirical results out in the world, and the theory may follow.  But I remain wary of my own lack of understanding of the theory in a quantitatively meaningful way, and would also welcome the chance to read a future version that had some of these aspects more clearly worked out and explained. ----Update: thanks for the response! I read over the updated draft also but I'm still not sure what insights we learn about the fragility of NLP models under this evaluation paradigm. For that reason I'm still confused as to whether this paradigm is better or worse than the original approaches of Gardner et al / Kaushik et al 2020, and so I'd like to stick to my score. **Update after author response:**I went over the author response and have had a chance to carefully evaluate the updated draft. I appreciate that the authors have taken the time to address two of my comments but I believe major concerns still remain unaddressed, so my evaluation remains unchanged.1. I appreciate the time taken to conduct the fastText experiments, would be helpful to show this on BERT (which is arguably your strongest model) as well.2. Per my reading it appears that you've shown how models behave if the replacements are sampled at random. I appreciate the time taken to show that. I believe this could be made better with a concrete discussion of the same.Cons 3, 4, 5, and Additional comments 1,2: I do not think these have been convincingly addressed. You point to Dynabench while suggesting that this would eventually lead to better evaluation sets but as it is shown in [1] and [2], "difficult" evaluation sets that are tied to one model are not necessarily difficult for other models. If the idea is to identify model's vulnerabilities and not use these as common evaluation sets, then this is more or less the same as generating adversarial examples (but called "difficult" examples, in which case this would be better presented as an application/extension of Zhao et al to identify adversarial examples in NLP). If the idea is to only use these examples for evaluation purposes, then the comparison to counterfactually augmented data (which you show does not include "difficult" examples per your definition) does not make sense since that is meant solely for augmenting training sets. Discussion in Section 4 and onwards is shallow and often unclear. That is primarily where I believe the exposition can be improved significantly. For instance, Section 4 says FIM values capture resilience to linguistic perturbations but that has not been discussed in any of the paragraphs that follow. Section 5.1 ends in a sentence that says, "By repeating this process multiple times, more robust classifiers can be created", and that is not discussed any further as to why you think that would be the case. It is also not supported by any theory or empirical results presented in your prior or updated draft.[1] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering." In TACL.[2] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. "Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension." In TACL.--------------------------------------------------------- Post-rebuttal:- I thank the authors for improving the presentation of the paper and including additional experiments comparing to latent ODE.  =====Update after rebuttal=====I have read the authors' rebuttal. Most of my concerns are addressed properly, and hence I am willing to increase my score from 4 to 6.  Post Rebuttal Update:Due to the remaining confusion among reviewers about the equations in the manuscript, I maintain my score. ###update###As no author response, I would like to keep my rating. -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing some of my questions and concerns.I have now updated the score. **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the clarifications and improvements made to the paper and have increased my score 5.My concerns about the generality of the framework (as also pointed out by Rev1) still hold, however, as an evaluation on non-image data is still missing. I encourage the authors to extend their work further into this direction, but as is, I would keep my recommendation to reject.##### ------- UPDATE ---------I thank the authors for their response to my concerns/comments. It seems like I have to defend my position for suggesting a rejection of the paper. While the response of the authors has clarified some aspects, some comments have not been adequately addressed.R1: However, the authors could not show how to measure the structural differences between the source and target graphs? Their measurement is not stable and is different between different datasets. How one can evaluate a new dataset is good for transferring or not (R5)? You could see how Ron Levie et al. showed this in their experiments. R2: But embedding using GCN is another definition for the function of the structure. However, it does not work well as node degree. Therefore, the author cannot point out that you only need to construct the features based on a function of the structure.R4: You can somehow use their idea to show how much difference is reasonable to transfer information. R5: So it seems this is one of the main drawbacks of considering \delta. If that is the case, you need to discuss it in the paper. R7: I believe the response is not sufficient. The authors need to add that to show they could positively transfer information. --- Post Rebuttal --- I read the author response and I keep the original rating due to the limited operating range of the proposed method. Post rebuttalThe authors addressed the concerns around the clarity of the paper and added useful additional experiments. I will increase my score to 7. *Post-Rebuttal Evaluation*I have carefully read the response provided by authors and checked the revised manuscript. I confirm my preliminary acceptance rate. =====================================================================================================After reading the authors rebuttal, my major concerns are fully addressed and I decide to keep my decision as weak accept *** after reading rebuttal *** All my concerns are addressed, and I decide to improve my evaluation. EditUpdated score from 4 to 5 **The revision answered my concerns about the experiment by enlarging it, which is why I upgrade the score I gave. I think it is a very good and interesting paper.** ***Post-rebuttal review***I have increasing my recommendation to a 6. This is on account of the improvements to the submission by the authors, a detailed rebuttal, and somewhat to align with the recommendations of the other reviewers. I still do find that the experimental results could be improved somewhat, but as another reviewer pointed out, this is not a huge concern within the majority of the equivariance literature. I thank the authors for an interesting read and thank you for a good rebuttal. ### After-rebuttal commentsThe authors andwered my questions. My decision stays the same UPDATE:I thank the authors for their detailed response and edits. Overall, the authors have addressed my comments, but I have failed to understand some parts. I am still (slightly) positive about this work. >> Please check again for typos. **Update From Author Reply**I am grateful for the author's replies, edits, and additional evaluation, all performed within limited time. This helps me feel confident in my accept (7) recommendation. My reason for not giving a higher score remains the limited experiments (which is likely not something to be addressed in two weeks), but even so I think the work is quite worthy of being accepted. The methods are a significant contribution and the experiments are sufficient to demonstrate they work. ======After author response: I have read other reviews and the revised version. I think the paper's overall quality has improved. I decided to change my score to 7. **Update** : Since most of my issues have been addressed, I have changed my rating from 6 to 7 COMMENTS AFTER THE REVISIONIt was amazing to see the authors updated paper with the new experiment on the baselines (Table 2) and machine translation (Section 6). Although the baselines (overlap and synonym) were strong, I can now see that the proposed approach is better than these simple baselines both for machine translation and summarization. Section 6 also demonstrates the usefulness of this work for machine translation trained with self-training. For this reason, I increased my rating.The impact of this paper would be greater if Section 6 could include more results on different MT datasets (e.g., WMT) and/or self-training for summarization. ===========Update: Thank you for your clarifications and updated paper, which addressed several of my concerns. I therefore increased my score by 1.> First, we want to stress that we are not proposing a reference-free evaluation metric for quality estimation. I understand it is not meant as a general quality estimation metric, and my point was more about the *reference-free* aspect. I tried to make a broader point which I think applies to any kind of reference-free metric (whether it is for quality estimation or specifically about hallucination). ---Comments after reading author response and revised paper Thanks for showing more results of how token-level hallucination detection can be useful/effective: e.g., (i) labels per POS tagging in Fig. 5 and (ii) application to low-resource MT in Sec. 6.For (i), I don't think it resolved my question directly: "The proposed metrics treat all the tokens equally, while in reality, tokens such as noun phrases or verb, for example, may have a larger impact on the hallucination issue than prepositions or articles." That is, a hallucinated NN, for example, might be worse than a hallucinated II, rather than asking how the labels are distributed by POS categories. So I still wonder if it makes sense (as a reliable metric) to measure hallucination at the token level (e.g., Table 5 Hal words %) but it remains unanswered.For (ii), it is indeed an interesting plus to the paper, showing that the token-level labels appear to be useful for downstream applications (even if it's not quite meaningful to measure the % of hallucinated words). I would suggest doing more studies on downstream tasks as mentioned in your response to enhance the paper if you are "not proposing a reference-free evaluation metric for quality estimation" (which seems a bit contradictory to "we hope to create a large-scale pretrained evaluationmodel for any datasets or models to be evaluated" in the conclusion section btw).  ---- Update ----I thank the authors for clarifications. I trust that the suggestions of all reviewers, taken together, provide substantial avenues for improving the work. However, at this point I must keep my score and encourage the authors to continue the work with the valuable honest feedback provided here. Thank you for the additional experiments. Especially, Figure 7 and 8 look promising.My conclusion from the experiments is that the "contrast" corruption (used for validation) seems to be general enough, in the sense that for many other corruptions, encountered at test time, the performance is good.However, as AnonReviewer1, I am not sure about why the methodology seems to work well for very different types corruptions at test time, and completely OOD data (Figure 7 and 8). More empirical/theoretical analysis would be nice.Increased rating to 6.------ -------------------------------------------------------------------------------------------------------------------------------Post rebuttal: I have updated my score based on the clarification provided by the authors. My remaining concern is that I still think the baselines considered by the authors is incomplete. In particular, the calibration under distribution shift techniques can still be applied, just using either just a single test image or their set of multiple test images. Admittedly, this approach would probably not perform well for a single image, but in Table 5, it seems like oftentimes multiple images are needed to even beat Ovadia et al. (2019). Post Rebuttal:Thanks for the response, and the new experiments. I continue to think that this is a nice simple method that works well enough to be interesting. I retain my initial rating. ## EDIT: UpdateThe paper was improved. In particular I like the added explanation of the batch-norm and some improved explanation and phrasing.Nevertheless, the experimentation remains weak both compared to the state of the art and previously published work (e.g. CIFAR reported in Trabelsi et al., 2017 and Gaudet & Maida, 2017).Therefore, I increase the score by one point, as this work is just very slightly above the acceptance threshold right now.  ===============Post-rebuttal:I would like to thank the authors for their rebuttal and addressing some of my concerns. However, after reading the updated manuscript as well as the other reviews, I decide to maintain my current ranting. I would still like to see more rigour in the paper: more of the mathematics need to be fleshed out and how the proposed approach compares with the existing works in complex/quarternion networks in a more mathematical way.  Update:I read the response of the authors. In considering author response:Thank you to the authors for continuing discussion on the points raised in my review, and for further clarifying the nature of the data as a kind of unidirectional ambiguity problem. I understand this better now and can see a contribution in releasing this data / data-generating process for other researchers studying autoformalization. On account of this I'm going to raise my initial scoring.On the subject of methodology, I still think there are reasons to reconsider this work.  As discussed, the translation baselines were not great.  I think it's not really fair to compare those models without pre-training on data that was too small to learn basic tree properties.  It is possible that translation models that perform string-to-tree translation would perform better here(1), though results from natural language translation would hint towards the pre-trained models still performing better.  Translation models used in the domain of programs seem more suitable as well, and there's a good number of these, and there is a natural desire to generate strings that reflect a properly nested tree (2).  There is also work on mapping strings to knowledgebase queries that seems similar in input/output (IIRC, Luke Zettlemoyer's had a number of important papers in this line).But at best these would still be comparisons of mostly off-the-shelf translation models, which doesn't leave the reader with much of a takeaway.So I'm left feeling that if the authors want a useful quantitative comparison, these methods should be explored.  Pre-trained model beats model trained on only in-domain data is not to me a story significant enough to warrant inclusion in the conference, even if it contributes a new dataset (as the modeling is presented as a contribution here).  Even off-the-shelf methods can of course be part of an important contribution when the authors show that they have pushed the field further with an important result (say, GPT) but I do not feel the evaluation in this case supports that conclusion.It seems more natural given that none of these methods are likely to out-perform a vanilla pre-trained translation model, that the problem description and qualitative evaluation are of the utmost importance.  I would really recommend expanding this beyond half a page, to give the reader a better idea of what problems are solved and what are remaining.  It also seems that some of the errors pointed out (like those involving ellipses) would likely be remedied by additional synthetic data.  As I'm the most dissenting reviewer, I would still hope the authors attempt to improve the results section with the additional page upon acceptance.1.Towards String-To-Tree Neural Machine TranslationRoee Aharoni, Yoav Goldberg2.Tree-to-tree Neural Networks for ProgramTranslationXinyun Chen, Chang Liu, Dawn Song =======================================================================After reading other reviews and authors' responses, I upgrade my score to 6. Despite its relatively small evaluation data, I think the setup of the task of autoformalization could still contribute to the community and inspire more researchers to make efforts in this direction.  --- Post Rebuttal ---I read the author feedback.  The typo in Question 1 is fixed and the issue with the edge weights is addressed. However, the proposed method requires model-specific modifications and cannot be applicable to other tasks on graphs, e.g., link prediction. Due to the limitations, I will keep the original rating. ==========Post rebuttal==========I appreciate the authors' effort on answering my questions. Meanwhile, the authors' response does not fully address my concerns. I keep my rating as it is.  After Response==============================I agree that different DARTS paper usually uses some different settings, the lower PC-DARTS performance in the paper could be reasonable.Search Space design sometimes can greatly impact the result of the final result, but it makes sense to modify the search space for a better result. I am glad that you state the situation of the performance on the original DARTS space.I also read other reviewers' opinions. Based on the author's response and my previous rating, I decided to keep this score. This is an acceptable paper, but still has something to do in the future, like a more comprehensive experiment part. == Post Rebuttal ==I am satisfied with the response "ensemble baseline" and "NA-DARTS-ES". But my concern about "Improvement not significant" is not addressed, which is also mentioned by R1 and R4. I will remain my score as 6. # Rating and comments after the rebuttalI think that in the revised version the paper has addressed many of the weaknesses pointed out in our reviews, hence I increase my rating to 6. Nevertheless, the paper still packs too much information which makes it difficult to read and appreciate.Regarding novelty, although I agree with other reviews that the core idea is not novel I think that it is important that the paper stresses the applicability and usefulness of considering likelihoods in deep learning models, as it appears to be not fully appreciated currently.Overall, I think that the paper would shine as a journal paper while it is only a borderline submission in its current form. Edit:### RebuttalI had read the rebuttal and the other reviews. It seems that there are some clarity issues, which are independent from my knowledge of the area chosen for the experiments. Moreover, outlier detection is not necessarily the only possible application.However, I consider that the Bayesian point of view (which comes from preceding papers) has been well highlighted in the revised version. These points put together, and given that I'm not sure of the significance of the experimental part, I do not change my rating. ----------- Updates during discussion -----------I have updated my score from a 4 to a 5 based on the authors' response. My justification is below in my comment to the authors. ----------------Post-rebuttal:After reading the rebuttal I maintain my score. The observations seem promising, the method a bit cumbersome but also interesting, but neither of them are fully fleshed out. The authors should either greatly expand the empirical analysis (in the non-linear setting) of their claims on intra and inter class variability in CL  and/or make the experiments of the DRL method more convincing and varied in scope AFTER READING THE REBUTTAL, I changed my score from 5 to 6. ===== After rebuttal =====I appreciate the fact that the authors update their manuscript, taking into account some of our comments (+1 for this). However, I still feel that the content is quite hard to follow. In general, I believe that the motivation is still unclear and the clarity low. Even Fig. 1 (that potentially should help the understanding) is so complicated without any explanation in the caption. Maybe the technical content is interesting, but I think that the paper is not accessible. I have tried to read the revised version submitted by the authors. Unfortunately, it is still very hard for me to fully grasp the proposed concept and to follow the derivation steps and the proofs. In my opinion, the paper is not publishable in its present state. ~~~~~This major concern is relieved after rebuttal.  ============================================The authors address most of my questions and concerns in the response, and update the manuscript. I decide to increase my score. Thanks for your response.I have taken a quick look at some sections of the updated paper. Some problematic paragraphs---e.g. the one that I long-quoted in my first review---are verbatim identical to how they appeared in the first version (other than s/"In addition"/"Also"). The few highly relevant references I mentioned have not been added yet, etc. As this is a work in progress, that is no problem; I understand that one would not be able to fix everything in one rebuttal period!My understanding is also that you plan to keep working on all aspects of the project: the writing, the evaluation, and the model itself. Best of luck with all this! After the rebuttal:I checked comments of other reviewers and response of authors.First, thank you for the detailed response. Since some of my questions and concerns are partially addressed, I improve the overall rating.However, there are still parts which should be improved:- Regarding manifolds: There are some statements which should be clarified and appropriately analyzed in the paper. For instance, it is stated that "The manifold of parameter tensors is a linear space, and so any linear combination of the parameter set with a tangent vector (another tensor) will remain on the manifold. We are changing the metric on the tangent space from the ordinary Euclidean metric to our channel directed (Sobolev, and re-weighted L2) metrics, which effectively changes the lengths of paths on this manifold of parameter tensors, making it a non-trivial Riemannian manifold."-- This statement claims that the "structure" of the manifold of tensors changes by changing the metric on tangent space by changing geodesic or paths on manifolds. However, it is not clear how the geodesic or in general, geometry of the manifold changes by just changing the metric on the tangent space (while, the tangent space can change depending on change of the manifold).  -- Then, it is claimed that this leads to a non-trivial Riemannian manifold (a nonlinear space), while it is also claimed that the manifold of parameter tensors is a linear space (why is it a linear space?). - Regarding experimental results: Thank you for the updated results. However, these results show that the accuracy of the proposed method is pretty close to the baseline. To show superiority of the proposed method, the experiments should be extended using larger benchmark datasets.-- There are Riemannian optimization method that encodes this channel-directed structure, but there are various Riemannian optimization methods which claim to improve accuracy of models in various tasks such as the following:S. Kumar Roy, M. Harandi, R. Hartley and R. Nock, Siamese Networks: The Tale of Two Manifolds. (oral), Int. Conference on Computer Vision (ICCV), Seoul, 2019.Lei Huang, Xianglong Liu, Bo Lang, Admas Wei Yu, Bo Li, Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks, AAAI 2018 (Oral)Therefore, the proposed methods should be compared with these methods in the analyses as well. -------------------------------------------------------I have read the response, and the rating is not changed. =================================**Update after discussion period**My feeling is still that the proposed method has a lot of interesting potential, but that the paper still needs some improvement. I've bulleted my main remaining concerns below:1. The clarity of the writing still needs to be improved in many places. Notably, the authors' discussion of their metric is mostly discussion-based (rather than providing concrete theoretical claims about their metric and its strengths). While this isn't necessarily a problem, such a presentation really needs precise language and phrasing so that the details of the claims and their supporting arguments can be completely understood. From what I can follow, I think many of the authors' arguments are going in good directions, but the writing should be improved to be sure.2. I still don't completely understand why $\beta$ is being fixed here. The argument in Appendix A.3 seems to say that if $\hat\beta$ varies, then the meaning of $\hat\gamma$ changes, and so looking at the change in $\gamma$ would not be meaningful. I agree, but that does not mean we cannot look at the combined change of $\beta$ and $\gamma$. It seems like this would be straightforward to experiment with on some smaller problems that only have parameters in the tens of thousands; I think it would be better to include such experiments rather than trying to argue verbally that such an approach will not work well.3. I am also not sure I see why we should not be interested in leave-one-domain-out CV. I appreciate the clarification that Figure 1 is actually showing CV's estimate of error, and definitely find this to be a compelling experiment. It is pretty surprising to me that the authors' metric could succeed in this case, given that the influence functions are an approximation to the parameter change as each domain is left out (this is true, whether or not this is the intent of the metric). I think further investigation of this point is needed. Would computing the proposed metric with the parameter changes under leave-one-domain-out CV not detect the OOD issues in Figure 1? Or is it just that feeding these parameter changes into the function measuring test error that is providing a poor assessment of OOD generalization in Figure 1?On a different note, I appreciated the increased discussion of "the shuffle;" I think this is an important part of the paper that didn't come up much during the discussion period. As a side note, I think it is more common to just call this sort of thing a permutation test. And, along those lines, it would be good to actually perform a permutation test (i.e. run over many shuffles and examine the distribution) so that we know for sure the reported shuffles aren't just unluckily high/low.=================================== ------- After Discussion --------The authors agreed that the paper is not ready for publication, yet. Thus I keep my original score. **Update**: Since there's no substantial author response, I'm keeping my score as it is. All the best for your future submissions! **Update after rebuttal** : I want to thank the authors for their rebuttal. However, after reading all responses and all new results presented, I still think that most of the weaknesses of this paper are still present. Two notes that I urge authors to take into account:* Even the updated Figs 2 and 3 are highly misleading: sure, the gains over a baseline ResNet network are big, but the comparisson should be vs other backbones that combine convolutions with attentions. * The authors should clearly say that they do use a convolution for conv1, and their modules start on top of that first spatial convolution.---------------------------------------- # Post-rebuttal updateI would like to thank the authors for the detailed feedback. I am now convinced about the statistical significance of the results. Regarding the additional study, while it is true that the combination of the changes, in addition to the softmax, was what made the results improve, the change is quite minor. Also, the biggest change comes from the fact that softmax is removed. The reviewer also finds the explanation of why this happens to be handwavy.Given the concurrent work [Wang et al. 2020] and the incremental nature of the innovation, the reviewer is not sure of the benefits of the current paper to be published at ICLR. Edit: Read responses, bumped score up to a 4, check full response below UPDATE on 12/7/2020:I read the other reviews and the authors replies.I hold onto my score and recommend the paper to be accepted. Here are some points I would like to highlight.1. Anonreviewer1 suggests the work is incremental, yet theres no citation to previous work on protein data augmentation. This is in fact the first major work tackling this issue.2. Anonreviewer1 points out the authors did not use a SOTA model. This is true, but the model they are using is little more than 1 year old. Moreover, while I agree with Anonreviewer1 that different models need to be used to see how generalizable the data augmentations are, I dont see this as a reason to reject a paper that is tackling an orthogonal issue of what modes of protein data augmentations work.3. Both anonreviewer1 and anonreviewer3 are pointing out that different augmentations seem to have worked best for different tasks, and hence the results are not strong or useful. But how would we know this if no one had even tried it? The authors found the results as such and thats an important contribution to the community's knowledge.4. I am not seeing the problem of data augmentation done on the validation set that anonreviewer1 is suggesting. The test set results should not be affected in a corrupt way in this case.5. I disagree with anonreviewer3s comment that the augmentations are with unreasonable intuitions. I dont think thats an objective reason to reject a paper.6. I agree with other reviewers that the contact prediction task should have been evaluated with whatever dataset that was available. Even then, the other 4 tasks in my opinion are enough as a first attempt at this area.Overall, investigating the protein data augmentation regime is something that is not fancy like building a new model or beating the latest SOTA yet a necessary task for which we should thank the authors. There are of course problems that will indeed need to be addressed in further work but there are also important insights in the paper. ### Post-rebuttal updatesGiven the answer of the authors. I will raise my reviews to 7 assuming the authors will add more about comapring with current sota methods, as discussed on this review. ======== score changed ==============My major concern has been addressed by the reply from the authors.  The revised paper has been improved.    Modifications after discussion:Increased score by one as the presentation in the revised version has clearly improved, along the lines requested in the original review. **Update**Thank you to the authors for the revisions, and great to know that the experimental results have improved significantly. In the absence of an updated manuscript, it is difficult to update my score appropriately, so I will leave it as it currently is. However, I think the work is promising and that an updated manuscript that incorporates the new experimental results and more carefully teases apart the contributions of the different components (as the authors have started to do in this rebuttal period) would be impactful. Thank you to the authors again for all of their hard work. _____**POST REBUTTAL**The authors have provided some clarifications. I suggest they use them to improve the paper.I'm increasing my score to a 5, thus still not in favor of acceptance. Post Rebuttal The authors have improved the context of their work and clarified their proposed method. While the technical novelty is somewhat limited, the proposed method does well and the benchmark introduced herein should be of interest to the community. As such I have increased my score. edit after rebuttal:My opinion about the paper has not changed. Although the general idea is interesting, my main concern is that the approach aims at performing defense against a specific attack. The robustness of the approach w.r.t. other attacks (such as L_2 and L_0) needs to be evaluated.==== Post-Rebuttal Comment=====I thank the authors for their detailed response to my concerns.While my opinion of this work remains largely similar, I raised by score from 5 to 6 for the following reasons:I do buy the argument that the proposed method "allow both developers and researchers to start from a strong method with a low barrier to entry in diverse domains".I do not share the concerns of R2 & R4 regarding the quality of results and fairness of comparison. As my primary concern (amount of technical innovation) is not shared by other reviewers, I am swayed to change my initial "on the borderline" rating to the positive side.I still would recommend adding the following evaluations:more diverse initial state for Figure 10.a more interpretable dataset for Figure 15: I think ShapeNet would demonstrate this point better. ### After authors responseAlthough authors have addressed one of my main concerns and some minor ones, I still have doubts regarding the fairness of comparison with the baselines (lack of proper hyper-parameter optimization) and therefore cannot trust the results. All in all, I keep my rating.  Post-rebuttal comments=======================================I appreciate the revisions and additional visualization provided the authors. The revised version of the paper provides clarifications that make the paper easier to follow. While the paper presents an interesting idea, I am not convinced of the effectiveness of the proposed method based on the results provided by the authors. Therefore, my final rating as 5 (same as the initial rating). Post Rebuttal Comments:I thank the authors for their feedback. I have no modification to make to my original review. Comments after rebuttal:-------------Thank the authors for the clarifications. I will raise my score to 5. Theoretical analysis of the proposed method is nice. But I still think the proposed approach was not well justified or motivated. Why is it the best option? And how are other simple baselines for improving both (not standalone) settings?  **Post Rebuttal**I thank the authors for the quick replies and updates to the paper. I keep my positive score.--- ------------------------------------------Score updated after rebuttal, please see comment below Post-rebuttal updates:I thank the Authors for their response. After reading all the reviews and comments I feel that there are aspects of the proposed approach that are not fully understood, despite the improvements. For example, those related to AugMix, and providing fully symmetric comparisons between Cityscapes and GTA5, as several reviewers have pointed out. For these reasons, I have decided to revise my ratings as I also recognize the importance of these observations.  Post-rebuttal updatesThank the authors for the great efforts in addressing the concerns. The new experiments on two new backbones DenseNet-121 and EfficientNet-B0 show the method can work well with multiple backbones, which is good. However, my other concerns remain unsolved. 1. Combination with AugMix seems necessary to demonstrate state-of-the-art performance and its orthogonality. 2. I still think the generalization from GTA5 -> CityScape should be listed together with CityScape -> GTA5. 3. The running time comparison should take the model's pre-trained time into account in Table 8.4. Regarding the blocks in the ablation study, I remember a ResNet50 for ImageNet has 4 blocks. Table 3 only lists 3 blocks (2,3,4). So the first block is not the first convolution layer. Therefore, I still keep my original rating. =======After author response: I would like to thank the authors for providing the details of each corruption in ImageNet-C dataset. I understand that it might be hard to compare with Volpi et al due to lack of implementation details. However, I still feel that this submission is a bit lack of depth and thus it may not be a good contribution to the ICLR community. I would like to see more theoretical or experimental evidence that can help us get a deeper understanding of this approach. Overall I decided to keep my score. ---- Post rebuttal ---I appreciate the responses from the authors, which partially address the concerns I was having. However, I am still not fully convinced that the proposed method is significant enough. Thus I am increasing my rating from 4 to 5. Final comments:I would like to again thank the authors for their time and responses. My concerns regarding novelty remain as described below, but the authors did clarify some aspects. For these reasons, I have raised my score from 5 to 6. If the work is not accepted at this venue, I would like to encourage the authors to continue with their work and submit to a later venue. -----------------------I thank the authors for their detailed reply. The revision after the initial review solves the concerns and questions I had. Very good work. I vote for accept. Thanks to the authors for providing a response to the review comments. ########################## Update after rebuttal ##########################I thank the authors for their detailed response; several concerns have been addressed in the rebuttal. I would encourage the authors to use commonly used practices to improve the robust performance of models in Tables 5 and 6. The use of early-stopping [3] can significantly boost the robust performance, and produce models with better clean accuracy than is presently reported for Adv. Training. I would like to update the score to 5 based on the author's response. Aside from the robust evaluation, the improvement in clean accuracy is of a relatively smaller magnitude given the disproportionate increase in training requirements. Thus, I have not further increased the score.[3] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020, https://arxiv.org/abs/2002.11569  Post rebuttal evaluationI thank the authors for providing answers to the raised questions and providing further experiments. Regarding Figure 3, I suggest that the authors provide accuracy as a function of wallclock instead of epochs currently reported in the paper. As a result of the authors' responses, I increase my score to 6. [After rebuttal] I think the authors improved their paper by taking into account the remarks. Given the last results obtained in Table 4, it looks like the structural features are indeed very good features in practice as they allow to boost the performances of a very simple invariant architecture like Deepset. I think the authors should explore how they can combine this approach with the coloring approach to get better GNN. =====Update after rebuttal=====I have read the authors' rebuttal. Considering the limitations and non-superior performance for larger datasets, I am keeping my score unchanged.  ---------------------------------------------------------------------------------------------------Satisfied with the response, will keep my score the same. [EDIT AFTER DISCUSSIONS] I thank the authors for their answers to my comments. Having read the various threads, I confirm my score and see interesting work in this paper.[/EDIT] ---------------------------------------------------------------------------**Post-rebuttal**See my post-rebuttal response below. I have some remaining questions but the authors addressed some of my concerns. Therefore, I am increasing my score. ----------------------------------------------------------------------------Looks like the post-rebuttal response is not available for the authors to see. I am copy-pasting it here. I thank the authors for their detailed rebuttal. **Deeper layers get more affected by backprop:** I appreciate the authors' response. The paper they referred to does not seem to have conclusive proof of which layers change more during the training. For example, Figure 1 in that paper shows that on FCNs, the latter layers change more. On CNNs, Figures 14, 15, the results are less conclusive. Anyhow, when I wrote the review the setup that I had in mind was a solution of the previous task as an initialization of the next task and not a random initialization and standard training -- a setup that had been the subject of the referred paper. In the former case (continual learning case), the solution of the previous task should inform on the next task and I expect the earlier layer weights to have small changes. However, what the authors of this paper showed in the revised draft (Figure 31) is quite interesting. They showed that, in the L2 sense, the weights of the earlier layers change more than that of the later layers. This shows that despite the large weight changes in the earlier layers their representations don't change much (as measured by the CKA). Am I understanding that correctly? Could I request the authors to verify this observation on ResNets (I believe Figure 31 is for VGG), and perhaps, other datasets as well?**Sec 4.1**: Regarding freezing the earlier layers and not seeing any performance degradation on the subsequent tasks, I asked whether this would also hold for setups where input distribution changes (e.g.) Permuted MNIST. Could the authors please address this and add the experiment to the paper?My other concerns are addressed, therefore, I am increasing my score.  ***********************************After carefully considering the rebuttal from the authors, I think I am more positive about the paper so I raised my score. The rebuttal clarifies most of my confusion about the paper, though more improvement can be done.  -----Post rebuttal comment: I appreciate the authors' responses, especially on highlighting the theoretical challenges. I have raised my score accordingly. #######################Update:The authors have addressed my questions adequately. In particular, my main concerns with the theoretical results and proofs have been fixed. I have updated the score from 3 to 6 for now. ********* Update ********The reviewer appreciates the efforts authors have made. However, the response does not fully address the issues. In the new experiments the authors used small sketch size to show clearer advantages of the learned IHS (Figure 2 & 4), but at that regime all of the methods including learned-IHS are converging only slowly and not practical compare to the slightly larger sketch size choices. The reviewer believes that such a comparison is not meaningful. In order to truly demonstrate the benefits of learned-IHS (which seems to be robust to small sketch sizes), the authors should choose for each algorithm the best sketch size and then compare them in run time, at least between learned-IHS and Count-sketch IHS.Beside the flaws in numerical experiments, the reviewer found that the theoretical contribution of the current version is incremental. From the reviewer's point of view, the meaningful analysis for learned IHS should certainly be how the converge relates to  the statistics of the training data, to show the benefit of the learned sketch theoretically (i.e. to show how much better the learned sketch SA compare to the unlearned ones in terms of Z_1 and Z_2). The authors have avoided such type of analysis and the main results are "safe-guarded" by the concentration of the random sketch, which are easy to derive.Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences. ### Post-rebuttal updates- Thanks for the detailed response and additional evaluation.- Q1. Fair point -- it appears that defenses do assume attackers query from a reasonably different distribution. This defense does make this assumption explicit by including it in the training objective ($D_{out}$ in Eq. 3). But then again, it seems typical for OOD-based defenses (e.g., AM).- Q3. This concern also generally connects to Q6. It would be nonetheless interesting to analyze scenarios where the attacker attempts to use some auxiliary knowledge to break the defense. This is also shared by some other reviewers.- Q4. Thanks for presenting the curves. Assuming strictly non-overlapping OOD data does seem like a strong assumption though.- Nitpick editorial comment: Please serif-based fonts for text in equations e.g. $argmax(\cdot) \rightarrow \text{argmax}(\cdot), index \rightarrow \text{index}$- I am slightly increasing my rating. ---------------------------------------------------------------------------------------------Post rebuttal update:After reading the clarifications from the authors, it is now more clear that the dataset is about the learnability of certain hypotheses as opposed to that of test-time verification. I am generally satisfied with the responses given by the authors, and willing to buy the statement that having a per-concept breakdown of learnability can be seen as a feature rather than a weakness pertaining to the use of a "toy" dataset. I think there are some valid insights as provided by this work, though I am more skeptical regarding the superiority of transformers on disentanglement tasks as the improvement gap was relatively small and there are various implementation details in the used transformers and relationship networks that could presumably shrink or even invert that gap. Overall, I think the writing clarity is still a significant concern. Several detailed passes over the paper were necessary just to get the general picture of what was going on. I think the toy example provided in the author's response is certainly helpful, and should improve the paper in this area somewhat. Based on the above, I am upgrading my rating to marginally above threshold. ======= UPDATE after reading author response:  The authors' example answered my question about the nature of the "uncertainty" in their setting. I have increased my confidence score and have retained my marginally-positive evaluation.  ### Updates after the rebuttalI like the paper and found the revised version more transparent. I support the engineering approach of the paper; however, as we all know, these papers often require authors to go to greater lengths to convince. After reading the other discussion and reviews, I think the authors can consider a few additional experiments. I would suggest investing in a more involved toy-experiment to better motivate the engineering solutions. If possible, authors can also consider a more careful ablation study to establish the relevance of each component on this toy-model. Further, the authors offered explanations for the training time aberrations; if possible, authors can consider including the equally-fast-variants in the revision to be more convincing.  ### EDIT: I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft. Unfortunately, I'm still not very happy with the motivation of attacking the drifting phenomenon on MINE. The main reason for removing the drifting effect is for moving average of history outputs. However, there are various ways for tackling this ( as pointed out in my original reviews, like using a non-drifted mutual information estimator with moving average or plugin some robust density estimators). Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. But actually, the stability of other MI estimators also allows them to avoids having exploding network outputs. Also, in practice, people don't usually run moving average on MI for representation learning. I encourage the author to explore the importance of moving average of MI estimators further.R3 suggests the author take some non-parametric estimators as baselines. But I think it's fine to only compare to some parametric(variational) methods on high dimension setting, where most non-parametric estimators fail. Nonetheless, it's always good to have additional experiments compared to some non-parametric methods in low dimension settings.Overall, I lean toward rejection given current concerns. Response to authors: After reading the authors' response, I have decided to maintain my original rating. The authors have not adequately addressed my main concerns.Novelty: The work here, as indicated by the authors, is largely an incremental improvement over an existing work MINE. The authors' response did not alleviate this concern and in fact reinforced it.Citations and comparisons to other work: The authors did not agree to even include citations to important literature in this area. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions.Theoretical work: The authors simply pointed to the theoretical work for MINE. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). More theoretical work is needed in this area to justify the use of these estimators over others.Some responses to other comments that may help the authors with further revisions:(1) The presentation of MINE should occur in the main paper as this is crucial for understanding the paper.(2) This was not clear. Perhaps the authors could include similar pointers in the paper with each of these issues.(3) The bias I'm referring to here is the actual statistical bias of the estimator. From Theorem 6, it seems that the drift problem does seem to create some bias but it would be useful to quantify that, which could then lead to a bias correction approach.(7) The way this is currently worded, it sounds like you are saying that training with a larger batch size is bad. This part should be clarified to avoid this.  ** Edit after author response **I've raised my score after the response from the authors regarding my concerns with the theory. ** Edit after further author experiments **I think the paper is now even stronger given the inclusion of additional experiments into the failure cases of EPIC. I have upgraded my score accordingly.  ============ after rebuttal =============Thank you for the response and revision, and I am sorry for my late response.  One experiment that I can think of to get around the issue of failure in RL optimization is to consider simple examples where RL algorithms can surely find the global optima.  This is the case for the tabular setting (like your GridWorld). This can be a proof of concept, but I admit that it might also be limited. So I think it is okay to keep your current presentation.  Update: I thank the authors for the detailled response. Due to the number of required changes and the feedback of other reviewers, I believe the paper needs a major revision before publication and still recommend rejection.--- After reading the authors' response, I would like to thank the authors to clarify some of the my questions. But my main concerns still remain so I plan to keep my marginally below score. The value of the key bound in Lemma 4.1 is unknown and it's not clear about the effect of the described empirical trick. For the experiments, the proposed SPACE algorithm indeed outperforms all considered baselines. But performing better than all baselines in the final performance seem to indicate that the choice of baselines and experiment design do not present a fair comparison. One would expect the final performance of a reasonable baseline algorithm to be similar to SPACE because the idea of the proposed method is to "accelerating learning" not achieving better final performance. The poor final performance of the baselines seems to be due to the inappropriate adaption of existing methods. update after rebuttal: the authors answered all my questions to my satisfaction. ** UPDATE after reading other reviews, author responses, and revised paper **I'm unconvinced by the arguments related to the resizing of feature maps between ResNet branches. The added experiment in A.1 does not address the question of misaligning the features between skip branch and convolution branch. Figure 8 suggests that the alignment with input is well preserved with resizing (i.e., Fig. 8(a, d) look similar). The illustration is misleading because it uses a kernel with constant weights and *extremely* oversaturates the colors. In other circumstances, we would see that each corner in Fig. 8(c, d) is different. This is because Fig. 8(b) will contain four identical blobs after the convolution, and Fig. 8(c) is by definition a central crop of Fig. 8(b). Thus, the bits we see in Fig. 8(c) are different quadrants of this blob, and they are in fact all different. This leads to the misalignment between skip and convolution branches, so that subsequent processing will see different data at each of the four corners. The figure makes it look like there are only small differences by oversaturating the colors to the point that only the footprint of the convolution can be seen. Using a non-symmetrical convolution kernel would have also highlighted the differences.The experiment also misses the larger point: The different architectures may cause differences in the results that exceed the effects of the padding itself. The paper claims to measure the latter, but I believe both AR2 and I are concerned that the paper may be measuring the architectural effects instead. As this was my main concern about the experimental design, my confidence in the paper's results is not increased, and my rating is not changed. If anything, I'd be inclined to lower my score because of how misleading the added Fig. 8 is. The rebuttal provide valuable information that strengthens the original paper. While some of my concerns are not resolved in the rebuttal, they require additional experiments and may be beyond the scope of rebuttal. .Update after Rebuttal:I do appreciate the argument given in appendix A.1 about the relative position that changes with cropping and that boundary information may get lost. However, it is not clear to me if that change in relative position matters because the network could take this into account. However, the issue that the feature maps between the shortcut and residual connection do not align with the bilinear interpolation seems to be much harder take into account for the network. To me it seems an actual example on the ImageNet classification (i.e. Table 2) that shows that the degradation in performance is not due to the resampling/misalignment of the feature maps in the ResNet would be very important. Moreover, a related problem is that it seems plausible that a network could extract position information from the spatially varying misalignment of the feature maps (in the image center there is no misalignment and on the border there is 1px (for 3x3 conv). The amount would reveal the position). Therefore an experiment that shows that this does not happen in practice would also be important. This is the same major concern I share with AR4 even after the rebuttal ---Thanks to the authors for the clarifications. I have read the other reviews and responses and still believe that the paper is a good contribution. Therefore, I am keeping my score. ________________________Thanks! The authors' update was well-received. I like the way how ARM formulates the unlabeled adaptation. Though "generalizing" meta-learning from labeled to unlabeled adaptation is natural and simple, ARM (proposed in this work) is crucial to highlight this and present the results to the community. I increased the rating to 7. Update after author response I thank the reviewers for their response. I appreciated the more careful discussion of the discrete claims (as other reviewers also noted). I also appreciated the efforts to give more justification for your architecture changes. While the actual experimental results still seem incremental in terms of raw performance, I think the other contributions of the paper are solid and worth having in the literature. Thus, I've updated my score. ----------------------------**Update after author response**: Thanks to the authors for answering my questions and for incorporating feedback into the paper. Through discussion, I think we got to the crux of the method: distance functions seem to work better than classifiers for imitation learning. I think this is a really neat observation, and potentially quite important; the experiments definitely support this hypothesis. That said, I don't think the paper goes far enough in exploring this hypothesis, either experimentally or theoretically. There are a number of confounding variables, such as data, loss function, and architecture, which each will need to be accounted for. I therefore stand by my previous vote (4) to reject the paper. With more thorough experiments and ablations, I think this will make a fantastic submission to a future conference. **Post-discussion update:**I would like to thank the authors for addressing (albeit partially) my comments, as well as the comments from other reviewers. While I understand that some connections can be made between the proposed approach and other approaches or aspects that go beyond local smoothing or oversmoothing, this is somewhat anecdotal in my opinion. More generally, it is still not entirely clear to me how significant are the contributions here. Reading the other reviews, it seems these concerns are also shared by other reviewers, although I still think the analysis here is not without merit and this warrants it at least a borderline score. Further, there are some interesting insights provided here, which place this work slightly over the threshold. Since marginally above the threshold was already the score I gave the manuscript initially, it remains unchanged. ------ After Discussion -------I had an interesting discussion with the authors that clarified some important points. I came to the conclusion that the topic is interesting and the study is valuable, however, not yet conclusive. Consequently, the paper is not ready for publication, yet. Thus I have lowered my score by one. I hope the authors continue this work, since I'm convinced a complete empirical study on the impact of the gradient norm along the training path is insightful and a valuable contribution to the community. ============After rebuttal:I thank authors for their response. I share concerns with others reviewers and I highly encourage authors to consider answering questions suggested by Reviewer 3 at the end of their discussion. I believe a systematic study of gradient norm is interesting but this work does not provide a solid set of answers. As such, I'm reducing my rating to 4. =================================================================Post-rebuttal update: Thanks to the authors for their clarifications. I have read author response and understand that the cons I mentioned above might be orthogonal to the focus of this paper. Thus I have revised my rating to 6. Post-author response: I have read the response and am satisfied with the answer. I am leaning more towards accepting this paper. Post-rebuttal comments: The authors have addressed my concerns in their rebuttals. All reviewers give positive comments to this paper. So I would like to give an accept to this paper. =====updates========Most of my concerns have been addressed by the rebuttal and all the other reviews are positive. l will remain my original recommendation. ---------------------------------------------------------------------------------------------------After Rebuttals---------------------------------------------------------------------------------------------------Thanks for your responses. I still have doubts about the proof of lemma 7. You not only change from '2' to '3', you also add a new term (the fourth term). I admit that $1 + 2/k < (1 + 3/k)(1  \mu*\eta)$ holds under your assumption, but you also need: $\mu*\eta> 1/k$, i.e. let the fourth term be greater than zero, to make the inequality hold, however, this contradicts the previous assumption. So I still believe this inequality does not hold.  ====After rebuttal:I read all reviews and rebuttals, especially the argument about Lemma 7. I agree with reviewer 2 that the Lemma 7 is not correct. The assumption of $\mu\eta < \frac{1}{42k}$ contradicts with the requirement $\mu\eta > \frac{1}{k}$ of term 4.  This error needs to be resolved before acceptance.  ===========================================================================================================Added after author response:--------------------------------------------------------------------------------------------I have read the author responses and other reviews. I believe authors have a sufficiently addressed my concerns regarding the quality of the paper. I understand the improvement in using momentum in this way and I think it is decent contribution. However, considering this improvement and the concerns raised by other reviewers, I maintain my score. Post-rebuttal update: Thank you for your response.  My concerns are relatively minor and I believe this work is above the acceptance threshold. ### POST-REVISIONThanks for revising your algorithm and clarifying its theoretical properties.I think the changes made to the algorithm do seem substantial: you've changed a complicated combination of multiplicative and best response updates on alpha to a simpler projected gradient update  (I am assuming that the experimental results have also been revised accordingly). The description is now easier to follow and the convergence results now follow directly from Chen et al.I'm raising my score to 6, but still find the novelty in the formulation to be somewhat limited.One pending concern is that the algorithm maintains one parameter \alpha_i per training example, which can be prohibitively expensive for large datasets. Of course, one way to alleviate this difficulty would be to replace \alpha_i with a parameterized function of the features, which however, would make your formulation very similar to Lahoti et al. (2020).Is there a way you could measure the violation in the pareto constraint in your experiments, to showcase how the classifiers learned by your method are different from those learned by the approach of Lahoti et al?****** ## After Rebuttal ##I thank the authors for their clarifications and efforts to improve their work. I still support acceptance. ---Post rebuttal---Thank you for the detailed response. My main concern was regarding the scalability of the method to larger environments, e.g. w/ visual state space. I agree with the other reviewers regarding limited applicability of the method, and maintain my original score (Weak Reject).--- --------post-rebuttal--------I appreciate that authors have provided rebuttal that addresses many of my questions. I've read the updated paper and other reviewers' comments. The rebuttal has largely addressed my questions. However I am still not convinced by the following. Without a clarification I cannot see the significance of this work. So I'd maintain my rating. I still don't get the motivation why studying MIA in image-to-image translation. I really wanted to solicit a motivational example. The authors merely say "image-to-image translation has yet to be studied in the context of MIA." I'm not convinced by this.A.4 The authors simply draw curves of MIA performance vs. learning epoch. I still don't know where overfitting happens. From the context, it seems that the proposed method will fail if the victim model is not trained to be overfitting. In practice, surely victim models are not overfitting? ========Update:Thanks for the author's feedback that clarifies my concerns. Adding the difficulty score into training looks like a more promising future direction.  I have read the response, and my rating is not changed. _**Update after author response**: I think the paper has improved substantially. The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score._ .###############################################################################Final recommendationI thank the authors for their detailed response. The authors have clarified some technical details and blind spots of their methods, have fixed their evaluation metric which was wrongfully comparing tree sizes on solved and unsolved instances, and have presented an additional experiment in the appendix on the original benchmark from Gasse et al. These changes are going in the right direction. However, I remain concerned about the experimental setup in the paper, and therefore my final recommendation is still rejection.First, I still find suspicious that the main experiment in the paper is conducted on only 3 (modified) benchmarks out of the 4 proposed in Gasse et al. The authors claim they did not run experiments on the 4th benchmark due limited computational resources, but at the same time they present complete results on the 4 original benchmarks in the appendix. Therefore I believe the explanation given by the authors is fallacious. Results on the 4th (modified) benchmark, and even better, on other additional benchmarks, would be much more convincing and alleviate any doubt about cherry-picking.Second, I am not convinced by the argument the authors present to justify their two solver settings: clean and default. I do agree that challenging problems should be solved under the default setting, however I do not see why decision quality should be measured using the clean setting. Decision quality matters in the default setting as well, and one could argue it matters in the default setting only, if the final goal is to improve the solving time of the solver on challenging problems. Moreover, if the authors want to use the tree size as a mean to measure branching decision quality, they must also provide the optimal solution value to the solver at the beginning of the solving process, in order to deactivate side-effects from pruning. See G. Gamrath and C. Schubert, 2018, "Measuring the Impact of Branching Rules for Mixed-Integer Programming".Last, the method proposed by the authors seems to be effective only in the specific benchmark they propose. In the additional experiments they present in the appendix (Table 4), their method does not convincingly improve over the original method from Gasse et al., as the performance gains on the Easy training instances degrade rapidly as one moves away towards the more challenging Hard instances. The very name of the paper, "Improving Learning to Branch via Reinforcement Learning", seems to claim that reinforcement learning improves existing learning to branch methods. However, the improvement observed by the authors is very specific to the "backbone" setting they propose, and does not seem to translate to the original benchmarks of the methods they compare to. The authors justify their choice of benchmarks saying "we focus on a more realistic industrial setting", but I am unsure whether the hypothesis of a "backbone graph" is particularly realistic in industrial applications. I do not think the original benchmark from Gasse et al. is particularly realistic either, however I would not give more or less value to either one of the two. As such, I believe it is crucial that the authors report experimental results on both benchmarks in the main body, and provide a discussion as to why RL seems to bring improvement in the restricted "backbone graph" benchmarks, but not in the original ones which have more variability. This, in my opinion, would have a much higher scientific value than simply presenting both a new method and a new benchmark, while disregarding how the method performs on previous benchmarks from the literature.In light of the changes made by the authors I am willing to raise my rating, however I still recommend rejection for ICLR Post discussion:I read the author's response and other reviews. I will stick to my rating and encourage the author to resubmit a revised version focusing on the antisymmetric case.  Update:I have read the authors' responses and other comments.  I still think that the theoretical results on anti-symmetric functions of this submission are novel, which is, however, not well-delivered. A lot of space is wasted to discuss symmetric functions, for which the contributions of this paper are not clear.  I suggest substantially rewriting this paper by only focusing on the anti-symmetric functions. =============================================================== %%%%%%%%%%%%%%%%%%%%%%%%%%%Added after author response. My enthusiasm for the paper has diminished because it seems to be more of an incremental step over Gao et al 2019 and the authors did not provide additional analytical insight into their new results.  ########### UPDATE #########I thank the authors for clarifying the method description, providing additional experiments and actively engaging in discussion. Since my main concern regarding the writing part was addressed I change my rating and I agree with the acceptance. ==============I have read the authors' rebuttal information. The authors have addressed my concerns with additional ablation studies and experiments to verify the effectiveness of the proposed multi-hop transformer. And also some experiments are provided  to illustrate whether the most attended object is the most important one. Therefore, I am still standing on the previous justification for accepting the submitted manuscript. # UpdateThank you for the rebuttal. The writing was improved and it is easier to read the paper. Nevertheless, the experimentation remains weak. I increase the rating by one point. **Update**I have briefly checked the updated paper and the corresponding Proposition 1. While I currently do not find any issues with the counterexample, there are still many tiny issues in the proofs that prevent me from recommending acceptance.- Prop 1 statement, $Q(a, s)$ appears again. It does not type check and not fixed in Def. 3.- Lemma 2 statement, the last quantity, what is small $z$ in the integral? I also don't see why the $\exists$ symbol is there, isn't $Z$ as a random variable already defined, so $p(Z | X)$ is simply the conditional distribution?- The step around (21) and (22) are very unclear. I can see what you are trying to do here, but I think it should be laid out step by step. ------ Update after discussion with authors ---------I would like to thanks the author for their efforts by adding additional experiments, which surely enhances the significance of the proposed approach. Based on these, I increased my score to 5.I have re-checked the final revised version, I think the current version still *requires proper organizations and justifications*. For example, the added experiments still talked about the accuracy, the in-depth analysis seems lacking. I think a substantial revision of the paper in terms of structure, idea presentation, and analysis is still needed. Based on this, my final score is 5.----------------------------------- ===================================================================================================The response and the updated version clarify and address many of my concerns regarding contributions and empirical conclusions. Overall, I lean towards acceptance. Post discussion period:-------------------------------I read the other reviews and the author's response. Thank you for performing these extra experiments - they make the experimental section more complete. I still feel that the technical contribution of this paper is marginal and stick to my original rating.------------------------------- After rebuttal: I am uneasy about the overstated claims made in section 2. That the architectures are small should really be mentioned more prominently. However reviewers #1 and #2 make a good case that what matters are the improvements presented in Section 5. Thus, I reluctantly recommend acceptance. Update: the authors have greatly improved the figures and tables, and expanded the captions, removing my major concern about clarity. I have improved my rating. I not not share the objection of reviewers 5 and 4 about the small size of the CNN, resulting in an inferior baseline in section 3, as the approach is still impractical and mostly a proof of concept that should encourage further research. What matters is that improvements in section 5 are built upon a SOTA baseline, inspired by section 3. update 1: I thank the authors for the rebuttal. My questions and concerns were appropriately addressed. However, other reviewers raised some concerns regarding the experimental set-up that I have missed. Thus, I will keep my score as is -- 7: good paper, accept. Edited score after author comments. -- After rebuttalThank you for revising the paper. I've read the revised section, and stand by my original evaluation.  **Update**My impression after the extensive discussion is that the remaining differences are possibly too subjective to come to an agreement:1) Whether the fact that the invertible reparameterization principle does not hold for anomaly detection represents a significant theoretical contribution. To me, I still dont quite see why, but I do believe there are other researchers with a stronger theory background more qualified than me to evaluate this, I will therefore downgrade my experience score by 1.2) Whether the fact that the invertible reparameterization principle does not hold has promise to further understand the practical issues like the CIFAR10 vs SVHN phenomenon beyond things that have already been discussed in the literature - I think quite likely not, the different arguments for and against have been discussed in our back and forth I think.I invite the AC to go through the discussion here to see the detailed arguments/rebuttals.My personal impression, in case the authors are interested, is also the manuscript might be further improved by giving more space to these two questions, i.e. the motivation of the principle and instead moving some of the proofs to the supplementary. I think it is quite straightforward to understand that the principle cannot hold and more difficult to understand why it should hold in the first place. ------------------------------------------------Updated after author response------------------------------------------------I have updated my overall rating in view of the author response. ###update###I have read the other reviews and the author feedbacks. I would like to keep my rating. --- Post Rebuttal ---I've read the author response and do not intend to increase my score. Thank you for answering my questions. It would be good to clarify in the main paper that conditioning on the trajectory is implemented in the way described in the author response. ----After reading the other reviews and the authors' comments, I still think the paper is excellent and should be accepted. ---------------------------------------------post-rebuttalI would like to thank the reviewers for their efforts to improve the draft. Most of my concerns were resolved. However, I agree with other reviewers on the fact that the proposed method only present advantages in certain limited networks and scenarios.  To calibrate this weakness, I downgrade my score to 7. Overall, I think the paper explored an interesting and promising direction to improve network robustness using second-order regularization and solid progress was made. I will recommend accepting the paper for publication.  Update after rebuttal:I increased the score to 6 and appreciated the revision of the paper. The readability is improved. However, I also have different opinions with the authors in terms of how empirical evaluation of algorithms should be regarded in active learning research. So I would further encourage the authors to apply their method on high-dimensional large scale data, even it may take a lot of computing resources or require actual sample acquisition.I agree that the goal of active learning is to reduce the burden of labeling data. But it does not conflict with the requirement of dealing with high-dimensional (feature space) data. Also, I see a lot of active learning works focusing on theoretical analysis but cannot be easily put into real-world applications, which actually undermines the significance of the theory to some extent. In the real world, a lot of assumptions would be violated. As the authors also mentioned that, it is "expected" that different feature space and data quality affects the performance. Therefore, I think the theory does not spare us from justifying our methods in practice.Last but not least, actual sample acquisition is not unrealistic if given real-world problems. So I encourage the authors to further demonstrate the nice properties of the proposed algorithms in more realistic settings in the future. ## After updating paperAfter reading the updates, I think the authors have considerably improved their paper and I have increased my score.While the core contribution, using MI over VAR, is clear. The evaluation is not strong enough, I still don't understand why adversarial defense is a reasonable way to evaluate the different metrics. Further it's now clear that the author's use the sklearn implementation of MI estimation, which depends on several entropy estimators which have high variance in practice. While the authors comment that it's "negligible" I am not convinced that this is actually easy or reliable.In summary I think the paper is not ready for publication. ---This reviewer wants to thank the authors for their detailed reply and for updating the paper to make it more self-contained. It reads much better now and is much clearer. The review score has consequently been updated from 4 to 6. Stronger adversarial experiments would be encouraged. Could the authors also include a definition of the VAR criterion (even though it is trivial, just to avoid any ambiguities) and maybe include a paragraph for the future applications of this? For this reviewer, it is not entirely clear yet what the value of this finding is: is it going to help with downstream tasks for ICA? Is this another argument in favour of preferring Mutual Information as a metric over variances in general? # UpdateThanks for the rebuttal. I have read the changes the authors did. The updated manuscript is more readable and more self-contained now. I am still of the opinion that the presented method should be put in a broader context (i.e., considering it in broader settings and/or for other methods) or be better analyzed theoretically. This way it will be much more useful for the community. For this reason, I keep the score the same. Update: The authors response has addressed my major concerns. Particularly, the revised version is more clear about how this paper relates to prior work, more comparisons/metrics are added, and issues I had related to clarity were addressed. The revised version has significantly expanded the scope of experiments, which better demonstrate the advantage of hierarchical features for normalizing flows. I have update my rating to marginal accept UPD: I am satisfied with the authors' response, and therefore I increase the score.  Post-rebuttal: I am satisfied with the authors' response and decided to keep my score. ***Post Rebuttal***I appreciate the authors's improved analysis. After reading the new version it is unclear to me whether the results are interesting enough to the ICLR community, and thus I would like to keep my original score.  -----------------------I acknowledge that I read and appreciated the author's response (both parts). The authors' reply mainly answers my questions, especially regarding the difference between applying the proposed techniques to the real and binary setups.I agree with all authors comments but would tend to confirm my overall score for two reasons:the architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contributionthe proposed mixing of real and binary weights may preserve the advantages of fully binary networks but, again, makes less clear the net contribution of the paper from a more theoretical perspectiveHowever, as I recognize that the paper contains significant experimental results, I would be happy to support acceptance if all other reviewers agree on that. **Post-Rebuttal**: No change in my score, please read my comments in the thread below. ####UPDATE: I have reviewed the author response and the revisions. All of my main concerns were addressed. I believe that these improved the paper further, and while they do not change my rating, the paper is still a clear "accept" in my view. ----- Post Discussion ----I have updated my rating for the paper after the authors have provided additional discussion and experiments to address my concerns. Edit: Updated score to reflect the changes from the revision. ------------------------------------------------------------------------------------------------------------------------------------------------Post rebuttal: While I appreciate the authors' comments, they do not fundamentally address my concerns that the paper is too unclear in terms of the meaning of its technical results to merit acceptance. As a concrete example, in their clarification, the authors indicate that they obtain "probabilistic safety guarantees" by checking the Lyapunov condition (5) using sampling. However, at best, sampling can ensure that the function is "approximately" Lypaunov (e.g., using PAC guarantees) -- i.e., satisfies (5) on all but 1-\epsilon of the state space.Unfortunately, an "approximately" Lyapunov function (i.e., satisfies the Lyapunov condition (5) on 1-\epsilon of the state space) provides *zero* safety guarantees (not even probabilistic safety at any confidence level). Intuitively, at each step, the system has a 1-\epsilon chance of exiting a given level set of the Lyapunov function. These errors compound as time progresses; after time horizon T, only 1 - T * \epsilon of the state space is guaranteed to remain in the level set, so eventually the safety guarantee is entirely void.One way to remedy this is if the Lyapunov function is Lipschitz continuous. However, then, the number of samples required would still be exponential in the dimension of the state space. At this point, existing formal methods tools for verifying Lyapunov functions would perform just as well if not better, e.g., see:Soonho Kong, Sicun Gao, Wei Chen, and Edmund Clarke. dReach: ´-Reachability Analysis for Hybrid Systems. 2015.This approach was recently applied to synthesizing NN Lyapunov functions (Chang et al. 2019). My point isn't that the authors' approach is invalid, but that given the current writing it is impossible for me to understand the theoretical properties of their approach.Overall, I think the paper may have some interesting ideas, but I cannot support publishing it in its current state Edit: after reading the author response, my score remains unchanged.  ### Update after author response ###Thanks again for the clarifications - After reading the author responses, the other reviewers' comments and the new version of the manuscript, I increase my score for the paper, as the authors now better state the relationship to Circuits and GRAB, and provide a significantly improved evaluation. The enhanced experimental section is now adequate for the paper's claims and offers additional insights into the usability of the generated rules.  ### UPDATE AFTER THE REBUTTALMany thanks to the authors for revising the paper, the new material is comprehensive and does a lot to address my questions about the feasibility of scaling up the model. I would know the authors are short on space, but I would request that the current Figure 2 be tweaked in some manner to makes the legend in Figure 2c bigger. The current legend is nearly illegible, and the difference between the model trained on size 10 vs 16 is important. Independently of this, I have raised my score in light of these new additions. I would like to thank the authors for the rebuttal. The added experiments have made this work better.While the collection of training data still seems hard (require exponential computation), I like the idea and the new experiments on generalizing from small training size to large ones. I still think the application domain of the proposed work is limited. The application to quantum computing is definitely interesting, but adiabatic scheduling is a more restricted domain and the contribution in this work is not applicable to gate-based quantum computers explored by Google, IBM, Rigetti, etc.Because the revised manuscript is stronger now, I have modified my rating accordingly. ############################# Update after rebuttal ###############################The authors response addresses my concerns and I would like to update my score to 7. The paper presents insightful findings about FBF, and proposes a simple and effective method for stabilizing single-step adversarial training. This is useful not only for FBF but also for other single-step defenses. Although the gain in robustness is marginal, stabilizing single-step training is useful. The authors also propose a computationally efficient method of achieving robustness similar to PGD training.  Updates:I have read the author's response and the comments of other reviewers. Although as an "approach as the proof of concept", limited experiments on large-scale GAN models are still necessary. Just provide evidence to prove it is feasible. This is a little disappointment for me. For my other questions, I agree with the author's response. Since I have given a positive initial rating (6), I will keep this rating. Thanks to the author's reply and AC's efforts. ### Edit based on the authors' responseI believe the authors have addressed part of the major concerns that I and the other reviewers had. Comparison with FST approach and triplet loss is clarified and supported by the extensive experiments now, however, most important things are in Appendix only. Based on the updated paper's version I change my rating from "5: Marginally below acceptance threshold" to "6: Marginally above acceptance threshold".One of the reviewers mentioned about comparison with language model (LM): here we could use character/phoneme based LM and vocabulary which can help to solve ambiguity too. So this could be considered as a good experiment for future work to show the great potential of ANE if it outperforms LM usage. Still I have several concerns, probably more philosophical from some point of view:- Rely on the force-aligning data for practical applications (in experiments it was ideal segmentation)- Usage of private data in experiments.Authors state "The goal of our paper is to introduce ANE and highlight some interesting things about it.". I don't see any points in the paper and author's comments why this is helpful/applicable/better than some another ideas. Authors mentioned in the comments that they cannot state that ANE is faster than the FST while having the same performance (this could be one good point that we have speed up using small embeddings + simple L2 distance computation as pointed by one the reviewers). About applications for continuous speech authors gave the comment "However, we are not even sure if speech recognition per se is the best application for ANE. We are hoping that the community will find other interesting uses, either with the embeddings themselves or the distances between embeddings.". Thus, I am feeling that the paper is not finished in that respect.  ----------------------Update after author response:Thanks to the authors for addressing some of my concerns by conducting additional experiments that are listed in Appendix A. I have now increased my rating from 5 to 6.  ### Edit based on the authors' responseI believe the author(s) were able to address many of the major concerns that I and the other reviewers had.  One issue is that much of this is placed in an appendix, so it doesn't form a core part of the main thread of the paper;  I also disagree about one small point (see my separate comment to the Part 2 message below), but this is minor.  Based on their more extensive investigation, I am changing my rating from "4: Ok but not good enough - rejection" to "6: Marginally above acceptance threshold". =======Post-rebuttal comments=========Thanks for the authors' response. However, my main concerns on the feature extractor and the contributions are still not well addressed. Moreover,(1) The authors agree that a good feature extractor is usually hard to obtain in practice. The feature extractor will significantly limit the applicability of the proposed approach.(2) I found that the authors use ImageNet pretrained models in the experiments. This setting is weird. In my knowledge, the pretrained models are seldom used in the existing federated learning studies. The model may already be good enough before training and thus cannot well show the effectiveness of the algorithms.  Thank you for the explanation and the update of the writeup.I still find the main message interesting but not critical for any real application. Hence my evaluation remains as weak accept.Thank you.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Update:Thank you for the answers to my questions and additional experiments! ### Update: the author's revision have clarified many of the points of confusion above, and have largely addressed my concern re: the value of formal languages in this setting. I continue to be concerned about the scalability of this class of methods, but since it is a problem common to other work in this field, I do not want to hold this too much against the current submission. Given this I have increased my score to a 6. ### ---#### UPDATEI thank the authors for their response. I am still concerned about the paper's novelty and contribution compared to the existing work as the differences seem minor. Also, the comparison with the approach by Camacho et al. (2017a;b) is not sufficient since different values of hyperparameter $\lambda$ may perform better. Given the authors' revisions and the added results, I have increased my score. However, I am still inclined toward rejecting the paper. update after rebuttal: I think the paper, with the added discussion, improved.  Author response: the authors clarified my questions, so I maintain my recommendation for acceptance. .-----------Thank the authors for lot of these responses. I'm still around neutral for this paper, but I will raise my score to marginally above acceptance I have read the authors' responses to all reviews and ultimately elected to leave my score as it is (weak accept). I think the empirical results are strong, and while I am not as troubled by the motivation and framing of the work as reviewers 3 and 4, I think their more conceptual and methodological critiques have merit, dampening my enthusiasm for the submission. Update:thanks for your reply. i remain not fully convinced of the improvements with the proposed method and i look forward to additional experiments in NLP. i do think this approach is valuable for additional study however and updated my rating to reflect this ----*Update as of December 5th. I would like to thank the authors for clearing up my concerns and would like to raise my score from 6 to 7. I think this paper would be a good poster that can provide complimentary insights through different experiments to the recent paper of Hermann et al. NeurIPS 2020. **edit, after revisions**  As mentioned in discussion below, I think the addition of intervention experiments makes the effects very clear and strengthen the paper.  I revised my score from 7 to 8. After rebuttal:  I appreciate the authors for the response. However, I feel that the conclusions drawn from this paper are not focused, there is no central message that i can get as a clear take-away for understanding CNNs after reading the paper. I do agree with each of the findings the authors make, it is just that the presentations and writings of the paper make me confused about what the authors are trying to convey through this paper. Therefore, i am keeping my score. ########################################################################################The rebuttal addressed my concerns - I increased the score Edit: The authors have not responded to any of the reviews, i lower my rating to 4 Edit2: Oh there was a misunderstanding, i probably was not logged in and didn't see any comments and reviews. I raised the rating and will read the answers and will rate again.Edit3: After reading the rebuttals, i raise my rating to 7  I have read the rebuttals and other comments and maintain my rating of the paper.  ## Post answerMy questions are appropriately answered and I appreciate the addition of section 3.4. I think my current score accurately reflects my evaluation of the paper, with the remaining concern being the magnitude of the contribution. =======I read through all the reviews and rebuttals and I could better understand and evaluate the paper. I updated my score to 7. Given that this replay scheme works fairly well (intuitively, empirically), easy to understand and implement, fairly sufficient amount of empirical experimentation, I would like to see the paper accepted (and adopted and improved by others).One more comment about staleness.I think staleness is a proxy measure for the (unmeasured) score of the 'current' policy on that level. So I would like to see (in future or revised version) some experiments that measure how well staleness measure correlate with such score. Further, the way staleness is designed properly reflects how the score degrades as the level isn't played.Some idea.It would be nice to make a connection to multi-task learning where tasks share some similarities. Currently, level is somewhat 'linearly' defined. If an agent plays level x, then staleness for level x' (something similar to x') doesn't have to be updated a lot compared to another task which might be dissimilar to level x. Hence, some similarity measure can be further employed (or learn a metric). **POST-DISCUSSION UPDATE**I want to thank the authors for correcting my misunderstandings, answering my questions, and providing additional material. As a consequence of this, I have raised my score to "Accept". To answer your question about what would be needed for a higher score: For a strong accept recommendation, I would have expected a mix of several additional things such as a clear impact outside of own subfield, code availability at time of submission (to evaluate how easy it is to reproduce the results and re-use the code), or more additional theoretical justification (in the sense of new formal guarantees for at least certain aspects of the proposed method). While not directly working in this subfield, I still think this work is solid and worthy of publication. Post Rebuttal:Clearly I was too cavalier in suggesting alternative bin selection methods, as the authors have clarified. I think the paper is much improved by the comparison to other techniques such as ECE_debiased. The rebuttal also made me re-read with the view of seeing the demonstration of how bias varies with sample size as a major part of the contribution. In addition, the concrete demonstration of the relevance of the discussion via fig. 2 is nice.On the whole, my opinion has shifted to be much more positive. I have been rating the paper somewhere between 6 and 7, and in such a case I would rather err on the side of acceptance. Two asides:1. I only realised this now, but I had misentered my confidence - it should have been 3 and not 5! I have corrected this now.2. Of course, actual constants matter in practice, but one way to select number of bins by considering errors in $\overline{y}_k$ is that the squared error in each of these scales as $B/n,$ which, when added over $B$ bins induces error at scale $B^2/n$. With the same logic as the maximum number of bins in ECE_sweep, this suggests one way to set $B$ to be as $B = \lfloor \sqrt{n} \rfloor,$ which controls this error term. Of course, this is only a heuristic, and the analysis is incomplete because it's not accounting for correlations between these errors and $(f(x_i) - \overline{y}_k).$In a larger sense, I still think that just looking at $15$ bins because that's all that has been looked at in the literature is not quite the right thing - basically I think that when studying an autotuning method, such as ECE_sweep, it is important to illustrate how it performs with respect to simple strategies based on some basic heuristics. This was the reason I wanted to score the paper at $6$, but I didn't bring this up when the authors asked for references and so felt it would be unfair to penalise on this basis.----- ### Original Rating**Rating** - 5: Marginally below acceptance threshold**Confidence** - 3: The reviewer is fairly confident that the evaluation is correct### Post-Rebuttal UpdateI applaud the authors for providing detailed responses to my (and other reviewers') questions and for updating the manuscript appropriately. Several follow-up thoughts to my original questions:1. **Binary classification.** I think I understand what you mean now. Basically you are still performing multi-class classification, but then you treat the calibration problem as a binary: is the top-1 model output correct or not? I think this still can be made clearer in the manuscript.2. **Simulation procedure**: Thank you for the clarifications.3. **$\text{ECE}_\text{SWEEP}$ equation**: I am still concerned that the equation given for $\text{ECE}_\text{SWEEP}$ differs from Algorithm 1. (Thanks for moving the algorithm into the main manuscript!) The issue is that I don't think Algorithm 1 is taking the maximum over the quantity in the parentheses of the $\text{ECE}_\text{SWEEP}$ equation. Algorithm 1 is not actually computing $\max_b g(b)$ for some function $g(b)$. Instead, it is first finding the maximum $b$ that satisfies some criterion, then calculating $g(b)$ at that selected $b$.  If the equation and the algorithm are the same, it would imply that increasing the number of bins will increase the estimated calibration error. However, this does not seem to be true. Consider a binary dataset that is perfectly balanced: $\bar{y_1} = \frac{1}{n} \sum_{i=1}^n y_i = 0.5$, and consider a constant model, i.e. $\forall x: f(x) = 0.6$. Then for the case $b=1$, $ECE = 0.1$. But for the case $b=2$, $ECE = 0$ (assuming equal-width binning).4. **Monotonicity constraint**: Thanks for pointing out the trivial constraint satisfaction.5. **"optimal bin count grows with the sample size"**: I understand the empirical and intuitive reasoning for why this should be true. However, I still wish that this notion could be made more formal.6. **Theoretical notions for why/when $\text{ECE}_\text{SWEEP}$ is less biased than other estimators of calibration error**: I am now more convinced that this is difficult to show, and I understand that a lot of the literature is based on empirical evidence. However, given that the results are empirical, I still would like to see experiments are non-image datasets.**Updated Rating** - 6: Marginally above acceptance threshold UPDATE: I thank the authors for their detailed feedback. The authors have addressed a number of concerns and I've increased my score to accept. I continue to think that the empirical section could be easily improved with additional domains, sensitivity to noise, etc. and a careful running example (Fig 1 unfortunately isn't). However, the current draft seems sufficient for publication due to the interesting algorithm (which is novel, as far as I can tell). Update after rebuttal:Thanks for the response! The authors resolve my concerns except for the baselines. I agree that MCTS is the most relevant baseline, but the authors should also include stronger baselines as well. Obviously, MCTS is far from the best performing algorithm on grid world navigation tasks. I encourage the authors to either adding stronger baselines or switch to another task where MCTS is the dominant algorithm. I still think the current evaluations are not adequate to publish in a top-tier conference. Therefore I will keep my score unchanged. **EDIT **The authors have addressed my concerns, and I have increased my score. Thanks for the efforts of the authors. After reading the response and other reviews,  I raise my score from 5 to 6. However, the proposed algorithm lacks analysis. So I cannot improve my score further. ___________________________________________________ ___Thanks for the response. After reading the authors' response and other reviews, I would like to keep my recommendation. ---------------------------After reading the authors feedback and comments from other reviewers, I raised my score from 5 to 6.  I agree the feature averaging is another testing strategy. Thanks for the efforts of the authors. Some of my concerns have been addressed. However, I still think that the experiments are not convincing enough for ICLR. So I keep my score._____________________________________________ ------------Update after rebuttal:"It appears that the comment made by the reviewer may stem from an assumption that two models which are compared for PD can be different in the operations they perform to generate the predictions."This is incorrect, my review does not mention such assumption and my statements hold without it. As stated in my review, I consider models with different weight magnitudes, making no assumptions on the underlying cause."PD, as we defined in Section 2, is aimed explicitly at measuring differences between predictions of a set of models that are supposed to be identical in all their components"Indeed, and my point is that comparing the PD of two sets of models that are not identical is also problematic **even if all models within each set are identical**, except for the PD in its Hamming form. More details below."Changing calibration between such models violates this assumption."Please check the celebrated work of Guo et al., "On Calibration of Modern Neural Networks": calibration does not necessarily consist of an explicit, additional component that modifies the model, and the same model trained in different ways can present distinct calibrations. More specifically, two sets of models can have not only the same accuracy, but the exact same predictions (i.e. there is a 1-1 mapping from each model in one set to a model in the other set that has the exact same predictions for all data points) but vastly different internal calibrations, which will result in vastly different PDs (to be overly specific, the scalar PD of a set will be different from the scalar PD of the other set) even though the two sets agree "point-wise" in terms of predictions."If one changes something about one of the models (including how calibration is done), one would expect them to predict differently, and have different accuracies."This is incorrect. First, I'm not assuming models are explicitly calibrated, only that they have distinct internal calibrations (confidences in terms of predicted probabilities, which depend mostly on the parameters' magnitudes). Second, "scaling the output layer weights by positive scalars" (quoting from my review) will not change a model's accuracy: while it changes the class-wise predicted probabilities, the rank of the logits is preserved. If the authors remain skeptical of this fact, let $\phi(x)$ denote the activations of the previous to last layer of a model, and let $\langle w_i, \phi(x) \rangle > \langle w_j, \phi(x) \rangle$, where $w_i$ and $w_j$ are the weight vectors of output units respective to classes $i$ and $j$ (i.e. $p(y_i | x) > p(y_j | x)$ for probabilities produced by a softmax over logits). Then for any $\alpha \in \mathbb R_+$, we have trivially that $\langle \alpha w_i, \phi(x) \rangle > \langle \alpha w_j, \phi(x) \rangle$ (hence $p'(y_i | x) > p'(y_j | x)$, for probabilities $p'$ computed from the new logits). Again, note that it is --not-- necessary for an external, explicit calibration factor $\alpha$ to be employed: training the network differently, or even adopting a different activation function -- just consider $\max(0, 10x)$ for clarity, which will scale $\phi(x)$ by a positive factor and yield the same observation as above."Specifically, if one flips the predictions of a binary classifier, the flipped model will have much worse accuracy from the actual model of interest, and measuring PD at this point is irrelevant."The fact that two classifiers with vastly different accuracies can have zero PD is worrying and shows that PD is not a trustworthy metric: claiming that such evaluation is 'irrelevant' and should not be done does not address the issue.Since the authors remained unconvinced that the PD is sensible to positive scalings of a model's parameters, and hence comparing the PDs of two sets of models with different activations (one activation per set) is not sensible, here is a more detailed explanation of this fact.Assume a fairly trivial example for clarity: two 1-d data points, $x_1 = +1, x_2 = -1$, and binary classification models $f_1, f_2$, where $f_1(x) = \sigma(w_1 \cdot \phi(x))$ and $f_2(x) = \sigma(w_2 \cdot \phi(x))$ are the assigned probabilities for the positive label, and $\phi: \mathbb R \to \mathbb R$ captures some notion of activation function and/or scale of weights before the final classification layer. For simplicity, let $\phi(x) = \alpha x$, for some $\alpha \in \mathbb R_+$, and feel free to think of $\alpha$ as a 'magnitude' of an activation function instead of some notion of internal calibration.Then, we have $P_{1,1} = (\sigma(\alpha w_1), \sigma(-\alpha w_1))$, $P_{1,2} = (\sigma(\alpha w_2), \sigma(-\alpha w_2))$, $P_{2,1} = (\sigma(-\alpha w_1), \sigma(\alpha w_1))$, and $P_{2,2} = (\sigma(-\alpha w_2), \sigma(\alpha w_2))$. The PD of the set consisting of the two defined models, after simplifying the 8 relevant terms, ends up being simply $\Delta_1 = |\sigma(\alpha w_1) - \sigma(\alpha w_2)|$. Let's pick some numbers to make this crystal clear: let $\alpha = 1, w_1 = 1.0, w_2 = 0.1$, so we get $\Delta_1 = \sigma(1) - \sigma(0.1) \approx 0.2$ (note that w.l.o.g. we can assume that $y_1 = +1, y_2 = -1$ so that for these weights both models achieve 100% accuracy).Now, take ANOTHER set, consisting of models $g_1, g_2$, defined similarly to $f_1, f_2$, but with $g_1(x) = \sigma(w'_1 \cdot \phi'(x)), g_2(x) = \sigma(w'_2 \cdot \phi'(x))$, where $\phi'$ (not the derivative of $\phi$) captures the the activation function and/or weight magnitude of layers preceding the classification head. Let $\phi'(x) = \beta x$ for simplicity. Consider the case where $\beta = 0.1, w_1 = 1.0, w_2 = 0.1$, i.e. the weights of $g_1, g_2$ are *exactly the same* as the weights of $f_1, f_2$, but $\phi'$ is a 'scaled-down' $\phi$ (e.g. a different activation function): in this case (note that both $g_1$ and $g_2$ achieve 100% accuracy as well), **for this new set of models, consisting of the pair $g_1, g_2$**, we get $\Delta_1 = \sigma(0.1) - \sigma(0.01) \approx 0.02$, a value around 10 times smaller than the PD of the first set of models, **even though the second set predicts the exact same labels for each data point**, and claiming that the set $\{g_1, g_2\}$ is 'more robust' than the set $\{f_1, f_2\}$ in terms of reproducibility is simply factually wrong. If the idea of having $\beta \neq \alpha$ sounds a bit of a stretch since the proposed activations are not simply 'scaled down' ReLUs, consider instead the case $\beta = 1.0, w_1 = 0.1, w_2 = 0.01$ and note that we again get $\Delta_1 \approx 0.02$ for this second set of models: the discrepancy in terms of magnitude of weights can be caused by different optimizers, different strength of $\ell_2$ regularization, or, as my original review already mentioned, smaller variance of gradients w.r.t. activation function.To reiterate, in the above example we did **not**, at any point, compute the PD of a set of models that had different components: both $\{f_1, f_2\}$ (the first set) had the same 'activation function' $\phi$, while $\{g_1, g_2\}$ had $\phi'$.Going a step further, which shows how problematic the PD is as a metric, consider an arbitrary set of binary classifiers $S_1 = \{f_1, f_2, \dots, f_M\}$, where $f_i(x) = \sigma( \langle w_i, \phi(x) \rangle)$ is the probability assigned by the $i$'th model of $x$ belonging to the positive class. Now, take *another* set of binary classifiers $S_2 = \{g_1, g_2, \dots, g_M\}$, with $g_i(x) = \sigma(\langle w_i, \phi'(x) \rangle)$, where $w_i$ is the **same** weight vector that model $f_i$ has (i.e. except for $\phi'$, the set $S_2$ is 'point-wise' identical to the set $S_1$). Finally, let $\phi'(x) = \beta \phi(x)$, where $\beta \in \mathbb R_+$, and feel free to check that for any $\beta$, every model $g_i$ from $S_2$ will agree with the model $f_i$ from $S_1$ in terms of predicted class (i.e. although the class probabilities will change, the rank is be preserved for any $\beta$). This means that $S_2$ produces the **exact same** predictions as $S_1$ for **any possible data point**. Taking $\beta \to 0$ yields in $g_i(x) \to 0.5$ for any $i \in [M]$ and possible $x$, hence **the PD of $S_2$ will go to zero, even though the PD of $S_1$ can be arbitrarily large and the two model sets $S_1, S_2$ agree point-wise in terms of predicted classes**. In other words, taking an arbitrary set of models with ReLU activations, copying its weights and replacing the ReLU by $\phi(x) = \max(0, \frac{x}{10^{10}})$, will yield a second model set with PD close to zero. Hopefully the authors agree with me that this trivial replacement of activation functions does not 'solve' any reproducibility problem in machine learning.With the above in mind, I urge the authors to re-evaluate PD as a metric. As mentioned in my review, the Hamming form does not suffer from this issue, but the reported numbers in this case seem to indicate that there is little to no reproducibility challenge for the adopted tasks. ***************************************************************************POST REBUTTAL UPDATEThe authors provided a detailed rebuttal which addressed many of my concerns, and clarified some points. I will update my rating from 5 (marginally below acceptance threshold) to 6 (marginally above acceptance threshold). After rebuttal: Thank you for clarifying details. I maintain my rating. Showing that the inductive bias from Z-order curve and shuffle-exchange allows generalization to larger sizes is very interesting. Overall I vote for accepting. =====Update after rebuttal=====I have read the authors' rebuttal. I still believe the novelty is limited, and hence I keep my score unchanged.  *** Post-Rebuttal ***Thank the authors for responding to my concerns in the rebuttal. For the contribution part,  the major technical contribution is the continuous noise schedule. However, it is very obscure in the original paper. As also suggested by R1, the authors should carefully revise the paper to make it more clear. In addition, I found the paper largely borrowed content from Ho et al. (2020). From the framework figure and Algorithms to texts and equations, only limited modifications are made. After reading the paper, my first impression is that the paper just uses a new model on image synthesis to address speech synthesis. The work is not well motivated and largely copying another paper in the method section is not a professional way.  Hope the authors can modify the paper by adding the new clarifications and explanations in the rebuttal to improve the paper. ***Post-discussion period comments***The authors have satisfactorily addressed all of my comments as well as, in my opinion, comments of other reviewers. Based on the latest revised version of the paper, I am increasing my score to 8 (from 7). I believe this paper is worthy of publication in proceedings of ICLR 2021 and I recommend it as such. ========After reading authors comments.In principle, I tried to understand the main steps of the proof, they looks OK. Although, I can not verify the details of the proof.I still think that ICLR venue is not OK for such long submissions. In principle, I am OK to increase the grade for one point. **Post Rebuttal**My main concerns for the paper are not withdrawn, yet not shared with the other reviewers. I think that is fine, it shows that perspectives differ, and that it is in part why multiple reviewers should read a paper. I also do see that the paper has been improved based on the provided feedback. Given that the paper does not contain major flaws, I upgrade my vote to 6.  POST-REBUTTAL: I have updated the score from 5 to 7 following the clarifications from the authors. *Post Rebuttal*Thank you for your responses. I am bumping up the score. ==================UPDATEThanks to the authors for their detailed replies and paper updates.I am still quite skeptical about the necessity of the use of INT for the 2% improvement of the Metamath GPT experiments. As I mentioned, just using the millions/billions of internal ITP/ATP lemmas has raised the performance much more in previous experiments. And given the "feed our GPT anything mathy" approach, I would expect that training on any sufficiently big ATP corpus might help the GPT.In general, I am also still quite skeptical that generating more and more synthetic corpora without good relation to real-world math (and motivation by it) has much meaning and will lead to much progress in the ML-for-TP area. The fact that some teams have burnt a lot of CPU/GPU power on dubious AI/TP setups does not mean that this is the way how things should be done (and how good AITP research is actually done in many cases). In fact, ATP is quite a good example of a domain where brute force helps only to a very limited extent. Rather than being impressed by the wasted resources, I would advise the authors to focus on resource-controlled setups and competitions such as CASC and CASC LTB.All this said, I do appreciate the relatively large amount of work the authors have invested in this research. So I increase the score and vote for the paper being published mostly for that reason, while encouraging the authors to focus their energy on more realistic setups.     Post discussion review----------# SummaryThe authors present evidence that the approximate rank of the features is correlated with the learned policy's performance and that this rank shrinks when using bootstrapping. They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks. Finally, they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases.# Reason for scoreThe authors isolate an interesting phenomenon and present some compelling empirical evidence. This is interesting work and I have have no doubt that it is of sufficient quality for publications.# Pros* The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent, a critical aspect of many RL methods. Using neural nets to learn (Q-)value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is. As a result, this subject of this work is quite important and likely of great interest to the field.* The experiments are well designed and relevant to the main thesis. The empirical results are well presented and easy to understand.# Cons* After a very productive and enlightening discussion with the authors, the only noteworthy issue is that this paper contains too many contributions for the format making some of them hard to appreciate. A more focused in depth dive into a subset of the theoretical contributions might have been preferable and possibly provide more insight.# ConclusionI strongly support the acceptance of this submission. After discussion with the authors and resulting updates to the paper, I don't see any reason for rejecting this paper. All of the major concerns from my initial review have been addressed. ### Post-rebuttal CommentsI thank the authors for the response and the efforts to update the drafts. I  think this submission made some original contributions.  =======================================================================Post AR:Thank you for preparing detailed responses, which helped me to clarify several questions. However, one of my major questions is still unclear.Although 4 out of the 11 KILT tasks are already included in the main paper, most of them are LAMA knowledge probing tasks or zero-shot QA tasks. It is still unclear how much and how robustly KALM can transfer to other downstream tasks with fine-tuning (e.g. Wizard-of-Wikipedia, FEVER, QA with fine-tuning). For instance, CorefBERT paper (which uses a similar idea but as you mentioned it use bidirecitonal attention) shows its transferability on QUOREF, six extractive QA benchmarks, DocRED, FEVER, five coreference resolution benchmarks and GLUE, which can convince me to choose CorefBERT over BERT.Experiments on the paper are promising but not diverse enough to make me choose KALM over GPT2. Thus, I would like to stick to my original score. -------Update: Thanks to the authors for their response, which helped clarify several of my minor questions and I believe those can be revised with a writing pass. However, I still think this paper has two significant deficiencies:1. The parameter size comparison still seems flawed to me. The authors say that one can discount the entity embeddings, but can we really? Arent they part of the models representation even if the inference does not use all of them in a single forward pass? Several neural net architectures exist (including large-scale LMs) that do not use all inference paths but still count them in the total params. The BERT vs GPT-2 example provided in the rebuttal is only a difference of 26k tokens (still significant!) but here we're talking about a few hundred million parameters!     At the very least, I think the true sizes of the models must be acknowledged and one can add the point on entity embeddings vs 'brain' parameters as a caveat, but it seems scientifically inaccurate to me to claim otherwise. To be clear, I don't think the size issue detracts much from the main contribution of adding entity embeddings here (i.e. this work may still be of interest even if the size of model is larger), but the current version of the paper has several claims about size savings that seem incorrect.2. The analysis of the proposed method (where it helps, where it fails, which hyperparams matter) is still lacking. The authors did mention one ablation in the supplementary that I missed, but I dont think that is sufficient for a reader to understand how to build on this method in this future without re-running all the experiments, doing an extensive hyperparam search, etc. After author response:Thanks for the response! After reading other reviews, I feel the novelty of this work is somewhat limited and it would be useful to highlight the differences with respect to previous work. Also a discussion on when your method is preferred over existing proposals. I am also decreasing my confidence score after reading other reviews. --------------------Post-rebuttal thoughts:See the comment block below.I would like to thank the authors for the detailed response and patient discussion. After the various discussions with the authors, I found this paper still has certain flaws unresolved (e.g., I still share the same opinion with R4 on that the belief-state-related arguments are somewhat not convincing enough and in shortage of stronger empirical support; and the lack of any large-scale RL tasks, even though the authors say it's a "future work", makes the value of this architecture more incremental in a lot of sense). I also agree with R1 that both the architectural modifications and the empirical results feel a bit incremental. I strongly encourage the authors to apply this on a larger-scale RL application (you don't even need to try something too large, but something of even a reasonable size is lacking now). It would be a much better and more natural choice than the synthetic tasks here to evaluate some of the core issues of the Transformer that the authors identify.However, I do appreciate the authors' rebuttal efforts where some of my questions are answered in a detailed manner. For instance, although the training is still slower, I feel its efficiency in a reasonable range now, and there's generally a slight speedup in the generation scheme because the K and V are shared.I have (cautiously) updated my score to 6, though still noting the various concerns I have above. Update: Thanks for the authors' response. After reading the response and the other reviewers' comments, I think the paper needs to be further improved, and thus I will keep my score. **Additional comment after rebuttal**Happy to hear that the authors plan to upgrade their draft. Since the submitted paper is not updated, the reviewer keeps the first rating but also looks forward to read a revised version in another conference or journal. --------Updates after rebuttal-----------Since the author did not propose an updated paper and new experiments. I keep my original score.--------------------------------------------------- Update after author response: I appreciate the clarifications, but given the lack of comparisons or discussion to related prior methods (at least Liang et al 2020 or some alternative equivalent), I cannot recommend acceptance at this point. The authors did not submit a paper revision as well. I think the idea seems promising, so don't take this as a critique of the research direction. ### My finial recommendation After the discussion and revision, the authors have presented more convincingly and more clearly the empirical significance and applicability of their method. I highly recommend the authors to highlight the lastest discussion in the final paper, especially the ill-conditioned argument, as it is highly relevant to the practitioners. I think this paper can be interesting for a moderate number of readers, especially the use of the open-sourced Roboschool could also increase its reproduciability. I agree to increas my score to 6.  Update: I thank the authors for their response and increase my score by 1.  ----**Update after author response**I appreciate the authors efforts in the revision. This has addressed my concerns about the related work and (partly) exaggerated novelty. As a result I have increased the score from a 4 to 5. In my opinion, the distinction between MPC and policy optimization is minor -- policy optimization when given enough iterations can match an optimal planner, and similarly MPC when equipped with a good terminal value function can match a policy learner. The practical differences between MPC and policy learning have also been extensively explored in prior work (e.g. GPS, POLO, AOP etc). Thus, while MBOP provides an interesting case study in the use of MPC for offline RL, there is limited novel insight. Nevertheless, I appreciate the authors for the thoroughness of the case studies, and for updating the paper based on reviewer feedback.--- ----------- UPDATE I ----------I thank the authors for their response to my concerns/comments. It seems like I have to defend my position for suggesting a rejection of the paper. While the response of the authors has clarified some aspects, some comments have not been adequately addressed.Still, the experiments and related works are my main concern with this paper. R1: Based on this response, It seems the paper is trying to tackle the negative transferability problem. While this is an important problem, the authors need to discuss that in the related works, for example [1]. However, in a multi-view learning problem, the main issue is that each view includes some features which can help the classification goal. In other words, part of the features in all views are noisy. At least, you need to add this experiment in my point of view.R10: [2,3,4] are a few examples that used DP for classification purposes. When one of the main arguments of the paper is adding uncertainty estimation using DP, discussing other related papers are important. In some cases, you can also compare with them.  Besides that, the authors can compare with HDP similar to what they did for the CCA-based methods.It would be great if the authors could report the classification results based on only using one view for Figure 4 as a baseline. The authors might also want to check the paper [5]. [1] Bayesian multi-domain learning for cancer subtype discovery, NeurIPS 2018.[2] Multi-Task Learning for Classification with Dirichlet Process Priors, Journal of Machine Learning Research 2007.[3] Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification, NeurIPS 2018.[4] Factorial Multi-Task Learning : A Bayesian Nonparametric Approach, ICML 2013.[5] Hierarchical Optimal Transport for Robust Multi-View Learning, NeurIPS 2020.----------- UPDATE II ----------I have read this paper again and went through the author's responses. While I appreciate the authors for their responses and believe there is some novelty in including uncertainty quantification, I'm still not confident with their experimental designs and literature review.Based on my understanding, this method is not a multi-view learning in a classical way. It is better to say that the proposed method is somehow an ensemble approach. Going through the model, the paper deals with each view separately and then tries to combine the classification results of each view by weighting them. There is some novelty here, where the weighting is adapting based on the uncertainty. However, in a classical multi-view, the problem is that you have different views, where features of each view are randomly noisy. So, the goal is how one can combine information from different views to improve the classification results. Still, I cannot see that. In the experiments, the authors make N views to be noisy, and the other N(+1) views are clean. Then they are trying to classify each view (somehow) separately and because they are reducing the weight of noisy views, their performance (slightly) will be improved. For example, please check table 1. For instance, the Handwritten dataset, the performance of DE, the only ensemble model as the baselines, performed almost similar 99.79 vs 99.97 ( 98.30 vs 98.51). This improvement can be due to tuning the other methods, and the authors did discuss this in neither the main text nor the supplement.In Figure 4, when the noise is small, the different models' performances are almost the same. Also, \sigma = 10^9 is not meaningful and with some sort of variance comparisons, one can find that that view is corrupted.The authors did not address if the method can work in real/harder situations when all features and views are noisy. The authors responded they do not have any restriction on that; however, they did not show either empirically or theoretically that the proposed method can handle this situation more suitable. I agree that the authors have tried on the real dataset, however, to see the performance comparison, you somehow need to randomly corrupt different features on different views.Since the method does seem to be a multi-view ensemble learning, I would have rather expected to try a more common approach for achieving the same goal first, for example to use one of the available Bayesian single-view methods for each view and they combine the results and see how the results look like.The authors claim that the negative transfer effect is not related here, which I can't entirely agree with. This term has been used in multi-task learning as well. The reason that no one uses this term in multi-view learning is that this is not a real scenario in multi-view learning. One view is degrading the classification performance because some views are noisy. If we do not use those very noisy views, the performance should be improved.I also want to point out that their authors show that removing each view can degrade their model's performance. And this is desire, especially when the number of views is small, as having three views means three models in their ensemble architecture. It is not clear that this improvement is due to better information sharing or more complex methods.Regarding the CCA-based method, one can use (Optimal) Bayesian Classification on learned space to get the uncertainty as well. Although, I agree that the proposed method is somehow an end-to-end learning method. I would suggest the authors somehow re-organize/write the paper as a multi-view ensemble learning method and compare it with those multi-view ensemble methods as the main focus of the paper. You also need to discuss [5] as it is very related to your work. Although, it is a Bayesian but not an ensemble method.  ---I read the rebuttal and the updated version of the paper. I think the authors did a good job resolving my concerns on novelty. I updated my score. ##########Update##########The authors have addressed many of my concerns. Moving Table 1 into the main text and adding the new columns is an important change, and it seems that the authors have improved various aspects of their presentation. I'm raising my score to a 7. Post Rebuttal--------------After reading the comments by the other reviewers, I have decided to keep my score at a 5. The authors reply, and the updated manuscript, helped my understanding of the paper. I was considering raising my score, however, the reviewers were nearly unanimous in their confusion regarding the framework or application of CNML. For future iterations of the paper, I suggest that the authors describe CNML, event-based control and their connection more explicitly. If the main contribution is using CNML as the classifier in event-based control, then it would also help to conduct experiments on meta-learning CNML in a supervised learning setting to further elucidate its effectiveness in the reinforcement learning application. I think your paper is very interesting, and I hope that the authors are able to use this feedback to improve their paper. Post-rebuttalAfter reading the rebuttal and other reviewers' comments, my score remained the same. The rebuttal helped to clarify some issues, but it is still not clear to me why the algorithm should work. I agree with other reviewers that a more careful revision of the paper, and a further analysis on the algorithm will be beneficial. --------- Post rebuttal ---------------I've read the rebuttal and I appreciate the additional efforts by the authors.Specifically, the authors have addressed my concerns comparing LWS (the proposed method) and LARS. Additionally, the authors have addressed my concerns regarding LWS by conducting more experiments. With another detailed read, I figured LWS updates $\alpha$ in a lookahead fashion. Specifically, $\frac{\partial \mathcal{J}}{\partial \mu}$ in Eq. 6 essentially requires one to compute the loss after the gradient update is being made, which gives it the potential to outperform the analytical gradient.Moreover, the authors have run additional experiments to demonstrate the usefulness of GRU, which makes the proposed method more convincing.While the authors argued that it is not fair to compare to AutoSlim, AOWS, and DMCP, I disagree. They are all relevant and strong channel pruning methods and the authors should have cited them and discuss the main differences (can be used to prune a pre-trained model or not) in the related work as opposed to omit them entirely.Overall, I find the paper interesting and it provides several novel aspects: GRU and LWS. Both are empirically verified to be useful in the channel pruning setting. However, the related work section can be further improved. As a result, I raised my score to weak accept. ---### Updates after rebuttalI appreciate the authors' efforts in revising the paper into a more rigorous and illustrative way. However, I still feel the current version difficult to follow, and I am not sure whether the conclusions are really correct or not. The conclusions are drawn from assumptions for empirical observations, but the scale of the experiments is not large enough, and the conditions for the assumptions are not specified rigorously. Therefore, I feel quite reluctant in raising my scores.My final suggestions for improvements in the future version:1. Instead of using accuracies on $\mathcal{D}_{shift}$ as a proxy for how much of the models performance can be attributed to its learning predictive robust features, showing the accuracy of the model on the shifted dataset under adversarial attacks would make it more convincing to me. 2. I would prefer the figures to be close to the text that elaborate the figures. For example, Figure 1 is in Page 2, but the details of this complicated figure is not discussed until Sec. 4.2. Post Rebuttal Update:After reading the author's rebuttal as well as discussion with other reviewers, I will maintain my score. I do appreciate the changes the authors made, and believe that they improve the paper (especially the new example with cat and dog features and the associated figure). I would encourage the authors to incorporate the remaining feedback, as it will be helpful as well. -------------------------------------------------------------------------------------------------------------------------------Post rebuttal: I appreciate the improvement in clarity of the paper; however, I think some work remains to improve the clarity of the paper, especially regarding why the two pathways are interesting. I think expanding on potential implications of this mechanism would be very helpful, in addition to the addressing the remaining issues raised by the other reviewers. ###################### Post Rebuttal #############################Most of my concerns on the current manuscript are addressed. I tend to increase my overall evaluation score to 6.############################################################# ------------After Rebuttal------------The authors have addressed my main concerns, and I have updated my score to 6. ++++ updates after discussion period ++++While the initial paper provided empirical evidence  for the claims of the paper, based on the reviews an appendix of about 9 pages was added in the revised version, and the focus now seems on providing theoretical support of the claims in the paper. Also several experimental results were added in the main paper in response to the reviews. I feel like all these (quite major) changes indicate that this paper is not mature enough for publication at this point. So I maintain my current review. Udpate: with the additional experiments and corrections in the paper, I believe the paper is in good shape and contributes to the literature in the field. Hence it should be accepted. ----post-rebuttal updateI appreciate the discussions between the authors. I plan to keep my original score, for the reason that, at least in my point of view, the difference of the two methods is subtle and it is not clear whether the subtle difference results in drastic improvement. Thank you for your feedback. I found that the analysis of the method in the revision is informative. However, the comparison with the baselines is still lacking, and the experiments are only performed in simple environments. For these reasons, I keep my rating unchanged after the rebuttal.============= Author feedback: The behaviour and performance of the algorithm in single-goal environments should be part of the paper. The paper should show not only where the algorithm succeeds but also where it fails. I appreciated the author's efforts to add diversity to the domains evaluated but they do not go far enough to change my score.  -- POST REBUTTAL --The authors addressed well most of my concerns.  I increase my rating. However, the authors need to address all comments of the reviewers and also discuss all missing related works in the updated version. ######## Post rebuttal ##############Thank you for your response. After going through other reviewers' comments and the authors' responses to those, I am comfortable with my original score.  ----------------------------------**Update after author response**:  Thanks to the authors for answering my questions during the rebuttal paper and for incorporating the feedback into the paper! My original concerns were about clarity, high-dimensional experiments, and visualizations. Since the paper has been revised to include nice visualizations and improve the clarity, I am increasing my score 5 -> 6. I think the experiments on HalfCheetah are a great proof-of-concept of the method! I'd encourage the authors to include some comparisons against baselines for that task. -------## Post-rebuttal updateHaving read through all reviews and the author's response, I am updating my assessment in light of the responses and new experiments. I agree with the authors that the derivation has theoretical value and is not a simple re-derivation of VIC. The new experiments and visualizations have been helpful (I am happy with the author's responses to R3), but the overall clarity of the paper is still lacking due to the dense mathematical notation. In light of this, I am increasing my score from 4 -> 6, slightly leaning towards acceptance. EDIT: The qualitative results help illustrate what the variational bias entails in practice, and indeed the worse coverage constitutes a problem worth overcoming. The Ant experiment was a good attempt at showing scalability, but the deterministic version isn't terribly informative since then the correction term does nothing. Could add stochasticity by taking some number of random actions between the states the agent sees. I suspect that as you increase stochasticity in this way the uncorrected method would degenerate. Would be a clear accept if you could show that, but as is the paper's contribution is bordering on acceptance. 5-->6   ===== After rebuttal =====I appreciate the fact that the authors took into account our comments and improved their manuscript (+1), however, I think that there is still space for improvement.My main concern, is the fact that we need the model to not be "fully trained" to do some analysis about "what the model learns". Therefore, regarding the data leaf, I am not sure if the trajectory that connects two points is actually moving on a "data manifold/leaf" or simply in the data space. Anyways, the behaviour of the metric away from the given data is kind of arbitrary since extrapolation analysis in neural networks is quite difficult. So the most crucial assumption is that the (learned) metric gives meaningful directions to move in the data space. In the experiments this seems to be the case "roughly", but the result could have been the same simply by the linear interpolation (which I think is missing). Similarly, if we just move linearly in a random direction, probably noise will appear gradually. Update after response:While I appreciate the authors' attempt to address my concerns, the fact that model is required to not be fully trained is concerning. It was in this context that I suggested label smoothing - that training on smoothed labels might address a sparse G matrix, but it seems like this point was not communicated clear enough by me and not understood by the authors. The authors suggest some applications of the model centric view of the data, but do not present any experiments regarding these applications. I believe the paper might be more convincing if those experiments are added in the next version of the paper.At this time however I will still have to vote for rejection. ##########################################################################Post rebuttal:I have read authors' rebuttal as well as the newly added contents.I appreciate the time and the details that the author goes in when addressing my comments. I am convinced that this method can be used even without the OPR module based on the ablation study. Overall, I think this is a high quality paper that deals with decentralised Multi-agent safety-awared learning. And learning the safe certificate is the shining point of this contribution. I recommend for acceptance and give my final score at 8.   ## Post-rebuttalThe rebuttal was convincing (see my comment below) and addressed my concerns. I therefore increase my score to 8. --- Edit: After rebuttal ---I thank the authors' for their detailed response to my questions and for clarifying so many aspects of the work! I also appreciate the time taken to make the minor corrections that help with the presentation. My main concerns have been sufficiently addressed - specifically the references to the dips in performance have eased my concerns with the validity of the experiments. The ablation with the beta parameter is also quite important I think and at least helps clarify that the issue for kickstarting may indeed be one of reaching a local optima.  Given the substantial effort (including implementing a new baseline) that went into the rebuttal I am willing to increase my score and recommend the paper be accepted at this venue. Post rebuttal update: The authors have addressed my concerns and the revised submission is much clearer, so I'm increasing my score to 7. Post-Rebuttal----------I thank the authors for their response. The additional baseline comparisons do strengthen the paper, and I have increased my score from 4 to 5. I agree with the authors that the mixture of Gaussians policy is substantially weaker than their method, and is a useful baseline experiment to have. However, the added experiments with random sampling are somewhat worrying---the performance improvement of the proposed re-sampling scheme is quiet minor over random resampling. In the future, the authors may want to investigate the random resampling for the systems in figure 14. # post rebuttalI have no further concerns and increase the rate to accept. After reading the response from the authors, I would like to increase my rating to 6: Marginally above acceptance threshold. - The authors have answered all my questions and they revised the manuscript as well, so I will keep my initial rating (weak accept).  **================== UPDATE / EDIT AFTER REBUTTAL PERIOD AND UPDATES TO PAPER ========**Summary of improvements during rebuttal and remaining concerns on the updated manuscript:Main improvements:- Improved the comparison with existing EM datasets by extending descriptions and text in Sec 2.1.- Added an experiment that shows that the same model (Unet) performs significantly better on previous databases than on the proposed (Appendix VIII, IX). This acts as a solid empirical evidence that the current database is indeed more challenging than previous ones, which was previously missing. This supports the value of the database.- The authors also clarified that they will release all 3 sets of annotations, which can serve various types of methodological developments, as databases like this are not common.- Added a short discussion of previous work on metrics for segmentation quality, which previously was entirely missing.- Extended the analysis by incorporating multiple more metrics (~10) that were previously missing, extending significantly over the first version, where only overlap-based metrics were considered (Dice, IoU). The new results support that for the specific problem of cell-segmentaiton, PHD agrees more with human perception than other metrics on this task, including the very related HD and ASSD.- Showed that the Skeletonization process, part of the proposed metric, improves various metrics (among which the very related HD-based metrics), which fulfills the previous gap of empirical evidence to support its incorporation in the proposed metric.- Rephrased most points where the text was ambiguous or incorrect.Summary and Reviewers Score adjustment:Overall, the revision has improved the document significantly. My primary remaining concerns have to do with the actual paper being of interest primarily to the audience interested in the specific task of cell-segmentation, and that the technical value of the PHD metric is relatively limited (as per my initial comments). However, the updated document supports much better the main claims of the paper, and this database could serve as a benchmark for general ML segmentation methods, benefitting the greater ML community. These improvements make me increase my score from a 3 (Clear Reject) to a 6 (Above acceptance threshold).-- new minor problems in the updated version --Some new minor points that I noticed. In case the work is accepted, please try to address them for camera ready:ASSD& which is not widely used in deep learning researches: Not a true statement. HD/ASSD/etc are very popular metrics in segmentation tasks (including with DL) and there have been efforts to even turn them into losses. Please rephrase/remove this statement.the consistency of the F1 score, IoU and Dice with the human choices was calculated (Sec. 3.2): Wasnt this done for the other metrics as well now? Update the text.Sec.3.2, Six popular& segmentation results: Refer readers to appendix for details on training/test config.1?Sec. 4, Then four evaluation&: four is not correct after the updates.Sec. 4, Discussion: it can be seen& methods: This no longer holds after the new metrics. Taking into account all metrics, if we naively count for how many metrics a method ranks 1st (which is what the paper did in the first place), then it seems the best is LinkNet, followed by U-net++, not CASENet (which only ranks 1 for PHD-1 and PHD-3).Please update the argument.Appendix V: texture => text ======================post-rebuttal:I have read all the comments from other reviewers and replies from the authors. The revised version partially addresses my concerns so I raise the score from 4 to 5. However, my concerns about the motivation of the work still exist, so I am still slightly leaning to reject this paper. ####Authors have engaged in the discussion, clarified questions about the paper and addressed comments in its newest revision. I have consequently revised my score from 5 to 6.  ------------------------------------------------------------------------------------------------------------Post rebuttal:The authors have devised an adaptive attack to craft the adversarial examples against edge maps and shown that the proposed technique is still remain robust. However, the essence of robustness in this work lies in the BINARIZATION of the input (i.e., binarized edge maps) which is shown in the previous work [1] and need not necessarily attribute to the shape information obtained through edge maps. I recently came across state-of-the-art deep edge detector [2] that produces non-binarized edge maps, which could be interesting for authors to validate their approach using such non-binary inputs. Hence, I maintain my initial rating and marginally accept this paper. [1] ON THE SENSITIVITY OF ADVERSARIAL ROBUSTNESS TO INPUT DATA DISTRIBUTIONS, ICLR 2019[2] Richer Convolutional Features for Edge Detection, CVPR 2017 ***Post-discussion period comments***I am pleased with the revision the authors have posted during the discussion phase. I am also satisfied with the authors' response to my comments and to the comments of the other reviewers. I therefore recommend that this paper be accepted into proceedings of ICLR 2021. The authors addressed my concerns and it seems my co-reviewers didn't bring new concerns to light, so I increased my score to 7 The questions above are well addressed in the response, and I would like to increase my score.    **after-rebuttal:** The authors go over the definitions and now make more clear statements in section 4, which makes the argument more concise. The current version is more enlightening to the reader. ------------------------------------------------------------------------------------------------------------------------------After rebuttalFrom the revised version of this manuscript, the authors resolve my major concerns such as clarity/reproducibility of the method, differentiation from the previous works including semi-supervised learning, and scalability. So, I've raised my score to 6. Thank you for the contributions! ### Update after author responses: While the author address some of my comments, I would have still liked to see a more detailed discussion of how the algorithm compares in terms of algorithmic scaling, which I think is relevant because it is a fundamental property of the algorithm, even if it is targeted towards understanding biology. So my score remains the same. UPDATE AFTER AUTHOR RESPONSE:I appreciate the authors' response and for clarifying the contributions. However I still feel that the experiments and datasets are too simple and not realistic. I am increasing my rating to reflect this. Post Rebuttal Comments:I thank the authors for their feedback. I have no modification to make to my original review. ### Modifications since discussion * retracted objection about inappropriateness of MNIST example * raised score from 5 to 6 Update: Thank you to the authors for clarifying the points brought up in reviews. I acknowledge that I have reviewed their responses.     =========    I have updated my score considering the paper has improved its quality after the revision (adding more experiments/baselines, comparison with the literature, etc.).=========   New updates: following the new comments of Reviewer 4, I also briefly check the code in the supplementary material and find it indeed seems to have the consistency problem (i.e., not reconstructing graph edges as mentioned in the paper). Thus, I am also wondering how the authors implement Graph-VAE in the rebuttal phase and whether the improvement of their proposed method over Graph-VAE is really from disentanglement or the differences in the autoencoder. Based on this potentially serious problem, I reinstate my original score and think the paper should be clarified before acceptance. ======After reading the author's comments I still keep the score. After rebuttal/revision, the paper still has a lot of steps, many of them are human designed and are not well-defined. For example, in the author response, they said "their effect distributions look similar, and so are merged into one class type", what is the criterion to merge them together? In the revised paper, what is the procedure "COMPUTEMASK" defined?  In line 38 of the algorithm proposed in Appendix F in the revised paper, what does the sign of "approximate equal" mean?  Overall, this makes it hard to reproduce and it is not clear whether the proposed approaches can be applied to other problems than the specific tasks mentioned in the paper.  -------The reviewer appreciates very much, and has read the response letter, which is mostly about clarity though it's not the main concern of the reviewer.  # After discussion and editsI acknowledge that I have read the other reviews and resulting discussions and I have read the relevant changes in the edited text. I have raised my score from 2->3 to reflect that several concerns were alleviated through the edits, but several new concerns (and old concerns) remain. I will summarize below.After the author edits, the issue with convergence and convergence rates appear to have been resolved. I additionally appreciate the much greater clarity in the analytical section. However, I still find the contribution to be borderline at best in terms of novelty of approach and I find that the evidence of applicability is still considerably lacking. The introduction of a regularizer to accelerate GTD methods is itself not novel. The form of the proposed regularizer is novel, however, I find its form to be unintuitive as it punishes making changes to the weights. There are some prior works that motivate this well (i.e. TRPO and other trust-region optimization techniques), but this paper does not appeal to prior works to motivate their regularizer. Instead, I must rely on the empirical study which does not investigate the learning speed of the proposed algorithm compared to baselines. In many cases, the proposed algorithm does not clearly outperform baselines. EDIT: After reading the other reviews, the author's responses, and thinking more about the concerns raised, I have increased my score. However, I still recommend rejection because of questions around the hyperparameters used in the experiments.--- #########Edits after the rebuttal#########Thank you for the responses. After reading them and the discussion with other reviewers, I still think the current contribution of this paper is marginal and I keep my score as 5. ----- Post Discussion -----Taking the other reviews and the authors' response into account, I still maintain my score. While I agree that it's good to be thorough in something clear and simple, it can still be done to a point of redundancy, and consequently seem less thorough in the overall picture and claims made. I'm still largely unsure on the choice to only apply the supposedly modular extension to GTD2, and not try it with TD which seemed like a clearer winner (apart from Baird's counterexample). As others suggested, there are additional methods which might be good to compare to, and other evaluation metrics might make more sense for the claims being made. Many of my concerns were largely brushed off as future work, that little got addressed- without having to carry out the experiments, high level comments/current thoughts could be provided regarding how readily the approach can extend to the scenarios suggested, or if there are nuances that need to be worked out, etc. ---After reading the other reviews and the authors' comments, I sill think that the paper promises more than it delivers, even if the paper was extensively rewritten as a consequence of many problems in the original version, so I will keep my score. UPDATE AFTER AUTHOR RESPONSE:The authors have addressed all my queries and made the changes that I requested. I am increasing my rating reflecting this. The paper has novelty, good experiments and improved performance. I would like to see the paper accepted.  ----### Post-Response UpdateI don't think the authors have answered my second set of questions. While there are some doubts remaining in the paper, the idea looks fine. Although, I think a new paper with DWT will outperform this approach soon. I do not change my vote at this time. ----- Edit after rebutal ------Thank you for the answers.The authors only answered partly my concerns. I'm still not convinced by their explanation on the NN curves. I don't understand how their performance decreases with the number of iterations, it seems that they weren't well tuned.Anyway I've been convinced by R4 and R1 that a few baseline models are missing. ##### Post-rebuttal I thank the authors for their thorough comments and detailed explanations for each question. I carefully read the whole, and it helps me understand the entire decision and the processes. However, I would like to suggest two things, regardless of its acceptance, but to make their claim more attractive to general readers: (i) re-sort out the indexes (maybe after rebuttal but before submitting your camera-ready version), and (ii) update figures of the sample with a bit more realistic one (with training the model on larger batch size, for instance). I hope this review phase would make your paper more powerful. (disclaimer: I did not check the soundness of the mathematical equations thoroughly but did check all the rest) *** After rebuttalThank the authors for the detailed explanation. After reading other reviewers' comments and the author feedback, I would like to keep my rating unchanged. EDIT:The revision addressed my concerns, I'm raising my evaluation to 7. Post rebuttal:The authors have addressed most of my main concerns and provided detailed experiment results, hence I increase my score to 7. ##########################################################################Comments after Discussion:I appreciate the effort made by the authors to elaborate on the causal formulation behind CIQ. However, the additional discussions in the paper are still confusing and raise soundness concerns. Some of the issues are discussed below.1- Rubin's Causal Model: The authors reference RCM in Subsection 3.1 for the causal formulation yet the rest of the work does not seem to use the potential outcome notation. Instead, Subsection 3.3 uses graphical models and the do-operator which follows the causal framework by Pearl. Then, in Subsection 3.4, the authors go back to reference RCM. It is not clear why this alternation between the two approaches is employed.2- If $z_t$ is defined as a function of $x_t$ and $i_t$, shouldn't the CGM reflect that with an arrow from $i_t$ to $z_t$ instead of it being the other way around in Fig.2(a)? Despite the attempt by the authors to elaborate on the causal formulation, I'm unable to map the structural equations such as Eq. (1) and the function of $z_t$ to the given CGM in Fig.2(a).3- The discussion in Subsection 3.3 leading to Eq. (3) sounds flawed to me. Quoting the authors, "the interference model of Eq. (1) can be viewed as the intervention logic with the interference label it being the treatment information". This statement is elaborating on the formulation of $x_t'$ where $x_t$ is intervened on and replaced by an interfered state when $i_t=1$. Alternatively, $x_t$ is kept intact when $i_t=0$. This intervention on the mechanism of $x_t$ happens whether we obtain $i_t$ and train the DQN with it or not. In this sense, the intervention is not happening under the CIQ framework only, but also when we simply train based on $x_t'$. Accordingly, it is not clear to me how "the learning problem is elevated to Level II of the causal hierarchy" due to the presence of the interference labels. To be clear, I'm not questioning the significance of using the interference labels in the training, but rather the causal story and formulation behind CIQ. Post discussion update=========After discussion with the authors, I have calibrated my score upward to 4, since the authors seemed willing to engage in discussion and correct/improve the paper, which I appreciate, but I still recommend not accepting the paper.The authors generally acknowledged (though have not yet fixed) the issue of wrong attribution of the APSP algorithm. This isn't just a matter of citing B instead of A; the paper (still) contains a lengthy discussion of why A is not suitable, so instead they must resort to B, even though in reality they just use A (and B remains unused). This is glaring since these papers are famous classics, widely taught in graduate courses, their content is well known, and it is puzzling how a diametrically incorrect representation of them made its way into the paper.The reason I dwell on this is that it signifies a larger issue with the paper. The original version was peppered with formal statements which were at best inaccurate, and even though the authors fixed (or said they would fix) the ones I pointed out, I remain unable to trust the overall technical soundness of the paper. The review time frame doesn't allow a reviewer to carefully verify every statement (nor would I want to), there must be some commitment of due diligence on part of the authors, that up to a small inevitable fraction of inaccuracies, the formal content is rigorously correct. I'm afraid the current version of the paper is quite off this mark.Putting formal soundness aside, my present understanding of the idea of the paper is the following: The authors observe that many basic computations on graphs can be parallelized into a few computations of small width and depth. Usual GNNs can implicitly implement this if their width is large enough, but this poses a computational burden, and there are obvious advantages to explicitly building this parallelism into the architecture. This seems like a sensible and potentially empirically useful observation, but the experimental section still seems too thin to make the case properly. That said, perhaps I have not fully understood the paper, since its frequent inaccuracies and fuzzy statements made it a bit hard for me to follow. In conclusion. I think the paper should undergo a substantial revision:1. Clean up the theory part and ensure its formal soundness,2. Crystalize the point of the paper (in particular, rather than just presenting GNN+, I hope a revised version would include a more thorough comparison with usual GNNs - not just dismiss them with some citations of prior works which allegedly prove limitations - this leaves doubts about the exact model and assumptions, particularly since as discussed above, the prior work is not always cited accurately),3. Possibly expand the experimental section. -------Post Rebuttal: I thank the authors for clarifying on my questions and updating the manuscript.  Post-rebuttal:Thanks for the clarifications in the rebuttal. It addresses some of my concerns. However, I am still concerned about the unfair comparisons of baselines using different settings, and the newly added ssl baselines all outform the proposed method on LRS by a large margin. Therefore, I would like to keep my original rating. Post author response stage: Given the response from the authors and the input from other reviewers I increased the score from 4 to 6. #########################UPDATEAuthors have clarified the doubts raised by my questions. I believe that the task proposed in the paper provides new insights on the weaknesses of deep learning models. Therefore, solving the task is important to advance the automation of proof assistants through machine learning. Based on this, I recommend for acceptance. ================UPDATEThanks to the authors for their replies and paper updates. My overall evaluation remains on the slightly positive side: I believe that conjecturing is an important task and Isabelle provides a nice corpus for that. Even if declarative proof corpora based on Mizar have been used for similar ML/TP tasks before and the work is not quite novel.Further notes:- I would recommend exporting the corpus in the TPTP format. This typically makes the ATP evaluation and building of ML/ATP feedback loops (much) more accessible to ATP researchers, allows including the benchmark in the CASC LTB (large-theory batch) competition, etc. This has been done before [6] for the large corpus of declarative Jaskowski-style Mizar proofs that can be easily used in a similar way as the Isabelle data provided here.- I do not quite agree with the response claim that E_conj is synthetic and focused on ranking and classification. It is derived from a real-world (Mizar) problem set and the ATP-synthesized lemmas are equipped with a metric of their real-world usefulness. So the various tasks such as regression, ranking, classification and (indeed) synthesis (there is nothing hard about synthesis in the E_conj scenario) have a direct impact in terms of suitable splitting of the real-world ATP problems and their easier solution. It is the same cut introduction task as the authors consider here, just with much more data derived from ATP runs and their characteristics rather than from human proofs. This kind of ATP-based data augmentation is one of the most useful ones in the ML-for-TP domain - quite often more useful than working with human proofs [7] because the ultimate evaluation scenario typically involves ATPs. So it is certainly not the kind of artificial/synthetic task that has an unclear real-world value.[6] Josef Urban, Geoff Sutcliffe: ATP-based Cross-Verification of Mizar Proofs: Method, Systems, and First Experiments. Math. Comput. Sci. 2(2): 231-251 (2008)[7] Daniel Kühlwein, Josef Urban: Learning from Multiple Proofs: First Experiments. PAAR@IJCAR 2012: 82-94 ------------I have read the authors' response. I do not think the answers addressed my concerns.On the positive side, I do think the paper is written well and the idea is clear. It is true that a soft-clustering of PDs on the true Wasserstein distance has not been done. The paper also provided proof of the convergence of the algorithm.My main concerns are the following:First and most importantly, the method is not particularly surprising. It basically combines the most classic soft-clustering algorithm and the Frechet mean computation algorithm of Turner et al. Theoretically, the proof is extending the convergence result of the soft-clustering algorithm to the Wasserstein distance of PDs. I would not say the proof is trivial. But it is not that surprising, as we already knew that the Frechet mean of PDs is computable. I would be much more excited if the theoretical result is about the optimality rather than just convergence.Empirically, the paper did not provide comparison with (Latombe et al. 18). Just saying that they did not do it exactly in the PD space is not good enough. A lot of practically powerful methods (persistence image, various kernels, etc) are approximations/relaxations outside the PD space. These approximations/relaxations can bring computational advantage, and sometimes better learning efficiency. Therefore, we need to know how this method is compared with (Latombe et al. 18) in efficiency and clustering performance. (Latombe et al. 18) can naturally have both hard- and soft-clustering versions. A thorough comparison with the different versions can show how important it is to stick with the PD space rather than the relaxation. My guess is that in practice sticking with PD space is not that important, or maybe even worse due to bad local optima of the Frechet mean. But I would be very happy to be proven wrong.Another issue is the limited experiments. The material data does seem to be a good fit. But the authors could use some of the classic topology-friendly data (shape, dynamic data, graph) from existing supervised methods. Any labeled binary/multiclass data can be used to evaluate clustering. An even more ambitious goal is to prove the usefulness of clustering in the supervised task. For example, the authors can show that a bag-of-words approach (using the clustering result) can improve classification performance.Overall, I feel that the methodology is not very exciting to me, and the experiments are insufficient. If the main argument is the algorithm computes on the PD space and the proof of convergence, this paper may better fit a theoretical conference. # Update after rebuttal & discussionsI thank the authors for their thorough rebuttal. While the technical details are acknowledged and addressed for the most part, the experimental setup could still be improved. R1 mentioned that the work by Lacombe et al. might also be applicable as a comparison partner. Investing in a more thorough scenario would strengthen the paper by a lot.# Further update after discussionsThe primary subject of our discussions concerned the experimental setup. While I still see this paper favourably, it would be strengthened by a more in-depth comparison with the work by Lacombe et al. The core of the paper would be more convincing if the utility of the fuzzy clustering could be highlighted better in a set of scenarios that are more comparable with existing TDA literature. ---------------------------------------------------------------**UPDATE after author rebuttal**The authors have made substantial improvements to the paper over the rebuttal period, but there is still work to be done to make this paper communicate as clearly as it should for publication. I share the concerns of Reviewer 1 and feel that this paper would benefit from more thought into how to present the ideas and another round of reviews.**Additional Feedback for future revision**The preamble to Section 2 is now quite helpful, but it would be even more helpful if placed prior to Definition 1! Helping the reader to think about the elements of $H$ as random variables and to think of causal factors as determining which environment you end up in would aid understanding of Definition 1.Depending on who you want your audience to be, I suggest considering adding an explanation of the "do" notation. While the sub-community focused on causality may be aware of the notation, the community interested in intrinsic motivation more broadly would probably be interested in this paper but not know the notation.I think that the following sentence could be quite helpful if written in a different way: " In our definition, $h_j$ are causal factors such friction with some particular coefficient of friction, or gravity with acceleration constant $g$ or other" (p. 14). It would be helpful to have the variable (e.g. amount of friction) separated from its value (which might represent the coefficient of friction). I'd like the concept that causal factors are variables that could take on any of a set of values made more explicit. "the outcome of running" (p. 14) is slightly ambiguous (at first I read it that running was the outcome, rather than the experiment. A phrase like "the outcome of the attempt to run" or the outcome of the running experiment" might be clearer.Consider using the singular they in your human example (p. 14). A decent primer: https://apastyle.apa.org/style-grammar-guidelines/grammar/singular-theyAdditional typos found:Let $o_{0:T} \in \mathcal O^T$ denotes  denote (p. 2).$k$ is both the length of the set $H$ and the comparator with $h_j$ (p. 2) Maybe don't use $k$ and $K$ either, since it still gives me the vague sense that they might be related, but I'm pretty sure they're not. Neither CEM nor MDL is written out in full (p. 4 is first use)."as compared to the vanilla CEM planner (Figure??)." (p. 7)"are causal factors such friction"  such as friction (p. 14) - - -### After rebuttal and discussion periodI want to state once again that it was a pleasure to iterate on this paper throughout the rebuttal period. It was gratifying to see it improve significantly. The reviewers all agree that this is a very valuable research direction and the authors have begun an extremely interesting line of inquiry. However, there is still a good deal left to be completed prior to this paper being fully suitable for publication. There are some critical areas of improvement still needed in improving the overall clarity of the proposed approach.One piece that stood out to me when re-reading the final submission and the other discussions with the reviewers, it's left unstated with any formal language how the causal interventions are chosen. My concern along these lines are a dressing up of monte-carlo approaches of varying the separate causal factors in more sophistication than is actually present in order to make the paper seem more concrete. Not saying that this is how I read the paper, but there is some space to wonder and be concerned by this. Along these lines, the distributions $q(z|S)$ are never concretely formalized nor is the inference process fully detailed surrounding how one may infer the $z_j^{(i)}$. I could maybe guess that the $z_j^{(i)}$ could perhaps be the cluster assignment but this shouldn't be something left to speculation and guessing... A simple statement of how these distributions are parametrized or even approximated would go a long way.As a final note, I would suggest that the authors revise their conclusion and discussion sections to incorporate the limitations inserted into the Appendix. This follows from the discussion I had with the authors about the general scoping and applicability of the proposed causal curiosity mechanism. It is implicit that the authors have robotics applications in mind yet have written in a more general purpose manner. I believe that adjusting their focus and naming the robotics-directed focus explicitly will help immensely.In the end, I unfortunately cannot recommend this paper for publication in its current "final" form for this conference. I do however really look forward to the complete and fully published form after another series of revisions and refinements by the authors. Best wishes. Final review:The authors updated the manuscript and removed tuning experiment on ObjectNet. I am still a bit concerned about the definition of "robustness", but the paper overall does look good for ICLR publication.  UPDATE:I have read the author feedback and the other reviews/discussions. I keep my original rating of 5. I think multiple authors raised the question of novelty relative to Barbu et al and the authors argue that they demonstrate the importance of context (whole image vs bounding box vs instance mask) for object recognition. Section 3.3 and Figure 5 is helpful in demonstrating it. However, the experimental setup is very limited (700 train + 300 test). COCO has 110K train and 5K val images and many more objects. If you argue that only 10 categories are common between COCO and ObjectNet, how many are common between LVIS and ObjectNet? I would strongly encourage the authors to leverage these pre-trained models and sharpen their message & contributions. I think they provide empirical justifications (important to the community) for expected results in moving from image to bounding box (same comment from multiple reviewers) but they need to de-emphasize that aspect and emphasize their results on context and robustness. A revision and resubmission to a different conference is encouraged. -------- after rebuttal --------Thanks to the authors for the response and the updated manuscript. My assessment stays the same, and below are my additional comments.1. About Appendix EThanks for the clarification about the initialization scaling. However, this raises more concern about the significance of the result. In Appendex E, it is shown that the scaling considered in the paper and the NTK scaling lead to the **same** gradient flow dynamics. This suggests that we are actually still in the kernel regime, in contrary to the main motivation and the claims in the paper. (As for the time rescaling issue, it doesn't matter in gradient flow since the difference can be absorbed by rescaling the learning rates.)Appendix E also mentions that several previous papers used a small multiplier $\kappa$ to make the initial network small. The authors claim that this makes the convergence rate slower. I don't think this affects the convergence rate, but it only affects the width requirement (see e.g. [1]). In fact, in stead of using this multiplier, there is another way to make the output zero without changing the NTK and without requiring a larger width, that is to use an anti-symmetric initialization -- see e.g. [2][3][4][5].[1] Arora et al. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks[2] Chizat et al. On lazy training in differentiable programming[3] Hu et al.  Simple and effective regularization methods for training on noisily labeled data with generalization guarantee[4] Bai and Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks[5] Zhang et al. A type of generalization error induced by initialization in deep neural networks2. About the motivating questionsThis paper proposes to answer two questions in the introduction. The first question is "Is the kernel regime, which requires impractical bounds on the network width, necessary to achieve good generalization?" First, I don't think this paper answers this question since the considered regime is still basically the same as the kernel regime. Second, even if it does, this question itself is not valid, since there are numerous previous theoretical works that study generalization outside the kernel regime, in more interesting settings, e.g. [6][7][8][9] and many more (none of which are mentioned in the paper).[6] Allen-Zhu and Li. Backward Feature Correction: How Deep Learning Performs Deep Learning[7] Allen-Zhu and Li. What Can ResNet Learn Efficiently, Going Beyond Kernels?[8] Wei et al. Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel[9] Woodworth et al. Kernel and Rich Regimes in Overparametrized Models.The second main question in the introduction is "Does generalization depends explicitly on acceleration? Or is acceleration required only due to the choosing an initialization outside the good generalization manifold?" I genuinely cannot understand this question.3. In the updated manuscript the authors state "To the best of our knowledge, this is the first non-asymptotic bound regarding the generalization property of wide linear networks under random initialization in the global sense." This is still false (and insignificant) since the stated result is a direct consequence of previous NTK work.4. I certainly understand that understanding deep learning is very challenging so it's a natural step to start with simple models. However I think this paper in its current form has limited significance and has major issues in how it discusses previous work, main motivations and contributions, etc., for reasons described in the review. Post rebuttal: The updates clearly explain the resampling procedure of this paper, and strengthen the theoretical part of this paper. As a result, I'd like to change my rating to 6 and recommend an acceptance.Additional thoughts emerged from the discussion with authors (which are irrelevant to the rating): I agree that experiments in the paper demonstrate the modified SAC algorithm has a significant improvement compared to baseline algorithms. And I believe that the paper could benefit from including some theoretical justifications to the loss function and data collection scheme (though I completely understand the difficulty of theoretically justify deep RL algorithm). For example:- Assuming all the loss functions can be optimized to optimal, will the policy converge to optimal or near-optimal solutions?- Assuming the value net can be optimized to optimal, how the resampling process change the gradient of policy net? In which case would the on-policy sample with truncated trajectory (i.e., the value function computed by Eq. (8) where the length of the trajectory is $n$) out-perform off-policy sample with full trajectory (i.e. SAC without resampling)? If I understand correctly, without resampling the error of the value net suffers from the amplification caused by distribution mismatch (which is potentially exponential?). And with resampling, would the error of value net come from the truncation?Additional minor issues:- Definition 5: There is no base (i.e., $n=0$) in the recursive definition of $\hat{v}_n^{soft}$.--------- # After discussion period:I have read all other reviews, resulting conversation, and have read the edits to the paper. After extended conversation with the authors and a deeper investigation into the empirical components of the paper, I find I have further concerns than originally realized in my original review and that many of my original concerns remain. I am lowering my recommendation from a 5 -> 3 to reflect the new concerns; namely the validity of the ablation study as detailed in-depth below. Author response: the authors answered my question about the absence of learning curves, and provided extra details. However, I still think that the paper could be clearer and more focused, a sentiment that I think I share with the other reviewers. Given my hesitation, I would therefore not vote for accepting this paper, but I acknowledge that the proposed method is original and interesting, so I would not mind if this paper were to be accepted. The author response answered some of my questions. But I cannot say that I now better understand the behaviour of the algorithm and the conditions under which it improves performance. I agree with reviewer 2 that analysing behaviour and performance in a simpler setting would be informative. While the writing has improved, it stills lacks the clarity and nuance one would wish to see in a paper at this conference.   === After rebuttal ===Since the technical contribution of this paper is not significant enough, I will keep my score as weakly reject. ##Updated Review##I'd like to thank the authors for addressing each of the points I made in my review and taking the time to include material that answers many of the questions I had.  This paper is in my mind a novel and interesting submission and a clear accept. -- Update: The authors have done more to acknowledge this limitation. -- Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4.------------------------------------------------------ after author responseI thank the authors for providing sensible answers to my questions, and analysis of their method compared to the minibatch version. I am still not sure about the fact that the rate of convergence of  SW is indeed indepedent of the dimension, as the dimension is indeed hiddent in the scalar product. Nevertheless, I believe this question is out of the scope of the paper, and I changed my final rating accordingly to the new version of the manuscript. ---------------------------------------------------------------------------------------------------------------------After rebuttals:The authors made put a significant effort to improve the submission, but I am still non convinced by the experimental results they are presenting. For instance, in Figure 4 there is no way of distinguish between BSS and Nash-Q. I suggest you to increment the repetitions of the experiment to highlight the improvement of your method over the literature ones. #########################POST-REBUTTAL UPDATE#########################Thank you to the authors for your detailed responses and for uploading your code. (Minor point: your README assumes that the GNOME desktop environment is being used; you may want to make the instructions platform-independent.) My main concerns about the learning process described in the paper remain. The authors indicate in their response that "it is difficult to quantify which is the most challenging part of the learning process". This makes it much more difficult to reason about whether the learning process is primarily about using the Bayesian Stackelberg game solver, and whether the interaction with the environment (given the limited number of trials) provides limited benefit. === After rebuttal ===I am not satisfied with the response from the authors. I can only hardly recognize the effort made by the authors during the rebuttal - most of my points were only briefly discussed in the response without revising the paper. - The suggested baselines were merely briefly discussed but not added to the comparison.- The results of the meta-dataset, which in my opinion is the most suitable dataset for the purpose, are still not included in the revised paper.- The stddev of the classification task is not still not provided, making it hard to justify the performance gain.- Why is RL left to future work?I have read the reviews from other reviewers. With the little revision from the authors, I have decided to keep my original rating and would not recommend this paper to be accepted. I agree that promoting experiments for real quantum hardware is important. But I don't think the team has yet to create a working platform that could be accepted to ICLR. If the open-sourced platform is the main contribution (rather than a new understanding of how quantum computers could be useful for machine learning problems), then the authors should submit the manuscript after having the open-sourced software available. Furthermore, a lot of the wording should be changed (the current version sounds like they are proposing a new quantum machine learning framework, while they are creating an open-sourced platform). -----------------------------Post rebuttal:I thank the authors for addressing my comments and updating the manuscript. I plan to keep my positive score. Post-Rebuttal Comment: I would like to thank the authors for thoughtfully answering my concerns and questions. I think the small modifications made in response to my comments have made the paper much easier to understand and I think the work is well presented and positioned. Ultimately, I have increased my score to Accept on the basis that I no longer have any major criticism of the work. I do hope the authors can make a more prominent note about appendix F1 in the main text as I think it is an interesting and important thing to highlight for those who may be skeptical.  == Post rebuttal update ==See my response to author's rebuttal below. In light of new experimental results, I feel this now meets the bar of acceptance at ICLR, and hence  updating my score to 6. ### POST-REBUTTAL UPDATEAfter reading the other reviews and the authors' rebuttal, I stand by my rating of 6. ------Post Rebuttal:I thank the authors for their response and additional experiments to show the performance of MOCO trained on ImageNet for the various downstream tasks considered in this paper. It is evident from the results that the authors presented in Table 5 that their best method (using their multimodal data) performs worse than MOCO trained only on ImageNet with InfoNCE. Hence, while this current work has some interesting novel insights of theoretical value, I don't think the complete proposed method of data collection and training is very practical or broadly scalable. It is likely to be of limited practical applicability.  In contrast to the authors' proposed cumbersome method of collecting annotated data using expensive gaze and motion sensors, cell phones and cameras are nowadays ubiquitous and image and video data is routinely uploaded to the internet by users all over the world. Using such abundantly available existing data on the web, which can often times simply be downloaded for free without any annotations, is what I believe is likely to be a much more practical and broadly applicable approach to solving the problem of representation learning via self-supervision. This concern is also shared by Reviewer 3. On weighing the various pros and cons of the proposed approach, I will maintain my previous rating. ===== Post rebuttal update =====I like the addition of TD Gammon like baseline and as a result of that I slightly increased my score. However please add more details about that baseline (e.g. did it have a comparable number of parameters to NDMZ?). You can also use performance of just the policy head (as a model free agent) to show what is the contribution of search.Also showing how performance scales with search would still be a valuable experiment. ===================(Nov 24) The authors addressed my concerns in the reply so I increased my score to 6. __________________________________________________________After rebuttal, the authors reformulate Theorem 2, and then I think it is right. I raised my score.I am still not convinced by their experiments on recovering bandits. ============== Post-Rebuttal ==============The authors' responses to point 1 & 2 do not sound (reflecting a question to another paper does not solve the problem). The authors mentioned "We made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach)", where the reference of Mei et al (2018) cannot be found in the paper, as a critical baseline. This raises a flag on the novelty of the work and completeness of the related work. Therefore, I am lowering my rating to 4. ## Post-rebuttal updateI have read the other reviews and the authors' rebuttals, and do not wish to change my review.I strongly believe that numerical task improvements are not in themselves a conceptual contribution. I look forward to the results of the analyses the authors mention in response to R3-Q2, to better understand what exact interaction between language and low-level visual input is being modeled.Along with R4 I remain unconvinced of the strength of the empirical results. The authors' response is not helpful here. I can't understand where the numbers (mean 60.74 IoU, std 0.06) come from -- taking stats across table 2 and table 1, I get very different results, so I must be misunderstanding where they come from.Significance tests would not take too much time -- it's not absolutely critical that you retrain the models for this. You can use data resampling methods instead. For example, on each individual dataset, run bootstrap tests comparing the predictions of your model and others on random resamples of the evaluation data and corresponding predictions.Pooling IoU results across datasets within model and then comparing between models can yield misleading results and should be avoided.  Edit: I have updated my rating based on author response. ## Post-Rebuttal CommentsI thank the authors for their detailed feedback. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting!I am not sure whether it is standard to show the learning curves with the "max achieved return so far" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm. Potentially, the stability could also be increased by adding a learning rate decay schedule?Overall, the rebuttal addressed my questions and incorporated many of the suggestions, therefore I am increasing my score and vote for acceptance. Post-rebuttal comments--After reading the authors' response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns. The inclusion of a clearer motivation, more discussion w.r.t. TPIL, and comparison to TPIL, all enhance my understanding of the contributions of the paper beyond my original review enough for me to increase my score from a 6 to a 7. .------------------------------------------------------------------------------------------------------------------Post-rebuttal updates:After reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I'm still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice. Nevertheless, I've increased my score to a 6 ----------Post-rebuttal commentsThank you for answering my all questions and updating the manuscript with some of my suggestions. The additions in section 4 clarify the motivation much better and also highlight the differences with prior work. I've increased my score from 6 to 7. -------Post Rebuttal---------Thanks for the feedback from the authors. After the rebuttal and discussion period, I believe that the contribution of the paper are mainly empirical. The Theorems (Theorem 1 and 2) are proposed to show the intuition of the bootstrapping and backward-update methods, and to connect the algorithm with recent theoretical results. During the discussion, there are mainly three concerns about the theorems among reviewers. - Firstly, the algorithm uses eps-greedy to help exploration, which is theoretically unclear. - Secondly, the theorems focus on episodic setting, while the algorithm is designed for discounted setting. - Lastly, it is unclear whether the algorithm is a frequentest approach or Bayesian. I believe that the former two concerns can be addressed in the following way, and don't weaken the contribution of the paper.- From the theoretical perspective, the feedback from the authors does tackle this problem. The eps-greedy will only add an epsilon-term to the final regret. I am eager to see the discussion in the final version. From the experimental view, the authors also claim that all the algorithms in the experiments use eps-greedy to help exploration. In that case, the improved performance of their algorithms are mainly due to their bootstrapping and backward-update methods, instead of eps-greedy trick. Besides, I think applying several widely-used tricks (such as eps-greedy) to the algorithm is acceptable, which makes the results comparable to other methods that also use the tricks. - To the best of my knowledge, there is currently no paper studying linear MDP in the discounted setting from the theoretical perspective. I think this is the reason why this paper studies the connection of the theorems in episodic setting. Meanwhile, it is hard to conduct experiments in episodic setting (In that case, the algorithm needs to learn the value function of all the possible horizon). Moreover, we can reduce an MDP with discounted rewards to episodic MDP by setting the effective horizon $H = \frac{1}{1-\gamma}$. This means that the theorem for episodic MDP can hold in the discounted setting with slight modification. Maybe such discussion should be added to the paper.However, for the last concern, I am still puzzled whether the algorithm is a frequentest approach or Bayesian, as there are many unclear statements.Overall, I believe the main contribution of the paper is from the empirical perspective. The theorem is intended to show the intuition of the algorithm and to connect the approach to the recent theoretical results. The experimental results look nice in general. I am a little disappointed that the authors missed the comparison with two related literatures in the initial version. I agree with R5 that these results may be done under a time-crunch. As a result, I change my score to 6, and I hope the above problems can be addressed in the next version. ## POST-REBUTTALI really appreciate the author's engagement and response during the discussion phase to help me understand the paper better, and the revisions incorporated in the paper.But after much thought, I do not think the current form of the paper meets the bar for publication.Here are my main concerns that I hope is useful for the next version or final submission.I think the core idea of the algorithm is uncertainty propagation is necessary for inducing effective exploratory behaviour. This core idea is theoretically motivated from sound strategies for exploration in finite-horizon RL, but as the paper addresses the discounted problem setting with deep neural networks the soundness is traded-in for computational tractability  which is a fine choice.But, currently the choices are presented in a confusing way, and the actual contributions of the algorithm are unclear: most importantly, is it a Frequentist approach to exploration or Bayesian?- under the Frequentist approach setting, utilizing e-greedy seems justifiable just based on the reasoning of this is the widely accepted practice in the field.- under the Bayesian viewpoint, which is the crux of the architecture used here (Bootstrap DQN), the attempted theoretical connections in the paper (Theorem 1) and the practical algorithm proposed (based on the ideas of Optimistic LSVI), do not provide a clear picture.To show theoretical soundness the algorithm is anchored to a Bayesian architecture and theoretical uncertainty connection, but for practical performance purposes the paper leverages reasoning from Frequentist Deep RL methods. Maybe this is a step in the right direction, and the extensive empirical results do seem to suggest it is effective, but the presentation is unclear. From the current draft:- OEB3 relies on the posterior of Q-functions  Bayesian- UBE uses posterior sampling for exploration, whereas OEB3 uses optimism for exploration  FrequentistTherefore, this can be improved and presented more clearly to communicate the idea. ==== Updates after the response ====I thank the authors for answering the questions in detail. Providing an example application does help readers understand scenarios where the threat model could apply. However, I still think such scenarios are not common but agree that the findings in this paper could be helpful for future security research. I adjusted my rating based on this better understanding.  # Rating and comments after the rebuttalI share the concerns of fellow reviewers regarding the practicality of the assumptions needed for launching the attack presented in the paper, however, based also on the discussion with the authors, I think that the paper is interesting regardless as it can lead to better insights regarding the development of suitable defence mechanisms for securing the architecture and the information carried by a Deep Learning model.I share also the concerns raised in the other reviews that the paper might be better appreciated by an audience focusing on cyber-security. However, I think that the subject can also be of relevance to ICLR as the paper made an effort to highlight the aspects more relevant to the ML community.Hence, leaving the aspect of relevance to the ACs' discretion, I think that overall this is a clearly written paper based on well executed research that presents some interesting results that are potentially impactful in the aspects concerning security of systems employing Deep Learning models. ---Following the author's reply, I've raised my score from a 6 to a 8. **Post response update:**I would like to thank the authors for their response and revision addressing my concerns, at least to some extent. However, after careful consideration as well as looking at the other reviews (and responses), I am still not convinced the case can be made for increasing the score beyond marginal leaning towards accept, which was the original score I have marked for the paper. The paper does provide some interesting empirical evidence showing that GNNs can be simplified to by sparsifying, or even removing, the learned weights for combining different channels between message passing steps. However, this kind of simplification is not particular to GNNs (indeed, many popular network architectures are overparameterized and can be significantly sparsified), and has little effect on the incorporation of graph structure in the network. The activation-only experiments are perhaps more specific to GNNs, but on the other had, they mostly establish the importance of message passing, which has already been pretty well established in previous work (together with certain well described limitations of current designs). As other reviewers indicated, the insights provided here are intriguing, but it is not clear whether they meet the level of significance and impact expected from an ICLR publication. --- . I was convinced by further introspection that this is a significant enough contribution --------------------------After reading other reviews are rebuttals---------------------After reading all the reviews from other reviewers and corresponding rebuttals, I think this paper is a good paper and enough to be accepted in ICLR. 1. I think it has a clear difference from f-GAN. It can provide a new loss function for the generative models which can further extend the success of generative models in the future.2. Experiments are not super interesting but at least it has some intuitions corresponding to the authors' claims.3. General theoretical results for the generative models (such as when should we use which loss) is a very difficult problem to solve. Maybe this paper can provide some intuitions for solving that large problem. But it seems too much to ask this thing to the authors of this paper. Without that, I think this paper is still worth to present to the ICLR readers and participants.Therefore, I am standing on my original score (7). =====updates======== After reviewing the other reviews and rebuttal, I will remain my original recommendation. **Post-Rebuttal**I appreciate the authors' efforts to make this submission much more solid. The investigation of the tail sample memorization phenomenon is insightful and would be beneficial to the community. However, the experimental results on standard benchmarks (e.g. ImageNet) are still not convincing enough. Thus, I will only increase my score to 5 (marginally below acceptance threshold). I definitely encourage the authors to further polish the manuscript and submit to future venues. Post Rebuttal:I do appreciate the efforts and additional experiments and theoretical analysis that the authors made in the rebuttal. While this paper proposed an interesting approach to long-tail recognition, some connections, distinctions, and comparisons with related work and thorough experimental analysis were missing in the original manuscript, as mentioned by other reviewers as well. Some of these concerns were addressed in the rebuttal, but not fully clarified. For example, the new comparison on the more challenging ImageNet-LT dataset (Table 3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission. ----------------------------------------------------------Post rebuttal: I have read the authors response and will go with my original score for this paper. **Update after authors' response**I want to thank the authors for their responses. My responses to the authors' comments are in the respective threads.--- ==== update ====I thank the authors for providing such detailed response. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. I hope to see this paper accepted.  **Update** : Since most of my issues have been addressed, I have changed my rating from 4 to 6 .------------Updates after rebuttal------------- Thanks the authors for answering my questions. However, I don't think my comments are well addressed. Even though the paper [d] may not provide public available code, the authors could either use results from the paper [d] or implement the proposed attack on the models used by [d] to see the difference **Update**: Thanks to the authors for addressing my comments. As it was pointed out by the authors, temperature rescaling is mostly applicable to non-linear loss functions. For linear loss functions, temperature scaling only linear rescales the gradients. The difference between the proposed PGD++ attack and PGD with linear DLR loss is small (see the author's response to AR4). The improvements are most significant for FGSM but FGSM is not recommended for the robustness evaluation. Given the limited technical novelty and small improvements for linear loss functions, my score remains unchanged. %%% post-rebuttal %%The authors replied to my comments related to the diversity condition in 3.2 and Theorem 1. Their answers did not fully clarify my concerns or misunderstandings, and it seems the authors didn't make any changes in that regard in the revised version. Except if I missed something in the review system, they did not answer my other comments.%%%%%%%%%%%%%%% Update: I would like to increase the score to 7. The authors have improved the paper and addressed my concerns. Update: I'd like to thank the authors for carefully addressing my concerns.. Reviewer 2 claims $P^4$ training is a special case of previous work. It seems the authors agree at least to some extent *"we agree that the P4 update scheme is closely related to dynamic trivializations"*. It is not clear to me how harshly this should be penalized. In some cases, it is interesting to research special cases of known general results. Unfortunately, it is hard for me to judge if this is the case here, as I know of the previous articles reviewer 2 refer to, but I have not studied them carefully. As a result, I am decreasing my confidence from 3 to 1 and my score from 7 to 6. I hope this does not discourage the authors, and wish them good luck with future research.  Update: I increased my score following the clarifications and addition of the BNN experiment during the discussion phase. Update: I've increased my score. ------------------Update:I've raised my score in light of author response and new results.  ---------------------------------------------------------# Review UpdateBased on the response and update from the authors I will improve my score from '5: Marginally below acceptance threshold' to '6: Marginally above acceptance threshold'. I generally believe that the presented approach merits publication. The methodology is sound and the experiments are convincing. I appreciate the fact that the authors made an effort to improve the writing. *** post-response ***Thanks for the clarifications and revisions in the manuscript.- A2: Lemma 1 (and its M-step repetition) show that the outputs of mirror descent x_{t+1} are guaranteed to be close to the minimizers of f_t. But the loss of x_{t+1} is measured on f_{t+1}; this is what I meant by "bad decisions". Thus I'm confused about the statement "the learner is not expected to make bad decisions"; Lemma 1 does not imply this. It only implies this when the consecutive minimizers are close (which is captured by C_T and S_T being small).- A3: This example makes sense (though in this example, the function is L-Lipschitz, not L-smooth). I also noticed that the example in Remark 5 isn't both smooth and strongly convex in any norm.- A4: Understood; thanks.- A5: I now understand (from the addendum in the paper as well) that this "user control of L_r" is basically referring to the standard selling point of OMD that the user can specify a regularizer that adapts to the geometry of the decision set and loss functions; I buy this point. This is separate from the ability to handle Lipschitz constants being large (which gradient descent can also handle, by having a smaller learning rate).Additionally, I'm confused about the response to R2's "simple strategy". The convergence rates of GD and MD are stated in incompatible ways. The right comparison would be to pick M so that the RHS of Lemma 8 in the appendix to be smaller than \eps. I agree that this work strictly generalizes [Zhang et al. '17], but I don't agree that this work bypasses computational difficulties of approximately minimizing a smooth & strongly convex f_t; as the authors point out themselves in a different reply, when r is quadratic, OMMD reduces to running OMGD.In my opinion, this algorithm and analysis are potentially worthy of publication at a top venue, but the manuscript evidently needs an overhaul in its justification of the setting and G_T. Thus my overall score is unchanged. ___________________________________________________________________________________Update after author feedback:I thank the authors for improving my understanding of the paper. I feel better about it after reading it again. One of the most interesting findings--that a translation network trained on one domain/set of classes generalizes to another--needs more discussion. Using this phenomenon to improve robustness is a good idea, and the MRT/MDA/MAT methods explored in this work are nice choices for this investigation. However, I agree with the other reviewers that it would be nice to include more baselines from other work where appropriate.The results in Table 1 are also very interesting and deserve more discussion--perhaps an analysis of whether a translation network trained on weak corruptions can generalize to create something akin to the ground-truth stronger corruptions, as the results in the table for MRT imply.Overall, I think this paper has some strong experiments and investigates a good idea, but the claims of a "paradigm shift" are overly grandiose, and some of the most interesting experiments could use more analysis. Additionally, the writing clarity and presentation could be improved. My initial understanding of the paper was flawed in some places, so I'll raise my score to 5. After author's rebuttal:"The other paper that the review points to ([4]) addresses a similar setting to our paper, but the approach is completely different. ... Moreover, [4] was published within three months of the ICLR submission deadline, meaning that it is essentially concurrent with our work."Yes, the reviewer concurs that a work published so close to deadline should be treated as concurrent and will raise the score. ------Updates after author response------I thank the authors for the response and it helps clarify some points. However, I am still not unconvinced that the paper is a significant departure from current work on adversarial robustness (see weakness 1 and 2 above). I think it would be much more interesting if the approach could yield robustness against a wider/different class of shifts compared to what it was adversarially trained against. Therefore, I am keeping my score at 5 and will not advocate for acceptance, though I am not completely opposed to it. -------------------# UpdateAfter reading the updated version of the paper, I think my main issues with presentation have been addressed. I now see this work as introducing a relatively niche problem and solving it elegantly. The text is clear enough for an average ICLR attendee to understand and contains additional background information in the appendix. I have increased my score to a 7.------------------- ---## UpdateI thank the authors for responding to my questions and revising the paper.As indicated in the first sentence of my review, the submission's biggest flaw is in poor presentation. I have read the revised version, but I am afraid to say that I remain disappointed. Although I can see that the authors have tried to address some of the common concerns, the changes do not go far enough. Given that the current version is a version in which "all [...] comments and suggestions have been taken into account," I do not see a reason to give the authors a benefit of the doubt, and have decided to keep my score as is. However, because my evaluation appears to be quite different from those of the other reviewers, I think it may be possible that they are seeing something in this paper that I am incapable of. Since I cannot profess to have a complete understanding of the content, I have decided to downgrade my confidence score.I will give two examples of ambiguous / confusing language. They are both from Section 2 after the paragraph heading **Definition of constrained black-box uncertainty modeling**.1. (The last sentence of the 1st paragraph) "Given this context, the pointwise forecasting system mentioned above is a function $\beta:\mathbb{R}^D \to \mathbb{R}$, which tries to approximate a certain conditional summary statistic (a percentile or moment) or $p(y | x)$.- My understanding of this sentence is that $\beta$ is a _statistic_ (i.e., a function of $\mathcal{D} = (\mathbf{x}_i,y_i)_{i=1}^{n}$) such that $\beta \approx \beta^*$, where $\beta^*$ is some functional of interest of the conditional distribution $p(y | \mathbf{x})$. For example, $\beta^*$ could be the conditional mean $\beta^*(\mathbf{x}) = \mathbb{E}[Y | X = \mathbf{x}]$, in which case $\beta$ is usually obtained by minimizing the MSE, or $\beta^*$ may be the conditional $\alpha$-quantile, in which case $\beta$ is some data approximation of the conditional $\alpha$-quantile.- Now, compare the above sentence to the following sentence from the author response: "For instance, $\beta(\mathbf{x})$ can be the conditional mean of $p(y | \mathbf{x})$."My point here is that throughout the paper, $\beta$ is used to refer to both a functional of the conditional distribution or a data-based approximation for the said functional. I find the lack of distinction puzzling, if not downright confusing.2. (The 5th paragraph. I think this is supposed to address my objection about the lack of a precise problem statement.) "The overall goal of the present article is, taking a pre-defined black box $\beta(x)$ that estimates a certain conditional summary statistic of $p(y | \mathbf{x})$, to model $q(y | \mathbf{x})$ under the constraint that if we calculate the summary statistic of this predicted conditional distribution, it will correspond to $\beta(\mathbf{x})$."- What is $q(y | \mathbf{x})$, and what is its precise relationship to $p(y | \mathbf{x})$? The first occurrence of $q(y | \mathbf{x})$ is followed by the description that it is "a conditional density model," but this can mean many things. Judging from the remainder of the article, $q(y | \mathbf{x})$ is either a model defined through a location-scale family or a model defined via a specification of quantiles. The proposed method is exclusively concerned with the latter kind of models. If this is the case, it would help the reader to have this said explicitly when the symbol $q$ is first introduced.- The usage of the term "statistic" does not appear to be standard. A _statistic_ is a function of the data $\mathcal{D}$, and is therefore a random quantity. A percentile or a moment of $p(y | \mathbf{x})$, on the other hand, is a functional of the conditional distribution $p(y | \mathbf{x})$, and would be non-random.- Even ignoring the previous points, the sentence made little sense to me. I will say more on this after the following paragraph.More on the subject of clarity, notation-wise, I am bothered by the proliferation of $p$'s. Aside from the occurrence in $p(M | \mathbf{x})$, which is rare and hence not a cause for concern, $p$ is used to represent the conditional density (distribution) $p(y | \mathbf{x})$, as well as a generic Chebyshev polynomial approximation $p(\tau,\mathbf{x};d)$. It is possible that I am failing to see some deeper connection here, but if none exists, I would prefer a more clear system of notations.Returning to the problem statement, after re-reading the paper several times and with the help of the author response, I have arrived at the following interpretation:Find an estimate / model $q(y | \mathbf{x})$ for the conditional density (distribution) $p(y | \mathbf{x})$ such that $q$ and $p$ have the same $\beta$. Here, $q$ is given by specifying the quantiles, and is approximated by a Chebyshev polynomial of degree $d$. The usual QR methods are inadequate, because the model they output will not have the same conditional percentile / moment / etc. as $\beta$.However, in situations in which $\beta$ is also being approximated, what is the point in constraining the functional at $\beta$? As pointed out by the authors, if $\beta$ is inaccurate, then this error is propagated to the quantiles, and the performance is worse. If this is the case, why is it of practical interest to enforce the constraint? Had the authors given a concrete, illustrative example, this question may not have arisen. However, absent a practical motivation, it is hard to understand why the constraint needs to be enforced at all.In any case, suffice it to say that I am still experiencing significant difficulty pinning down the exact goal of the paper.At the time of writing my initial review, I had hoped that the issues with clarity would be fixed in the revision, leaving me free to evaluate the paper based on the merits of the proposal. However, this has not been the case. Now that I am more familiar with the paper, I realize that the biggest issue about the clarity has much to do with the organization, ambiguous language, etc. Some of it has been addressed in the revision, but the effort did not go far enough. Even leaving aside any reservation I have about clarity from the point of view of methodology and/or theory, I have to say that I am deeply dissatisfied about the lack of practical motivation. I believe the authors intended the paragraph after Section 2 to be a response to concerns about poor motivation (which have been raised by other reviewers, not I): "The present article was motivated by a real-world need that appears in a pointwise regression forecasting system of a large company. Due to the risk nature of the internal problem where it is applied, uncertainty modeling is important." It would have been nice if the authors had chosen to respond by giving a _concrete_ example, complete with a real data set and an actual task to which their method is an adequate solution. However, the example given in the added paragraph is far too generic and vague to be useful as an illustrative example.Finally, when I asked for theoretical justification, I was looking for some actual proofs about e.g., consistency guarantees, approximation quality, etc.---## MinorI find the size of the text in the plots to be much too small and hard to read. Also, please label all axes in Figure 3. UPDATE after author response:Estimating arbitrary quantiles to approximate the predictive distribution of a black-box model is a novel problem to look into. In response, the authors have attempted to address my major concern and they are mostly clear. I would suggest the authors add more discussion on the motivation of the problem settings, e.g., whether it arises from an actual business problem, why satisfying constrained black-box uncertainty problem is a must-have, etc. [EDITED AFTER DISCUSSION: My concerns are largely addressed and the paper is now stronger. I very much hope to see it at the conference, and have updated my review and rating accordingly.] ### Final Recommendation after Author Response ###I would like the authors for the very active and productive rebuttal period.  Actually, the current version has significantly deviated from the initially submitted version. I have read most of the paper a second time to take all changes into account. The authors considerably improved the paper and have addressed some of my concerns. I increase my score to 5. The reason for not increasing the score to 6 or 7 is mainly that clarity of the paper is lacking. To give two examples: * Presumably because of the many changes during the rebuttal, the organization, structure and the quality of the manuscript is not always sufficient: to name one of several examples, the core Theorem 1 comes without any reference to its proof or any proof sketch in the main paper (there exists a proof in the appendix, finding of which required scrolling through the entire appendix). * As stated in my initial review, since DANN is a central method in the experiments, it should be briefly summarized to make the paper self-contained and also the specific trainingI think the core issue with the submission is that there is simply too much content for one paper: * formalizing 3 threat models * a proof of separation of maximin and minimax * empirical evaluation  on two datasets (MNIST, CIFAR10), three attacks (transfer, two adaptive attacks), three defenses (DANN, AdvS, TTT), and two settings (homogenous, inhomogeneous)Because of the page limit, a lot of details have been moved into the appendix, making the paper difficult to read. Moreover, even taking the appendix into account, details remain unclear such as how DANN was trained. Moreover, I do not find it convincing to move related work to the appendix; relating the current work to other work should be an integral part of the main paper.My recommendation for the authors would be to strengthen the focus of the paper: I think the inhomogeneous setting does not contribute much and could be removed. Also I don't see much value in MNIST and the weak transfer attacks. Moreover, the parts on preliminaries and threat models is too long for a conference paper and could be shortened. I think if focus were improved and the main document became more self-contained, the quality of the work would be considerably improved. For the time being, I see the submission still marginally below the acceptance threshold. #### Thank you for the authors to having answered my questions and having addressed all my comments. My main concern was about the generalisation of the proposed architecture and the results at the current revision are convincing to me. I believe this direction of the research together with the approach taken make a nice contribution to the conference. In consequence, I have raised my score from 5 to 7.   <After Rebuttal>Thank you for your detailed response.I will keep my positive score because my concerns are resolved partially. ------Authors' response addressed properly my request. POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. Most of my concerns were clarified and I still think the paper should be accepted. However, I agree with Reviewer 4 that additional experiments would be good to better tease out the reasons for this method working. After rebuttal: I'm glad they added some experiments and analysis I wanted to see, so I raise the score to 6. As the authors said, the paper is a proof-of-concept of unsupervised hierarchal scene graph learning, and the rebuttal to some degree reassured me. For example, modeling a cube on top of another top as two parts of an object (which was weird to me: why not each cube as an object?) helps edit tasks where the top cube is enlarged but the "on top" relation is maintained. The downstream representation transfer also makes sense. Of course experiments are still toy from computer vision perspective, but I'm now okay with acceptance. **AFTER REBUTTAL**The rebuttal largely addresses my concerns about implementation details.I am pleased to see the additional experimental results provided by the authors; I think that these do improve the paper. I still feel that some well-tuned handcrafted approach could likely perform on-par with the results of the proposed method, but the comparison with [Wei et al] show that achieving such results is at least not trivial, which does help to better ground the complexity of the task. The additional experiments with a three-level hierarchy show a bit more evidence for scalability than provided in the original paper. While these extra experiments do strengthen the paper, I feel that they don't really address the core issue with the paper, which is whether there is any hope for the proposed method to scale to more complex and realistic datasets.Overall I think that this is a reasonable paper and I still lean slightly toward acceptance, so I maintain my original rating of 6. Post-Rebuttal Comment=====I thank the authors for the detailed response and the updated results. While my overall opinion of the work is slightly more positive post-rebuttal, I still maintain that this is a clear reject, primarily for the following reason:The technical contribution (adding a hierarchical layer to SPAIR and demonstrate the hierarchy can also be learned without supervision) is not significant enough to accept purely based on a "proof-of-concept" of a "new" direction.For incremental contributions, I expect the experimental results to be more convincing to be acceptable, a few points that I still really expect to see:comparisons on harder datasets when one doesn't have to go into a specific metric to show an edge over a prior art that is targeting a different applicationresults on decomposition with more significant overlaps (especially in 2D) and on objects where part boundaries are harder to infer (actual 3D objects is still preferable)object-level manipulation (row 3&4 in fig 4 did not match description) and latent space interpolationI would also strongly encourage the authors to highlight the similarity and different between SPAIR and SPACE when introducing the latent code formulation. Update ==Thank you for your response and clarifications. I have left my score as is. Thank you for the response! Most of my questions are addressed (except for comparisons with other high-precision text generation methods, and error analysis, which I'd like to see but not sure if possible). I think the discussions and clarifications make good sense to me. Indeed, I agree that there are a lot of cases where focusing on quality at the cost of "diversity" is desired, which justifies the motivation of this work. Post-rebuttal feedback:Authors, thank you for your feedback. The additional comentary and results around diversity have strengthened the paper. However, my other concerns, as described below, remain, and so I have not increased my score, but I have indicated that if an effort is made to address these remaining issues, I would recommend that the paper be accepted.After reading the reviews and the authors responses, I have several remaining concerns.-First and foremost, the authors have confirmed that the on-policy results that they compare to are using weak baselines to normalize reward (constant, avg. of last 100 steps). Strong context-dependent baselines are known to be crucial to the performance of on-policy methods. The authors attempted to do some experiments with a MIXER variant without a learned baseline (Ranzato et al, 2016) given my concerns, but MIXER without a learned context-dependent baseline is not MIXER! This is serious, as the conclusions stated in the paper cannot be made until the technique is compared properly to on-policy methods (i.e. that at least utilize context dependent baselines... those with learned q functions [e.g. An actor-critic algorithm for sequence prediction, Bahdanau et al, 2016] are often even more effective)-The authors did not tone down their claims, or criticisms of on-policy methods requiring MLE pre-training in the paper, despite the fact they also initialize with and ML model, and interleave ML updates during training! The tone of the paper is clearly in need of revision, as I discussed in my review.-This is not a major concern of mine, but it worth mentioning that the novelty of this paper is actually on the low side. This is a standard application of truncated off-policy learning, and the cited paper out of Bengio's lab (Serban et al, 2017) is in the text domain, and describes essentially the same off-policy approach (although this paper is arguably more clearly presented, and more focused and thorough wrt investigating off-policy variations). In addition, as another reviewer mentioned, the connections to and related work that considers Jenson-Shannon and reverse-KL minimization are strong (and interesting), but they are not discussed/referenced at all.With all that said, for the most part, I actually like the paper. If the language/position of the paper is toned down/updated, and the results are updated to include stronger on-policy baselines (regardless of the outcome), I think that the paper would be above the acceptance threshold. ---Post-rebuttal update I have read other reviews and responses, and I am willing to decrease my score to 5.My initial review was based on assumption that RS (Hen et al., 2020) is significantly different to this paper, but after reading other reviews and responses now I can see that it is indeed questionable. Also, It was somewhat surprising to me that the paper copied wrong values from the RS paper, as pointed out by R5, considering that RS is the closest related work of the paper.Although I still think the proposed method is reasonable and the empirical results are impressive, I agree with other reviewers that the paper should have done more analysis on why the proposed method works to convince the reviewers (and the future readers), and to give more insights on why the combination of losses (and the use of WTA) is a right way to go. -------------------------------------------POST REBUTTALThanks for the author's swift responses. I appreciate that. However, the paper still needs to provide more evidence and precise comparisons to clarify the mentioned concerns. Three major concerns are listed below:1. Why does the proposed pipeline perform better than RS? The paper should add the proposed modifications one-by-one into the RS method and show/discuss how these individual design choices affect the performance.2. WTA is worse than rank statistics in Appendix Table 5. Is it a counterexample of WTA? It needs more experimental supports to show WTA is a better method.  The hyper-parameters used in all experiments should be provided in a table and explain how the parameters are selected. Please make sure the same tuning budget is given to all the methods for a fair comparison.3. What are the necessary changes to make end-to-end training possible (or not possible in RS)? The paper should summarize RS's design and explain why it can not do end-to-end training, then explain what modifications are made to make this possible. This comparison is better to be illustrated in figures.Overall, this is an interesting paper, and the community will be benefit from it if well presented. Please consider revising and submitting to another venue. **Update: After considering the other reviews and subsequent discussion, I have decided to maintain my score. As covered elsewhere in the discussion, the key shortcoming of the paper in its current form is that it provides little insight into why the proposed method outperforms the highly similar method in Han et al. For instance, the fact that WTA is crucial for the method proposed in the paper but does nothing for Han et al. is rather mysterious. I think it is necessary to provide more insight into these differences before readers can have full confidence in the proposed method.** ===============================================================================After reading the rebuttals and comments from other reviewers, I decided to keep my original score.My biggest concern hasn't been addressed. I understand there is a comparison between different alternatives in appendix A, but only WTA is outperforming RS in the existing work by a large margin. All the other reviewers and I are curious why this is the case.   Performing an A/B testing type of analysis can only tell us WTA is the best in some limited benchmarks. However, what would really make this framework principle and shine are the insights and messages we can learn from your solution. I further thought about the phenomenon and came up with some hypotheses. For example, WTA is an LSH that the collision probability of h(x) and h(y) corresponds to the ranking/ order alignment of $x_0...x_{d-1}$ and $y_0...y_{d-1}$. Because the choice of K is usually 2,4,8,16, it is more like local ranking information within a few random dimensions, while RS in Han et al is comparing the ranking of x and y globally (across d dimensions). Also, if you see your table 7,  when k goes larger, your performance saturates and degrades. If you start by analyzing the fundamental difference between RS and WTA along with your appendix B and C, you can maybe hypothesize that local ranking (among random dimensions) is more effective than global ranking and verify it with further experiments.  E.g. Intuitively with simple calculation, the half of the pair-wise order information among 4 elements (K=4) could be preserved with WTA hash code ( it is the index of the max so if the hash code is 0, it records 3 out of 6 relations: element 0 > elem1, >elem 2, >elem 3).  ##**Post Discussion Comments:** So the author did a **filibuster** and **flooded the discussions** with bloated comments. In this manner it was close to impossible to keep track of anything. **There has to be character limit for responses otherwise this is not feasible**. I looked at the videos and your physics simulators looks **really catchy**. At one point in time, the pole of the cartpole is at 10-11 o'clock and the cart starts moving right (the pole has close to no velocity and hence only a very small angular momentum). In this setting it would be natural that the pole would fall down if this state is maintained for a longer period (which it is in the video). However the pole goes upwards into the balancing position. This is really weird. And don't get me started on the pen-orientation as the pen sometimes floats mid-air. For this setting the gravitational constant really does not seem right. I also wouldn't consider the task solved as this is more an really uncoordinated movements for three specific configurations. For the simulation studies some doubts remain, but the authors improved the paper. Therefore, I am going to increase my score to weak accept. Nevertheless, the experimental evaluation could be improved and the paper would really benefit from real experiments.  Rebuttal update: the authors have gone beyond the normal scope of a rebuttal phase to update their experiments and the motivation of the work, and for that reason I have improved my recommendation to be above the acceptance threshold.----- Update: I appreciate the authors addressing my concerns. I have increased my score accordingly. **Update**: Thank you to the authors for addressing the comments and updating the paper. I decrease my rating from 4 to 3 as the original claims of the paper were disproved by the experiments with black-box attacks on CIFAR10 which showed that Neural ODEs offer little advantage over Resnets. I believe that more experiments are needed to demonstrate that Neural ODEs offer robustness advantages. For example, when increasing the computational budget for one step FGSM attack, the accuracy stays the same, which might indicate that Neural ODEs obfuscates the gradient. Experiments with gradient-free attacks also indicate that Neural ODEs obfuscate the gradient (PGD has higher robust accuracy compared to gradient-free attacks). The authors should do an extensive evaluation with stronger attacks (PGD with DLR loss or CW loss with multiple random restarts up to 100-1000, AutoPGD); the study of the gradient obfuscation (confirm that when $\epsilon=1.0$, the attacks can always succeed). ---I have read the authors' comments.  The addition of the boundary attack experimentwas an excellent step; however, it underscores a requirement for further analysis tounderstand *why*, apparently, in some cases the additional theoretical bound *fails* toconfer significant robustness.  The suggested "natural robustness" is only sometimespresent. Often clean accuracy is much reduced, so the method is not yet one I would consideruseful yet.  For me, understanding when the method works well (or not so well) wouldbring this work out of the realm of interesting theoretical bounds into one of moregeneral interest. ### Original Rating and Confidence**Rating** - 4: Ok but not good enough - rejection**Confidence** - 4: The reviewer is confident but not absolutely certain that the evaluation is correct### Updated Review after Author ResponseThank you for the thoughtful responses to my questions, and apologies for the delay in updating my review. In general the responses have addressed my questions, and I have updated my rating accordingly. However, there are still several issues in my mind:1. **Connection to active learning** - The authors correctly point out that active learning does not assume access to labels, whereas the proposed method does. However, I believe the comparison against selection algorithms such as k-center greedy should be included in the paper for completeness sake, and not just left as a comment in this forum.2. **Dataset Distillation: Instance Selection** - Upon re-reading the paper carefully, I found this subsection in Section 3.6 to be confusing. Why are there multiple $D_i$? The section seems to only talk about one $D_i$.3. **Set vs. Tensor**: In the new "representation learning" subsection of Section 2, the paper stresses that the model learns a representation of a set, instead of an individual data point. However, many tasks (e.g., image reconstruction) do impose some sort of ordering. The paper lacks clarity about where permutation-invariance is used vs. not used.4. **Other places that need clarification**:  - Equation (2): presumably you threshold the output of the sigmoid function, right? Because in Algorithm 1, it seems like $z_i$ are supposed to be boolean-valued.  - Appendix: there are several missing closing parentheses **Final recommendation**I support accepting this paper. While I am a bit disappointed the authors did not add in the paper the results they discussed during the rebuttal, I think the paper is interesting and clearer than at submission. ###############################################################################################After the discussion period:I thank the authors for their responses and for updating the paper. The authors have added a deeper analysis on the impact of the dynamic graph, however I still believe that the novelty of the paper is a bit limited. I have slightly increased my score to 6. ############# UPDATE #############I thank the authors for their response and for updating the manuscript according to our questions. I agree with the other reviewers that the novelty of this paper is quite limited. However, the idea of using a dynamic, learned graph from a general large search space of models is interesting, provides good empirical results on both image classification and object detection and the authors provide ablation for the new components that motivate the paper. I maintain my initial score: 6. Post-rebuttal feedback-------------------------------I thank the author for their reply and I encourage them to make the suggested improvements to the paper. #############AFTER RESPONSEI would like to thank the authors for the detailed response. I encourage them to keep working on MetaCURE: With some improvements I believe it will provide a valuable contribution to the meta-RL field. ------------------------------------------------------------------------------------UPDATEThanks for your detailed response!A1: I see, that makes more sense to me now. I'm still not 100% convinced that it might not be better to let the policy entirely meta-learn what a good exploration policy is, and only use the inductive bias (terms 1 and 2) for how to do so for meta-training and anneal those terms over time. This would mean that at test time the only thing that the exploration policy should do is maximise the return in the *exploitation* phase - and basically figure out what the best way to do so is (so you'd need the "missing term" I describe above). But that's just a hunch, not sure if that would actually work better.A2-A8: Thanks a lot for clarifying. I'll keep my current score given all reviewers agree that the work is unpolished in its current form, and because the authors plan to propose a new version of MetaCURE soon. I think this is very interesting and promising work and look forward to reading an updated version in the future! Edit following authors' response  ==========Thank you for your detailed response and updated version. I think the new revision is significantly improved, mainly in more quantitative analyses and details in several places. I have updated my evaluation accordingly. See a few more points below.1. Thank you for clarifying your definition of concepts. I still think that the word "concept" has a strong semantic connotation, while the linguistic elements your analyses capture may do other things. The results in appendix E do show that some semantic clusters arise. It's especially interesting to see the blocks in some of the heat maps, where similar "concepts" are clustered together (like the sports terms in AG); consider commenting on this. 2. The new quantitative analyses are helpful. One other suggestion that I mentioned before is to connect detected concepts to external resources like WordNet or ConceptNet. That would help show that "concepts" are indeed semantic objects.  3. The motivation for replicating as normalizing for length does make sense, although the input would still be unnatural. The comparison to "one instance" is helpful, but it's interesting that the differences between it and replication in figure 2 are not large. It would be good to show results that substantiate your assumption that without replication there will be a bias towards lengthy concepts. Does "one instance" detect more lengthy concepts than replication? 4. The results on frequency and loss difference in 4.5 are very interesting. There is another angle to consider frequency: words that appear frequently often carry less semantic content (e.g. function words), so one might conjecture that they would require less units. It may be interesting to look at which concepts are detected at each frequency bin.5. Minor points: section 2.2 still mentions "regression" where it should be "classification".  6. A few remaining grammar issues:- "one concept has a less activation value.." - rephrase - end of section 3.3: "this experiments" -> "these experiments"#ERRO!  UPDATE:After reading the rebuttal, I am increasing my score to 6. The authors alleviated some of my concerns but my major concerns about their novelty and the impact of their results remains. [New comment:] I have read the authors' explanations and clarifications that make me increase my rating. Regarding the technical novelty, I still don't think this paper bears sufficient stuff. If there is extra quota, I would recommend Accept. Edit: The updated results need consistent baselines. For example, the method of [7] should be consistently compared against. Additional ReviewThis paper did NOT handle the non-differentiability and non-linearity very well. We can see this from the following three perspectives:1. Proof idea: the proof of this paper is noisy version of the convergence analysis of  a simple convex problem --it treats the contribution of the non-linearity and non-differentiability as bounded noise.2. The network size is of order n^6.3. Network size requirement is dependent on \lambda_0. 1.Proof idea: The proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in Appendix (at the end of this updated review). The only difference between linear regression and the problem in this paper is the changing patterns due to the non-linearity of ReLU. However, this paper views the changing patterns as noises compared to those unchanging patterns (e.g., S_i v.s. S_i^\perpendicular). The key trick is that if the actual trajectory radius (i.e.,the largest deviation from the initial point) R is much smaller than the desired trajectory radius R (given by a formula), then along the trajectory, the contribution of non-linearity is just O(n^2 R), which is small compared to the contribution of linearity, i.e., -\lambda_0 (shown in proof on page 9). Following the above analysis, if the experiment shows that R is really small compared to R, then the approach of treating non-linearity as noise is fine. However, it is not the case for the problem studied in the experiments (Sec 5, Fig 1). In figure 1, we can easily see that the maximum distance R is O(1), which is far larger than R = c*\lambda_0/n^2 =10^-6 when n=1k. Therefore, the proof idea used in this paper is fundamentally not able to explain the phenomenon shown in the experiment. In fact, to address this issue, authors need to consider significant contribution of non-linearity, instead of just viewing them as noises. 2. The network size is too large. This paper requires O(n^6) neurons, that is 10^18 neurons for n=1000 samples used in the experiment. The theoretical trick to make R&lt; R is to note that R can be bounded by O(1/sqrt{m}) while R is independent of m, thus picking a sufficiently large m can make R very small. In a word, the reason that this paper requires so many neurons because of the inability of properly addressing non-linearity. 3. I found the dependence of the network size on the least eigenvalue funny, although the authors claim this tool is elegant. After authors add Thm 3.1 in the revision, I realize that the dependence on \lambda_0 might come from the fact that authors do NOT handle the issue of non-differentiability. Let us see a simple example. Assume I have a dataset with \lambda_0 = 1. Now I am adding one more data point (x=0_d, y=1) to the dataset. After adding this sample, \lambda_0 clearly becomes 0. It seems I am just adding a constant 1 to the loss function and the gradient descent can also converge to the global min with a linear convergence rate since the constant does NOT contribution to the gradient. However, it seems the proof does NOT work. This is due to the fact that the gradient of the non-differentiable points are NOT well defined. Here is a simple example: h(w)=(y-ReLU(w*x))^2, where x= 0, y =1. By the definition provided in this paper (Eq.4), we can easily see that dh/dw = 1 for any w, even if h(w) = 1 for any w. This means that the constant can provide fake gradient information and make  the maximum distance become infinity, (R=\inf). Therefore, the whole proof collapses. In fact, changing the gradient definition from I{z&gt;=0} to I{z&gt;0} does not address the issue and we can see this from this example w=g(w)=Relu(w)-Relu(-w) has a zero gradient at w=0. In summary, the problem considered in this paper where the size m=O(n^6), maximum distance R= O(1/n^2) is too easy compared to most problems in practice where m=\Theta(n), R=O(1). To address the latter problem, we need a better definition of subgradient and need to analyze the significant contribution of non-linearity and non-differentiability, instead of just viewing them as noises. =================================Appendix===============================The proof basically follows from the convergence analysis of the following linear regression problem (note that u_j is fixed):           \min_{w_1,...,w_m}\sum_{i=1}^{n}(f(x_i;w_1,...,w_m)-y_i)^2 = L(w_1,...,w_m)where   f(x;w_1,...,w_m)=1/\sqrt{m}\sum_{j=1}^{m} a_j*(w_j^T x)*1{u_j^T x&gt;=0}Gradient Descent Algorithm:-Initialization:-For each j=1,...,m: a_j ~ U({-1,1}), u_j~N(0, I)-Fix a_1,...,a_m, u_1,...,u_m-Update:-For t = 1,...,T    w_j(t+1)  = w_j(t) - \eta* \nabla_{w_j}L(w_1,...,w_m) for j=1,..., m.In this problem, since a_j and u_j are fixed, then model f is just a linear model w.r.t. w_js and the above problem is just a simple linear regression problem. Therefore, it is not difficult to prove the linear convergence rate for the gradient descent for the above problem under some mild assumptions.  Note that in this paper, u_j(t)= w_j(t) and are not fixed in iterations, i.e., patterns can change. ========================= -------------Revision--------------I disagree with most of the points that AnonReviewer3 raised (e.g., second layer fixed is not hard, contribution is limited). I do agree that the main weakness is the number of neurons.  However, I think that the result is significant nonetheless. I did not change my original score. Edit: I am changing my rating from 5 to 6 based on the authors' response. update: The authors' feedback has addressed some of my concerns. I update my rating to 6.================= This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values. [UPDATE] Authors' response to my questions did not change my opinion about the overall quality of the paper. Both theory and writing need a major revision.  After rebuttal:See the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning. Edit: Following response, I have updated my score from 6 to 7. Update after the author response: I am changing my rating from 6 to 7. The authors did a good job at clarifying where the gain might be coming from, and even though I maintain that decoupling the two variables is a simple modification, it leads to some valuable insights and good results which would of interest to the larger research community.------- The authors have put in an effort to answer the earlier questions, and acknowledged the aspects which cannot be directly addressed at this point. The draft/appendix has been updated to include new material and technical results. While the work and ideas are still in somewhat early stages, it will be good to see the ideas (of using random learning rates) get discussed more widely -- so I am updating my score to be above the threshold.  POST REBUTTAL: I think the paper is decent, there are some significant downsides to the method but it could constitute a first step towards a more mature learning-rate-free method. However, in its current state the paper is left with some gaping holes in its experiment section. The authors tried to add experiments on Imagenet, but these experiments apparently didn't finish before the end of the rebuttal period. For that reason, the paper probably should not be accepted for publication (even if the authors manage to finish running these experiments, we would not have a chance to review these results).  Thank you for your reply. I see no issue with the analysis failing to account for large , it obviously would be nice if it could but alas it does not.I think that this is a well-written paper with an interesting, novel contribution to parallel optimization, and I stand by my opinion that the paper should be accepted to ICLR. ***EDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7. +++++++++++++++++++I have updated my rating after reading author's responses EDIT: See bellow, raised score from 4 --&gt; 6. ****************************I've read the new comments from the authors and the new version of the paper. I think that the paper has improved significantly in terms of presentations, coverage of related work. I still see that the contribution is somewhat limited, but I'm updating the score to better account with this new version of the paper. Revised Review:The authors of this work has taken my concerns, and concerns of other reviewers, and revised their paper during the rebuttal period. They have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the Appendix section), and committed to open-sourcing the platform post publication. For these reasons I believe this work is now at a state that should be published at ICLR, and I revised my score from 5 to 7. I hope other reviewers can reread the work and post their updated comments.I'm excited about the work, because it incorporates good ideas from A-Life / evolution / open-endedness communities, to introduce new paradigms and new ways of thinking to the RL community. I look forward to using this environment in my own research going forward, regardless of whether this work gets accepted or not. Good luck!Minor comment: On page 4, the section 5 Experiments, I think "Technical details" should be in bold font before the sentence "We run each experiment using 100 worlds." so it is distinguished from being part of that sentence. After revision:The authors have addressed all points in my review. Although I will not be increasing the score, these fixes certainly increase the confidence of my evaluation and I think it deserves to be accepted.====================  * EDIT: I have re-evaluated the careful and comprehensive response to my concerns by the authors. I thank them for their effort in this. As many of the concerns were related to communication and have been addressed in the most recent draft, I think it is appropriate to move my review upwards. The revisions make this paper quite different from the original, and I am happy to re-evaluate on that basis--this is a peculiarity of the ICLR open review procedure, but I consider it a strength. I note that "data augmentation" in machine learning appears to have collided with a term in the Bayesian statistics literature, and the authors have provide a number of citations to support this. I strongly recommend "variable augmentation" going forward, as that is an accurate description (you are augmenting a random variable, rather than the input data domain). This appears to be one of the growing pains of the field of ML which has distinct and often orthogonal concerns to classical statistics around density approximation and computational issues.* ===================For reasons outlined in my comment below, I updated the score to 5 (for the new heavily updated version). UPDATE:I have read the authors response and the other reviews.  While the authors have made some improvements, my core criticism remains  the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.The authors also seemed to have missed the point of the Kearns &amp; Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.   ===========  comments after reading rebuttal ===========I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns.- I strongly encourage the authors to shorten the paper to the recommended 8-page. - The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC.- The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting.- Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. ====== second round after rebuttal =============I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. - GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. - I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. comments after reading response ===========The authors make a good response, which clarifies the unclear issues from my first review. I remove the mention of the concurrent submission.Specially, the new Appendix D with the new Fig. 4 clearly explains and shows the benefit of WC over W_zca. Post-Rebuttal================I thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods,  clarifications of the proposed approach, adding references to related work, I will  increase my score and suggest to accept the paper. A novelty of this work is that it is the first paper that methodologically analyses FRIs for DNNs a reasearch area which might shed new light on the understanding of how vision systems work and the source of misrecognitions and the limitations of recognition systems.In light of the changes of the paper and the clarification on the novelty aspect of this research, I suggest this paper to be accepted as it constitutes novel research in understanding how DNNs recognize image content and its similarities and differences to human vision. ============================================After discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating. EDIT: On the basis of revisions made to the paper, which significantly augment the results, the authors note: "the call for papers explicitly mentions applications in neuroscience as within the scope of the conference" which clarifies my other concern. For both of these reasons, I have changed my prior rating. Final Comments =======I thank the authors for updating the manuscript with clarifications and for clear replies to my concerns. I agree with R2 to some extent that the empirical performance of the method, as well as the formulation,  is interesting. In general, the authors addressed my concerns regarding how optimal transport training model interfaces with MLE training and the choice of using scaled softmax for computing the Wasserstein distances. However, I still find myself agreeing with R3 that the choice of sets to compute Wasserstein distances (as opposed to sequences is somewhat unjustified); and it is not clear how the theory in Sec. 3 justifies using sets instead of words, as the data distribution p_d is over sequences in both the W-2 term as well as the MLE term. This would be good to clarify further, or explain more clearly how Sec. 3 justifies this choice.Also, I missed this in the original review, but the assumption that KL(\mu| p_d) = KL(\p_d| \mu) since \mu is close to p_d for MLE training does not seem like a sensible assumption under model misspecification (as MLE is mode covering). I would suggest the authors discuss this in a revision/ camera-ready version.In light of these considerations, I am updating the rating to 6 (to reflect the points that have been addressed), but I still do not believe that the method is super-well justified, despite an interesting formulation and strong empirical results (which are aspects the paper could still improve upon).  ================================Thanks for the clarification and revision! It addressed some of my concerns. I would stick to the current rating, and vote for an acceptance. After reading through the authors' comments and rereading parts of the submission, I have become a little more positive about this paper. I am still unsure about the contributions to the ICLR community. The authors merely state "We believe our contributions to ICLR community are clear and valuable" without backing up this claim with an argument. But in the rest of the authors' comment, they make some good points. I think those points should be made more prominently in the paper itself. I would suggest that the authors describe their approach as using different, complementary encoders of the input sentence and consensus maximization. If they wish to describe this as multi-view learning, that's fine, but I think using the term "consensus maximization" (or something more descriptive like that) in prominent places would be helpful. If the approach is applicable beyond sentence embedding learning, then it would behoove the authors to describe the approach in a general way so that readers will see how they can apply it to their own tasks. As currently written, the paper is very much focused on sentence embedding learning, which causes me to think that the paper is more appropriate for an NLP venue. But it is true that ICLR publishes papers that are application-specific, so I can't consider this to be a deal-breaker for the paper.I raised my score to a 6. ======= After revision =========I still think this is a very interesting, novel and relevant idea that desires attention. However, on the same time, I agree with the points raised by the other two reviewers which are all well-motivated and relevant concerns. Therefore, I join the view that the paper is not yet ready for publication but I do encourage the authors to improve their work and resubmit to another venue. [Post revision update] The authors' comments addressed my concerns, especially on the experiment side. I changed the score. --REVISION--After reading the authors response and looking at the new version of the paper I decided to increase my score. The paper tackles very important problem and I strongly believe it should be presented during the conference. --- Post Rebuttal ---I had some confusion and concerns about the paper. Most of the confusions were addressed and made me view the paper slightly more favorably. However, my main concern wasn't addressed. In particular, it's unclear if the benefits are because of the "Value Density Estimation" benefits or if it's because of the mix of non-bootstrap updates. Comparing to a method that mixes in more Monte Carlo estimates (e.g. n-step bootstrap) would address this concern. Theoretically, the authors suggest that the reward should be 1/epsilon with epsilon -> 0. However, it seems like this was not done in the experiments, and it's unclear how epsilon should be decayed in practice. Given these concerns, I will maintain my score. ***Post Rebuttal***Thank the authors for the response. My main concern about the generality of the proposed method is not fully addressed. The baseline HER and GAIL can easily be applied in the domains with discrete action space. Also, the base RL approach for discrete action space exists. Experiments will be more convincing if the proposed method can outperform the baselines on various domains. ===== POST REBUTTAL UPDATE =====I maintain my score of 6.  While the proposed approach is interesting, the experimental section could be strengthen to firmly stake a claim that it's broadly applicable.  (c.f. point 2. in the review) -------------------------------------------------------------------------------------------------------------------------UPDATEI have read the other reviews and the author's response. Thank you for your thorough answer! It's great to see you've taken all feedback into account and updated the paper significantly. After looking through the changes in the paper I'm raising my score from 5 to 6. Some last comments:- Several other reviewers also raised concerns that the "fully" offline setting might be unrealistic. I saw you added a motivation for this in Sec 3, which makes me a little bit more convinced. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in-distribution tasks.- I understand how the reward function issues in ML45 could be the cause of the inconsistent results. Maybe ML1 would be a better choice at this point, or indeed waiting until v2 of the benchmark is released.  ============Update after discussion with authorsI had two main concerns:- The modification to MAML was unconvincing to me.- The offline meta-RL formulation should include behavior policy as part of the task definition.After a very detailed response from the authors, I am now happy with the response and extra experiments w.r.t. the MAML modification, but I still have concerns about the formulation. In particular, reading the final version I still think the policy giving the behavior data is treated as an after-thought and is instead assumed constant across all tasks. For instance, IMO figure 1 should contain multiple examples of the same "RL task" that are different "offline RL tasks"; i.e. learning to swim using guidance from a 3-year-old and learning to swim using guidance from Michael Phelps. This is one of the key differences, IMO, between meta-RL and offline meta-RL and given that this paper's main contribution is introducing offline meta-RL, I feel it really should be very clear about this point. It may be fine to first introduce the correct general version and then say something like "it may be useful to assume each RL task is given by an expert of roughly the same characteristics", i.e. we can assume behavior policy is constant across tasks. However, right now the original formulation directly borrows from regular meta-RL and I believe that may confuse future papers in offline meta-RL.I've increased my score from 4 to 5 since I'm now less concerned about the MAML improvement, but I cannot recommend acceptance given my concern about the formulation. POST REBUTTAL EDIT: I thank the authors for providing detailed answers regarding my concerns. I have updated my score in light these comments. Below are some additional comments in response. Response to comments regarding C1) and C2): While early stopping with pre-conditioned updates differentiates this work from (A. Rudi et. al 2019), the analysis still requires the knowledge of the population covariance. Indeed, while the authors have included a section (Appendix A.3) showing that the operator norm of the population and the inverse regularised empirical covariance can be controlled, it would be insightful to discuss to what extent this allows the analysis for the pre-conditioned gradient descent to be extended to an approximated population covariance.  Response to comment regarding C3): I am inclined to agree with reviewer 3, in that the manuscript is difficult to read due to the larger number of fragmented results. In this regard, I feel the authors should focus on a single phenomenon that is supported by both the parametric and non-parametric aspects of the paper, for instance, how pre-conditioning helps against misalignment.      Response to comment regarding different prior on ground truth (point 4. third bullet point): Note that some concurrent works have studied the case of different priors on the ground truth [2,3], which are likely relevant in this case. Minor Comment: The pre-conditioned updates for non-parametric regression (4.1) use notation $\alpha$ where as Appendix D.8.1 uses notation $\lambda$, with the discussion then switching back to using $\alpha$ and $\lambda$ being used in reference to the regularisation used within FALKON. The switching of notation is possibly confusing here. [2] - D. Richards, J. Mourtada, L. Rosasco "Asymptotics of Ridge (less) Regression under General Source Condition", arXiv:2006.06386 (2020) [3] - Wu, D. and Xu, J. "On the Optimal Weighted $\ell_2 $ Regularization in Overparameterized Linear Regression" NeurIPS 2020 ### Update after rebuttalThank you for the clarifications.A couple minor comments:- regarding theorem 7, my comment was that it would be useful to include the conditions on eta in the theorem statement in the main text (though I do not feel strongly about it)- regarding "misalignment" and the relationship between the random effects model and the source condition, I appreciate the improved explanation of this analogy, but I still find that the last paragraph in section 3.2 could do a better job at providing the right intuition (skimming through the Richards et al. reference pointed out by R4 gave me a better intuition). ## Post rebuttal commentsThe authors largely modified the paper according to the comments, with a lot of additional content. While this is quite beneficial, the paper raised many questions, some of which may need further treatment (for instance, increasing the number of objectives has an effect on the number of Pareto optimal solutions that is is not trivial). ** EDIT **The authors added many experiments, tables, figures, sections, and an appendix. The changes are too substantial and the paper looks like a completely new one. The purpose of author rebuttals is to address issues like a reviewers uncertainty about a point, an incorrect assumption, a misconception, or a misunderstanding of a part of the paper, not to revamp the paper completely.The paper was clearly incomplete at the time of its submission, and I still vote for its rejection.  [Revision]Greatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one. Update:I thank the authors for their clarifications. I have raised my rating, however I believe the exposition of the paper should be improved and some of their responses should be integrated to the main text. Update:I thank the authors for the response. Unfortunately, the response does not mention modifications made to the paper according to the comments. According to pdfdiff, modifications to the paper are very minor, and none of my comments are addressed in the paper. I think the paper shows good results, but it could very much benefit from improved presentation and evaluation. I do recommend acceptance, but if the authors put more work in improving the paper, it could have a larger impact.------ Update:I feel the idea of this paper is straightforward, and the contribution is incremental. To improve the paper, stronger experiments need to be performed. [Second update] I'd like to thank the authors for their detailed response. The authors have made changes that I believe improve the overall quality of the submission. I now lean towards accepting the paper, and have increased my rating from a 5 to a 6.Most notably: (i) they clarified that their secret-detection model was retrained on sanitized data in their experiments, (ii) they added details about their experimental setup and the algorithms used for their experimental evaluation, and (iii) they added experiments to the appendix of the submission that evaluated their framework on synthetic data. I do, however, still have some concerns about how well the privacy guarantees of the proposed algorithm would hold up in practice against a motivated adversary (since formal privacy guarantees appear to be relatively weak right now).As a minor comment, there may be a typo in Equation 20 of Section 7.2: the case (u, s) = (1, 0) is handled twice, whereas the case (u, s) = (0, 0) is never handled at all.[First update] I find the authors' problem statement appealing, but share concerns with Reviewer 1 about the privacy guarantees offered by the proposed method, and with Reviewer 3 about need to clarify the experimental evaluation. No author response was provided; I've left my score for the paper unchanged. (Note: this update was posted a few days before the end of the rebuttal period; the submission was subsequently updated.) --After rebuttal: thanks many points were answered. -----------Edit: most of the issues listed in "cons" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7. REVISED:I've revised by review upwards by 1, though I still recommend rejection. The authors improved the scholarship by adding many more citations and related work. They also made the model details and implementation more clear. The remaining problem I see is that the results are just not that compelling, and the experiments do not test any other graph neural network architectures.Specifically, in Table 1 (synthetic experiments) the key result is that their tree-transformer outperforms seq-transformer on structured input. But seq-transformer is best on raw programs. I'm not sure what to make of this. But I wouldn't use tree-transformer in this problem. I'd use seq-transformer.In Table 2 (CoffeeScript-JavaScript experiments), no seq-transformer results are presented. That seems... suspicious. Did the authors try those experiments? What were the results? I'd definitely like to see them, or an explanation of why they're not shown. This paper tests whether tree-transformers are better than seq-transformer and other seq/tree models, but this experiment's results do not address that fully. Of the 8 tasks tested, tree-transformer is best on 5/8 while tree2tree is best on 3/8. In Table 3, there's definitely a moderate advantage to using tree-transformer over seq-transformer, but in 5/6 of the tasks tree-transformer is worse than other approaches. The authors write, "Transformer architectures in general, however, do not yet compete with state-of-the-art results.". Finally, no other graph neural network/message-passing/graph attention architectures are tested (eg. Li et al 2016 was cited but not tested, and Gilmer et al 2017 and Velikovi et al 2017 weren't cited or tested), but there's a reasonable chance they'd outperform the tree-transformer.So overall the results are intriguing, and I believe there's something potentially valuable here. But I'm not sure there's sufficient reason presented in the paper to use tree-transformer over seq-transformer or other seq/tree models. Also, while the basic idea is nice, as I understand it is restricted to trees, so other graphical structures wouldn't be handled. Update after Author Rebuttal--------------After reading the rebuttal, I'm pleased that the authors have made significant revisions, but I still think more work is needed. The "hard/soft" hybrid approach still lacks justification and perhaps wasn't compared to a soft/soft approach in a fair and fully-correct way (see detailed reply to authors). I also appreciate the efforts on revising clarity, but still find many clarity issues in the newest version that make the method hard to understand let alone reproduce. I thus stand by my rating of "borderline rejection" and urge the authors to prepare significant revisions for a future venue that avoid hybrids of hard/soft probabilities without justification.    ---------------------------------After reading the responses from authors, I have clearer noticed some important contributions in the proposed methods:1) A novel regularization function with a scaling factor was introduced for improving the capability of binary neural networks; 2) The proposed activation function can enhance the training procedure of BNNs effectively;3) Binary networks trained using the proposed method achieved the highest performance over the state-of-the-art methods.Thus, I think this is a nice work for improving performance of  binary neural networks, and some of techniques in this paper can be elegantly applied into any other approaches such as binary dictionary learning and binary projections. Therefore, I have increased my score. -------------------After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not.If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication.  [Post rebuttal]I would like to thank the authors for their clarifications. However, I am still concerned with the novelty. The absence of provable mixing rate is also a potential weakness. I think a clearer emphasis on the novelty, e.g. current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.  ** Post-rebuttalScore increased to a 7 following rebuttal and paper revision. Thank you for your explanations and the associated addition to your paper. This resolves most of the concerns I had noted in my initial review. The fact that LTVAE seems to learn a latent structure in which the Y_i are highly dependent on each other is quite an interesting point, and it indeed matches the intuition we can have about the data in general. I'm raising my rating following these improvements.I am however not really convinced by your approach to empirically compare the performance of LTVAE wrt an equivalent model where the Y_i are independent. The immediate reason is that your reported test log-likelihood (-120) is much worse than that of a regular VAE as per your paper (-84.9), which is quite surprising as the regular VAE would be a special case of both LTVAE and this independent approach. This leads me to believe that your approach of first training using LTVAE and then amputating it has an issue. This likely makes the model fall into some local minimum that it would not have reached if trained from scratch, especially given TLVAE seems to learn a structure where the latent variables are highly dependent. Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space. 1) With regards to transfer, it would be useful to see a comparison with a purely neural network baseline: one can imagine pre-training the neural network baseline on the go-to-pose task, and then fine-tune the network in the collect-wood task. This may be a fairer comparison.2) The authors reasoning makes sense, and I believe that this is a strength of the preceptor gradients framework.3) I see. Thank you for clarifying.7) Agreed, expressing the program at the right abstraction level would be crucial. Although writing a controller program whose inputs live in a relatively simpler abstraction level is easier, a major challenge I potentially see with how the perceptor gradients would scale. For example, in the Minecraft tasks, the outputs of the preceptor network are categorical variables over the agents x and y position. However, presumably for a task with this simplicity it may be possible to use conventional non-learning computer vision methods to obtain the x and y position. Beyond merely increasing the complexity of the preceptor network, there seems to be a conceptual issue: how would the perceptor gradients approach scale to scenarios where the state is not that easy to symbolically specify? This is presumably the motivation for learning from pixels in the first place. For example, one can imagine manipulating a non-convex object like a cup, or a soft object like a stuffed animal. Would the output of the perceptor network be in these cases, and would the output space of the perceptor network need to be custom-designed for each differing object geometry? I think the perceptor gradients approach is a good step towards learning systems that generalize better, but it seems that future work would need to address the challenge of scaling the approach to domains where the output space of the perceptor network is more complex than categorical positions.Overall, the revised version of the paper has addressed some of my concerns, although it still seems to me that more future work would need to be done with respect to point (7) for the perceptor gradients approach to have more impact. Despite these concerns, I believe the paper is a good first step towards tackling an ambitious goal, so I would recommend acceptance. ****Final comments after reading the response and the reviews:Regarding the fairness of the review, success rate is not sufficient to evaluate navigation agents. A random agent can achieve 100% success if it is given enough time. So it is totally fair to ask for a metric (such as SPL) that is a function of both success rate and episode length. I am going to increase the rating to 5 since some of my concerns have been addressed. There are still a number of issues:- The authors did not run the experiments with the termination action. I disagree that this is orthogonal to the focus of the paper. This is not just an additional action. It indicates whether the agent has learned anything or it is just a combination of better obstacle avoidance and luck. The SPL numbers are so low (maximum SPL is 6.19%) so adding the termination action will probably make the method similar to random.- There is a huge gap between success rate and SPL numbers. For instance, success rate is 66.4, while SPL is 5.84 (note that for some reason the SPL numbers are multiplied by 10 in the table). I doubt that the agent has learned anything meaningful in comparison to the baseline. I understand that the task is hard, but this gap is so huge.- A separate policy is trained for each sub-target. This doesn't scale. There should be one policy for all targets. Thank you for the answers to questions regarding the LadderVAE and the choice of prior distribution. The outcome of both the node perturbations and the linear conditional density experiment is in line with I expected. Overall, I the contribution of the work is bit clearer now and I'm raising my score to reflect this. The primary actionable recommendation I have is to add visualizations of the data under different node perturbations to the supplementary material for the best model learned on MNIST/Omniglot so that readers may have a better intuition for what low-frequency structural changes and high-frequency elements correspond to.  Thanks for your answer corrections, and additions to the paper. This mainly resolves the issues I noted. As such I see your contribution as an alternative parametrization compared to the FC-VAE which changes the optimization landscape, favoring more sparse latent graphs. This is worth of interest, and I'm raising my rating to reflect that.The comparison of performance however remains fragile in my opinion. Indeed, it is completely relevant to compare models changing only a single thing (here the latent structure and way it is trained). However different factors of change do not interact in ways that are easy to predict, and it is not obvious to me that applying the same change to the original LadderVAE would necessarily similarly improve its performance. Thank you for your further updates which I think improve the paper further.  I have consequently decided to increase my score to an 8.  One final question that would be good to answer in the camera ready if you can would be to try and see if you can establish what dependency structure (if any) for the generator is compatible will the learned encoder (i.e. is there any generative model for which the learned encoder structure is a faithful inverse).  This is not a critical point though and not something I would expect to be successfully addressed during the discussion period. EDIT: After the rebuttal, resulting in several changes and additions to the paper, I am changing my rating from 5 -&gt; 6. Thank you for these clarifications. You local stability analysis is interesting but what it says is that EMA does not change anything in term of second order stability (fortunately it is not worse, but it is also not better). Moreover, your simple bilinear model has pure imaginary eigenvalues and thus the eigenvalues of its Jacobian never lie into the unit ball (whatever the step-size).I'm not sure the discrete local stability analysis you provided is a good point in favor of EMA. (Look like it actually shows that your iterates cannot converge to the equilibrium in the bilinear case)However the experimental approach of this paper remains interesting as I developed it in my review. That is why I will not change my grade. Post-rebuttalThanks for addressing my questions. With the new comparisons and discussions wrt the most relevant methods, I believe the contributions of the paper are clearer. I'm revising my score from 6 to 7.  Besides many technical concepts are not really accurate on how they are presented. It needs further attention and improvements.The paper reads more like a review papers than a new research article. I remain to my initial decision. UPDATE:I read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong. =====I appreciate the authors' efforts to improve the paper. However, there is still substantial room for improvement in writing clarity. For example, the authors optimize the reverse KL from typical supervised learning, which makes even the title of the method confusing. The method that was experimentally evaluated can be derived more simply without the two-step procedure by directly taking the gradient and add the heuristically motivated per state indicator. This in itself is interesting and the authors demonstrate that it works well experimentally. I think the paper would be substantially more useful to the community if the authors focused on that contribution alone. As it stands now, I find the paper difficult to read because most of the theoretical results have no bearing on the method. After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ The rebutal and the revision of the paper solve my comments.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ After rebuttal: I'm also still not convinced by the experimental evaluation. For this reason, I slightly downgraded my overall rating. --------------------------------------------------------------------------------Post-rebuttal update: Based on the improvements made during the rebuttal period, I have raised my rating. I believe Fig. 2 now makes the contribution of this paper more clear when compared to the conventional binary network. According to Fig. 2, the proposed method in this paper allows to multiplex and select binary inputs rather than performing XNOR operations, which makes the novelty of this paper rather limited and incremental. Therefore, I still believe this paper stands below the acceptance threshold. # Changes after author responseThanks for addressing the concerns in the review with the new revision. I am revising the score to 7 from 5 based on the reply and the revisions to the paper.--- ######### After author response #########I thank the authors for their responses and updates to the manuscript. While I still feel that the connections between optimization instability and the observed phenomena could be developed further, the updates strengthen the paper and the experimental results are extremely interesting. I have increased my score for these reasons. ** after rebuttal: thanks for the efforts in updating the paper. I will stick with my score. --- POST REBUTTAL ---Modified my score after the rebuttal, since (1) I believe the re-purposing achieved by this work can potentially broaden the applicability of flow-based generative model (2) the authors have toned down the abstract and clarified the contributions in the intro, which now better reflects the value of the work. I am still leaning towards rejection at the end given the limited originality of the proposed method and the lack of a more comprehensive discussion of different possible approaches, but as means to the same end. For example, the relaxed inference problem can be solved with an MCMC method. These should all be discussed and compared if the contribution is about repurposing a joint likelihood model using flows. PS. the last line (the references) of the last page might have been a mistake.  ### Update after the rebuttalI think that the ideas in this paper are interesting and can inspire new uses. All of us agree that the problem presented here was important, and there is a lot of work to be done in this domain. However, after reading the discussion with other reviewers and their reviews, I believe the manuscript can benefit from another review round. Specifically, the authors can benefit from a thorough revision of the claims in the paper. Further, I would encourage authors to at least investigate how naive amortization approaches fair (irrespective of the result, authors will develop a stronger case for their line of work.)  ### UpdateThe authors have addressed most of the above criticisms. Even though part of the definite answer about directly extracting the posterior from the MI network is left to future work and no test of their method on a high-d example was included, I believe that the paper is a valuable addition to the literature. I have increased my score accordingly to acknowledge the authors' replies. ========== Post-Rebuttal ==========Concerns on paper/results clarity still persist. Lowering my rating to 5. --------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:I would like to thank the authors for answering the questions. I believe that an updated version addressing all the concerns in detail will find its place in other future conferences. Original rating is maintained. =============After rebuttal:Thank you for improving the clarity. Unfortunately, the following issues are still unresolved to me:- Figure 1: this geometrical argument seems to be at the core of Eq. 1 but I still have a hard time understanding it. You might want to formalize the argument in text.- Experiments are not convincing. Even the original MNIST is not a representative dataset for optimization methods. The theoretical contributions of this work is not enough to justify having limited experiments.- The rebuttal says: "The hyperparameter k in DCL is tuned by trial and error on the test set.". Does that mean mistakenly using the test set as the validation set? === After rebuttal ===Thank you for your answers. I'm keeping the rating at 3.1)+2). I'm still not convinced that it's fair to claim an improvement of X% for a curriculum that, relying on final weights of a "vanilla" SGD-trainedmodel, converges in _additional_ X% to the 100% of "vanilla" time.3). Fair enough, but the revised draft still reads like examples are sampled from it.I double checked the context of citing (Graves et al, 2017) and believe it's still imprecise as in the original draft. -------------------------------Post-rebuttal comments-------------------------------Thank you for taking the time to revise your submission. I will maintain my original score of 4. The main justification for this is that the two main weaknesses I see in this paper (the first two I list in the original review) remain unresolved, and it indeed is unclear whether the second one could feasibly be addressed without significant changes to the core methodology currently proposed. -------------------------------Post-rebuttal-------------------------------Thank you for revising the submission and the clarification in the rebuttal. After reading the rebuttal and other reviews, my main concerns about the novelty and computation cost are still unsolved. Therefore, I will keep my original score. ## After revisionThank you for answering my question!> Our approach can be directly used to fine-tune pre-trained language models if word level tokenization is used.It should be acknowledged that no multilingual word-based BERT exists as far as I know, partly due to the challenge with large vocabulary space and generalization. The sub-word approach you mentioned is a good proposal, but without evidence, I cannot assess whether it works. As a result, it's still a limitation of *this paper*, and should be clearly discussed in the paper.> The complexity of the proposed method.I am referring to the complexity w.r.t vocabulary size. The softmax normalization contains `nV` items, where `n` is the number of sense clusters and `V` is the vocabulary size. With the current limitation of word-based models, the vocabulary is already larger than sub-word models, the extra `n` factor cannot be disregarded.> We train bilingual language models without sense-level translation loss (and without projection), denoted by Bi-SaELMo-NT, for ablation study.Thank you for the ablation study! However, I cannot assess it as support for the claim as it's not an apple-to-apple comparison, and I cannot evaluate this paper based on future projection. As a result, I have to maintain my rating based on the revision. ======================================================================**Update:**I would like to thank the authors for their response. The lack of adverse impact of the proposed approach on monolingual tasks and the described ablations certainly help strengthen things on the experimental side. However, my fundamental concerns still remain:* I'm unsure why the ELMo baseline is so much worse than that reported in literature.* With respect to the difficulty of training BERT, I can certainly empathize with the authors about the limited resources available in an academic setting. However, given that BERT was a key area of focus of the paper's methods section, showing the experimental results for BERT, even if BERT-tiny; note that BERT-tiny has just 2 layers and 128 hidden units, as opposed to 4 layers and 512 hidden units which this work uses (based on Section 4.1 and Table 7-- which corresponds to BERT-small, refer [here](https://github.com/google-research/bert#bert)), which should help reduce computational burden by quite a bit.On account of this, I maintain my original rating of 5. **After Rebuttal**I have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami. I understand that the authors have taken a pessimistic approach to compute the lower bounds of risk in the minimax setup of meta-learning. However, I believe the paper is an important step in this direction. Theorem 3 and Theorem 4 are important additions to the paper, which show a margin between the pessimistic lower bound of the risk and the actual risk with the introduction of structure to the learning setup. Overall,  I enjoyed reading the paper, and I have increased my score after the rebuttal. ######################################################################Post-Rebuttal Updates:6. Thank you for running the experiments on small datasets and for clarifying my additional concerns.  In light of your clarifications, I do find this work to be a practical extension of Shampoo and am in favor of accepting the work.  I am still surprised by the fact that delayed pre-conditioner computation still leads to improved convergence over 1st order methods, but the presented empirical evidence demonstrates a consistent benefit in wall time across a number of settings.  ################################## Updates: I have changed my score after reading the author's responses. **Post Rebuttal**Thanks for the response. I still didn't get an answer for why the second order term and not the first order term is of interest in the Taylor expansion. Also the paper does not seem to be updated with the promised changes, particularly error bars. So I am leaving my score as is.  Edit after Rebuttal/Response Period:----First, an apology to the authors that a dialogue did not occur during the response period; the authors response was prompt, and my (R4) response was not, thus they were not given an opportunity to respond this response to their response to the review (...the number of recurrences may indicate why this was not possible, given limited reviewer time resources).I think the authors misunderstand my questions about the nature of the uncertainty they're capturing:  Is there intrinsic uncertainty in the observed phenomena (e.g. medical images of tissues), are we capturing mixture proportions of deterministic states which have been mixed due to quantization, OR is the uncertainty due to the raters, i.e. found in the labels ONLY due to differences in label generation?  OR, a third case, is this moot because it does not change the outcome?I understand that there is no explicit modelling of raters. However, my concern was that what we are capturing is intrinsically the uncertainty due to raters, even though no actual rater indicator variable was provided. This would be analogous to learning, unsupervised, the writers of the various MNISTs digits. While for MNIST this is surely difficult due to the number of writers (and their anonymity), for medical images we will likely have a limited number of raters. Having the posterior code collapse to a rater indicator appears problematic, not a desirable outcome, and likely if any one rater has correlated outputs across samples (which seems reasonable; some raters may be more or less conservative with their tissue labeling, boundaries, better/more careful at delineating curves etc). What prevents the capture of this signal, or is this the actual variation we intend to capture in the first place?R3 further included this interesting question in their initial review:> In the shown samples, for a given input, many of the different outputs seem very similar and could be considered from the same mode. This can potentially make interpreting the probabilities more difficult than claimed in the paper, especially since the model is trained with a large number of codes. A very plausible scenario could be that one of the most likely modes is split between multiple low probability outputs and thus doesn't show up on the top ouput [sic]. Can the authors comment on this potential issue?If outputs are correlated i.e. overlapping in the original image domain, should the measured uncertainty be aggregated across codes?I disagree with the characterization of the Gaussian VAE calibration in the "Ranking Probabilistic U-NET" response section. Simply because it does not accurately fit the function (one mode vs. many) does not mean we can't evaluate the learned unimodal beliefs. Yes, it is misspecified. Does this mean the rankings are meaningless? Surely the discrete model is misspecified (there are more possible masks than codes), but the ranking is claimed to be meaningful. The discrete model may have a better fit, and make the argument that it is better specified, but this doesn't mean you _can't_ evaluate the Gaussian VAE.I stand by my initial rating and reasoning, though I note to the AC that, given space, this could make an acceptable poster. It is, in my opinion and in gross summation, an improvement on the Prob. U-Net by way of improving the Gaussian VAE sub-model of the Prob. U-Net to the VQ-VAE. This allows for sampling from a discrete set of codes which hopefully correspond with modes of the generating distribution in the data domain, instead of sampling from a parametric density, which, while continuous, has only one local maximum. After reading the authors' response, I'm revising my score upwards from 5 to 6. UPDATE:I think the rebuttal addresses some of my concerns. I am especially glad to see improvement on en-fr, too. Thus I raised my score from 5 to 7. Thanks for updating the paper and addressing my concerns, mainly regarding the novelty. I will change my rating accordingly.  Response to rebuttal:The authors have addressed my question about the weights being still in the same RKHS. I still think the motivation and experiments are not very satisfactory. Therefore the paper is very borderline. However, I would like to bump my rating a bit higher. Update on Nov 30:I updated my score to 6 and my confidence level to 3. [Post-rebuttal update] No author response was provided to address the reviewer comments. In particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern. I've left my overall score for the paper unchanged.    xxxxxxxxxxxxxxWhile I appreciate the authors' rebuttal and revisions, I still do not see sufficient contribution here worthy of a regular ICLR paper. Addendum:  You know, what I really love about ICLR is the effort authors make to refresh their paper and respond to reviewers.  You guys did a great job.  Really impressed.  50x2D now clarified and some of the hasty/unexplained bits fixed. AFTER READING REBUTTAL I've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. I strongly agree with authors when they state: "We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings". I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.  == I think the authors' response is reasonable. They have added clarifying material to the paper addressing my concerns. I have raised my rating from a 5 to a 6. =================================EDIT: I confirmed the revisions regarding the notation issues, but there still have confusing parts.* Definitions of norm operator \| \| is unclear.  * L_1 is mentioned below (1), and used other parts (3) or Algorithm 1. Equation (12) in Appendix uses |W1|_1^2, which looks like the l1 norm as well. Use consistent notations.  * Equations (12, 13, 14) uses \|\|_2 or \|\|_1 to specify the type of norm, whereas (5), (6), (7) and other parts after (15) use \|\|. This confuses me. What do you mean by \|\| without subscript?  * \|\| operator taking to symbols is a weird notation for me. Usually, norm is defined for a single vector (or a matrix). For example in (5), I would write \| b - G(z_b) \|, if you want to measure the difference between b and G(z_b).The experimental result is impressive, as the other reviewers mention. I strongly recommend clarifying the notation to better deliver the method. ============================I think the reviewer addressed my questions and concerns in the rebuttal, so I raised my rating to 6. [Edit: as mentioned by the other reviewers, this extension isn't as novel, given Ha et al's work, hence that reduces my confidence about accepting this paper further...] Update after author response: I am changing my rating from 4 to 6 in light of the clarification and new experiments.------- Updated rating after author response from 8 to 7 because I agree that Figure 1 and some discussions were confusing in the original manuscript.-------------------------------------------------------------------------- After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. ##### Update:I appreciate the clarifications and the extension of the paper in response to the reviews. I think it made the work stronger. The results in the newly added section are interesting and actually suggest that by putting more effort into training set design/augmentation, one could further robustify the agents, possibly up to the point where they do not break at all in unnatural ways. It is a pity the authors have not pushed the work to this point (and therefore the paper is not as great as it could be), but still I think it is a good paper that can be published.----- I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter "drop rate" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. ====== ----------Apart from one concern (refer to comment), the authors have responded to most of my other comments. Based on this, I think the paper does offer an interesting analysis of the STE approach used for training binary networks, hence I'm increasing my score. ------------------------After reading the author response, they have sufficiently addressed my main concerns. I think that this is a good paper that will be of interest to those concerned with understanding the training of activation-quantized / hard-threshold neural networks. I have thus increased my score. ------------------------The author response have addressed most of my concerns. Thus I have increased my score. ---After paper revisions: Thank you for the updates. The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community. ~~~~~After Rebuttal~~~~~~The rebuttal still cannot justified such a random deep prior well. I keep my rating unchanged.  -----------------------------------------------------------------------The authors have addressed my comments about other deep generative models and hyperparameter sensitivity. However, I still think the paper is more suitable for other venues with readers from the neuroscience community. Hence, I change my rating to 5. === post-rebuttal comments ==I maintain my pre-rebuttal rating. Although the authors address my concerns reasonably well in the rebuttal, they are mostly not reflected in the paper revision (particularly on the choice of using SDN, which is quite an important piece of information for readers). There were no attempts to make a comparison with Seesaw loss, evaluating on LVIS, and releasing code anonymously (as other ICLR submissions did). Updates after discussion/revision period:It appears that the paper has improved. However, the changes appear to be so substantial that the paper is now essentially a different paper which would require a new review process. --------- I have read the reviews of others and the author's response. My main impression of the work remains as it was: that it is  nice idea with small but significant empirical success. However, my acquaintance with the previous literature in this subject is partial compared to the acquaintance of other reviewers, so It may well be possible that they are in a better position than me to see the incremental nature of the proposed work. I therefore reduce the rating a bit, to become closer to the consensus. Opinion post rebuttals:After reading the rebuttal and the other reviews, I remain of the opinion that more work is needed before warranting acceptance. This paper indeed learns prototypes on the fly in contrast to e.g. Mettes et al. 2019. That method was however not designed for hierarchical knowledge, while several recent CVPR papers were. Since the novelty over these papers is limited and direct comparisons are lacking, more research is needed. Post rebuttal update ===Thanks to the addition of better baselines, I've increased my score for this paper. While I'm still not super convinced of its potential for application, I find the idea original and worth discussing at the conference. Thank you for addressing all the comments.- I am satisfied with the explanation from the authors regarding Theorem D.4 and the revision adequately addresses most of the comments.- Regarding differentiability, it is fine to retain the original definition of differentiable games while your result requiring thrice differentiable losses. However, the justification you provided in your response needs to be added in to the paper and preferably as a note immediately after Definition 1 to avoid the misinterpretation. - Another question Lemma D.7 - Towards the end of proof, the application of Cauchy Schwartz is not clear. You show that ||-\alphaX(\theta)|| = \alpha^2||X(\theta)|| < c. However, it is not clear how the equation below that holds? Specifically, why does the negative sign in the first fraction disappear and somehow the overall term becomes >= \alpha||\Psi_0||/||-\alphaX||.It is recommended that the authors proofread all the proofs and equations and try to use notations and show derivations without making them confusing for the overall presentation. It would also help to number equations for quick reference.Overall the paper presents strong theoretical results with adequate empirical evidence. It certainly addresses an important problem of trade-off between convergence and stability in multi-objective settings and I have updated my score from 6 to 8 to strongly support it for acceptance. After reading the authors feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? This paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss?After reading the authors feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? This paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss? -----------------------------------------------After rebuttal: after reading the answers, I got answers to most of my questions. Some parts of the paper are vague that I see that other reviewers had the same questions. Given the amount of change required to address these modifications, I am not sure about the quality of the final work, so I keep my score the same.  == I have read the rebuttal. Thanks for the response. ==== After reading the revisionThe revised version is indeed more clear about how the teacher network works, and I have tried to understand the later parts of the paper again. The result of the paper really relies on the two assumptions in Theorem 2. Of the two assumptions, the first one seems to be intuitive (and it is OK although exact conditional independence might be slightly strong). The second assumption is very unclear though as it is not an assumption that is purely about the model/teacher network (which are the x and z variables), it also has to do with the learning algorithm/student network (f's and g's). It is much harder to reason about the behavior of an algorithm on a particular model and directly making an assumption about that in some sense hides the problem. The paper mentioned that the condition is true if z is fine-grained, but this is very vague - it is definitely true if z is super fine-grained to satisfy the assumption in Theorem 3, but that is too extreme.Overall I still feel the paper is a bit confusing and it would benefit from having a more concrete example. I like the direction of the work but I can't recommend for recommendation at this stage. # UpdateThe submission has been drastically rewritten (the diff is massive) and I think it is in much better shape, answering some of my concerns around reproducibility and generalization. Furthermore, it reinforces the strengths of the approach (esp. around its flexibility).I am willing to recommend acceptance, but I have some further questions (hence I have only updated my score to a 6 for now). They are mostly related to the comparison with IL (important to validate the claim in the paper that the proposed approach is quantitatively better than both existing IL and RL methods). See discussion below for details. REVISIONBased on the authors' responses, I withdraw points 3-5 from my original review. Thanks to the authors for the additional experiments. On (3), I indeed misunderstood where the moving average was being applied; thanks for the correction. On (4), the additional experiment using the global mean features for real data convinces me that the method does not rely on the stochasticity of the estimates. (Though, given that the global mean works just as well, it seems like it would be more efficient and arguably cleaner to simply have that be the main method. But this isn't a major issue.) On (5), I misread the learning rate specified for "using the autoencoder features" as being the learning rate for autoencoder *pretraining*; thanks for the correction. The added results in Appendix 11 do show that the pretrained decoder on its own does not produce good samples.My biggest remaining concerns are with points (2) and (6) from my original review.On (2), I did realize that features from multiple layers are used, but this doesn't theoretically prevent the generator from achieving the global minimum of the objective by producing a single image whose features are the mean of the features in the dataset. That being said, the paper shows that this doesn't tend to happen in practice with existing classifiers, which is an interesting empirical contribution. (It would be nice to also see ablation studies on this point, showing the results of training against features from single layers across the network.)On (6), I'm still unconvinced that making use of ImageNet classifiers isn't providing something like a conditional training signal, and that using such classifiers isn't a bit of an "unfair advantage" vs. other methods when the metrics themselves are based on an ImageNet classifier. I realize that ImageNet and CIFAR have different label sets, but most if not all of the CIFAR classes are nonetheless represented -- in a finer-grained way -- in ImageNet. If ImageNet and CIFAR were really completely unrelated, an ImageNet classifier could not be used as an evaluation metric for CIFAR generators. (And yes, I saw the CelebA results, but for this dataset there's no quantitative comparison with prior work, and qualitatively, if the results are as good as or better than the 3 year old DCGAN results, I can't tell.)On the other hand, given that the approach relies on these classifiers, I don't have a good suggestion for how to control for this and make the comparison with prior work completely fair. Still, it would be nice to see acknowledgment and discussion of this caveat in a future revision of the paper.Overall, given that most of my concerns have been addressed with additional experiments and clarification, and that the paper is well-written and has some interesting results from its relatively simple approach, I've raised my rating to above acceptance threshold. update: the authors have assured me in comments that the model is trained end to end - changing rating to good.. Thanks for the detailed responses. After reading the author response and the updated paper, I am satisfied on several of my concerns, many of which were due to the writing in the earlier submission. The updated results on various comparisons are also good.  I have updated my score accordingly. Some qualitative analysis of the results would have been nice -- examples of protein pairs where they do well and other methods have difficulty as they don't use the structural similarity info / global sequence info used by this paper. But maybe those can be in a journal submission.My only remaining concern is on the lack of reporting average performance on the test data (which used to be the norm until recently for papers submitted to ML conferences). ########## Updated Review ##########The author(s) have presented a very good rebuttal, and I am impressed. My concerns have been addressed and my confusions have been clarified. To reflect this, I am raising my points to 8. It is a good paper, job well done. I enthusiastically recommend acceptance. ################################ Score updated after reading authors' response. Updated score from 6 to 7 after the authors addressed my comments below. The authors' response clarify the difference between this work and the Natasha paper. My concern is addressed. post-rebuttal======thanks for the feedback, I update my rating of the paper I am glad to see that the authors have fixed a number of the issues discussed in my review. Per these improvements in the draft, I am increasing my score from 5 to 6.  ----Post-rebuttal review:I appreciate the authors' efforts in including the new experiments in Sections 4.4 and 4.5. In my opinion, these new results and the discussion in Section 5.2 add great values to this work and make the contributions of this paper substantially clear. I've increased my rating to 6. Post Rebuttal: The draft paper improves on the original paper and demonstrates possible concealment of the program. I adjusted my rating upward to 8. PS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve &gt;96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score. * Update:Thanks for you answer and clarification. While the Morph-net appears novel, the authors only report result for image classification task and don't achieve as good performance as standard convolutional baselines. Given the current empirical evaluation, I find hard to assess how significant is the contribution. I would encourage the authors to either compare on a task where dense networks achieve state-of-art performances or extend their approach to 2D inputs. My apologies for the delay. I'd like to acknowledge that I had a few misunderstandings that were clarified by the other reviews, the rebuttal and a few more passes through the latest manuscript -- thank you for that. I have updating my score to reflect this. I agree with R2 that the approach is novel but I am not yet convinced it is a good paper. There are a few axes of variation important to consider for proposed solutions to this problem:(a) Different kinds of missingness:MAR, MNAR, MCAR are three of the common kinds of missingness enountered in data. Currently, the manuscript does not experiment with the latter two kinds of missingness. The paper studies the MCAR case in Table 1. I'm left wondering what the table of results would look like when stratified by different kinds of missingness. Furthermore, for imputation with the goal of downstream classification, not all features of the data are created equal and only a small handful of them may be relevant for the classification task. How well does the proposed method reconstruct the most predictive subset of features (selected via some simple method such as L1 regularized regression)? (b) Multi-modality in the observed features:I think the paper does do a good job at tackling this axis of the problem by capturing multi-modality in the latent variable. This is where I see the utility of the qualitative experiments in image inpainting and the comparison to the Universal Marginalizer.(c) Computational complexity vs accuracy tradeoff:As pointed by external readers, there certainly are other deep generative model based methods for imputation. The authors point out the alternatives are slower *at test time*. This is true but it is also important to note that it is so because this approach amortizes that cost during *training* by attempting to condition on all possible missingness masks. It still remains to be seen whether this approach yields more accurate results than the other slower approaches.For reasons (a) and (c), I think this paper is borderline. Revision----------Thanks for taking the comments on board. I like the paper, before and after, and so do the other reviewers. Some video results might prove more valuable to follow than the tiny figures in the paper and supplementary. Adding notes on limitations is helpful to understand future extensions. ============= After Reading Response from Authors ====================The reviewer would like to thank the authors for their response. However, the reviewer is not convinced by the authors argument. The target NRF model, the generator and the sampler are all different.It is understandable that modeling continuous data can be substantially different from modeling discrete data. Therefore, it is non-surprising that the problem formulations are different.As for SGLD/SGHMC and the corresponding asymptotic theoretical guarantees, this reviewer agrees with reviewer 2s perspective that it is a contribution made by this paper. But this reviewer is not sure whether such a contribution is substantial enough to motivate acceptance.The explanation for better mode exploration of the proposed method given by the authors are the sentences from the original paper. The reviewer is aware of this part of the paper but unconvinced.In terms of experiments, sample generation quality seems to be marginally better. Performances in multiple learning settings are comparable to existing methods.A general advice on future revision of this paper is to be more focus, concrete, and elaborative about the major contribution of the paper. The current paper aims at claiming many contributions under many settings. But the reviewer did not find any of them substantial enough. Post rebuttal===========================I thank the authors for their responses and additional experiments. I understand that the focus of the paper is the cross-silo setting, however one of the key questions of an empirical study is to identify the limitations of the proposed approach. Experiments in Tables 8 and 9 still consider a relatively small number of clients and do not provide empirical insights into when the proposed approach begins to degrade. For future revisions, I recommend an empirical exploration that helps the reader to understand the limitations of the proposed method. I have read author response and thank authors for their response. I have decided to keep my initial rating. I am not convinced by the response that the proposed method has a clear advantage over Fixmatch as some of their reported numbers for Fixmatch are very different (worse) from the ones from the original paper. Edit: I have read the changes and the author responsed, but have decided the keep the score. **Update after authors' response**I want to thank the authors for their response, and I am happy to see additional results on shared sets of weight values (which allows to easily relate the work to methods for training low-bit-width networks). To further increase impact and significance of the work it would be necessary to really flesh out the advantage of the proposed method over other, similar methods ("it is not too surprising that the method works, but why would I prefer it over other methods?"). Nonetheless, the paper presents novel empirical analysis that adds to the body of work on non-standard training of neural networks. To make a clear stance for the reviewer discussion I have therefore increased my score to 7, though I would rate the paper at the lower end of score-7 papers. --- ### Update after author replies and discussionI have updated the review score after reading the authors' reply and revision of the paper. Regarding the author responses, I have updated my rating. __________Update after the discussions:I would like to thank the author(s) for all their comments. Although most of my concerns have been addressed, some questions remain topics of contention. Before discussing these topics, I will first append to this review my answer to the author(s)' last comments, as it was their wish to keep hearing from me after closing of the discussions:  $\ \ \ $ The assumptions (3.1, 4.1, 5.1, 5.3) made on the function sequence $f_t$ for convergence of the proposed algorithm are unconventional as they require that the expected absolute variations of two function values at the points visited by the algorithm be bounded, or that the squared variations of two function values obtained by Gaussian sampling from points visited by the algorithm be bounded. So formulated, the conditions for convergence involve the algorithm's trajectory $x_t$ as much as the function sequence $f_t$, and they are difficult to verify. In an attempt to identify sufficient conditions for these assumptions to hold true, I made three suggestions: (i) and (ii) were concerned with the boundedness of the sequence of points generated by the algorithm, and (iii) was the case of bounded incremental variations of the sequence $f_t$, e.g. martingales. In their reply, the author(s) were right to rule out (i) and (ii), which indeed were unrelated. This leaves us with (iii) as a possible setting for the proposed algorithm.  $\ \ \ $ In my last comment I argued that the case (iii), where the sequence $f_t$ undergoes incremental variations uniformly bounded in expectation, was covered by the approach taken in Bach & Perchet (2016), where two function queries obtained from perturbations around the same iterate are processed at each step. The Bach/Perchet approach is cited in the paper for comparison, but it is called impractical as it would not apply when $f_t$ varies over time $-$ argument I disagree with and that I attempted to refute in a brief discussion involving martingale-like variations for $f_t$. When the author(s) of the submission object to my regret analysis in the case of martingale-like noise on the basis that the assumptions they make also cover non-zero-mean variations with similar uniform upper bounds on the moments, they do not address the main point of my comment. My intention was to show that it does not take much effort to consider the approach used in Bach & Perchet (2016) in settings where the cost function is changing over time, for as long as the cost variations are incremental with bounded moments. This can be seen by noting that the convergence result derived in the revised version of the supplementary material for the residual-feedback algorithm with unit-sphere sampling can be reproduced for the Bach/Perchet approach under the considered assumptions. I take it that the author(s), who excel at deriving the convergence rates for such algorithms, will not disagree. Although the assumptions used in Bach & Perchet (2016) (uniform zero-mean increments) may look somewhat stricter, they have the merit of being clear and simple, as opposed to Assumptions 3.1, 4.1, 5.1, 5.3, which involve the trajectory of the algorithm and can't be verified easily. They are also sufficient to improve the convergence rates for higher degrees of smoothness compared to the early algorithm by Flaxman et al. (2005), which was the objective of that paper. Higher degrees of smoothness failing which it is difficult to improve the convergence rates, as confirmed by the convergence rates given in the submission. In my sense, one important message conveyed by the submission is that the approaches proposed in the submission and in Bach & Perchet (2016) can both handle bounded additive noise, and both fail in the more general framework of adversarial learning. By calling the Bach/Perchet algorithm impractical for their setting, I believe the author(s) of the submission missed to chance to compare the two approaches from a fair perspective and to answer the simple question that comes to mind when reading their paper: is the residual feedback technique really useful in the stochastic learning framework, or isn't convergence just as fast when the function queries are processed by pairs as in Bach & Perchet (2016), or in the reference paper by Nesterov & Spokoiny (2017) ?-------That being said, the following issues remain in this submission:$\bullet$ The assumptions (3.1, 4.1, 5.1, 5.3) made on the function sequence $f_t$ for convergence of the proposed algorithm are unconventional and difficult to verify, because they consist in properties of the iterates of the algorithm.$\bullet$ In our discussions, only incremental sequences $f_t$ with variations uniformly bounded in expectation have been identified to meet those assumptions. In my sense, this particular setting is also covered by the approach taken in Bach & Perchet (2016), where the function queries are handled by pairs obtained from perturbations around the same iterate. Also, I still find it unfair to call the latter approach impractical for the considered setting.$\bullet$ Since the convergence rates derived in the paper show no clear improvement, compared to the early approach of Flaxman et al. (2005), the arguments of the submission lie in the experimental results, where I don't think the algorithm by Bach & Perchet (2016) is given a fair treatment (for the reason explained in the previous paragraph). Besides, the application considered in Section 6.1 reduces to the unconstrained minimization of a polynomial of high degree that is neither Lipschitz nor smooth, which is a basic requirement for the convergence algorithms. This makes the convergence of the algorithms highly dependent on the initial point, unless optimization is done over a compact set, but I don't think the projection step was implemented for the algorithms.$\bullet$ In constrained optimization, the problem that the proposed algorithm samples function values outside the feasible set has been partly addressed by the author(s), who provided a variant of the algorithm based no longer on Gaussian sampling, but on sampling over a sphere. Partly because only one convergence result for a particular setting was derived, and it remains unclear (as pointed out by Reviewer 4) if all the benefits of Gaussian smoothing and all convergence results would also extend to spheric smoothing. This discussion is missing. In my opinion, the extension to settings where the functions can't be sampled outside the feasible set is not absolutely imperative in all frameworks (the author(s) have provided counter-examples), but it would be useful to know the limits of the proposed technique. All things considered, I would not recommend the submission for presentation at the conference. Independently of the final decision, I hope the author(s) will make the most from the discussions with all the reviewers.I would like to make a last comment about the submission and the discussions that followed. It is natural that the author(s) give the best picture of the algorithm they propose. Yet in the paper the contrast is particularly strong between, on the one hand, the haziness surrounding the assumptions made on the function sequence $f_t$, or the negligence with which the algorithms were applied in Section 6.1 to a problem not actually meeting the conditions for convergence, and on the other hand the severity with which the Bach/Perchet approach was disqualified as a possible method of solution. This contrast gives the reader an overall feeling of partiality, which makes the reviewing task an intricate, contradictory, and unappreciative one. -----Update after rebuttal:I appreciate the detailed answers to my questions and the authors' revisions. I also read the other reviewers' comments. While the new assumptions address my initial concerns, the new versions depend on the algorithm being implemented. As far as I can tell the assumptions might be satisfied for one choice of step-size while not being satisfied for another. Also, I agree with the other reviewers that generally in OCO one considers a worst case sequence of functions. A discussion of this issue in the main body of the paper seems appropriate.After addressing these issues, I think this work would warrant acceptance to ICLR. For now, however, I am not changing my score. ####################  After Rebuttal  #################### I thank the authors for responding to the comments and have read them carefully. The authors have addressed my concerns in the rebuttal. Updates:Thanks for the authors' response and the revisions. However, the paper still lacks strong and significant results. Therefore, I keep my previous rating.  ------After rebuttal: Thank the authors for the response. I think my original concern largely stands and would like to keep my original evaluation of the paper. **Update: As noted elsewhere in the discussion, the authors have addressed my primary concern. I will therefore increase my score from a 6 to a 7.** ----I have read the response and am keeping my score. I agree that the simplicity of the results/model is valuable, but additional theoretical results (even extensions to Theorem 1, with more involved but stronger claims) would greatly improve the paper and make its contributions closer to what is expected of a ICLR submission. Extending the result to consider noise should be straightforward and yield a reasonably simple claim, which can be further verified empirically by adding synthetic noise to the gradients, adding label noise, and/or adopting extremely small batch sizes. As it stands now, the submission is still lacking in terms of contributions. Edit: I have read the other reviews as well as all author responses. The other reviewers noted meaningful concerns, but I believe the authors have clearly addressed most of these points. I still believe this work is an "accept". -------Post Rebuttal-------Thanks for the feedback from the authors. I am glad to see that the new theorem (Theorem 2) has successfully tackle the problem about the prior knowledge of the variation budget. I have updated the score accordingly. UPDATE:The authors have consistently improved their argumentation over the course of the review and addressed my concerns sufficiently. I have increased my score and recommend acceptance. Following the rebuttal:I checked comments of other reviewers and response of authors. Most of my questions were addressed in these responses. Therefore, I improve my rating. -----EDIT:I thank the authors for their detailed response. I also appreciate the effort that's been put into refining the draft and undertaking the additional experiments. Most of the points I've raised have been addressed. The manuscript has been made more self-contained. Additional material on autoregressive models suitable for channel-wise quantization and the inductive bias of ordered representations is especially useful. As a result, I've increased my score.Unfortunately, I don't feel my concerns re. paper's relationship with Rippel et. al's work have been fully addressed. It is still my understanding that the role of the analysis in 3.1 is to add rigour to the otherwise similar general argument in 2.1 of Rippel et. al. While more rigour is always good, in this case it doesn't lead to new insights, and feels tangential to the rest of the paper. As such 3.1 could be moved to the appendix, which will also help making the page limit.In addition, I am still not fully happy with how Rippel et. al's work is discussed. A reader not fully familiar with the earlier work might get an impression that it's simply a worse version of the proposed method. It's important to make it clear that the considered method extends the work by Rippel et. al's, applying nested dropout to an autoencoder that is discrete and "variational" (arguable), and considering a different application. This would not diminish the importance of this work, but would give credit where it's due.Overall, following author's response I am leaning towards acceptance, but will let the AC judge the importance of the points above for the final decision. Update - Thank you for the response and updates to the paper After RebuttalThank you for your detailed response, and I will keep my positive score. .Update: Thanks to the additional experiment and discussions, I have increased my score --Post-rebuttal--I have increased my score in response to the authors' clarifications and additional experiments and updates added to the paper. POST REVISIONFollowing our discussions and taking the changes made to the manuscript into account, I have decided to increase my score and recommend acceptance. My concerns have been adequately addressed. I believe the paper has been clarified, results are more carefully evaluated and claims are sound. I believe that this work consitutes a well-selected application that addresses a relevant research question with important clinical implications. In my opinion, this deserves to be aknowledged and may be of interest to others in the ICLR audience. My hope is that this application stimulates more work in EEG-based machine learning and also encourages others not to shy away from difficult application domains such as psychiatry, where we really are in need of new solutions to old and - unfortunatley - quite persistent problems.Thank you very much for your hard work!--------------------------------------------------------------------------- ---- Post-rebuttal comments----The rebuttal and the paper revision address my concerns. I keep my original rating. After the rebuttalI thank the authors for the detailed feedback. I am a bit with AnonReviewer3 about the concerns on the descriptive languages of the paper "less nonlinear", "more expressiveness". Moreover, the behaviors of different nonlinearities in the Fig.3 and Fig.17 are related to the specific initialization and batch normalization, which is rather not a global view of the landscape. I would keep the score unchanged. -- EDIT: Thanks for the nice feedback during the rebuttal. I am happy to stay with my rating of clear acceptance. Post Rebuttal Update:I appreciate the author response, but I will maintain my score after reading the rebuttal and discussion with other reviewers. It still appears to me that the motivation and clarity can be improved, and so I would recommend focusing on those aspects in future revisions. Additionally, baselines such as "allocating a region for every single test point" should be compared to in a clear way (as opposed to being in the appendix), as such baselines seem natural to compare to. EDIT: Thanks for the clarifications. Unfortunately, none of the responses are enough for me to update my rating.One thing regarding point 1 in particular: the transductive setting seems contrived for adversarial robustness as it does not seem to correspond to a plausible threat model. It's true that in the transductive setting, the examples don't have labels, but since clean accuracy >> robust accuracy, just caching predicted labels on the clean examples is roughly as good (which can be done even if test labels are not available). ===================================================I have read the author response, and I have to downgrade my score to 3 because of the validity of experiments. A minor issue is that the theoretical result is not impressive since the technique is basically the same as ICML'20 paper, except that expectation is taken here which makes things easier.My major concern is about the experimental results. The additionally provided Table 3 makes me question the implementation of the experiments. For example, a=0.005 should be very close to the true label (by Definition 3), but the accuracy is only around 70%. For LeNet on SVHN dataset, the accuracy of training with true label y should approach 90%, e.g. see the report in [Coverage Testing of Deep Learning Models using DatasetCharacterization, 2019]. Indeed, currently best model would give <5% test error. Such a big gap is questionable. The only reason I can think of is that this is a typo and larger 'a' should actually mean more weight on y. Then, the results say that a=1 (corresponds to the true label without smoothing) gives best performance (around 90% which makes sense). However, this actually means that the proposed method is ineffective. In any case, for me the validity of experiments is poor.I'm fairly confident in my evaluation. However, if any reviewer or area chair points out that I made a mistake, please kindly correct me and I will re-evaluate the work. Currently, since the experimental results are questionable for me, I would suggest a rejection of this paper.  My enthusiasm for this paper remains fairly high after reading through the other reviews and responses: I think the results (in the main body of the paper) look strong. In particular, this approach is competitive with MixUp, and I prefer this simpler label-only approach to MixUp. I don't think this work is duplicative with the ICML 2020 deep knn paper cited by another reviewer (that paper filters out noisy examples, rather than smoothing labels). And I think the theory is novel in its synthesis oof KNN theory with label noise.However, I am unnerved by something that Rev 2 pointed out about the ablation results in the new Table 3 in the appendix: in Definition 3, the weight given to the assigned ("true") label is $(1-a)$, so we'd expect to see higher (or at least competitive) accuracy for lower values of $a$ when label noise is not too severe. For SVHN, at least, this seems like it should be true since we know the SOTA accuracy is in the 90s. Instead we see the highest accuracy is for the highest values of $a$. That makes me suspect the order is reversed. However, if this method works, we should expect to see lower churn for higher values of $a$, which we do, which would indicate that things are in the right order. This apparent contradiction is disconcerting since some of these numbers are cited in the comparison in Table 1 in the main paper.In an ideal world, it'd be nice to have one more round of discussion with the authors about Table 3. Because it was added to the appendix late in the review process and no further discussion is possible, I am electing to leave my score as is. However, I urge the authors to take a look at Table 3 and verify that there is no error. If it is possible for the authors to post a public comment and/or contact the AC with an explanation, they should do so.I'm also lowering my confidence to reflect my uncertainty about Table 3.----- .+++ Post-rebuttal +++I've read the rebuttal and am content with the response. I'm maintain my recommendation to accept this paper ------------After author's response----------------My major concern is about the connection between the universal approximation theorem and the proposed architecture. In the paper, the authors mentioned that " the following universal approximation theorem for DKL implies that this effect can be compensated by adding parallel learners". However, the fact that a multi-output learner is not equivalent to M different single-output learners makes it hard to justify the proposed architecture theoretically from the universal approximation theorem. I think this is something that is crucial to be justified, otherwise, the theory does not really match with the proposed method. -- post rebuttal---After carefully reading the authors response, I still think the assumptions are not well motivated. For example, 'On a clean bounding box, even a less well-trained classifier can produce a lower-entropy (high-confidence) prediction with low background scores. Furthermore, it is less likely that two different classifiers both predict the same wrong class with high scores.'. Both of these assumptions seems very strong. They might hold sometimes, but they can not be generalized. Also, I still think the technical novelty is not enough in this submission. Therefore, I am not very positive about this submission. ##### ======================== UPDATE =========================Thanks for the authors' clarifications.After a careful re-evaluation of the paper, I have many concerns about the performance of baselines on the StarCraft II benchmark tasks. The reported performance is not consistent with those reported in the SMAC benchmark paper (see Figure 4,5,6 in [1]) and QPLEX paper (Figure 5,8,19 in [2]). Moreover, I also evaluate the available GitHub codes of baselines on my own, which is consistent with [1,2].Using results in [1,2], QTRAN++ significantly underperforms the baselines on the StarCraft II benchmark tasks. Moreover, the paper claims that it uses the standard StarCraft II benchmark, the latest version of SC2, and the default baseline codes.Due to these concerns, I tend to lower my rating.[1] Samvelyan M, Rashid T, de Witt C S, et al. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.[2] Jianhao Wang, Zhizhou Ren, Terry Liu, Yu Yang, and Chongjie Zhang. Qplex: Duplex dueling multi-agent Q-learning. ICLR submission. https://openreview.net/forum?id=Rcmk0xxIQV ### *Edit after author comments:*I have read the author comments and the latest paper revision. The authors have noticeably improved the clarity of the paper in several places and adjusted their claims about the stability of the algorithm, and the improved ablation studies are appreciated. Unfortunately, after thinking it through very carefully and despite the author comments, I have not been able to understand some aspects of the model, for example why gradients from the tracking loss are backpropagated into the value function that is supposed to track the "true" action-values. Several parts of the architecture seem to have a complicated dual purpose, which makes it difficult to understand what is going on and why the model is performing better. I suspect that other readers might also encounter similar issues, which makes it difficult for me to raise my rating. I've decided to leave the rating at 6 (marginal accept). ----------------------------------------------------------------------**Updates**: After reading the other reviews and the rebuttal, I still maintain my current score. The additional experiments on the converged results are good to me. As I'm not very familiar with the performance in MARL literatures, I have decreased my confidence from 3 to 2 to reflect some of the concerns of other reviewers involving the performance for the baselines. Thanks for your replies!From your reply, I think this paper would be much stronger if you include empirical comparisons to Task2Vec, or improve Task2Vec. I am increasing my rating to borderline accept, but to be honest, the current version would still be a weak paper without such comparisons. --------### Post-Response UpdateUnfortunately, the authors' response is not satisfactory on multiple issues. Thus, I reduce my rating by one point. ================================= Update ===================================I slightly raise my rating, as everything is correct and well organized. However I'm still not sure if this is enough contribution or just incremental compared to the existing computation of proximal gd. I'd leave it to other reviewers.  Update: Nov 30 2020 == Thanks for the authors for the reply. Thank you for running those experiments.I had a few more clarifications needed from authors. (a) Magnitude pruning typically invovles a fine tuning phase after removing the weights, was this carried out? For eg: Fig 1. a behavior was why I asked this question (b) I would recommend authors to add error bars Table 2. has results that are quite close between the methods.I raised my score but still below accept due to the above reservations. After rebuttal:- I  have read the response of the authors. - Some concerns are adressed: for instance some empirical results are moved to the main paper,  the tuning of the hyper-parameters is discussed and details about the groups of variables are provided.- Nevertheless it is still unclear why $s_t$ instead of $\tilde{s}$ is  used in  algorithm 1 and what might be the performances of the deep models  trained using $\tilde{s}$.- The baseline model is now moved to Adam with weight pruning. One concern of the review is to detail  how Adam, Adagrad are used to optimize the fitting term with a group lasso regularization. This point was skipped in the new version, hence it's not easy to assess the effectiveness of the Group Adam, Group Adagrad methods. ===== POST REBUTTAL UPDATE =====I have updated my score based on the response. I hope the authors can add PWIL+TD3 results (point 1. in review) in the next revision. ==================================UPDATEThank you for replying to my questions and clarifying in the document. I have raised my score to "5" after the author's response. While now I believe that once can come up with a scenario where the proposed theory of Homotopy SGD outperforms the vanilla SGD, it is still not properly demonstrated in the paper; there are a lots of hidden strings attached to the provided convergence bound (explained in my response).  ============ after discussion phase =============My important questions about locality of PL are not explained. For example, on why the same locality argument cannot be done on Vaswani et al, 2019's analysis on standard SGD. As I also stressed in my original review, I believe the idea of the paper is interesting and can be useful, however the merit of the paper is not explained clearly in the paper. Rather than comparing by SGD with vague arguments, I think the authors should clearly explain under what setting is homotopy preferable to SGD and why, which will make the paper much more accessible and impactful. Given the lack of explanations, unfortunately, I keep my score for rejection. ------------------------------------------------------------------------------Thank the authors for their good job during rebuttal.Most of my questions have been addressed. However, my main concern is still that the improvement of CO2 on MoCo v2 is incremental. During the rebuttal stage, the authors did not include more empirical evidence on whether CO2 can improve MoCo v2 well, or theoretical analysis on why the improvement is incremental, both of which are accepteable to me. This severely limits the contribution of this paper. In all, I would not change my score. --------------------------------------------------------------------------------------------------------------Update:Updated score from 5 to 6. The author response clarified some key questions and the updated paper incorporated some of the feedback. ** after rebuttal: thanks for addressing the comments. I have revised my score based on the discussion. Updated score from 5 -> 6 after clarifying feedback from the authors.  ########## EDIT ##########The authors have addressed all my questions. #########################################################################Post-rebuttal.I would like to thank the authors for their reply, which addressed some of my questions. While I now agree with the main theoretical result when applied to a ReLU nonlinearity, this of course also reduces the area of applicability of the proposed technique. I am also happy to see additional empirical results, which I believe will be the key to making this paper much stronger (since the current theoretical result is actually quite straightforward when applied to the ReLU nonlinearity). But I think the results are still a bit insufficient to make this submission sufficiently strong. One of my concerns is the final accuracy in Figure 2. We can see that the unregularized model surpasses the accuracy of the refitted model and could potentially get higher (or even much higher) should it not have been cut at <100 epochs. I will be excited to see an updated and improved version of this paper in the future, but in my opinion, the current version still needs a bit of work and is not entirely convincing. ----- post rebuttal ---- The authors haven't addressed my questions. I would keep my score unchanged. One more comment: I suggest the authors compare to a related baseline SimpleShot [6] that is arguably less complicated.[6] Wang, Yan, et al. "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning." arXiv preprint arXiv:1911.04623 (2019). I insist (update): band-limited is a very very inappropriate terminology for convnets. I am waiting for author's' answers (need for clarifications): the paper needs to be revised. After Discussion---I increased my review from marginally below to marginally above acceptance threshold. Most of the remarks I had were at least partially addressed. If the paper gets accepted, I'd still recommend looking at some of them again and clarifying more. A simple, explicit remark somewhere around Table 1 explaining that N_p can indeed exceed C due to relevant parts of the search tree being preserved across time steps would help a lot. Some more explicit discussion about why the difference between using a trained value functions vs. heuristics / terminal results matters so much that it makes this substantially different from prior work would also help (I understand that it is because in prior work the only advantage of storing all those extra nodes was really just that it could retain slightly more information from backpropgations in those nodes, whereas in your case it changes which state is the state that gets evaluated by a trained value function, but this should be more explicit in the paper). ---### After rebuttalSee my reply below for my comments after rebuttal. Overall I feel the paper still has room for improvement and there are several open issues, but it has been improved so it is marginally above acceptance threshold now. # Post-Rebuttal CommentsI've maintained my score at 6.During the comment period, the authors made progress in improving the clarity of their presentation. As with reviewer 3, I feel that there is still room for improvement; in particular, moving some experiments in Section 5 to the appendix could make for a more focused paper with a clearer message for the reader. (Unfortunately, we did not have enough time during the comment period to get there). I'd also note that the paper is now at nine pages; this means that I am holding it to a higher bar.Overall, however, I continue to recommend an acceptance as the method to trade off natural accuracy and certified robustness is simple and significantly improves on the state of the art; for me, these strengths outweight the remaining issues.  Post rebuttal====Thanks the authors for clarifying and revising the paper. The updated version does look much clearer to me, so I updated my ratings. I am still wondering the difference of biprop vs. a classical quantization-aware training for ternary networks. I did read the response to R3. From my understanding it seems that:1. biprop doesn't count 0 as a parameter, while TWN does;2. biprop prunes a larger network (WRN50), while TWN trains a network of the original size (ResNet-50);I am not sure if the superiority of biprop comes from these reasons, instead of LTH itself. biprop still looks more like a QAT algorithm than a LT-finding algorithm in the sence that1. it does not train the pruned network after finding the LT as the original LTH paper;2. it directly learns the binary weights.Just out of curiosity, but I think clarifying these concerns would make the paper stronger. **Update after the author's response**: The authors have answered my questions during the rebuttal period and I am satisfied with the response. Hence updating my score: 7 $\to$ 8. #### Post-rebuttal comments:The author response has well addressed most of my concerns on technical details and experiments. Edit: I have read the authors' response as well as the other reviews. Based on the additional results and added feature selection details, I now agree that ESP is generally applicable.  Update:I thank the authors for their response. I read the authors responses and the updated paper. As the authors have addressed some of my concerns and questions, I have adjusted my rating. However, I still have my concerns regarding the novelty and methodological contribution, and the classification setup. I also agree with other reviewers that the presentation can be improved. **Post-RebuttalI appreciate the authors taking the time to respond. Unfortunately, my belief that this paper is not strong enough from the deep RL perspective to warrant acceptance has not changed. If other combinations of tools do not work, then the authors should improve this justification, with ablation studies or stronger theoretical motivation. Ill add that my score is not influenced by my concern that this paper may not be a good fit for ICLR (I leave that choice to the AC). ======post rebuttal: My concern regarding the experiments remains. I will keep my score unchanged.  Addressed Concerns:- corrected the typos.- Majority of the weaknesses are addressed. *** Post response comments ***I thank the authors for the clarifications and additional data provided in the revision; most of my concerns have been addressed. I have to say it is a bit worrying that the indistinguishable statistics in the first layer also occurs for the WideResNet models, which needs more explanation and exploration since it is a key motivation of the method. Also, the point raised by R1 on the evaluation is somewhat concerning. As such, I think this paper is borderline even though I am raising my rating to 6. Update after rebuttal------------------I am very interested to see the results of the experiment mentioned in the rebuttal.  Although you mention that gameplay dynamics and ability to interact with objects is crucial, I still feel that this isn't as cleanly demonstrated as it could be without addressing these confounds.  However, in retrospect I also realize that my initial score suffered from tunnel vision on that single issue, which is important to several claims in the paper, but is by no means the only contribution.  The paper is ambitious in scope, novel, and well written, so I have increased my score. ---------After rebuttalAfter the discussion phase, I agree that with R3 that in cases such as with periodic patterns, auNN would be a better way to incorporate such knowledge. In this case, the paper would be stronger if related experiments on such patterns are included, to highlight what the key difference between auNN and typical BNN.Gap uncertainty is indeed a real problem in BNN, and I am glad that auNN performs well on this, this is also why I would like to see more comparison to other baselines (e.g., NPN) with good performance on gap uncertainty, to better gauge its improvement/capability (e.g., see https://arxiv.org/pdf/1611.00448.pdf Figure 1(right) where there is also a gap in the middle, although not as large as the example from the auNN paper).If this paper is accepted, it would be great if the author could include R3s summary during the discussion into the final version (see below) as well as proper BNN baselines, which I think will be quite helpful in positioning the current work:Typical BNNs use stochastic weights and deterministic activations, while the authors model (auNN) uses deterministic weights and stochastic activations. You are definitely correct in asserting that uncertainty in weights manifests as uncertainty in activation functions. Directly modeling activation function uncertainty however provides distinct advantages, has not been studied carefully before.Experiments mentioned in the first paragraph would also be a good addition to make the papers point. ----------------------------------------------------I raise my score to 6 after the rebuttal given the clarification the author added to answer my confusion. UPDATE TO REVIEW FOLLOWING AUTHOR REVISIONS AND COMMENTS---------------------------------------------------------------------------------------------------I thank (and commend) the authors for their detailed, point-by-point responses.The authors have made a good effort in their revisions to improve and clarify the exposition, and rectify the other comments made by the reviewers. The paper could still benefit from a deeper rewrite -- there's just so much that can be packed into a conference paper with limited real estate, and the authors are seeking to make several contributions under the umbrella of one submission (as the title suggests). So clarity suffers, and impact will suffer as a result. But, in my mind that shouldn't necessarily be a show stopper at this stage, in light of the revisions. I am therefore upgrading my recommendation. ## UpdateI thank the authors for some of the updates and response that address my concerns with clarity, but my main concern related to the interpretability aspect has not been resolved. The authors suggest that one can discard some of the rules learnt by the algorithm if the model becomes too large, using the weights in the linear layer to determine which rules should be kept. This seems like a good direction to explore, but it is unclear if a few rules typically dominate the predictions, or whether one will sacrifice significant accuracy in order to gain interpretability. As stated in my original review, I would have liked to see analysis of the *tradeoff* between interpretability and accuracy of this method.A few other things:* I still do not understand exactly how the hyperparameters were tuned. The authors have provided a list of those that were tuned, but I still don't understand the process used for tuning them.* Some of the responses to other reviews have reinforced some of my concerns. In particular, it seems the distribution of weights attached to rules is not optimal for the proposed (but unevaluated) rule pruning method. Also, the authors conflate statistical significance with practical significance when replying to Reviewer 4. After rebuttal:Comment:I am suspicious of your argument on "complex due to multiple resolution pathways".Surely there is always a (unique) canonical deviation. For example, one could imagine extending Prolog to allow for any literal to be resolved; not just the left one. However that just adds extra complexity. Because we have to resolve all literals, we might as well do in a left-to-right order. Surely in your case, we can always do the leftmost (for example) one. Or the rules can be defined so that we can resolve them left to right.One counter to my argument might be (parent, ancestor) -> ancestor parent -> ancestor.Here we cannot go left-to-right, but need to go right to left. We do not need to search over orderings.This issue is faced daily by Prolog programmers; we choose whichever one of (using your notation): (parent, ancestor) -> ancestor (ancestor, parent) -> ancestor works with your engine. I don't see why they get around it without problem and you claim it is not possible. The rules may depend on the order used, but what is the problem with that if it drastically reduces the search space? You don't have to search for all proofs; just one. ----------------------------------------------Update after Rebuttal: I have read the other reviews and authors' responses but do no change my scores.I still share the impression of Reviewer 3 that several choices in the design of the benchmark seem to be very restricted and arbitrary (e.g., the single possible derivation sequence).Related work in terms of rule learning has been incorporated partly now, but there is no proper comparison -- especially in terms of the dimensions mentioned in the previous item. Also, there have been proposals similar to the E-GAT model the authors introduce which are completely ignored in the paper (e.g., MEMORY-BASED GRAPH NETWORKS, ICLR 2020; Graph Neural Networks for Social Recommendation, WWW 2019). ------ Post Rebuttal------Thanks to the authors for the extra experiments and feedback![Lottery baseline for Table-1] Although RigL does not need dense network training, it cost more to find the mask (Table 2 of the RigL paper).[Random tickets] Random Ticket = LT mask + random re-initialization rather than random pruning + random init. The front one will be much more interesting. "Because the LT mask + random reinitialization = random tickets fail in the previous literature. According to the explanation in the paper, it can also be the problem of random reinitialization. Thus, strong supportive evidence is that show proposed modified random reinitialization + LT mask can surpass random ticket performance." I personally do the experiment that performing proposed initialization on random tickets and the performance is unchanged. Of course, there may exist lots of reasons for the results. I will not degrade the paper according to my experiments.Other concerns are will-addressed. Thanks!Although I do like the idea of this paper, I think it might need to be revised and resubmitted, incorporating the extensive discussion presented by all the reviewers. I tend to keep my scores unchanged. But I dont think this is 100% a clear reject and depending on the opinions of the other reviewers I would not feel that accepting this paper was completely out of bounds. After response: While some of my concerns were cleared after the response, the experimental evaluations presented do not sufficiently support the claim that the method can handle nonlinear constraints.  ---Edit after rebuttal: I am satisfied with the changes to the paper and increase my score to an accept. ********************Edit: responses after rebuttal:> The class of PVFs are developed to be applied and evaluated in the online learning setting.This is not true. See for instance off-line and zero-shot learning experiments with PVFs that can learn new policies which do not interact with the environment and still show generalization.> We think this may also be a reason to the insignificant improvement compared with their baselines in their experiments (their Figure 2), even the inferior results in simple tasks like InvertedPendulum and CartPole. In contrast, our proposed policy representations enable PeVFA to be compatible with normal-scale policy networks and we then demonstrate the superiority of PPO-PeVFA against PPO in standard continuous control task of OpenAI GymI would strongly discourage you from claiming this. PPO-PeVFA is built on top of PPO, so it is very hard for your algorithm to perform worse than the baseline. On the other hand, PVFs and PENs are novel algorithms that rely COMPLETELY on the prediction of the value function. It is expected that they might outperform baselines in some environments and be comparable or worse in others. The PSSVF proposed in the work on PVFs is outperforming the baseline ARS (Mania et al. 2018) in all environments but Reacher, even when the policy is a neural net with 2 layers and 64 neurons per layer.About Q2:1. If the authors are using the $L_{\infty}$ norm, then the results practically apply only to finite state MDPs. Indeed, this is the case for the papers [2,3,4] cited by the authors in the comment above. However, the experiments proposed in the paper deal with continuous state spaces, so the theoretical results are disconnected.It is still not clear what the assumption in Corollary 2 means and if that assumption is met in practice. If the authors cannot justify the assumption, the Corollary should be removed. The novelty of Theorem 1 is not obvious, since it seems to be a quite trivial result from Mathematical Analysis.> The results show that PeVFA consistently shows lower losses (i.e., closer to approximation target) across all tasks than convention VFA before and after policy evaluation along policy improvement path, which demonstrates Corollary 2Again, is the assumption of Corollary 2 met here? If you have a logical rule (or corollary) of the form: assumption -> consequence, and you show that the consequence holds, this does not imply that he assumption is true!One main argument in favor of PVFs is that even with a much bigger policy, PVFs are able to outperform the results of PENs. This suggests that Raw Policy Representation (RPR) can be a strong baseline and any work trying to introduce a different policy representation should at least compare to RPR. Without such a baseline it is difficult to assess the benefits of the proposed policy representations. Note that PeVFAs are PVFs! The PVF formalism already accounts for all kinds of policy representations.Of course, all of these relations should also be clarified in title and abstract.(Mania et al. 2018): Horia Mania, Aurelia Guy, and Benjamin Recht (2018).  Simple random search of static linear policies is competitive for reinforcement learning.  In Advances in Neural Information Processing Systems,pp. 1800-1809, 2018. ================================================================================================================The authors has addressed my concerns well in the discussion period, I have increased the score to 6. ### Post rebuttalI would like to thank all the reviewers for valuable feedback. My review score stays the same. ----------------Post-rebuttal thoughts:I would like to thank the authors for their detailed response and the revisions made to the paper. I'm updating my score to 5 as part of my concerns are satisfactorily addressed, and I wished I could have more opportunities to discuss with the authors on their response. In general, my opinion is that the authors have introduced too many "artificial" components to the study (e.g., soft gradient coupling, the convergence/divergence indices) that make me slightly dubious of how generalizable this characterization is. For example, as the authors indicated, spectral normalization creates a different phenomenon (at a cost of worse performance), but with no change to the structure itself (so unlike the soft gradient coupling), a different phenomenon could be challenging the conclusion of the paper.My suggestion would be that the authors delve deeper into the observations here and better integrate the revisions with their original approach (e.g., the high-dimensional discussion; the spectral normalization discussion, etc.)----------------  ## After Author ResponseI think the changes to the paper have improved it. In particular, the reference to Spoerer et al. gives more weight to the motivations of the paper. I'm increasing my score slightly as a result.However, the relation to prior work remains hazy. The model of Ciccone et al., for example, has shared weights, stability over increased depth, and still performs as well as ResNets generally. Doesn't this go against the conclusions of this paper? ###########Updates: Thanks for the authors' response. The modified version improves clarity. I think this paper provides nice observations and initial analysis to the community and can be beneficial to future work, so I recommend this paper to be accepted. ### Post RebuttalI have read the other reviews and the author's response. I also had another look at the revision. While I appreciate the work that was put in the revision, my main concerns remain:* This paper proposes an alternative to VAE-based approaches to learning disentangled representations, which are known to require hyper-parameter selection based on ground-truth factors (as pointed out in Locatello et al., 2019). Hence, to improve over VAEs in this regard, it is crucial that the proposed framework does not suffer from this same issue or least consistently performs better compared to VAEs. Unfortunately this is simply not the case: across Figure 2 and Table 1 there is no variation (among CF, GS, DS, LD) that consistently performs well or consistently outperforms VAEs. Similarly in Figure 3c it can be seen how the GAN-based approach and VAE-based approach perform similarly (especially if the outliers on the top would be included in the mean). Similar fluctuations can essentially be observed across all figures and which does not take into account yet the choice of GAN or other hyper-parameters that were kept fixed (as per my initial review). Further, notice how in Table 7 (when using ProGAN as opposed to StyleGAN), the best performing variation (DS) now performs more than 50% worse according to MIG -- which is not factored in the comparison to VAEs.* The abstract visual reasoning experiment is flawed, since the comparison to van Steenkiste et al. (2019) figure 11 considers a random selection of pre-trained VAEs for which it is unclear whether they were disentangled or not.* DS is a simple heuristic that generally does not outperform other approaches. Indeed, notice how not reporting the results for DS separate for each layer (as was done previously) now results in large fluctuations. In particular, for the four datasets considered in Figure 1 DS outperforms other methods only on 3D shapes. Further, although the authors in their rebuttal argue that DS is not a simple heuristic since they build [DS] on the intuition that singular vectors of the Jacobian provide a set of locally ``independent directions with respect to the perceptual metric, there is no evidence provided that this intuition is correct. I had suggested ways according to which this part of the contribution could be strengthened (i.e. by analyzing it in the linear case, or attempting to automatically select a good layer) but this was not further explored.I do think that there is significant value in a systematic analysis of GAN-based approaches applied for disentanglement in this way, as it could serve as a useful benchmark for existing (VAE-based) approaches to compare against. However, the current set-up falls short at this as it is not sufficiently systematic and certain variations remain insufficiently unexplored (like other kinds of GANs). Further, this is not how this work is currently motivated in the paper or how the comparisons are performed.These concerns are irrespective of whether the reader needs to perform a manual comparison to Locatello et. al (2019). Although I noted in my review that this is far from ideal, I can understand how this may be necessary if Locatello et al. (2019) can not provide a smaller range of hyper-parameters. In that sense, I believe that the authors did a good job incorporating VAE results at such short notice. -------------UPDATE-------------I thank the authors for their response. The revised version certainly looks better. I'm still happy to recommend this paper for acceptance.  **Thanks to the authors for the response. The addition of a human study and CoCon+ has made the paper substantially stronger, as it resolves most of my concerns. The authors provided plausible explanations for the remaining questions. The paper should now be considered a clear accept.** ------- **Written after rebuttal:**Thank you for the substantial improvements to the paper in terms of clarity (in particular Figure 2 is helpful) and additional experiments/human evaluations. Despite some underlying questions (from me and other reviewers) that remain, I have updated my score and am now leaning towards acceptance. =============================== UPDATE =============================== Checked the proof again and want to confirm that, is it using the composition with pointwise maximum? If so I believe that it's correct, just be more detailed with the proof. Now I'm only concerned with the technical sophistication of the paper. Would be good to discuss deeper NN. Also good to make clear the advantage over other convex or nonconvex but optimizable formulations. **Update after rebuttal - I would like to keep my score**1. Although the authors have provided dev set numbers, the fact that test set numbers were computed, is highly concerning. In fact, for ATIS, the best dev model is different from the best test model. The other models should never have been evaluated on the test set. 2. Am still concerned about the datasets being old and saturated, and would love to see results on more recent datasets. -----------------------------------------------------------------------------------------------Compared with the latest related work, the author added the qualitative and quantitative comparison against "Recurrent Feature Reasoning for Image Inpainting" in the image completion task. And it shows the outstanding performance of this paper. They have also reformatted the paper so the paper becomes clearer for readers to read. Consider the above all, I think its a good paper and is worthwhile to be accepted.So I improving my rating to 7: Good paper, accept Post discussion: I'll defer to R2 and R4 for judging the theoretical contributions and am convinced that they are not as significant. =========After rebuttal I will still keep the score. The Assumption 1 is still quite strong. While it is true that $p(\mathbf{a}|s) = \prod_i p(a_i|s)$ holds for any policy with decentralized execution, the assumption of full observability is really strong and it doesn't seem that it can be got rid of easily. With this assumption, $p(\mathbf{a}|s) = \prod_i p(a_i|s)$ is actually a trivial assumption, since all information needed to make a decision of action $a_i$ for agent $i$ is already contained in the full state $s$ and can be determined independent of each other. In the revised paper, the authors suggest that communication can solve it but this would require thorough communication over the entire MDP, which can be hard to achieve (and if that's easily achievable then there is no need to study Dec-POMDP anymore). Without relaxing this assumption, I have concerns that the theory is not substantial (which are also concerns from other reviewers like R3). One baby step is to at least assume each agent may receive a noisy version of the full state $s$, and see what's going on. I thank the authors for additional experiments. Note that in addition to the proposed theory, there are many possible explanations of the empirical results presented by the authors. E.g., as a general rule of thumb in RL training, using offline data is often worse than using online data. Without detailed analysis, it is hard to tell. I would rather use an environment that is more complicated than the matrix game, but much more simpler than SMAC, which is partial observable and has too many moving parts.  --------------------- UPDATE Nov 30 --------------------------------------------------------------------------------------I find the updated manuscript to be a significant improvement over the initial version. The class of problems is now clearly described and the problem formulation explains the challenge and the need for an RL-based solution approximation.Seeing now that this work is not about a particular deployment scenario, but rather aims to present a method that can be used to solve a general class of constrained routing problems, another set of question arise.Probably the main limitation that precludes from evaluating the significance of contribution and seeing this work as general contribution to the list of solution for routing problem is the fact that is was evaluation on only single instance of such problem, with only single set of experiments parameters (20 robots / 200 tasks). Since the paper aims to propose a general method that is widely applicable is it only reasonable to expect to see experimental evidence that when applied to a diverse set of problems of this class it comes out on top. How does it fare against other similar routing optimization problems? How does if fare against other methods or AM is the only method that is worth comparing to?With this in mind I am raising my score for this paper from 3 to 5 "Marginally below acceptance threshold". The reason for not going higher is the lack of demonstration of applicability of this methods to a wider range of problems. ******************After rebuttal:Glad to see this was just an error in the decription of the algorithm! Score increased from 4 to 7. We'd even increase our score to 8 if the authors added an analysis similar to the one of Figure 3, just showing which OF is used. As stated in the original review: The experiment could be to add/remove objects, and observe the number of object files, or to corrupt specific files and see which object becomes unpredictable, thus identifying which OF corresponds to which object. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response. All my concerns are addressed. I will increase my score to 6. **Additional comments after the discussion**The authors have thoroughly replied to all the comments from the various reviewers.After reading all the discussions (other reviews as well as replies from the authors), it appears to me that the practical relevance of this contribution is not completely clear. The computational cost of each iteration is large. The benchmarks do not show clear improvements in computational. after rebuttal: The authors have addressed some of my major concerns in an updated version. for this reason I raise my score to a point where I can recommend acceptance. I now add after-rebuttal comments at the end of each item of my original review.**after-rebuttal**: the authors have changed the lower bound to a true lower bound. .**Update**: After reading the rebuttal, I am maintaining my previous score. I believe that, given the related work reviewers have mentioned, the paper most likely requires some experimental comparisons with these other methods. The rebuttal arguments are good, but not convincing enough that I can believe in this method's superiority without seeing a comparison with, e.g., CaCE   Update after rebuttal: I thank the authors for their responses to all my questions. However, I believe that these answers need to be justified experimentally in order for the papers contributions to be significant for acceptance. In particular, I still have two major concerns. 1) the faithfulness of the proposed approach. I think that the authors answer that their method is less at risk to biases than other methods needs to be demonstrated with at least a simple experiment. 2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating. [Update after author's rebuttal]I do not see any reason to modify my rating. I also identified the self-citation , but it did not affect my rating or evaluation of the paper.  UPDATE AFTER REBUTTAL =========== I have read the author's response. While the case study for industrial applications is important, it would probably be much more impactful if the same study was done on a much larger/realistic scale. For instance, right now it appears that each edge vehicle gets an already available dataset for federated learning, which may have been cleaned and preprocessed properly. For claiming a real industrial deployment/importance, it would have been great if the study was conducted with vehicles receiving real-time data from real vehicles which is prone to be extremely noisy (although the reviewer is not sure if this would be possible for regulatory reasons (e.g., if such learning experiments would be safe enough on real autonomous vehicles as these applications are safety-critical)). Currently, the paper neither has significant enough contributions from novelty side, nor from industrial deployment angle. Hence, as such, the paper cannot be accepted. Perhaps more application-oriented conferences maybe more suitable for this kind of work. I do not think the response addressed my concerns. I would strongly suggest authors reconsider the design choices where I raised questions. Note that these are not only clarification questions, but also fundamentals of machine learning and federated learning. **Update after author response:** I appreciate the authors' efforts to address my comments. Part of the comments on the presentation of technical details have been addressed, such as the meaning of "position" of a vertex and "-" in Equation 9. However, the main concern in terms of the technical depth has not been addressed well. There is a lack of performance guarantees and theoretical guidance on the choice of parameter values. For example, how much worse in terms of the attack impact when the weakest vertex is found by the heuristic algorithm instead of by a full traversal? The results in Section 4.2 and Table 3 are empirical and not theoretical. As such, I could not change to a more positive rating.   (edit: see discussion below, I have adjusted the score to above the acceptance threshold) ----- Update: Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m. ## Update after RebuttalThanks for the detailed reply. You have answered a number of my questions, but many of my main concerns remain, in particular points 3 b) and c) as well as 4). Furthermore, incorporating all the clarifications and additional discussions into the paper would in my opinion amount to a substantial revision beyond the usual scope of revisions after rebuttal. I will thus stick to my original rating and recommend rejection for this paper. # Update after the rebuttalThank you for including the comparison to TDMs - the results are now much more convincing.I think this should be one of the main results in the paper (which should be presented in the main text rather than in the appendix).Also, I'd suggest revising the paper to highlight new contributions compared to TDMs in the intro/method section. I increased the score assuming that the authors will reflect my final suggestions.Regarding distributional RL and cross-entropy loss, distributional RL does allow a probability distribution as target (not just either 0 or 1 and just like yours). I'd suggest the authors to take a closer look at the paper and clarify differences (I still think the cross entropy loss is a special case of distributional RL with discount factor of 1). * Update after reading author(s)' response: Thank you very much for the detailed answers to my questions (as well as the other reviewers' comments/questions).  I have upgraded my score; wishing you all the best.  ----------I appreciate the response from authors and the additional experiments. I do think the semantic search task adds value to the paper. However, the paper continues to be centered around the SentEval benchmark results. While SentEval is a useful benchmark to evaluate sentence representations, it doesn't reflect well how these representations will be used in practice. A fine-tuned BERT model will likely perform strongly on these tasks. The paper would be far more compelling if the authors can provide strong evidence that the sentence embeddings do well on tasks where using a BERT model is either less effective due to performance or computational reasons. I prefer to keep my score. ------------ I appreciate the response from the authors to my review, as well as to others. My concerns on the intuition are most not solved. Although in this DNN dominating era, we cannot expect the explainability as we had before, I still believe that a solid work should be grounded on a reasonable basis, which could be in a high level, such as BERT and SBERT. Let's refer to the example given in the model architecture. The projection of the sentence vector of "Life is a box of chocolates" is left-concatenated with the masked embeddings of the second sentence. This operation is very much lacking in intuition, how come the projection of a sentence representation can be concatenated with the embeddings? In addition, "The second encoder shares the same weights with the encoder used to embed s1", considering their inputs are very different, weight sharing for the two encoders are also problematic.Another point I just noticed, although the authors claimed that their model is better than SBERT, and did a comparison with SBERT-large, they did not compare with SBERT-base, which makes the conclusion unreliable.--------------------------------------------------------------------------------------------------------------------------- --------update after reading the response-----------Thanks for the authors' response. Mainly empirical and limited in methodology novelty. So I tend to keep the score. After rebuttalThanks a lot for the authors' extensive experiments and good explanation to my questions. I would increase my score. The basic idea of this paper is promising and useful. However, there are still several problems after reading the rebuttal.Figure 2 shows that GD can distinguish different noisy levels, however, it is not a very realistic setup when training with a noisy dataset, to be specific, the dataset only has one noisy lever rather than multiple. Furthermore, GD is not able to give an early-stopping criterion on noisy datasets from Figure 2 where test error keeps decreasing but GD is increasing. Including noisy datasets analysis seems not a significant contribution. -------------------------------------------------------- Post Rebuttal --------------------------------------------------------------------------------------------I carefully read the reviewer's comments and the rebuttal. I think the author did not get my major concern on competing algorithms. I acknowledge this proposed method is a novel problem setting and my goal is not to ask for summarizing the difference in problem settings. However, the technical approach, which is based on (batch-)training sample selection and reweighting, could be validated through comparing against more comprehensive baselines, as I pointed out, under similar settings.Furthermore, I am not convinced by the claim that unseen perturbation cannot be justified if there exists a similar perturbation assemble during training. The reference-based perceptual similarity is a well-studied domain which could be directly adopted to evaluate the overall "difficulty" or "out-of-distribution"-level. And I am not convinced by the author that the experiments you have tried cannot be shown in paper due to page limit: at least you could include in supplementary, as they are important to justify if your experiment setting really validates generalization ability. In sum, I will keep my current rating.  UPDATE: I would reduce my score to a 6 based on the opinions of my fellow reviewers. It appears that the restricted scope and lack of experimental results is quite a problem within this community and venue. ----COMMENT AFTER REBUTTAL PERIOD:Given that there was no rebuttal, I keep my initial rating. Post-rebuttal update: I thank the authors for their response. The points raised above were largely acknowledged and the authors chose to not revise the manuscript, so my assessment remains the same. Edit:I have read the authors' response and the other reviews. I still believe that this paper is not ready for acceptance.  ## After Rebuttal and Discussion PeriodI want to first say that I really appreciated the opportunity to review this paper. It was awesome to see the authors willing to respond to the various suggestions and questions that the reviewers provided. The paper has improved over time but some significant areas of improvement remain. There continues to be some discontinuity between the stated scope of this paper as XRL and the proposed CDT approach. During the rebuttal period, I felt that the authors didn't do enough to soften their claims to match the support provided by the contributions present in the paper through both the methods and experimentation. This is not to say that this is not valuable work. As discussed, the concepts and ideas are strong, I however feel that the execution and scoping of this work is a little off from clearly communicating the contributions it makes. One aspect will be through a full evaluation of the utility of their approach as explainable through user studies or other qualitative means. The claims made in this paper regarding explainability are currently unsupported.Additionally, I believe that there is significant room for improvement in the experimental portion of this work. If there were a way to provide some ablations of the CDT approach as additional baselines as well as the SDT/DDT benchmark, it would significantly improve this portion of the paper.As it stands, I have not chosen to adjust my score. I do however urge the authors to continue in this line of work. I believe that there is strong merit with the direction they've begun. I look forward to seeing a future completed version of this work. ******* After RebuttalI have read the author's response ~~~~~Based on the discussion and the updated manuscript, I am happy to say that most my concerns were addressed and at least partially resolved.  I have updated my score accordingly. # Update after author discussionMy original review mentioned an apparent similarity to VideoFlow and a weak experimental setup as the main reasons for my score. Through discussions with the authors, I am now convinced that VideoFlow is indeed a quite different model, which significantly adds to the novelty of the proposed model. As for the experiments, I had misunderstood the setup. In terms of probabilistic multivariate methods, my experience lies in the multi-output Gaussian process domain where it is common to consider tasks where only some outputs are considered missing (i.e., part of the test set), thus testing the models' ability to make use of partial information at a given input. This was the setup I had in mind, but the proposed model is not easily adapted for this. This is not a limitation of the proposed model as much as an inherent difficulty of handling missing data with neural networks. The experimental setup used in this paper instead considers a training set and a test set entirely separated in time, which is not a setup where GPs usually perform well, so the additional experiments I had requested do not make much sense. Indeed, the baseline methods chosen for comparison in the paper are both reasonable and strong.In conclusion, the authors have satisfactorily addressed my concerns with the submission. I still think the proposed idea is good (although not groundbreakingly novel), the paper is well-written, and the model thoroughly discussed and tested. I view the submission as a significant contribution to time-series modelling and is therefore recommending acceptance.---------------------------------------------------------- =====I have read the authors' response and my comments remain the same as above especially the paragraph regarding novelty. I am keen to see a revised version of this paper. ----After rebuttal:Thank the authors for providing a revised version. The paper is more readable, however, I still hesitate to give 6 for the current version.  I would say that the fact pointed in this paper is really interesting, but this paper may need further improvement in the presentation. #######As no author response, I will keep my rating. --------After rebuttal (updated)---------------Thanks for your detailed response. I would like to apologize for my late feedback. Since your rebuttal is long without proper organization (5 pages without properly using markdown), I may miss several points. I directly commented my feedback on my original review, by the marked textSeveral of my concerns have been surely fixed and the understanding of the proposed approach is much more clear. However, it is still difficult to change my score because of the following reasons:Current paper still requires careful polishing for facilitating reading. e.g. The authors claimed several times they will update the paper with pseudo-code, it is still missing after discussion. The paper and the rebuttal are still dense, which make the reader difficult to understand.The diverse datasets (such as NLP, digits experiment in the original DANN paper) and additional in-depth empirical analysis (not only accuracy) are indeed necessary. I think you do not need to claim a significantly better performance with SOTA. The detailed empirical analysis is more important.I hope my additional comments and feedback can help you improve your paper. Further comments after the rebuttalFirst, I would thank the authors for the great efforts in trying to address my comments. I should say that many of my previous concerns have been clarified. Again, I like the theoretical part. Nonetheless, I agreed with some other reviewers that the experiments seemed not to fully convince me.  Particularly to myself, the authors may want to show some analysis on how their method could truly stand out by even a toy example, which may further enhance the paper. I tend to keep my original rating  after reading all the rebuttal messages in the whole thread.==================== Update after reading authors' response.The authors didn't address my question on the statistical significance of their results compared with baselines.The authors can address this question by "perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant."I suggest the authors to perform experiments to compare with SOTA baselines in the entire field of domain adaptation, instead of just comparing with a subset of algorithms that are based on adversarial training. After all, you want to see how your work stands in the entire field, rather than limited to a sub-field. ------------------------ After reading the response, I still think that the work is promising and would like to keep my recommendation.  ----------------------The rebuttal did not change neither my confidence, nor my impression about the paper. So I did not change my rating, however I acknowledged that other reviews may be more proficient to judge this paper properly.  -----------------------------------Post revision update:All concerns addressed. Score updated. ----Post revision update: my concerns have all been addressed. ----- Post Discussion -----I appreciate the clarifications made in the discussion regarding additional experimental details and whether the methods were fairly compared. However, given that much of the claims rely heavily on the empirical evaluation, I think further experiments with more rigorous statistical analysis is necessary. Even with the correctly plotted standard errors, the results still largely do not appear significant, suggesting that 5 seeds is just not enough. There are good recommendations by Henderson et al. (2017) and Colas et al. (2018) for the empirical evaluation, and I think it would additionally strengthen the paper to formalize the notion of heterogeneity (e.g., be able to approximately measure it, and convincingly argue that this is what's underlying any differences in performance).  ---### Updates:Thanks for the authors' response. My major concerns were related to the comments (1) and (2) in Cons, but it is now clear why the authors consider only the Gaussian copula in the paper. My other concerns are addressed, too. I upgraded my rating after reading the authors' response. ++++++++++++++++++++++++++++++++++++++++++++Edit after author response: We thank the authors for their response to my concerns and those of other reviewers. **Update after rebuttal** I would like to thank the authors for the additional details provided in the rebuttal and revised version. All my concerns have been adequately addressed. Given the importance of the topic and the development is reasonably solid, I would like to recommend for its acceptance.  *Update after discussion*: The authors have addressed all the points that I raised during the discussion. I appreciate the effort, and I'm increasing my score accordingly. After rebuttal: no rebuttal, so I will keep my score.----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- #########################UPDATEThe discussion phase has highlighted several major issues: 1. There has been a significant conceptual shift in the problem definition (i.e. from estimating the intrinsic dimensionality of the class manifold to quantifying its geometrical properties). 2. I'm not convinced about the validity of some arguments/statements used by the authors to support point 1. For example, the statement "The intrinsic dimensionality of class manifolds is absolutely the full dimension of ambient input space, but this is a completely uninteresting observation" is not fully supported and I'm not even sure that is true.3. Furthermore, the paper is still in its original form. It has been difficult to keep track about the modifications that the authors should do.To conclude, the article is not ready for publication, yet and therefore recommend for its rejection. I encourage the authors to further investigate the topic and carefully consider whether the statements provided in the discussion phase are true. **Update after rebuttal**I would like to thank the authors for the detailed rebuttal, but my feeling now is that the rebuttal is making it even more complicated and sometimes conflicting with itself. I believe the paper needs some careful rewriting and updates to clarify its points and assumptions. Concretely, the paper is built upon the premise that each class manifold is a submanifold with dimension lower than that of the ambient space. I pointed out in my review that this premise may not hold at al, therefore the paper is fundamentally problematic. Then, R2 in one of his/her responses raise the same question, perhaps after reading my question. Then, I see a difference in response to R2 and my comments. For R2, the response is "The intrinsic dimensionality of class manifolds is absolutely the full dimension of ambient input space", which is effectively acknowledging that my critique is valid. However, the response to me is "This is very easily refuted by the ubiquitous and universal existence of adversarial examples". I don't really see why there is a discrepancy here. Besides, the argument that is used to refute my argument, namely existence of adversarial examples implies class manifolds are lower dimensional than the ambient space, is apparently wrong and can be easily refuted. By and large, the existence of adversarial examples only means that the decision regions are thin at every location, I can totally have a fine mesh of the data space that achieves this.  ------------------------------------ Post-rebuttal comments ------------------------------------With the authors' answers, the inclusion of miniImageNet results in the main text, and revisions to the writing, my concerns have largely been addressed and I have increased my rating from a 4 to a 6.I have a few further suggestions to make: first, make it clear in Table 4 via captioning and/or markings that UMTRA and LASIUM results for miniImageNet depend on the use of full unlabeled ImageNet data to train the AutoAugment and BigBiGAN models, respectively, while the CACTUs results do not. This results in an unfair comparison, and naturally raises the question of how LASIUM would do if we used a generative model trained only on the meta-training split of miniImageNet, which would be more in line with the protocol used for Omniglot and CelebA. It seems fair to assume that it would do worse than the current LASIUM results in Table 4, and probably the CACTUs results as well. This relates back to my point in the original review about sample fidelity: miniImageNet has the most diversity among the datasets considered, and it is hard for generative models to capture this diversity given a relatively small dataset. It would be good for this to be conveyed to the reader.The usage of the extraneous ImageNet data for UMTRA and LASIUM does not conform to the definition of unsupervised meta-learning proposed in Hsu et al. (2019) and adopted in this work. Some discussion of the problem assumption may be warranted: in what practical circumstances would extraneous, relevant, unlabeled data be available? And when it is, why would one not use all of the data (e.g. the entirety of unlabeled ImageNet) to do unsupervised meta-learning and/or unsupervised representation learning, like in Table 12 of Hsu et al. (2019)?Overall, despite the unfair miniImageNet protocol, I still advocate for weak acceptance as the method does show competitive results for domains that are more suited to generative modeling. For the sake of clarity for readers, though, I strongly encourage the authors to implement my additional recommendations. Update: After reading the other reviews and responses, and in light of the authors' updates to the paper, I have increased my score to a 6. Update:Although the paper has improved, I still vote for rejection. The new insight of binary-classwise v/s multiclass UCE as a regularizer seems poorly explored in the paper and would benefit from closer study. This appears to be the basis of the improved results in table 1. ---- update ----In response to the author's comments and extensions, I've raised my score. Edit: Upgraded rating from 4 to 5-----Thank you for addressing most of my concerns.I have increased my rating, but the paper still needs some work in my opinion. More specifically,* a more thorough comparison with [1], e.g. by including the quantitative comparison you were working on.* an improved representative experiment from the problem domain. The newly added one is a bit simplistic and not motivated enough. Although stated to be 2D, it is (as far as I can tell) indistinguishable from a 1D regression task? How do you handle the 2D output in GPR? How are the data points generated that are used as training data, and what could motivate such a generation of data in a plausibly real setting? ################### After the rebuttal ################I thank the authors for addressing the issues that were raised. The paper is indeed addressing a very practical issue in the ML community.After reading other reviewers' comments and concerns I decreased my score.  ############################################AFTER THE REBUTTALI have decided to keep my score unchanged. The authors include a new "road navigation experiment" in Section 5.2. However, I have several concerns about it:* This is a synthetic experiment that the authors have created themselves. Therefore, it does not address my concern on seeing actual real-world problems that can be solved with this approach.* The results of GP-NC (the proposed approach) and GP are very similar. I don't really see that the predictive mean of GP-NC avoids the "negative samples" more than the standard GP.* In fact, the design of this experiment is not clear to me. They say at the beginning that they use the present location ($x,y$) to predict the next location ($\hat x, \hat y)$. How do they model these two outputs? How do they obtain the plotted error bars for this model? This is not sufficiently explained. My impression is that they are just fitting a standard 1-dimensional GP being the input the x-dimension and the output the y-dimension.  ## Update after rebuttal:The authors provided convincing evidence that the proposed similarity regularization performs better with regards to robustness than $L^2$-$SP$. I adjusted my rating accordingly. I still think the contributions are borderline because the proposed regularization is not backed up by theoretical arguments and the augmentation approach is incremental. Update: I am happy with the changes and am keeping my score. ---The authors' comments strengthen the argument for the method.   I (perhaps liberally) conceptualizetheir approach as one of tethering "close to a rotation" but still don't have a simple understanding ofwhy/when this should be better than tethering "close to original parameters". My original rating is unchanged. ##############  Update after rebuttal ############## I would like to update the score to 6 based on the author's response. I have not increased further due to the limited novelty of the paper. However, the observations in the paper certainly add value to the research community.I request the authors to consider reporting performance of other defenses in Table-16 using the recommended settings in their final version.  ---- UPDATE: Thanks for the response, I have responded below and kept the score constant. Since I'm currently not actively working on the practice of adversarial robustness, the other reviewers are likely a better judge on the usefulness of the results of the paper for the community. Thanks for your response and the updates on the paper. This addresses some of my concerns and clarifies my questions. I have updated the review accordingly.I think that it's up to the AC to decide if the two pending concerns are important or not. Specifically,If the evaluation on a single task is sufficient for evaluating the representation contribution of this paper.If the delta of the contributions of this work to those of Dai et al. (2019) and Yang et al. (2019), are sufficient for this paper to get it accepted. Update after reading the response from the authors:I would like to thanks to authors for answering my questions and revising the draft. I agree that the main strength of this work is to explore data augmentation methods which are modality agnostic. Since most of the data augmentation research is domain specific, I think this paper will increase awareness for modality agnostic data augmentation methods. I think it's a good paper, hence I am revising my score.   I have read the author response and have decided to keep my initial positive rating of 6. Update after rebuttal: I appreciate the authors' response and the new ablation study. I maintain my original score (marginally above acceptance threshold). I continue to think it's a strength that the method works in many domains, but this strength is slightly diminished due to the variability of domain performance (e.g.: image domain). # Update after author responseThanks to the authors for their response. I appreciate the inclusion of Figure A13,  though more details (specifically, whether the plot for LR=0.2 is both LR_find and LR_eval, as I suspect but can't confirm) and more runs (specifically, the decoupled LR lines in Fig 4a that outperform LR=0.05, probably the LR_find=0.05 and LR_eval=0.2) would be appreciated. I don't agree with the authors' claims about standard LT (rewinding to iteration 0) being equally as valid of an object of study as rewinding to later iterations on these large-scale networks, because by the original definition of lottery ticket (a sparse randomly initialized subnetwork that matches the accuracy of the full network in the same amount of training time), there do not exist standard lottery tickets on ResNet-50 at nontrivial sparsities using the standard learning rate schedules, and the resultant "lottery ticket" network trains little better than a random subnetwork [1, Figure 10], implying that LR_find may not actually be a relevant hyper-parameter in this large-scale rewind-to-0 context. I still think more discussion of this point is also warranted when discussing results on ResNet-50 when rewinding to iteration 0.Regardless, I believe that the paper presents and thoroughly validates an interesting hypothesis, and I maintain that the paper should be accepted. ## Edit after discussionI thank the authors for the clarification. I still like the work and believe it is promising. However, I don't find too convincing the arguments around problem-space and malware experiments, I am afraid. Also, it seems there is still experimental bias in the evaluation (on malware), which hinders a bit the opportunity to assess the actual effectiveness of the authors' approach. I would really encourage the authors to reason about the points raised in the review in a more principled way. __________________________________________________________Post Rebuttal:Thank you for replying to all of my questions.. Plugging in your new metric to DARTS seems to be promising, especially if it alleviates the DARTS collapse problem. Given that the community is more interested in one-shot NAS algorithms, this might be worthwhile pursuingFrom the new plot in Figure 4 and NAS experiment in Figure 5, it is evident that the sum of training loss is able to rank the networks more effectively in the first 50 epochs. So one could use SOTL-E for early stopping rather than validation accuracy. This would also be effective in hyperband where the architectures are discarded after training them for very few epochs. Update after author response:I thank the authors for their detailed rebuttals as well as for engaging with the reviewers. It'd be worth adding a small section on how to scale this method for higher dimensional action spaces (bullet 2 in author response). Comparison with Metz et al (2017) is valid and appreciated. Based on this, I am happy to increase my rating from 7 to 8. EDIT: The authors clarified the presentation, gave a nuanced response to my concerns about the relative scalability, and promised to discuss the relationship to the actions-in architecture in the final version (it's not as central as I first thought given the limited role played by universal mixers). Solid work in its current state. --------After discussion:After reading the author's response as well as the opinions from other reviewers, I will stick to my original rating. The authors resolve most of my questions and concerns and I look forward to a revised version of the paper. After reading the authors' responses and other reviews, I would like to stick to my original rating to accept this paper. Though there are minor problems that I still have concerns (e.g., why not using re-sampling for all methods which is a better strategy than using fixed points), I am satisfied with the responses to my primary concerns about the paper.  -----------------Post-rebuttal reviewI carefully read through the rebuttal and other reviews and I would like to keep my original rating. The author(s) addressed my concerns and I think it is a good paper for the community. --------------after the rebuttal---------------I appreciated the authors' effort in addressing my comments and questions. I maintain my score of weak acceptance for this paper.  **After rebuttal responses**:I have read the authors response to my concerns, as well as the other reviews. I maintain my current evaluation with a weak acceptance of the paper.  ## Post RebuttalI have increased my rating slightly but still don't think the paper is ready for publication.  I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive). Post-Rebuttal Update: The response from the authors addressed several of my concerns and several clarity issues where fixed in the update paper. However, I don't think the results on ModelNet10 provide strong support for this method. While I don't think it is reasonable to expect this method to outperform other works which are specifically designed for mesh/point cloud inputs (given that this method is more general), I think there needs to be some application outside of super-pixel classification where the proposed method shows an clear advantage.   ---Post rebuttal commentsThanks for the responses. After reading these responses and other reviews, I still has concerns on the justification of proposed convolution compared with other popular graph convolutions, and also the limited experiments on superpixel image recognition. ---## Post-revision updateI cannot seem to add a new comment at this time, so I am editing this review instead. The updated paper seems to be an improvement, although some of my original concerns remain.Improvements:- Thank you for adding the additional baselines, which do give more context as to how this approach relates to prior work.- The section describing $c$, $f$, and $\Phi$ is clearer in the revision, and is much easier to follow.- I also appreciate the clarification about nondeterministic choices in Z3.- The overly-strong claims about outperforming state-of-the-art approaches have been qualified appropriately.- Some of the confusing details regarding the evaluation method have been moved from the appendix to the main text, which makes them much easier to find.Remaining high-level concern:- I'm still not convinced that the results of this evaluation method are that meaningful.  + NLL under a pretrained model doesn't necessarily imply better low-level structure, for reasons stated in my initial review.  + Ease-of-discrimination of your extracted constraints doesn't necessarily imply better high-level structure, especially since the proposed model is trained on those constraints directly.Other remaining issues in the revision:- A few comments from my initial review have not been addressed:  + "To train model (i), ... resulting relational constraints" remains unclear  + In section 4.1, $p_\phi(c|z)$ is referred to as "a VAE" but is just a small part of a VAE  + In section 5, the description of how A3 is applied to the music domain is missing  + Table 1 is still difficult to interpret, in particular regarding the higher-is-better vs lower-is-better columns, and the (new in this revision) bolding of the human data, which I don't quite follow.- Figure 1's caption now states that rhyme and meter constraints are the reason for the green and purple edges, but this is confusing because there are purple edges between poem lines that don't have meter constraints.- Regarding semantic content, the authors state in their response that "The semantic content of the poems (or lack thereof) reflects defects in the underlying deep generative models". But Section 4.2 approach 1 seems to imply that each of the lines are sampled independently of one another, which seems like a strong limitation that the underlying models do not have. Perhaps the notation is unclear, and the model does get the full input context; if this is the case, I would suggest revising the notation.I have raised my score from 5 to 6 in light of the improvements, and with the understanding that my concerns in "Other remaining issues in the revision" could be fixed in a final version of the paper. I'm still borderline on accepting the paper, however, due to my concern (shared with Reviewer 3) about how meaningful the evaluation results are and whether they match what humans mean by high-level and low-level structure. UPDATE: Based on the Author's response, I have updated my score. Update: I appreciate the authors response and, in particular, providing more explanations about eq. (3) (now eq. (5)). However, the explanations are still largely heuristic and based on unproven claims. To reflect the authors efforts to improve the paper, I am increasing my rating from 3 to 4. Update: After reading the other reviews/responses, I think there are persistent concerns with the breadth of experiments and the substantiveness of the contribution; although the manuscript is somewhat improved by the authors' updates, I'm keeping my score at 5. After rebuttal, I raised my score from 4 to 6. --------Update after the author's response:I was not convinced by many of the authors' responses.1. While the author(s) agree that $F(\lambda)$ is non-differentiable, they keep the gradient updates in the paper with a footnote referring to sub-gradient methods in (Boyd et al. 2004)? Moreover, it is not clear what the sub-gradient would refer too when the loss function is non-convex, what would $H_{\lambda}$ refer to in section 3.2? One possible way to solve this issue might be through a mild smoothing technique. But as presented the theoretical part is not rigorous.2. The positive definiteness assumption in Lemma 1, although will most probably hold when the three functions are convex; this limits the application of the results as typical ML loss functions involve non-convexity.3. My comment on the subscript of the functions $f_i$ and $g_i$ in Lemma 1 was addresses by adding a comment mentioning that the subscript will removed for the simpler notation. I believe one subscript does not make the notation complex. Rather defining two notations might confuse the reader.Despite my tendency to have this novel idea and work published, I believe the authors need to be more careful in writing and dealing with the theoretical section of the paper. I will hence decrease the score to 4. Thanks for the clarification, and for updating the main paper to address these concerns. I'm going to keep my rating for now (leaning towards acceptance), and will discuss with other reviewers later. # No RebuttalSince there is no rebuttal, I have not modified my score. *********** post rebuttal comment ************Thanks for the comments and clarifiactions.Just to add that: I agree that Rosenfeld et al (2020)'s work on general poisoning is concurrent to yours (as that part is expanded in sub-sequent updates to their Arxiv, not the original ICML paper).  And anyway the bounds of this submission are stronger. Yet, I think that a brief discussion/comparison with this aspect of the  Rosenfeld et al (2020) paper is helpful to the reader. UPDATE: The authors have removed the attention calibration study. My new score is a 7. This is a good paper and I heartily recommend its acceptance. After rebuttal :Thank you for the responses. I'm not sure why the authors didn't perform the experiments on the correlation between the previous factor-based disentanglement scores and the proposed disentanglement score in the limited supervised setting. For example, if there are 5 factors, I propose to evaluate the proposed disentanglement score for every possible pair of the factors (10 pairs) and average the scores. I believe this paper handles the valuable topic but it is not enough to be accepted since the experiments, which are crucial I believe, are omitted (comparison with the other disentanglement score, baselines to Delta VAE, comparison with [2]). Also, I concerned that other readers might be confused with the ("factor-based" disentanglement and "symmetric-based" disentanglement ) and (limited-supervision and weakly-supervision) (a new section should be added to handle these topics if this paper should be accepted). Furthermore, discussion with the related works is not enough.I lower my confidence rate to 3 (5->3) and vote for weak reject (3->4). But, I hope this paper would be accepted after revisions in the future. ### **Post Discussion Comments:** The authors provided videos of the task. While I still find that this paper misses clear benefits/use-case for the machine-learning community. I still don't clearly understand why one would need 54,000 parameters for a  3 parameter system. One could store a really big table of this one 1d system for this amount of parameters. However, the paper is well executed and clearly written in regards of technical aspects. The motivation remains doubtful for me. Due to the execution and clear writing, i increased my score to weak accept.   Update I have read the author's rebuttal, and happy to see that a discussion regarding parameters is added (Figure 9). Other than that, my personal concern is similar to Anon Review 3's -- it seems that the core idea of the paper is drowned in too many technical details (granted, many of these are needed in order to implement this correctly). I wonder if a clearer discussion can be made like this -- you have a variational inference problem with certain independence assumptions, so write this out in the most abstract manner possible. To come up with a concrete objective, the question then becomes "given the factor graph, how do we add the networks (this basically lines 11 - 16 in Algorithm but not in the flow of the main text)". I think a better presentation and clarity in the main paper would greatly help acceptance. ---------------------------------Post-Rebuttal evaluation.I would like to thank the authors for their detailed answers, especially regarding the interpretation of the gap function.Based on their answers, I decided to increase my score to a 6. -------------**I've read the authors' response. I'm still concerned with the novelty of the paper given there are similar results for EG/OGDA. Therefore, I stick to my original rating.** **Update after the authors' response**The authors have provided a comprehensive response that address most of my comments, and clearly clarified the novelty and key differences with prior work such as mBART (which also seems to be a key concern in the other reviews). Given the satisfactory authors' response, I am therefore raising my score to "7". Overall, I believe this paper presents a good contribution to the field. ----------Post-update:Thanks to the authors for your response. I deeply acknowledge the promising results achieved from your work, which is impressive. I still feel hard about the contribution, though I also acknowledge the difference and it is an excellent practical trick.  UpdateFirst I'd like to thank the authors for their detailed rebuttal. I have upgraded my recommendation from 3 to 4. As mentioned in my review I believe this approach is interesting. However, as pointed by reviewer2, the experimental section lacks completeness. I think this experimental section would be suitable for a workshop, but not a conference. I am excited to hear you are considering to use this method as an inspiration for real problems. I'd like to see the paper resubmitted when you have obtained such results. UPDATE: I thank the authors for their detailed replies and running additional experiments. This resolves my questions and I'd keep my recommendation to accept the paper. Post Rebuttal: I like this paper and would vote for it to get accepted on the merits of: (1) Their finding about how MDL can be an indicator of inductive biases of the models; (2) introducing an experimental framework for studying inductive biases of the models. I'd also like to appreciate the authors' efforts to address reviewers concerns in the rebuttal. I agree with reviewer #5, that the paper can be better contextualised in the related research area but I think the paper is improved from this aspect a little bit during rebuttal and this is something that in general can potentially be fixed for the camera ready version.  ########################################UPDATEThe paper provides important and novel empirical observations about the use of machine learning to perform mathematical reasoning. The authors have addressed and clarified some doubts about the originality of their idea and the reproducibility of their experiments in the discussion phase. I believe that the paper is now ready for publication and I'm happy to recommend for its acceptance. # Post-rebuttal updatesI've tentatively updated my review score from 6 to 7 for the following reasons:* The authors clarified the relationship of their proposed method with Squeeze-and-Excite, and promised to add an explanation to their paper.* The authors clarified the relationship between their method and CondConv in OpenReview comments, and promised to update the submission accordingly.* The authors reported additional experiments on ResNet-18 and MobileNetV3(small). These new experiments help increase my confidence that the proposed method is broadly applicable.* The authors provided inference time numbers for MobileNetV2 on CPU. AnonReviewer5 raised a very good point: that reducing the number of trainable parameters may lead to much faster models because of memory bandwidth considerations. I'm enthusiastic about this line of work, and think the inference time measurements provided in the rebuttal are a promising first step in this direction. ***************************************After reading the rebuttal and considering carefully, I think the authors' response addressed some of issues in my mind so I raised the score. However, in terms of theoretical foundation, the current paper draft is still only marginal, which requires substantial improvement.  ===== Update =====I have read the authors response, the updated paper, and the other reviews. I believe that the changes made by the authors address my concerns about the motivation and the lack of clarity; section 2 reads much better now. It seems like the main complaints of the other reviewers are in the lack of more difficult workloads, and the lack of theory. I personally dont find the lack of theory very important. I think the novelty comes from the simple observation, which no one to my knowledge has come to before, and the experiments support the idea empirically (which I think is what actually matters). I also find it a bit uncomfortable penalizing the authors for not running experiments on ImageNet, and I think the variety in architectures that the authors tried, compensates for this. I do agree that a more modern set of workloads (transformers, or even the same setup as AdamW) would have made the paper much stronger. I increased my score to a 6 because I think the paper in its current form is enough to get accepted, but there are still improvements that could be done to make it much stronger. =============Update avec rebutal and discussion with AC and reviewers.After discussing with the others reviewers and AC, I have come to share their concerns with the overall fragility of the paper. We agreed that the methods is sound and likely to work better than AdamW, the proofs are not sufficient. In particular, the authors should strive to provide experiments on different training sets (ImageNet) with learning rate cross validation. The authors do not systematically compare across learning rates which make it hard to interpret the results as being conclusive. In fact only CIFAR-10 is evaluated with multiple learning rates.The remark by Elya Loshchilov should also be adressed. Note that you cannot use stochastic noise as a justification, because for heavily overparameterized neural network, the amount of stochastic noise at the optimum is zero (i.e. perfect fitting of the training set). But there is another explanation: Intuitively for a fixed $\beta_2$, $v_t$ goes to zero as the current gradient goes to zero, and the ratio of the gradient by $v_t$ will converge to some constant, which prevents convergence. $v_t$ goes to zero at the same speed as the gradient but with a delay of $1 / (1 - \beta_2)$. If there is no convergence, then the gradients won't actually go to zero.The only way to prevent $v_t$ from going to zero is to have $\beta_2 \rightarrow 1$ (i.e. the previously mentioned delay going to infinity), but in that case $\bar{v}_t$ won't go to zero neither. This is only an idea of a possible justification and I would encourage the authors to think carefully about this stability issues in the next revisions.Finally I would encourage the authors the remove from the theoretical analysis parts that are not actually used (which I and other reviewers have noted). -----------------(Nov 26.) After reading Review1 I lower my Confidence.(Nov 29.) Taking into account the other reviews, the authors' responses to these reviews and the discussion, I now think the paper is not quite ready for publication and lower the score to 5. ------- UpdateThe response and update was a huge improvement. I now much better understand the goals, and the authors added some content that I think made the paper stronger and clearer. One outstanding issue is still that I do not understand why E_M[Q_M^pi] is the gold standard, and why bias is measured relative to that quantity. I explain this more below, but first mention some of the additions I really liked. For an upcoming paper, if this issue is remedied, I think this will be a good paper. Thank you for introducing Proposition 2 and the explanation about bias beforehand. This very much clarifies the motivation for using TD errors. The result itself highlights that using TDU will have a lower bias to the true expected TD error if the bias for the action-values for (s,a) and (s,a) are both in the same direction (both positive or both negative). Otherwise, however, it looks like the bias of TDU is strictly worse. One issue though with this result is that the magnitudes of these quantities could be different. The comparison between bias for TDU and the bias for Q seem a bit like comparing apples and oranges, and I would in fact expect the TD error to be smaller in magnitude and so naturally have a smaller bias. How much is due to this and how much to relative bias reductions? I suspect there is a real bias reduction here, but clarifying this would help.For the variance result, it seems better to directly report 47, and the discuss ramifications, rather than writing that they all need to be approximately equal. By the way, it is too bad that the result is a bit weaker for the variance, which is precisely the quantity you care about for defining your rewards. But nonetheless this formalization is helpful and provides solid insight.I like that the empirical work was improved, including adding some comments about significance. I also very much appreciate the ablation, where you use the variance directly from the bootstrapped action-values as an intrinsic reward. My only concern here is that the magnitude of rewards might be quite different, since the TD error should be smaller than the action-values themselves. This might mean different beta are needed.However, I remain unsure about the importance of this bias that TDU mitigates. You state: Our analysis shows that biased estimates arise because uncertainty estimates require an integration over unknown future state visitations. It remains a strong statement that uncertainty estimates require integration over unknown future states. As one example where this does not seem to be true is the Kumaraswamy paper you have cited. They show that if you use LSTD to get estimates of the action-values for a fixed policy, then you can get an estimate of the variance of the parameters. Maybe this setting assumes too much, and so it does not invalidate your result. But, I do believe a more clear argument is needed in this section for this result, as I expand on below.Methods that rely on posterior sampling under function approximators assume that the induced distribution, p(Q¸ ), is an accurate estimate of the agents uncertainty over its value function, p(QÀ ), so that sampling Q¸ < p(Q¸ ) is approximately equivalent to sampling from QÀ < p(QÀ ).  This is a strong statement. I do not see why it is true. Are you suggesting that the agent must have the true Qpi? Is it not enough to use uncertainty estimates (epistemic uncertainty) for a function approximator? This result seems to show: if we want to mimic uncertainty estimates over true action-values for different models, then this is not possible under function approximation. But, that is maybe reasonable. Instead, shouldnt we ask: how can we mimic uncertainty over approximate action-values for different models? (i.e., relative to our function class)Additionally, you call E_theta[Q_theta] an estimator and discuss its bias. But, isnt that quantity not random? I presume E_theta[Q_theta] is the expectation, and the difference to E_M[Q_pi] is the bias. But, then what is the estimator that is biased?Minor comments:However, in non-tabular settings that involve function approximators, obtaining accurate uncertainty estimates is almost as challenging as the exploration problem itself.  What does this mean?state-action transitions  what is that? I think you mean just transition, since you condition on the whole thingIn the proof of Lemma 1, the Variance would remove the expectation over r term. Your result still holds, but the proof itself looks like it should be separated into the two cases.While Proposition 1 states that we cannot remove this bias unless we are willing to maintain a full posterior p(¸),  It is not clear how this result shows this. What if I maintained a full Gaussian posterior over theta? Would that solve the problem? What is a partial posterior? Update: Thanks the authors for their response. Based on the other reviews and authors' response I decrease my score by 1 point.   ------ Post-rebuttal update ------The reviewers have addressed my concerns for the most part and I am happy to update my score to recommend acceptance. I also hope to see the A3C experiments in the final draft of the paper. ################################################Update after reading other reviews and author responses:I am happy to keep my score and remain positive about this paper; the authors have answered my questions and partially addressed my main concerns in the revised paper. Like Reviewer 3, I would hope to see complete results for A3C in the final paper. ## Update after rebuttalAll my concerns have been addressed, including the possible alternative explanation of the experimental results. I strongly recommend the paper to be accepted. ##########################################################################Post-rebuttal:I have read other reviewers' comments. Since the authors did not provide feedback to our reviews, I would change my score from 5 to 4. ** After discussions **I have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper.  =======================after rebuttal:I thank the authors for the response. I still have concerns re their comparison with GCKN (ICML-2020). The reproduced results by the authors are different significantly from the published one in Table 1 of GCKN paper (~5%). E.g. MUTAG is 92.8 in original paper, but the authors report 87.2 for GCKN. The difference is significant.Therefore, I will keep my original rating.  -----UPDATE: Thanks for the response, I have responded below and kept the score constant. ### Updates after rebuttalAbout the contributions of the paper, it does provide some interesting points of view.The authors revise the paper and modify their basic assumption. However, it seems to be a hollow assumption without any guarantee. The concerns are as follows: First of all, why the authors claim it as a diagonal matrix when actually observing it as a block-diagonal matrix? Secondly, considering the updated assumption is only tested on one layer of VGG and ResNet, I am really worried about its generality and reproducibility.Totally, the weight distribution assumption is the cornerstone of the paper, but the assumption seems to be not convicing. Specifically, the assumption in the first version of the paper is a paradox to some extent, and the revised version casually modifies the assumption without too much verfication. I have to decrease my rating to 4. POST-DISCUSSION: The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality (as opposed to theoretical rigour). I will thus raise my score to a weak accept. During the rebuttal, I concluded that this submission is highly confusing, rather misleading. I was led to believe the authors are in fact talking 'pure 16b MAC' - meaning 16b FP multiplies and 16b accumulate. After reading their responses to R4, I now learnt that they in fact are using 32b accumulates as is already standard practice in BF16 - If so, they have little or nothing new to offer. Rounding discussions the paper focuses on become highly secondary. BF16 is already well-understood and accepted. Their writeup was highly misleading to say the least. I change my rating based on this. They use a representative set of benchmarks which include, CNN-based Resnet, recommendation proxy DLRM, and NLP proxy BERT. Novelty is limited, but critical: They reiterate prior observations that accurate weight updates are more critical for maintaining accuracy than forward-backward gradient updates. Former is what suffers when round-to-nearest cancels out small updates. Stochastic rounding has also been published before and shown to still miss the accuracy mark with >0.,1% accuracy gap for some important benchmarks. Novel part of this work is suggesting a Kahan summation based software fix on top of stochastic rounding to overcome the accuracy loss.Primary issue: This is a very confusing and misleading writeup. Authors should have clearly spelt out that they are in fact talking about BF16 - which is well-publicized for years now as 16b mule and 32b accumulate - and not labelled it as 'pure 16b MAC' - which is already known to exist as well, and proven to be not sufficient for DL training. In light of this, this work has very little value-add. I change my rating now to 'clear reject' for their misleading writing style. ---------I have read the authors' response. The authors have addressed most of my concerns, but I still think the motivation is a little farfetched. Considering the paper indeed explorees some aspects (in theories and experiments) of the use of buffers in asynchronous Byzantine Learning, I will improve my point to 5. ***Post-discussion period comments***The authors have done an adequate job of responding to my queries and have also revised the paper in light of the comments of all the reviewers. While the paper could always be improved, I believe it is now above the threshold of acceptance and it should be accepted into the program, if possible. I am raising my score for this paper in light of the discussion and the revised paper. ----### EDIT: reply to authors' responseI thank the authors for their in-depth response and revision. The additional results comparing to pre-trained features definitely adds perspective, and it shows OTKE giving a very modest boost indeed, mostly over the weaker input features. The revision of the paper also improves the clarity, but my comment around Sec4 remains.I stand by my original score for the 3 reasons I had originally listed. Update after author response:Most of my methodological concerns have been address (except for the ablation studies).  The scientific application here is not super well-motivated.  It would improve the article greatly to show a greater utility (or at least, clearly describing future utility for answering scientific questions).  It is mentioned that "A full application of the method to study the dependence of contextual signals in mouse visual cortex will be the focus of a follow-up publication."  That's vague; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments. --- Update after author response period ---Thank you for the clarifications! After reading the other reviews and the paper updates, I still feel that the paper requires an additional, empirical evaluation to prove the value and contribution of the approach. Otherwise I find it hard to tell to what extent / in which situations / for which user group the tool is useful. While I appreciate that the authors made changes to their manuscript based on the reviewers' comments, I thus keep my recommendation for rejection for the current version of the paper. Thank you for the detailed authors' response, and for meticulously taking the feedback into account. The response has addressed most of my concerns.Some further comments:1. In Eq. 6, I think "up to $i \leq I/2$" should be replaced with "up to $i \leq (I/2 - 1)$", since $\lambda$ would be negative with when $i=I/2$. Other than this, the equation looks good to me.2. I look forward to the addition of the results with the "decay curriculum" into the main paper. It is encouraging that the proposed approach outperforms this simpler "decay curriculum" baseline.Since the authors have addressed most of my concerns, I am therefore raising my recommendation to a "6". Modified score: thank you authors for your thorough response. Given the new information and baselines, I think this is a promising paper that passes the acceptance threshold. Post-rebuttal:I concur with R1 in that the results look poor when taking into the account how close the proposed method is compared to CenterNet and that it was finetuned from a CenterNet. Therefore, I'll lower my rating to 6. --------------------**In the rebuttal, the authors resolved most of my concerns, therefore I increased my score to 7 as promised.** Post RebuttalI have read the authors rebuttal and updated manuscript. The authors have taken some steps to clarify parts of the manuscript, but my main concerns remain and thus my score is unchanged. In particular, the authors did not clarify how the two optimisation problems in Eq. 2. relates and what this means algorithmically. The authors defend their empirical setup by stating that meta-testing is still K-shot. While this is true, it is also true that their method have enjoy a greater amount of meta-training compared to baselines. What I would have liked to see is an ablation that trains the baselines for 2x meta-updates, but this is unfortunately not provided. Echoing other reviewers, if stronger attacks are considered, these should not be relegated to appendix.  Post-rebuttal: I'd like to thank the authors for the rebuttal and for pointing out some additional results that I had initially missed. While they're definitely welcome, it is still not clear to me why the main work is based on FGSM and why simple defense techniques have not been considered, both of which significantly weaken the manuscript.  I also see that very similar concerns have been raised in the other reviews , confirming that these weaknesses of the manuscript are quite salient.  ### Recommendation after Author Response ###I have read the authors response and appreciate the clarification by the authors. I was aware that other attacks (FFGSM, RFGSM, RPGD), were in the supplementary material. However, I don't see any reason for focusing on the particularly weak FGSM in the main paper. The main evaluation should always be based on the strongest not the weakest attack. Moreover, also RPGD was used as a transfer attack and as such is quite weak. In summary, I keep my recommendation of rejecting the paper. === EDIT: post rebuttal ===Thanks for the response and addressing the issues for a more serious research paper. ## Edit after authors' responsesI have upgraded my score (from 4 to 5) based on the clarifications provided by the authors and the updated manuscript. Please see the details in my extended comments: https://openreview.net/forum?id=ZcKPWuhG6wy&noteId=V7Wy0Mpsz7Q Post-rebuttal updatesThank the authors for the efforts in answering the questions. The responses have addressed my concerns about the robust training and using training loss to measure diversity. Exploring the data augmentation for robust training will be interesting. Besides,  training loss is usually sensitive to some other hyper-parameters such as optimizer and learning rate. So, investigating the robustness of this metric is also meaningful.Moreover, there is still a gap from applying the proposed metrics to training guidance. The current research mainly shows there are relations between the two metrics and data augmentation effectiveness.  How to merge them into one easily observable is still missing. Therefore, I keep my original score. -----After rebuttal:Thank you for the revision and the clarifications.It is now clear that this work actually proposed two different neurons: the horocycle neuron defined on H^n and the Position neuron defined on R^n (removing one point). After the revision, they are proved to satisfy the universal approximation property. They share similar level sets (although the density of the level sets is different). It would be interesting to see their relationships through formal arguments and a more careful empirical comparison.This work needs background knowledge in hyperbolic geometry and may not be easy to read at the beginning. That could explain the criticism regarding clarity. Overall, I believe this paper developed important tools along the line of hyperbolic deep learning and still recommends strong acceptance. Thank the authors for showing their effort in revising the paper.Some of my concerns have been solved thanks to the rebuttal.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  --- Update after discussion ---After looking at the concerns raised by the other reviewers and the author responses, I have the following comments:1. I think the that authors have more than adequately addressed the concerns raised by reviewer 4. With that said, I agree with reviewer 4 in that the link between theorem 1 and the proposed objective and its approximation is not made sufficiently clear in the text. 2. I whole-heartedly agree with reviewer 1 that a clear and concise list of the assumptions made would improve the paper.Overall, however, I think the paper should still be accepted and will keep my original rating. .UPDATE: I am downgrading my score to a 6. Based on the opinions of my fellow reviewers, it seems that perhaps the theoretical results are based on scenarios that are too simplistic for the community at hand. Moreover, there are clearly some readability issues, based upon the reactions of the other reviewers ***** post rebuttal comment *****Thanks for sharing the response. Unfortunately, the very basic issue with the definition used in this paper, and its implications to practice, remains unsolved.To clarify the definition, you just need to focus on the following simple example: what if the model has zero risk/error?  If you perturb a point, it would still be correctly classified. Yet, they still show that adversarial examples are inevitable even in this setting. This already shows something is fundamentally wrong with the definition used.Your response is that the attacker/adversary will not get to change the label, but only the features. But please note: the adversary *is not allowed* to choose the label. The adversary picks the features, and it is up to the model to correctly classify it or not. If an attacker changes the picture of a cat to to a picture of a dog, the neural net (or any other model) should call this dog (and if does still calls it cat it would be a a mistake not the other way around). The ground truth (i.e., the concept function) determines what is correct and what is not. The above issue is not imaginary, it has real affect on the experiments, and as I said, it is important to report in the experiments whether or not they attacks lead to *actual misclassification*.I hope these comments will help improving the paper, since as I said, the topic of this paper is a very important one, and so exactly because of this, it is important to have the basics right. --------- Update after author response ---------I thank the authors for their response. I have updated my score. I am slightly leaning toward acceptance though I think the paper might benefit from a revision based on some of the points raised by other reviewers, including comments about motivation for correcting the bias as well as the scalability (the proposed method requires storing n^2 additional values which is expensive for large networks).  ---Update: I have the other reviews and rebuttal. I am still leaning towards acceptance, although I do agree with the other reviews that there is little motivation for the idea. While the experiments show improvement, a discussion on convergence rates or variance could show why one should try this idea in the first place, and if \mu_* (rather than regular momentum) is indeed an ideal that the algorithm should approximate. ========================Final RecommendationI have read the rebuttal and decided to keep my score. I think this study needs to be further motivated.I also want to clarify a minor issue in authors rebuttal. In contributions, you say: "We demonstrate that alignment is an invariant for fully connected networks with multidimensional outputs only in special problem classes including autoencoding, matrix factorization and matrix sensing. This is in contrast to networks with 1-dimensional outputs, where there exists an initialization...". My point is that the matrix sensing problem that you study here has 1-dimensional output. #################################################################################### Updated:The authors' rebuttal addressed my concerns and I lean toward acceptance. **Post-comments to the author's response**- Replies for Q3 and Q4 are good.- However, the responses for Q1 and Q2 did not address my concern well. - For Q1, the authors claim that the representation capacity is in the CNN features not in the fully-connected layer. Then what is the meaning of utilizing the teacher's projection matrix for better representation learning? - Overall, I still this paper has more strengths than weaknesses, so I will keep my rating (6) === update ===The provided response is reasonable and helps address some of my concerns. I would keep my rating as acceptance. ### Recommendation after Author Response ###I have read the author response and appreciate the effort spent by the authors on this response. My main criticism was addressed and the author's feedback is very convincing. The authors have not yet added this additional content to the paper. Assuming they will include it in the final version,  I am confident that this paper will meet all standards of ICLR and recommend acceptance. I increase my score accordingly to 7. **Note**: I updated my review score after the original review was written. See comments below for details. -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Update after the author's comment:I appreciate the effort of the authors to add more experiments that all suggest that the ensembles tend to perform better in the small data regime. This makes the case stronger and the story more compelling, hence I raise my score. However, the paper is still missing the core explanations or a hint of why this may be happening, hence I still can not recommend the paper for acceptance. POST-REBUTTAL:I thank the authors for their response.At the outset, I am satisfied with the authors' responses to my questions - all the questions were answered. I do agree with other reviewers that the idea is incremental. Learning the prior across tasks is not very novel, as pointed out in references cited by other reviewers. However, on the bright side, the authors have done a good job in answering the questions, and the comprehensive experimental results are promising. The overall idea looks a bit incremental from the GAN literature side but maybe a good lead for meta and incremental learning literature.I change my decision to "Weak Accept". Update after Rebuttal:The authors provided clarifications and improved the manuscript. In particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. I am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. Based on this, I changed my evaluation and now suggest acceptance.  ##################################Update after reading other reviews and author responses:Thanks to the authors for answering my questions. However, my main concerns were not addressed, so I will keep my score.  **After Author Response and Discussion:**Thanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I think that the number of independent runs used (especially on the smaller domains), and the way the results are presented with the min-max extent makes me less convinced of the results than I was in the initial review. Adding many more independent runs (seeds), especially on the smaller domains, would improve my confidence a lot. Overall I think the paper is of interest to the community, but the experiments and their analysis could be improved. Update: Not all the experiments are particular thorough and the novelty less than expected. Post-rebuttal: I would like to thank for the detailed responses and the careful revisions made in the paper. Overall, the paper is now definitely improved in certain ways and initiates an important discussion on the value of episodic training & classifier synthesis for few-shot learning, as opposed to typically-simpler metric-learning based approaches. The paper also approaches this problem from an interesting point of view, by focusing on sample utilization in the episodic training of PN.  However, I still find that the the paper remains somewhat weak in its current form for the following reasons:- I maintain my view that NCA vs PN are not direct alternatives to each other, considering that PN allows learning a representation that is optimized for class-average to sample comparisons, whereas, NCA uses a sample-to-sample distance based loss. The fact that the very construction of these two models, despite the similarities pointed out, blurs the strength of the overall NCA vs PN based discussion on the value of episodic training.- The claims about the advantages of non-episodic training of NCA is mainly based on the observation that NCA creates more positive/negative labels. However, it is not clear whether it is the more efficient utilization of training samples or just the differences in terms of the predictive model formulations. (Perhaps, averaging based prototype computation is a bad idea, after all, which may not have directly anything to do with episodic training.)- To this end, Fig. 3 is indeed interesting, but again the results are not very clear. Here, careful optimizer re-tuning specially for each case can be necessary as subsampling degrades the gradient approximation quality, which creates the question how much fundamentally important efficiency in batch utilization is, as long as one uses a proper optimizer. Overall, I think the paper makes a valuable step in an interesting direction but the paper fails to make a strong-enough case. Overall, I  improve my rating by a single level to 4, but find that the paper is not stronger than this in its current form. [Post rebuttal]I am still leaning against the acceptance of this paper due to concerns about the limited impact of this submission. The topic of "limitations of episodic training" has been widely studied and in fact most SoA few-shot learning methods adopt a combination of supervised and episodic training for this precise reason. The exploration into the relation between the number of sample pairs and learning performance is indeed correct, but no strong conclusions can be reached due to the logical jumps required. The authors established that the performance is correlated with number of pairs with a log/square-root curve, and that ProtoNet performance is similar to that of subsampled NCA (fig3). The mechanism behind why this is has not been elucidated. I think many questions can be explored to strengthen this paper, for example:Are classes embedded tighter together? Are hard negatives pushed further apart? Does NCA induce a non-euclidean geometry that is more expressive than the euclidean geometry naturally induced by ProtoNets?Can the classification problem be converted into a pair comparison problem, so that PAC learning theory can be used to explain the shape of this curve? What is the sample complexity of the NCA classifier compared to the Prototypical classifier?Regarding the proposed method, NCA is certainly an improvement over ProtoNets, but performs worse than existing methods in most experiments. This makes me doubtful of the impact of this work on the methodological front. Better modelling of relationships between few-shot examples has been implicitly and explicitly studied too. For instance, many works adopt sample level set functions in the form of transformers, attention modules, and graph neural networks. Arguably, these additional architectures are more expressive "deep" alternatives to NCA, and hence achieves better performance than the proposed method. After rebuttal:Thanks very much for the detailed response! I do agree with the AC's comment that *I don't see a drawback the use of only one sample and no info about the target, rather I'd like to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair and still be competitive or outperforming them.* Also,  I would recommend the authors to have more experimental validation and resubmission. Thus, I keep my score. ---Thanks for the detailed response. I can agree that the observation function gamma in the form of addition can model many noisy signals. However, the argument in the paper that the proposed method works for an arbitrary gamma still lacks experimental validation. So I would like to keep my recommendation. My other concerns have been addressed. ----------------------------------------------------------------------------------------------------------I have read the revised manuscript and the comments of all reviewers. For concerns that the experiments are not sufficient to validate the proposed method, I am leaning to the author's rebuttals that the datasets have been chosen to meet the heretical assumptions of context-agnostic learning. The given datasets seem to be sufficient to demonstrate the effectiveness of the proposed learning method.Therefore, I will not change the rating. ***Post-rebuttal***I want to thank the author for addressing my concerns. However, I still have issues with the evaluation and the clarity of the paper. I think the paper requires another round of revision before it is ready for publication. [Post rebuttal]After more discussion and reading through the revision, I think this is a good paper and will be useful to the community for an instance of test-time defenses. ######################### post-rebuttalI appreciate the additional explanations and experiments by the authors. I also read the public discussion threads. I raise my score to 6. Two things for future:- Make it work on bigger and more realistic images, imagenet, pascal, coco, etc. Now the adversarial community and deep learning community in general, highly relies on experiments, because theoretical guarantee is still mysterious. So we should push the field forward, by proving ideas on harder datasets. - Explore stronger attacks, particularly gradient-free attack to avoid the obfuscated gradients. Given the rebuttal, I am willing to raise my score to a 6 due to the added StyleGAN, PGAN, and other experiments and improved paper layout / clarity. The added experiments are welcome addition to the paper and demonstrate this technique. The lip and eyestaining are interesting results and I do hope this direction gets explored in the future. =====POST-REBUTTAL COMMENTS======== I applaud the authors for the very detailed response and the efforts in the updated draft. Many of my questions were clarified.  However, there are still important questions remaining.1. In addressing my Q4, "We do believe that a separate discussion is needed on whether the registry or the user-end devices should retrain the models, given that users can potentially be the attackers in the malicious personation case. This discussion will entail an exploration of the tradeoff between training efficiency and training security of generative models, which is beyond the scope of this paper." This raises the feasibility question of decentralized approach proposed in this paper.2. In addressing my Q6, "We note that just like problem settings for adversarial attacks, there is a potential mismatch between our expectation of the attackers' capability and their actual capability. Therefore, we cannot assess whether these constraints are necessary, but we acknowledge that they are not sufficient." This needs more investigation.3. Additionally, in addressing R3, "Lastly, we agree that it is definitely interesting to understand which latent space the keys should be embedded. So far on FFHQ, attributability is achieved for 20 keys derived through the proposed method. More keys on the way (see wall-clock costs for computing keys and models in the supplementary). All of the keys share the same structure (eye shadows and lip stains), which may suggest a limit space for attributable keys. We polished the discussion on approximating the capacity of keys through an optimal packing problem, which is an open challenge." If the number of keys are limited, this seriously impacts the contribution of this paper.Overall, I am more positive. I am willing to raise my score to 6. However, the paper is still somewhat borderline. [EDIT AFTER DICUSSIONS] I thank the authors for their answer to my comments. I agree with the summary of the Area Chair and do not wish to modify my score.[/EDIT] Update: I thanks the authors for preparing a response and for providing additional experiments. I believe these additional experiments will benefit the paper. However, these experiments will require a full re-evaluation of the paper which, in my view, is beyond the scope of a response to a rebuttal. ==================================================================Post-rebuttal comments:I thank the authors for an extensive response and the other reviewers for brining up many relevant questions!= Main positive additions: LORL was successfully combined with another unsupervised approach, SPACE, on the CLEVR dataset.LORL+SA outperforms IET, which has the same amount of supervision. It does lose to NS-CL slightly, which has access to pre-trained object detectors.I like the added analysis of the QA types, data efficiency etc.= Some of the remaining issues:The positive impact of the objectness score on performance was not demonstrated. To show its benefit the authors had to propose yet another evaluation scheme (precision and recall of the reconstructed scene graph).The training objective for PartNet-Chairs should be discussed in the main paper, not in the appendix. Also, perhaps I am missing something, but would not one still need some negatives to train it?Minor: Fig 1 in the revision is still wrong, i.e. the example for PartNet-Chairs dataset still illustrates the QA task.Overall, I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited. The authors have considerably expanded their evaluation, but at the same time have introduced another confusion (as pointed out by R3 during post-rebuttal discussion): it appears that using 25% of supervision leads to lower segmentation performance, contradicting the main claim of the paper. I therefore decrease my score to 5. I hope to see an improved version of the paper (with more exciting technical contributions) in a future venue! ### Post RebuttalI have read the other reviews and the rebuttal. I appreciate the extensive revision and response of the authors. Indeed, several of the minor issues/clarifications that I had raised have now been addressed.However, as noted in my initial review, my main concern with this work is the highly limited novelty and the significance of the findings:* LORL is essentially a simple application of unsupervised object-centric representation learners (like MONet, Slot-attention) to the language-guided visual reasoning framework proposed in MAO et al (2019), where the pre-trained vision module is interchanged. As I have previously argued, the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules, which is not very interesting. Indeed, this score is not needed for segmentation (which is the primary measure of success that the authors have adopted).* The main finding, which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance, is not very surprising. Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations, the considered type of questions (and DSL) provide a sizable amount of supervision I believe (as is also evident from the observed fluctuations in Table 9).One issue that I noticed in the revision is when comparing the results in Table 9 and Table 10. It can be seen how when training on the visual reasoning task using only 25% of the provided data (i.e. 22.5K as opposed to 90K) actually reduces segmentation performance, i.e. from 83.51 (image only) to 81.01. This is surprising, and perhaps somewhat concerning, since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance. The authors do not provide an explanation for this behavior, while it appears to invalidate the main claim regarding the benefit of LORL in the general case.For these reasons I remain in favor of a rejection. ======After Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I very much agree with the authors that the problem is very interesting, but as of now more work needs to be done in terms of "downstream applications".  **Post-rebuttal update**The authors addressed most of my concerns during the rebuttal, added relevant discussions, experiments on Montezuma and clarified the reported evaluation metrics. I raise the score from 7 to 8. -------------------------------------------------------------------------------**UPDATE from responses**Thank you for your detailed response to my comments.The majority of my comments describing the weaknesses of this paper are related to both the conceptual and experimental components of the corridor example, so I want to recommend not including the example at all. It does not add clarity or provide any conceptual insights. This is in agreement with your stated primary focus, which is on demonstrating the empirical performance of a novel intrinsic reward design. Further, if you removed nearly all of the parts of the paper referring to the corridor example, you might have space to explain the detachment and derailment and the relationship of your method to these problems, as well as the other terminology that is currently not defined. It may be appropriate to include the idea briefly as motivation.**It is not clear to what extent short-sightedness is a serious issue and how much it affects exploration.**None of the papers have the term "short-sighted" in them, so at the very least, you are using a new name for a problem and then not defining it clearly. To the best of my reading, the problem you describe as short-sightedness is not described in these papers. Ecoffet et al. focus on derailment and detachment (which are different from the short-sightedness problem described in this paper) and while Guo et al. do describe myopia, their paper is not about intrinsic reward, so the type of local optima that they get stuck in are quite distinct from the optima created by the intrinsic reward design that the short-sightedness problem refers to.I am a strong proponent of using tabular simplifications to better understand problems, but in this case, the tabular simplification has not allowed for a sufficiently detailed understanding of short-sightedness. For this reason, I think it is important to look to the function approximation setting. In the function approximation setting, shortsightedness isn't clearly a problem. Your results in Table 2 suggest that RND doesn't suffer from short-sightedness in the same way that the tabular count-based method doesit does not spread out between the corridors evenly, which seems to be the behaviour you are looking for, but it doesn't get the short-sightedness effect either.**The pseudo-mathematical analysis in the appendix seems seriously oversimplified and flawed.**When I said that it was over-simplified, I meant that the assumptions that were made are inappropriate. I am not looking for more complex analysis; I am looking for analysis of an equally simple problem but one that is correct. The primary problem with the analysis is that it sort of stops in the middle. There is a great deal of set-up, and then the final point is simply a qualitative statement that if the growth of a quantity is proportional to how big it is, then the quantity that starts out larger will continue to get larger. The differential equations are not needed and not even used to make this point.1. While it is true that you do not state that the second-longest corridor will obtain the second-most exploration, I would like to understand what makes the choice of the second corridor to explore different from the choice of the first corridor to explore. From what you've written, it sounds like the agent is going to become obsessed with the longest corridor, and continue exploring it until it is "exhausted." If the first corridor is never exhausted, why are the remaining three corridors not explored roughly the same amount? If the first corridor is exhausted, why isn't the next corridor that the agent becomes obsessed with the second longest, since it is now the longest corridor that is not exhausted, making it a similar choice to the initial one. In this case, wouldn't the second-longest corridor receive the second-most exploration? While I understand that because the action is sampled from the distribution output by the policy network instead of taking max, the "exhausting" would not necessarily happen neatly, but it still seems that the Q-value of the second-longest corridor should become the second-largest, again resulting the second corridor being sampled the second most.1.3 I am not disputing that this could be thought of as getting stuck in local minima, but I believe that the idea that "alternately feature lower visitation counts" is a completely different problem from the long corridor preference (which may actually have higher visitation counts, but is more preferable due to accumulation).2.5 Since you do not know why your method outperforms the baselines, it is inappropriate to say that BeBold mitigates the detachment problem. 2.6 I'm sorry that I don't understand this response. Can you explain the relationship between reward clipping and dedication?3.3 The response does not address my concern. The primary problem is that the way detachment and derailment are described does not seem to be accurate. The example, "in which the agent gets trapped into one (long) corridor and fails to try other choices," partially addresses the problem of detachment, but does not appear to have any relevance to derailment. However, I am also of the opinion that for the authors to adequately support claims about these concepts, they must be described in enough detail for the reader to understand.4.2 I understand that a difference between RIDE and BeBold is this use of an embedding network, and I can see why you might hypothesize that it might affect performance, but again, I don't think there is any evidence given to show that this is the reason that BeBold outperforms RIDE, so it should be stated appropriately.6.1 While I am familiar with RND, the problem I am trying to point out here is that the terms appear in the paper prior to any explanation of what they are, which severely hinders readability.**Additional Feedback (Here to help, not necessarily part of decision assessment)**I recommend using the terms _target_ network and _predictor_ network for the two networks involved in RND, rather than teacher and student. Since target and predictor are used in the majority of the paper by Burda et al., they're more memorable for readers of the paper, so I think it will be less confusing.New Typos: "The results is averaged"  The results are averaged (p. 5) ## After RebuttalThe authors' response clarified some of the issues and partially addressed some of my concerns. Based on this and the remaining reviews I have decided to keep the score unchanged.One additional comment regarding the evaluation: In the authors' response they state "Finally, we would like to point out that it is common practice to use GCN as a subroutine for Meta-attack against different defense methods. This was shown in the original Meta-Attack paper, as well as multiple follow-up defense papers." I would like to again point out that the fact that this is a common practice is not ideal, even though multiple follow-up defense papers use the same strategy. We have already learned the lesson in the computer vision literature that adaptive attacks are the least we can do to evaluate heuristic defenses (see [1]) and even that might not provide strong evidence. ---------------------------------------------------------------------------------------------------------------Update after discussion:As stated in my initial review, I wanted to see an in depth discussion of what the implications of the binary tree assumption are for the types of transformations that are produced and the impact on the metric using the Jacobian. If these concerns are addressed, I think the paper will be greatly strengthened in the future.  ================Post rebuttal================I thank the work done by the authors during the rebuttal. However, my main concerns regarding novelty and experimental comparisons still stand - I think it is insufficient to say that your method is 'complementary to most if not all existing methods, so we could easily combine the proposed method to the others to further accelerate' without showing this or comparing to current approaches. The other reviewers have also brought up important concerns regarding experimental details which I concur with. # UpdateI thank the authors for their comments and answers. While they agree on some of the concerns I raised, others are still left open.I believe they could be addressed in new major revision of the paper and I encourage authors to do so. ################################################################################################Post-Rebuttal Update:I'd like to thank the authors for their updates, the additional experiments are especially welcome. After taking into consideration the responses and the new evidence, I believe the concerns I raised still stand. Therefore, I keep the previous rating. Post-Rebuttal:I want to thank the authors for their responses and clarifications. I think the revision already improved the quality of the submission quite a bit. However, I still believe that there are some aspects which need a better presentation and clearer discussion. For example, a more direct discussion and (empirical) comparison to other approaches like AnnealedVAE is necessary, as also other reviewers pointed out, to justify the points made (qualitatively) in the paper. The added results in figure 6c already provide results in that direction.  I appreciate the clarifications in the notions of action and action sequence. Although I agree that the notions are comprehensible in the toy example and dSprites setting, I still think that the point I raised in my initial review applies. In order to provide a well-defined notion a more formal definition is required. To me it is still unclear what an action sequence in the case of e.g. images of faces should be.I genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion. Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. However, I would like to encourage the authors to do a major revision taking the issues raised by the reviewers into consideration and to submit again.###  After rebuttal: First of all, I would like to thank the authors for all their effort on the rebuttal and the revised paper and I really appreciate that. After carefull discussion with AC and other reviewers, I would, however, have to decrease my score to 6 due the lack of significant technical novelty. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Post Discussion--------------After discussion, i have raised my score from 5 to 6. ##########################################################################I thank the authors for their responses. I carefully read them (revised manuscript) and also read other reviewers' feedback. The reviewers clarified some of my misunderstandings. However, I would like to remain at the current rating because my original concern about the novelty remains the same.  I raise up my score after the authors' changes [Update after reading authors' comments]The critical issue is resolved: it seems their method is indeed better than plain BO, which in turn is better than random parameters. Though I still have a little doubt about how practical it can be in real scenarios, I increase the score to marginally accept. ---------Post-rebuttal commentsThank you for adding the additional experiments and analysis. The results with the basketball dataset (Table 4, Figure 6) and the visualization of the latent distribution (Figure 5) address my inital concerns and showcase the versatility of VDM. I've increased my score from 6 to 7. AFTER REVIEW UPDATE: I find the revised version much improved explaining the inference model much more clearly. The lack of clarity was for me the main reason for evaluating the paper as below the acceptance threshold despite the fact that otherwise I found the paper to be good and useful for the community. As the lack of clarity has now been, in my view, resolved, I increase my score to 7 - Good paper, accept. -------------Post-rebuttalReporting PyTorch-computed FIDs risk the fairness when comparing against previous work. The repo quoted by the authors had a well-known issue, leading to much lower numbers compared to those computed by the original TensorFlow repo. Although this issue has been alleviated following some code update this year, the numbers still won't exactly match those of TensorFlow. Therefore, from a scientific perspective, the FID numbers have to be recomputed with the original TensorFlow code for a rigorous publication.In addition, the FID number reported by the authors for their model is computed between 10k samples and the test dataset, while most other FID numbers in Table 1 are computed between 50k samples and the training set (following the original settings of [1]). This also makes the comparison not fair.I completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality. However, since the authors choose to report FIDs and compare against those directly ported from previous work, they have to follow the convention and ensure a fair comparison. Since I didn't get a satisfying response from the authors (authors claimed to "have corrected it", they just changed phrasing of their response but didn't re-compute scores), I decide to lower my scores from 7 to 6. I am still marginally inclined to acceptance, but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation.[1] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. and Hochreiter, S., 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems (pp. 6626-6637). ===== Post Rebuttal UpdatesI thank the authors for responding to my comments. The work shows improved generative performance by using a multi-scale architecture, however the approach is the same that used in previous GAN works. Furthermore, the generation quality is not as good those of recent GAN works and I don't believe the added EBM benefits are significant.  In addition, other contributions, such as the use of the Swish activation and domain transfer has also been noted as used in previous work [1]. I also have additional empirical concerns over the experiments.I list additional comments below:1) The diverse inpaintings (Figure 11) do not really look very diverse to me and seems to suggest that the model is not learning a likelihood. To evaluate diverse inpaintings, it would be good to follow past work and evaluated on ImageNet images where only the top half of an image is conditioned. [3]2) The likelihood evaluation are hard to interpret in A.5. An issue with evaluating AIS based likelihood sampling on MNIST for upper and lower bounds of likelihood depends heavily on the large number of steps of sampling required.  Upper and lower bounds depend heavily on the number of steps of sampling run (with unrealistically high likelihoods obtained when running only a few steps of AIS). It seems unlikely to me that the proposed model obtains a significant boost to log-likelihood compared to past approaches, and it would be good to report both the number of AIS transition distribution (and ensure that it is same used in [1]). In particular, I believe that this approach is likely to perform poorly with a large number of gradient steps (as the rebuttal response noted), which is required for proper evaluation of likelihood. 3) The related work is still missing older work in the area of EBM training. Instead of adding additional references to recent work on score based generative modeling, I think the others should cite past works that have used score matching to training energy models. For examples, such works include [4, 5, 6].  4) I didn't find the out-of-distribution results to be a particularly compelling application of the model (although its good that it performs similarly to past approaches). The only results that appear to be better are uniform (which in my experience performance across all models fluctuates) and interpolation. Furthermore, the model from [1] used in section 4.2 is not conditional. 5) I wouldn't say this paper is the first to generate 512x512 samples with EBMs. For example see [3].6) It's difficult to evaluate an open source code release, since the code is not provided at the time.7) Regarding the approach in [1], when doing source translation images are initialized from ground truth images from a seperate class.[1] Yilun Du and Igor Mordatch, Implicit Generation and Generalization with EBMs[2] Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, Pixel Recurrent Neural Networks[3] Tian Han et al. Divergence Triangle for Joint Training of Generator Model, Energy-based Model, and Inferential Model[4] Jascha Sohl-Dickstein et al. Minimum Probability Flow Learning [5] Saeed Saremi. Deep Energy Estimator Networks[6] Hyvärinen, Aapo. Estimation of non-normalized statistical models by score matching. *AFTER REBUTTAL* I would like to thank the authors for their rebuttal. I increase my score to 5. I am still unsatisfied with the presentation of the idea, because it is still rather hard to follow. ----Review update after revision: my concerns have been addressed. ####EDIT####I am satisfied with the reviewers response and will raise my score to a 6.I think the authors should take care though in speaking practically about their work. The authors highlight the difference of 'excitation-inhibition mixture" in their response, which is indeed a mathematical difference to the existing work but, at least in the context of neuroscience, I am not sure if this serves any scientific purpose. Do the authors believe this has biophysical meaning or bears a relationship to neural coding? I do still think some mention of GLMs will be useful as context for models that also infer neuron-neuron E and I interactions.I appreciate the authors efforts to explicitly highlight the advantages of this work as compared to Apostolopoulou 2019. **Update:**Thanks to the authors for their detailed response to my review. Unfortunately after reading the response, I don't understand how it addresses some significant concerns I have about this paper and therefore I can't increase my score. In particular:- Author response says "Measuring the transferability from $M$ to $M_{adv}$ is actually the opposite of our concern, since our threat model deals with the transferability from $T$ (the protected augmented model) to $M$ (the target model)." I'm suggesting defining $T$ as a vanilla model and $M$ as a PGD-trained model. I don't see why your threat model would preclude such a choice.- Author response says "$M$ is hidden from the adversary. Then, the adversary does not have access to $M$'s output, and cannot mount a black-box attack to thwart a binary classifier $M(x)/T(x)$." The adversary doesn't need full access to $M$, only access to the output of the binary classifier $M(x)/T(x)$. Presumably they have at least indirect access to the binary prediction; otherwise the defender would be able to detect adversarial inputs but wouldn't be able to take any meaningful action based on the detection  (e.g. denying access to the attacking user, etc...).There is still a possibility that I've misunderstood something fundamental, so I leave my review confidence as fairly low. Rate updated 30 Nov.Rate updated 30 Nov. final recommendation:after looking at the other reviewer's scores and exchanging some more information with the authors (thanks for the great exchange btw), I would maintain my score of 6. I feel this line of work is definitely useful for program synthesis, especially in the analytical aspects (using adversarially generated examples as benchmarking tools and as representative explanations). the comparisons against other genetic programming approaches, I feel, is not too necessary once the paper is framed as "using adversarial examples to instrument / analyze synthesizer behaviours".  ---=====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified. However, unfortunately, I still think more needs to be done to demonstrate the effectiveness of the proposed model on large dataset and analyze the effectiveness of each module (binary NAS and CMP). I keep my original decision for these reasons. *****   Post Rebuttal  *****Thank you for your clarifications! After reading the rebuttal and comments of other reviewers, I am increasing my score.   --------- After rebuttal ---------Thanks to the authors for the response and updated paper. I keep my original score and recommend acceptance for this paper. Update:  Thanks for the detailed response.  I appreciate the additional figures and other results that you have provided.  However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.  It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.  Additionally, there's some limitations to the way the language model is being leveraged and the types of knowledge it can extract.   I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews. Thank you for your through response.The answers 2 to 5 have clarified all of the respective questions I had made.For question 1, though your answer was clear, I still have slight reservations with respect to the novelty of the paper with respect to GNODE, even after considering your response here, since the contributions described in the answer amount to a simple (though effective) modification to the previous method and a new motivation. Nevertheless, despite the possibly incremental updates with respect to previous work, your results are still stronger than the ones seen in GNODE. Moreover, the other reviewers seem to be content with the novelty of the paper as this was not an issue brought to attention by the others. Therefore, taking all of these factors into account, I will update my score to a "marginally above acceptance threshold". #### Post Rebuttal(Copying from the discussion below)I would like to thank the author(s) for their response. After going over them, I am still not very confident about the paper would stick to my initial assessment. Following are my primary concerns:"We note that there is a distinction between wanting to see something from another point of view, versus wanting to answer a question from another point of view. The former is where re-rendering is appropriate, but we do not make the claim that this alternative (view rendering + VQA) performs better or worse empirically."I understand the distinction. But the issue still remains. Why is the out-of-the-box "view rendering + VQA" solution insufficient? Is there any empirical justification for it? If not its hard to see the value in the current setup. A potential way to address this could be to run a simple out-of-the-box "view rendering + VQA" baseline."(2) R3 and R4s concern about camera information being provided to the model and its potential infeasibility in practice: In real world settings, camera rigs can and do have knowledge about where they are situated in the world, for instance using SLAM or GPS coordinates. In that case, it is not unreasonable for e.g. an autonomous vehicle to answer queries by performing rotations and/or translations of its current viewpoint."The concern was not about the viewpoint of the observer but the new viewpoint from which the question has to be answered. Also, the location of the new viewpoint need not be converted into float and appended to the question. It could be expressed in natural language. For example "viewpoint of the driver in the other car" like in the example provided by the paper. In the current setup the information about this viewpoint is provided in terms of exact coordinates, which makes the setup less interesting and not so practical.Although the authors improved some of the figures, the latest version of the paper does not seem to address other clarity concerns like a clear section for the dataset; organization of text and figures; removing unnecessary equations ---Post-rebuttal reviewIn my initial review, my main concern was the motivation for the patch-based classification is unclear, and I asked some questions related to the further potential usage of the proposed method, particularly focusing on robustness.During the rebuttal process, the authors address most of my concerns well in their responses.- Main motivation: this work more focuses on the mathematical analysis of the image classification tasks, rather than performances (accuracy, runtime, ...). It makes sense to me, and I think this motivation needs to be encouraged to explore by many researchers.- Runtime or learnable parameters: tracking the runtime comparing to deep methods is non-trivial as the author clarified. However, the authors showed that the number of learnable parameters is much less than deep models (580k for the proposed method, a few M for AlexNet). I think this comparison fairly shows that the proposed method is efficient than deep models in terms of the number of parameters.- Other analysis that can support the motivation of this paper: I'd expect to see more robustness analyses such as the black box adversarial attack results, but I agree that this is out-of-scope of this work. To me, the responses to the additional comments are not helpful, but I understand that my questions can be out-of-scope of this work.Hence, I'd like to change my score from 5 to 6. ### **Rebuttal Update**I thank the authors for updating their draft. I think this paper is worth accepting due to its ease of use and many different features, but I will keep my current score at a 6.**Here's why:**My experience in these types of benchmarks have usually shown that **completely static** backgrounds (when cleanly used - there can be very subtle things that can still correlate with progress) do not actually affect generalization, but rather, anything correlated with progress will cause overfitting. The difference between this benchmark's background and e.g. a ProcGen/CoinRun's background is that CoinRun's background still moves when the character moves (which implies a slight correlation with forward progress).From the authors' responses to Q 3.2 and Q 3.3, I believe that the main visual overfitting is occurring with the floor tile, as it is the only spurious object that is correlated with progress. This is because, as the authors have stated, the actual background image does not actually change when the agent is moving as the background is too far away. The authors also reinforce this aspect when they added the saliency/spatial attention maps in Appendix A.1, which shows that most of the attention is focused on the floor tiles, especially on test environments.This means that I suspect that the randomized static background portion of the benchmark does not affect generalization, and I urge the authors to rethink this portion of the benchmark.  *Post Rebuttal*The authors have mostly addressed my concerns - I still think SAC should not only be the only RL method in the paper, and though I'm still not convinced more drastic camera variations wouldn't make this more interesting I think all in all this is a decent contribution and probably should be accepted. ---In the revision, authors provided detailed clarifications on their approach and practical utility of the proposed method. Therefore, I am changing the rating of my review. EDIT: The authors addressed my main concerns, fixed a crucial bug in the claim/proof of Theorem 2, and added many more clarifying details to a revised submission. Therefore I will keep my rating the same -------------As a result of the feedback, the part I was concerned  became clear, so I would like to raise the score. -------------------------------------------------------------------------------------------------------------------------------------I appreciate that this paper has some merits. But I lower my rating because of the limited usage of the proposed method.It seems it cannot handle high-resolution nature images. Most of the experiments use constrained images e.g. biomedical images or images with small resolution. **Post rebuttal comments**I thank the authors for the detailed response. I think that some points have been clarified and corrected, and I have increased my score slightly to reflect this. I still think the paper needs another iteration before publication though. *****Post Rebuttal*****I would like to thank the authors for the rebuttal. While I think more discussion / comparison to approaches without an explicit independence prior would further improve the paper, the authors have clarified many of my doubts. I have therefore decided to raise my final score. **Updates from Author Feedback**While I would have liked to see a draft with the changes, I feel reasonably sure the authors will improve Appendix C to make the statements mathematically precise. I am confident in the statements and proofs. While the presentation can be verbose and casual, I think it is justified to increase accessibility, so long as the proofs and statements are formal and precise. Based on Author responses I have increased my confidence. POST-REBUTTAL COMMENTS:=========================Several of the original concerns, mainly about the scope of the contribution, have been addressed by the rebuttal, so I've increased the overall score. See my responses to the rebuttal for the concerns that remain outstanding.========================= **After Author Response and Discussion:** Thanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I agree with the other reviewers about the experiments: they feel rushed, and the design and presentation is not as careful as it could be. I think the paper could be greatly improved by a little more attention the presentation of the experiments and their analysis in the main text. ################################################Post-Rebuttal:Thanks the authors for their detailed responses! The authors responses addressed most of my concerns. In particular,1. the authors showed that their results generalize to Tiny-ImageNet and ResNet50 with additional experiments. 2. the authors added some discussions on the theoretical side. Besides, I also appreciate the addition of experiments on AugMix and PGD which imply the bidirectional causality of  class selectivity and perturbation robustness. Overall, I think this work will potentially be a good addition to the existing understanding of trade-off between worst-case robustness and average-case robustness for the community. Therefore, I decide to increase my score to 6. Update:Thanks the authors for their response. All my concerns are addressed and I decide to increase my score from 6 to 7. #########################################################################POST-REBUTTAL RESPONSE:I read the author's rebuttal but have decided to not increase my score. I still have doubts over the claims in this paper.  score: 5 -> 6 **After rebuttal**I'd like to thank authors for their efforts to address my concerns. They have addressed most of them, so I increased my score from 5 to 6.However, there are two concerns that couldn't be resolved during the rebuttal period:(1) I am still not sure if the proposed task is practical. At glance it looks realistic, but I couldn't find a detailed scenario that can only be solved by the proposed task. Any real world scenario I can think of is closer to [Lee et al.], which is a prior work of this paper. Authors provided an exploring robot example in the thread of responses, but I think [Lee et al.] fits better for the provided one. I recommend authors to find a concrete use-case in real-world applications, which can only be solved by the proposed setting (or at least [Lee et al.] is not applicable; in the revised intro, you may emphasize that there are some real-world problems that [Lee et al.] is not applicable but yours is). R1 and R4 seem to have a similar concern.(2) the scale of experiment is too small. As CIFAR-10/100 have a limited number of data for your purpose,  you can borrow some data from tinyimages (FYI, CIFAR-10/100 are a subset of 80M tinyimages) or focus on ImageNet.I am okay with the lack of novelty on the proposed method. For a newly proposed task, I think proposing a simple and effective baseline is good enough. However, because of the two concerns above, I cannot strongly agree with its acceptance. Feedbacks after author response:I am maintaining my rating after reading the author response. It is a close decision. I like the topic, but I think the draft still has room for improvement to become a great paper. The updated draft is much clearer and answers some of my questions. Most importantly, the updated theory section explains why small beta can be bad: it slows training. While I appreciate the clarification, I think this argument still doesn't fully align with the experiment results. As the CIFAR-10 experiment points out, using small beta can still lead to good performance, so the main problem for small beta seems to be instability (rather than slow training), which the current theory couldn't explain. To improve the theory, I hope the paper can provide more insights into the instability caused by small beta. For example, I wonder if this is somehow connected to the slow training argument; perhaps the failed runs indeed suffer from slow training.  === EDIT: post-rebuttal ===I thank the authors for their patient and detailed replies regarding my concerns. I noticed that the authors have addressed the concerns to some extent in the updated paper (e.g., the remark in Conclusion). I'm also aware that the method can achieve the machine-learning goal of transporting a reference distribution to the data distribution, regardless of my concern. So I raised my score by 1 point.Nevertheless, I still feel uncomfortable to give a positive score. The current presentation of the motivation may confuse or even mislead the community. The authors present the Monge-Ampere Eq. (2), which solves for the _optimal_ transport from the reference distribution to the data distribution. But the method is constructed by simulating the gradient flow of f-divergence. Although the resulting transport serves for transforming the reference distribution to the data distribution (since the gradient flow minimizes their f-divergence), the transport is unnecessarily _optimal_ (there may be multiple transports to transform the reference distribution to the data distribution; the gradient-flow transport is one of them, which may not be the optimal one that solves the Monge-Ampere equation).For the authors' reply "Our method is based on a first-order approximation to the Monge-Ampere equation", I think the "Monge-Ampere equation" therein is different from the original one (Eq. (2)). If the method, which is constructed from simulating the gradient flow of the f-divergence, is to be treated as a first-order approximation to some Monge-Ampere equation, then the Monge-Ampere equation is between two adjacent/neighboring distributions on the gradient flow curve, but Eq. (2) is between the two ending points of the curve (i.e., the reference distribution and the data distribution). The two Monge-Ampere equations have different solutions unless the gradient flow coincides with the geodesic on the Wasserstein space, which is unnecessarily the case.I agree that "An important advantage of the proposed approach is that it allows general energy functional in constructing the gradient flow from the reference distribution to the target distribution". But I think it does not have much to do with _optimal_ transport or the Monge-Ampere equation. This thought works since the gradient flow minimizes the energy functional, whose minimum is achieved only when the two distributions coincide (when the energy functional is a proper divergence/discrepancy). Post rebuttal comment: Having read all the reviews and in light of the additional experiments, I am slightly raising my rating. The reason I am still not fully convinced is no experiments indicating the quality of descriptors learned by the method.  Update: I appreciate the detailed replies to my questions. Indeed, some of the points I raised were addressed well and the paper updated accordingly. However, some new concerns were also raised by the replies:- Using 3 seeds for the experimental evaluation is an extremely questionable evaluation protocol. There is no way to know if any of the results are going to hold up. - It's also clear now that none of the experiments are comparing to benchmark numbers from other publications. It would have been more confidence inspiring if the method was tested on a set of tasks where external benchmarks have already been established. - This is particularly true for the new results that were added to the paper, e.g. the QMIX results. It's difficult to make sense of them and the instability points towards a potential hyperparameter issue.   - All baselines for the 'prisoners' case should at least compare to the fully cooperative case of adding up the rewards. Comparing to a DQN baseline that maximizes individual rewards is a red herring. - It's odd that all experiments require less than 1000 episodes to train. This is very unusual for challenging multi-agent RL problems. It would be great to understand if the main selling point of LToS is sample-complexity/learning speed or if there is something else going on.I also agree with the concern raised by other reviewers that the paper is currently not positioned clearly. All things considered, I believe my score is still appropriate for the paper. However, I also believe that a future version of the paper with clarified positioning and more thorough experimental evaluation could make for a compelling contribution. ---Thanks for the response. I've increased my confidence.  --------------------------------------POST-REBUTTAL COMMENTSAs a result of the discussion the paper has improved, so I'm increasing my score. However, the core issue, that the proposed benchmarks don't seem to capture the difficulty structure of either real problems or more complex benchmarks, remains.-------------------------------------- Considering the modifications made on the paper, I increase my score I have read the author response and stand by my original score of the paper. [Post rebuttal] The rebuttal has not address any of my main concerns, so my rating stays. POST-REBUTTAL COMMENTS========The authors did not make a good effort to address my comments and failed to update the paper. Therefore, I maintain my original decision. **After Rebuttal**The author's reply partially resolved my concerns, although the diversity of models has not improved, nor has it significantly decreased. Thus I have increased my score from 3 to 4.  After rebuttal-  I read the response of the authors. The spotted typos are fixed in the revision. Some  questions/concerns  have been tentatively. However the novelty in the paper is still not blatant or how the use of distance such as MMD or Wasserstein to match the features is under-explored. Hence I intend to keep my rating. --- after rebuttal ---Thank the authors for their response which addressed my concerns. Based on the response, the revision, and other reviews, I would like to keep my score unchanged at 6. ### POST-REBUTTAL UPDATEThe authors' response did not address my concerns. Given that most current evaluations metrics for video generation/prediction have some shortcomings (including human evaluations), it makes more sense to include a wide range of metrics that showcase the strengths and weaknesses of a method rather than to argue against their inclusion in the paper. Additionally, the authors failed to mention very relevant prior work (Latent Video Transformer). Therefore I do not think this submission in its current form should be accepted and I have reduced my rating to a 4. --- post rebuttal update ----I appreciate the authors for their response. However, the arguments on FVD are still not quite convincing (i.e., FVD still has reasonable correlation with actual generation quality; if the perceptual metric is inappropriate, then the authors should have tried other metrics. Also, the authors did not address other concerns such as concerns on computation time, scalability, small-scale human evaluation, etc. I maintain my score to rejection of this paper. Review update: I thank the authors for their response and for revising the paper based on the comments. The revision addresses several of my concerns. I still think the theoretical guarantees should be stronger and therefore change my score to borderline 5. ***After author response***Thanks for the response. Perhaps I didn't explain my concern about the motivation. SVM with uncertainty sampling works well, yes; however, it is equivalently several different things including selecting points closest to the decision boundary in Euclidean distance but also selecting points with highest predictive entropy and selecting points with smallest predictive margin. In other words, it seems that you are extrapolating from the SVM case to the neural network case and it's not clear which of several equivalent things will work in the neural network case.Furthermore, this paper doesn't even use Euclidean distance but instead uses a newly defined distance. So I really don't find the motivation or theory convincing. -------------------------------------After rebuttal:Thank you for the dedicated consideration of my comments, but there are a few remaining concerns that are not clear.1. The editing effects of edge maps are not distinct from those of segmentation maps. More specifically, except for the background in Fig.3 and the second face in App.D, most of the examples demonstrate the same kinds of manipulation as shown in the samples of the segmentation map manipulations. This includes moving, stretching, and erasing the objects. I think that the qualitative results of edge modification are not sufficient to prove its effectiveness, compared to those of segmentation maps.2. As shown in the image segmentation video the authors provide, a user needs to segment every single object which are selected for the manipulation. Moreover, segmenting small and fine objects requires further elaborate and laborious annotations from the user, resulting in a critical bottleneck for practical use.Due to these concerns, I would keep my previous rating of 6. Marginally above acceptance threshold. ----------post-rebuttal----------I appreciate that authors have provided rebuttal that addresses some of my questions. I've read the updated paper and other reviewers' comments. In general, I'd like to maintain my initial rating as a borderline paper. Here are two main reasons.First, I realize that authors are not familiar with the policy, because they did not attach the appendix to the manuscript but uploaded as a separate file. As a result, their updated paper did not address issues in the review effectively. Authors simply say something like below - "We promise to improve our writing based on your suggestions in the revised version."- "Due to the space limit, we put our related experimental details into Appendix C." (9 pages are allowed by policy)- "We will supplementary the results of Pani Mixup(+hidden) to justify it if accepted."Second, in terms of other data augmentation, the authors merely say "As data augmentation is a common trick, consistent improvement can be easily expected across all methods". I don't know if this is true unless there is a justification. ------update after rebuttalAfter reading the author's response, the authors stated that they indeed identify the optimization difficulty and consensus distance in theory, while only empirically justify its generalization on training performance. As also pointed out by reviewers 1 and 3, the gap between the convergence rate/consensus distance and the generalization capability still exists, causing the mismatch between the theory and the simulations. But at the same time, the work can also serve as an initial good start and raises good points for the literature. I will keep my score unchanged. Update: I appreciate the response to address my concerns carefully. My major concerns were the lack of novelty and some unclear descriptions on details in methods and experiments, but most of them have been well addressed. At first, I didn't see the technical challenges when applying DKL or any multi-task GPs into hyperparamter optimization. After reading the response and the manuscript again, I'm convinced the task augmentation plays a critical role in this setting. And, additional information from the revised manuscript helps to understand the details in method and experiments. So, I increased my score by two points. **Comments after Author Response**I thank the authors for their response. The explanation of the main approach has certainly improved and the details of task augmentation and warm start are much clearer now. I also appreciate the added discussion on bias in Stochastic Gradients for GP training. The reference cited does seem to indicate that the training will converge despite bias in gradients. The warm start and task augmentation approaches still seem a bit heuristic to me. The task augmentation approach seems to assume an inherent linearity in errors/noise for the metric of interest which may not hold in practice. The choice of loss function for warm start also seems rather arbitrary. It is not entirely clear why choosing $\mathbf{X}^{\text{(init)}}$ that minimizes (12) is a good idea. However as my main concern about the validity of SGD in training the model has been addressed and both the task augmentation and warm start approaches seem somewhat intuitive, at least at the idea level, I am increasing my score by one point. ------------------------------------------------------------------------------------------------------Update after author response------------------------------------------------------------------------------------------------------1) Context and content attentionThank you for updating and saying that only some of the implementations include bias! I think now this part is not misleading and can be of interest. I still have some concerns that this part does not fit the whole story very well - but this is the matter of taste. In the current state, I think it is ok :)2) On the comparison with head pruning and on the paper going beyond practical realm.I agree with your comments, but I do think you should make it very clear in the paper. In the current state, the paper tried to make practical contributions and, since they mostly do not hold (e.g., head pruning is simpler in practice), it's hard to appreciate the paper's value. I think you need to modify the things you highlight, and with proper discussion it would be much better. For example, if you state explicitly that in practice pruning may be simpler, but your results say/illustrate something other than practical applications. You won't lose because of it; in fact, I think the opposite.Overall, I think the paper has improved during the discussion period. In a hope that the authors address my later comments and discuss the pruning in the text, I'm raising the score.  Update after author response =====If you look a deeper look into Transformers, in case of BERT, the attention block only takes around 25% of total parameters. It is that suspicious that collaborative MHA takes 18% less time in practical since it requires many factors e.g., GPU kernel fusion.Regarding the performance on MNLI, from Fig 5, it shows that when D_k is larger than 512, MHA reaches to the baseline in terms of accuracy. Additional information from Tab 2, these models have more than 101.4M (in case of D_k = 384) which is almost the same as the original baseline. However, the performance on GLUE is dropped largely, thus it is hard to support papers' claims. -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing my questions and concerns.I have now updated the score. Update: I have read the authors responses and am happy that they have addressed some of my concerns in my review, especially comparisons with previous works. However, I still have the concern that the improvement seems very minor compared to previous works. Thus, I decide to keep my score unchanged. Post-rebuttal:Thanks for the feedback. Some of my concerns have been addressed, but I am still worried about the consistency issue and how to handle the large-scale problem.  The authors' response addresses a small part of my concerns, so I change my score. However, I still recommend rejecting this submission (Details are shown in my response). UPDATE:After the reviewers clarifications and some further explanations of the implications of Theorem 2 (in Appendix E) I think now that the paper tells an interesting story and thus I will vote to accept. ----------------Post rebuttal-----------The issues I raised still persist in my mind. Also, other reviewers have similar issues and also more issues other than what I point out, which seem valid issues to me. I will keep my score as is. ====================================================================================**Post-Rebuttal**After reading the rebuttal and the other reviewers' comments, my concerns persist:- The technical contribution is limited. Adding a pretext task of homography prediction itself brings little insights regarding how it improves upon contrastive representation learning from a different perspective. - The experimental results are not convincing compared to the recent advances. The authors are encouraged to include ImageNet results as well as transfer learning evaluation.Therefore, I would like to keep my initial rating as rejection. Update after rebuttalI want to thank the authors for a long and highly detailed rebuttal. They clarified a lot of my questions and hopefully in the process they were able to improve the paper. However, my listed weaknesses still stand:there is no transfer learning experimentsthere not seem to be any consistent gains with the proposed approach over other methods, as seen in Table 4, after 500 epochs (when the models indeed have probably converged). Gains can be seen for 100 epochs, but from the absolute numbers it is obvious that models havent really convered at that time. Furthermore, as the authors clarified, they do require 30% higher training time, something that make the claim that they learn faster even weakerThere is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results.I will therefore retain my rejection rating. ----post rebuttal----I am downgrading my score as the authors did not address most of my concerns. The paper can be further improved with the suggested experiments as well as discussion on distributional uncertainty but the revised version does not appear to contain any of these suggested changes.  ----- i've upated my rating after the authors' response. ####################Post Rebuttal#######################################I meant tiered-ImageNet (https://github.com/renmengye/few-shot-ssl-public). I apologize for the mistake. I think the rebuttal has not addressed my concerns, especially the one related to the 'ill-defined' tasks. The 'ill-defined' tasks detected from the proposed method could be 'hard but good' tasks benefitting the test. The authors didn't deny this. And the proposed method tries to push the model to learn less from such tasks. A natural deduction is that it could affect the classification performance. I think if the authors can take a further step to disentangle uncertainty estimation and classification, this con of the proposed method can be removed in the future.  ---------------------Updates after author response---------------------The authors basically posted their response at the last minute of the window which eliminates the possibility for further discussion. While it is lengthy and point-to-point, I found it failed to clear up any of my concerns.I am very disappointed that even after they recognized the misleading arguments in "convergence" analysis, in Theorem 3 of the revised version, they are still claiming $\| \nabla F(w_R) \| \leq \epsilon$ when $\delta < O(\epsilon^2)$ as "convergence". Such conclusion will be EXTREMELY MISLEADING if readers missed or did not carefully think about the condition on $\epsilon$. Authors may want to refer to a calculus textbook for the rigorous definition of convergence.Regarding technical contribution,- Authors acknowledged that their analysis does *not* guarantee convergence to stationary point, since there is an additive $O(\delta)$ term in the gradient upper bound. This immediately diminishes their theoretical contribution.- Authors did not justify their technical novelty. In fact, if we carefully look at their analysis in the appendix, it follows from standard SGD with a slight adjustment to the biased gradient induced by label smoothing (which is acknowledged by the authors). This is an additive gradient and thus is easily controlled; it is also the main reason why in their main theorem, the gradient upper bound suffers a non-vanishing $O(\delta)$ term.- Authors argued that compared to running SGD from scratch, the benefit of LSR+SGD is the introduction of $\delta$. However, this quantity itself is out of control. Note that even they were able to show $\delta=O(1)$, it is not strong enough here since this can easily be obtained by running SGD. The only way that I see will save the paper is to show under distributional assumption of the data, that $\delta = O(1/d)$ for example. However, I did not see how to make it happen, and authors completely ignored such analysis in their response.Overall, this is a paper playing tricks on its technical parts. It is decorated with bunch of mathmatical analysis most of which is known and standard, and the introduction of new insights is minimal. I will be shocked if it gets accepted in ICLR or equivalent conferences. **EDIT** my original score was 5; i have read the authors response and updated my score to 6.My score represents the experimental side of the paper, i do not feel confident judging the impact of theoretical contributions. ================= after response Thank you for providing the response. I would like to add some comments to the response.About Q1-- I agree that it is important to work on the original ordinary learning as the first paper in this direction, but it makes the story of the paper inconsistent. The adverse affect of LSR might only show up in other problem settings (distillation/few-shot/transfer learning) and not in ordinary learning, and if that is the case, it is unclear why the paper focuses on ordinary learning. (This is just a comment on the story/motivation of the paper.)About Q2-- Although Fig.1 shows the benefits of the one-hot label over label smoothed version in latter stages of training, I believe it does not show how the number of K plays a role in the performance and the difficulty.About Q3-- I agree that 90 epochs is sufficient for the LSR baseline, and the current figures demonstrate the benefits of the proposed method, but I was more interested in the difference of (s) in TSLA(s). From Fig.1, it still seems like TSLA(60,70,80) are improving at the end of 90 epochs, while TSLA (30,40,50) seem to have converged. *** Post Response Comments ***I thank the authors for addressing the points raised. I am raising my score accordingly to 7.Nit: The y-axis labels on Figure 7 and 8 should probably say "Clean accuracy" instead of "Robust test accuracy". ** update after rebuttal **Thank you for the detailed clarifications. I still find that the recommended method used in practice, which only encourages robustness to local random perturbations, is too disconnected from the motivation of manifold regularization, and will thus keep my score. *** Post response comments ***Thanks for the detailed responses and clarifications, especially regarding the manifold used. I suggest that the notion of $\epsilon$-manifold regularization is made clear upfront in the manuscript (e.g. in the introduction) to avoid misunderstanding.The new results with the dense $\epsilon$-manifold regularizer obtaining 5% less accuracy suggests to me that the specific approximation scheme used is indeed responsible for at least part of the benefit, and it would be useful to understand precisely why this is so since this is the core-contribution of the paper. I also agree with the other reviewers that the overall method as implemented seems a bit disconnected from the core idea of manifold regularization, and rather appears to be a variant of stability training (Zheng et al. 2016). As such, I will be keeping my original rating; nonetheless, I want to again thank the authors for the interesting and vigorous discussion. Update after the rebuttal:I read response from the authors and other reviews. I have increased my score to 5 given that authors now performed somecomparison with Liu et al. However, I still believe that the threat model is not realistic and that attacker can not be bounded by the budget that is produced by generator network here. While authors wrote quite detailed response, I did not find it convincing enough. As R5 points out similar issues, I would encourage authors to think more on how to tackle the problem better. Perhaps the threat model can limit the attacker to all possible perturbations under the certain volume budget, but that would be quite different from the idea in this paper.  Thus, I can not recommend acceptance at the current state.============================================================================ Update: I thank the authors for their response. I've read the other reviews as well, and indeed R2 had similar concerns to my own. I'm glad to see the more comprehensive comparison to Liu et al., which paints a fuller picture of the effects and trade-offs of the approach. The argument behind the motivation, however, feels much like setting up a straw man for Lp robustness. For example, the authors argue that their approach is label and semantics preserving unlike uniform perturbations; however this is quite frankly only the case for extremely large perturbations in MNIST-like settings which are unrealistic by design (most papers do not consider such large radii for exactly this reason). Uniform perturbations seen commonly in CIFAR10/Imagenet settings are practically invisible and consequently are equally semantics preserving and close to human perception. If the authors do wish to pursue this argument that these are truly more semantics preserving, then this needs to be backed by evidence. The authors weakly suggest this is the case because the budgets look similar to the content in the images. However, this does not imply that an adversarial attack within this budget is label preserving (i.e. many of the presented examples have large budgets in the background directly adjacent to the label-content of the image, which can easily change how the content looks), and so this needs to be justified carefully if this claim is to be made. The authors also incorrectly equate the restrictions imposed on an attacker from learned perturbation radii to that of a uniform radius. These are *not* equivalent, especially in the security setting where these are night and day; the first amounts to the defender choosing the rules of the game that work optimally for them, whereas the latter is a *defense agnostic* rule that both the defender and attacker must obey. This is a significantly easier setting for the defender that needs to be properly motivated, as restricting an adversary to a fixed perturbation set is inherently different from restricting the adversary to a fixed perturbation set that the defender gets to choose. The reason why one would want to maximize certification volume needs to be properly motivated, as it is no longer applicable to the usual adversarial security setting and comes at a cost to the usual robustness considerations. To recognize the addition of the necessary comparison to past work, I have improved my score slightly. However, I would still argue that this is below the threshold, as their central claim of learning *semantic preserving* perturbation budgets is not justified despite being a central component of the paper, as well as the motivation for why it's considered beneficial to choose the most easily certified volumes for robustness in the first place (and certainly not helpful from a security perspective).  -----------------------------------------------------------------------------------------------------------------------------------The author answers to reviewers' questions clearly with supportive details.  In addition,  the newer version has added useful new baselines and results. I have increased my score as all my concerns get cleared.Overall,  I think  this is an interesting paper and should be encouraged. But I still has some concerns as the application is very limited on medical images. The author has compared all the baselines and shown that the generalization capability for the proposed method is similar to fully supervised method, which seems not doing well.  I would be more convinced if the proposed can be tested on few shot benchmark which are natural images.  ********************************The rebuttal is very detailed and contains many useful new baselines and results. The authors answer to reviewers' questions carefully and with great supportive details. I appreciate the efforts and many of my questions are answered, therefore I have increased my score. I'm not  an expert from medical image field. So it's a little bit hard for me to evaluate the significance of the reported results and proposed methods. However, I think this paper is well-motivated and current version has greatly improved over the first draft.  For the machine learning community, it's an interesting and important application and thus should be encouraged. ----Update:Thank you for answering my questions and running the additional NDS0 baseline, this clarifies a vital aspect of the paper. I'm still concerned that in its current form, the learnings we can extract from the paper are limited.The new baseline confirms the expected, that for low-dimensional synthetic tasks where the ground truth model is known, pure sysid is a perfectly fine strategy. This is not surprising, and a good sanity check to perform. Unfortunately, this is also the largest part of the experimental results, and I'm not sure we can learn that much more beyond this.Results on 5.2 do look promising-- neither NODE nor NDS0 perform as well as NDS, and the new baseline has strengthened this result. But as the only data point demonstrating an advantage of the method, with little analysis to why and when it does well this still feels a bit limited.I still think this line of research is very valuable, and I encourage the authors to study the properties of this approach further (e.g. noise, missing systematic terms, investigate why partial NDS performs so well in some tasks), and investigate other non-trivial domains where NDS or variants can shine.I've raised my score to 5. -------update-----Thank you for your response. The discussion and the revised manuscript clarified some of my concerns regarding your work. I appreciate that the authors will focus on the parameterization of PartialNDS and the effect of the amount of employed prior-knowledge. However, my concerns regarding the overall performance, reported as very high errors overall, still remain. This might be due to how the experiments are designed, how the results reported or something else - but nevertheless it needs more attention and further investigation. Note: After rebuttal, I increase my overall score from 6 to 7. ##########################################################################Post rebuttalThe author's response does not address my primary concern and I'd like to keep my original score. I was concerned about the method because the proposed method seems to overkill the original version's simple experiments. The newly added experiments and clarifications make sense to me.  === AFTER REBUTTAL ===The authors have raised a few fair points in the rebuttal (especially point 1), so I've adjusted my rating accordingly.  ===================Update after discussion: The difference between excess loss and absolute loss is an issue I overlooked. After discussing with other reviewers, I realized a new question: how to compute the excess loss L(n, q) without knowing the the best possible estimator (oracle) in the class? This is a key step in the proposed algorithm when fitting the predictor. The paper has not explained it. As of now I do not think that is possible, except if strong assumptions are made, such as the oracle has 0 loss. In the experiments, if the authors use the estimator trained with full data as the oracle to compute the excess loss, that would invalidate the practical usability of the approach because the goal was to predict the performance without having full data to begin with. ### Overall evaluation: The paper is interesting and tackles an important problem. However, it lacks clarity in many aspects.=====POST-REBUTTAL COMMENTS======== I thank the authors for answering my questions. Their answers did clarify some aspects, such as the ratio of mixed sources, the optimal data collection, and the relation of the proposed work to active learning and multitask. However, the answer provided on my comment about the distinction between excess loss and absolute loss is not sufficient. I believe this is an important point, and calls for a major revision, not just a promise to clarify the point throughout the paper. The same comment applies to the experiment when comparing methods on the basis of computing the excess loss and not showing the absolute loss, which is the actual measure of quality of an estimator. ==============Update after rebuttal:Thanks to the authors for their response. I enjoyed this paper, and I'm keeping my score unchanged. In terms of multiple datasets/diverse models, I appreciate the various models/dataset/baselines currently included as a proof of concept. However, I'd be very interested in a more systematic study with significantly more models and datasets to understand better precisely when excess error as function of data composition and size can be reliably extrapolated and the extent to which the trends observed are "universal." UpdateI have revised my rating based on the updates. The paper has good theoretical insights, however I agree with the other reviewers that more completeness is required to make the proposal stronger. -------------------------- ===== After rebuttal =====As I have mentioned in my review, I am not an expert in the field of neural ODEs. Regarding the technical contribution I think is quite interesting, but probably rather small as far as I understand from the rest of the reviews. In my opinion, the current paper is very much focused to audience which is very familiar with the topic, and its impact on a non-expert probably limited. Therefore, I tend to keep my score and vote for rejection, because I feel that the submission has to become more accessible to general audience. Update: Thanks to the authors for the work done during the rebuttal period. The proposed method seems working but it's still not very clear when it is beneficial to use it. The authors provided some intuition about the cases when the method is effective, however, I think it is necessary to more thoroughly explain the applicability of the method before sharing it with the community at the conference, that is why I don't change my score. ## Post-rebuttal updateAuthors have addresses some of the issues outlined above, in particular the additional comparison in sec. 5.4 and table 7 is informative. However, the RAE numbers indicate that a simple 10-component Gaussian Mixture is superior to the complex model of NCP-VAE with a Gaussian prior on a small VAE, and the superiority of NCP with GMM prior base provides little information as RAE with 50-components GMM could have performed even better. It's also not clear how 2s-VAE, RAE and WAE would scale to bigger architectures such as NVAE, which raises the question if the NCP was the optimal one.It's surprising to see the 2-stage VAE to perform worse than the standard VAE, whereas (Dai & Wipf, 2018) have shown a significant improvement in the original paper. I think, this result needs further elaboration and validation.Regarding the qualitative evaluation: figure 12 does show that the NCP-NVAE has fever poor samples, although it's hard to tell whether the difference is significant. Another issue is that the comparison does not include prior works.As a result I bump by score to 5, but I think the paper still needs more work to make a sound argument for the proposed idea. Post Rebuttal: I believe the positioning of the paper could be improved but at the same time empirical results in the paper are strong. I am adjusting my score to 6 with a confidence of 4. **AFTER REBUTTAL**I would like to thank the authors for their rebuttal and all updates. The paper is related to other ideas in the literature, however, it constitutes an interesting contribution to the field. I appreciate all new results and discussions. I keep my original score. Edit: I have read the authors' response and the other reviews. I still believe that this paper is not ready for acceptance.  Update: I have read the author's response and decided to keep my review, confidence, and score. ---- ====== POST-RESPONSE UPDATE ======After reading the authors' responses and the rest of the reviews, I still stand by my original score.That being said, I do agree with the other reviewers that the framing of certain discussion points could be improved to avoid misconceptions. Specifically:- There are places where the narrative feels accusatory towards earlier attacks (e.g., words like "flaws").- It would be good to explicitly state the scope of the paper relatively early (e.g., "clean-label attacks on deep learning for images") and explain this choice.- Clarify that this is not meant to be the ultimate poisoning benchmark but rather a first step that allows us to make progress in terms of attack development and evaluation.It would be great if these could be incorporated in the next revision of the manuscript. =============================================================================================After discussion with the authors and reading the other reviews I still believe that the paper should not be accepted and therefore stay with my original review. While the authors have shown improvement over previous work (MH-GAN) using a very natural idea, this previous work in turn has not provided sufficient evidence that MCMC-GANs are useful.I believe that we should not accept further MCMC-GAN papers, before this methodology has shown any improvement on a plausible use-case. Updated recommendation after major changes to the submission: thank you for addressing my comments. Updates: Thanks for the authors' response. I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted. ### Updates after the rebuttalI appreciate the authors' timely response. The latest update explaining the regularization effect of using a fixed canvas $x_c$ and a learnable mask to avoid overfitting to spurious features, which seems to be inevitable if using a learnable $x_c$, does seem plausible and differentiates itself from the general idea of UAT. From the updated content in Appendix H, using a learnable $x_c$ does not seem to be able to find interpretable patterns. I feel contributing a new method with some improvements is worth an accept.However, I still feel the results are not conclusive enough. Regardless of the final decision, I hope to see more comparisons with simpler baselines in the future version. To highlight the novelty and effectiveness of the proposed method, the authors should try to compare with more baseline approaches that do not have presumptions on $x_c$ and initialize $x_c$ randomly. Currently, $x_c$ are selected as images having highest prediction scores from the model. Instead, we can learn $x_c$ from random initializations by using some approaches that enhance the transferability of adversarial examples. There are simple methods on improving the transferability of adversarial examples, e.g., adding Gaussian noise when crafting adversarial examples [1], or adding different random data augmentations [2]. These methods may also lead to interpretable patterns but the effectiveness of the resulting patterns could be stronger. I do not fully agree that a desirable canvas has to be a neutral or unbiased image. [1] Wu, Lei, Zhanxing Zhu, Cheng Tai, and Weinan E. "Understanding and enhancing the transferability of adversarial examples." arXiv preprint arXiv:1802.09707 (2018).[2] Huang, Qian, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. "Enhancing adversarial example transferability with an intermediate level attack."  CVPR (2019). --------------------Post rebuttal comments:Thanks to the authors for the helpful comments -- they indeed help clarify some of the confusions. As a result, I have upgraded my score. However, I am still leaning towards reject because I feel there are still open questions that may hinder the adaptability of the proposed method in the real world. Specifically, given the response to question 3 above, it would help to know what are the real world situations where one uses a randomized classifier and is still interested in model interpretability (the two seem to be at odds with each other as randomness inherently seems a bit arbitrary). Another concern that I have is about Eq. 3 in the paper: Why is the sum function chosen to compute global explanations from local ones? There seem to be multiple ways to do this (e.g., median, sum of absolute values) and it would help to know what are the (dis)advantages of not using other aggregation functions. ################################################Post-Rebuttal:Thanks the authors for their detailed response! After reading the responses, I decide to maintain my initial assessment.The statement "full images are highly out-of-distribution for a model trained on images with only 5% unmasked pixel-subsets and hence such a model cannot properly generalize to fully unmasked images" makes sense. However, I still feel that the authors need an experiment of this flavor to support their claim of We show misclassifications often rely on smaller and more spurious feature subsets suggesting overinterpretation is a serious practical issue as mentioned in my initial review as well as pointed out by R1.Besides, I also find the point "the observed phenomenon is very model-dependent" raised by R1 is a valid major concern. In the authors response, they did not add extra experiments to address it. " We indeed find that models trained on 5% pixel-subsets can generalize to the corresponding 5% pixel-subsets of test images. " - the stated experiment trains and tests on the same model so it does not address the concern that the observed phenomenon is model-dependent. In order to address this concern, the authors need to add some experiments on transferring across architectures (e.g. train on SIS of ResNet and test on SIS of VGG).  ===============================Post-RebuttalI thank the authors for their detailed answers.  After reading the other reviews and the author's rebuttal, I maintain my rating of 6 for the paper. My concerns on the description of the SIS methods and results on the proposed mitigation are not addressed.I am not convinced as R1 and R4 that training on the SIS and testing on the full image is the correct way of testing if SIS is sufficient for the model's predictions. If we would present SIS images with unrelatable labels (A instead of Cat, B instead of Dog, C instead of Boat, etc) to humans and ask them to learn the mappings, I am confident they could achieve good results. As pointed out by other reviewers we can see some patterns in the SIS. Showing a full image afterwards and asking to predict (A, B, C, ...) would be quite difficult however. It's easy to infer the pattern from the full image, but the other way around is more difficult. To me the most important is that a given model architecture can be trained on the SIS of another trained model (with different random initializations) and still be able to learn and generalize. That alone shows in my opinion that the dataset contains undesirable statistical artifacts shared by training and test sets, and as the authors says in the paper 'interpretability method that faithfully describes the model should output these nonsensical rationales. I believe R4 makes a valuable point when saying '[...] I think it might be more useful to look at the mean SIS size of those that are wrongly classified by humans'. This seems to be a better way of gauging what threshold should be used for the size. -------------------------------------------------------- Post Rebuttal --------------------------------------------------------------------------------------------The author did a great job in terms of addressing most of my concerns and answer all the questions. I also like the updated paper. The update reflects most major concerns from the reviewers. Thus I would like to raise my score to 7 and would like to champion this paper. I do think the paper would potentially provide great value to the community not only due to its open-source effort but also as a general approach to improve simulator efficiency for RL, despite it's not "novel" methodology-wise.   **Update post-rebuttal**The authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7. ------------Post-rebuttal response:The authors addressed most of my concerns so I continue to recommend acceptance of the paper. Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency. They also conducted additional experiments to verify some of my questions. ~~~~~~~~~~~~~~Post review and discussion remarks:I think the authors have improved the paper significantly during the review period. However, three of my main concerns about the paper remain to a degree that I'm not confident about the paper's value (or risk of misleading followups). (1) That the set of challenges is somewhat arbitrary, some tasks are using "real" ground truths while others are simply running on known trained oracles.  (2) That implementation of strong offline RL benchmark algorithms are missing (because they don't exactly apply across domains) even though they can always be applied in this setting even if "exact" conditions are not applied in every case (just like Gradient Ascent or BO were) (3) That the API needs to offer more for this to be a good benchmarking suite. I've been most concerned about 2 and 3, and after reading the code, I find that it is still too "bare bones" to be a good package. I looked back at OpenAI gym, and there are several abstractions that they make, including actions, observations, environments, spaces that help the implementer unify how they deal with the complexity underneath. So far as I can tell, most of what design-bench does is load a csv matrix into an task.x, task.score(x) , and also lets the user access some approximate oracle task.y. This are critical to the process but their abstraction as related to the paper are not clear to me at this time. How is task.y computed, how is the ground truth actually representative of reality. How does optimization depend on the choice of oracle for task.y? Having read the code, useful elements are in there to make for a good package, I feel like it needs improvements and another review for scientific soundness.I've updated my score to address the improvements made. The paper scores somewhere between a 4 and a 5 for me. _____Post author rebuttal:I appreciate the authors response and overall the authors have addressed my concerns. I am thus raising my score. The only point that I believe still stands is #7, though I should have updated earlier. My issue with claiming this as the first use of end-to-end learned subgoals in navigation is that there have been many recent works from goal-conditioned hierarchical RL that use end-to-end learned subgoals, e.g., https://arxiv.org/pdf/1712.00948.pdf, https://arxiv.org/pdf/1805.08296.pdf, https://arxiv.org/pdf/1909.10618.pdf. Navigation to a known goal is a version of this problem and indeed in these works, the approaches are shown navigating between states. Others have applied end-to-end to navigation and manipulation, http://proceedings.mlr.press/v100/li20a/li20a.pdf. Overall, application of end-to-end HRL to the navigation problem is an interesting area to study, but to claim it as a major contribution I believe the paper should thoroughly examine the tradeoffs as applied to that problem, which I believe requires a detailed and standalone work. [Post Rebuttal]Thank you to the authors for addressing my question. The paper presents a simple and elegant approach to the AudioGoal task, backed by extensive experiments and good writing. I'd like to maintain my positive rating. -------------UPDATE: I have read the authors' rebuttal and revised draft and have raised my score to a 5. # RevisionAfter a fruitful conversation with the authors I am changing my decision.The conducted experiment presented in Figure 4 confirms an advantage of L-conv.The paper has a room for improvement. However, it contains valuable discoveries and can be clearly placed in the field of symmetry-aware models. The proposed approach is well-described and is properly evaluated on shallow networks. While the generalization of the results to deeper models is not possible (and the reasons are described by the authors correcly), the approach is interesting by itself and is important to the field. *****Post Rebuttal*****I would like to thank the authors for their comments. They clarified most of my doubts regarding the paper, most importantly about learning the basis $L_i$. While I agree with reviewer 3 that there is room for improvement when it comes to experimental evaluation, I appreciate the changes made during the rebuttal period and keep my original score. ** Edit after author response **The authors have addressed the concerns I had so I've increased the score accordingly.  **Update Post Rebuttal:** The revised manuscript addresses my main concerns, and is stronger than the original, so I am raising my score to accept. For the final manuscript, I urge the authors to better integrate the added content from this discussion period into the main text, instead of relegating it to the Appendix.   [Edit: Score updated, see discussion below] ----Post-discussion update:Having read the other reviews, author response and updated paper, I still think this paper is borderline. The insight that disentangling transformations as naively defined is impossible for topological reasons is valid and interesting, but seems to have been already observed by others, e.g. Falorsi et al. Nevertheless the paper does a good job explaining this so it could be useful, as some authors seem to not know about this issue. The definition of disentangling still seems a bit vague to me, and I'm not convinced of practical applicability of the proposed method.  **Post rebuttal**I thank the authors and other reviewers for their comments and discussion. While the direction the authors pursue is of unquestionable merit, I remain unconvinced that the work as it stands is sufficiently impactful for this venue.  Updates after author responses and discussion:(1) After the updates, most of the positioning issues have been improved(2) The technical exposition is improved, and the main argument I was bothered by is somewhat improved, though it still has a big jump near the end.(3) I'm still bothered that to demonstrate improvement the algorithm is tuned on a per-example basis but the baselines are not.  The results certainly show that the former is reasonable, but to make the comparison fair the latter needs to be done as well. AFTER READING THE AUTHORS' REPLY, I HAVE CHANGED MY RATING TO 6. Update: taking authors comments and improvements into account I've updated the rating from 6 to 7. ----**Post rebuttal comment**I thank the authors for detailed rebuttal and new empirical results. I also have read other reviewer's comment, and decided to keep my original score. The main concern of this paper, the weak novelty, still remains (also pointed out by Reviewer#1/4).  **After rebuttal responses**: I have read the authors updated draft and response to my concerns, as well as the other reviews. The updated paper provides a clearer framing and some missing baselines have also been included. I raised my evaluation to a weak acceptance for the paper.  **After rebuttal**The rebuttal does not address my concerns so I lower my rating due to the following reason:It was not clear where the improvement comes from so I asked for an image-level baseline, which is trained using contrastive methods. The rebuttal does not provide that. It is mentioned that adding the image-level baseline to the proposed approach even improves the results without providing any evidence. My concern was that an image-based method trained in a similar way might provide the same results. I cannot really judge if the proposed method is effective or not due to lack of this baseline. Several previous embodied representation learning works are outperformed by simple image-level baselines.  -- Update after author responseThe authors have addressed some of my concerns, I have revised my score accordingly. I am not convinced that the method is significantly better than the baselines as it performs worse on some tasks/datasets and all the tasks in the paper are similar. Even the heuristic policy used for gathering the data is similar to the downstream tasks. The addition of other types of downstream navigation tasks such as objectnav or imagenav would make the paper much stronger. I like the noisy pose and depth experiments but there's no information on the type and amount of noise. I am assuming a zero-mean gaussian noise, which is not very realistic in my opinion. The standard deviation of the noise is not reported. I encourage the authors to add some other downstream navigation task and add realistic noise with relevant details to the camera-ready version if accepted. **Update**: After reading the rebuttal and revised paper, I am keeping my score the same. There is a lot in this paper I really like -- an important prediction target and race analysis among them -- but still have concerns about clinical utility. In standard photoplethysmography, the causal graph is PaO2-->light absorbance, which leads to a clear story of what a fingertip SpO2 sensor is doing (modeling an imperfect but mostly unidirectional relationship). The causal graph in this paper is less clear but probably has some kind of cycle like ventilation-->SpO2-->ventilation, which is  a much more challenging story. I think there are two good routes this paper can take: (1) focus on how to clinically justify a model with such a potentially complicated causal graph; is the model learning the vent->O2 relationship, O2->vent, or some combination of the two? How can we be confident we know that? To which scenarios will and won't a model that has learned such relationships generalize? (The rebuttal has analysis that is a good start, but I think not enough to fully answer these questions) Or (2) acknowledge that the scope will necessary be limited when causal structure is so complicated and poorly understood, and actually limit the tasks the model is capable performing to those on which we can be confident it will do well. I think either (1) or (2) would involve major changes to the structure and goals of the paper, and probably require new experiments.  ### ProblemThis paper considers the effect of label noise on stochastic gradient descent. The setup is that there is a vector $v \in R^d$. We observe samples from $v^2\cdot x$. We only have $n < d$ samples but $v$ is $r$-sparse for $r < n < d$ which makes recovery possible information theoretically. The main result is that stochastic gradient descent with label noise, and without any explicit regularization will recover the ground truth. whereas adding spherical Gaussian noise does not.### Pros and ConsThe problem is a clean toy problem with which to illustrate the gap between algorithms. It shows a clean separation between the power of label noise and that of random Gaussian noise. The model appears to be the simplest model where one can hope to see the regularization effects of noise (the simpler linear regression model wouldnt show these effects). One possible criticism could be to ask if understanding this model is truly getting us closer to understanding what happens in deep nets. At this point it is hard to say, but proving such a result even in this simple model is not trivial, and is definitely a contribution.### EvaluationI think this is a solid theoretical contribution on an important problem, and the paper should be accepted.### Further commentsI was a little confused by the comment that the coefficients are assumed to be in ${0,1}$ since they then satisfy $v_i^2 = v_i$ as this seems to linearize the model. The authors should probably clarify that this is actually not what is going on. It might be better to use a different setting of parameters even for exposition.  Post rebuttalThe authors addressed the concerns around having a deterministic gating only baseline. I will increase my score to 7 ------------ After Discussion ---------I am still of the opinion that the manuscript is not without flaws: the theoretical result is quite messy, the empirical evaluation is still limited, and the generalization bound could still be vacuous.However, the authors provided new experiments that indicate that the bound might not be vacuous in the considered setting, though. The authors also provide a preliminary runtime analysis that suggests the costs for using the surrogate loss do not explode (please include a proper runtime analysis in any future version of this paper). Since the authors furthermore did address my main points in my review during the discussion, I have increased my score to 7. ####################AFTER RESPONSEI would like to thank authors for their detailed response and for their effort in improving the paper according to reviewers' suggestions. Unfortunately, after authors clarifications, I still have some doubts on the concerns raised with C1 and C2 (see below). Thus, I am keeping a slightly negative evaluation for this paper. Authors claim that the main benefit of APT over a prior work method (MEPOL, Mutti er al., 2020) is a lower variance of the gradient estimation, thanks to the choice of avoiding importance weights corrections. However, in Figure 6 MEPOL does not seem to suffer a particularly high-variance. To me, the most likely reason for the improved performance is that APT guarantees an action-level feedback as opposed to a trajectory feedback (see [1]).However, I am still skeptical about this action feedback: the reward-to-go becomes non-Markovian and Bellman equations does not hold anymore (see [2]). This casts some doubts on the actor-critic procedure APT employs to optimize the rewards. Authors may have a good point on the notion that the encoder is breaking the dependence between policy and rewards, but I think the topic warrant some additional discussion.I would suggest the authors to rephrase this work to give a more central role to the scalability to high-dimensional observations, which I believe is the main contribution of the paper, and to include a more thorough discussion of (Mutti et al., 2020) in the main text (beyond the related work section).[1] Efroni et al. Reinforcement learning with trajectory feedback. Arxiv, 2020.[2] Zhang et al. Variational policy gradient method for reinforcement learning with general utilities. NeurIPS 2020. Post-rebuttal:After reading the author feedback and other reviewers' comments, I would change my rating to 6. I partially agree with reviewer #3 and reviewer #5 that this work is an incremental extension of LORD. In addition, like I mentioned in the "Cons", the styles are decided by the pre-defined transformations that are dataset-specific. For different datasets the transformations are defined differently based on the properties of that dataset. This might restrict the method from being extended to new datasets with styles that are not easy to find corresponding transformations. However, I think this paper still have some inspiring aspects, like the idea of disentangling class-specific attributes and common attributes among all classes. And the results of this paper seems good compared with previous approaches. So I would give the rating of 6. Post Rebuttal: The authors have resolved my main concerns. However, the reliance on the nature of transformations applied is still an important factor. Nonetheless, I do see value in the approach presented in this paper. I am revising my rating to 6.  **update: the authors have answered most of my questions** Thanks for your response, clarifying. -------------------after rebuttal----------I thank the author for your answer. Here're the response to your latest reply.For point 1, I am aware of your performance and encourage you to add the discussion in the main paper.For point 2, I still doubt it since I think your claim may only hold for the ImageNet experiment on the Outputs Norm term. According to figure 2, the network does not faster converge with the help of BN. The regularization gap happened in the late stage. But, the gap of two networks on the feature embedding norm and mean output norm happens at the very beginning and keeps increasing. (except  ImageNet experiment on the Outputs Norm term)Overall, I think the paper may bring some insights to understand the BN and would like to keep my original score. I thank the authors for providing further information and answering the question raised by the reviewers. Based on the responses and clarifying the issues I had, I have adjusted my score accordingly and improved it from a 5 to 6.  UPDATE:Authors, thanks for your updates.Some of cons are gone, and the paper is better now, but my main concern stays: it's unclear if the results are about the problem solving ability or the zero-shot learning ability. Thus, I corrected my score to from 4 to 5. My rating has been updated after the rebuttal. i have read the author(s)' comments and I have updated the ratings based on their replies. Thanks for very extensive clarification. Adding these comments in the final revision would significantly improve the quality of the paper. --EDIT-- updated score to 7 after the author's response to questions and changes to the paper.  Post-rebuttal comments:My concerns are resolved. I have changed my vote to acceptance. (7). --------update after reading the response-----------Thanks for the authors' response, but some non-trivial concerns are still not adequately addressed.1) The inconsistent comparison results between SNGAN and the proposed method over CIFAR-10 and LSUN Bedroom datasets.2) I can see the benefit such as compositionality from the proposed method of training EBMs. But the paper still seems to overlook the importance of giving the readers an overall picture of the state-of-the-art of learning EBMs. Table 1 should be expanded to include more state-of-the-art results from EBMs, whether using auxiliary generators or not. **Post rebuttal (round #3)**Thanks to the authors' effort on the rebuttal. Despite the extensive efforts, I feel the review/rebuttal iteration is not satisfactory, possibly due to some miscommunication.To be clear, I want to re-emphasize that significant parts of my concerns were about **misleading claims** on prior work, and comparison to them was the next step.- For example, I just wanted to clarify that the claim "type A ignores the noise and cannot learn the stochastic mapping" is wrong. The paper could simply fix the claim instead of including a massive related work section. Maybe my review also has some responsibility: I could simply say **fix** the wrong claims, instead of indirectly delivering by pointing them out.Also, as I explicitly mentioned the concerns A,C,D,F, the rebuttal could address them point-by-point.- In particular, I'm not convinced that cBN/sBN is **not applicable** to continuous conditions, as sBN predicts the BN parameters from the continuous latent variable.Despite the remaining concerns, I raised the score (from 5) to 6 as the architectures with multiplicative interactions are an important and timely topic. However, I think the paper is on the borderline, and the rebuttal and revised paper could be much stronger.------ Update: After seeing the changes the authors have made, I have slightly increased my score. Thanks! Update: see comment below for rationale behind change from 5 to 7. =================================**Update:**Thank you for you responses and clarifications to mine and the other reviewers' comments. Thank you also for baking so much of the reviewers' feedback into your latest draft, in particular by improving how the comparisons are presented in the abstract and the rest of the paper, by updating Table 2 to make it clearer and cleaner, and by adding experiments and analyses in Section 4. As a suggestion pertaining to the latest draft, I agree with R5 that a consequence of the additions to Section 4 have made it a little crammed, although I certainly understand that the constrained spaced forced the authors to make this trade-off. Overall, I continue to maintain that this is a good paper with novel ideas that are well-justified by experimental results and analyses, and would like to reiterate that I believe that this work is a clear accept. **Additional comment after rebuttal**Unfortunately, the reviewer would like to downgrade my first score. The remaining concerns are about R2-A1 and R2-A2.In R2-A1, the authors add some discussions about experimental results and explanations about Gu et al. (2020). The second discussion about generating pseudo labels would be the most considerable theoretical difference between the proposed method and Gu et al. (2020). The authors state that the proposed method addresses the problem by incorrect pseudo labeling of Gu et al. (2020); however, they fail to show quantitative nor qualitative discussions that support the statement. The third discussion seems just showing the proposed method achieves similar performance to that of Gu et al. (2020). As discussed in the fourth comment, the authors could add another result showing the proposed method is complementary to Gu et al. (2020).In R2-A2, the authors honestly state that the hyperparameters are tuned according to the test data evaluation. However, it should be avoided to evaluate unsupervised domain adaptation methods since there are no labeled data in the target domain in a real setting. Moreover, Figure 4 (a) shows that the proposed method is sensitive to lambda on Office31 dataset. When lambda=0.5 on all dataset, the proposed method achieves SOTA on VisDA as shown in Table 5, but seems to fail on Office31. =========================================================================================================Update:Thanks for the authors' response. But unfortunately, the most significant concern has not been addressed well, which is regarding of the paper's novelty. I thought the contributions of this work are incremental, and the authors' rebuttal did not convince me well. Btw, the authors claimed that [A] used KL-divergence while their work used mutual information maximization, however, minimizing KL-divergence is actually one kind of mutual information maximization. What's more, the authors even did not clearly point out which kind of mutual information maximization they used in either the manuscript or the rebuttal. Thanks again for the authors' efforts, but I choose to maintain my original score. EDIT: After discussion, I have increased my score and am recommending weak accept. See the discussion with the authors for detail ==========Post rebuttal============Thanks for the authors' response. The authors have addressed some of my concerns. I have raised my score accordingly. After discussion with authors------------------------------------------Some of the concerns of the reviewer have been addressed and the reviewer is raising the score to reflect it. The paper still has some major concerns preventing the reviewer from recommending acceptance of the paper: - Similarity of the theoretical analysis to https://proceedings.neurips.cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf  and https://arxiv.org/pdf/1804.11285.pdf .  - Hard negative mining is pretty standard in many learning domains. Both of these issues can be addressed by a better review of the related work and more accurate identification of the novelty in the paper. Given the limited theoretical novelty, a more robust empirical evaluation establishing the value of the extreme outliers against state of the art approaches would have been useful. Also, if the authors want to investigate theoretical results on OODs, one challenge is the lack of a formal definition of OODs. The paper is a good work in progress but not yet ready for publication. The reviewer is hopeful that the above suggestions will make the paper stronger.   # Update after initial revisionI have re-read the revised version multiple times now, and I thank the authors for the amount of work they put into their revision. I am raising my score for the next discussion round. That being said, I want to point out why I cannot fully endorse this paper yet. First, from the point of topological data analysis, the central algorithm is a comparatively small extension of the Geometry Score paper. I agree that this is a superb idea, yet the main contribution for me lies in the application of that technique to disentanglementand for this to be fully understandable, some more work is needed. For example, putting the central algorithmic details into the appendix will make the adoption of the method that much harder. Moreover, while I appreciate the overall story and description of the method, I do not think that readers will understand how this disentanglement is actually _achieved_ by means of the proposed TDA approach. I would therefore prefer to see a more 'technical' or 'algorithmic' description of the contributions in the main paper, in particular since I think that the ideas of conditional submanifold topology require more attention.My expertise is more the topology and less so the application of disentanglement; nevertheless, this paper strikes me as highly ambitious with a lot of potential, yet somewhat unfinished in its present form. I do believe that it has the potential to be extremely impactful with some additional modifications (concerning conciseness, but also experimental details).I fully realise that this is not yet the desired outcome for the authors; I shall endeavour to discuss this further with my fellow reviewers to see that we can reach a consensus! [Update after author responses]: The authors have addressed most of my concerns and I've updated the score accordingly. Post-Rebuttal Update:I acknowledge having read the author's response and I also glanced over the new experiments.  [After rebuttal]  Thanks for the clarification, the aim of the authors becomes clearer. However I still think the paper requires more work before publication. Your definition of graph mapping is still unclear. From your definition, it looks like the only dependence of $\Psi$ with respect to the graphs $G_t$ and $G_{t+1}$ are through their number of vertices N and M: " bijective mappingÈ:{1,...,M+N}  {1,...,M+N}with  the  additional  restriction  that  for  the  setInsÈ:={jdN|È1(j)> M}we  obtainÈ1(InsÈ) ={M+ 1,...,M+|InsÈ|}"How can this mapping be related to the edit distance of $G_t$ and $G_{t+1}$.After reading the other reviews, I think the authors should clarify if their gaph mapping is related to the standard graph matching pb see https://en.wikipedia.org/wiki/Graph_matching. Update after author response: I appreciate the authors' efforts to address my concerns and to raise some interesting points I missed. I still find the paper's insights are lacking some novelty to be published, but I think that this line of research is worth it! ------------------------ ----------------------------------------------Update after Rebuttal:I have read the authors' response but do no change my scores.The experimental results alone do not offer a theoretical justification. The paper does not have to contain the latter but then the evaluation would have to be exhaustive.I see that the authors have done a huge number of experiments. However, if basic standards like standard deviation are neglected in order to run just more experiments, the results of the entire evaluation remain questionable. Similarly, related approaches have to be considered adequately to have an appropriate comparison to SOTA. The authors have not made them available yet. --- Post rebuttal:I've read the author's response and there is no change in my scores. - The given argument regarding better-generalized aggregation function aid in training deep GCNs is not clear and convincing. - I agree that doing a detailed ablation study on a large dataset is expensive. In which case experiments on either synthetic or other smaller real-world datasets would be helpful. #####Update######Thank the authors for the response. I keep my score as 6. # After RebuttalI have read the response to my review and the responses to the other reviews. The summaries of the paper in the other reviews helped to clarify my understanding of the research question the authors were aiming to answer and how they went about doing so. I have re-read the paper and the revisions have helped to make this story clearer.With that said, I remain concerned with many technical aspects of the paper, for example:* The scale of the networks studied.* I still don't understand why this particular definition of utility imbalance is well-motivated.* I generally don't think that two dimensional loss landscape visualizations are informative since they discard an enormous amount of information from the full loss landscape. To show that minima are related, I think it is better to use interpolation (mode connectivity).I am also still concerned about the writing. The revisions, alongside the other reviews, were enough for me to get a sense for the research question and technical story, but I still struggled to make sense of the details.Since the other reviewers appear to have better understood the paper, I have raised my score to a 4 and decreased my certainty to a 2. I suggest that the AC weight my review much less heavily than the other reviewers, who seem to have better understood the technical details. I appreciate the author response but unfortunately they are still a bit vague (1,3), or not supported with experiments (2,4). I still maintain my rating of 4. ---------Update post-rebuttal: Thank you for addressing most of my comments. The new results on ImageNets are greatly appreciated too. However, in light of other reviews, I would have hoped that the authors try better training procedures on SWEEN-7 (e.g., MACER even if it means using other ResNets instead of VGG), as otherwise it is difficult to judge whether ensembling really helps. It is unclear why MACER was not used on ImageNet (since all models are ResNets). Overall, the work explores a really important direction of research, but could benefit from further improvements. Post rebuttal response:I thank the authors for their response to my review and I take their point that establishing theorem 2 points to a limitation of using smoothing with ensembles, but I think this point could be made much more clear in the main text. Following a reading of all of the other reviews and re-evaluating the paper I remain optimistic and slightly positive about the paper as I think it is an important and interesting research direction; however, I am not fully convinced to increase my score given that some of the empirical comparisons could be greatly strengthened to be more than marginal improvements over MACER trained networks. I think that any insights that arise during the process of strengthening the results would contribute to a better understanding of the method and a stronger paper.  after discussion with authors ======== During the discussion phase, the authors addressed my concerns and improved their results. Therefore I increase my score to reflect this. ***During rebuttal: I thank the toy examples provided by the authors.  --------- Comments after reading the author response:I thank the authors for adding the experiments and applying the suggested modifications! I have updated my score based on the changes and the clarifications made on the related work, and also the results of the mounted attacks.  ### Post-discussion feedbackThank you for the valuable discussion and revisions. I believe the paper has improved and I've updated my score to reflect that.The new analysis in Figure 4 and its surrounding discussion offer some useful insights. I think the authors could still improve this analysis a bit in a final version of the paper (if only just to make the plots a bit easier to parse visually), but the TD-error instability of the baselines seems reasonably clear and I'm inclined to agree with how the authors frame this as an example of the problem they solve. In addition, the added discussion around discounting is also valuable; and the simplification of the algorithm and added ablation experiments improve the quality of the contributions. ==== Updated reviewIn light of the authors' revisions, I'm happy to raise my score to 6. I think the paper is better, although I still wish the presentation was more compelling -- as given, this seems more like an exercise than a contribution with a demonstrated impact. On the other hand, this level of contribution seems relatively on par with e.g. other deep RL papers.Regarding the revisions, I would encourage the authors to integrate them with the main text. For example, Figure 3 really wants the weight=1 result (maybe as two separate panels -- comparison to other algorithms, a); ablation on weights, b)). ----Update after rebuttal:I appreciate the authors' answers and revisions of the manuscript. The theoretical presentation is clearer with the new notation and I appreciate the improved Figure 2. I appreciate that the authors followed my suggestion to evaluate on more than 3 random seeds. Statistically, however, 5 random seeds are not much different. I was envisioning using at least 30 random seeds. Is computation a major bottleneck? Maybe a simpler and faster method could be used to showcase the benefits of the new objective. Overall, the new version of the paper is better, but I think the empirical evaluation and the writing should be further improved. I updated my score accordingly.  =============================================================================================================After author discussion: After discussion with the authors, I am now convinced that any applicability of the theory proposed in this work to GANs is fundamentally tied to univariate latent space or generator output since the "hidden convexity assumption" does not allow for a multivariate set of latent variables to be combined to a multivariate set of outputs.I still find the theoretical findings and method interesting, but I think that the work requires substantial refocusing and the identification of more examples of "hidden strong convexity" before being published at a top-tier conference. I therefore change my rating from 7 to 5 and recommend rejection, for now. ------Thank the authors for the responses. While I agree with some of the points made by the authors (e.g. understanding the base case is interesting), I am still concerned about the significance of the result. Therefore I would like to keep my original evaluation. After Rebuttal: I thank the authors for the rebuttal. I have also read the other reviewers comments. Unfortunately, the rebuttal is unconvincing and sometimes vague. I keep my original rating. **Update Based on Author Feedback**I am grateful to the authors for their detailed replies to my questions. I understand some of the minor points better and am happy they have revised some unclear parts. Overall, I think this paper has some real strengths: theory-guided design, important problem, novel methods. I do feel (similar to R3) that the experiments and applications could have been far more convincing. Weighing these strengths and weakness, I still tilt slightly towards accept (6). **Post-Rebuttal**The authors have addressed my comments and revised the manuscript accordingly. Recommending an accept. After Discussion:I think this is a good paper and I would like to see it presented at ICLR2021.  ---- Post-rebuttal ----Thank you for addressing my concerns and providing the additional experiments and baseline which I think make the paper stronger. I have updated my score as a result. POST-REBUTTAL=============="First, we would argue that there are not many examples of planning approaches outperforming non-planning approaches when using imperfect learned models. Providing such a demonstration in challenging image-based benchmarks is one of the contributions of this work."- I disagree with authors. The idea of planning using imperfect models have been explored as early as 1998 (e.g. Dyna algorithm in Reinforcement Learning: An Introduction by Sutton and Barto)."computational analysis"- Thank you for adding this section. It addressed my concern."non-determinism"- Thank you for adding more clarity. It addressed my concern."Statistical significance"- Great to see the extra runs. Please include the bars for Figures 5,7 in the final submission. **Post-Rebuttal**The authors have done well with their additional page, and many of the concerns I had have been dealt with the latest iteration of the paper. I have increased my score. Re: Motivation for c and d. The current *objective* for c and d is described in the paper, but the motivation is not. I hope the authors add to this discussion in the next iteration.  EDIT: The authors have answered my questions and it clarified a lot. Thus I raise my score by one. However, I am still suspicious about the value of the DP result: for example, it is not discussed why would the residual mapping have an L2-sensitivity G (as stated in the assumptions of the theorem). Also, the reported privacy values in the end of the revised version of the paper (epsilon > 1000) are not meaningful. As far as I see, the experiments give an example where this given membership inference attack works better for DP-SGD protected model than for this residual perturbed version. However the paper does not give any privacy guarantees for the residual perturbation method. Whether the L2-sensitivity can be obtained with e.g. batch normalisation remains unclear to me. I think that the paper would require a careful rewriting. === Response After Rebuttal ===Thank you for what I found to be an impressive and convincing rebuttal, both in terms of the detailed responses to every comment---and not to mention the extensive rewrite of this paper in the limited amount of time available. In short, I think that the paper is significantly improved, and is now at a level of quality that I feel warrants acceptance. As such, I am choosing to raise my initial score of 4 to 7 (good paper, accept).As the revised version introduces many changes, I have some new (minor) comments:The last line of the abstract claims "sets that are often factors of 5 to 10 smaller". This is true for APS and NAIVE, but not adaptive top-k. You might wish to qualify this statement as to which comparison you are referring to.Thank you for clarifying the introduction w.r.t. problems with NAIVE. The third "problem" seems a bit funny to me; it doesn't identify a problem with NAIVE, but rather that other methods are better (which seems to stem from problems 1 and 2). Can you clarify this point to make it more precise?Typo, related work: "[...] data-splitting version known as split conformal prediction THAT enables conformal prediction [...]"Related work when talking about equal coverage per class, you might want to cite the earlier reference of mondrian conformal prediction: Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence machine. Technical Report, 2003.Theorem 1's remark on "this result is not new". Not to nitpick, but I'm not sure which aspects of Thm. 1 makes it "first appear" in Papadopoulus et. al. and Hechtlinger et. al? This result can likely be found in (or if not explicitly in, then derived from) from late 1990's work by Vovk and co. on Ridge Regression Confidence Machines, Transductive Confidence Machines, etc. Or the journal/book papers (i.e., Algorithmic Learning in a Random World, Hedging Predictions in Machine Learning, etc).(It goes without saying, however, that this revision is much better in terms of proper citation than before!)Proposition 2 seems to have some funny crowding around the \subseteq operator.Nice work on adding section 4. In the proof of proposition 3 I believe you accidentally left out set size (| |) around C(X) on the r.h.s.In the discussion, you mention improving efficiency via prediction cascades. You might be interested to refer to a concurrent ICLR submission (https://openreview.net/forum?id=tnSo6VRLmT) which does precisely this, and to which your work on larger label spaces seems quite complementary. ## UpdatePlease see my reply to the authors' message. **After Authors' Response:**In my understanding the main contribution of this work is the identification of a new search space for activation functions. This might be very similar to earlier ones but the authors convinced me that it is a useful contribution. All my comments were addressed and as far as I saw also the comments of all other reviewers were addressed. For this reason, I increased my score. Post rebuttal: I have read the authors responses with to my 4 questions, and appreciate how detailed and honest they are. They satisfy my concerns. *********updated after rebuttal periodI still consider this as an interesting contribution, and stand with my original rating. It would be useful if the discrepancies and similarity between the connectivity structures in the model and the anatomy could be more carefully discussed in the paper. #############post-rebuttal############I have carefully checked all other reviewers' comments, the authors' response, and the revised version. Thank the authors for their detailed feedback. They have addressed my concerns on the unclear presentation. However, joining the comments from other reviewers (particularly R3), I still think there are two major issues that prevent me from further increasing my score.Q1. It is still unclear why the proposed model can tackle the over-smoothing issue in existing deep GCNs.This paper has theoretically revealed the benefit of adaptive ensemble paths towards better trainability. Given the claim in Introduction, it is still unclear why such benefit can be used to relieve over-smoothing, particularly due to the missing analysis of the output dynamics. As already pointed out by R3, [3] has set up a nice notion of framework on explaining how over-smoothing happens and why deep GCN fails. It is a pity that this paper has not put their analyses into this framework and discussed the relation with the over-smoothing issue. Actually, a more in-depth discussion of over-smoothing on general GCNs (including ResGCN, APPNP) has also provided in an arXiv preprint paper [4]. It does show that the residual networks are capable of slowing down the convergence speed to the subspace and thus alleviating over-smoothing. Since the idea of random wiring is initially proposed in CNNs, the contribution of this paper that we expect is to answer how this idea can be utilized to solve the specific weakness in the graph domain.Q2. The experimental evaluations are still unconvincing.It is thankful that the authors have additionally provided the performance of SIGN and APPNP in the revised version. Yet, the reported accuracies of APPNP seem weird and much worse than other baselines. I do not agree with the authors' response that APPNP is not intended to address the over-smoothing problem. As experimentally shown in [5] and theoretically analyzed in [4], keeping the connection between each middle layer and the input layer is able to prevent the output from converging to the subspace caused by over-smoothing, and thereby deliver desired performance with the increase of depth. As this paper has conducted experiments on a newly-public benchmark under inconsistent experimental setting up (raised by R3), it is hard to justify the significance of the proposed idea compared with previous methods, specifically given the irrational observations on APPNP.Hence, I still believe this paper is below the acceptance line. [3] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In International Conference on Learning Representations, 2020. [4] Tackling Over-Smoothing for General Graph Convolutional Networks, arXiv 2020. [5] Simple and Deep Graph Convolutional Networks, NIPS 2020. ============Post Rebuttal=============================Thanks for the additional experiments and the updates. The new results are informative.  ==========================================================================================While I still hold my concern on the i.i.d. assumption of the context as it is less interesting both practically and theoretically, the author response and the revised paper clearly resolve my other questions and concerns.  I am increasing my score to 6. Edit after rebuttal: I have read the author response and thank the authors for their comments and answers to my questions. I would like to keep my rating as it is. I recommend authors to tone down the claims around being first MultimodalQA dataset and position themself properly with respect to previous related work if accepted. EDIT: Based on the author's modifications to the manuscript, I have increased my score from 5 to 6. ---------Thanks for the clarifications. I have raised my score.I agree that the method is easier and more general than methods such as Adversarial Autoaugment. It will be interesting to see how the approach generalizes to larger/complex datasets without an expert specified family of transformations or without a good generative model. ---**[After rebuttal]**Thanks for the clarifications. My score remains the same after reading through all the other reviews and the replies from the author. During the rebuttal, the authors attempt to answer many of the questions raised by the reviewers. While some of the replies are less satisfying, I still find this work to be worth to publish. I would encourage the authors incorporate the changes in the revision if the paper was accepted.  ==== post rebuttal ====It is an interesting observation that federated averaging and several other variants of it may be cast as instances of expectation maximization (EM). However, unfortunately, these connections have been made in a rather informal way, not resulting in much of new insights into convergence of such federated algorithms, or other practical tweaks to them. The paper then moves on to propose FedSparse, an abrupt shift from the original observation to federated learning with sparse priors. The study of FedSparse is thin, with unclear motivations, and with not so strong experimental results. Connections with the plethora of existing literature on sparse signal processing, specifically as it is cast in the EM framework, e.g., approximate message passing, is missing. In the end, the reader is left with two interesting ideas but it is not clear what the take-home message is.My suggestion to the authors would be to "formalize" the connections between federated averaging and other instances of these algorithms (e.g., draw from EM literature to provide convergence guarantees, or maybe provide other variants of these algorithms with more favorable properties) and resubmit the paper with more formal connections to the EM framework. In my opinion, FedSparse is itself a separate paper that needs a separate motivation, set of hypotheses, and experiments. Added after reading author response:Authors have sufficiently addressed my concerns and I'm planning to maintain my generous score based on my initial understanding. However, other reviewers have raised many important concerns and I encourage authors to improve the paper based on those. Update after reading author response:Thank you for the thoughtful response.  I appreciate that you have added extra implementation details and am changing my score to a 6.  Regarding the limitations in scope:  My apologies if my review was confusingly worded.  I just wanted to clarify that I had meant that the counterfactual generation method itself may have limitations (not the high-level DST task, which I agree has broad uses).  The concern is that the adversarial example generation strategy might be too domain-specific to transfer easily to other tasks, and that this might limit the impact of the proposed counterfactual generation method.  I think this is somewhat related to the parts of the methodology that R1 described as "ad-hoc", "engineering intensive", or reliant on heuristics.  I still have some reservations about the transferability of the proposed methods, though the authors' response to R1 did clarify a bit on this point.------------------------------------------------------------------------------------------------------------------------------------------- --------------------------## Post-rebuttal UpdateThe authors have shown new experiments on icy environments that show good results for the proposed method (DARC). This directly addresses my point (and recommendation) about trying out experiments similar to the aggressive driving on icy road example that was mentioned in the introduction. Having read through the other reviews and responses by the authors, I feel that most major concerns have been addressed. As such I am inclined to increase my score from 7 -> 8, recommending acceptance of the paper and entrusting the authors to include the new experiments in the main paper.A minor note: My second point under "Other issues/comments" section was not answered in the rebuttal. I hope the authors can clarify this in the future either in the main paper or appendix. ---------------I thank the authors for their response. However, I am inclined to keep my rating after having read their response. ----------Update after author response----------I thank the authors for the detailed response. I think the fact that the near-linear time JL approach follows more or less from previous work needs to be clearly mentioned in the paper. I also think some experiments would be nice, and it would be reasonable to use some of the heuristics which the authors suggested, and sparse JL can often be reasonably efficient in practice. In light of all this I am keeping my score, but would encourage the authors to perhaps further pursue these directions. ===== update =====I am very grateful for the patient and detailed response. Due to limited time, I wasn't able to quickly follow up on the discussion.  I think the current quality of the paper is improved, so I increase the score slightly. However, I still struggle to follow some of the statements even after reading the response, it could be my comprehension or something to do with style/writing. /*************************Post-Rebuttal**********************/The authors address many of my concerns well, and I agree with their rebuttal.The modified manuscript also looks good, too.I raise my rating. =======================(Nov 24) The author response addressed my concerns and I therefore raised my score from 5 to 6. I particularly like the idea of using successor representation for density ratio learning. --------------------------------------Most of the concerns are addressed by the authors, and I raised my score accordingly. Update: score updated after author feedbackThis has been fixed in an updated version and is now not a problemThis has been fixed in an updated version and is now not a problemThe authors argue in rebuttal that ReLU networks on spike-free data is still different from a linear classifier, some experiments were addedAuthors have answered this in the revisionAuthors have answered this in the revisionUpdate After author feedback many of my concerns have been addressed, in particular the optimization problem templates are much easier to understand now as well as the derivation of the algorithm. Also some important differences between linear classifiers vs ReLU networks on 'Spike-free' data have been clarified. These were my main concerns and thus I am inclined to raise my score. =====POST-REBUTTAL COMMENTS========As I mentioned earlier in my review, I like the result and I feel it could be interest of the TCS community. However, as correctly raised by other reviewers, ICLR may not be the right venue for this paper and also it would be beneficial if authors improve the presentation of their result and the motivation of their work further.  POST-REBUTTAL UPDATE:I thank the authors for the detailed response. Based on the proposed changes I will slightly increase my score, but I still believe the paper needs additional work before meriting publication. Update after rebuttal: Thank you to the authors for their detailed response. The clarification and updates in the geometric skews section and the more explicit justification and connections between the framework/theoretical results and the experiments improved readability and clarity. I also appreciate putting greater weight on the theoretical contributions and "easy-to-learn" task definitions, which provide a simple but non-trivial test case for robustness research. I'm increasing my score accordingly. I stand by my initial review that this is a strong submission, and having read through the other reviews and author responses, I am raising my confidence level as well (I think I have a solid grasp of this work's potential import). I disagree with critiques of the paper's novelty and practicality -- I think it provides new insights into OOD problems with substantive theory (not common) and provides actionable insights to boot. Also, the the revised manuscript is much improved. I hope this gets accepted.----- ####post rebuttal####The contribution of this paper is marginal only. The link between adversarial robustness and calibration has been explored previously: Snoek et al. NeurIPS 2019 have shown that adversarial training as part of deep ensembles leads to better calibration under domain shift. Unfortunately the authors do not compare their approach to these deep ensembles, but only an ensemble of differently initialised vanilla networks without adversarial training, which they call deep ensembles (section 5.1). Also in terms of label smoothing the contribution is marginal: in their rebuttal the authors show that MixUp training - a different implementation of label smoothing combined with input smoothing - has a better performance than their method (ECE of 1.8 MixUp vs 2.3 their method for CIFAR-100); they do show that further post-processing improves their method, but this is likely true for MixUp too (results not shown). For ImageNet results for MixUP are not shown, nor for calibration under domain shift where MixUp is likely to perform well too. Taken together, this suggests that the link between adversarial robustness and calibration is mainly a link between OOD samples and calibration: generating OOD samples with input smoothing in MixUp works very well compared to the proposed approach, as does adversarial training in deep ensembles (both of which was shown in prior work). In summary, the proposed approach lacks novelty and performs worse than baselines for complex datasets.Lack of code during the reviewing phase means it is not possible to review reproducibility of results. The author addressed my concerns. Ill keep the score 6.======================= ----------### Feedback after discussionI would like to clarify my thought here. Recall that using the weight decay is equivalent to adding the L2 regularization to the training objective. An important observation here is that the addition of the L2 regularization to the proposed objective will make the global optima non-trivial (apart from the trivial ones I raised), and there might be a hope that the new global optima has some useful properties. What this observation indicates is that the use of the L2 regularization (or weight decay) is an essential factor for the proposed method to output something meaningful. This fact also implies that the analysis of the objective function alone (without the regularization, in Section3) is no longer meaningful. Moreover, because the L2 regularization (or weight decay) is an essential factor, the tuning of its weight should have a major impact to the resulting model. I therefore think it will be important to investigate the effect of such a weight in the experiments, instead of just using a standard weight. After response: Thanks for the clarifications. However, conceptually important questions are not yet clarified yet. For example,  the objective itself can be made into a pure square function (and hence strongly convex) in both classical Wolfe's formulation and the proposed. As the authors are pointing out, the main issue is in designing separation (or projection) oracle for the constraints which corresponds to the base polytope in the context of submodular optimization and was the main motivation for Wolfe's algorithm. Moreover, authors mention that main difficulty in using projections is intractability but it is not clear why the linear optimization performed in the proposed algorithm is efficient. Post rebuttal:- The authors have addressed most of my main concerns and the additional experiment looks good to me. Therefore, I increase my score to 6. However, some experiment results are still missing and the paper still needs some editing before published, especially the experiment section. For example figure 6. is misleading since the comparison is not fair. ### My final recommendation The authors have attempted to address some of my points but these points require more time to fully address as they require to run more experiments. For the current form, I remain my inital score and recommend rejection for this time.  -------------After author's response--------------As the other reviewers also pointed out, some baselines are missing and ablation studies on meta-data are missing as well. Meanwhile, my concern is that the presented idea in the paper looks very similar to that of the neural process, except the paper is optimizing over a point estimate of z while NP is optimizing over q(z|D). This remains unexplained in the author's response. Therefore I am keeping my original evaluation at the moment. _UPDATE AFTER REBUTTAL_The authors have improved the paper somewhat by expanding and clarifying the discussion on some key parts. While I think there is still much room for improvement in the paper, the general consensus seems to be towards acceptance. I will not oppose if that is the decision, and have increased my score accordingly. However I remain very borderline and I am not sure if I am fully convinced by all the claims.One specific issue: I think the authors should make it more clear in the paper that the experiments are done in 128 pixel resolution, in light of R1's questions. It is important that the reader be aware of this, as the noise inputs arguably become much more important in high resolutions where there is more stochastic detail. I personally did not realize this when writing my review, and now wonder how the results would be at e.g. 256 or 512 resolution. If possible I would suggest the authors still run such experiments. This also probably explains my comment above on the lack of apparent visual differences in inversion results. [Update after reading authors' comments]Based on the authors' and other reviewers' comments,  I keep the score unchanged. Comments after authors' rebuttal:Thanks for addressing my comments. However, I think the current submission needs further work. - The authors agreed with me on my point that the "max-margin" claim might need further work, and since this is an important claim in the paper, I cannot improve my review score after the author response.- Different data splits should not be a barrier for comparison against previous work. UpdateI have increased my score to 7 after reading the authors' response and the updated paper.###  ---**Update**---Increased score 5 -> 7 thanks to clarifications from authors. Post Rebuttal =====Thanks for your response and for revising the paper. I have increased my score accordingly. Some Typos/Mistakes:p. 1: In paragraph 3, introduce  T .p. 1: Introduce  5K[5M5V].p. 2: there exists functions -> existp. 3 (and elsewhere): for factored Markov chain -> & chainsp. 3: Knapsack setting -> the knapsack settingp. 3: & is more concise -> are p. 3: We use & estimation of  5IÆ  -> & estimation of  5IÆ , respectively.p. 4: do not harm to the & -> remove "to"p. 4 (and elsewhere): omits higher order factors -> I believe you mean here that  5  collects some higher order terms in the expression.p. 5: all possible value of -> values p. 6:  5K[5M5V]  -> Did you mean 5K[5M5V]p. 7: for a  5[¯¯  -> for a factor of  5[¯¯ p. 7: focus -> focuses p. 7: & but counts the cumulative cost & -> "counts" has made the statement rather unclear.p. 8: from state to & -> from states and budget to actions (or state-space & to action-space)p. 8: With prob. at least -> shortening probability to "prob." does not seem to provide any gain in the space. p. 8: To be more formally -> formalp. 8: in interesting future work -> an interesting & ### Post Rebuttal ### Thanks for the clarification, I have adjusted my score accordinglyWhile the paper's second contributions allow the incorporation of a more refined resource constraints, it appears to me that the the proposed approach is unlikely to yield a computationally tractable algorithm even in the tabular setting, when the original state space is small. Indeed, the vector of remaining resource levels are embedded into the state, so that it will results in a curse in dimensionality even when the original space is of a manageable size. Overall, based on my evaluation on the theoretical and practical impact of the paper, I find that the contributions fall marginally below the acceptance threshold.  Finally, after the statement of the regret bound in Theorem 2, it is helpful to compare with the regret bound in the recent work by Tian et al. 2020, similarly to how the proposed algorithm is compared to Osband & Van Roy (2014). ############################################################Post-Rebuttal:After reviewing the concerns raised by the other reviewers, and the responses provided by the authors, I have decided to adjust my scores.Moreover, I was disappointed that the authors did not use the extra one page to move some material from the appendix to the main text in order to elaborate on the proposed method. Based on author feedback, here are some additional comments:I would like to thank the authors for their response to the reviews. As noted in my original review, the proposed method is a well-engineered method for a particular type of document recommendation problem. Empirical performance on the chosen data sets is impressive compared to the baselines. The claim on gain in training speed is suspect as there is a hyper parameter tuning step that has been not taken into account while reporting the speed gain over the baselines. Label correlations are not taken into account for label embeddings and might hurt the performance when there are not many documents in the training data for the long tail of labels in many real-world applications. Random embedding puts unrelated labels into the same bucket. Though this doesn't seem to hurt retrieval performance in the experiments reported in the submission thanks to filtering, it is not clear how training will be affected by this non-semantic bucketing of labels. Overall, I think the submission has several things going in its favor though there is substantial scope for strengthening. I've updated the rating.  ===============================================================================================================\After rebuttal:\I have read the authors' response, but since the actual body of the paper has not changed much from the original submission, I stand by my original rating. === Response After Rebuttal ===I thank the authors for their responses to my comments. After reading the response as well as the other reviews, I still stand by my original rating. I still find the motivation and empirical results non-compelling, given the current version of the paper. Update after rebuttal period================ The connection between the contrastive learning objective and discriminative learning is made via "resemblance". And the author claims the "resemblance" as a theoretical contribution, which the first reason I vote for a clear rejection. This issue has not been addressed by the authors. The second reason for my rejection of the paper is the paper requires an effort to make it self-contained, especially for the experimental section. I remain my score of clear rejection.  ==================== Post rebuttal ====================I thank the authors for their response and additional experiments. Please see my comments below.Relation to [1] The first version of [1] appeared on arxiv in February. I am not sure if it has been published since then or not, but regardless it is sufficiently ahead of the ICLR submission deadline to consider it a prior work. [1] provides a Mapper optimization algorithm in Figure 4. There are some minor differences with your algorithm, but I don't see how they make your algorithm more communication efficient. Generalization analysis in [1] is for the mixing parameter learned from data (i.e. adaptive), that is why there is no dependency on it. Your analysis is for a fixed mixing parameter, but since it is not possible to know it in advance, learning it from data seems to be more reasonable. So I am not sure what is the advantage of a theorem with explicit dependence on it. I agree that the convergence analysis is new, but [1] also has two more algorithms.Experiments Despite that your paper has more experiments, EMNIST experiment in [1] is better suited for studying personalization in my opinion. EMNIST results in Appendix B in the submission are for digits only (and also use fewer clients, but that is less important). One of the reasons that a centrally trained model on EMNIST performs worse than personalized models is the shift in the distribution of characters and digits across clients. This is not the case for MNIST/CIFAR10: if I train a neural net using all of MNIST train data it will easily achieve 99+ average test accuracy (without any personalization). How the test data is split across clients does not matter because accuracy on each digit is roughly the same. That is what I meant by a "single global model with good performance (i.e. trained on the full dataset)".Having worked on FL with personalization myself, I've noticed that it is quite hard to achieve a meaningful improvement over the vanilla FedAvg + fine-tuning. This is also evident from Table 2 of [1], where none of their algorithms offer a convincing improvement. This paper claims that an algorithm very similar to Mapper [1] indeed outperforms FedAvg + fine-tuning. I'd be happy if it is so, but I do not find the provided experimental evidence sufficient. I recommend reproducing the EMNIST experiment of [1] (please don't discard characters, use the same number of clients per communication, etc.). If your algorithm can achieve 91+ accuracy, I'd consider it an improvement. In that case, a more detailed discussion of the differences with Mapper [1] that enabled the improvement would also be great.Regarding the number of parameters, based on eq. (1), personalized model is a convex combination of two models. So if I understood correctly, the number of parameters is increased both during training and testing. After the discussion, my concerns were fixed. The paper explores the interesting relations of Mix Up and Uncertainty, which is useful and will be the right fit for the conference.  ------post-rebuttal updateI appreciate the authors for the responses. While the other reviewers give high reviews about this manuscript, I would keep my original reveiw for the following reasons. 1) This manuscript is incremental in nature. I agree with R3 that "understanding which techniques can be combined and which cannot (and how to fix it) is important for developing the field and feels like a small step to the right direction", but a somewhat unsurprising result lacks technical novelty. In one of the responses the authors wrote "The spirit of this work is to point out that not all data augmentations can be combined well with ensembles." I think this potentially means that the conclusion the authors made on mixup could not even transfer to other data augmentations. This even further limits the contribution of this manuscript. So I guess the above argument from R3 is not convincing, or one could try to study the effect of batch norm and ensemble and wrote another good paper. 2) The empirical performance of the method is limited and thus whether the proposed method is useful is a question. # Post-response updateThank you to the authors for very helpful clarifications. This paper provides a reasonable start for a new potential direction in NAS research and so may be worth presenting at the conference, but the justification and applicability of the method is somewhat limited. I therefore stand by my original assessment. # After rebuttalI would like to thank the authors for the hard work during the rebuttal. The ablation study of the data augmentation strategy and other added results are very helpful. Regarding the explanation of the flexibility and invariance, although I could get some intuition, I am still not fully convinced. So I keep my original rating. One possible way to make this work stronger and meet the acceptance criteria is to provide some empirical (or even better, theoretical) analysis of the influence of $\Sigma$ on the flexibility and invariance of a network. ===================Thanks for the rebuttal and the additional empirical results. In general, I really appreciate the "brave new idea" proposed by the authors, which could inspire new insights on neural architecture design.  However, my major concern still exists: the proposed metric seems to be sensitive to the choice of initialization functions. For example, uniform initialization [0, 1] seems not work at all but no further explanations, which may indicate the proposed method may work in a different way (e.g. taking advantage of some search space's bias).  So, I keep my original rating.    Pros:1. this paper studies an interesting problem, "imitation gap" in imitation learning. 2. the paper is easy to follow. Especially, the example part is easy to understand.3. the intuition for this idea is well-explained.Cons:1. from my perspective, the basic idea is dynamically using imitation learning and reinforcement learning for agent learning by a weighting function. It is a straightforward idea but lacks some novelty. 2. the main contribution of this paper is proposing an advisor and integrating it with the imitation and reinforcement learning process. However, only averaging loss to leverage imitation and exploration is a comparatively little contribution.3. in experiment PD, adding advisor from the beginning seems not fair enough, since at the beginning the reward is much higher than baselines. Maybe the experiments need a comparison before the advisor is added and after the advisor added.In summary, in this paper, the motivation is clear and easy to understand, and the problem is worthy to study. But the contribution of this paper is a little limited. A better solution is needed. [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. I  do believe some of my concerns have been addressed while the main issue on the lack of some necessary evaluations (e.g. num. of demonstrations) remains. Thus I may not be able to escalate my justification to even higher. =====================Post RebuttalI went through the authors' reply. My first concern is resolved by the reply. Form the authors' replies to all reviewers, I believe this is an incremental work. It is technically sound, but the lack of involved and novel technical contributions makes it more belong to an incremental work. Thus, I will keep my score unchanged. EDIT: The statements about ERO clarify the contribution considerably. 6-->7 ## Post-rebuttal commentsThanks the authors for the response! I've read it and other reviewers' comments. I feel the authors didn't directly answer my questions and just reiterate what they have in the paper. Unfortunately, it is still unclear to me how to perform meta-training on standard FL training tasks, for example, shakespeare in [1]. In this training task, there're total 700+ clients. Does that mean in the meta-training phase, we need to sample 700+ clients for each episode? How to construct this meta-train dataset from a standard federated dataset? Update after rebuttal-----------------------------Thanks to the authors for addressing my concerns. I have updated my score. Update:After seeing the author response below, no change to my score. After reading the rebuttal: I increased my score to 5. I still feel that the writing style is hard to follow. Besides the examples that I wrote in my original review, there are many other places where the notations and definitions are not clearly written.  *********after reading rebuttal *******I have increased my rating, but may still have some concerns. See below.  After reading the responses from the authors and other review comments, I maintain my previous rating of this paper. I am not convinced that the proposed approach is a simple form of analogy reasoning. Trying to build a relationship between the proposed approach and analogy reasoning is uninformative and misleading. ##### Update ######Although it is responded that the simulation setting used in the paper does cause blowing up samples or marginal variance, it is in general impossible or the setting assumes too sparse case where considered graphs are almost empty. In addition, as you mentioned, I also ackowledge that it is a widely-used setting; however, there are a lot of papers that are rejected because of the unfair simulation setting. I like the main idea of the paper a lot, and hence, I hope the authors set the simulation setting more carefully. Furthermore, it is really frustaring answer that the authors consider the only case where graph is uniquely idenfiable from the pure observations. As you know that is really rare when the number of nodes is large (p > 50).  # Post-rebuttal commentsHello everyone,I have read the author's response and I am leaning towards rejection. The paper can be divided into two halves. The first half where the authors obtain bounds on ranks of DAGs is the main contribution of the paper and is clearly interesting. The second half of the paper tries to shoehorn these bounds into an algorithm for learning causal DAGs from observational data which is disappointing and is clearly below standard for the following reasons:1. The bounds depend on the underlying DAG which is unknown and therefore cannot be estimated from samples. Therefore the authors propose using "structural priors" to obtain these bounds. The authors don't mention where they get these structural priors from. Furthermore the bounds are only useful to restrict the hyper-parameter search space in the matrix factorization approach which is applicable to linear SEMs. These bounds can only be used "qualitatively" to guide selection of regularization penalty in the nuclear norm approach which is necessary for non-linear SEM methods.2. The theoretical results would still be useful if the authors could adequately demonstrate that for certain family of graphs the maximum degree can be high while the rank can be low therefore learning DAGs subject to sparsity constraints (whose sample complexity depend on the maximum degree) can perform worse than learning DAGs with rank constraints. However, this is not clear since in experiments the authors only show the SHD as a function of "average degree" and not "maximum degree". Figure 2 again compares rank against average degree and not maximum degree.3. The experiments are only performed in the low-dimensional regime at a fixed sample size (3000 samples and 300 nodes). Update:The authors have explained the issue raised in the review. It's not ideal that the algorithm requires the knowledge of rank beforehand, but it's okay if this point is clearly communicated in the paper. I would keep my current score.  # UpdateI thank the authors for extensive replies and updates to the paper. Most of my questions are answered, and the paper quality is substantially improved. I would not be opposed if other reviewers recommends to accept it. Unfortunately I still can't raise the score and advocate for it myself, since:1) Table 2 doesn't really show superiority of new nonlinearities, since the bolded (bottom) entry has both trainable $\lambda$, centralization and Mixup, while the baseline of dSELU only has mixup and trainable $\lambda$, no centralization. Without centralization (Mixup + trainable $\lambda$), lSELU/sSELU/dSELU perform comparatively. Without trainable $\lambda$ (only Mixup), they perform a bit worse than dSELU. With only centralization, BN is still better. Further, even after rebuttal, I still believe that making $\lambda$ trainable effectively cancels the preceding theoretical discussion, and makes this subset of results somewhat unrelated to the main idea of the paper. Finally, Table 1 shows a more robust benefit over dSELU on CIFAR-100, but not on CIFAR-10/TinyImageNet (where s/lSELU can be both better and worse than dSELU), which is in my opinion underwhelming given the added implementation complexity.2) Figure 4 is very promising, but I find that SELU doesn't look that bad on it, which again makes me wonder whether the new improved self-normalization actually matters in Tables 1/2, especially given gradient clipping and other heuristics in Table 2.So at the moment I find that the proposed nonlinearities are promising in terms of both self-normalization (Figure 4), and generalization (Table 1/2/6), but these results appear to be largely unrelated to each other, and neither of them in separation is strong enough to convince me to try s/lSELU over dSELU (given additional implementation complexity + the boolean hyper-parameter of whether to use lSELU or sSELU) or over BN (given additional hyper-parameter $\epsilon$). I wish the paper either showed clear use-cases where one can't train BN/SELU/dSELU networks at all in reasonable time (but new nonlinearities allowed it due to superior normalization), or more robust generalization results. ----------Update after rebuttal: I thank the authors for the detailed response. Some of my comments have been addressed. However, some of my major concerns remain such as the insufficiency of the hyperparameter tuning for the baselines to do a fair comparison. I am keeping my score. **UPDATE:** I appreciate the authors' responses and the engaged discussion. However, I still think that the claims of the paper are not sufficiently supported by the presented results, and maintain my original rating. ##################################Post-rebuttal:The idea is good, but the experiments and analysis are not enough to validate the proposed idea. The paper is not ready for a publication. ## After rebuttal update.Given there is no rebuttal, there will be no update. -----------------------------Rebuttal: Thank you to the authors for addressing the comments and for the changes to the paper, and thank you for adding the examples on Scurve and Swiss Roll and the comparison with Isomap. I find it slightly surprising that Isomap performs very well for local metrics on the spheres dataset, because Isomap tends to preserve larger distances. Not sure I fully understand why.Related to my initial concern about clustered data, my impression would be that in addition to the spheres visual example in the main paper, it would have been good to add the Scurve or the Swiss roll to emphasize that UMATO is not specifically designed for clustered data (all the examples in Table 2 are for data with implicit clusters). How do the methods perform in terms of the local and global metrics for the Scurve and Swiss roll datasets? Would zooming in on PCA reveal similar local structures to UMATO? What is the difference between y_i and y_i' used in eq (8)? Are they the same?Second line on page 2: Should it be UMATO instead of UMAP?   ##########################################################################I raised my score based on the author's response. Post Rebuttal:I appreciate the authors' responses. The "novelty" over Zhu et al. was never under question in my review, I was mostly confused about how to weigh the significance of the findings, how useful it is to know some numbers for the version of the dataset created by the authors (which is not really the original Imagenet classification task), and if the submission actually does "pinpoint" what the problems are, how and when they manifest, to what extent the dataset is responsible vs. the training choices. Having read the other reviews, responses, looked at the updates, I'm still unsure --  if there were something in this paper that was new or surprising and not more or less already known from existing works (perhaps not precise numbers, but then the paper is essentially using a synthetic, modified Imagenet anyway), I'd be more enthusiastic about pushing up the rating. But as of now, I'm retaining my initial rating. *****Post Rebuttal*****I would like to thank the authors for the detailed rebuttal.The authors state: "The main goal of our paper is NOT to introduce novel models, but rather to introduce a NOVEL benchmark and insights/ingredients to study causal induction in model-based RL" and "It is true that the models we use do not learn an explicit structure for causal learning" which corresponds to my original reservations to the novelty of this paper. The authors introduce a benchmark / software for evaluating causal induction in RL models, where the user can specify a causal model and its influence on the environment can be examined. I remain unconvinced that the introduction of a RL evaluation benchmark (even one allowing for the presence of arbitrary causal networks) counts as novel at ICLR.Further, the statement "we used some of the typically common models for this purpose, such as GNNs and modular networks. Though these models do not learn an explicit causal graph, they do learn structure that could allow them to discover causal relationships under certain assumptions" confirms my point that no causal formalism (e.g. connection to the data generating process) is accounted for. It merely means that directed relationships can be modelled.I agree with the authors that (Lopez-Paz et al., 2017) only uses observational data and does not have any connection to RL, but it is an example of an approach to extracting causal relationships from images in a sound way when it comes to causal inference. This is a side comment and does not influence my assessment of the paper.To sum up, my reservations towards the degree of novelty in this paper have not changed (I agree with the authors' summary of their contributions, but disagree as to whether proposing a new benchmark constitutes novelty at ICLR) and I recommend a rejection of the paper in its current form. ##########################################################################Post rebuttalI'm happy with the author's response and would like to keep my original score. Post-rebuttal:Thanks for the feedback. The goal of learning causal representation is ambiguous, and it is absolutely a good research topic. However, I fail to see an obvious contribution of the current version. Researchers in this field are usually clearly aware of the limitations for existing methods in causal learning. The problem is how to handle it, e.g., how to give a appropriate definition of the causal variables, how to theoretically show the identifiability and consistency, and then propose a practical solution.  === Post rebuttalThe authors' rebuttal addressed some of my concerns, but my primary concern still remains that that benchmark may be a bit too contrived, where the observations made in this paper may not generalize to more complicated real-world situations. The authors also made some far-fetched arguments in the rebuttal by claiming some concurrent works [1, 2] as "the 'real-world' version of the environments used in the paper," which, to be honest, further lowers the rating of the paper on my side: why is this paper worthy of acceptance if there exist more realistic benchmarks?I also agree with R1 and R3 that there are no new methods proposed in the paper, and the insights derived from benchmarking a set of existing methods may not be considered novel from the point of view of the ICLR audience. As a result, I keep my rating the same.[1] Physically Embedded Planning Problems: New Challenges for Reinforcement Learning, https://arxiv.org/abs/2009.05524[2] CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning, https://arxiv.org/abs/2010.04296 The paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency. In the beginning, "I think it is an interesting piece of work, and it may be helpful to develop the basic structural design of neural networks. ". However, after getting the response from the author(s), I more doubt the significance of the work in this paper: although many types of models have been proposed in this paper, the improvement over the baseline models is limited. I did not lower the grade on this paper since I thought it would be interesting and important (if effective) to extend the traditional real number field to more complex algebraic structures. NOTE: a lot of disputes are around "the huge title 'AlgebraNets'". However, I did not receive justification response from the authors. A possible reason may be the authors are not aware of how big the topic it is, and were so attractive/confident in the current experimental improvements (which is also very appreciated). 6. Post-rebuttal UpdatesIn light of the authors new experiments and response, I have increased my score.  The bootstrap error shows slight deviations under the MSE, and thus, I do feel that the scope of generality of this work is still limited.  However, I feel that the presented framework is novel and the experiments consistently demonstrate that the bootstrap error is consistently low under soft-error in a number of settings.   ===== Post-Discussion Update =====I appreciate the authors' efforts for responding my concerns and comments. The provided response and the update of the introduction do help better explain the motivations and the implications of the proposed generalization framework. Therefore, I raise my previous rating a little bit to reflect this. Although the proposed framework may suggests a potentially-interesting future direction for the deep learning research community, it still does not fully convince me its feasibility. In particular, the paper would be much stronger, if the author can dig deeper with the Real-World test (soft) error, which I view it as the end-goal of a real-world classification system, to provide more specific directions on how to make it smaller, or more importantly, how it changes the current deep learning training paradigm. EDIT: The author response addressed all my concerns and answered all my questions, in particular that the exponential running time is a worst-case bound that is indeed loose in practice. I believe that the revised version will be much clearer and am therefore increasing my score ---Thank you to the authors for their comments and clarifications.  I still am skeptical that the proposed dataset is a fundamental improvement over Quick Draw---ultimately, both datasets contain compact sketches from a single category containing relatively few strokes. But given that your updates and clarifications have addressed many of my questions, I am raising my score. Post-Rebuttal:I would like to thank the authors for addressing the questions and concerns. I still believe that the general idea of conditional networks might pose a relevant contribution to improving OOD generalisation. However, after rereading the submission, reading the other reviews, and taking the rebuttal into consideration, I think there are some aspects which need some revision and clarification. I comment on this in more detail below. Therefore, I stand with my initial rating of borderline, but I would like to encourage the authors to revise their paper taking the points raised by the reviewers into consideration and submit again.- **Section 4.2, discussion of figure 4 and hypothesis 2:** I thank the authors for the added discussion. However, the results are a bit at odds with the premise of the proposed approach, in my opinion. The bullet points which detail why hypothesis 2 could be interesting, are refuted by the results presented below in that section. I believe the result that activations look different (what concerns scale of activation and which features are active) by itself is less surprising as the approach tackles the normalisation of activations explicitly. But more importantly, other than that, I would say the activation patterns for different cities look qualitatively similar in both models and does not align well with the story of the paper. So, in my opinion, the activation patterns on the left (AMLL U-Net) in figure 4 look very similar for all cities, and also the patterns on the right (Cond. U-Net) look quite alike for different cities. Therefore, the interpretation of these results in the context of conditioning on auxiliary information remains speculative. I believe this part of the submission requires a careful reconsideration. - **Geocoordinates as metadata *t_n*:** A potential reason for the difficulties in the previous point could be the choice of metadata *t_n* in the segmentation example. I thank the authors for elaborating again on the choice. Still, I am not convinced that this is the most suitable choice to present the advantages of the proposed approach. If the conditioning network really just performs city ID classification, I would not be sure that any kind of useful (generalisable) features are extracted. This could be a potential explanation why no conditioning influence on the results in figure 4b is observed.- **Difference Cond. U-Net:** I shouldve been more explicit in my question. On page 6, last paragraph of Generalization via conditioning it reads: *We identify as Cond. U-Net those models in which both the encoder and decoder are modulated, which yielded a small gain in performance over just modulating the encoder or decoder alone.* If I see correctly, Cond. U-Net should be replaced by Fully Cond. U-Net, as the provided answer suggests, too.###  ::::::Post-Rebuttal update::::::After reading the new revision, I decided to keep my initial score. I do not consider the need of groundtruth real data for metric computation as a strong disadvantage. The authors report some numbers on Recall in Table 1 but it only shows that Recall is consistent with RND, being much cheaper to compute. Therefore, I do not see any reason to prefer RND over established diversity metrics. -----------------------------------------------I appreciate the authors to improve the clarity and consistency of the method.I understand it has a practical value, easy to use and yet benefit from the method.However, if the authors want to stress its practical value, we need more convincing experimental results beyond MNIST and CIFAR10.Thus I concluded it is still below the acceptance threshold. POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. Most of my concerns were addressed. This is a simple, but nice idea. After reading the rebuttal and the other reviews I am recommending to accept the paper. ----------After RebutalI appreciate the authors response to my questions and they addressed some of my minor concerns on the clarity and presentation. The authors confirm that that their work is a pure study on UQ for forecasting. In this case, the methods evaluated is not very specific to the forecasting tasks, and the conclusions are not very surprising either. Therefore it seems there is limited technical merit from this paper. The setting of the COVID forecasting task is also a bit strange and the authors response does not fully address my concern. I would like to keep my ratings unchanged. **Post-rebuttal**Thank the authors for the rebuttal. It addresses parts of the raised issues. However, my rating keeps the same after reading the rebuttal and other reviews because1. the contribution and utility of the proposed method are not significant;2. the writing needs improvement; and3. the experiments are not convincing enough, and its advantages over previous methods are not clear.  ========================== Post Rebuttal ==============================The authors did a good job in addressing some of my concerns in the rebuttal. Thus I am increasing the score in response to the clarifications. However, I feel there is still some improvement space for the experiment part of this section, and I encourage the authors to incorporate the changes, including ImageNet experiment and following stronger baselines to make the results more solid and convincing. ---------------------------------post-rebuttal---------------------------------I appreciate that authors have provided rebuttal that addresses many of my questions, though I'd like to maintain my initial rating due to the following comments. I think this paper is at the borderline.In terms of explaining why "Euclidean distance is not sufficient for learning such a hierarchical regularization", I don't find the illustration example in Section 2.3 intuitive or concrete. I don't think Eq3 adds much as the paper does not explain further. Perhaps the confusion is from that the paper does not explicitly explain what "optimal classifier" mean in terms of Eq3.The authors only say "those parameters and schedule were optimized for SGD on plain networks, probably sub-optimal for our proposed methods. It is not clear whether other methods suffer severely from the choice of learning rate and scheduler. As far as I know, SGD is sensitive to the initial learning rate. So I am worried that setting the same learning rate is not fair to comparing different models that have different structures.From the updated paper, I find the blue line in Page-2 confusing. It is not clear about the logic: why diversity reduces over-fitting. (Xie et al. 2017) studies this point with a complete paper. But the way that authors simply put it is quite unclear how this statement is related in the context.Visualization is interesting to look at. But it should be better analyzed. For example, visually all methods produce similar tSNE visuals in Figure 5. But are there any essential difference? --Update after rebuttal--The reviewer thanks the authors for addressing the key questions and concerns and have updated the confidence score accordingly. * Post rebuttal update:  Several of my concerns about clarity of the experimental section were addressed. Therefore I increased my score and now I am inclining towards accepting the paper. From the discussions, it seems that there are quite a bit of concerns raised about the experimentation process. On the other hand, the responses, and presentations in the paper, are also quite convincing to me. So I believe this result is ready to appear in the conference, if anything for the further discussion/interest it will generate, and would still like to recommend acceptance of this paper.         Thanks for the clarifications from the authors. The discussion was very helpful.  ---- update:Thanks for the update. I guess the "intuition" is driven mostly by empirical results, which I suppose is ok but may be worth digging into a bit more. I have updated my rating. ####update####The experiment results are not surprised, but strong enough. Still no very strong baseline provided in this submission, but it might be good to set up a benchmark in this direction. However, T5 model needs more computational resource and the experiment results are hard to replicate. Overall, I would like to keep rating. After authors' response:Thanks for the response! I will keep my score. **EDIT: with author feedback and changes to manuscript (and supplementary) I think that the study is more interesting than expressed in my original review, however the sim-real gap with respect to applicability to robotics is still a major concern IMO. I have updated my score accordingly.** ======== POST REBUTTAL RESPONSE========After reading the feedback and revision of the paper, most of my above concerns are addressed. I agree that the comparison with HER/CER is not fair. I also notice that the author added more references and details based on all 4 reviews.  Thus, I decided to improve my score on this paper. ===Post rebuttal===I would like to thank the authors for the detailed response and clarification of my questions. I believe that this paper will be valuable for the ML community. **Update** : Since nearly all of my issues have been addressed, I have changed my rating from 5 to 7. Best of luck :) ===================== Update =====================Raised to 6. A theoretical calculation on 4. would be great, if it can be done before publication. NN can be simplified, say linear, or one hidden layer linear.  After reading the rebuttal, some of my concerns are addressed by the additional experiments. But I also agree with other reviewers that the result is not very surprising. As R4 mentioned, the proposed method depends on the a specific downstream task where the "small" "general" BERT can be further pruned. For a fair comparison to previous work, baselines that are applied to a specific fine-tuning task need to be compared. ===== ---Post rebuttal:I thank the authors for responding to my questions. While some minor points are cleared up, I am still not completely satisfied with the rather vague notion of "bilingual knowledge" (what even is the "correct axiomatic translation correspondence", is it the (idealistic) true data distribution one tries to model?). Similarly, the use of the MI as a measure of success is still unsatisfying to me since it relies on a "trick" (mismatch of vocabulary between source and target) which might not even be relevant in practice (since most models use sub-words anyway).Overall, I think this research direction is promising, but I keep my recommendation the same. The paper would greatly benefit from another round of revision to clear up these points and clarify the presentation if it is to be useful to the research community. Update after Reading Authors' Response:I appreciate the authors thorough response and the new results for different crop sizes. Its not too surprising that the performance varies with the crop size and that the best crop size depends on the task and environment (among other variables such as camera resolution, camera viewpoint, and object sizes). And so, I share the concern of the other reviewers: its unclear whether this approach would be similarly successful in other manipulation settings without prior knowledge of the task outside of RLBench. --- Updated score ---The authors did a nice job of addressing many of my concerns. But there are still some lingering issues with the experimental design (especially with the reinforcement learning experiments). The main concern I have is why the variance for your results is so low from run to run. This could suggest the problem is too easy, or that there is a bug somewhere. While I don't think it is enough to outright reject the paper, it still puts me on the fence about a strong accept. ==== Update ====Increased score according to the revision and discussion. ************After Rebuttal:I thank the authors for their multiple clarifications, and apologize for my initial misunderstanding.I understand now that the flow model is used to compute $p(x|z)$ as a function of $x$ and $z$. Maybe the "decoder" terminology is a bit confusing here, but this is quite a nice idea overall. It would have been nice to see multiple samples from  $p(x|z)$ for a fixed $z$, to evaluate the expressiveness of the model.I'm raising my score to acceptance.PS: Some typos remain in the revised version, eg: "varnishes", "tne" ------------------The authors answers one of the most important concerns, I have raised score to 6. =======================================================================================================Comments Post Rebuttal:I still find the technical contribution of this paper to be a good one. However, As stated in my original review, there were some clarity issues with the manuscript. While, I personally was able to follow along, the other reviewers are correct in their claim that the writing can be quite confusing (Reviewer 4 makes a strong case). To that end, I have decided to decrease my score to a 5 because I can no longer say the manuscript is ready for publication. Nonetheless, I urge the authors not to be disappointed. The presented work has many merits, and I am sure that with a thorough "clean up" of the text, this paper can be a great one! ### Final Recommendation after Author Response###The authors have addressed several of my main concerns. It would have been helpful to study transferability to tasks beyond image recognition, but overall, I think the paper has been considerably improved. I increase my score to 7.Two remarks regarding new content: * I find it misleading to denote PGD as a targeted adversary and additive Gaussian noise as a random adversary Section 7. "Targeted" usually refers to an adversary that aims at achieving a specific misclassification (target class). Gaussian noise is not really an adversary, rather a distortion/image corruption. I would recommend clarifying the naming to avoid confusion of readers. * Is there any particular reason to use PGD(3) in Table 1b for evaluation? Would the effect hold also against stronger attacks (more iterations etc.)? ---Post rebuttal---Thank you for the response, and thank you for checking the performance comparison against the white-noise perturbation. It would be interesting to see a future work involving means other than Adversarial training (e.g. including other simple mechanisms like weight decay and dropout) to help reduce the overfitting effects in the pretraining phase. I would like to keep my score as is.  ----------- updates after reading author response -----------After looking at the authors' response, the revised paper, and the other reviews, I'd like to update my rating from "Accept" to "Marginally below acceptance threshold". While I still like the overall idea of this paper and I believe this is an interesting direction of research, the additional results in the revised paper actually raise more questions and there are issues that need to be addressed in this current study.1. Properly quantifying "proposal neglect"The improvements shown in Table 3 are very small, which do not support the claim that the proposed method produce "more and better boxes". Table 3 might actually not be the correct way to assess the "proposal neglect" (or resolving proposal neglect). I would expect to see a more in-depth result analysis. For instance, when comparing with the SOTA TFA approach, for each image on average, how many more correct objects are detected due to the proposed approach being able to find boxes that TFA cannot. Something like this would clearly indicate the benefit of the proposed approach.2. Sensitivity to hyperparametersIn Fig. 4 (left), there is a huge sensitivity to the hyperparameter selection. For instance, the performance of the proposed approach would drop drastically by just changing the number of RPNs from 2 to 3. This creates uncertainty for people to use this approach. Furthermore, for the COCO results, when the hyperparameters are not selected based on this set, the improvements are pretty small as compared to TFA. I believe a more robust hyperparameter selection method would be needed for the proposed approach to thrive. Post-review comments:After reading the reply I decided I will keep my low score though it hurts to do so for a paper into which the authors definitely invested a lot of energy. Here is the reason why:I think there are usually two ways in which a paper can make an important contribution: Through a new insight or through a new piece of modelling that will be widely used afterwards. I think in it's current form the paper presents neither.The potential insight I see is proposal neglect. However even with the added experiments in Table 3 I find the results neither sufficient to prove the effect exists. The GT boxes are already added so overlooking objects while fine-tuning should not be a big issue. If it was it should have a bigger impact (improvements in Table 3 and on general performance are minimal).So what about a widely usable piece of modelling? Despite the anecdotal motivation the presented method improves performance. So the question becomes: Will this be widely used? My prediction is that it won't. The improvement is rather incremental but the effort to use it is high. While the model is simple it comes with three additional hyperparameters which the authors tune individually for each experiment (Section 4, Hyperarameters VOC & COCO). This is the biggest issue I have with the method: It requires tuning a bunch of hyperparameters to achieve a marginal improvement.Compare this to TFA [1], the method the paper builds upon. TFA is built upon a very simple insight (fine-tuning the heads works better than comparing representations) and because the insight and model are easy to use they will be the basis for a number of follow-up works (e.g. this paper). I cannot see the same happen with the presented method as long as hyperparameters have to be tuned for each dataset and split. I am sorry but in my eyes this is bad practice!.To me this means the method will likely be of no lasting value. While it is SotA in the one-shot case for now I think the insights and methods used for achieving this performance cannot be used by other groups to improve performance even more. So what can I recommend the authors to do with the paper? I think there are two ways to increase the papers contribution: 1. Study different effects and problems of RPNs in few-shot object detection. A better understanding will for sure help moving the problem forward. Even if the end result is: The RPN works surprisingly well. That would be a great insight as well in my eyes as it would free resources to address the other problems.2. Improve the method so it provides a significant gain without any additional hyperparameter tuning. At only 3% more computing time using it would probably be a no-brainer if it was not for the hyperparameter tuning.[1] Wanget. al ICML 2020, "Frustratingly simple few-shot object detection"------------------------------- --Post Rebuttal--Thank the authors for the response. I agree with other reviewers that the task is interesting and the submission has great potential, but might need another round of editing. The results show "a single perturbation from a random state of a completely different MDP" is not normally distributed. However, I agree with R4 that applying a single perturbation from a random state of another MDP does not make sense unless the two MDP and the two states have some similarities. EDIT: Updated recommendation to acceptance (7) following author response. =========After rebuttal======After the rebuttal, my main concern remains. Specifically, the paper defines a variational distribution q(z|x0 via a hierarchical construction: z_0 ~ N(0, 1), z ~ N(z_0, \eta I), which is essentially a zero-mean Gaussian. And I suspect the this is a bad variational distribution and it will induce high variance. The author said they didn't the hierarchical construction to define the variational distribution, because they fix z_0 after sampling. I don't think this is a formal way of defining a variational distribution. One reason is that even if they fix z_0, the proposal distribution will be a z_0-mean Gaussian, and the mean is randomly drawn from N(0, 1), which will not match the true posterior distribution (they only optimize the variance parameter). I think this should be make clear and investigated in more details. I will keep my initial score. ----------------Post-rebuttalI appreciate the authors' response and additional comparison against previous work. I do think that proper comparison with previous work is important, as it allows us to know better when and where the proposed approach is beneficial.  Edit: Markdown problem with nested listsEdit: Upgraded rating due paper improvements------------Thank you for addressing my concerns.Figure 1 is not updated (it looks the same w.r.t. the GP, but the change from variance to std should be noticeable?). **Update**My assessment remains unchanged. See the comments for details.The proposed approach is circular---it tries to sample points at a "sufficient distance" from the data, then tries to learn this "sufficient distance" by predicting the points. This circular dependency should be broken somehow. The current paper set this distance arbitrarily by sampling from a Gaussian with covariance $I$. [post discussion edit]After discussion, I lowered my  score to 'marginally above acceptance threshold':- the theory section of the paper looks very interesting to me- I find it hard to see clearly from the experimental section the extent to which the method beats existing methods- I feel that the experiments could be made more rigorous to clearly show the benefit compared to other techniques- concretely, I feel taht the tables could be structured in such a way that one can glance at each single table, and see clearly in what way LTP is better than the baselines. Concretely, for the results tables:- table 2: LTP gives worse accuracy than Renda, and worse compression. The text mentions training epochs are fewer for LTP, but the table doesn't show this benefit (there is no column with number of training epochs)- table 3: this table is a little apples and oranges I feel. it shows that the number of training epochs is less for LTP than Renda, but the compression ratio is slightly less. I feel that you could have compressed a little more, to make the compression ratios comparable. In addition, I feel it is important to include the accuracy in the table. Without accuracy, then I feel it is not possible to compare.- table 4: I feel you could do whatever is needed to do to ensure that the baseline model you are using matches the baseline that other teams are using. This could mean porting NTP to caffe, or porting caffe network into torch. Currently, the LTP pruning is on a worse 'parent' model, and performs worse than the other baselines in terms of accuracy. I'm not sure it's sufficient to hand-wavingly just add/subtract the delta in performance between the baselines to the LTP results (which is not explicitly being done, but if one doesn't do that, then one would have to assume that LTP performs worse, I feel)- which only leaves table 5 that plausibly provides an apples-for-apples comparison, but only for a single baseline Post-discussion update: I have read the updated paper and other reviews, especially from reviewer #2. While I am still positive about the approach/methodology, I am not confident about the technical details of the experiments, without which, it's very hard to justify the effectiveness of the method. I share other reviewers' views regarding inconsistencies, e.g. Tables 2-5, that have not been fixed in the updated paper. Post-rebuttalI have read the rebuttal and other reviews. The first version of this submission fails to cite any highly relevant works in long-tailed recognition and generalized few-shot learning. I believe that addressing this issue would require a major revision. The authors argue that their proposed few-shot imbalanced setting is different from the long-tailed recognition problem and generalized few-shot learning setting.  But this is only partially true. It is unclear whether previous approaches for long-tailed and generalized few-shot learning can be already applied to address the few-shot imbalanced problem. Moreover, I am not convinced that the proposed setting is more appealing than those two existing imbalanced settings because the 5-way classification problem seems to be an artificial setting that rarely happens in practice. Finally, I realize that the proposed setting is actually not new. [Lee et al., ICLR 2020] have explored a very similar setting. Thus, I would keep my original review and recommend rejection. === Update ===The arguments in the rebuttal clarify some confusions I had and illuminate the contributions of the paper further.   I am now completely on the fence about acceptance, but will tip toward positive. ===Update===After reading the authors' feedback and other reviews, I would keep my current rating. -------------------after rebuttal-----------------------I thank the authors for their rebuttal. Since the authors reply near the discussion phase end, I cannot ask follow-up questions. The answers partially address my concerns and thus I would raise my score to 6. For novelty , the author answers three points. For the first point, fuse two channels of information are not so convincing on the novelty aspects. Also, there needs an extra cost to collect process the images. For the second point, how would you formulate a regularize? After reading the author feedback, I would like to thank the authors and I agree with them that it is critical to test hypotheses on large-scale datasets. However, I still think that the contribution is marginally below the acceptance threshold. ===================================After a revision===================================Thank you for your efforts to revise the paper. The revised parts about related work look good to me. I agree on that citing all those EBM application papers is not necessary. But doing so can provide a comprehensive and complete development of  the DeepNet-EBM. Again, this is not required and it will not affect the rating.   I also acknowledge the existing contributions in the current paper and admit that such a direction is promising, but I still feel that the current paper doesn't fully explore this area with more solid experiments. Thus, the whole contribution is quite marginal. By taking into account all these concerns, I will change my rating from 4 to 5.       ##########################################################################################The response addressed some of my concerns, but I am concerned about L-ALFA taking such a large part of the paper and then shown to not help so much over just picking the final laye nor being much faster than the baseline. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated. Although I do like the objective of this paper and some of the approaches, I think it might need to be revised and resubmitted, incorporating the extensive discussion presented by all the reviewers.    # Post Rebuttal: I thank the authors for taking the time to address my concerns. The paper is well written and the proposed approached is promising. I therefor recommend acceptance. ---------- After feedback ---------- First of all, I greatly appreciate the authors patient response to me during the feedback period. The discussion was really fruitful. Unfortunately, I still have a concern about interpretability of the proposed method, which is a central topic in the paper.> First, we find it a bit strange when the reviewer says it is difficult to find importance/meaning of comparing motifs is unclear, we clearly show our method does find importance in NAS-Bench-101, all 3 tasks of NAS-Bench-201 and DARTS search space (Fig 1 and 7) -- if we can't find importance/distinguish different motifs, none of the results we've shown would've been possible. Even when a method works empirically, if a rationale behind the procedure is not clarified, a paper would not be scientifically convincing. Thus, I still do not think my claim is strange.> Second, the example the reviewer gives is not a case when averaged gradient fails. On the contrary, it is exactly an example of when averaged gradient works. A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability, as it doesnt consistently explain the network performance by itself (just based on such motifs, one cannot conclusively deduce the impact on performance of an arbitrary, unseen architecture in general) ...In the last response, the authors explained the interpretability issue through combination of motifs, but it did not resolve my concern. To simplify the discussion, consider a bit extreme case in which only one motif is employed in a network simultaneously, and assume WL parameter h = 0. Let g(c) = d \mu / d \phi^j |_\phi^j=c. Then, consider a hypothetical case as follows:motif a) g(1) = 10, g(2) = 10 ... g(10) = 10, g(11) = -10, .... g(20) = -10 : AG = 0motif b) g(1) = 1, g(2) = 1 ... g(10) = 1, g(11) = 1, .... g(20) = 1 : AG = 20In this example, b) has a larger AG, but a) can have larger importance in practice, and now, since only one kind of motif is employed simultaneously, the explanation of the authors cannot be applied. For the exploration purpose, I do not find any rationale to consider that b) is more important than a). I know that these are extreme examples and may depend on an application scenario, but my point is that these examples reveal difficulty of interpretation of AG. The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration (just referring other papers without discussing details in a sense of the above averaging). The explanation through marginalization also does not get rid of this question. Since the interpretability is a main theme of the paper, providing a better interpretability of AG would be desired. Post rebuttal comments=========================I thank the authors for their responses. I encourage the authors to continue the line of work on replacing the random sampling of NAGO. Given NAS-BOWL surrogate, one can do Thompson Sampling instead of random sampling. On the MKL side, the same weights for all the kernels might be the cause of worse performance. I would also encourage the authors to verify that. Nevertheless, those are minor comments and I still think this is an important work to bridge BO and NAS. I will keep my score. Post-rebuttal: The authors did not provide feedback and therefore I keep my score. EDIT after discussion:Thanks for the authors' comments and revisions to this paper. I'm glad to see that some of my concerns have been addressed so I decide to change my score.Just some minor comments about writing for the revised version. I think the proof sketch is clear and easy to follow the logic with some room for further improvement. Like the other reviers' comments, I think some part of the paper like notations should be with clear explanation.------------------------------------------------------------- Added after reading author response:I believe authors have addressed the issues I raised about clarity in certain parts of the paper in their response sufficiently. I agree that in this paper, the convergence rates' dependency on m has been improved significantly compared to previous work. Thus I increase my score. I encourage authors to investigate m's effects on the convergence rate more to see whether there is a structural limitation in federated learning settings, perhaps better left for future work. ### Post rebuttal feedbackIt's good to see that RMSD experiments are added and the results are better than Simm et al. Therefore, I am raising my score to 6. I also realized that the validity calculation is different from standard graph generation methods. The validity results now look reasonable to me. Update:I raised my score to reflect the added metric and experiments that improved the paper. =========== POST REBUTTALThe rebuttal is very helpful and "to the point" in clarifying the issues that I had raised as concerns the impact of the assumptions taken in the theoretical proofs. The fact that complexity hinges on average node degree and that any vertex cover sufficies for the proof confirms that the approach put forward in the paper might work out of the theoretical box. The Authors also suggest that the theoretical framework can be translated to a running model with a certain ease and that, in fact, its practical implementation and empircal assessment is on the way. Which brings me to the key point in my assessment. I am convinced there is value in this work and in the theoretical contribution in the paper.  I am not convinced that this paper can have a strong impact without an empirical validation. As I have underlined in my review, there are several related works in literature, with a similar theoretical flair which, nevertheless, provided at least a simple empirical validation. I believe that this paper shold do the same: it would be stronger, more complete and with a higher potential to influence the community. As it is, this is a borderline paper (leaning on the positive side).  **Update following author response and reviewer discussion:**I would like to thank the authors for providing a response, and in particular for providing further justification for their injectivity assumption. However, the main concern remains the lack of empirical validation, even on toy examples, showing that (or whether) the derived theory here goes beyond a hypothetical thought exercise and can be implemented in practice. It shouldn't be difficult to provide such examples, assuming the proposed approach does indeed work as indicated by the theory developed here, and without such examples, the paper seems rather incomplete. Therefore, unfortunately, my score remains unchanged at this point, although I would like to encourage the authors to keep pursuing this direction. ### Update 1 after discussion:Score raised. Update: I read the reply and thank the authors for the clarifications. Post-response update: I thank the authors for their response. I updated my score, but still think the paper needs improvement to be of interest to the ICLR community.--- ======= Review edit after authors' revisions ======The changes made by the authors in the title, abstract, introduction and conclusion to narrow the scope of the paper, better contextualize it, and make it more humble and truthful are very welcome. The extra experiments, figures, addition of error bars and new statistical tests are also a real plus. In doing so, the authors addressed all of my major concerns.For these reasons, changed my evaluation score from 5 to 8. ======= Review edit after authors' revisions ====== Most of my concerns have been resolved in the significantly-improved revised version of the paper. After rebuttal: I would like to thank the authors for paying attention to the comments and providing additional experiments and results. I updated my score. ================================== -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------The author response addresses some of my concerns. So I have updated my rating from 4 to 5. I recognise the novelty of the proposed framework but I feel that the main framework has not shown a clear performance advantage given several other components are added. Therefore, I am unable to give a clear recommendation for acceptance.  Post-rebuttal: Thanks for the authors response. After reading the responses and other comments and checking the updates to the paper, I retain the score and my recommendation at weak accept. ### After rebuttal phase ###The comments by the authors and the revised version of the paper successfully address my concerns. I thus recommend to accept the paper. ---- Final Rating ----The authors' response has resolved my concerns. I would keep my positive rating. Post-rebuttal updateOverall, I am still borderline on this paper. I appreciate the effort the authors have put in during the rebuttal phase, and would say the paper is now clearer. The inclusion of mean/median normalized scores for the Atari results in the main paper has improved the experimental section. However, the main drawbacks that remain are empirical; the smaller scale comparisons between methods need to be updated to give a like-for-like comparison in terms of numbers of parameters etc., as the authors acknowledge, and the paper still lacks baseline distributional agents in the large-scale experiments. This is important, as it means it is difficult to assess the impact that, for example, SR(lambda) may be having on the experimental results. The authors addressed my concern so I increased my score to 8. ----------------------- ----------------------------------------------------Post-rebuttalThanks for the new ZSL and GZSL results on ImageNet. The results are convincing to me. My other concerns are properly addressed too. I read the concerns from R4 and R5. To my knowledge, using common sense knowledge graph and the GCN with self-attention are novel in the zero-shot learning literature. I decide to increase my score to an accept. Post-Rebutal: I really thank the authors for their efforts  following my comments. I think they have really addressed my concerns. I therefore raise the score of the paper and recommend it for acceptance.  --- post-rebuttal feedback ---The authors have addressed most of my concerns. My rating for this paper therefore remains on the positive side.  The authors have provided standard errors which improves the confidence that the improvements in the experiments are significant.  **POST-DISCUSSION SUMMARY**I want to thank the authors for answering my questions and correcting misunderstandings and updating the paper. I still recommend rejection of this work given that the novelty is not yet fully clear. While the updated list of contributions is indeed a better match for this work, claiming that correct inductive biases improve generalization does not seem to be a new insight. As mentioned in my initial review, I still believe that the general line of work has a lot of potential and want to encourage the authors to fully address the general concerns raised in the reviews and resubmit the work. ****************************Thank you for the response and updates to the paper. Given the number of changes required, I encourage the authors to resubmit elsewhere with the updated paper, ideally with additional experimental comparisons as discussed. Note that there are many other offline RL works, such as BCQ or BEAR, that you could use (see, e.g., "D4RL: Datasets for Deep Data-Driven Reinforcement Learning" or "RL Unplugged: Benchmarks for Offline Reinforcement Learning" for relevant algorithms).  Additional comments: I have read the authors' response and the other reviews. While my initial concerns have been mostly addressed, there are still concerns from the other reviews and I have revised my score accordingly.In addition, I am not completely satisfied with the authors' response concerning higher-order interactions. In the end, even if you add other nodes, the operations are simply matrix operations and so you can model any higher order interactions with a single matrix. This should be clarified further.______________________________________________________________________________ %% post-rebuttal %%Though I appreciate the efforts of the authors to clarify their methodology and assumptions in their answer, these clarifications (which I still don't fully grasp or agree with) have not been reflected in the revised version. This work still needs a significant revision and, in my opinion, cannot be accepted in its current form.%%%%%%%%%%%% After reading the rebuttal:I keep my score. The authors should rethink how to claim the novelty in a concrete way. The presentation needs to be improved for readers not familiar with information geometry (they should be the majority). I do not get 6) in the rebuttal. ==============Update after rebuttal:Thank you for clarifying the numbers in Table 1 should match the main text. I enjoyed this paper, and I'm keeping my score unchanged. +++++++++++++++++I appreciate the clarified messages of the paper, and would like to see them emphasized more clearly in the next version of the paper. But due to the limited experimental scale on ImageNet (added in rebuttal, and in my understanding, it only verifies one of the multiple observations mentioned in the paper), I'm still leaning on rejection. I updated my score from 4 to 5. ==========================After rebuttal=================================Thanks very much for your efforts to address my concerns. I kept my score unchanged. I agree with most of the responses, except for the response to "L2 penalty, backtracking line search for determining the pruning ratio". This paper is not proposing a practical algorithm but a revisiting, so I don't think the computational cost is a bottleneck in preventing you from using more advanced methods to get more robust conclusions. After rebuttal: I thank the authors for comprehensive rebuttal, which addressed most of my concerns. I update the score accordingly.  _[Edit: I'm glad the author clarified some of the tensions between the different definitions. The ERM bias proof seems also okay, but it's still not clear how ERM performs with all the additional assumptions of the IRM result. I'm still not on the same page regarding $\kappa$, but I'm willing to take a leap of faith on the bits that I'm uneasy about. I would suggest this paper is shared with the community, since I believe it has enough good insights.]_ --- EDIT POST REBUTTAL ---I thank the authors for their detailed answer(s) and the efforts they put in editing the revision (in particular I agree with other reviewers that Prop. 17 adds clarity). Overall my stance on the paper has not changed and I still recommend acceptance. =============== UPDATE ==============After reading the concerns of the other reviewers and the author response, it seems that many of the concerns remain unaddressed especially the top concerns. Accordingly, I have decided to lower my score. **UPDATE AFTER DISCUSSION:** R4's exceptionally thorough review raised a number of important concerns which I initially did not recognize. Moreover, the most important of these concerns went almost entirely unaddressed by the authors. Since the paper has not been appropriately revised, I have decided to lower my score from a 7 to a 4. --------------------------------------------------------------------------------------------------------------------------------------------I have read the rebuttal. The experiments in the rebuttals shows the effectiveness of TOQ-Nets in other large, real-world dataset. These experiments should be added in the revision of the paper and I would like to change my score to 6. ==Post Rebuttal Response:I read the rebuttal, and unfortunately it didn't address most of my pressing concerns. I appreciate the authors' efforts to add new experiments on other datasets. However, in my opinion these new datasets are not very relevant for the action recognition community, i.e. they are small, and they are rarely used to compare the effectiveness of a particular model. In my initial review, I listed a few datasets that are most commonly used for action recognition comparisons. In my view, without comparisons on these more popular datasets it is very difficult to tell the real value of the proposed approach. If the authors could demonstrate close to state-of-the-art performance on those datasets I would be more convinced that the proposed approach is effective. Currently, most of the comparison are done w.r.t baselines that are implemented by the authors which is insufficient in my opinion. Therefore, I stand by my original recommendation of rejecting the paper. ==============Update after rebuttal:Thank you for clarifying that aux-outputs is itself a contribution and not simply a baseline for comparison. I also appreciated the additional experiment showing examples where In-N-Out can outperform aux-outputs. I'm raising my score from a 6 to a 7 accordingly. Update after rebuttal:The authors have addressed my concerns. Contrary to my initial understanding, the paper builds off of prior work in a methodical way, and the pseudolabeling stage of In-N-Out makes more sense now. I have raised my score from 6 to 7. **NOTE: updated score after seeing author replies and updated draft. I believe that this work is exemplary in terms of being careful about baseline construction, something that is unfortunately too often overlooked in our field. Additionally, it rigorously highlights another important point that I believe many often overlook, that "there are now enough optimizers"; community effort should be diverted from introducing small variations around Adam and instead invest focus on more meaningful improvements in scaling machine learning optimization. I do still believe that ImageNet and a larger transformer experiment would be extremely valuable to add to a later version, and hope the authors can eventually secure the computing power to add these.** Update:I have read over the changes made by the authors, and also the other reviewers responses. I am maintaining my score, because I dont think the current version of the paper is enough of a contribution to get accepted. As mentioned in my responses below, I would be happy to accept a future version of the paper that addresses my comments above.  Edit: After the rebuttal period, I maintain my original rating but am increasing the confidence of my evaluation from 4 to 5. I thank the authors for their hard work and engaging in discussion. I agree with Reviewer 3 that tuning with a fixed seed and the lack of search space refinement is a major weakness. The lack of a larger dataset further limit the applicability of the results. As such, I do not believe the paper in its current form should be accepted to ICLR.  ---Update after rebuttalI thank the authors for the response. I think that the revised version improved clarity. The overall impression I have of the paper is still similar to the initial one.The final proposed loss is reasonable, but just the combination of two existing ones with minor modifications. About this, even in the revised version the authors seem not to discuss and justify (at least in the main part) how the adversarial point $x'$ in computed at training time, which is different from TRADES according to Section C.1.About the experimental part, I thank the authors for adding the new experiments in Section D. However, the baseline in Table 4 seems a bit weak, at least for 10 steps ADV. For reference, the baseline WRN-28-10 in (Gowal et al., 2020) has robustness under AutoAttack + MultiTargeted close to 51% (I see that here a WRN-28-8 is used, but I wouldn't expect such difference).Moreover, although a minor concern, the authors didn't address the question about the step size used on MNIST.Then, I keep my initial score.Gowal et al., "Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples" =======================After reading the authors response ============================I thank the authors for answering all the questions that been raised by the fellow reviewers. Looking at the responses and changes made to the paper, I have increased the score from 5 to 6 after the authors clarified the issues I had with the paper. Overall this paper demonstrates the effectiveness of using a GNN for negotiation dialogues. I feel that this approach can be applied for any non-collaborative dialog settings and the claims of interpretability make this approach better.  Final review:The authors addressed most of my concerns. Just a nit that it could be helpful to add the total FLOPs into the table (together with # of params) just for completeness.  ***After Rebuttal***I have read the reviews by other reviewers and the responses of the authors to the questions posed by other reviewers. I enjoyed the additional experiments that the authors added to the paper during the rebuttal. The paper has shown interesting observations on the informative samples present in real-world datasets for different architectures. However, I still believe the method proposed by the paper is too inefficient for simple algorithm changes like changes in initialization. Hence, I am keeping the score the same after the rebuttal. ================ After the author response, I raise my score by 1 ================Thanks to the authors for the detailed response and extended discussion on theoretical results and experiments, and I have raised my score by 1.  ================After the author response, I raise my score by 1 (see my comment to them) --------------------------------------------------------------------------------------------------After rebuttals the authors significantly improved the presented work, including and discussing some relevant work which was previously missing.  =========after rebuttal===========I appreciate the authors effort to address my questions. I still think this paper is below my expectation especially if it is put into the context of RL.  I would expect to see how this method can solve or help solving a fundamental problem (e.g., reducing sample complexity) in RL or be applied to a novel application (e.g., nonstationary RL tasks). Otherwise I didnt see why it is necessary to compare this method with other statistical testing methods in the context of RL, if they can be easily made in other non-iid settings without mentioning RL.  After author responseThe authors have responded to most of my questions/concerns satisfactorily.Changing my score from 4 to 6 .[Decision after reading rebuttal]The authors appropriately addressed my concerns. The added experiments definitely reinforce the results of the paper and the added text help clarity. I recommend acceptance of this paper and would argue that its quality is reflect by an 8, up from my original score of 7 ---------------------------------------------------------------------------------------------------------------------------The authors have addressed my previous concerns. The experiments on a larger dataset are a plus. Therefore, I would recommend an acceptance of the paper. ###post rebuttal###I have read the updated version of the paper and still feel that this paper may have errors regarding the flexibility and purpose of LIME. The idea is nice, but the paper and evaluations would benefit from more polishing before publication. I maintain my original score.Misrepresentation of LIME:Section 3: LIME does not assume that the sampling neighborhood is the same as the true distribution. It may assume something weaker, such as that the function being explained is fairly smooth in the sampling neighborhood. Note that this can be a feature of LIME and not necessarily a bug: if for example x1 and x2 are fully correlated in the data distribution but the classifier only uses x1, it would be impossible to tell this if sampling only within the data distribution. By sampling outside the data distribution it becomes apparent that the classifier is using x1 only. Also, LIME assumes black box access to the function, so I don't fully understand your statement that "we generally do not have access to the true labels of instances generated through sampling". It seems like you may be defining the "correct" explanation with respect to the true data distribution, rather than to the classifier. LIME is meant to explain a black-box classifier. If the classifier is wrong, LIME should reveal what the classifier does (that is, the explanation should also be "wrong" with respect to the true data). The "framework capabilities" is also simply not true: users can define the data point to be explained, as well as their own similarity kernel and/or kernel width.Evaluation:It's not entirely obvious to me how we can be sure that CLIME is producing the "right" explanation in C.1, C.2 without knowing the function f. If changing the training set changes the classifier f, then it is correct that the explanation should change. As mentioned above, evaluating whether or not an explanation is "correct" should be done with respect to the classifier, not the underlying data distribution. In "Detecting Adversarial Attacks", it's not clear from the text whether or not you retrain the adversarial attack with your perturbation function. Further, I suspect that LIME may also be able to identify the sensitive feature for sufficiently small neighborhood sizes when sampling in binary space Z'. It seems like a straw man argument to compare an optimized version of your sampling procedure to the default version of lime.Minor: Equations 1) and 2), if they are describing the usage in Ribeiro et al., should include a weighting function.Figure 2 seems not to be explained in the text and would benefit from more description. _______________________________________________________________________________________________________________________________________________________UPDATE: After reading the rebuttal I think most of my concerns have been addressed and I am updating my score accordingly.   ## Post-rebuttal commentsI am impressed at the amount of time the authors have spent trying to clarify the different concerns.  But unfortunately my concerns are not fully addressed, and also a new concern arises: if this much space had to be spent clarifying different confusions of the reviewers, I think the paper could do with a complete overhaul.  I would suggest that the authors take into account the general confusions that arose in this review process and rewrite the paper such that those particular confusions are alleviated.  For instance, I struggled for clarity on what makes this contribution non-trivial, whether the contributions are primarily theoretical or primarily empirical (and how I should understand the balance between the two), and the fairness of the empirical comparison.  Rewriting the paper such that those three concerns---and the others pointed out by the other reviewers---are discussed clearly would be a significant improvement. ==================== post rebuttal ==============================I do not think my concerns are addressed by the discussion. However, I also think this is a well-written paper in general and I will not be upset if it is accepted. My main concerns,(1) The authors fail to show that the proposed method is non-trivial. I think this concern is raised by multiple reviewers. The authors keep clarifying the technical difficulty (especially the theory) of applying LARS etc, but my main concern is the necessity of these knobs added by authors. After a few rounds of discussion, we reach to a conclusion that adversarial training is different from standard training, but I do not think that could be considered insights from this paper. I would strongly suggest authors consider explaining why LARS is necessary by either theory or intuitive insights, and make it clear what exactly the difference is. (2) The authors claim contributions for large scale setting (ImageNet with large number of available GPUs), but the experiments are somewhat worse than previous results. Lacking computation resources is a good excuse, but since the authors claimed they can use larger batch size with smaller number of GPUs, I do not see a technical reason why they cannot use their method to re-run the large scale experiments to directly compare with previous results. ** UPDATE**Read the author answers, thanks. There is a significant amount of new material added in response to the reviews, including some interesting new findings (e.g. Tab. 3, 4). Unfortunately that did not help the clarity issue. I did not see a new example in Section 3.2.Basically, the stronger points of the paper (the results) are stronger, but the weaknesses (clarity, motivation) are not really addressed. The new version uploaded by the authors is an improvement but I still lean towards rejection. * Thank you very much for adding the comparison of GIZA and fastalign. However, simply swapping a model for alilgnment does not given us details about the tradeoff of alignment quality and the end-to-end results, since alignment models have different characteristic to capture the correspondence in two langauges, e.g., assuming linearlity and/or fertility. I'd rather like to see a much simpler approach of distroting alignment to avoid influence of the models employed for word alignment.* Thanks for the explanation. I'd like to see ablation studies regarding the loss.* Thank you for adding the tests. The authors answered my questions so I am increasing my score to 7. ----- ** Post Rebuttal **To best of my understanding, the authors have addressed all my questions and suggestions with the appropriate revision of their paper. Specifically, the necessary discussion of hyperparameter selection is added and presentation of the runtime&solution quality results (i.e., raised in point iv)) have been improved with the inclusion of important details, additional discussion of related work is added (i.e., raised in point i)) and questions are addressed (i.e., raised in point ii) and iii)). As such, I have updated my rating accordingly. #### Post-rebuttal comments- The paper should further elaborate on the smooth reward predictions and how online learning in the sparse reward setting can be possible with LatCo. It seems the method requires a specific initialization/implementation of the reward predictor, for instance, to overestimate rewards so that the method has to explore the areas where reward is overestimated and pull down the predicted reward. The paper should explain how this was implemented. This kind of exploration would be prone to the curse of dimensionality if the state representation of the environment is high-dimensional. The authors should discuss this limitation thoroughly. This might also explain why the tasks in the experiments are limited to 2-dimensional states.- I wonder about the discretization of the colors in Fig 8. Higher quantization of color should be provided so gradients of the reward landscape can be assessed.- The paper still does not detail the update rule for \lambda_actOverall, the author response has addressed some of my technical concerns, but the main challenges are only addressed partially. The paper is still borderline and might need another thorough round of improvement and resubmission to another venue.  ============= Post-Rebuttal Comments ===============Thanks to authors for their response and efforts in updating the manuscript. Some of my concerns were addressed. However, I still think that the novelty is fairly limited. For example, additional experiments in Fig. 5 produce quite intuitive results in the sense that any scheme that yields smaller t_c will have higher accuracy at a specific time slot. FedMes reduces t_c with the assumption that it is faster to communicate between edge servers. Systems-level experiments to thoroughly study the effect on t_c will greatly improve the contributions.  .## Post-rebuttal commentsThanks the authors for the clarifications! Most of my concerns are addressed. But the newly added asymmetric topology is still very benign and there is only 3 cells. I agree with other reviewers that this paper can be further improved ## After author responsesBased on the revision of the draft and the authors' responses to the review, I am raising my score to 7 from 6.--- -----### Edit: reply to author's response and updated paper (also see strikethroughs in the original review above)* Fold classification: let me withdraw my concern here, and will defer to other reviewers & AC judgement if this task makes sense with protein structure as input -- indeed it may not to be a trivial task. * Framing as (a) representation learning: improved in the updated paper, (b) convolution: still stands - the point cloud convs are not a very good comparison, since there is no graph structure there. (c) pooled coarsened graph stages: thanks for the pointer to end of Sec5.* Positioning wrt message passing: the paragraph is a big improvement, removing some claims about over-smoothing. However re: "the message passing function is learned": this is still very much within the default MPNN framework from Gilmer et al. Altogether, the whole method would still be much better framed as a graph-based network, rather than shoehorning this into a description of a single "convolutional operator".  This will allow a proper discussion of what is currently the end of Sec5, where the graph does not correspond to an atom-level graph anymore, rather they now correspond to amino acid or coarser level graphs - it is confusing that this coarser graph stages are so briefly glossed over.-- The citation to "can also be understood in a message passing framework (Kipf & Welling, 2017)" is off, should be "Gilmer et al., 2017" https://arxiv.org/abs/1704.01212 In conclusion, I am raising my score from 4->5, leaning towards 6. There is a lot of good work in this paper, and I would consider the paper a clear accept with the same method and same results, if it were thoroughly rewritten based on graph neural networks. Requiring full atomic structure as input to the method is the major limitation to the application and impact of the method. Update: The authors have included an additional experiment around fold generation in Sec 6.6. However, no baselines are included, so it is difficult to understand the result in context and understand how this method generalizes compared to existing methods. The authors have also included two additional baselines: Bepler, et al. and MSAs. More analysis is needed to compare this with SOTA in representation learning. The authors compare to "Elnaggar et al. (2020)" but it isn't clear which model was used. Elnaggar et al. (2020) have released a series of different models. The authors should clarify this in the camera-ready and ensure they used the best models released by Elnaggar et al. I have increased my score. =======================================After reviewing the response from the authors, I decide to change my evaluation score and confidence score. Post Rebuttal Comments: Authors have addressed most of my concerns and as a result I have increased the rating from 5 to 6. Thanks! ### Response to RebuttalThank authors for taking the time to clarifications and considering my comments.We appreciate authors' efforts to add additional experiments results in Table 1 and Table2. However, the performance improvements are marginal (more or less 0.7) and speed of Graph Transformer is slower than transformer.Even though additional explanations about positional encoding (Appendix 8.1) can resolve our concerns, layer iteration (Figure 6) are not still clear for us, e.g., what are orange blue, yellow nodes? how layer iterations are used in graph transformer? Authors should refine its main context to increase understanding instead of adding lengthy Appendix for us. This such paper presentation and organization are not clear to understand.Considering the above points, we still remain our decision. After authors' response to revisions, I reconsidered my evaluation and updated the score. **Update following discussion:**Following the revision by the authors and the discussion with them, I am updating my score from 3 (Clear rejection) to 4 (OK, but not good enough - rejection). This reflects in great part the revision the authors made to have the main paper (limited to 8 pages) be self contain and present their main results, while using the appendices for complementary and technical information.However, I still maintain the paper is not ready for publication in its current form. The extension of UMAP to implement the optimization via a neural network applied to input data rather than directly assigning coordinates is rather straightforward. The advantages it provides over UMAP in terms of natural inference on new data without the need for separate (more computationally intensive) out of sample extension method are a direct result of this neural network implementation, and they would be true not only for UMAP, but in fact for any method implemented in a "parametric" way via a neural network compared to nonparametric coordinate assignment. Similarly, allowing the addition of reconstruction or classification objectives in training is clearly a direct byproduct of this neural network implementation as well, and not unique to UMAP.Therefore, an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks, for example, to enable visualization or improve various tasks. The authors already look into this to some extent by comparing to parametric tSNE as one alternative approach, but there are many others, as I mention in the initial review, relying on constructions from topological data analysis and manifold learning - most, if not all, of which rely on some graph construction on the data and then ensuring the coordinates provided by a hidden layer in the network match the relations encoded in the graph, similar to the proposed UMAP loss term. How are reconstruction and classification affected by using such other regularizations compared to the UMAP one? Is inference speed the same for these other approaches? How does training speed compare between them? One can clearly expect some tradeoff between such properties and the geometric information encoded by different methods (UMAP and tSNE emphasize clusters, while other methods may emphasize other patterns), but this should be discussed and demonstrated clearly rather than just ignoring the vast amount of related work on parametric approaches to capturing intrinsic geometry in data.Now, beyond the described lack of relevant comparisons for autoencoding and semi-supervised classification, even simply as a parametric implementation of UMAP (which would be a rather narrow scope, which is not very enticing as a motivation on its own),  I am not sure this work is sufficient to establish the presented approach. First, for the inference or embedding speed - this is essentially and out of sample extension task. As such, even if one insists on only comparing to UMAP-based methods, there are multiple OOS methods that can be used, such as Nystrom, geometric harmonics, etc. Some analysis of the tradeoff between extension quality and speed seems warranted here, but as I said previously - I think a comparison should also be provided to other parametric embedding methods beyond just OOS of UMAP (and tSNE for that matter). Second, as the authors clarified in discussion - their approach relies on the suitability of the UMAP loss to be incorporated directly in the network optimization, essentially comparing activations to the UMAP graph. However, an alternative approach presented in related work is to provide a loss term between activations and a UMAP embedding. This second approach is more general, since other embeddings can also be considered there, but also probably has some disadvantages (for example, the a priori fixed dimensionality, as the authors suggest). The differences between these two approaches should be addressed better in the manuscript, and importantly, since previous work exists already on the embedding loss approach, the authors should present a comparison establishing the benefits of the graph-based loss one, in addition to discussion regarding them. To conclude, the idea behind this work seems reasonable, albeit rather straightforward since it's a reimplementation of the UMAP optimization. However, as it currently stands, I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones. I would like to encourage the authors to invest the time in adding such comparisons and clarifying not only how they are different from other methods, but also how they are better, and why choose UMAP to begin with as the basis for their proposed loss terms (compared to various other approaches - not just tSNE).--- Response to authors:The authors have largely responded well to my original concerns. However, after reading through the discussions with other reviewers, I agree with reviewer 2 that more work is required to make this publishable. In particular, this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function. Given this, I have downgraded my score accordingly. =======11/22======I am deciding to keep the same scores as before. Some of the initial concerns remain. I think the paper still lacks motivation wrt the GPT2 model generating missing edges. Thank you for getting the latest results, the paper is stronger than before and with some more work, I am confident it will be a good contribution to the research community.=====11/24======After having read through the explanation behind using GPT2 as edge features (and sufficient backing by 2 closely related work), I am increasing my score to 6. I think the discussion helped in somewhat convincing me that this approach would work for ConceptNet because of its limited schema. === Update After Rebuttal ===I commend the authors on a through rebuttal and active rewrites/experimentation. I still think the work is good, and can warrant acceptance. However, I still find the empirical results to be only moderate at best (though I appreciated the authors' rebuttal and significance testing). I am keeping my score the same. ## Response to the author feedback:We appreciate the authors for the extra effort to demonstrate the abiltiy of FLAP by adding more experimental results and discussions. Meta-reinforcement learning for adaptation to OOD tasks is an interesting field. Although the idea of FLAP is simple, the strong experimental results have demonstrated that by predicting weights rather than optimizing, FLAP is a fast and effective meta-RL algorithm for adaptation to OOD tasks compared to previous approaches that did not focus on this area. I believe FLAP will help draw more attention to this field and provide a direction for more theoretical analysis, thus I have increased the score. ### After the author feedbackI appreciate the authors for more control experiments and additional discussion in the manuscript. In the current manuscript, it is clearer that the "adapter network" predicting the "final linear layer" of the network is a unique component in this paper. I found that using an adapter network trained to predict linear layer parameters during training and used to infer parameters during testing is a setting that was not explored. However, I still see strong connections to previous works and I'm still not sure how to place this paper among such related works.I see the approach in this paper looks similar to RL^2 approach e.g. meta-training an LSTM based policy and meta-testing on unseen tasks. In this case, inferring the hidden state of LSTM looks similar to what the adapter network does in this paper.  Two differences of this work from LSTM based RL^2 are 1) neural network architecture, and 2) training objectives.One can construct a neural network architecture that is identical to an adapter network during testing time. One can train this network end-to-end during meta-training and use the exact same inference as this paper during meta-testing. This approach could be called RL^2 with a special neural network architecture that predicts the last layer of a neural network. If predicting the last layer of a neural network is an important component, it should be studied as an instance among variants of RL^2 with slightly different architecture. One practical concern about the architecture studied in this paper that this network may not scale to a case where the action space is large and the policy uses a large penultimate hidden state. In this case, predicting the parameter of a linear layer becomes very expensive. Because of the existence of this special case, I am not fully convinced about the claims that this method may work well in general.This paper trains the adapter network to predict parameters of a neural network instead of training the adapter network end-to-end during meta-training. Because of this difference, the method in this paper cannot be called RL^2 and it could be claimed that this paper explores a method that is not explored previously. If the training method is an important component, there should be at least one ablation for this detail.I still have a concern that it is not clear what's the main finding in this paper. Specifically, whether architecture is important or objective is important. I see that the paper already compared with RL^2, so adding an ablation study on using linear parameter prediction for RL^2 and discussing the relation to RL^2 would further improve the paper.  ============ UPDATE =============Thanks to the authors for their feedback. I appreciate the efforts on clarification and loose-end tying.One outstanding thing to clarify to help us understand whether Claim 1 can be fully supported: - AFAIK the Table 1 / Table 5 that underpin this claim are comparing numbers copied from previous papers with numbers generated from Domain Bed benchmark? However I suspect the splits are not the same. For example, some previous benchmarks have a fixed split by default, while I understood Domain Bed use multiple random splits? If so the numbers are not directly comparable, and it still may not be fair to make a strong claim that tuned ERM outperforms prior work. ## Post RebuttalI thank the authors for their response.  The addition of the recurrent SAC baseline helps the paper.  I disagree with R1 that it is a stronger baseline as FLARE outperforms it in all tasks and stack SAC similar or better three (arguably four) of five tasks. Instead it shows that recurrence isn't common in off-policy RL because it doesn't always perform better.  While recurrence is considerably more common in embodied 3D environments and this work may be less applicable there, I don't foresee DM control style RL benchmarks going away anytime soon and this believe this method will be useful. Update after rebuttal: The authors have added a recurrent SAC baseline to one set of experiments. The results indicate that the recurrent SAC is a much stronger baseline, and the variance of results is high enough that I am not convinced of the benefits of the proposed method.The authors argue that "many state-of-the-art RL algorithms" "are non-recurrent", and "frame stacking" is "largely untouched since its inception and is used in most state-of-the-art RL architectures", "recurrent architectures" have "additional overhead from training and implementation". I do not believe this is true. Recurrent architectures are commonly used in RL algorithms (for example RSSM in Dreamer) and are widely available in open-source implementations (for example https://github.com/openai/baselines/blob/master/baselines/common/models.py). There are some prior papers which use the frame stacking heuristic for a fair comparison with DQN, but this heuristic or the non-recurrent model architecture is not a part of the RL algorithm itself. Since this paper proposes a method for extracting temporal information, LSTM/GRU are very natural baselines in my opinion and should be added to all experiments.The authors argue that "it is very reasonable and common to have a new method improving a majority of the environments but not all", I agree with this, but the examples given by the authors such as Rainbow DQN, Dueling DQN, Dreamer etc perform experiments in many more environments and performance improvements are larger. I believe a much larger scale study is needed to compare Flare with recurrent baselines and make conclusive statements about performance gains. ---Update: upped score from 4 to 5; see comment thread.Update again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words. Update after rebuttal:My initial concerns were clarified by the authors and the paper substantially improved.  *Post rebuttal comments*: I appreciate the author's efforts for the rebuttal, however, the feedback did not adequately address my questions. I am not changing the score. Post rebuttal update:I am still positive about the paper and believe that it deserves to be published. However, I agree with Reviewer 2 that some non-trivial rewriting is necessary. The flaws are most likely not very hard to be repaired but it will require substantial additional work and the new version needs to be carefully checked. This is the main reason why I downgraded my score from 7 to 6. Post rebuttal update:I read the other reviewers' responses, and, although I am still positive about this paper, I agree with R2 and R4 that safely fixing the theoretical proofs would require a full revision. For this reason, I am lowering my score to 6.%%%%%%%%%%%%%%%%%%%%%%%%% -----------After Rebuttal--------------I decide on up my score by 1 since indeed the authors are only required to cite papers that are peer-reviewed and published before 2nd August. I decide not to up my score any further because my other two concerns remain. I appreciate that the authors performed additional experiments. While the generalization performance is better than MLRN, it does not outperform other baselines such as MXGNet.  Update after rebuttal: I thank the authors for their responses to my questions. They satisfactorily answer most of my concerns. Overall, I agree with the concern that the proposed approach is specific to RPM and it's unclear how well it (or parts of it) would generalize to other problems but I think the approach is quite interesting, novel and achieves state-of-the-art results. Hence, I think the contributions of the paper are significant for acceptance.  Update: I thank the authors for providing additional 3D metrics, and comparisons to unsupervised 2D segmentation techniques. I have updated my rating. Update: Thanks for the authors' response. The authors have clarified the equations, addressed the concerns of claims of no supervision, and provided more experiments and metrics to back the current set of claims. I do believe this paper to be interesting enough for an ICLR paper, and have updated my score accordingly.Also, do note that the motion segmentation works do not assume a single foreground object, they assume an arbitrary amount of them. However, you are correct in that they assume an entire video as input, as opposed to the proposed method which can segment individual frames. ******Update after author response: The authors have addressed my comments and my recommendation remains unchanged.****** After the discussions and the interactions with the authors, it came to my attention that one of my comments was wrong - I looked at a different paper that led me to have the conclusion that the authors cited a wrong performance number from a competitor in the literature; it turns out that I was wrong and I apologize to the authors.The paper at the initial submission version read very rough, with a lot of grammatical errors, typos, or misleading/incorrect statements. In the experiment section, it is stated that three datasets were used for evaluations but in Table 1 it appears there is the fourth dataset ILSVRC2012 used, which never mentioned in the text, nor is mentioned in the paper on how that dataset is used (a portion like WebVision or the full). The whole paper ended up with no conclusion or discussion. After the discussions with the authors, it became clearer that the mentioned three datasets in the text were for training and ILSVRC2012 was used for evaluation. But still it would be a lot clearer to have such a statement in the text.I had the comment that the title of the paper was misleading. The title reads: ROBUST CURRICULUM LEARNING: FROM CLEAN LAEL DETECTION TO NOISY LABEL SELF-CORRECTION. However, the proposed method, together with all the reported evaluations, focuses on learning the noise (in the labels); neither clean label detection nor noisy label correction is addressed. The authors disagreed with me but I was still not convinced by their argument. For noisy label self-correction, it may be relevant; but for clear label detection, I dont think so.I had a comment regarding the scale of the datasets used in the evaluations. But after having read the competitors work such as MentorMix, I now took it back and agreed with the authors. On the other hand, I agree with the comments raised by the other reviewers on lacking the ablation studies. I appreciated the authors efforts to report back the ablation studies, though only in part, and the results appeared to be convincing to me.So overall, after the discussions and the revision provided by the authors, I am convinced that the paper is above the acceptance threshold. The paper does have presentation issue, and lacks extensive ablation studies. ====================Thanks the authors' response. Based on the originality of Theorem 1, I increase my score by 1 but still a bit worry about the sufficient algorithmic contribution beyond TRPO.  After reading reviewer2's comment, I realize that there is literature proving much stronger results that I was unaware of. I still think these results should be used as a standard baseline for certified poisoning defense, but due to the lack of novelty, I have to downgrade my score.  ----updating in light of author's response-----I am upgrading to 7 as my concerns have been addressed.Other reviewers have questioned if ICLR is the right venue for a paper about a library.The paper does describe novel algorithms that lower the big-O cost of computation, similar to how Strassen's algorithm is a non-trivial modification of the naive matrix multiplication algorithm. After reading the rebuttal from the authors, as well as the updated draft, I agree that this style of presentation and framing is much more approachable to people not familiar with sig/logsig transforms. Thus I vote to accept this as a *library* paper. Update:The authors have revised the paper, which helps the presentation somewhat (though headings like "The Grouplike Structure" still come at the reader without much context).The authors added a more application-oriented benchmark, which makes the more convincing case for practical speedup of 210x.Certainly the new "Intuition" section is helpful in explaining the transform.The NT library is an interesting counterpoint on the library front. It feels a little bit apples and oranges to me because of the broad scope of that library (give me DNN, I give you NTK) as opposed to the narrower scope of this one (give me sequence, I give you [log][invert]signature). NT is munging your entire DNN into a GP kernel; signature is an implementation of O(a dozen) ops.I remain somewhat skeptical that the signature transform can enjoy wide applicability given the exponential scaling behavior, unless first and perhaps second order terms suffice for practical use-cases.--- -----**Update:**After the rebuttal, I update my score to 6/10 (see the justifications below). ___________________________After the rebuttal I raised my score. ==========================================================================================Update after rebuttal:Thanks for the detailed author response. I think the paper is interesting in providing an analysis on how optimizing the linear relaxation in CROWN (although the verification method itself seems to be similar as the one in Fastened CROWN in AAAI 2020) can lead to better loss smoothness and tightness, which seems to improve the performance of certified training. The author replies have addressed some of my concerns in my initial review. However, there are still some outstanding concerns:1. After more consideration, I think the More favorable landscape paragraph is still insufficient to address the second point in my above cons. The author response argues that some $(p/q)$ have looser or tighter bounds, but these are considered for relaxation *locally*, not the tightness on the final output, while the relaxation optimized in the paper is to tighten the final output. Thus it remains unclear why tighter final bounds with improvement from the unstable ReLU neurons makes the model favor unstable neurons less.2. AnonReviewer2 has reminded me that the Fastened CROWN work had a similar method about optimizing the lower bound of the linear relaxation in verification, which seems to be very similar to the method proposed in this paper, in terms of the verification part in certified training. Although this paper focuses on certified training and has some different analysis, the major modification on the method is still on the verification part, and thus I agree that a discussion on the comparison with Fastened CROWN should not be missed. The authors did not add it in the discussion period.3. It is promising that the proposed method outperformed the modified CROWN-IBP ($\beta=1$) and IBP, and there seems to be a significant margin. But the proposed method fails to make a significant improvement compared to the original CROWN-IBP (the 1->0 one), e.g., the improvement on CIFAR-10 eps=8/255 or 16/255 is negligible. Overall, I am keeping my recommendation as rejection for the current version of the manuscript.  Update:While some of my points have been addressed and the quality of the paper has been improved, my main concern, that the experiments do not support the central claim.  The authors argue that because their method has a smoother loss landscape then similar methods, it performs better.  In the updated paper, the evidence that the method performs better than similar methods has been made clearer, but the improvement is is still marginal.  The more pressing matter however is that the conclusion that the cause of the improvement is from a change in the loss landscape smoothness is based only on a qualitative comparison of only five methods (as the benefit is not consistent between methods in any category).  This is enough to at best demonstrate a weak correlation, but not enough to demonstrate causation.  ----------------------------------------After rebuttal:Thanks for the answers. I have raised my score even though I still think the paper could have done a better job at comparing against other methods.- Regarding group-averaging methods for the equivariant case: It is a trivial extension, specially the approach of giving GNNs unique IDs and then averaging their representation, which (Loukas, 2020) shows it is universal (but (Loukas, 2020) did not consider averaging). Regarding training, it is always performed stochastically via data augmentation and Monte Carlo estimated in test. For the generalization error of the stochastic optimization, it is still unknown (some new results show promise (Chen et al. 2020) and (Lyle et al. 2020)) in the same way that the generalization performance of other universal methods is still unknown.- Chen, S., Dobriban, E., & Lee, J. H. (2020). Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond.- Lyle, Clare, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. "On the Benefits of Invariance in Neural Networks." arXiv preprint arXiv:2005.00178 (2020).- Loukas, Andreas. "How hard is to distinguish graphs with graph neural networks?." Advances in Neural Information Processing Systems 33 (2020). === Post Rebuttal ===I appreciate the careful response provided by the authors, which reminds me of the significance of extending existing theoretical results from invariant to equivariant models. Therefore I have raised my score by 1 point. post-discussion:I read the author's response and other reviews. I still think this is a very strong submission and would like to see it accepted.I encourage the authors to add a section discussing their generalization of SW theorem + Theorems 31,32 as the authors suggested. another small point: I couldn't easily find the definition of "stable by concatenation". ### Post rebuttalI thank the authors for their response. My previous rating still applies. _Update after rebuttal_: The authors have been very forthcoming with further analyses in the rebuttals. I believe the findings deserve to be published and that bringing attention to these issues is a good thing, and accordingly I have bumped my rating up a bit. Update after Rebuttal:I'd like to thanks the authors for addressing the comments really thoroughly. The additional experiments using anti aliased networks are indeed very interesting. I agree with AnonReviewer4 that some of the material in this submission is probably known to people already. However, I do strongly believe that many people are unaware of the impact and importance of these effects and therefore I believe publication of this content is important. Update after rebuttal:The author responses and manuscript revisions have addressed most of my concerns. Now I lean towards acceptance.  ------ UPDATEI have now read the other reviews, author response and updated paper, and have decided to maintain my rating. Rebuttal Edit (Increased score form 4 to 6)Thanks for the discussions. The primary reason for my score increase is discovering that the power of the framework is finding a representation that is quick at exploiting new opponents. I have been sufficiently convinced that this approach is not simply finding a NE (a sticking point in my review). I think that there are a collection of ideas here that are publishable and are of interest to the community.The reason for not giving a higher score is that I think the points the paper made could be clearer: specifically I think the phrase "base policy" could be better replaced by "base representation" / "base model". I was stuck on the idea that the base policy had to be a strong one (eg a NE), and close to exploiter policies in policy space rather than parameter space. Re-reading after the paper update, I am worried that a significant portion of readers may fall into the same trap despite the authors' additional edits. Tightening the story would make this paper more appealing. I also broadly agree with the other reviewers suggestions / concerns.For future work (also mentioned by another reviewer), I think there is no reason the "base policy" could not also be a strong policy too. I believe with minor adjustments to your framework this could be achieved, and one would have a model that both has low exploitability and is fast to adapt to new opponents - a potentially powerful combination. **After rebuttal:**The responses address most of my main concerns, and I have increased the rating from 5 to 6. As discussed during the rebuttal, in the future, having additional experiments that compare between OSG and other appropriate population generation baselines would be helpful. _[Edit: I read the response of the authors. The lower bound for vanilla KD seems good. But the author still fail to adequately explain the gain of Thm 4 (now Thm 3). The requisite upper bound on the critical radius uniformly over all potential functions generated by the teacher means that the result of the theorem still has an implicit dependence on the teacher's complexity. For this reason, I will leave my recommendation the same.]_ Update: I appreciate the authors response. The answer to my semiparametric efficiency question, however, is too terse and unclear. Theorems 1 and 3 provide only error bounds; I dont see how they, together with the semiparametric efficiency of OLS, directly imply semiparametric efficiency for the student model. Semiparametric efficiency involves the optimal asymptotic variance. The authors might have confused the concept with rate optimality. Nevertheless, the paper is a nice contribution, and I will keep my rating. --------------------------------------------------------------------------#### Update after rebuttalThe authors have explained the reasons for the comparing experiments in Appendix E and updated the result of time complexity, which I think worth including in this paper. After Rebuttal:The authors did a great job in addressing most of the issues I have, and made many changes that helped with the clarity of the paper. There are still some remaining issues like Figure 2 assigning a wrong geometric meaning to the clusters formed by taking means of hyperplanes, and the uncited references, which are simply added to the references section (which should be fixed). But I think the added survey results are a great addition and a persuasive proof about the increased interpretabililty of these models. Therefore I'll increase my score to 6. The authors did a fantastic job of answering questions, revising their manuscript in accord with reviewer feedback (Sec 3.4 title), and even adding new experimental results based on reviewer suggestions (mid-training hierarchy) and reflecting best practices in interpretability research. I was really impressed by their nimbleness and responsiveness. I will raise my score to a 7: I think this is a very solid paper and excellent research effort around a nascent idea. In particular, I think its impact is limited by- its close coupling to naturally hierarchical problems, e.g., multi-class classification with a taxonomy- its close coupling to image data and tasks- its heuristic nature: fully train neural net, infer hierarchy via clustering, retrain neural net, then map a priori labels onto inferred hierarchyThe "10" version of this paper (maybe future work?) would propose a way to infer the hierarchy on the fly and show how to apply it onto other kinds of data and problems with different structures.----- -----------------after rebuttal------------I would like to keep my origin score due to the pros listed above. Update:The authors have addressed many of my comments below. As such, I am increasing my score. ---------After rebuttal: Thank you for the response. I have decided to leave my score unchanged. ---------------------Edit after author's responsesMy first concern is addressed by the authors' response. My other two concerns were not really addressed, but I think these concerns should not preclude this manuscript from getting accepted. So I'm updating my score. [Update after author's responses]I appreciate the responses provided by the authors. I think they answer my questions. In consequence, I update my review to favour accepting the paper.--- After rebuttal:I appreciate authors' detailed responses and an updated version of the paper. They mostly clear my concerns and doubt. I increase my rating to accept. -------------------------------------- **Update after response of the authors**The authors partly corrected the points I mentioned, and I thank them for extensively addressing my comments. However I still believe that the paper oversells its results and analysis, although in a much less strong manner (e.g. in the last sentence of the abstract) and that many concerns remain. For instance, my concern on the tensor initialization has been partially addressed by the authors which showed that it performed as well as a random initialization close to the solution. The very small dimension (d = 5) for which these simulations were performed is however still a concern for verifying this analysis, which would need to be more rigorous and much more extensive to justify replacing the tensor initialization with the random one.As a consequence of all the changes made by the authors, I have raised my grade from 3 to 4 (and I would grade the current state of the paper between 4 and 5). **After rebuttal:**Thanks to the author for providing additional experimental data. But without the results of imagenet, it is difficult to judge the effectiveness of this method on complicated data. So I decided to keep the original score. After reading author responses:Thank you to the authors for your detailed responses. With regard to the highlighted implication that "the harder feature can be obscured completely by a spurious one; i.e., there are settings in which the model just won't adopt the harder feature at all" --  to clarify, while my phrasing may not have made this apparent, I was assuming this implication in my interpretation of the results. So my impression of the finding is not changed substantially by the author response. However, I do want to give appropriate acknowledgment of the value of explicitly testing/confirming intuitive explanations of model behaviors, and it is clear that other reviewers find value in the contribution, so I am bumping my score up a bit.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Comments after author rebuttal:Looking at the author's comments (as well as the other reviewer's feedback), I think that the authors have made a good job with the responses and I'm now more convinced about the usefulness of this work. I'm increasing my original recommendation to 6 "Marginally above threshold". --------AFTER DISCUSSION WITH AUTHORS---------Thanks for the clarification. Some of my concerns have been addressed and I have raised my score. But I keep the concern that the proposed defense framework may be easily broken in practice given that the attacker can have unlimited power.  ------------------------------------------------------------------------------------------------------------------------------------Post rebuttal:The authors have addressed my concerns in the rebuttal. However, I also agree with the other critical points raised by other reviewers (particularly Reviewer 4) that are of major concern. Hence, I retain my initial score and marginally accept the paper. Thank you for your answers.------ ---- Post-rebuttal comments----Thanks for the response. After reading other reviews' comments and the rebuttal, I think this paper is in a good shape now. Thus, I am willing to increase my score to 8 and recommend acceptance. Post-rebuttal.Thanks for a detailed response that clarified some questions and concerns that I previously had. I think the updated paper is stronger and I am inclined to raise the score to 8. ********Post-rebuttal. The rebuttal didn't raised any concerns and made the paper even stronger, thus I am keeping my score. ### Updates after the rebuttalI found the approach clean and believe it has some merit among competing approaches. However, as detailed in discussions, I am skeptical of the scalability of the approach. I understand that an approach with scale limitations is acceptable and believe authors can benefit from an honest discussion about tradeoffs--which is currently missing. Further, I would suggest adding non-trivial experiments in the same vein as the ones in $\mathcal{M}-$flow paper. I think with these revisions, the paper can make for a welcome addition to the flows-for-density-on-manifold-literature. --------------Post-rebuttalI would like to thank the authors for the response. However, my concern on the theoretical part remains. For densities within the manifold, I suggest reviewers consider experiments similar to those in [1][2]. I also agree with R3, R4 on the reinvention of the connection between KL in the data space and KL in the latent space. In addition, I agree with R1, R3, R4 on the scale of their experiments and scalability of the approach. As such, I will lower my score from 6 to 5.[1] Mathieu, Emile, and Maximilian Nickel. 2020. Riemannian Continuous Normalizing Flows. arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10605.[2] Lou, Aaron, Derek Lim, Isay Katsman, Leo Huang, Qingxuan Jiang, Ser-Nam Lim, and Christopher De Sa. 2020. Neural Manifold Ordinary Differential Equations. arXiv [stat.ML]. arXiv. http://arxiv.org/abs/2006.10254. .Update:I thank the authors for their significant updates to the paper.  Given the extended effort made by the authors, I am willing to raise my score to 5. My conclusion however remains the same, this work is not a significant advancement that we would expect to see at a conference such as ICLR. [I think the paper provides a useful case study and I appreciate the caveats added to the conclusion, but based on additional discussion with the other reviewers, I think that the paper's more general claims remain unsupported and insufficiently moderated. I am reducing my rating accordingly.] ## Post RebuttalI thank their authors for their response.  I have decided to maintain my rating, overall I still think this is a good paper that presents an interesting set of results.  The result that image prediction accuracy correlated better with asymptotic performance than reward prediction accuracy continues to intrigue me.  My fellow reviewers have some concerns that while I don't agree, I think they could be avoided with some changes in the presentation to the paper:* The models used.  I understand the decision to move the model description to the appendix (not everything is going to fit in 8 pages), but the main paper would benefit from paragraph or two describing why which models were chosen and then referring to the proper places in the appendix for the full details.* Reward-only model.  There was considerable concern about how much smaller that models is than the others.  Additional results showing that one with a comparable number of parameters does as poorly if not worse would be beneficial.  The 0% line in Fig 6 and Fig 8 is very similar to this hypothetical (if it exactly) so I believe this will pan out as expected.* Presentation of results.  In my read, the most interesting result, anti-correlation between reward prediction accuracy and asymptotic performance, comes at the end.  Without that result, there is a way to read the paper as "yes image prediction helps, it increases the amount of supervision given to the model".  That result and the difference between online and offline shows that there is something unexpected going on here so perhaps leading with those and/or highlighting them more would help. update after rebuttal: the answers were convincing, and the paper improved.  ## Post Rebuttal commentThe authors have clarified the contributions of their work and improved the manuscript accordingly. Given this and the other positive points about the paper I am willing to increase my score to accept. Due to the many relative improvements of FLAG,rather than absolute rankings, on the OGB leaderboards, I think that the overall contribution is above the acceptance threshold.1. Node property predictiona. ogbn-products: +2: DeeperGCN+FLAG, +5: GAT+FLAG, +6: GraphSAGE+FLAGb. ogbn-proteins: +1: DeeperGCN+FLAGc. ogbn-arxiv: +1: GAT+FLAG, +8: GraphSAGE+FLAG, +5: DeeperGCN+FLAG, +4: GCN+FLAG, +1: MLP+FLAGd. ogbn-mag: +1 R-GCN+FLAG2. Graph property predictiona. ogbg-molhiv: +3: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +2: GCN+FLAG, +3: GIN+FLAGb. ogbg-molpcba: +2: DeeperGCN+VN+FLAG, +2: GIN+virtual node+FLAG, +1: GCN+VN+FLAG, +1: GIN+FLAG, +2: GCN+FLAGc. ogbg-ppa: +1: DeeperGCN+FLAG, +1: GIN+virtual node+FLAG, +3: GCN+virtual node+FLAG, +1: GIN+FLAGd. ogbg-code: +2: GCN+virtual node+FLAG, +4: GIN+virtual node+FLAG, +4: GIN+FLAG, +2: GCN+FLAG .------ Post-Rebuttal Update ------I have read the author's response, thank you for adding more experiments with other graph data augmentation techniques. However, the explanation and justification for the "biased perturbations" still seems a bit weak and under explored in the experiments section, so I've decided to keep my initial score ######## Post-Rebuttal Updates:I appreciate the authors' response, but my main concerns were not addressed. Particularly, I still believe the novelty of this method is extremely limited, as it directly applies an existing method on graph node embeddings. The main novelties are biased perturbation and unbounded attack, which are both very simple modifications, and were not adequately studied in experiments. Although the authors show extensive comparison on various datasets, their comparison does not particularly show the contribution of biased perturbation and unbounded attack. They only show the effect of biased perturbation using a single number in Table 2, which is not convincing.  After rebuttal: The long discussion with the authors to clarify the questions in my review and further related questions actually shows that the paper is not really clear and would still benefit from a substantially improved presentation. So, I slightly downgraded my overall rating of the paper. #### After rebuttalQ1: Thanks for running the additional experiments. Unfortunately, the results are not strong enough to convince me to use the searched architecture instead of BiFPN. It will also be interesting to see how BiFPN works under similar FLOPs with the searched model (e.g., the structure of EfficientD4 or D3).Q2: Please do run the ResNeXt101-DCN experiments for the revision or next submission. These are critical to make people use the proposed method.Q3: True. However training from scratch requires 6x longer training time according to [1], and is considered as a drawback.Q4: Thank you for the clarification, this makes the contribution clearer. However my concerns on changing the backbone remains (Q3).Q5: Thank you for considering. I agree the ranks in the leaderboard is a main factor for design choice. This also highlight the importance of Q2.  ----- Post Rebuttal Update -----After the author's rebuttal and the discussion with other reviewers, I have decided to lower my score to 6.The reason is that during the review and discussion process it was pointed out that the main contribution of the paper is more incremental than novel, as both multi-hop GATs and diffusion for GNNs have been explored before in similar ways. Additionally, even though the authors provide some theoretical grounding for their work, some of it is a bit disconnected from the rest of the paper, like the relation with PageRank introduced in section 3.2, which is not referenced or discussed in any other section. ----------------------------------------------Update after Rebuttal: I have read the other reviews and authors' responses. While I do think the novelty of the contribution is sufficient, given that  the paper referenced in another review has not been peer-reviewed yet, the new ablation results in Table 1 show that the paper's contribution is not outstanding. I adjusted my score. --- UPDATED SCORE ---First of all, I would like to thank the authors for carefully addressing my comments.In light of the authors' response, and after a thorough and careful discussion with the other reviewers, I have decided to update my score to 5 (five).In summary, I appreciate the authors' effort to signal the differences between their work and Elvin et al. While I agree with these, I still think this is only an incremental contribution. For further reference, after carefully discussing the paper with other reviewers, these two published papers were also pointed out where multi-hop attention is addressed. Namely,https://openreview.net/forum?id=rkKvBAiizhttps://ieeexplore.ieee.org/document/8683050I apologize for not finding these papers in my first round of reviews, but it does not really alter my evaluation of the paper.I think that the most novel contribution is on the spectral analysis. But this is only stated, with no real insights developed, and not emphasized enough. More insight on this would definitely bring a novelty. More specifically, novelties that would have made the paper more interesting: (i) a different way of computing the attention coefficients, that would be more parameter efficient (as opposed to eq. (1)) in the paper, (ii) actual useful insights into what the frequency response of the learned filters look like in the attention matrix as opposed to the given support matrix of the graph, (iii) a comparison between the spectral basis of the learned attention matrix as compared to the support matrix. --- Post-rebuttalI thank the authors for responding to all the questions and getting back with additional experiment results.Major concern: While I understand the motivation and how having attention scores over nodes multiple hops away can be powerful, I'm still not convinced with the approximate realization. It is not clear how diffusing attention defined over 1-hop neighbors is powerful over attention methods defined over immediate neighbors that contain k-hop information aggregated from diffusion.Also, the performance drop and overfitting issue with GAT or diffusion-GCN can be combated similarly by sharing weights across GNN layers and also using a higher-order diffusion matrix at each GNN layer. Added after the author response:The authors adequately addressed my questions and concerns. I appreciate that the authors provided the error bars for their experiments and tried out the idea of modifying epsilon_k, but I wish the authors left more time for the discussion. I think the paper is a valuable contribution to the domain of irregular time series. I am increasing my score to 7. =================Post-discussion:After reading the authors' response and the changes to the paper, I must unfortunately stick with my current score. I applaud the authors for taking my feedback on board, and certainly many changes have been made to improve the paper, but my main concern of novelty regrettably still remains. The paper offers a very simple improvement over Jung et al (effectively scaling the importance measure), which I believe is quite incremental in this setting. While I believe simple advances can often have broad impact (eg. dropout, batchnorm, etc), in this case it is not clear that the proposed change offers any benefits outside of the very specific area of importance-based continual learning. ---update: Thanks for the clarifications. I've read the response and other reviews and have updated my rating. #####################    After Rebuttal   ####################I thank the authors for responding to the comments and have read them carefully.The authors have addressed all my concerns in the rebuttal and I vote for acceptance of this submission. ###################################Update:The authors have addressed the concerns I had in my initial review. I have raised the score from 6 to 7. === Post rebuttal ===The inclusion of runtime comparisons with search based methods is a welcome addition, as it showcases that the method provides an interesting avenue to utilize GPUs for this kind of search task. Effectively trading up-front training time for deployment computational time. However, the question regarding the ability to handle larger state spaces is only touched on in the appendix and it is not clear from that description what the state space was. If the proposed approach works for planning in 6D environments or control of 7DoF manipulators then this is very interesting and should not be hidden away, so to speak, in the appendix but be highlighted in the main paper and showcased.In light of the considerable amount of work that has been put into the revision I changed my score from 3 to 4. While I could see improvements and clarifications I could not see the main concern I had, handling of high dimensional problems, being addressed. _Post-Rebuttal_:Unfortunately, the authors neither did update their paper nor addresses my comments. Therefore, I'm keeping my recommendation of rejection. =============== after rebuttal: I thank authors for the responses. After reviewing the authors' response and other reviewers' comments, I keep my original rating.  --- Update after discussionI agree with other reviewers' idea that the contribution of this paper is somehow limited. Since my previous suggestion (7, accept) is based on that the author can successfully address how to control the error of the critic theoretically. However, with only the experimental elaboration study, as I said in the response to the author, the contribution of this paper is limited. Thus, I would switch back to 5 (marginally reject) based on that after reading other reviewers' discussion. -------------------UPDATE AFTER RESPONSEThanks for the response and the additional experiments. The comparison between ACE and these other techniques is nice to see, although I'll note that both SWAF and voting shouldn't make totally independent predictions in tasks like NER, but should at least respect constraints in the label space (not sure if there were applied or not).In the end, my opinion of this paper largely comes down to the practicality of this technique and its likelihood to be adopted more generally. This results in a large, complex model, and while I am now convinced that the authors have a better ensembling/combination technique than some others, I think it still falls short of a real "neural architecture search" contribution or a really exciting result. Updates after discussion/revision period:I think the revisions have improved the paper, but I'm not willing to increase my score or to fight for the paper. Overall, I think the paper represents a minor contribution, with its rigorous experimentation and some of its ideas, and that others may benefit from reading it, but I don't know that it is at the level of a typical ICLR publication. -------- >> commented after the rebuttals:Thanks for the answers. However, I think the paper still suffers from too many basic issues (still standing from the review). Main one is that it is not clear what the current definition is and what it achieves that previous definitions miss. There are informal texts, but they are not coherent formal and verifiable. Post-rebuttalThanks the authors for the clarifications! I appreciate it. However, some of my concerns are not addressed. - The main concern I have is about the new insight on local update methods. Basically, the author obtain the insights based on a newly proposed algorithm (L2GD, let's call this algorithm B) and a new problem formulation (let's call this formulation B). However, they want to apply the insight from algorithm B and formulation B to algorithm A (original local update methods) and formulation A (original FL formulation). It is obvious that one cannot draw this conclusion because both the algorithm and the formulation are different.- Second, as I stated in the original review, I don't think one can obtain the insights from the analyses in this paper. The author didn't directly answer my question and just said "they didn't expect people to interpret it in this way". But it is still unclear how to correctly understand their insights.Based on the above two points, I strongly feel that their main insights about the effects of local updates should be further and carefully examined. The current version could be misleading. Besides, I also have the following minor concerns:- The authors claim that [Yu et al. ICML 2019] didn't consider the heterogeneous setting. This is not true. Although [Yu et al. ICML2019] assumes that the gradient dissimilarity is uniformly bounded (which is widely used in literature), their setting is still non-iid setting. It's unfair to say that they only study the IID data setting. So the second motivation of this paper does not make sense to me. The authors oversell their contribution. More precisely, their contribution is not the first proof under data heterogeneous setting but should be the new proof without data similarity assumption.- "non-local cousins" is unclear and hasn't been properly defined in the paper. For local SGD with mini-batch size ,  local steps and  clients, there are two non-local cousins: (a) SGD with mini-batch size ; and (b) SGD with mini-batch size . It seems that the authors misused these two algorithms. In the response, they agree that [Yu et al. ICML 2019] proves "with data dissimilarity assumption, local SGD can improve the communication complexity of classical SGD". Here, classical SGD refers to algorithm (a). In the updated paper, they cite two papers from Woodworth et al. to support their claim. However, the non-local methods in Woodworth et al. is algorithm (b). The authors should formally define which non-local algorithm they want to compare with.- In the paper, the authors claim that they prove for the first time local methods can improve the communication complexity of the non-local cousins. However, this statement is overselling. The more precise version is that they prove that the variance-reduced version of local methods can improve the communication complexity of the vanilla non-local version algorithms.- It seems that the authors want to claim a lot of contributions in this single paper and they didn't organize these contributions well. Hence, it causes difficulties for readers to understand their true novelty. I recommend the authors to rewrite the paper and carefully consider the paper structure. For example, if I understand correctly, the main contribution of this paper should be the insights on local updates. However, the authors didn't show any experiments on this insight in the main paper (they put them in the appendix). Instead, they just validate the effect of variance reduction in the main paper, which is just a minor point. Also, in the introduction, there is a long paragraph to introduce L2GD as one of the main contributions. However, as discussed in the responses, L2GD is not a new algorithm. The authors don't need to give it so much emphasize or should not claim it as one contribution.- Also, in [1] EASGD does use multiple local steps. The authors should compare L2GD with EASGD, as they both are designed to minimize the new formulation.##  ---### After rebuttal:After reading the rebuttal, I feel my main concern is still not addressed by the response. The authors agree that they may provide a different kind of guarantee for the certificates as in (Cohen et al., Salman et al.). An empirical comparison between the output of the proposed model and the mean from sampling is not sufficient. We hope the authors can improve on this point and provide formal robustness guarantees like those in (Cohen et al., Salman et al.). After response:Thanks for addressing the concern about normalization. It appears that other reviewers have a concern about such normalization as well. I suggest the authors remove the results with normalization entirely from the main paper and only have it in the appendix for anyone that is interested in such normalization. On the other hand, without normalization, the results have changed for the under-parameterized regime (which makes more sense to me) and the proof looks quite different in the over-parameterized regime as well. I did not have time to check the proof and I believe it is better to resubmit the paper as new because of the major changes. Finally, I still have concerns about the fact that only variance is discussed. I suggest the authors state their results in a setting where both bias and variance exists and the features added to the model are related to the response. Otherwise, it is a weird message that it is good to add pure noise as features. It feels like although we can design multiple descents in the overparameterized regime when noise is large, it is very likely that the 0 estimate achieves the best prediction risk. So there is no point to go into overparameterization and multiple descents at all.    In summary, I have raised the score to 5. I believe it can be 6 or 7 if all issues are addressed, but I am afraid that the paper looks basically new after these changes and thus I am not sure whether it should be still considered for this conference.   Update: The issue with normalization is fixed in the new version and I am increasing my score. Based on the problems found by the other reviews and having read the rebuttals I have modified my score. ### Post rebuttal Thank you to the authors for their detailed response and their effort in improving the presentation of the paper. I was impressed with how much the paper improved in this second version. In particular, I very much appreciate that the introduction starts with a simple one sentence explanation of the problem of neural network verification. This can be further improved if it included (almost) no maths, which can be deferred to the Background section. The figures in the updated paper are very good and a huge improvement of presentation. Finally, the paper now includes two clearly stated theorems, which also make the presentation and contribution much clearer. I have increased the score I gave to the paper. Regardless of what the outcome for ICLR will be, I would like to encourage the authors to re-iterate on the presentation to really crystallize the problem, definitions and the suggested approach --- the paper is already so much better than the first version, and even just a little more work can make it even better.  Post-Rebuttal Comment:I thank the authors for responding to my comments. The updated version fixes most of the criticisms raised in the review, and I have raised the score accordingly. My new score is "5", partly because I believe that after performing such a large-scale and comprehensive overhaul of the paper (which was certainly necessary), the paper should go through a full new reviewing process.  ---------------------------Update after author response:I thank the authors of the paper for significantly improving the prose of the paper, and I agree that the changes make the paper more self-contained and approachable. I have kept my ratings as my score was for primarily for the strong experiment results (and the score was also conditional on the paper being more polished). I am happy to support this paper for acceptance, but I am a little concerned about the degree of changes in the final version versus the initial submission, given the number of concerns the other reviewers had. Update:  After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4.  There is potential here, but it will benefit from strong revisions. ---------------------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:I would like to thank the authors for providing relevant details and a thorough rebuttal to all the issues raised by the fellow reviewers. Original rating is maintained. ## POST-REBUTTALThank you for rebuttal which has helped me, to some extent, to understand your model. I still believe the formulation needs to be clarified before the contributions can be assessed objectively.For example, on page 2:> Given a realization of timestamps $\mathbf{t}_1, \mathbf{t}_2, \ldots ,\mathbf{t}_N$ with $\mathbf{t}_i \in [0, T]^D$ from an inhomogeneous (multi-dimensional) Poisson process with the intensity $\lambda$. Each $\mathbf{t}_i$ is the time of occurrence for the $i$-th event across $D$ processes and $T$ is the observation duration.This is confusing: only a single one of the dimensions will likely represent "time". Other dimensions will be space, etc. Furthermore, why would every dimension need to be within $[0, T]$? Different dimensions might use different units and be in different ranges.I am upgrading my score slightly, because I sense that your contributions might be interesting once properly explained, but in the present state I believe the paper is not ready for publication. # Changes after rebuttalThanks to the authors for their answers to the questions and their revisions to improve the manuscript. It is useful to have further descriptions of reversible computing for an audience that may be unfamiliar with the topic. I would encourage the authors to make further revisions to more concisely show the scientific value of the work while leaving some of the details to tutorials or other documents.  Other venues more focused on scientific computing, programming languages, or Julia may also be more suitable. If the language also attracts more users and applications built on top of it, then the case for publication will also be stronger (consider that the PyTorch paper was presented at NeurIPS 2019 even though the first release was in 2016).--- *AFTER REBUTTAL*I would like to thank the authors for their hard work! I increase my score to 6. --------- Thank you for the detailed reply and revisions. I have increased my review score because several of my concerns have been addressed. I am not entirely convinced that the baselines use current best practices, and several claims in the paper regarding the inductive biases and calibration. Nevertheless I think the algorithms proposed in the paper is practically useful.  =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified, particularly concerning missing experimental details. However, unfortunately, I still think more needs to be clarified in the actual paper write up, notably on the points of non-adversarial as well as the method description. #############################################################After author response: I thank the authors for their answers. However, as noted by other reviewers, experimental results and comparisons with related work are lacking, and I cannot increase my score without major changes to the paper.  ### My final recommendation The authors did not fully address my points. I remain my initial score and recommend for rejection.  ------------ UpdateThank you for the clear reply. Unfortunately, I remain concerned about the significance of experiments. I mentioned above that 4 or 5 runs is typically not enough, and because the standard errors are overlapping, the differences could be due to chance. The addition of a result with 10 runs is a good step. But, as part of the reply, the authors state: "Figure 3(a) shows the learning curves of all methods on the SlimHumanoid-ET environment over 10 random seeds. First, one can not that SUNRISE with random weights (red curve) is worse than SUNRISE with the proposed weighted Bellman backups (blue curve). Additionally, even without UCB exploration, SUNRISE with the proposed weighted Bellman backups (purple curve) outperforms all baselines. This implies that the proposed weighted Bellman backups can handle the error propagation effectively even though there is a large noise in reward function." However, if you look at this figure, the error bars all still overlap. 10 random seeds is still not enough. I am also not confident that the issue will be remedied, as the authors additionally state in the rebuttal: "we believe that SUNRISE is evaluated in a broad collection of domains in the RL literature and the performance gap is also noticeable." An insignificant gap across many domains does not tell us anything. Actually, if you take the runs and tried to do significance tests by pooling all the runs across environments, then maybe the result might actually be significant. But, of course, there will be higher variance due to differences in the environments, so it is not obvious this would be true. Nonetheless, this could be a natural next step. ------ After rebuttal: I am lowering my score for the paper. I am not convinced by the responses to several of my questions, in particular to what I felt was an exaggerated insistence on the paper being about  Variational Transformers, the rather artificial connection to ELBO (noted by several reviewers), and the lack of self-contained description in the paper of the actual technique used, MC-Dropout, which might be explained in simple and sufficient terms on its own. Also, I am disappointed that the authors did not update in any way their submission to reflect the reviewers comments (contrarily to misleading expressions in the rebuttals). It is therefore impossible to know whether such unconvincing claims as that made in the conclusion With the new tools above & would be maintained in the final version. ---Post rebuttal: The authors have partially addressed my concerns with regards to experiments on actual domains. I think this is a central part of the paper and these experiments could be improved, however I am willing to augment my score to 5. I am still ambivalent about the paper but I wouldn't fight against it being accepted. =====Update after rebuttal=====I have read the authors' rebuttal. I am increasing my score to 5 as some of my concerns are addressed. 1. I do not agree with the responses for the "Motivation --- Causality in Running Example". First, the GNNExplainer tries to maximize to mutual information, which means important edges for the prediction should be kept. Then why does the GNNExplaienr suffer from the confounding association? Second, "individually feeding the top edges into the target GNN" is not convincing. "An  edge is important for the prediction does not mean the prediction score is high when feeding individually.2. I still believe the proposed method is very straightforward and the novelty is limited.  **Post-Rebuttal**My main concern was regarding the clarity of the paper. I believe it is improved in the revised version, so I increase my rating. Post-rebuttal:Thanks for the feedback. By optimizing Eq. (3), the learned graph is not guaranteed causal, so I would like to suggest the authors not emphasizing "causal". **AFTER REBUTTAL**I appreciate the author's efforts to improve the clarity of the submission and to add a comparison with Prob-NMN. However I remain unconvinced that this style of model can possibly scale to more realistic data. I've upgraded my score from 4 to 5, but I still generally lead toward rejection. ################################ After response:Thank you for the clarifications. The response and changes address several of my concerns. While I will keep the score currently, I would consider it slightly higher given the additional information (i.e. ~6.5). --- Post rebuttal ---I'm keeping my initial score. My concern remains (and is apparently reflected by R1): the datasets and results do not allow to draw clear conclusions. The paper overall furthers our understanding on robustness only in a very limited way. The new dataset adds another specialized dataset to the mix. I disagree that the community should first exhaustively and randomly add datasets to the literature without coming up with a definition of robustness or at least try to categorize. The authors in their rebuttal criticize the community that they are looking at robustness and distribution shifts in a too simplistic way, but at the same time the presented work doesn't make an effort to change this either.To close the remaining question:- "Could you elaborate? We know that humans and primates can generalize to new renditions that they have not seen before (Itakura, 1994; Tanaka, 2006), while some other species cannot. Consequently more than training data matters."With "extreme", I mean distributions shifts that keep semantics, but change appearance strongly. If we go as far looking at biological systems, then yes, it is not only about training data. There is an additional mechanism at play that we don't know and currently can't replicate in ML. Given our current understanding, it is presumptuous to suggest that NN architectures and training approaches as they are covered in this work will be able to do these kinds of generalizations at the level of humans or primates.- "The empirical reality does not currently allow for a simple, single-cause characterization of robustness"I completely agree to this statement. However, this doesn't mean that characterization of robustness cannot be done by taking into account multiple factors systematically.I acknowledge that collecting a new dataset is a non-trivial effort and can be useful.  I acknowledge that the paper proposes an additional augmentation technique that seems to improve results in certain cases. All factors together taken together lead to my final score. ---**Final review**After reading the paper, other reviews, and author responses carefully again, I decided to remain on the rejection side because- I think the proposed dataset does not really guarantee the robustness against "real-world distribution shifts" because  - This paper did not rigorously define what the "real-world distribution shifts" are. In the final response, the authors mentioned that *"It is clear that temporal, hardware, geographic, and rendition shifts occur in the real world."*, but to me, it is not clear whether they are really common and representative in the real-world deployment scenario and really threaten deep models.  - Because the real-world distribution shift is not well-defined here, I feel the "robustness" is also ill-defined too. According to the author response, robustness is defined as the accuracy gap between "in-distributed" samples and "out-of-distributed" samples (not critical, but OOD is usually defined as the same data distribution, but unseen class. I think this terminology need to be polished). However, here OOD (distribution shift) is ill-defined, and the robustness test is heavily dependent on the test dataset.  - Thus, if we just test the "robustness against real-world distribution shift" on the proposed dataset only, it can lead to wrong conclusions, e.g., assume we have a model can be specifically better in a specific shift, e.g., rendition shift, but not generalized to other shifts, then ImageNet-R benchmark cannot measure how this model is vulnerable to the other shifts. It will confuse researchers in this field. Hence, I think this paper needs more justification for the new dataset (e.g., why the chosen shifts? why 200 classes for ImageNet-R? why different three datasets?), and need more human studies (e.g., humans can correctly classify the shifted images and non-shifted images) such as [3, 5, 7, 8].- This paper is not clearly presented. After reading the paper, I am still confusing about how to understand the experimental results. To me, the benchmark results cannot answer these questions well. I think R2 has a similar opinion on me in this criterion.- It is not mentioned in my previous reviews, so I lower the weights for this part to the final decision, but there are already some datasets benchmarking the dataset distribution shifts, e.g., PACS [9], NICO [10]. It may not be true that this kind of distribution shift is only measurable by the proposed datasets. But, as my first words, I noticed that I did not mention these datasets in my previous reviews, and these datasets will not affect my review a lot.  - https://domaingeneralization.github.io/  - http://nico.thumedialab.com/[8] Shankar, Vaishaal, et al. "Evaluating machine accuracy on imagenet." International Conference on Machine Learning. PMLR, 2020.[9] Li, Da, et al. "Deeper, broader and artier domain generalization." Proceedings of the IEEE international conference on computer vision. 2017.[10] He, Yue, Zheyan Shen, and Peng Cui. "Towards Non-IID Image Classification: A Dataset and Baselines." Pattern Recognition (2020): 107383.Of course, building a new dataset is a non-trivial effort, and measuring real-world robustness is not an easy task (maybe it even can be an impossible task). However, I think this paper can not clearly present how the proposed benchmark can solve the real-world distribution shifts and how can we move forward in the next directions.To sum up, I think this paper is okay, but not enough to be accepted to ICLR main conference paper. However, I will respect all decisions made by AC. Update after the rebuttal:Thanks for the responses. Given that the other reviewers also raise serious issues I will stick with my initial rating. The paper seems not to be ready for publication at ICLR [Addendum after review: most of the concerns have been addressed by the authors with a large set of additional experiments.][Addendum after review: As the authors point out, this is only partially true, as each feature can be selected for multiple groups.][Addendum after review: the authors have added a random forest to the experiments. However, this is the only method whose parameters are not fine-tuned (which might be a smaller problem for random forests). No time comparison is given, and some accuracy deviations appear well within the standard deviations. Still, this is a very good addition to the paper.][Addendum after review: This point has been partially addressed.] Update (Nov 24, 2020):After reading through the author responses and the updated version of the paper, I feel like a sufficient number of my concerns have been addressed to increase my score to 6.  Specifically, the motivation has been made clearer, the related work section is no longer redundant with the intro, and the authors gave an adequate explanation about the necessity of their attention-based alignment method.   =====POST-REBUTTAL COMMENTS======== The authors provided additional experiments on CIFAR-FS and Omniglot. The results show that their methods outperform the baseline method adversarial querying (AQ). It is still not clear whether the methods work in the reinforcement learning setting. As the original MAML paper shows that MAML works for RL problems, it is important to address this question. Otherwise, it could potentially limits the applicability of the proposed methods in the paper.I still have concerns over their novelty and the significance of their contributions. Overall, I applaud their effort to address my comments. I am more positive on the paper than before. My rating is a solid 6. The paper in the current stage does not warrant a higher rating for ICLR in my opinion. ===============================After response ====================== The authors have addressed all my concerns.  I would like to keep my initial score. ===============================After response ======================Thanks for stressing my concern, the additional experiment makes the empirical result more convincing. Overall I think this is a good paper, I prefer to keep my score and rate it as accept. ------------------------Update after author response:I have increased my score in light of the substantial improvement to the manuscript and experiments. EDIT: Raised score from 4 to 6 after the reading authors' response. The response clarifies some of the novelty issues, and it clearly shows the advantage compared to previous methods like TAS. However, I still have concerns about the novelty; the insight why the proposed method is better than TAS is still not very clear to me. I hope the author can further improve the draft for the final version. EDIT: Raised score from 4 to 6 after the authors clarified some points and added additional experiments. **Response after author rebuttal period**I highly appreciate the detailed explanations and discussions the authors have provided during this period. It indeed clarifies my concerns and helps me better understand the setting and proposed solution. However, my major concern about the generalizability of the proposed solution still remines, as there are too many design choices depending on domain knowledge. I would keep my original recommendation; and if the paper could be accepted, I would like to encourage the authors to discuss the limitations of the proposed solution.   After the rebuttal.The authors partially addressed my concerns. I have read other reviewers' comments. I decide to remain the current score. Edit: I have increased my score in light of the response and manuscript edits. The manuscript is improved, but I think the method still needs more development. There are a number of interesting pieces but the final picture of an improved protein model is not fully resolved. ---I have considered the rebuttal provided. Particularly the aspect that data-augmentation would result in inconsistent supervision is an interesting point and experimental analysis of the same would be useful. However, I am not convinced that the paper provides a broad enough solution. In view of this I raise my score from 3 to 4, but maintain my view that the paper is presently not good enough for acceptance. **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the improvements made and clarifications given by the authors.I would keep my score recommendation at 5 (marginally below acceptance threshold) at this point, however, mainly due to (i) a limited novelty (as partly acknowledged by the authors in the response) and (ii) a limited experimental evaluation (missing high-resolution datasets such as ImageNet or real-world AD datasets such as MVTec-AD) of the current manuscript.I encourage the authors to build on their current results and extend their work with additional datasets and an analysis of geometric shifts also at training time.##### ==== Post discussion Update ====I am updating my score to accept after the discussion. Update after rebuttal: I appreciate the authors' response and clarifications. I maintain my original score. ---**Post-rebuttal update**My main concerns in the initial review were three-folds:- Potential flaws in the analyses based on VAE and adversarial attacks- Unclear connection between the MI analysis and the proposed method- Small performance gap, and even sometimes worse performance, compared to the baseline methods (Mixup, CutMix)After having discussions with the authors, I will keep my initial score because:- I am still confused about the MI-based analysis conclusion. The authors mentioned *"We make no claim that increasing or decreasing the mutual information measure will have a strong impact on performance. Instead, we contend that MixUp works by forcing the model to ignore sample specific features (thus learning compressed representations  the reason for discussing the information bottleneck theory) and that CutMix works by mimicking the real data whilst preventing example memorization."* in the rebuttal, but these two conclusions are not trivial to me (by the MI analysis).- Even if we ignore the first part, my second concern still remains. The authors mentioned *"That is the problem FMix tries to solve by removing the horizontal and vertical edge artefacts from cutmix. Our belief is that cutmix biases models towards these edges as they are a guaranteed feature of the data and learning about them would reduce the loss since these edges can tell you how much of each source image is present in the input (a key part of the objective)."*. But if this paper assumes that the rectangle masking strategy of CutMix makes bias, then I think other CutMix variants such as AttentiveCutMix or PuzzleMix should be considered as the comparison methods. Hence, I disagree with this statement *"A comparison to masks generated using additional models (and, thus, significant additional computation) does not seem fair to us."*- For my last concern, the small performance gap, the authors claimed that this method *"was also used by the second place team in the BengaliAI Kaggle competition"*. It is good evidence that FMix can sometimes offer benefit to real-world applications, but I think more evidence that FMix can really solve problems of previous MSDA in a certain scenario, e.g., the edge bias as pointed by the authors. ----Update: thanks for the additional human evaluation results! These help and the results on excluding unimportant entities seem strong to this reviewer. Perhaps it might be more helpful for the annotators themselves to try to interact with the summarizer in some way, but that's a more minor point.Anyways, I bumped up my score from 5->7. Update:Thank you for the answers to my questions and additional experiments.  ---Rebuttal--I acknowledge having checked the authors' response and the revised version of the manuscript, that has been improved since the first revision. Authors did not answer to my request for more details about the hyper parameter selection procedure that has been adopted.  ---------------------------After rebuttalThank the authors' response to my comments. They also provided new results (i.e., graph generation tasks) to address the Q.4 in my comments. I was about to lower my score when the first reply came as graph generation tasks are just future works in the original paper, but I changed my mind with these results. Although other reviewers may still question the novelty and contributions, I think I would stay with my score according to the comparison and good results reported in the paper. Looking forward to seeing the code someday. **************************************************************************************** POST DISCUSSION UPDATE ****************************************************************************************Thank you to the authors for the discussion. Given that the relationship between AF and F has now been addressed, I will increase my score. However, since the connection mainly hinges on the empirical correlation between the two, I still don't think that the paper quite establishes the theoretical connection between the three continual learning methods that it aims for. Finally, I'm not quite convinced by the potential impact of the insights, which seem fairly specific to choosing the batch size of SI, so overall I think the paper still is below the acceptance threshold. ******************************************************************************************** END OF UPDATE********************************************************************************************* **Update to review**I am increasing my score from 5 to 6. I believe this paper is a good paper. However, an extremely extensive discussion with other reviewers has left some questions / concerns. Although I disagree with some of these, I agree with others:- Some claims are overstated in the paper. The authors already changed these claims somewhat in the updated paper. Some reviewers are arguing for further changes. I think some claims can still be reduced, particularly, the link between AF and EF (and hence the link to EWC).- One of the biggest reason I find this paper is interesting is not mentioned (enough) by the authors. In my opinion, this is a big reason why the work is significant, and if I were writing the paper, I would put it as one of the biggest motivations:    -  There have been works recently looking at the Generalised Gauss-Newton approximation (= EF for classification), and trying to view optimisation algorithms as approximating the Hessian matrix. For example, see Khan et al., 2018 ("vAdam"), Kessler et al., 2020 ("BAdam"), Zhang et al., 2018 ("Noisy Adam"), Osawa et al., 2019 ("VOGN"). Such works provide evidence that different approximations of the EF can work well. Although I am not aware of previous works using the absolute value of gradients (as in AF), this paper provides evidence that such an approximation might be worth considering. Should we try and approximate the Fisher matrix in more ways in CL? - Finally, it is my personal opinion (although others disagree) that the current paper is significant enough / provides enough insight already to be a good paper. However, performing a further state-of-the-art experiment or similar would undoubtedly improve the quality significantly.I very much look forward to an updated version of this paper. ---## Post rebuttalIn response to my doubts about a unified framework, the authors claimed that their *theory* could *predict* an algorithm or hyperparameter's performance.Since there was no description of the theory, I assumed that the theory is:> If an algorithm is different from AF, its performance is expected to be poor.And the authors refuted my interpretation:> We claim that SI and MAS work because they are similar to AF.The authors claim that the theory somehow applies to SI and MAS but not others. However, I could not find any description of why the applicability is restricted and to what extent it is applicable. I think these are vital parts of a proper theory. Without them, a theory is useless since we cannot decide whether it applies to a new CL algorithm until we actually run some tests.Also, I want to emphasize that association is not causation. The authors should have claimed, "SI and MAS work, **and** they are similar to AF," instead of "SI and MAS work **because** they are similar to AF."Even after the discussion with other reviewers, my concerns are not resolved.Therefore, I retain my initial rating. POST-REBUTTAL COMMENTS========I would like to thank the authors for their response. The authors have clarified the method in their response. I appreciate all the experimental details in the appendix. I tend to agree this is a promising idea and worth explored.However, this paper clearly cannot be accepted in its current form. The paper is poorly organized and poorly written. The method in the paper needs to be clarified. Their theory goes nowhere and proves nothing. In terms of the experimental results, the authors choose unclear baselines (which they claim state-of-the-art) and report improvement in terms of percentage increase (percentage over percentage). None of this is convincing to me.I slightly increased my score. *************Updates: Thanks for the authors' response. However, not of all my concerns are well-addressed. Based on the current methodology and the novelty of this paper, I remain my overall rating of this paper unchanged. ------### Comments after the discussionThank you for your detailed response. I think most of my concerns were addressed so I updated the score to 6. Updates after author responses and revisions:I was positive about this paper previously and am glad to see that the authors have done a great deal to try to respond to our concerns and strengthen the paper. I am more positive about the paper now and have increased my score to an 8. I think this paper is going to be useful for the community and I know I will reference it later and direct others to it who are interested in learning more about position embeddings in transformers (whether or not it actually gets published). -------------- ------------------------------------------------------Edit after author response:The authors have sufficiently addressed my concerns with the additions that have been added in Appendix D. As such, I will increase my score from a 5 to a 7. ##################################################################After author response: I thank the authors for the additional experiments. It partially addresses my concerns. However, the qualitative results do not always show improvements in image quality, and most of the low-quality samples are still generated after cleansing. Its also hard to clearly notice differences before and after cleansing in Tables 9 and 11 (2D-Normal). It would be better to use a multi-modal Gaussian with some modes being more likely. Overall, I am still concerned about practical applicability of the proposed approach. I keep my score of 7.   ====== POST-RESPONSE UPDATE ======I appreciate the author's response and the additional illustrations provided. At the same time, my concerns remain:- I still disagree with the claim that PCA directions are "semantic and meaningful." Yes, some of them might correspond to image changes that are intuitive, e.g., Figure 4, but it is still impossible to draw any such conclusions without manually inspecting individual directions. In other words, what do I learn about my model by reading Table 1?- I understand the process of counting the number of images. However, I still think that it is a fundamentally flawed metric. Based on this definition of "distinct", if I change the value of a single pixel by 1/255 I get a distinct image. This is clearly not an intuitive behavior. As a model designer, what do I understand about my model by look at these astronomical numbers.Overall, while I still find the broad direction interesting, I believe that the paper has fundamental issues and is hence unsuitable for publication. == Update after author response ==Thanks for your response. I believe it is widely agreed upon that black box evaluation is not meaningful in security settings, and that we should use white box attacks. Therefore, I don't find the justification for omitting PGD and CW convincing. I also still do not feel that counting images in pixel space is a meaningful metric. Therefore, I have kept my original score. From the discussions, the authors make it clear that "compressing" the computation graph is possible without the need for expensive operations (as is the case in typical "lifted" inference literature). The approach does seem to be simple to implement , maybe a bit more detailed analysis and clarity as suggested by others as well could strengthen the paper further. Post-author response: I appreciate that the extra experiment I asked for was conducted and the equations were fixed. While I think Review5 raised some interesting discussion points which should be included in the final version, I still think the paper has merit, even if larger-scale pre-training would have improved the results. Thus I raised my score to 7. UPDATE:As the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version. With this I think this is a good paper that should be accepted. -------------------------------------------------------------------------------------------------------------------------------------------------------------------------Update: After going through all the discussion, I'd be happy to raise my score to 7. The authors did a great job in clarifying all the concerns raised by the reviewers. This make the paper a much stronger publication.  Rebuttal phase:I am updating my score based on the numerous clarifications of experiments and improvements of framing the paper.----- ### Post Rebuttal UpdateI thank the authors for their response. However, I still have some remaining concerns about novelty, since to me this paper reads as another application of MAML to domain X, to speed up performance in the particular domain. Furthermore, I think it would be good to evaluate generalization to larger extent, for example on different conformations of dynamics as opposed to a fixed set of parameters on the synthetic dataset. -------------- Post-Rebuttal Comments -----------------Thanks to the authors for their response, and for updating the manuscript. Some of my queries were clarified. However, updated Theorem 1 seems to raise more questions. In particular, Assumption 4 looks very restrictive to me. If adversaries manage to produce large values of \Gamma, they can inflict a large error as per (7). The paragraph after Theorem 1 does not mention how entropy and loss based filtering methods can achieve small \Omega_{max}, but only says that "if the entropy-based filtering method successfully filters out the model poisoned devices, and the loss-weights \beta_i(k) of the adversaries are significantly small for data poisoning and backdoor attacks... ... then we have a small error term". It is not clear what guarantees the filtering schemes yield. Due to these reasons, I still think the paper is not yet ready for publication. UpdateThanks for the rebuttal, it addressed my main concerns. The new results on CIFAR-10 and CIFAR-100 with fine and course labels match the results on the checkerboard task. This increases the generality of the main claims. The new experiments also confirm that the level of noise in SGD has a key role in finding minimal representations. Furthermore, they show that when training with SGD with enough amount of noise, the usable information with fine labels increases initially and then decreases. This improves our understanding of the phenomenon introduced by [2], which was later debated by [1]. For these mentioned reasons, I updated the rating from "5: Marginally below acceptance threshold" to "7: Good paper, accept".[2] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR,  abs/1703.00810, 2017. **UPDATE:** Most of my questions/comments are addressed in the revised version of the paper and the author responses. I maintain my support for acceptance. ------Update:In general, I am happy with the authors' responses. They did show the advantage of the introduced self-supervised loss. Although the self-supervised loss is intuitive, incorporating it into MONet is non-trivial and it does outperform MONet. Despite that such self-supervised works are hard to work on real scenes, this paper does have some merits. I am willing to increase my rating. Post rebuttal comments:Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score. =======Post-rebuttal=======I thank the authors for their comments. The current draft isn't a bad start, but in agreement with the other reviewers, it needs more work before it's ready for publication. Please carefully consider our recommendations for your next draft. **[UPDATE, 30 Nov]: Rating raised after reading the authors rebuttal.** Update ==Thank you for your detailed response. The newly added clarifications and sanity checks have greatly improved the quality of the paper, and I am therefore increasing my score from 4 to 6. I believe the model capacity comparison (Table 6) is especially important for demonstrating the value of the new architecture, and would recommend mentioning that result in the main paper. === Post rebuttalThe authors' response and the revisions to the manuscript have greatly improved the quality and clarity of the paper. Most of my major concerns regarding the implementation and evaluation details have been sufficiently addressed; hence, I decide to increase the score from 5 (Marginally below acceptance threshold) to 6 (Marginally above acceptance threshold). #### Post-rebuttal comments:The authors' comments addressed my concerns on method and experimental details mostly well. I keep with my rating "6: Marginally above acceptance threshold". ---------------------------------------------------------------------------------------------Thank you for the detailed response. I have updated my score from 4 to 5. ******** After Rebuttal ************I carefully read the authors' response and unfortunately they do not address my concerns. Based on my research background in adversarial robustness and uncertainty estimates, I would keep my original rating unchanged as this work has very limited contribution to these two areas. Thanks to authors for the clarification.  I have updated my score. However, a easy search with the title of the paper "Recurrent Attention Walk for Semi-supervised Classification" in Google can find the code in GitHub.  -----Based on the issues pointed out by the other reviewers. I have decided to reduce my score for the paper.  __________________________________________________________**After rebuttal**I thank the authors for their responses, but I will keep my original rating of the paper. I think this paper has potential and a new submission will have a better result.Suggestion for improvements:1. As the other reviewers pointed out, the proposed method is computationally expensive. Although this is a downside, it is not really a major one, as long as the authors have a proper analysis of the complexity and of the actual inference / training time. The authors pointed out that their contribution consists of proving that the adaptive receptive fields are useful, and I agree that an efficient method is not necessary for this, but the analysis should be made.2. I think that additional ablation studies are needed. For example, the experiment in Figure 4 a) supports Proposition 1 but only on Cora that is homophilic and where the mean of the nodes is a good aggregation. The experiments with added noise are a good start. Maybe also compare GRARF with GAT with a simple strategy of selecting the edges (top k according to $e_{i,j}$, or all with $\alpha_{i,j} > 0.5$ ).3. Improve the motivation from Section 2.  **Post-Rebuttal:- I appreciate that the authors have conduct revisions on the current version. However, I think the current paper is probably still not strong enough for ICLR.  ----------------------------------------------------------------UpdateThank you for the comments. But there is a misunderstanding. MANGA as well as PEARL are online methods. They just need the observed data during the episode. It can encode the observation data in online manner. I think it is not evident whether IMPORT performs better than MANGA or PERAL. I agree that RNN is general, but on the other hand, I am afraid that the internal state of RNN does not converge and usually fluctuate from time to time. It may be difficult to get a persistent policy during the episode in the same environment. I would like to encourage the authors to perform more convincing experiment and make the claim of the paper consistent with the experimental findings. ======= UPDATE ========I appreciate the authors' efforts during the rebuttal period, but I still retain my initial assessment of the work.Overall, I find the proposed approach promising and easy to understand, but believe that the experiments can be improved to better substantiate the claims in this work. In particular, I believe that the benchmarks can still be more carefully chosen to better evaluate IMPORT's ability to perform sophisticated exploration. I find the 3D Maze experiment to be quite nice, as it clearly highlights a shortcoming of TS exploration, but I would find the experiments more compelling if there were additional benchmarks testing such exploration. The authors commented that exploration in meta-RL is about inferring the task to solve, which I agree with, but I think such exploration can still be made more "sophisticated" by requiring careful sequences of actions to lead to distant states, which reveal this task information.In addition, several issues were raised regarding the TS baseline during the discussion period. The results in the bandits setting appear to be lower than those reported in other works, and it still seems like PEARL can be adapted to be a drop-in replacement for the TS baseline. I agree with the authors' assessment that the basic form of PEARL explores the setting with multiple episodes, but PEARL could just resample from the posterior every few timesteps, which is already what happens in the TS baseline.I do think this work should be published in the future with a more careful selection of experiments. In the rebuttal the authors have argued that their method studies a different problem from that of unsupervised video segmentation, and thus a comparison to those methods is unnecessary. Moreover, they mentioned that no established metrics exist for evaluating the degree of disentanglement of a representation, thus no new experiments can or should be added to the paper. I can see that there is a difference between image-based scene decomposition and unsupervised video segmentation. Once you move to video-based methods, however, the difference start to elude me. At least in the classical works, such as Brox and Malik, ECCV'10, the problem is defined in exactly the same way - decomposing a video into object/background regions in a fully unsupervised way. The only difference I can see is that in those works decomposition was the end goal, whereas this paper attempts to use it as a surrogate task for representation learning. This would be a valuable contribution if the authors could show that the resulting representations are superior to those learned with other unsupervised objectives (say, contrastive learning) at least for some tasks (say, object detection). Unfortunately, such evaluation is missing from the paper, thus I still find find that the benefits of the proposed approach are not convincingly demonstrated. #####################################################################Thanks for the  response from authors! I will keep my score. Note: initial score was 4; updated to 5 after taking a brief look at the experimental sections; will check again later to take a closer look at the updated "motivation" part.--- ### After response to authorsI thank sincerely the authors for providing a detailed answer to my concerns. I changed my note to 6. I believe the paper is interesting and show strong empirical evidences that the method is worth considering. I am not giving a higher grade because in my opinion the writing of the paper could be signifcantly improved.  ---#### UPDATEI thank the authors for their response. My major concerns regarding the paper's clarity and the implications of its theoretical results still hold. Therefore, I would keep my initial score and recommendation. Updates after Reading Authors' Response:Thank you for the detailed clarification and the new results. From the response, it seems that the proposed modifications to SAC do not and are not meant to solve the exploration problem in sparse-reward tasks. Instead, its aim is to improve sample efficiency on standard dense-reward tasks. The impact of the work then feels quite limited: the proposed modifications are specific to SAC and do not meaningfully improve performance on the sparse-reward task. For these reasons, I will keep my original evaluation. ----EDIT:after seeing all reviews, and most importantly thinking about it and pondering the answers given by the authors, I am sorry that I must lower my score. I still like the paper, that could be accepted in my opinion, but I think it oversells some contributions that are unfortunately not exploited (the brownian interval thing in particular, or am I wrong ?)---- *Post-rebuttal*All reviewers agree that this paper is not up to the mark. While the revision does include several additional related works, they are not very well integrated with the rest of the discussion on the paper. For example, how would some of these memory networks perform? How would Neural Turing Machine do? Considering this, I am hesitant to improve my rating for the paper, even if the collection of related works will certainly help in the re-submission. Update 1: I appreciate the authors' response. I think including the "on-going research" points as the authors brought up  and improving the clarity will greatly strengthen the submission. I still think the current form is not ready yet and would like to keep my score as is. ### UPDATE:Per my response in the thread, I appreciate the authors' replies and updates, but I am keeping my score because- Even with the new 2-layer results, I find this experimental setting still too limited;- Even within the conducted experimental setting, the benefit of likelihood-guided initialization over He init is not robust (notably on small dataset sizes, which is contrary to the expectation that a good prior will be more beneficial in such cases). Edit:### RebuttalI did read the authors' rebuttal, and the main issue, i.e. the significance, has not been addressed. I cannot take into account the new experiments, since they are not in the paper. Anyway, an experiment with a 2-layer network would be a significant modification of the present paper, which would be not acceptable during the rebuttal phase. UPDATE AFTER REBUTTAL:I would like to thank the authors for their responses. After reading the updated version I still would recommend to reject the paper. The reason is that the paper is written for a very narrow audience and is hard to understand for readers who are not familiar with this area of research. Also I  feel like some of my concerns where not properly addressed in the updated version (e.g., issue 2 and 6). After reading the authors' response, I still think that the novelty of this paper is very limited. I decided to keep my score at 4. ***POST DISCUSSION UPDATE***I am satisfied with the authors' response and will increase my score to an accept.***END OF UPDATE*** --------------------------POST DISCUSSION UPDATEThe central part of this work about how the extra network weights only affect the curvature still confuses me. But I'm more motivated by the proposed objective function that borrows idea from non-Bayesian robust learning literature. #################Post-Rebuttal Reviews: Thank the authors for the detailed responses. The proposed approach is an interesting add-up for the Laplacian approximations. However, I think the paper still deserves more works. As far as I am concerned, applying the approach to multi-layers instead of only the output-layer is important. I will keep my score for now.################## POST DISCUSSION UPDATEI like the proposed method and I will keep my score.END OF UPDATE =========================================================================================================After rebuttal (and a final question):Thank you for your responses and the experiments I requested. I hope the authors would add discussions with M5 and quantitative results in the final version. Before I finalize my ratings, I have a final question after seeing supplementary material L. I believe M5 is enough to achieve the goal of this paper since $[Z, C]$ captures disentangled factors in $Z$ and $[Z, C]$ improves the reconstruction quality. Then, what is the advantage of MSVAE over M5?=========================================================================================================After the final question : Thank you for your reply. I misunderstood the experiment M2 and propose M5. I believe M5 is just a variant of M2. The authors should add quantitative results (FID and disentanglement score) comparing the baselines (in supplementary material L) in their final version. I would raise my score to 6. Minor : M2-M4 denote mutual information (paper) and baselines (supp). Please use different notations. ================================================================Post Rebuttal:Thanks for the revision and detailed rebuttal. I think the clarity is largely improved now. As the paper has sufficient technical novelty with fairly clear description, I'm slightly inclined towards acceptance. However, I would recommend further clarifying the following point.>We want to point out that NeMo has exactly the same amount of supervision as all other methods- The fact that NeMo jointly reason occlusions in an unsupervised manner was not clearly explained. As the authors described, this property makes difference in terms of robustness to partial occlusions. Clarification on this in the text is very helpful.  --------------------------------------Updates after author response:---------------------------------------I think the revisions and the responses did address all the concerns I had, in particular towards assuring that the method and baselines leverage the same information for inference. Additionally, the experiments where one can 'search' for the optimal subtype instead of assuming known subtype at inference also showed encouraging results.Overall, I think the paper writing and presentation is much improved, and I would argue for acceptance as the paper presents a simple and intuitive idea which is shown to work (rather surprisingly!) well. **Post-RebuttalThe authors have addressed most of my concerns. I have increased my score. Although the additional experiments on the variance/checkpointing are helpful I would still like to see more discussion in the paper itself.  === post rebuttal ===I am satisfied with the response and i will keep my score. (Nov 24) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6. ----------------------------------(Dec 3) Taking into account the other reviews, the authors' responses and the changes made by the authors, as well as the extensive and controversial discussion, I rate the paper still with a score of 6. Update:The responses cleared up some aspects of the Hamiltonian engine and I adjusted my score accordingly. Still, additional baselines would improve the paper answer some open questions and validate some of the claims: Is the Ham. Eng. indeed faster than optimizing with a cheap force field? Is there then an advantage compared to featurization of the RDKit coordinates with one of the many MPNNs for 3d coordinates? This would be interesting to analyze. **Final update:** The main criterion used by the authors to search invariant predictors is not correct and is in fact not satisfied by the invariant predictors. For this my suggestion to authors is to modify their criterion in a way that it is at least satisfied by the ideal model you want to learn. **Update**: Thanks to the authors for clarifying most of my clarification questions.  I'm not sure I was able to fully follow your argument about why these results can't be adapted to a two-color dataset (see above), but to indicate that you've clarified many of my questions I've increased my confidence score to a 4.  Good luck to the authors.**Further updates**: I'd like to elaborate on my thoughts a bit more with the hope that the authors may find it useful for future versions of the paper. - First, I really appreciate the authors for performing additional experiments with EIIL during the response phase. I wish to emphasize that, after a long discussion with R3, I've some strong disagreements with their review regarding the "simple fix":   -   I personally think EIIL is out-of-scope as it is a recent algorithm. It doesn't sound like a "simple fix" to me. However, it's great that the authors were able to show that their algorithm works better.    -  I don't think pooling all datapoints and splitting it will work. You'll end up with a dataset with label-color correlation in both domains, and both domains will be identical. So IRM won't work here.  - One can always come up with some hacks like "pool all environments and carefully split them back" that work under the assumption that there's $\Lambda$-spurious correlation. But those hacks would be sub-optimal if there were no $\Lambda$-spurious correlation.  Therefore, this is an unfairly powerful "overfit" hack, and does not make a good baseline. You want an elegant solution that works whether or not there's $\Lambda$-spurious correlation as you won't know whether that sort of a spurious correlation exists in practice.  - As a side note, in light of the above point, I think it's important that the authors also demonstrate that the CDM constraint added preserves the performance of IRM on the original CMNIST dataset.   - Hopefully the authors can keep the EIIL results for future versions of the paper as it only makes the paper stronger.- I don't think the paper should be heavily penalized for the lack of a realistic dataset, because it's hard to verify $\Lambda$ spuriousness on realistic benchmarks. However:  - I'd strongly encourage that you consider trying similar experiments on a dataset like say Rotated-MNIST (or Rotated-MNIST+ to be more precise, if at all possible).   - Even better, you could consider whether similar experiments can be done with Celeb-A where you have access to image attributes like hair color etc., (See Fig 2 https://arxiv.org/abs/2005.04345) and you could try creating different environments by sampling differently in each.  - If you think that's it's impossible to create a $\Lambda$-spurious dataset, you might want to explain in future versions of the paper as to why that's not possible.- I understand R3's main concern which is that the algorithm in this paper requires that the distribution of the causal features $X_{causal} | Y$ to be the similar across all environments. It seems like IRM doesn't expect this sort of invariance, while algorithms prior to IRM do require something of this sort (including CDM, DANN etc.,). I think one actionable way to address this concern would be to show that there are datasets where IRM + CDM does better than CDM (just like how IRM+CDM does better than IRM in CMNIST+). This way we can see why combining IRM and CDM offers something unique.- Finally, I want to appreciate your efforts in trying to clarify all the reviewers' concerns (at least I found them helpful) and also to update the paper accordingly, add experiments etc.,  ## Second reviewThanks for taking all my comments seriously. After clarification of the difference with RFLO I see that this work is even richer than I thought and I increase my grade to 8. It seems that other reviewers did not appreciate that training a network without back-prop requires nontrivial engineering and theoretical considerations which are well described in this paper, I truly think it is a pity if this work is not accepted.I fully agree with the difference between RFLO and Snap-1 that you describe in your reply, and I think it would be really great to put that somewhere in the paper. As you suggest it would be great to explain that you did not use random feedback weights for RFLO.This would also be a great opportunity to explain how did you extend RFLO to a GRU network in Figure 3. I find it a bit puzzling, that RFLO appears worse than an untrained network in Figure 3 (even early in training as in seen in Figure 3.B). Is there any additional difference in the network model for these two baselines, like one is using leaky RNN and the other one GRU or something like that?I find the piece of JAX code incredibly rich. It would be great to publish that along with the paper! JAX is not yet very well spread, and we see here that it is a very promising tool for custom gradient in RNNs. ## Post response updateThe author's response has clarified most of the missing details in the paper. I still have an issue with reporting results for 3 runs --- even if the variance is small for 3 runs, that does not imply that there won't be outliers when one does more runs. Nonetheless, the proposed method is insightful and the paper has significant pedagogical value. I'm moving my score from 6 to 7 and I hope the paper is accepted.  ==========After rebuttal: The rebuttal resolves most of my concerns. It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training, which seems novel to me. I also realize that the closed form dynamics are not for individual parameters, but in terms of some statistics of the parameters, e.g., the sum of the parameters. This makes the results not as existing as what I thought. That is why I decide to raise my score to 6. ----------------After the rebuttal: I'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications. Summarizing my impressions from the discussion, the two main points of criticism are that the proposed analysis is not fully predictive (depends on the norm of the gradients that depend on the empirical data), but rather provides the laws that  govern the dynamics, and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly. The validity of the continuous dynamics is demonstrated in numerical results only.  I do agree that a fully predictive framework on SGD directly would be very intersting. Yet, I think the authors are taking important steps towards such a framework, and considering the fact that SGD often behaves surprisingly/unexpectedly (as also stated by R3), I am still quite impressed how accurately the theory matches the actual SGD behavior. For our understanding of how symmetries/invariances in the weights of network architectures influence the training, I believe this paper does provide interesting insights such that I recommend its acceptance. As for a final score, I could go down to a 7 to account for the concerns raised by my fellow reviews, but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible, and this aspect should be reflected by the confidence rather than the rating. Thus, I'll give the authors the benefit of the doubt and keep my score, since I really enjoyed reading this paper.  Update after the rebuttal: I would like to thank the authors for the detailed reply and for addressing raised issues in the submission. I appreciate the authors' rationale, but the "standard" structure of papers makes it is easier to follow. The same for a conclusion, for the authors it may be reiterating the same ideas, but personally I found conclusions the best place where one can quickly get a flavour what has been done in the paper to assess whether it is actually worth spending time reading it in details. Also, they are helpful in cases like this when a reader (me) is outside of the research field of the paper. Regarding discussions, I appreciate that there is discussion for Theorem 1, but there are theorems and propositions stated in the formal language only which would benefit by being repeated in plain English. If they are just technical results required for the main proof, they may be moved to appendix then.Overall, I am increasing my score to reflect positive changes in the submission. There is a minor mistakes:- In the first line of Conclusion: "obtained aN explicit"========================================================================================================= ** updating the score to 5 ** **After rebuttal**Thanks for the detailed response.This paper can be seen as an interesting attempt to use self-supervised on time series data. Although the basic idea is similar to SimCLR, It is still interesting work considering the  computation complexity and new loss function.So I update my score to 6. Update after rebuttal==================I will remain my score of weak rejection. ---Post rebuttal---Thank you very much for the response, and I understood that some of the concerns I raised can be resolved empirically under appropriate conditions.   After the response, however, I think I am still comfortable with the original score, because this is (as I understood it) a theoretical paper and I felt that it is necessary to make a judge based on theoretical solidness.  ######## Post rebuttal ###########Thank you for your response and the efforts to improve the presentation of Section 2. After going through other reviewers' comments and the authors' responses to those, I am comfortable with my original score.  ---### After RebuttalIncreasing rating based on the authors' clarifications on the source of the gains. Open to further changes based on further review and discussions with other reviewers   *** post-response ***Thanks for the response and extensive modifications. I have increased my score to "weak accept" to reflect the improved clarity. I think the paper remains on the borderline, for the following reasons:- I still doubt the practicality of this algorithm in any realistic settings. The experiments demonstrate that one can gain on the variance term from importance sampling in SGD, but not the practicality of the moment-sketching one-pass estimator of multiple gradient queries on practical problem sizes. Thus I disagree with the "vastly superior" characterization; it is well-known that the gradient estimator this paper seeks to efficiently approximate is a good choice; a convincing proof-of-concept is that it's useful to use sketching to efficiently approximate it (whether it's worth the computational overhead + approximation error). The wall-clock time experiment is independently interesting, perhaps showing that you don't even need the sketching ideas from this paper to benefit from importance-sampled stochastic gradients. That said, my evaluation discounts the experimental part and evaluates this as a purely theoretical work.- The theoretical work is an application of [Mahabadi et al. '20], without discernible major theoretical innovations. Though, in my opinion, the application to SGD makes it creative and relevant enough for publication, despite practicality concerns.- Some clarity issues remain, rendering the paper hard to digest: squared-norms in main paper vs. norms in appendix, hidden dependences on p, "follows immediately". After thinking about it, the "implicitly computed" tensor contractions in the comment to R4 makes sense, but this should probably be pointed out clearly in the paper, if I correctly understand that it removes all d^p factors from the analysis. ---### Response to AuthorsI'm satisfied with your responses, especially strengthened experimental results and the clarification of methods in the revision. As my concerns were on doubtful empirical results in the submission---although I thought the approach of the submission was sufficiently novel---I updated my score from 4 to 6 accordingly. I don't know if the authors will keep working on this direction, but I think it will be interesting to see the performance of *off-policy RL* with the proposed method.  ### [Comments on the Revised Paper, Rating and Confidence] The revised version now contains a much clearer description of how the layers are integrated into the algorithm, fixes several typos and reorganizes (and enriches some details of) the numerical experiments following comments of the other reviewers. However, most of my major concerns above still remain (which is expected as the authors didnt get a chance to see my review, and I sincerely apologize for this). 1. For example, although the authors now provide some more detailed explanations about impractical projections in Section 4.4, it is still unclear why one cannot use the projection as a post-processing step instead of a layer, and what the authors are trying to convey in the more detailed discussion about impractical projections with a growing storage of previous policy networks in the revised draft here. 2. Also, the authors may want to clarify some new terminologies and notation introduced in the revision. For example, are contextual policies just policies with state-dependent covariances? And what is the index $t$ in the Adam updates in Algorithm 2, and should $a$ and $s$ also be $a_t$ and $s_t$ here? 3. Another issue I noticed is that compared with the revised draft, the results (in terms of which method is optimal, and whether or not the proposed layers improve over PPO/PAPI) shown in the plots and the tables are not very stable, which indicate that different runs give pretty different results. Such kind of instability may also be relevant to the inconsistency between Table 1 and Figure 2 in the original draft mentioned in the original review above. Overall, I decide to maintain my original rating. **Post-rebuttal comments.** Thanks to the authors for their response. The updated version of the manuscript addressed my main concerns and recommendations. Now, it is clearly improved, figures and metrics updated and the proposed methodology is better presented. Authors even did major changes on the structure of the paper, what I recognize as an important revision. Having said this, I raised my score. Update======I have looked through the response and edited version. The updated evaluation looks solid and provides a potential solution to a robust overfitting problem. Although the work is primarily empirical in nature, it may inspire directions for future work to look into more theoretical explanations of robust overfitting.  ---- UPDATE: Thanks; I have read the response, kept my score, and responded below. **After rebuttal**I'd like to thank authors for their efforts to address my concerns. I didn't change my initial rating, due to the two main concerns below:(1) To me, the main argument of this paper sounds "when a large (and maybe diverse) OOD is given, adding an OOD class to the classifier is better than baselines." Since the large OOD setting has already been proposed by [Hendrycks et al.], the only contribution of this work is on the empirical observation that the proposed method is better than baselines. While the observation is interesting, I think the contribution is not enough as a full ICLR paper at this point.During the rebuttal period, R3 corrected it that "the main question investigated by the paper is how to best use the outlier exposure set," and this sounds better. However, authors didn't emphasize the setting but their method, such that their main argument is (if they intended to say as like what R3 understood) misleading. Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. However, the only statement about the setting I could find in the intro is that "as in Hendrycks et al. (2018), we uses additional samples of real images and text from non-overlapping categories to train the model to abstain, ..." i.e., rather than elaborating/emphasizing the setting (together with their method), they just cited a prior work.In short, I recommend authors to rewrite abstract/intro as suggested by R3, to properly emphasize their contribution.(2) The comparison is unfair, as authors didn't re-evaluate baselines in the same setting (they had to make it the same as much as possible) but just pasted numbers from original papers. Even the comparison with the closest prior work [Hendrycks et al.] is unfair, as the prior work fine-tuned the model while the proposed method trained the model from scratch.Regarding the performance of similar methods evaluated in [Lee et al. (a)] and [Dhamija et al.], I think the main reason why they didn't get the same observation is on the size of OOD dataset, i.e., they didn't train their model with a large OOD dataset like [Hendrycks et al.] or this work. As this work claims, it might be true that when a large OOD dataset is available, adding an OOD class to the classifier is simply good enough. **Update after author response:** I have read the other reviewer's comments. My take is that at a high level the contribution of this paper is above the bar of ICLR, but the experiments aren't controlled enough so I vote for a weak reject.Hendrycks et al propose encouraging high entropy predictions on the outlier exposure set, instead of classifying them into a reject class. Going further, Hendrycks et al claim "but we find that even with OE, classifiers with the reject option... are not as competitive". As I understand, Hendrycks et al are basically saying the (simpler) method in this paper does not work as well - but Hendrycks et al don't provide experimental evidence for this claim, it's merely stated. The original paper might in fact discourage practitioners from trying this approach, and instead using the high entropy approach. In fact, this was a question in my mind when I read Hendrycks et al last year.Instead, this paper seems to show stronger results, with a more intuitive and simpler method (just classify the outlier exposure set into a separate class) that Hendrycks et al suggest doesn't work as well. Conceptually, the approach in Hendrycks et al also seems more brittle and there are distinctions between these two methods (e.g. see Vernekar et al 2019, Analysis of Confident-Classifiers for Out-of-distribution Detection).So what's the potential practical impact? If this paper didn't exist, I suspect practitioners would use the method in Hendrycks et al, and not try the reject class method, because of that paper's claims. But with this paper, practitioners might use this method or try both, which seems like a good impact.So barring problems with the experimental setup, I'd give the paper a 7 / accept, and so I'd encourage the authors to continue on with this work.To me the decision hinges on the quality of the comparisons. I am inclined to agree with R1 on the quality of comparisons. Taking a closer look at their paper, they have no detailed discussion about the hyperparameters and experimental setup, which is key when the main contribution is a fine-grained comparison with Hendrycks et al. R1 raises a question about fine-tuning vs trained from scratch, and it does look like this paper trains from scratch whereas outlier exposure fine-tunes. The outlier exposure set is also different. While it is a smaller set in this paper, Hendrycks et al say "experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models.".Overall, I agree that the best thing would probably be for them to do a more careful  and controlled comparison, include all these details, and submit to the next conference. In my review I did mention that their comparisons were unclear, but they didn't take the chance to update their paper and misleadingly responded that "The OE method is closest to ours, so we are able to match their training regimen well", but as R1 points out there seem to be salient differences. Post-revision update------------------------Thanks to the authors, I think that the revision provided by the authors makes the paper substantially stronger. The inclusion of the more complex Shapes3D dataset substantially improves the experiments, and I think the discussion has improved. I have revised my rating to a clear accept in accordance. .UPD: The authors addressed my concerns and added additional experiments. The paper is improved, therefore, I increase the rating Final Evaluation (Post Rebuttal)---The author response and accompanying paper revision clearly and effectively addressed each of the 3 main weaknesses I pointed out, so I raised my rating. ----Post-revision update---The authors have provided results on a second dataset 3DShapes and two more models  Factor-VAE and a perfectly disentangled model. However, the construction results of the GT decoder is much worse that other models, see Figure 2; it does not reconstruct the details of the "heart" shape even for training and the edges of the "square" are not straight. This begs the question how good the GT decoder is. The open question is, what is the generalization capability of a GT decoder that can both reconstruct and disentangle perfectly?Wasserstein auto-encoder has been shown to disentangle better and the regularization term is on the aggregate posterior instead of individual samples. Without results on WAE, the paper should refrain from making broad claims on disentanglement.  Furthermore, it would be interesting to investigate GAN based approach such as InfoGAN-CR as well.For an experimentation paper, it should be more thorough and go beyond just two shape datasets.I applaud the additional results the authors provided. I still think the paper is borderline (more toward 6 now). If it fixes the aforementioned weaknesses, I would recommend accept.  ==========Post rebuttal==========I appreciate the authors' great effort on answering my questions. The response clears many confusing points from the original draft. Meanwhile, I still have concerns on how the idea of contrastive learning is handled in this paper, which could have been better shaped. In sum, I have increased the rating accordingly. Updates:Thank you for the clarifications, new study, and theoretical justification. The new results demonstrate the utility of PSEs even when policies are not exactly optimal. My concerns were thoroughly addressed by the authors response, and Ive updated my score accordingly. ------------------------------------Authors did answer some of my doubts through their response. However, I am still not very convinced if the paper has sufficient novelty to get accepted in ICLR. I increase my score from 4 to 5. final recommendation:I maintain my score. I think this is an interesting piece of work that can easily be used as citations whenever the discussion of "why don't you just relax your program to be differentiable" comes up, and I can cite this paper and say "no that does not work theoretically". **Comments after Author Response**I thank the authors for their response. Queries 1,2, and 4 have been adequately addressed. Regarding Query 3, I appreciate the addition of the Deep Ensemble results though I find that the text of Section 5.5 has not been changed to reflect the same. Specifically the paper still says "For the IMDB age prediction dataset, results show that PIVEN outperforms both baselines across all metrics". This is now incorrect since there is a third baseline, DE, which appears to outperform/match PIVEN for this dataset. However the explanation that this is because the population age is approximately Gaussian makes sense to me and so keeping in mind the good performance of PIVEN on the other datasets, and the improved explanation and added ablation study for hyper parameters, I recommend accepting the paper as long as the relevant corrections are made in Section 5.5. ----[post rebuttal]I thank the authors for their revision and clarifying many of my questions and adding results based on that. I have read other reviewers concern and while I agree some room for improvement on clarity, I still think the paper brings in value. Unless there's technical flaws spotted by other reviewers that has not been resolved, I'm still leaning towards accepting (increased score from 6 to 7).  ---------------------Post rebuttal: I thank the authors for their rebuttal. Let me focus my reply on a few important points. I first thank the authors for clarifying the meaning of Condition 2. In this sense, Condition 1 is the key main contribution; however the current proof does not look correct to me, and the revised argument is far from being sufficient. In particular:- Point 5 of the rebuttal: I think the revised argument here is incomplete. The given argument concerns trivial facts and does not imply the claim. For example, what if the distribution of the terms is symmetric, the expected sum is zero, and hence the quantity might be of order smaller than d? Note that this is an example problem; there are multiple problems with the proof of Lemma 1.4. For example, the paper claims this for all k; but if k is something like d^100, would things hold? What would stop the magnitude of the weights to grow with time?- Point 6 of the rebuttal: The CLT, when applied w.r.t. the randomness of the weights, says that for a fixed $x$, $\sum_{r=1}^d \hat{a}_r \phi (\hat{w}_r x) / \sqrt{d} \sim N(0,v_x)$ approximately. That is, there is a non-zero probability (w.r.t. the randomness of weights) that the claim in the paper for a fixed $x$ fails. As such, to reason the claim for many $x$, one requires doing probabilistic arguments very carefully.The paper should execute the proof very carefully. It is not just a matter of technicality; I suspect some of the claims are actually wrong.More importantly, Condition 1 alone is insufficient to claim dynamical stability at any time k. What should qualify for dynamical stability is rather the existence of a well-defined limiting dynamics exists (which is argued heuristically in Appendix C), and its proof. In the current writing, its unclear how Condition 2 is crucial; while it studies interesting properties, it is very restrictive.As said in my last review, one thing that has been missing is really whether the insight here differs qualitatively from the known NTK and MF limits. Further looking at the limiting dynamics in Appendix C, one sees that they are qualitatively either NTK or MF. There are possible degeneracies due to scalings and the use of logistic loss, but these do not lead to much deviation from NTK or MF behaviors. If one is to use a squared loss for instance, what one would obtain in Figure 1 is just the line connecting NTK and MF; all other points in the band outside this line are degeneracies due to logistic loss. The behavior on this line, again, is qualitatively either NTK or MF, and this is shown (somewhat implicitly) already by a number of past works.I would imagine a rigorous derivation of the limiting dynamics for each point in the band revolves around the renormalized dynamics in Appendix C. When translating from the renormalized dynamics to the original one, the extra scaling factors will complicate the proof (for instance, they can blow up Lipschitz constants). Again this has to be done very carefully. ---- Post-rebuttal ----The additional ablation studies on the attention and MADDPG-p are helpful, and address most of my concerns - though it is still not clear to me whether any of the baselines compared to is exactly equivalent to the method used with attention weights (1, 0, 0, ...). I share the concerns of Reviewer 4 about the significance of the results - this is still not in the paper, and not present for the new experiments. Overall, I have not changed my rating. [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4's concern on the significance of results is valid. Therefore, I will keep my initial justification as is.  I have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain. ----After reading the author responses and updated manuscript, I have raised my score from 6 to 7. I raised my scores from 4 to 6 after the author updated their draft and answered my questions. I appreciate their efforts in addressing my concerns and improve the paper. The current version is good enough to be accepted and it also compares with other approaches thoroughly. However, I still feel the symbolic module is too simple in this work and does not distinguish it from other works.  ===Post-rebuttalI'm upgrading my score from 5 to 6, because some of the ablation experiments do make the paper stronger. Having said that, I still think this is a borderline paper. "Co-ordinate conditioning" is an interesting approach, but I think the paper still lacks convincing experiments for its main motivating use case: generating outputs at a resolution that won't fit in memory within a single forward pass. (This motivation wasn't clear in the initial version, but is clearer now).The authors' displayed some high-resolution results during the rebuttal phase, but note that they haven't tuned the hyper-parameter for these (and so the results might not be the best they can be). Moreover, they scale up the sizes of their micro and macro patches so that they're still the same factor below the full image. I think a version of this paper whose main experimental focus is on high-resolution data generation, and especially, from much smaller micro-macro patches, would make a more convincing case. So while the paper is about at the borderline for acceptance, I do think it could be much stronger with a focus on high-resolution image experiments (which is after all, forms its motivation). Thanks for the detailed response! The new revision addresses my presentation concerns and answers my questions, so I've increased my score to a 7. I still think it would be useful to present a slightly more targeted set of ablations in the main paper rather than the kitchen sink in table 2: e.g. just - "co-grounding" / nothing- progress monitor / progress inference / both / neither- best model + data augmentation / best model (no data aug)Haven't proofread the new draft carefully but "state of the arts" in Table 1 is wrong, so you should do another copyediting pass before the final version if the paper is accepted. Update after author responses:--------------------------------------------Authors, thank you for your feedback.While it is true that generalizing RAML and SPG into a common framework is not trivial, the presented framework simply augments the dual form of SPG (i.e. REPS [16] in the SPG paper) with a RAML term. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.Wrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. However, the unusally poor performance of MIXER remains unexplained, and falls short even of scheduled sampling (SS), which suggests a lingering major issue. REINFORCE techniques rely on 1) strong baselines, verified by the authors, 2) larger batch sizes to reduce variance, and 3) pre-training to reduce variance and facilitate efficient exploration. If the MLE is undertrained or overtrained (the latter the more likely issue given the plots), then MIXER will perform poorly. Actually, it is now standard practice to pre-train with MLE+SS before RL training, and this is really the (also dynamically weighted objective) baseline that should be compared against. The current REINFORCE results (MIXER or otherwise) really need to be updated (or at the least removed, as they are not captured by the framework, but the comparison to PG methods is important!).More generally, I feel that the experiments are not yet comprehensive enough. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is.Overall I feel that the paper is still in need of substantial maturation before publication, although I have revised my score slightly upward. I am satisfied with the author's response and changes they made to the text. I still think the paper brings significant contributions to the area, by showing that even generating the pseudo-tasks via unsupervised clustering method allows the meta-learning to happen. [Updated after reconsidering other reviews]Although this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation. [update] After carefully reading the response (also from other reviewers), I decide not to change my rating. --Revision: The authors have added HER baselines. Agreed with the authors that comparison of per-timestep perceptual reward vs terminal state perceptual reward is a good topic for future work. After considering the author feedback and their effort to address my concerns, I've decided to raise my rating to 6. Thank you for the hard work. ######### After considering the proposed improvements, I decided to raise my mark to 6. Thanks for the good job done! ###### Post-Revision ########################Thank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. Unfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. ###### Post-Revision ######################## Update:Score increased. Revision after rebuttal:The new version is definitely clearer and easier to read, hence I support the paper for acceptance and change my rating to 7. There are still minor improvements that can be done in Section 4 to improve the overall clarity:* About the metrics, the "Average attack success rate" and the "Target command recognition rate" should be clearly defined, probably under the description of the attack methods.* The Adaptive attack approach could be introduced unter "Attack methods" in 4.1.* Table 4 is not easy to read, the authors should improve it.* The first paragraph in Section 4 ("The presentation flows ...") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of Section 4 or to Section 5. **** After Revision ***********I thank the authors for diligently revising the paper according to the reviewers' suggestions. I have increased my score for the paper. I still think the experimental evaluation can be more thorough. For example, it would be good to show the effect of varying the \tau parameter and the number of available labels (k). It would also be good to experiment with the Flickr graph without any sparsification and to add uncertainty estimates to the results in Table 1. **** After Revision *********** ==================After rebuttal==================The authors provided new experiments supporting the proposed method. I am happy to increase my rating. Revision:I thank the authors for the all the extra experiments they performed. The paper looks good to me, and increased my evaluation accordingly. ********* updated review *************Based on the issues raised from other reviewers and rebuttal from authors, I started to share some of the concerns on applicability of Thm 1 in obtaining information on low k Fourier coefficients. Although I empathize author's choice to mainly analyze synthetic data, I think it is critical to show the decays for moderately large k in realistic datasets. It will convince other reviewers of significance of main result of the paper. ------After rebuttal:I gave detailed responses to each part of the rebuttal below. Here is the summary:Although the response addresses some of my concerns. There are still major issues with the experimental study. 1) there are existing, relevant and well-studied multi-task setups with negative interference. Method should be experimented with some of those setups. 2) Multi-task baseline in the paper is naive and far from state-of-the-art. Paper need strong baselines as discussed. Hence, I am keeping my score. Paper needs to be improved with a stronger experimental study and need to be re-submitted. ---------[UPDATE]Regarding the comment "Our paper resolves the question posed in ICLR Best paper 2017 "Understanding deep learning requires rethinking generalization"", I don't think that analyzing networks with explicit regularization resolves the questions stated in Zhang et al paper. As other reviewers mentioned, there are a number of other papers that formally define quantities that correlate with the generalization error, and are larger for random vs true labels. There are also other papers showing that one can tune some parameters of the optimization algorithm to avoid overfitting on random labels (while it is a modification to the algorithm, it is still similar to explicitly regularizing the Lipschitz constant of the network) (see e.g., Dziugaite et al work on SGLD).Therefore, the claim in the abstract "A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?" needs to be toned down a bit.In my opinion, the work presented in this paper is a valuable contribution to learning theory. The new version of the paper is easy to read. Therefore, I recommend acceptance if the authors change the claim about resolving  the questions posed by Zhang et al.Another typo: - for convergence, we require that the network also grow(s), I have read the authors' detailed rebuttal. Thanks. ==========================I change my rating on this paper to be 6, after the authors' response. Update after response:The authors have provided improvements to the introduction of the problem setting, satisfying most of my complaints from before. I am raising my score accordingly, since the paper does present some novel results. 2. "DefenseGAN is broken"You are right about DefenseGAN not being broken.3. The evaluation methods used in our work are standard methods which are widely used in all other previous adversarial defense works. FGSM and PGD are indeed widely used, but many previously proposed defences used additional attacks (like transfer-based, score-based, decision-based). Please check https://arxiv.org/pdf/1802.05666.pdf for an in-depth discussion of this issue.4. "Gradient masking"You are right that the gradient masking effects visible in the graybox attack doesn't necessarily indicate gradient masking in the white-box setting (but still means that hyper parameters of the attack have not been tuned properly).Given the discussion I will increase my score by one point, but the lack of a reliable robustness evaluation and the reduced novelty compared to DefenseGAN still puts it below the acceptance threshold in my opinion. Update:------------After looking at other reviews and author rebuttals to all reviews I am raising my grade. Update: score updated to 8 (from 7) following discussion below --- In their rebuttal, the authors satisfyingly addressed my concerns. Hence, I am upgrading my overall rating. Update: I thank the authors for their response and revision of the paper. To me, results on WN18RR and FB15k-237 are inconclusive w.r.t. to the choice of using Riemannian as opposed to Euclidean space. I therefore still believe this paper needs more work before acceptance. The latest revision is a substantially improved version of the paper. The comment about generalization still feels unsatisfying ("our model requires choosing c* in the support of P(c) seen during training") but could spur follow-up work attempting a precise characterization.I remain wary of using a neural net reward function in the simulated environment, and prefer a direct simulation of Eqn5. With a non-transparent metric, it is much harder to diagnose whether the observed improvement in List-CVAE indeed corresponds to improved user engagement; or whether the slate recommender has gamed the neural reward function. Transparent metrics (that encourage non-greedy scoring) also have user studies showing they correlate with user engagement in some scenarios.In summary, I think the paper is borderline leaning towards accept -- there is a novel idea here for framing slate recommendation problems in an auto-encoder framework that can spur some follow-up works. Thanks for addressing my comments. The social pooling mechanism improves Indep-RNN as expected, however, as you show, it's not better than your method. This makes the results stronger. Additionally, the plotted trajectories shine light on the behavior of the trajectories. The trajectories are still better than the baselines after this additional information. Given the authors' response, I have increased my score. It will be nice to see this work take a semi-supervised or unsupervised route in the future :) ############ Updated Review #################I have read the author(s)' rebuttal. My decision stays unchanged. In my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for ICLR. Additionally, the author(s) did not update their submission to reflect the changes. I thereby recommending rejection to this submission. ########################################## Post-rebuttal update ===The authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.Hence, I've increased my score for this paper. ========Thank you for the response. Given that there is no consensus on the questions posed by AnonReviewer2, there will be no update to the score. I appreciate the author response and additional effort to provide comparison with MoNet. I have raised my rating by 1 point. It should be noted that the edits to the revision are quite substantial and more in line of a journal revision. My understanding is that only moderate changes to the initial submission are acceptable.----------------------------------------------- Edited: I raised the score by 1 point after the authors revised the paper significantly.-------------------------------------------- Post-revision update----------------------------Thanks to the authors for their revision. Unfortunately, I still feel the contribution and value of the work is not well communicated. Many of my previous points still stand. It is not a sufficient response to just say that the two propositions are the main contribution and the critical summary of findings and their relevance. ** Update after the authors' response **I have read the authors' response and other reviews. I still believe that my evaluation is correct at the moment. At the same time, I believe that the research direction is very and promising, and I hope to see the updated version of the manuscript published in the future! ### Review update following author discussionI've read the author responses as well as some of the discussion with the other reviewers. Overall, this is valuable work and I've considered raising my score, but I think a weak accept is appropriate, all things considered.  I've raised the confidence score for my review, as I understand the technical details better now. I think a key strength compared to prior work is the empirical validation of the approach on a variational RNN.  However, the significance of the paper in terms of the novelty of the ideas, both conceptual and technical are overstated in my opinion, hence the weak accept. One other point of feedback for the final version in case of an accept:I also share R3's concern about the paper's positioning in relation to the extremely general dice enterprise framework (or, even the bernoulli factory, for that matter) as somewhat misleading for the particular use case in Section 2.2.  The exact same multinomial sampling scheme is in fact more succinctly presented (proposed?) in BRPF [Schmon et. al, 2019] as the "Bernoulli Race" (which the authors have cited).  I would think that the current scheme is a special case of the "Bernoulli race" that uses a particular form of the acceptance probability parameterization similar to prior work (e.g. VRS [Grover et. al. 2018]). See Section 3, specifically e.g. Eq (10) and Proposition 2 from BRPF [Schmon et. al., 2019], which can be compared to Eq (3) and Proposition 1 in the current submission, respectively. Minor nit: In step 3 of the algorithm (right below Eq (8)), you use the notation $z_t$ for sampling a new variable from $q_\phi$. However, this $z_t$ has nothing to do with the latent variables that are used in computing the constants $c^i_t$. The way it's written makes it appear as if there's a circular depdendence of the $c_t$ on $z_t$ and then the $z_t$ is again resampled, which changes the $c_t$. For this reason, it maybe better to use a completely unrelated variable for the sample from $q_\phi$ in step 3.  -------------FINAL RATINGI have carefully read the other reviews and authors' replies.I agree with other reviewers that the rebuttal is satisfactory, and raised the score accordingly. Thanks to the authors for their effort. ----------- Update after rebuttal:I am very pleased by the answers of the authors, in particular, with the additional experiment showing that the algorithm could be extended with more advanced exploration strategies. I reviewed my rating accordingly.  Regarding the answers provided by the authors, I increase my score  --------After discussion:After reading the author's reply as well as the opinions from other reviewers, I will stick to my original rating since 1) the writing makes the paper hard to understand 2) current experiments cannot support the central claim of spatial-temporal reasoning. While the author resolve some of the concerns, they are encouraged to further polish the paper and use more evidence to support their claims. ===== Post Rebuttal =====The reviewer is satisfied by the authors' response. I am fine with raising my score. In the final paper, it would be nice if the authors can include a detailed discussion about the dependence of $m$ in their results.  UPDATE: I've upped my score to a 7, and would like to see the paper accepted.  **Post-rebuttal update.** Thanks to the authors for their response to all my questions and comments. I also read the updated version of the manuscript, which is clearly improved and the rest of reviews and comments by the AC. Looking to that, I agree with the rest of reviewers about the quality of the paper, so I raised my score and I recommend to accept it. ===============After Reading Authors' Response ================The reviewer would like to thank the authors for their detailed response and careful revision of the paper to address the reviewer's concern. However, the reviewer is not persuaded by the authors' response. Specifically,1. the reviewer is not satisfied with the explanation and modification to address the scalability issue stemming from both the cause-specific subnetworks and the output layer. Simplifying the structure and parameterization of cause-specific subnetworks when many are present seems like a comprise rather than a principled approach to address the issue. The same is true for the exponentially distributed parameterization of the output layer.2. It is the reviewer's impression that for point process neural networks, it is possible to use the covariate information for prediction, as opposed to the claim given by the author. Since the authors did not provide a proper response to my questions, I have lowered my score from 7 to 6. I think this paper will have a good chance to be a good paper if evaluated more comprehensively, as suggested by reviewers.  Revision: Figure 1 is convincing and hints to why SN-GAN acheives slow decay while in principle it only tries to control the spectral norm. I think this paper is a good contribution as it provides a simple and efficient algorithm to precisely control the spectrum. Moreover, a recent work ([2], theorem 1 ) provides theoretical evidence for the importance of controling the whole spectrum which makes this contribution even more relevant.[2] M. Arbel, D. J. Sutherland, M. Bin kowski, and A. Gretton. On gradient regularizers for MMD GANs. NIPS 2018 Apologies for my mistake about prior work on applying transformer networks to music: while reading other papers on music generation, I had encountered a few citations of a 2018 paper that directly applied transformer networks to music generation. After going back and inspecting, I found that the paper being cited was in fact the arxiv version of your paper, effectively blowing my mind! This changes my opinion. Originally I felt that even as an application paper, the technical novelty was thin since transformers had been applied to music in the past. But given that these results are in fact the first on applying transformers to music, I think they do make sense at ICLR. I have changed my rating accordingly.Further, thank you for your diplomatic response! Thanks for your responses. Yes, I was referring to use the gated attention approach to use as another baseline and use different metric (rather than data efficiency ) to evaluate this framework.Most of my concerns were addressed. As a result, I've updated my score. That said, I am hoping that the authors will add more tasks and metrics to evaluate this platform. Data efficiency metric is good but not enough. ----Post-rebuttal reviewI appreciate the authors' efforts in clarifying some of my concerns. However, I am still not convinced the comparison has been made fair. Many numbers from Table 1, such as ZOO, Opt-attack, QL-attack and AutoZOOM seem to be directly adapted from the papers rather than implemented and reproduced based on the same setting as the proposed attack. In particular, given that QL-attack is a published work, one of the state-of-the-art method and its codes has been released, I would really love to see a direct comparison using the same data samples and threat model. I would also like to emphasize that implementing all attacks under the same setting is crucial, since different attack methods may have a different criterion to determine attack successfulness. For example, QL-attack has some pre-defined distortion (L2 or Linfinity) for determining an adversarial example is successful, in addition to a different predicted class. EDIT: the concerns were mostly addressed in the revision. Update: the authors' response and changes to the paper properly addressed the concerns below. Therefore the score was improved from 6 to 8. I have read the authors' response and other reviewers' comments. Thanks the authors for taking great effort in answering my questions. Generally, I feel satisfied with the repsonse, and prefer an acceptance recommendation.  -----------Revision-----------In light of the discussion with the authors, the revision made to chapter 4 and in particular the proposed modifications to section 2.4 for a camera-ready paper, I revise my score to 6. ---I have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7. EDIT:  In their revision the authors addressed these concerns well and the paper is much more convincing (see longer comment below).  In light of this I have changed my rating from a 5 to an 8.  ==== Updated Review Following Rebuttal ====The authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. (Score raised from 8 to 9 after rebuttal) ------update----------I appreciate the authors efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it.It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as Improve our theoretical understanding of neural networks, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis.Specifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even resnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as much thinner networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., winning tickets near or below 10-20%, depending on the level of overparameterization of the original network.The observation of winning ticket weights tend to change by a larger amount then weights in the rest of the network in Figure 19 seems natural and the conjecture of the reason magnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization.The figures could also be improved and simplified as the lines are hard to read and compare.    ------------------------------------------------I have read authors' comments. I raised my rating. After the rebuttal.- the authors address most of my concerns.- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.-------------------------------------------------------------  ------------ Response to rebuttalsThe writing of the introduction has been greatly improved. The authors suggested a practical scenario of using high level features. I am still somewhat skeptical. To do this the users need to map raw input to good representations. It seems that there are two ways this can work: users and service providers agree on publicly available representations, or the service provider is willing to share everything except the top layer. Both assumptions seem rather restrictive. Nonetheless even with practically useful scenarios, it is standard practice to use good features/representations whenever available, so not really an academic contribution. I am evaluating this paper only by the Lola contribution. The presentation of LoLa can still be improved, but I see the main ideas. I think the paper could benefit from additional discussions and experiments. For example, when a practitioner wants to solve a new problem with some design need (e.g. accuracy, latency vs. bandwidth trade-off), what network modules can he choose and how to represent them? I think this type of general discussion can improve the significance and usefulness of the proposed approach.  --edit:The authors have sufficiently addressed my questions and concerns and have performed additional analysis.  My biggest concern of weather or not the RFM-augmented agent was capable of learning without a pre-trained agent has been addressed with additional experiments and analysis (Figure 8). Based on this, i have adjusted my rating to a 7. Update post rebuttal-----------------------------The experimental setting that is a little lacking. Qualitatively and quantitatively, the improvements seem marginal, with no significant improvement shown. I would have liked a better study of the tradeoff between visual quality and diversity, if necessary at all.However, the authors addressed well the issues. Overall, the idea is interesting and simple and, while the paper could be improved with some more work, it would benefit the ICLR readership in its current form, so I would recommend it as a poster -- I am increasing my score to that effect. --------- review after rebuttal----------#1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. #5 fine-tuning with enough epochs. I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that training from scratch is better when the number of epochs is large enough. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates fine-tuning is faster to converge.  However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of scratch B, as ResNet-56 B just reduce 27% FLOPs. The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training.Questions:- Are both methods using the same learning rate schedule between epoch 160 and epoch 320?- The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5,  but it turns out to be 92.68(±0.19) in reply#8.- It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison.#6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. The authors find that when the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning.  This could be due to following reasons:       1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning.       2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios.-------- update ----------------The authors addressed most of my concerns. Some questions are still remaining in my comment Review after rebuttal,  specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version.However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7.    edit: I'm still not convinced about this article novelty, I really like the overall idea but it seems that this kind of contribution is better suited for short paper. UPDATE (after author response):Thank you for updating the paper, the revised version looks better and the reviewers addressed some of my concerns. I increased my score.There's one point that the reviewers didn't clearly address:  "It might be worth evaluating the usefulness of the method on higher-dimensional examples where the analytic forms of q(x|z) and q(z) are known, e.g. plot KL between true and estimated distributions as a function of the number of dimensions." Please consider adding such an experiment.The current experiments show that the method works better on low-dimensional datasets, but the method does not seem to be clearly better on more challenging higher dimensional datasets.  I agree with Reviewer1 that "Perhaps more ambitious applications would really show off the power of the model and make it standout from the existing crowd." Showing that the method outperforms other methods would definitely strengthen the paper.Section 5.4: I meant error bars in the numbers in the text, e.g. 13 +/- 5. **** EDIT *****I have read the response of the authors and appreciate their clarifications and the additional information on the runtimes. See my response below for the concern that remains about the absence of the estimate of the log likelihood for the VAE experiments. Besides this issue, the other comments/answers were satisfactory, and I think this paper is of interest to the research community, so I will stick with my score. ------------------------------------------POST-REBUTTAL COMMENTSThanks for your comments.Re: A2, time augmentation in finite-horizon settings increases the size of the state space you need to keep in memory by at most a factor of 2... But in any case, further discussion of this issue will have to wait until the revision with the additional experiments that you mentioned.Re: A5, regarding "D" being analogous to "gamma" -- fair enough, but this approach is still a meaningful baseline to compare against.Regarding the statement "contrary to this, initializing state values pessimistically requires task-specific knowledge", this isn't accurate. In a MAXPROB instance, initializing the the value function at all states to 0 is a problem-independent pessimistic initialization. Of course, task-specific knowledge may help design a better one, but it isn't _necessary_.In conclusion, as the original review mentioned, I believe that the presented "loop penalty" idea may well have conceptual merit, but I encourage you to think more carefully how you "sell" it, because so far neither the original submission nor the rebuttal present convincing arguments that it is better than the alternatives either theoretically or empirically.--------------------------------------------------- ### After rebuttal phase ###The answers provided by the authors and the revised version of the paper sufficiently address my concerns. As such, I recommend to accept the paper. I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines. Still, I feel that in the current form, the paper can be accepted. ############################feedback to authors' response#############################I'm aware of the non-convex setting is valid, but since the corrected proof of the theorem is not uploaded, I will raise my score to 3.  **Update to review**Thanks to the authors for responding. They did clear up point 5 (above) for me. However, I shall keep my score of 4. Unfortunately I cannot see the new revision of the paper that the authors refer to, meaning I cannot change my score.  update: Thanks for the response. However, there is no updated revision in the revision history of this paper. Based on the flaws that I have previously pointed out, it is impossible for me to validate if my concerns were actually adequately addressed without seeing the updated version. I will keep my score unchanged.  After The Rebuttal: I really appreciate the author response and it is a shame that the revisions do not seem to be correctly uploaded. Unfortunately, the responses to my comments rely heavily on references to the revision that I cannot see, making it impossible for me to validate if my concerns were actually adequately addressed. The other reviewers have mentioned some very valid concerns about the submitted draft as well. As such, I continue to lean towards rejection of the submitted paper as significant revisions are certainly needed.  Post rebuttal comment: Having read the reviews from other reviewers who are subject matter experts, and the authors rebuttal which helped clarify most of my concerns, I am increasing my rating for the paper.  I recommend acceptance as two of the reviewers are convinced about the positive impact of the paper.  **Post rebuttal**With consideration of the improved readability of the new submission and comments of other reviewers, I have modified both my initial rating and confidence. Post rebuttal update: I have read the author response and updated paper, as well as the other reviews, and have decided to maintain my rating. The rebuttal provides valuable information that was missing in the original paper and improves the readability. Therefore, I recommend accepting the paper. ### Post-rebuttal update:I am raising my score from 4 to 5, to recognize extensive updates to the paper experiments and numerous clarifications during the discussion, as well as to acknowledge that some of my initial concerns were not justified, e.g. asking for similar studies for MLPs (which was effectively done in the original submission), or questioning whether the Frobenius norm in Figure 7 really goes down (which was demonstrated better in the latest update).However, I cannot give it a higher score / advocate for acceptance because I still find section 4 (and follow-up discussion) to be more misleading/confusing than helpful when it comes to explaining the observed phenomena.Precisely, even after the discussion, I still believe that1) Experiments in Figure 7 should be done with circular-padded convolutions (to rule out the more trivial explanation of the decreasing norm), and on small subsets of CIFAR10/Imagenet32 (to establish whether low-norm is indeed associated with poor performance on image classification).2) A precise definition of norm for nonlinear networks should be provided, and a link with the Frobenius norm of the linear networks should be established. I don't understand what exactly "the norm in the corresponding RKHS" means, and how the reader was supposed to infer it from the text or our earlier discussion. Further, if possible, this norm should also be evaluated on the actual nonlinear networks, and compared to other notions of norms discussed in prior literature (e.g. https://arxiv.org/pdf/1706.08947.pdf), where lower norm is typically associated with better test accuracy.3) A proper discussion about the non-monotonic dependence on depth in the context of provided intuition should be given. Current intuition can be interpreted as either predicting monotonic decrease in accuracy with depth (learning solutions of lower and lower norms), or as predicting a sharp drop in accuracy at a certain depth (point where the minimum-norm solution can be learned), but neither interpretation explains the hill-shaped dependence, which again makes me question whether this is indeed the right explanation.Without section 4, the paper still has novel empirical results, but in my opinion they are neither surprising (e.g. a hill-shaped dependence of accuracy is my default expectation of any NN hyper-parameter) nor actionable (there are no hints regarding what the peak depends on / how to guess it) enough for publication at this time. A more rigorous investigation into explaining the phenomenon, or a more comprehensive empirical exploration to identify what does and what doesn't influence the best depth, would make this a great paper at a later conference.Best,R4. *****************************After reading carefully the rebuttal from the authors, I raise the score a bit as it somewhat clarifies some confusion. However, the paper still needs improvement, particularly in terms of analytical results.  [Post-rebuttal] I've read the rebuttal, which answers to many points I raised. I've reflected it in my score. [Update post-rebuttal: I thank the authors for addressing some of the concerns raised by the reviewers. My stance remains, also given the outcome of e.g. the 10% experiment -- verification of the main claims remains difficult. My score already reflects that I'd be happy to see the submission accepted.] I have read the feedback and discussed with the authors on my concerns for a few rounds. The revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments.I have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. I have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score "6" reflects my opinion that the approach is not principled.========================================================= After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. I believe this is a strong paper and it represents an interesting contribution for the community.Still things to fix:a) a dataset used in 4.2 is not statedb) missing articles, for example, p.5 ".In practice, however, we need a weaker regularization for A small dataset or A large model"c) upper case at the beginning of a sentence after question: p.8 "Is our Adv-BNN model susceptible to transfer attack? we answer" - "we" -&gt; "We"==================================================================================== --REVISION--I thank the authors for their rebuttal and clarifications on the threat model and end goals of their attacks. I remain somewhat unconvinced by the usefulness of extracting architectural information. For most of the listed attacks (e.g., building substitute models for adversarial examples, or simply for model extraction) it is not clear from prior work that knowledge of the architecture is really necessary, although it is of course always helpful to have this knowledge. As I mentioned in my review, with current (undefended) ML libraries, it should be possible to extract much more information (e.g., layer weights) using cache side channels. Update after author response: thanks for your response; I think the revised paper largely addresses my comments and those of the other reviewers, and I continue to hope it is accepted. Here are two small notes on the related work section of the revised paper:- In distinguishing OCD from DAgger, you note that the optimal policy is computed rather than provided at training time. In fact, structured prediction applications of SEARN (Daume III et al., 2009, which should also be cited) and DAgger often have this flavor too, such as when using them for sequence labeling (where optimal continuations are calculated based on Hamming distance).- Please include a reference to Goldberg and Nivre's (2012) dynamic oracle work. [UPDATE]I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6.------------------------------------------------------------------------------------------------------------------------------------------------------------------------ The revision motivations are much clearer and to-the-point, and the inclusions of Figures 2 and 4 are very helpful for understanding where this method lies w.r.t. VIB.My primary concerns in my original review were the framing of this work as being "finding good representations", but it seems like this is really about finding good representations relevant to some other known variable (such as the labels). Since you appear to be making some broader claims w.r.t. representations, this opens you up to comparisons to methods that are arguably more general in that they operate primarily as unsupervised methods, including the self-supervision and data-augmentation-driven methods I mentioned. You are correct that they do not address MNI as directly as in your work, as each of these could encode information irrelevant for predicting some other known variable. However, as "representation learning" tools, they are far more powerful, as demonstrated in their ability to work on high-dimensional datasets.As a study of how to learn MI models between two known variables, X and Y, this work has a lot of value, but I would make the setting a bit more clearer in the beginning.I do wish that there were more datasets here, as a study with CelebA attributes or CUB captions / attributes would be very convincing.Your concerns about MINE are duly noted, but MINE comes with the strong advantage of not needing to specify the posterior (for instance, a noise-injected nn will work). The additional network needed is just another encoder similar to the one used in your e(z_x | x). In the IB setting, this works precisely as with GANs, except in this case the encoder tries to (adversarially) make the joint distribution resemble the product of marginals. Besides, MINE is demonstrated to work better than your baseline, so shouldn't that be of note?Anyways, as the revision is a bit better, I'll increase my score to a 6, but I need to read more thoroughly the other reviewers' concerns before I move any further.One thought on making this fully unsupervised, and possibly a stronger tool for learning representations: what about sampling X and Y from a random mask (i.e., a crop) and the corresponding negative mask on the image? Enforcing MNI would result in a latent representation which contains the information that is shared between the positive and negative masked areas, which is closer to these self-supervision methods I mentioned. [UPDATE] I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------Revision------------------------I am not changing the score. I disagree with AnonReviewer2 regarding the significance of the results.  The assumption that the states are observed is indeed a weakness of the paper. However, understanding non-linear dynamical systems is extremely challenging and this paper provides strong convergence guarantees. Furthermore, there are several insights in the analysis that may be useful in future work. --------------I would be maintaining the same score. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. ----Updated after author feedback----Upon reading the author feedback, I have downgraded my rating from 7 to 6, because the author feedback is not satisfactory to me in some respects. In my initial review, my comment on the experiment on MNIST is not on correlation between the maximum likelihood estimation and visual quality of generated images, on which the author feedback was based, but regarding the well-known property of the KL divergence due to its asymmetry between the two arguments. Also, regarding the experiment on the Ising model, the proposal in this paper provides an approximate sampler, whereas for example the MCMC provides an exact sampler with exponential slowing down in mixing under multimodal distributions. In statistical physics, one is interested in studying physical properties of the system, such as phase transition, with samples obtained from a sampler. In this regard, important questions are how good the samples are and how efficiently they are generated. As for the quality, it would have been nice if results of evaluated free energy as a function of inverse temperature (that is K_ij in the case here) were provided. The author feedback was, on the other hand, mainly explanation of general variational approach, of which I am aware.I still think that this paper contains interesting contributions, and accordingly have put my rating above the threshold. ***** EDIT ******I thank the authors for their clarifications. They have sufficiently answered my questions/comments, so I will stick with my score. I can see what you're trying to do with Eq (6) after the change but if it's related to the conditional GAN and GAN+class, it's better to cite them and discuss the differences/similarities in the paper. With additional experiments, the evaluation seems to be more complete. However, the results in Table 1 and Table 2 do not show consistent improvements of the proposed models.  For example, if we only look at the human evaluation, it's not clear why phredGAN_a has the highest score among hredGAN, phred, phredGAN_a, phredGAN_d for TV datasets but has the lowest for UDC dataset. It's better if we can see more concrete analysis. I increase my score because of more complete evaluation. However, due to the overall limited novelty/improvements and unclear presentation, the paper is still under my expectation for ICLR. I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.)However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited.As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation.  *********************Update after author response:I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. I think this is an exciting contribution to dually learning component manifolds for demixing. [UPDATE] I appreciate the authors' detailed response and revisions to the paper. I've updated my score accordingly.  Updated Thoughts- The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am inclined towards increasing my rating.- In addition, I have mentioned some updates in the comments which might make the paper stronger -- centered around clarifications regarding the computation of the top-k info-gain term. Updated Thoughts=================I was primarily concerned about a lack of analysis regarding the technical contributions moving from AQM to AQM+. The revisions and author comments here have addressed the specific experiments I've asked for and more generally clarified the contributions made as part of AQM+. I've increased my rating to reflect my increased confidence in this paper. Overall, I think this is a good paper and will be interesting to the community.I also thank the authors for their substantial efforts to revise the paper and address these concerns. Update: I have read author's response (sorry for being super late). The response better indicates and brings out the contributions made in the paper, and in my opinion is a strong application paper. But as before, and in agreement with R1 I still do not see technical novelty in the paper. For an application driven conference, I think this paper will make a great contribution and will have a large impact. I am slightly unsure as to what the impact will be at ICLR. I leave this judgement call to the AC. I won't fight on the paper in either direction. #update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.------------------  Authors have addressed most of my issues and hence I have revised my decision. AFTER RESPONSE:   Wow guys, what a great revision.  Thanks so much. ------Rev. In light of the rebuttal and the following discussions I have updated my rating to 7. After reading the revision: I have raised my score by 1 point and recommend acceptance. After Response:I appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue. After reading the authors' response and their additional experiments, I still see this work as a very decent paper, but not a very exciting one. This is why I rank this paper somewhat above the acceptance threshold. I could be proven wrong if this approach becomes the method of choice to prune networks, but I would need to see a lot more comparisons to be convinced.   After rebuttal:I read the authors comments and I understood the technical contribution more and raised my score.  Implementing/appriximating the response oracle is non-trivial. For MWU, I still think that the above paper should be cited (citing the Adaboost paper is not enough) since the paper shows MWU solves the min-max game. Update: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. However, I wonder if the reasoning for PR2 is limited to "self-play", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.Also, maybe explain self-play mathematically to make the paper self contained? Extended review of Update 17 Nov:I would like to note that I liked the fact that you used several optimization algorithms in your comparison. To my best understanding, several algorithms shown in Figure 3 (e.g., BOBYQA and L-BFGS) would benefit from restarts and it is fairly common to use restarts when the computational budget allows it (it seems to be the case for Figure 3).The results shown in Figure 4 are hard to trust because it does not seem that we observe mean/median results but probably a single run where the results after 1 iteration are drastically different for different algorithms. For instance, after one iteration BOBYQA only tests its second DOE point. Here, again, the issue is that one iteration for BOBYQA is 1 function evaluation while it is several (10) function evaluations for other algorithms. In that scenario, it would be more fair to run BOBYQA with 10 different initializations as well. I don't understand "Due to the restriction of PyBOBYQA API, we can only provide the function evaluation of the final solution obtained by BOBYQA as a flatline in Figure 4". At the point when your objective function (which is not part of PyBOBYQA API) is called, I suppose you can log everything you need. ----------------------------------------------------------The authors have addressed my comments and as a result I changed my rating to 6. * RevisionI took into account the discussion and the newly added experiments and increased the score. The experiments verify the proven effect and make the paper more substantial. Some additional comments about experiments follow.Training loss plots would be more clear in the log scale.Comparison to "SGD BN removed" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017). Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm. Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1? ========================================Revision: I have read the reply from the authors and it clarified several matters. I adjusted my rating of this paper (from 6 to 7). # Post-rebuttal RecommendationThanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I  also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result.Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., "de facto standard in RL"), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work.As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of  just observing  / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience.  This might also require adding a third game to ensure more generalizable experimental insights. [COMMENTS AFTER AUTHORS' RESPONSE] After the rebuttal provided by authors, all raised questions and criticisms have been fully solved. Therefore, I recommend for a full acceptance. ===Update: The authors have made a good effort to address the concerns raised, and I believe the paper should be accepted in its current form. I have increased my rating from 6 to 7, accordingly. ************* Revision *************I am convinced by the rebuttal of the authors, hence have modified my score accordingly. ------ Updating score based on authors' response. --------------------------------------------------------------Post rebuttal: Table 3:the results indicates that only Albert-xxlarge achieve very high performance (222M). however, comparable models to other approaches, such as Roberta-base or albert-xlarge achieved around ~57% performance which is within margin of previous arts. for example, SimpleTOD with gpt2-base (124M) achieved 55.7% and ConvBert achieved 58%.Therefor, it is unclear why Albert-xxlarge get so much higher performance compared to other encoders, since same tokenization and domain-slot relation is used. Based on results in Table 3, there is inconsistency in which domain-slot relation does not always results in better performance,  and it depends on the choice of encoder too. Overall, the proposed architecture is very similar to TRADE model, in terms of using an encoder for dialogue history, slot-gate and slot-value classifier. The only difference is in using a much powerful pretrained encoder.  Update:The revision further improved the paper and addressed most of my comments. I am still positive and voting for accepting this paper. ## Edit after RebuttalI thank the authors for their engagement with my review. Many of my comments and questions have been resolved and, consequently, I have increase my score and **recommend accepting this paper.** After rebuttal:The authors addressed some of my concerns, and the proposed optimization plan is interested.1. I still think that the proposed architecture does not outperform SOTA architecture, like SlowFast.   - First, in the rebuttal, the authors mention that with hand-crafted strategies, the proposed model is 0.4% better than SlowFast; nonetheless, the hand-crafted strategies train longer epochs than SlowFast. (SlowFast is trained with 196 epochs.) As authors propose new strategies, they know better how different strategies could affect.   - Second, in the rebuttal, the authors improve I3D by 1.7% with optimization planning, which means, SlowFast might outperform the proposed model if involving optimization planning. Moreover, in the revised Table 7, the SlowFast does not have more FLOPs than the proposed network under the same backbone. Simply checking Table 8 might be confused. 2. About the optimization planning, it is still confused about how many epochs spent on the explored nodes. E.g. in Figure 4, for the Kinetics dataset, I do not understand why there is a black edge between S4 and S5 as S4 is not reached. (Authors noted that the black edges mean the explored strategies.) And why not there is no number on those explored edges? Another example, if we look at Table 3 and Figure 4 together, and again for the Kinetics dataset, the summation of those red numbers is 248, but this number is larger than any number in Table 3. As the authors mention that they included the epochs of the explored strategies in Table 3, that means they only spend 29 epochs in the exploration (if we treat Figure 4 for DG-P3D in Table 3.). For me, it seems too good to be true.Thus, I keep my rating.  After reviewing the authors' response:The authors have agreed to include missing sparsity level-results and have commented that such results are in line with the trends observed in other experiments.  Furthermore, the authors' response addressed all my questions and concerns, for which I'm raising my score.======================================== == After Rebuttal ==Q1 provides a helpful clarification. Some of the more concrete possible drawbacks listed above, especially those about pruning "false positives," no longer seem as much of a concern.Some of the earlier investigation that the authors report, on more direct methods for finding dead connections, may need to be given a more complete treatment in the paper itself. I am not convinced that the direct methods cannot be done on arbitrary architectures, given that previous literature has managed to in a wide variety of examples. Without considering the simpler techniques, the leap to a more complex method for what could be a relatively simple task doesn't have the necessary support. ##### Post-rebuttalI appreciate the additional results in the rebuttal. I raise the score but it is still slightly below the acceptance. The reasons are 1) incremental novelty; 2) insufficient experiments. Also, I found in table 3 that, the larger-capacity model is less robust than the smaller-capacity model against white-box iterative attacks? This is strange. -------------------Update:Most of the weak points were appropriately addressed by the authors and I have increased my rating accordingly. --------Considering the counter-example given above, I'm lowering my scores a bit.  The proof of theorem 3 is less than clear.  The proof for the first half of theorem 3 (a) is quite obvious, but the proof for the second half is a bit hand-wavy.In the worst case, the second half of theorem 3 (a) will be invalid.  The most general GNN will then have to use an update function in the form of the first half of 3(a), and all the other analysis still holds.  The experiments will need to be rerun.--------Update: the new revision resolved the counter-example issue and I'm mostly happy with it, so my rating was adjusted again. === After Authors response ===The authors fixed some major issues. That is why I improved my grade. However I'm still concerned about the scalability of this algorithm -- bumping down my score because the misleading title was not addressed by the author response.-- bumping it up again because the authors have reacted. After rebuttalThanks for responding to the concerns in the review.The main point in the author's rebuttal is that the major contribution of the paper is a novel QNN that can be efficiently simulated. However, the proposed method of simulation is only an approximation of the real run through CLT. The author provided some conceptual justification of this approximation but there is no numerical verification. The approximation can be inaccurate when the variables in the summation are correlated or the number of variables is small. It's unclear how the accuracy of simulation can be guaranteed.Overall the paper proposes an interesting method of implementing BNN using quantum device, which has potential advantage over classical BNN and may be simulated efficiently with some approximation. More solid justifications of the advantages would improve the quality of the paper. --------------------------------------------- ### UPDATE AFTER THE REBUTTALMany thanks to the authors for their reply, and my sincere apologies for forgetting to include my bibliography in the earlier response (you can find it above)! I also appreciate the information about the comparison to [1], as well as the promise of a clarifying statement about the appropriate experimental context for the use of quantum phase estimation.While I agree that the comparison to previous QNN models is favorable, I still think that the use of quantum phase estimation in the proposed method is problematic, as it would significantly delay any potential deployment of the proposed method on a quantum computer. I don't disagree with the authors that their statement regarding the ability to demonstrate quantum supremacy is technically correct, but they haven't addressed my question about _why_ there would be any advantage to carrying out a quantum supremacy experiment on such a model. Studying QNNs for the sake of QNNs isn't a compelling justification in my eyes, and without any strong practical advantages (either definite or potential) provided for the model, my rating unfortunately remains the same. ===== after updates =====Thanks for the edits  I believe the overall paper is more clearly presented, now.I still think it is a stretch to consider the calculator domain is a program induction problem: it is a regression problem, from an input string to an output integer, or alternately a classification problem, since it computes the result mod 10. The only way I could understand this as a program induction problem is rather obliquely, if the meaning is that any system which is able to compute the result of the calculator evaluation has implicitly replicated internally, in some capacity, the sequence of instructions which are evaluated. I don't think this is really very clear though; for example, given two calculator programs, one a subprogram of another (e.g., "4*(3+2)" and "3+2"), do the resulting "induced" computations share the same compositional structure? The examples of program induction in section 2 are largely architectures which are explicitly designed to have properties which mimic conventional programming languages (e.g. extra data structures as memories, compositionality, &). In contrast, the calculator example in this paper simply uses an LSTM. That said, I think it's still a great example! Learning a fast differentiable model which accurately mimics existing non-differentiable model has tons of applications, and has exactly the same challenges regarding synthetic data. I have to say I find the new section 8.3 a bit intuitively challenging; e.g. it's not clear really how long a waiting time of 48 log(2|X|/\delta) / (p|X|^2 z^2) really is. But, to that end, I appreciate the empirical discussion in 8.48.6.I've updated my review to increase my score  I lean towards accepting this paper, as it is a timely contribution and I think it is important for future program synthesis papers to take the results here to heart. I've reduced my confidence slightly, as I have not fully reviewed the new proof in 8.3. After revision ====The authors have done a great job addressing the concerns I had about the clarity. Consequently, I have raised my score, whereas my fairly low confidence still remains.  ####### . After rebuttalThe author makes more clear indication of the performance contribution of the completeness of recovery. As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.* Different motivation/derivation of NES.* Concept of adversarial distributions.* Regression network for good initialization.* Introduction of accuracy-iterations plots. Update After rebuttal:Given the authors' rebuttal to all reviews, I am upgrading my score to a 6. I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach. ###In light of the revision, I have revised my score given the rewriting of section 3 that addresses the second con I raised above. However, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, I feel that the paper still require improvement before it can be accepted. Comments after rebuttal:The  paper has clearly improved. It leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. When dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 Nevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points! In response to the author's comments, I have increased my score.The practical implications of this theoretical work are unclear. It's nice that it relates to DQN, but it does not provide additional insight into how to improve existing approaches. The authors could significantly strengthen the paper by expanding in this area. After reading the rebuttals, I appreciate the authors addressed most of the my concerns above. Thus I have adjusted my scores to reflect that. I think the work contains some theoretical contributions as opposed to [1], which is used to prove convergence for many actor-critic algorithms, under certain regular assumptions. It would be great to see this result working on more general conditions. Please address the above concerns in the final version, and also stress the theoretical contributions as compared to existing results.  Revision:I thank the authors for their detailed feedback, but still think the work isn't quite ready for publication. After reading the other reviews, I will decrease my score from 6 to 5. Some sticking points/suggestions:- Some of my concerns remain unanswered. E.g. the actor-critic method 3.8 is driven by the Bellman residual, which is not the same as e.g. the MSPBE used with linear function approximation. There is no harm in proposing variations on existing algorithms, and I'm not sure why the authors are reluctant to do. Also, Brouwer's fixed point theorem, unlike Banach's, does not require a contractive mapping.- The paper over-claims in a number of places. I highly recommend that the authors make their results more concrete by demonstrating the implications of their method on e.g. linear function approximation. This will also help contrast with Dai et al., etc. ---Update: I think the experiments are interesting and worthy of publication, but the exposition could be significantly improved. For example:- Not sure if Figure 1 is needed given the context.- Ablation study over the proposed method without sparse reward and hyperarameter \alpha- Move section 7.3 into the main text and maybe cut some in the introduction- More detailed comparison with closely related work (second to last paragraph in related work section), and maybe reduce exposition on behavior cloning.I like the work, but I would keep the score as is.--- Edit: Some of my suggestions were incorporated in the rebuttal, but my sentiment is still that this is almost at the acceptance threshold. The large focus on biology makes much of this paper harder to evaluate or appreciate. NEW:The authors have addressed the concerns I had with the manuscript. **********I would like to thank authors for their feedback. After reading their feedback I still believe that novelty is incremental and would like to keep my score. =============== after rebuttal ===================I appreciate the authors' feedback and slightly raise the score. Though the compression results look good, I still have some concerns about the method. The motivation of the proposed method is not strong. The proposed mask is greedy and sounds ad-hoc. The proposed progressive pruning looks expensive. The proposed method looks time consuming. For the experiments, I would love to see the training time comparing with baselines in table 1, not only the ADMM method in table 2. A fair comparison could be wall-clock time, or number of gradient updates for neural networks. Revision: Although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:- Missing experimental evidence for the efficiency of the NN search algorithm.- Experiments are using Parzen window for estimating likelihood which  are known to be unreliable in high dimensions.  - None of the suggested experiments were considered. In my opinion these experiments could improve the quality of this work. - Moreover, as mentioned by reviewer 1, Grover et al., 2017 provides evidence contrary to what the authors claim but this was never addressed so far in the paper.- Theorem 1 makes rather strong assumptions: as pointed out by reviewer 1, assumption 3 is unlikely to hold for the distributions used in practiceFor these reasons I recommend a clear reject.  --REVISION--The paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of "preventing" or "avoiding" posterior collapse is too strong, as I agree with the authors that "it is unknown whether there is a better local optimum that [activates] more or all latent units". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like "reducing" or "mitigate" instead. Response to Authors-------------I've read all other reviews and the author responses. Most responses to my issues seem to be "we will run more experiments", so my review scores haven't changed. I'm glad the authors are planning many revised experiments, and I understand that these take time. It's too bad revised results won't be available before the review revision deadline (tomorrow 11/26). I guess I'm willing to take the author's promises to update in good faith. Thus, I think this is an "accept", but only if the authors really do follow through on promises to add uncertainty quantification and include some complete comparisons to KL annealing strategies.  --REVISION--I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR. Update: Lower the confidence and score after reading other comments. ---Thank you for responding to my comments and updating the paper. I have slightly raised my score to reflect this effort.There are new claims in the results section that do not seem to be warranted given the human evaluation. The claim is that the human evaluation results validate the use of the automatic metrics. The new human evaluation results show that the proposed abstractive model performs on par with the extractive model in terms of conveying the overall sentiment and information (Table 2), whereas it substantially outperforms the extractive model on the automatic measures (Table 1). This seems to be evidence that the automatic measures do not correlate with human judgments, and should not be used as evaluation measures.I am also glad that the title was changed to reflect the scope of the experiments. I would now suggest comparing against previous work in opinion summarization which do not assume gold-standard summaries for training. Here are two representative papers:Ganesan et al. Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions. COLING 2010.Carenini et al. Multi-Document Summarization of Evaluative Text. Computational Intellgience 2012.  ===Update: I am happy with the clarifications and the changes to the manuscript, and have increased my rating accordingly from 6 to 7.  I incline to my current score after reading the response and other reviews. I have read the discussion from the authors. my evaluation stays the same.-------- *** Updated ***After reading the updated paper, responses, other reviews, and looking at related works more closely, I have changed my score to a 5. This is due to several factors. Although the paper's core idea is definitely interesting, the fact that they use hardcoded features, rather the standard setup which uses pixels, makes comparison to other methods much more complicated. In particular, I think that the comparison to DQN-PixelCNN is unfair, as this other method makes very few assumptions about the inputs (only that they are pixels). The authors sort of point this out in the main text, but this is somewhat misleading. They say "PixelCNN uses less prior knowledge than our approach". In fact, it uses as much prior knowledge as any RL method which operates on pixels. Granted, this is nonzero, but it's vastly less than what this paper's method assumes. The other comparison is to SOORL (which uses a different state encoding altogether). The comparison to SmartHash is fairer, although the variant of SmartHash they compare against is not the main method the paper proposes (a generic autoencoder-based state encoding which makes minimal assumptions about the input). It would have been better if the authors included experiments for their method using such a learned state encoding.Reporting SOTA results on very hard tasks using extra hardcoded features or other domain knowledge is potentially misleading to the community as to how far along we are in solving these tasks, and extra care should be taken to put these results in context. Otherwise, for those not familiar with the subtleties, this makes it seem like these tasks are being solved when in fact they are not. My concern is that other works may then be asked to be compared against these artificially high results. Having many different task setups also makes comparison between different published works confusing in general. Other works (such as Ostrovski et al) have been able to make progress on these tasks while staying within the standard pixel-based framework.These concerns would have been partially mitigated had the authors made it *very* clear that they were assuming substantial prior knowledge, which makes their method non-comparable to others which do not make this assumption. This could have been done in the introduction (which was one of my comments, but this was not included in the updated draft). I.e., something to the effect of "We emphasize that our approach assumes substantially more prior knowledge than other approaches which operate only on pixels, and as such is not directly comparable with these approaches". In addition, I would have liked if the authors had followed the suggestion of Reviewer 1 to include results in pixel space, even if negative, but this was not done either (using a simple autoencoder-based representation, like the one in the SmartHash paper, would have also been fine). As it is, statements such as "Our approach achieves more than 2x the reward of prior non-demonstration SOTA approaches" and "our approach relies on some prior knowledge in the state abstraction function, although we compare against SOTA methods using a similar amount of prior knowledge in our experiments" are quite misleading and unfair to other methods which do not assume access to prior knowledge (the second statement is untrue for the case of DQN-PixelCNN). Another point which I had not noticed previously is the very high sample complexity (2 billion). One of the motivations behind model-based approaches is that they are supposed to be more sample efficient, but that does not seem to be the case here.  ================================I've read all other reviewers' comments and the response from authors, and decreased the score. Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm. I agree to other reviewers. The algorithm assumptions are strong. ===================After feedback:Thanks for authors' feedback. Some of my concerns have been addressed. But the novelty is still not significant. On the other hand, the dataset used in this paper is simple. Specifically, at least the first 3 datasets are single-label and the number of classes is not large. They are too simple to support the claim. It's better to use multi-label datasets to show that the proposed method can really capture the correlation of labels. Comments After rebuttal==========Thank you for adress my concerns.The response and the revision resolved my concern (1). However, the most important part, the possibly problematic loss is not resolved. It is true that sometimes (10) can achieve good results with good regularizers or a good set of hyperparameters. However, theoretically, the loss is ]only pushed down the desired answer, which may make the training procedure quite unstable. Thus I still think that a different loss should be used here. The authors have done a good job in the revision and have clarified points that were unclear in the first version. I have remaining reservations on significance, but move rating up a notch to reflect the extensive improvements and  the authors' confirmation that they will release the data.  * RevisionThanks for your response,  the paper new  version address my main concerns, I appreciate the new experiment looking at the eigenvalues of the  end-to-end Jacobian which clearly shows the advantage of the AntisymmetricRNN. [Second Update] I still find the method proposed in this paper appealing, and think that it may have practical applications in addition to providing significant research contributions. A key question that was raised by the other two reviewers was whether the proposed approach was fairly evaluated against existing state-of-the-art solvers. The authors have responded to these concerns by adding clarifications and new baselines to their paper. However, based on the discussions to date, I feel that I am not sufficiently familiar with related work on SAT solvers to say whether the other reviewers' concerns have been fully addressed. If they have been, I'd strongly lean towards accepting the paper. As for the concerns from my original review: the transferability experiments reported in the author comments below are quite informative, and I'd encourage the authors to incorporate them into the paper (or an appendix if space is an issue). I'd also encourage the author to incorporate the full comparisons against Z3, PicoSAT, MiniSAT, Glucose, Dimetheus, and CaDiCaL from Section 5.1. (I've updated my rating for the paper from 5 to 6, and my confidence score from 3 to 2.)[First Update] Based on the feedback of the other two reviewers, I believe that I was missing some important context about SAT solvers when I wrote my initial review. Reviewer 1 and Reviewer 2 both raised serious concerns about the types of SAT instances that were used to evaluate the experimental setup, as well as about the use of Z3 as a baseline for solving random SAT instances. (No author response was provided.) Given this additional information, I've lowered my score for the paper from an 8 to a 5. I do think that the approach is interesting, but have reservations about the experimental evaluation and the claims made by the current submission. (Note: As the paper authors point out in the comment below, this update was mistakenly submitted a few days before the end of the rebuttal period.) ------------UPDATE: Score changed based on author resposne------------  ---------------------------------------------Thanks for the update. But are they fair comparisons (evaluation only in terms of accuracy)? Different methods expand the network different amount. Hence, they should be compared on this metric too. ==========================================================================REVISIONThe authors' additional results and responses have addressed most of my concerns, and I've raised my rating from 6 to 7.&gt; We remark that the identity mapping loss L_idt is already used by the authors of the original CycleGAN (see Figure 9 of [2]). Thanks, you're right, I didn't know this was part of the original CycleGAN. As a final suggestion, it would be good to mention in your method section that this loss component is used in the original CycleGAN for less knowledgeable readers (like me) as it's somewhat hard to find in the original paper (only used in some of their experiments and not mentioned as part of the "main objective"). Post rebuttal: I am satisfied by the points mentioned by authors!---------------------------------------------------------------- **After Rebuttal** I would like to thank the authors for their rebuttal. I agree that it is not fair to assess the merits of the current work based on papers that were not available at the time of submission (or that, strictly speaking, have not been published at the time of submission). Indeed, to an extent, pointing out the ArXiv paper encourages authors to simply submit their works there to get a "publication" stamp, which on a community level is undesirable (papers on ArXiv aren't reviewed and citing them as scientific sources is problematic to say the least). I suppose the only point is that there exist works that do similar things in a more compelling fashion. It's encouraging to see that the authors checked for robustness of their method, and I appreciate the efforts. With these two issues resolved to a certain extent, I am willing to increase my score.  **Update after author response:** I appreciate the authors' efforts to address my concerns. Thanks for the correction on QBC, I appreciate it. I still believe that the paper needs to accompany a more comprehensive evaluation and qualitative insights to highlight the effectiveness of the proposed method. For instance, it is common practice in Active Learning to report mean accuracy over multiple runs of the same experiment as data is sampled based on a particular heuristic which isn't always deterministic. Furthermore, the choice of warmstart samples could also influence the results, which is why it is recommended to conduct multiple runs of the same experiments and report mean performance. In such a scenario, any claims that arise from only one run of the experiments (as in this paper) should be taken with a grain of salt. I also appreciate the pointer to Equation 4 but how does it translate in practice is another important piece that is missing. BERT based models produce highly confident predictions and to visualize the distribution from your empirical investigation would help bridge the divide between the equations and the empirical results (how they actually turn out in practice). I am also not convinced by the authors' response to why the difference in performance of the BERT model trained on randomly sampled data vs the model trained on data sampled via ALBUS stays within the 1-2% range, often increasing with more data. I believe there are critical questions regarding this paper that need to be addressed before the paper is published, hence, my score remains unchanged.--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ################################################################Summary:This paper provided an efficient algorithm (LLBoost) to boost the validation accuracy without spending too much time tuning hyperparameter. The algorithm is theoretically and empirically guaranteed.################################################################Reason for Score:This paper provides an innovative way to improve generalization performance. My major concern is about experiment part. Since the algorithm use valid data to tune the parameter, it should another held-out test data to show the result. However, the author only did experiment on test-data for model ResNet-18, which is not sufficient to support the paper.################################################################pros:1, This paper gave an efficient LLBoost algorithm to quickly improve the validation accuracy. The algorithm is theoretically guaranteed.2, The paper clearly stated the intuition of the algorithm. The paper considered models that have fc layer as the last layer (most of the current models have this property), and transformed the problem into a linear regression problem.3, A surprising point of the algorithm is that it does not impact the training loss.################################################################cons:1, While the algorithm has a theoretically guarantee, the experiment part did not convince me. This is the major concern for the paper. The author tune the parameter using valid data and say valid accuracy is improved, which is not enough. It should another held-out test data to show the result for all the experiment. However, the author did an experiment on test-data only for ResNet-18, which is not sufficient to support the paper. Also, The author should put this test-data-ResNet-18 experiment in main part of the paper, not Appendix.2, Section 3 (preliminaries and method) is not well-organized. I cannot see why the author put this two lemmas here.(1) why "Lemma 1 implies that LLBoost does not affect training predictions since it only ever adds a component orthogonal to the span of the training feature matrix"? The lemma 1 seems having nothing to do with the LLBoost algorithm(2) what is the purpose for lemma 2? The paper doesn't clearly state it.I understand the reason after the author explained it in response. But I strongly suggest the author to explain it in paper for the final version.3, Based on my understanding of this paper, the algorithm has to be applied to an existing pretrained model which is sufficient good. If we don't have a good pretrained model, does this algorithm provide a better (or comparable) result than the well-tuned model? I am just curious about it and hope the author to do some experiments in the future. -- Post-rebuttal -----------------------------------------------------------------------------------------------Given the improvement of the last revision, I increase my rating to 6. The revised version has been very much improved, especially in the abstract and introduction Sections. Still I think it's important to additionally have one or two sentences to make very clear on the meaning of calibration, as to not confuse readers. -----------------------------------------------------------------------------------------------------------------------------------------------------------------POST REBUTTAL-----------------------------------------------------------------------------------------------------------------------------------------------------------------The rebuttal has addressed most of my concerns and I am happy to increase the score.----------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------Post rebuttalI thank the authors for providing detailed answers to my concerns. Considering the concerns of the other reviewers and the authors' answers and additional experiments, I think that the paper provides a sufficient contribution to an important research topic. Therefore, I retain my initial rating. Additional reviews ====================== The authors have resolved some concerns, especially the explanation/justification of the experimental results in section 5. As I have commented in the initial review, this paper provides suggestive experimental results (the effects of training anomaly dataset) for future anomaly detection research. Therefore, I have decided to raise my rating from 5 to 6. **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the clarifications and improvements made during the rebuttal phase, which I think have further strengthened this work.I find the key contributions of this work to be (i) demonstrating that recent methods that include labeled anomalies into training can suffer from unfavorable biases, and (ii) providing a framework for a theoretical analysis of this setting.Though I see that the presented results are somewhat what one would expect, to my knowledge such an analysis hasn't been carried out in the existing literature.Since weak forms of supervision (here few labeled anomalies) appears to be a promising research direction for anomaly detection, I find this critical and rigorous analysis to be worth circulating the community.For these reasons, I would keep my recommendation to accept this work (score: 7)##### --- Thank the authors for the detailed response. This paper investigates an interesting issue. However, the overall technical contribution and novelty of this work remains to be a concern. After reading the response and the comments of peer reviewers, the rating is maintained as follows.  Update: I have read the author's response and decided to keep my review, confidence, and score.--- ######################After reading the author's response: While i still think that extending the experimental evaluation along the axes described above would improve this work, i decided to raise my score.  **Post-rebuttal**The rebuttal addresses the raised issues. An experiment supports the generalizability of the proposed metric. A new image-to-image translation experiment on COCO-Stuff is added. It shows the proposed method's potential for other translation tasks (the results are in Fig. 16, not Fig. 15). Taking the rebuttal and other reviews into account, I would like to recommend accepting the paper for its excellent results and simple yet effective idea.  Update: Thanks for the response from the authors. The comments 3/4/5 from authors convincingly address my concerns. Regarding other classifier-based metrics, note that not all of them requires separate training and testing procedure. For example, the leave-one-out 1-NN accuracy [1] does not. Also, I'm still not sure about the technical novelty. Therefore, I keep my original rating.[1] Xu, Qiantong, et al. "An empirical study on evaluation metrics of generative adversarial networks." arXiv preprint arXiv:1806.07755 (2018).--------- ***UPDATE: The authors have addressed some of my comments. I appreciate their efforts for making the paper clearer. That being said, I would still keep my original score, because my major concern (evaluation) has not been addressed. Also, the paper would be much better if its writing and organization can be improved. ---UPDATE: Thanks to the authors for responding to my questions and updating the submission. I will keep my score as is, since I think that the paper would greatly benefit from more practical/rigorous empirical evaluations to demonstrate the usefulness of the approach.  ## Post-rebuttal commentsThanks to the authors for the comments.  I will leave my review unchanged for this one.> The term suspicious: we agree with the reviewer that the word suspicious is an informal and subjective term, and there is no universal notion of what it means for adversarial examples to be unsuspicious. In fact, the core contribution of our paper is to address this issue.To be clear, what I mean is that you should choose a *different* term than "suspicious".  While it may not sound as good to use a more specific technical term describing your actual measure that you are introducing, it will prevent the inevitable confusion that arises when an informal, vague term is overloaded to have a specific, quantitative definition. Update======I have read the author response, which largely does not change my score. I would kindly point out to the authors that the "the technical challenge is that the feasible region is not an ball, and computing the projection is challenging in high-dimensional settings" is in fact not very challenging at all. As mentioned in my initial review, the "split and merge" is exactly the standard projection operator on the proposed set and is not a "new heuristic", and so it is exactly PGD and not an adaptation of PGD. I also understand the other reviewers concerns on suspiciousness, which we all brought up. This likely needs to be thought about and presented more carefully, for example by posing it more formally if the authors insist on this framing.  # Post-Rebuttal CommentsI'm afraid I'm maintaining my score as a 3. The authors' response did not do enough to address my concerns:### Potential for Attacks on Saliency ModelsUnless I'm misunderstanding, the perturbed images presented in Fig. 1 are the results of adding perturbations intended to cause a change in the behavior of the **classifier**, not the **saliency model**.### Evaluation of attack strength on robust classifiersI still don't understand why the success of the dual-perturbation attack against models that were not trained to be robust to such attacks is of interest.### Suspiciousness of adversarial examplesThe images identified in Appendix L are visually suspicious due to the sharp boundaries between the "foregorund" and "background", not because of a noticeable change to the background. The fact that these images are considered unsuspicious by the metric defined in the paper suggests that the metric needs to be modified. ### UpdateI have read the author's rebuttal and the comments of the other reviewers. Sadly, the comments of the other reviewers seem to align with my own thoughts and the rebuttal doesn't resolve those questions. I would like to maintain my initial rating of 4. More generally using the same model to measure saliency as well as to construct adversarial attacks is not properly justified and secondly, I am not convinced that putting a large perturbation on the "background" as determined as DeepGaze  is a proper side-by-side comparison with AT where AT is allowed  much smaller perturbation. UPDATE:After reading the author's updated paper and comments I have decided to improve my rating to a 6 since some of my concerns have been eased. The most important concern being eased was the type of bias was too constrained at first and now there are experiments with a more unconstrained version of bias that is more convincing.Overall, I would say that future versions of the paper could look into a task and dataset that are close to their domain of applicability and where they can contribute an increase in performance. That would strengthen the case of this paper. I think a rating of 6 is fair for this version of the paper and I thank the authors for their efforts in updating the work and addressing my concerns directly and efficiently. ---### Updates:I thank the authors for their response. Some of my concerns are addressed. However, unfortunately, I still think the assumption of the proposed approach is too strong to have broad applications. I will keep my original score. [original score: 3 (clear rejection)]11/24: Updated score based on updates from the authors. The addition of FLOP counts and more baselines in the experiments section greatly improved the paper. The proposed approach appears to achieve excellent FLOP-accuracy tradeoffs relative to existing approaches.[2nd score: 6 (marginal acceptance)]11/30: Updated score based on updates from the authors. The discrepancy with some baseline numbers has been resolved and the authors added clarifying information to the paper regarding the counting of FLOPs. ============= Post-rebuttal:I thank the authors for their careful responses. After discussing with other reviewers:- I agree that the sparsification method proposed here is also in principle applicable to GCN-like models;- The authors should have provided results for "FastGAT"-style sparsification on GCN, rather than countering the reviewers using passages like "Hence, it is mainly the question of necessity rather than applicability which guided our choice of studying the GAT model in depth."- If such a GCN model ends up competitive, the focus of the paper could switch to the sparsification method itself rather than the GNN model it is applied to.In light of these discussions, I am decreasing my score to a weak accept, and I hope the authors will take this advice for the next iteration of their work (which otherwise, in my opinion, deserves being published in a strong venue). === Post-discussion update ===I thank the authors for engaging in the discussion.I am left somewhat divided on the paper. During the discussion phase, the statement of the main algorithm in the paper changed quite significantly: in the original version each node could attend only to its neighbors in the sparsifier, while in the current version each node attends to all its neighbors (i.e., full graph attention is computed), and the sparsifier is only used in the subsequent feature update. The authors eventually explained that their analysis holds for the latter algorithm (even though its running time is not faster than non-sparsified GAT), while the former algorithm is what they actually implement since it has better running time (albeit no formal guarantees).I don't take issue with the divide between the theory and the implementation (as long as it is made clear in the paper). I do think, however, that perhaps the theoretical content ended up doing the paper more harm than good. Effective resistances measure the "importance" of edges to the connectivity of the graph - this is a general phenomenon, and the sparsification algorithm of Spielman-Srivastava is just one (beautiful and useful) manifestation of it. I can see why the practical algorithm would work even if it cannot be explained formally via spectral sparsification, and including the slow unimplemented algorithm just for the sake of its analysis feels a bit forced. What does it add to our understanding, and was it worth making the paper that much more confusing? Ultimately it's the author's choice, but even now the way the writeup deals with the two algorithms is still "evolving", and it is not clear how a final version would look. (I don't think the current form makes sense, since the algorithm now titled "FastGAT" is not faster than GAT, and section 3.1 zigzags between the two algorithms somewhat awkwardly.)Nevertheless, in the end the authors were straightforward about all this in the discussion. As I said originally, I like the overall approach, so as long as the clarifications about the gap between the analysis and the implementation are included, and pending other reviewers' concerns about novelty and experimental validation (I am less versed in the empirical literature on GNNs so prefer to defer to them on those points), I think the paper could still be accepted. ---post-discussion update---I greatly appreciate the authors' effort to actively attend the discussion. In general, I like the idea to leverage the graph sparsification technique to accelerate GNN training. However, I think there are still several fundamental questions that should be well addressed before this paper get accepted.  First, I do not see clear reasons to emphasize a specific model GAT. One concern arises during the discussion, where the sparsification technique here only works for GAT instead of GCN or other GNN smoothing models, which brings me some concerns about the technique. Second, the fundamental difference between this method and GDC [1] is still not clear. Both methods emphasize the low-frequent section of the graph connection. Why does the sparsification method here work while GDC does not work, though the sparsification method here can also be viewed as a type of graph diffusion (GDC)? ## Post rebuttalThanks for providing detailed explanations.Overall, I think the authors tried hard to prove the concept of edge sparsification helps for speedup of attention GNNs which I also think they explanations/rebuttals succeeded in doing this, though the established theory seems quite standard and is not the same as in the experiments.However, pertaining the results I still do not see a claimed comparison of GCN, FastGAT-sparsified GCN in the revised work based on the replies to R3 (only FastGCN is reported. Note this is not a direct adaption of the authors method, but from the previous node-sampling literature). As is claimed by the authors, performing sparsification on GCN does not provide seminal speed boost. However, while GCN is a strong/simple baseline without heavy parameter tuning, its hard to make a clear justification on why a sparsified heavy-attention-computation network (FastGAT) would do any good if its final results are barely comparable/similar to a naive GCN baseline with a similar running time.Given the idea of sparsification is not novel which I stand on a similar point with R3 and specifically with its linkage to GDC. I think the current paper may need to be improved with some more evidence on proving the edge sparsification method is superior to other node sparsification versions of attention networks e.g. empirically/theoretically better (FastGCN-sparsified, ClusterGCN-sparsified models) on a complete set of datasets; or with a more rigorous comparison to other simple GCN-versioned sparse methods that do not need the heavy attention computation in the first place.For the current status, I would still lean towards a rejection. EDIT: Lowered score by one point after reviewer discussions. The evaluation could certainly be better and the difference to earlier mask-based methods more clear. **AFTER REBUTTAL**After reading the other review's and the author responses, my opinion is unchanged: This is a well-written paper with a simple and effective method, and a clear accept. After rebuttal, the technical novelty is still not convincing. The masked encoder is widely used in image inpainting works. Combining multiple existing techniques may work better than existing methods, but it has little impact on the community. Thus, I do not change my rating. Post Rebuttal Update:I have read the author's rebuttal, and appreciate the additional experiments. I will maintain my score due to the importance of extensive evaluation for showing empirical robustness, and a lack of proper baselines. In particular, I think that some version of "adversarial training with coarse labels" should be better than standard adversarial training in terms of hierarchical robustness; only after that can we evaluate the benefits of HAR in comparison to "adversarial training with coarse labels." ### Post-Rebuttal Update ###I thank the authors for their response and edits to the paper. In particular, the authors have tried to address comments from the reviewers pertaining to further empirical evaluation of their approach. However, I still have two concerns about the paper post-rebuttal:[Baselines] I share Reviewer 4's concern that the authors do not compare to several natural baselines. As I mentioned in my review, I am still not convinced about the design choice of using multiple networks for each set of coarse/fine-grained labels. There are a host of approaches (both training losses and architectures) to deal with hierarchical classification in the standard setting. The authors do not justify quantitatively why adapting these methods to the robust setting (i.e., incorporating ADV/TRADES there) will not work. Further, I do not find the justification of fine-grained labels within a dataset being uneven for using multiple networks to be sufficient . This seems like a more fundamental problem that should be fixed by changing the dataset/class hierarchy. In fact, having even coarse/fine classes is essential to justify the merits of HAR in the first place.[Evaluation] Based on the new results added to the paper in the rebuttal phase, it seems that HAR does worse than vanilla ADV on coarse accuracy under untargeted attacks (Table 1); while vanilla ADV  does worse than HAR using worst-case attacks (Table 2). If we compare the minimum over the two attacks (as is standard to correctly measure robustness), the coarse robustness is actually *better for vanilla ADV than HAR* (24.60% vs 20.71%). This finding seems to go against the main claim of the paper.[Other] There seems to be a discrepancy in Table 1---the robust accuracies go up between the PGD20 eval and PGD50 eval.Based on the concerns listed above, I will maintain my original score. ======== after discussion phase ==========I still think that the merit of the method is unclear due to reasons: 1) It is not clear how the method behaves without Lipschitz Hessian assumption. 2) The method only obtains the state-of-the-art complexity of $\epsilon^{-3.5}$ with large mini-batch sizes and the complexity with small mini-batch sizes (section 2.1) is suboptimal (in fact drawbacks such as this needs to be presented explicitly, right now I do not see enough discussions about this.). 3) Adaptive variance reduction property claimed by the authors boils down to picking "small enough" $\beta$ parameter, which in my opinion takes away the adaptivity claim and is for example not the case in adaptive methods such as AdaGrad. 4) The comparison with AdaGrad and Adam with scalar step sizes is not included (the authors promised to include it later, but I cannot make a decision about these without seeing results) and I am not sure if Adam+ will bring benefits over them. 5) Presentation of the paper needs major improvements. I recommend making the remarks after Lemma 1 and theorems clearer, by writing down exact expressions and the implications of these (for example remarks such as "As the algorithm converges with $\mathbb{E}[\|\nabla F(w_)\|^2]$ and $\beta$ decreases to zero, the variance of $z_t$ will also decrease" can be made more rigorous and clearer, by writing down exactly the bound for the variance of $z_t$ by iterating the recursion written with $\mathbb{E}\delta_{t+1}$ and highlighting what each term does in the bound. This way will be much easier for readers to understand your paper).Therefore, I am keeping my score. After rebuttalThanks for the feedback. However, I am not persuaded by the answers of Q1 and Q2 and would keep the score unchanged.  This paper studies ways of adding edges to graphs to improve the result of spectral embedding / clustering. It refines existing embeddings using by measuring edges' effect on Laplacian eigenvalues, and adjusting such edges to reduce the distortions. The performance of the algorithm is justified using developments of worst-case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy.Strengths:+ the paper puts together a lot of different ideas, many of which have solid theoretical foundations.+ full experimental evaluation on a moderate sized data set that demonstrates both good results and good performance.Weaknesses- the presentation could use significant improvement- the ideas are a bit disconnected, at least when one try to follow the ideas mathematically.I find the approach taken by this paper quite interesting: node embeddings are now widely used to preprocess graphs into vector data more friendly to learning pipelines. Many of these methods add additional edges imperatively / via local methods, without taking the overall data set into account. Iterating this process based on the overall embedding is an interesting, but algorithmically more intensive approach. To carry this out, the authors combined a variety of interesting tools, as well as some high performance packages that they developed. I feel the novelty of this combination, as well as the gains obtained, should make this result of broad interest to those working on spectral clustering/embeddings. However, many of the other reviews point out gaps in both the ideas and presentation, and I'm inclined to agree with them that this paper can benefit from a thorough revision before appearing at a major conference. Update: In the revised version, the authors have addressed some of my technical concerns, therefore I slightly increased my score. #########################################################Final Recommendation:I have considered the authors' responses to my comments, as well as the assessments given by other reviewers. I still feel as though, while the method looks as though it may offer a potentially useful practical alternative to other graph learning methods, in its current form I do not think it is presented in a manner which warrants acceptance at a prestigious conference such as ICLR. If the decision overall is that the paper is not to be accepted, then I wish the authors well with their work and hope they take into consideration the comments of the reviewers as I do believe the work has potential. --- Update after reading the rebuttal and other reviewsThough some concerns have been addressed, a critical issue remains unresolved, which is that the experiments use only 3 runs. As an additional point, it is useful that you've identified MAPPO as a good multi-agent algorithm. However, for clarity, it is better to focus the paper around MAPPO rather than strictly calling it a benchmarking paper. Further, it would be good to clearly state in the paper that the results now include reward normalization.  I have read over the rebuttal and discussion and will keep my evaluation score unchanged as I see value in benchmarking papers such as this for the community. ####### Post rebuttal #########I thank the authors for their well formed responses. They address all my concerns effectively. I keep my original rating and recommendations.  ==== Updates after the response ====I thank the authors for answering my questions and the updated manuscript. Im keeping my score and recommendation. ---------------------After the rebuttal------The authors partially addressed my concerns. I remain the current score. #####################update:  I have read authors' response to my comments and also read other reviewers' comments and discussions.  The main concern of my comments is still not clear. I will keep my rating unchanged.  Update during rebuttal: I have read other reviews and authors' answers to them. My main concern about the approach to be potentially overcomplicated has been address and I was convinced. I am raising the score for the paper. /============================================================================================================  Post rebuttal Comments:Thank the authors for the response. I keep my score as 5. ##########################################################################Comments after rebuttal period: I appreciate the authors' efforts on increasing the clarity of this paper. However, the answers to "How do you order those m clusters?" are not convincing (as well as the responses to R4's 2nd concern, which mentions the same problem). It is a key point in generalization across different graphs. According to the experimental results, I believe there is an implicit mechanism in the proposed method that achieves the ordering, but it is not clearly explained and analyzed. I will keep my score. Though I like the paper, and I believe it has good results, I am now seriously concerned about the quality of the numbers in Tables 1 and 2. The origin of many of the results in these tables is in doubt. The authors say that the results for TransE, RotatE, ConvE, ComplEx , DistMult, and Minerva are taken from the corresponding papers. But when I look at these papers, I see different (or no) numbers.The values for Minerva in the current paper match for FB15k-237, but do not match for WN18RR. The values in the Minerva paper for UMLS or Kinship are way better than in the current paper. Now Minerva uses a different evaluation protocol, but I seriously doubt the Minerva numbers for UMLS and Kinship.RotatE has no numbers for UMLS or Kinship.ComplEx has none of the numbers reported here.Distmult has none of the numbers reported here.The numbers in the current paper for TransE, DistMult, RotatE, ConvE and ComplEx seem to be taken from the RotatE paper. But RotatE has no numbers for UMLS or Kinship.So overall, the provenance of the numbers in Table 1 and 2 is in serious doubt. I realize that the authors do not have a chance to respond and modify the paper. I hope the PC members can weigh in on what can be done at this stage. Even though I like this paper, my score will go down based on the poor quality of Tables 1 and 2. -----After rebuttal:I think the authors well-addressed my comments. Thanks for their efforts. I would suggest that the authors include some results in their response in the paper or supplementary results if they have not done it. Updated review: I appreciate the authors' response and have updated my rating. However, I still believe that the clarity of the exposition could be improved.  Update after rebuttal: I appreciate the authors response and have updated my score. Please see some lingering thoughts below:1. It looks like the experiments show that just the NN based Gaussianization of latent space is not effective when the noise distribution is significantly different from Gaussian. The authors had to  introduce an additional VST in the latent space to make it work. I think this is a limitation of the work - the method just by itself is not capable of handle of noise type significantly different from Gaussian. In my opinion this should be acknowledged in the paper. 2.  I think it is unfair to use a pretrained HQS model for comparison. The authors should retrain the model on the particular datasets they are interested in.   ##Updated Review##I'd like to thank the authors for their comments, clarifications and modifications.  I still believe that this paper is a novel contribution with clear impressive results. ## After author feedback ##Thanks for the paper update, and now I have a better understanding of the proposed approach. I have updated my review to the following:Previously Widrich+ (2020) showed that integrating transformer-like attention (or equivalently modern Hopfield networks based on softmax)  into deep learning architectures outperforms existing methods (kNN and logistic regression) for massive MIL such as immune repertoire classification. More specifically a pooling layer can be formed by attending over a repertoire of instances with a fixed (but learnable) query vector.This work provides theoretical analysis of such a layer for its energy function, convergence of updates, and storage capacity, and points to directions of how such a layer can be understood and controlled. It extends the previous experiment:1) apply HopfieldPooling (attention with fixed learnable query Q) to more MIL datasets (animal image and breast cancer)  and achieve state of the art results. 2) apply Hopfield (attention) to 75 small UCI benchmarks replacing feedforward nets. Here Selu units (Klambauer+ 2017) are used to map input to storage Y  and query R. The result is quite positive beating previous approaches including SVM, random forest, and SNN (Klambauer+ 2017)3) apply HopfieldLayer (attention with fixed training data Y as storage) to 4 drug design tasks  acting as an instance-based learning approach.The result seems quite interesting indicating that general purpose layers such as  feedforward, pooling and nearest neighbors can be improved (in terms of robustness, learnability, or controllability) by adding attention like operations.I think the paper can talk less about existing results, and focus more on the new results and their analysis:- remove [Immune Repertoire Classification] result since it is from previous work.- move the Drug Design experiment details to the main text, and add some comment about under what condition Hopfield outperforms/underperforms RF.- for the UCI benchmark experiment the transformer layer (Vaswani+ 2017) seems to be a natural baseline and should be compared to. Suggestions for the presentation:- Should only in the future work section state that Hopfield can potentially substitute LSTMs or GRUs, since it is all hypothetical with no experiment result at this point.- The word "implemented"  in Section 4 seems misleading as there is nothing changed in the Bert model structure? "Transformer and BERT models can be implemented by the layer Hopfield."  - Can be more specific in descriptions. For example in the description of (2) Layer HopfieldPooling and (3) Layer HopfieldLayer in Section 3, R and  W_K can be  referenced again for "state (query) patterns " and "The stored (key) patterns" respectively.- It is probably more informative to replace figure 1 with a table to directly compare the energy function and updating rules of different Hopfield nets--i.e., classical, exponential and attention.- Avoid using "x" in equation 1, since the symbol has already been used for the stored patterns.- "HopfieldLayer" seems to be a very strange name. **Update**I have updated my score to 7.One of the points that was not explained in the original paper was that (ignoring function approximation effects) an optimal solution for $J_b$ (the OffPAC objective) will be optimal also for the original off-policy RL objective $J$ (i.e. estimating the on-policy objective in an unbiased manner from off-policy data). From this point of view, I agree that optimizing $J_b$ directly is an interesting question, despite the fact that the exact gradient for $J_b$ may be less similar to the gradient of $J$ compared to the usually used approximate gradient of $J_b$ that drops the $\nabla_\theta Q$ term. It still remains unclear which of the two methods has a theoretical advantage over the other in the function approximation setting (in terms of optimizing for $J$); however, because it is unclear, it is interesting to evaluate the method proposed here and to perform experiments as done in the paper to try to find out which method performs better.The results were mixed; however, the evaluation is fairly thorough and some potential advantages of the new methods such as generalization in the $\theta$ space and zero-shot learning were explained.The discussion in the paper is much improved compared to the original version. Also, additional ablation studies such as testing what happens when the $\nabla_\theta Q$ term is dropped were added (when $Q$ includes $\theta$ as an input). Moreover, LQR experiments for $Q(s,a,\theta)$ and $V(s,\theta)$ were added in the appendix (the results here do not give as good a match as the $V(\theta)$ formulation gave, but they are reasonable).______________________________________________________________ UPDATE: I thank the authors for their detailed response and updated paper. I'm now more inclined to accept. Final recommendation:I have read the authors' responses as well as the comments from my fellow reviewers. I would like to keep my rating of the paper (8). **Update**I appreciate the effort by the authors to clarify some of the issues, most of which are addressed in the rebuttal, so I will raise my score to 6.I still feel like the $I(w, y)$ part needs to be dealt with a bit carefully, especially there is a invertible mapping between the two on the generative side. The simple graphical model seems like $w \leftarrow x \rightarrow y$, where left is encoder and right is data generation procedure.  ------------------------------Post-rebuttal:I appreciate the authors' feedbacks. However the authors' response to the proof of Theorem 1 is not the most convincing, which is a big part of the claimed contribution.  After rebuttal: I thank the author for their response but I have to lower my rating by one step after reading the comments of Reviewer2. Updates after discussing with the authors1. The paper is not very clearly written, and I had misunderstandings. Some of my comments above are not right.2. However, I will not change my rating. I found the convergence rates stated in the paper are misleading. The paper claims $O(1/T)$ convergence rate. In fact, this is WRONG. The authors assume the Frobenius and trace norms of $n\times n$ matrices are CONSTANTS. This is not possible. The norms are $O(n)$. Simple arguments can show $|| \xi ||_F = G$ is $O(n)$.3. Based on the right assumption that $|| \xi ||_F = G = O(n)$, the required number of iterations is $T = O(n^2)$. The algorithm is not communication-efficient. It is more expensive than communicating the $n\times n$ kernel matrices.4. After reading my comments, the authors changed their notation from $G$ to $\gamma$, $C$, $G$, and $H$. They are also Frobenius and trace norms of $n\times n$ matrices. The authors assume $\gamma$, $C$, $G$ and $H$ are constants. This is WRONG. They are $O(n)$.     - For example, if they use the bound of Rahimi and Recht,  then $|| \xi - K ||_F^2 = G^2$ is $O(n^2)$.  A bound as good as $|| \xi - K ||_F^2 = O(n)$ would surprise me; if the authors know such a bound, please let me know.     - Let me strengthen my point again: IT IS WRONG TO ASSUME MATRIX NORMS ARE CONSTANTS! If the authors can prove they are constants, they need to show me the proofs. If they cannot, they should assume Frobenius norm and trace norm are $O(n)$. -- After rebuttalI've read the authors' feedbacks and other reviewers' comments. My major concern was the clarity of the manuscript as other reviewers mentioned, and I believe the concern has been resolved during the rebuttal period. I adjusted my ratings representing that. I've read the authors' feedbacks and other reviewers' comments. R5's main concerns are the clarity and the motivation of Bayessian classifier and off-policy learning. That should have been resolved from authors' feedbacks.  POST-REBUTTAL RESPONSE:Thanks for clarification on Theorem 1 of this paper, i.e. that the novelty is in interpretation. I agree that the "denoising" between observed and generated data is an interesting idea.I read the author's additional experiments on CelebA. In Figure 10, VampPrior is still qualitatively superior to the author's best result $SWAE(\beta^*=0.5)$. I have some skepticism over the reported results for WAE-{GAN/MMD}, which are much worse than the results in the original paper (Tolstikhin 2018). The authors appear to have used different encoder/decoder architectures, which complicates the comparison. Is WAE's decreased performance due to choice of architecture or algorithm? FID scores on CelebA would be also helpful.All told, I raise my score, but still harbor some doubts over the empirical advantage of this work. *********The authors partially addressed my concerns. Therefore I raise my score to 6. -------The authors have addressed most of my concerns. I believe the paper is a good contribution to the literature on normalizing flows; therefore, I firmly vote for acceptance.  Update: after reading the feedback and discussing with the other reviewers, I decide to keep my score unchanged. ***** Post Rebuttal *****I thank authors for the clarifications! After reading the rebuttal and comments of other reviewers, I am increasing my score.  ============================================I thank the authors' for their response.I am satisfied with the answer regarding gradient clipping. As such, I raised my score to 5.However, I still cannot get away from the thought that random verification seems to speak for a rather humble a contribution. Combined with the lack of breakdown of "actual training" vs. "verification", I am rather hesitant to give a score higher than this. Regarding the experimentation on ImageNets, I understand that there was lack of time to add more experiments on this front. However, it would provide a more concrete message if the paper includes these additional experiments. UPDATE: The authors have addressed my concerns and I have therefore increased my score. UPDATE: The authors have addressed my concerns. Taking also into account the responses to the other reviews, my positive view of the paper has been confirmed and I have therefore increased my score. --------------------------------------------UPDATE AFTER AUTHORS RESPONSE: The authors partially addressed my concerns and I (slightly) increased my score.  **============  Summary of improvements and revisiting reviewers score after rebuttal ==============**Summary of main points of improvement related to my comments after the revision:- The authors have significantly extended the analysis within the paper with more experiments to investigate the proposed measures and the deep nets behaviour in question. This adds a lot of value to the paper, offering more insights and support for the main claims.- The authors have added many in-text clarifications about certain design choices (and improved some, e.g. by average k-max instead of max), which makes the study much clearer and adds confidence to the reader about interpretating the investigation and its results.- The authors have added a study of the influence of the k parameter in the measures, which simultaneously offers confidence in the conclusions (conclusions hold for a significant range of values for k) and offers new insights about how separability happens (1 neuron vs multiple).- Reproducibility is greatly improved, both by improved clarifications of the experimental settings within the text, and by providing the code in the supplementary.- The authors improved discussions about the results with respect to related literature, taking into account and linking them also to papers that at first glance seem to be contradicting (e.g. references [1,2] I provided above that show tight clustering improves generalization). These links and discussions further improve my confidence in the conclusions.Overall, the authors have addressed sufficiently most of my concerns, significantly improving the manuscript. The updated manuscript provides significantly more insights, which should be of interest to a part of the community. The extra analysis provides better support of the studys hypothesis and claims. Remaining weaknesses of the paper include the CIFAR-only experimentation (we do not know if conclusions hold beyond it). I am happy to increase my reviews score from 5 (marginally bellow acceptance) to 7 (Good paper, accept).= Minor =In case the paper gets accepted, I would suggest the authors to try to complete Fig 4 with the layer-wise investigation, which they said was too expensive for the rebuttal period (I cannot estimate how expensive that can be. Hopefully it is possible in longer time period, and would complete nicely Fig 4). **================================== Update after rebuttal ==================================**I appreciate the reviewers' hard work for adding this many new materials in a short period of time. A large number of my concerns have been addressed and the quality of paper has improved significantly. Some newly added experiments give a lot more insights than the original draft. As such, I am in favor of accepting the paper and have increased my scores from 4 to 6.However, I still have a few lingering questions in the light of the rebuttal and would love to see them addressed should the paper be accepted:1. Re: figure 4, the question is why the performance of the green curve is so bad. I was wondering if some of the labels are wrongly labeled to induce this effect.2. For reducing the feature map to a single quantity, why is the max operation chosen? Intuitively, it makes more sense to use mean or even order statisitc to make the metric more robust.I also understand that many of the experiments cannot be added in a short period of time and hope that the authors add them as promised in the rebuttal. ###########################  Post Rebuttal Comments###########################After reading the rebuttal, I am keeping my original score. For my first concern, they authors mention space as being a limitation for not providing analysis on the "surveyable" latent space, but as far as I know, additional experiments addressing my concern could have been added to the supplementary material. For my second concern, the authors talk about excelling in synthetic fidelity, however, there are fidelity measures for generative models that were not used in this submission. MSE is not a fidelity metric. I suggest that the authors address the concerns raised by the reviewers in future submissions, and it's highly likely that the work will be more solid. Post-rebuttal===========Thanks to the  authors for  their detailed response to  my  questions. Some of the answers are indeed satisfactory, but some questions remain - such as extensive comparisons to other methods (probably using more datasets), how  the method would  behave (practically)  with  a different fairness measure like DP, and more  carefully situating the method in  the  fairness literature. I encourage the authors  to keep pursuing this interesting  direction. ------------ Post Rebuttal ------------------I read other reviewers' comments and the authors' responses. I like the idea of applying the GAN framework to compute counterfactual distributions. However, I could also see why other reviewers are not particularly excited about it. The authors managed to apply the GAN approach to obtain counterfactual samples in some specific datasets. However, many questions regarding the proposed methods are left unanswered, e.g., under which condition the proposed GAN approach is ensured to obtain unbiased counterfactual samples. With this being said, I think this paper could be most improved by further elaborating how it contributes to the existing causal inference literature, especially in computing counterfactual probabilities. Due to these reasons, I intend to keep my score but won't strongly champion for it. Post-Rebuttal:I would like to thank the authors for their rebuttal.The updated version of the paper has addressed some of my comments.However, I still fail to understand why the method should 1) converge in general, and 2) converge to a good solution. I have updated my score accordingly.===================================================================================================== Update======After much effort, I can say that I understand the paper. The edits appear to have incorporated all the identified missing information. I have thus updated my confidence and slightly improved my score, however I am not confident that the current presentation of the approach will be understandable by a reader without contacting the authors, given that the difficulty I had in understanding the paper (and my initial confidence) stemmed primarily from missing information and poor presentation for the approach. Although the results do seem to improve upon past work, its impact will suffer if it is difficult to understand for a non-reviewer reader. I would be more confident if a fresh set of eyes could understand the details of the work without having to go to the authors to clarify so many details.  ## Post-rebuttal responseI have read the other reviews and the authors' extremely thorough responses  much appreciated! See the thread below for some brief responses to the rebuttal sections in turn.I regret posing far too high a standard in my original review. The authors' rebuttals have helped to quiet my doubts a bit, and better understand the utility of this paper as a product for cognitive science. I have accordingly revised my judgment quite a bit upward. ## Response to commentsI thank the authors for their comments and their revision, which have clarified the aim of this work. Having read the authors' responses to this and other reviews, I realize that in my initial assessment I had misjudged the nature of the manuscript. After careful consideration, I have therefore increased my rating. **Post Rebuttal Comments**:I appreciate the authors' efforts on the rebuttal, most of my questions are answered and addressed. However, I still have a major concerns. As agreed by the authors' response, this paper lies in a detailed analysis of existing tricks. I thus hope the paper could bring some new insights, but the mentioned "other important conclusions" in response is somehow commonly known in the NAS community. This makes the paper like some empirical supplementary material for existing papers instead of a new one. Therefore, I keep my original rating. ########### UPDATE #########I thank the authors for their responses and for updating the paper. I think this work introduces some new and valuable ideas for generating videos conditioned by an action graph and I recommend the acceptance. After author response: My main concerns about this paper are technical novelty and weak experimental results. The authors did not make efforts to address the two aspects properly. For example, I have listed several issues in my comment 3 about the specific experimental results, but the authors did not try to address them at all. Their responses to Reviewer 4 about the transductive ZSL are not relevant to the weak experimental results of concern.  As most of my concerns are not resolved in the response, I see no evidence to upgrade my rating.I appreciate that the authors have revised their responses. However, the added response still does not address the two specific questions (the reasons why AREN is not included in Table 3, and AREN+CS not in Table 4) in my comment 3. My final evaluation about the paper is on the negative side in that its technical novelty is moderate and the experimental results are not convincing.  Post-Rebuttal Evaluation [FINAL] I would like to thank the authors for their response and for updating their paper based on the reviewers feedback. Following these updates, I'm changing my recommendation to Rejection for the following 2 reasons: 1- the technical novelty of this paper is moderate and there's a significant gap in the model performance as compared to state of the art, 2- the authors failed to provide convincing answers to many of the reviewers concerns, including motivation for not using semantic embeddings during the training process, and not comparing their approach to  transductive ZSL ones which achieve a higher performance. *Post-Rebuttal Evaluation [FINAL]*I would like to thank the authors for their response and for the updated version of the manuscript, I appreciate their efforts. Unfortunately, I still believe that the paper lacking about original contribution and I am not fully convinced by the authors' comments on the relationship with [Yang et al., A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015] and [Liu et al. Generalized Zero-Shot Learning with Deep Calibration Network, NeurIPS 2018] which I still judge highly overlapping with the current methodology.Furthermore, although authors clarified on the avoidance of using semantic embeddings for the training methodology, I do not see a sharp point in pursuing this approach given the high gap in performance with prior art (GAN-based). For all these reasons, I regret to confirm my initial rejection score. **Post-rebuttal update: the score is increased from 3 to 5. See the response to authors' comments.** EDIT: the authors have removed the claims I was concerned by, as well as adding a requested baseline. The new quantitative results on stabilization also make their method much more compelling. The new hopper results are a bit noisy (would it stay stabilized if training continued?), but are still quite promising in that they show an ability to handle more complex dynamics.4-->7, thanks for the great revision effort! **Post-Rebuttal**After reading the rebuttal and other reviewers' comments, I am actually on the fence for this submission. On the one hand, it provides several interesting observations about the phase transition in long-tailed recognition, which would be valuable to the community. On the other hand, its experimental evaluation needs to be strengthened. The authors are encouraged to include more many-shot/medium-shot/few-shot analysis across the dual phases.Therefore, I upgrade my score to 6 (marginally above acceptance threshold). ----post-rebuttal updateI appreciate the authors for the responses. Some of my concerns have been addressed, so I increased my score. However, I this the current version still lacks critical insights and some justifications are questionable. -------------------------- Post-rebuttal ----------------------------I read the authors' rebuttal and I appreciate their efforts. I would suggest that the authors incorporate those clarifications into their manuscript. I would also suggest that the authors re-motivate their paper and modify their approach section.In terms of the discussions to related work, I do think [A, B, C] is about long-tailed recognition/detection, not few-shot learning. For instance, [A] works on LVIS, a long-tailed object detection dataset; [C] works on iNaturalist, which is clearly long-tailed. [B]'s Fig 1 clearly shows that the problem is long-tailed. I'm surprised that the authors simply said that [A, B, C,] works on different problems but did not intend to discuss the similarity in methodologies.There are some very important questions not addressed yet, specifically, my comments 3 and 4: There is no analysis if the proposed algorithm resolves gradient distortion. There is no analysis if graphs and memory banks are really needed. I also read other reviewers' comments and I agree with R4 that the current version still lacks critical insights and some justifications are questionable.Given these, I would keep my initial score unchanged. --after rebuttal--I appreciate the response, and it addressed my initial concern on the mutual information bound. However, the major concern remains which is the toy-ish setup and its practical value. The main result authors showed is that dropping some negatives doesn't hurt the performance if one adjusts m using EqCo rule. While this is interesting, the current setting of few negatives (by throwing out available negatives) is toy-ish, and may not reflect the real situation when a mini-batch is small thus only few negatives are available. I'd really like to see how this could be used to deal with the situation where you indeed only has few negatives. In MoCo, one could always easily buffer negatives with EMA the network, so there's no need to use a smaller set of negatives. In SimCLR, there seems to be a bigger potential of improvement from EqCo with actual smaller batch size, but the authors should also tune the learning rate (for smaller batch sizes) to make the baseline convincing, and compare to the results where large batch size is used. While the authors added a new point of alpha=4096, the current results of the paper are still incomplete, so I would keep my score. I'd encourage the authors to update/complete these results regardless the paper is immediately accepted by ICLR. ============================================After rebuttal:According to the reviewers' feedback, I would keep my score to 8 and still vote for acceptance. However, there are still two details that are expected to be fixed in the future version. First, the reason why momentum update in SiMo is important is not convincing to me. It is not clear why letting $\theta_k=\theta_q$ cannot ensure the loss becoming smaller. More theoretical and experimental analyses are expected to address this issue. I still encourage the authors to rethink this detail. Second, the computational cost is provided in the feedback. I encourage the authors to include the numbers in the paper. 35\% additional cost cannot be ignored. ==== update ====I found the bound the author provides is problematic (see my response for details). In short, the bound is problematic in that the bound will be more accurate when only one sample from the product of marginal (or negative) is used, i.e., $K=1$, while more negatives leads to less accurate bound. This definitely counters the intuition in theory where more samples should let you estimate the KL divergence between $p(x,y)$ and $p(x)p(y)$ better.I believe there is a mathematical issue of directly setting up $m=\tau \log \frac{\alpha}{K}$ as in Eq(5). Though the empirical results look good, I recommend **NOT** accept papers with theory issues. **Update after revision**The revision improved the paper. Thanks for taking care of my comments.Justification of sine activations, generalization to unseen speakers experiment are nice additions.The new title is a bit better and I think it may be OK since the goal is to perform a moving source simulation for single speech sources. Multiple speech sources can be simulated separately and added together as mentioned. The authors may consider possibly a better name: "Neural binaural synthesis from mono speech" which emphasizes that the synthesized target is "binaural speech" from a single speech recording.Just a few more points.1. I think it is essential in wavenet to apply the model in an auto-regressive fashion over samples. Just using the network architecture and the loss function from wavenet is not equivalent to "using a wavenet model" since an essential part of the model is the autoregressive sampling which makes sure the samples are dependent and coherent. Without auto-regressive sampling, the resulting sound is poor as observed by the authors. So, I suggest to emphasize that "autoregressive sampling" is not performed in the paper to avoid misleading the readers.2. More explanation of 2.5D is appropriate. One wonders if using a larger STFT window size would improve its results. POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft, in particular with respect to :the new experiment (complementing the original one on blobs) whose results are reassuringcorrections and improvements in the proofsthe new experiment on climate changeThe two-sample test part has been modified to take into account some of my comments but still lacks rigor. It doesn't explain the permutation procedure used and whether the p-values are just valid or exact. Now, the term "significance level" (replacing "p-value") is used to refer to the probability of type I error, which is not totally accurate in general. If the p-values are just valid and not exact, the significance level is a bound on the type I error. This should be fixed in the final version.Regarding my main concern, i.e. showing the interest of a decision-theoretic divergence using some custom loss, the new experiment on climate change is a good illustration of the interest of such divergences.In summary, I see useful theoretical results and a step in this not-much-explored direction of tailoring divergences and two-sample tests with a decision-theoretic perspective, i.e. "detecting differences that matter", and thus I decided to raise my score. ------------------------------------------------------------------The authors sufficiently addressed all questions in their rebuttal and I will therefore increase my score. --------------------------------------------------------------------------------------------Thank you for addressing my comments, I have updated my score accordingly. =====Update after rebuttal=====I have read the authors' rebuttal. However, my concerns are not well addressed. 1. There are a lot mask based methods for interpretation in different domains [1] [2] [3] [4]. Existing methods [1][2][3] are providing post-doc explanations for a pretrained model. I still believe "the connection between the proposed method and data compression is not convincing". 2. I still believe the novelty is limited. Hence, I am keeping my score unchanged. [1] GNNExplainer: Generating Explanations for Graph Neural Networks, NIPS 2019[2] Real Time Image Saliency for Black Box Classifiers, NIPS 2017[3] Learning to Explain: An Information-Theoretic Perspective on Model Interpretation, ICML 2018[4] Rationalizing Neural Predictions, EMNLP 2016 **Update after rebuttal:** I appreciate the detailed responses by the authors. I'm willing to increase my score based on the responses, but unfortunately I'm still not ready to recommend acceptance. In my opinion, the paper is simply not mature enough yet for publication (the significant amount of revisions required during the rebuttal period attests to this, I think; a mature conference paper should not have to require this much revision during review). In particular, the following fundamental issues still remain for me even after the revisions: 1. The misleading language about "temporal smoothness" in real-word data remains throughout the paper despite the fact that the paper doesn't address temporal smoothness as it exists in real-world data.2. The authors promise some new experiments on more realistic stimuli, but as it stands the paper still only includes experiments on static images with mostly toy data and I have no way of knowing whether any of their results would generalize to more realistic data. The experiments with multi-scale stimuli suggest that that generalization may be non-trivial (e.g. in that experiment, the baseline model with no memory or gating mechanisms actually performs the best).3. Which brings me to my final point: I still don't think the authors have adequately explained why and how the proposed mechanisms work. For example, the authors say: *"Our working hypothesis is that averaging across multiple members of the same category increases (in some datasets) the proportion of variance in the hidden units that is associated with category-diagnostic features."* Why the hedging *in some datasets*? The experiments with multi-scale stimuli clearly demonstrate that the proposed scheme doesn't work in all cases, but what exactly are the conditions under which it would work better than the baseline model? The authors need to make these a lot clearer.------------------------------------------ ### Update after response: The authors have quite thoroughly addressed most of my concerns with updates and new experiments. So I will increase my score to an accept at this point. ---Comments for rebuttal and revised paperThanks for providing a detailed response and an improved version of the paper. One thing that I am still concerned with is how come the updated ablation study is so different from the initial results. Originally, the differences between KGEDCg and KGEDCg-GE and KGEDCg-FKG were very minor (one of my questions above), but now the margins are as large as 7+ pts. Given such discrepancies without explanation, I'd hold my original evaluation. Author response: the updated paper is much clearer, and its abstract and introduction allow to clearly see what will be the contributions of this paper. I thank the authors for having followed my advice about this point. With this problem addressed, I recommend accepting this paper. ----- Post Discussion ----Updates to the paper have helped make technical parts of the paper more clear. The paper has also been edited to improve the motivation and experimental explanation. ##########################################################################Post RebuttalThe paper has been updated to include additional reviews about nested optimization. I would like to keep my original score. Updated review: Thank you  for your response and  for your efforts in addressing those points. I raised my score to 5 for the clarifications made to the document. However, I still think there are unjustified claims about the method, especially those related to the strong correction scenario and how introducing fresh generated samples is an effective procedure (conceptually / theoretically). **Acknowledgement of author responses**The authors' responses were helpful to clarify the settings and basic ideas of the proposed solution. I highly appreciate their effort. However, the limitations of the proposed solution and limited novelty still outweigh the merit; and therefore, I will keep my original evaluation.  The authors have made clear some of my concerns and made revisions accordingly. Thus I am in a position now to recommend this paper, thus I update my initial recommendation from 5 to 6. --- After reading the authors' response and other reviews. I still believe the paper has made a good contribution thus I would stick with my original rating. Post rebuttal comments: Thank you for the rebuttal. The broad concerns of insufficient novelty remain and I am sticking to my initial paper rating.  ****************************************Post-rebuttal comments*******************************************************************************************************************************************After reading authors feedback, I've increased my score from 3 to 5; however, the paper in its current form is still a borderline. *************************************************************************************************** After reading the rebuttal:I keep the score.In my view, the Taylor-series analysis is a very straightforward approach such that existing frameworks apply to the setup considered in this paper. Such a straightforward approach renders the results restrictive and hence I do not consider it a significant novelty.The proof of Theorem 1 just rewrites the existing proof with minor modifications for the considered setup and hence do not require any additional restriction. Emphasizing Theorem 1 is not very appropriate for the rebuttal.The "generalization" I mentioned is for the \phi function. There are already several works on mirror descent/FTRL with a time-varying regularizer (Orabona et al. (2015) is for FTRL, as clarified in Orabona's lecture notes on online learning). Without an example where the \phi function is not the gradient of a potential function, I do not see the necessity of this generalization.For the same reason above, emphasizing Theorem 1 is not very appropriate for the rebuttal.My point is to clarify the dependence of the convergence rate on the problem parameters. I do not think the proofs in the appendices provide explicit characterizations of such dependence. To make such explicit characterizations, I think specifying a step size instead of a range of step sizes is perhaps necessary.I suggest the authors rewrite the first two paragraphs to make the motivation of the problem setup clearer there. Section 2 is OK. Post-rebuttal comment:I acknowledge having read the authors' response and I have also glanced over the updated version of the paper. -------------------------Update after author response:Thanks to the authors for addressing my questions! Updated: The authors added the requested timing complexity data and additional experiments with better baselines to compare the proposed RC algorithm against. They present valid issues with reproducing some of the previous work. While a theoretical proof establishing the worst-case time complexity of RC to be better than random sampling plus Seq A* would be ideal. The empirical data presented does support the claim that RC algorithm is useful for finding more optimal solutions for large maps faster than Seq A* plus random sampling. Post Rebuttal Response: I would like to thank the authors for considering my review and for making some of the suggested edits. I think this paper provides a valuable contribution and point of discussion for those interested in the interplay between robustness and uncertainty in deep learning. I do consider the experimental evaluation in this work to be sufficient given that the authors consider many applications which already exist in the literature, and in my view it is out of the scope of an evaluation/methodology paper to necessarily advance the state-of-the-art in applications of the method they seek to evaluate. As I have no major standing criticism of this work, and believe that it provides a useful and interesting contribution to the conference I have increased my score. Response to rebuttal: the authors have drastically improved the quality of the submission with the new experiments and clarifications, I have therefore increased the score to a weak accept.------------------------------------------- I have read authors' feedback and will keep my original score. After reading author replies:I would like to thank the authors to respond to my doubts on some of the results. But I decide to keep the review and the score, because Theorem 1 and Claim 1 are still not well explained. In particular, the explanation like "if the 2nd inequality in Eq. 11 is violated, the network can not capture the amount of information measured by the entanglement entropy " still looks like a conjecture or intuition rather than a mathematical statement. --------------------------------------------- --Post-rebuttal edit: I read the authors' reply and thank them for the clarifications. I maintain my score of 6. The authors' rebuttal has addressed some of my confusion regarding the paper, which is greatly appreciated. The additional baseline of early termination would still be interesting to have, though I agree it's not critical for the presented line of work. In general, I think the work is interesting and will keep my current score (6).================================= ---- Post-rebuttal comments----Thanks for the response. The rebuttal addresses all my concerns. I am willing to increase my score to 8 and recommend acceptance. ---- Post-rebuttal comments----The author rebuttal + revised draft adequately addresses most of my concerns  in particular, the experiments on online adaptation and semantic segmentation are strong, and the additional context on the DA results is helpful. I would still have liked to see DA results on more challenging benchmarks but nevertheless think that the paper proposes an interesting approach and is worth accepting. ---- Post-rebuttal comments----The rebuttal and the paper revision address my concerns. I fully recommend acceptance. I want to thank the author for addressing my concerns. Many of my concerns are resolved. I have updated my rating after the discussion. I agree with the authors that the novelty of the submission lies in scaling up the existing methods. However, I'm not sure if this is enough for the paper's scientific significance required by this conference.  Edit post reviewer responses:- I had some misunderstandings in the review above, which have mostly been cleared up. Ive raised my score accordingly ##################################################################[Edit]After reading the authors' response and other reviewers' comments, I have lowered the scored to 5. The reasons are bellow.This paper provides good analysis of the coverage and balance of robustness benchmarks. Based on the analysis, the paper presents a method to construct a benchmark of Non-Overlapping Corruptions. I think such analysis is valuable and interesting, despite some concerns about the coverage comparisons (which are only partially addressed by the authors' response). I agree with Reviewer#1 that the comparisons between ImageNet-C and ImageNet-NOC are not solid. And that "the paper should have done an equivalent evaluation, comparing ImageNet-C as a whole vs. ImageNet-NOC as a whole." Since the core idea of this paper is to propose a better robustness benchmark, solid comparisons between ImageNet-C and ImageNet-NOC are critical.I also agree with Reviewer #3 and Reviewer #4 that, the improvements provided by ImageNet-NOC does not make a significant difference. The reviewer appreciates the analysis of the coverage and balance of the robustness benchmarks, which in my opinion, is valuable. However, since this paper focuses on proposing a better benchmark, solid evaluations between these two benchmarks are critical.  ------------------------------------UPDATE:After reading the author's response, the reviewer's comments, and the revised version, I have increased my score. See my response to the authors for more comments.    ---I appreciate the thoughtful rebuttal provided by the authors. My main concerns are on Q2, i.e., the practical usefulness of the algorithm. I do think that the authors provide a convincing argument on "we can only understand what we can understand," hence we should set up a hypothesis and see if it aligns with the explanation. However, I think the usefulness is not well-supported in the current state of the paper. The authors can come up with (a) a stronger example of such a hypothesis and (b) a better measurement of how the hypothesis aligns with the explanation to strengthen the paper. Regarding Q3, I still think that it is not correct to provide the same explanation for different algorithms when they produce the same output. Hence, the proposed algorithm should be modified to consider this aspect.  # EDIT: UpdateThank you for the rebuttal.Despite the skepticism of the other reviewers, I still think that this is a valuable, thorough paper of high quality.  =================Score raised to 6 after inclusion of MA + SA results in rebuttal. =====Updates: Thanks for the authors' response. I carefully read other reviewers' comments and responses. My concern on the missing study of empirical or theoretical support of claim "framework can filter out domain-specific information while preserving the amount of domain-shared information" was also raised by other reviewer. Overall, I still believe this paper provides new insights for this field. UPDATE after rebuttal ==I am willing to update my recommendation to weak accept after the rebuttal. Some of my main concerns remain, but added experiments, comparisons and discussions alleviate some of those. I still believe that there is too much focus on fairly useless metrics such as number of parameters which might lead to future work that will follow this trend, but the results are overall strong enough to warrant acceptance. UPD: the score was updated after the rebuttal stage. ***********After the discussion period, I have increased my score to 7 as the authors have provided a strong set of ablations and changes to address my concerns and those of the other reviewers. ------------------------------------------------------------------------------------------------------------------**After Rebuttal and Discussion**I appreciate the changes and additional experiments added by the authors, and am recommending accept. For the final version, I would strongly encourage the authors to make the representation change/representation reuse argument clearer. Specifically, the authors might want to first present the BOIL algorithm, highlight key aspects of the algorithm (frozen head), present performance results, and then the ablation results in Appendix N to highlight the importance of freezing the head. After this, the paper could switch to a discussion on representation reuse/change and present the analysis results, making clear what is happening at a layer level vs at the algorithm level.Right now, I think it's still a little confusing that representation change often refers to the algorithm not the layers, and this actually reduces the impact of what is a very striking result!I hope the authors can make these changes, and with the clearer messaging, this should be a very interesting paper for the community, and provide many interesting directions for future work! =========================Response after author rebuttal: The authors answered my concern about evaluating the interpretability of the approach. They evaluated the method with a few clinicians, and I'm glad to see that they preferred the authors' method.Adding these results + clarifying the points I included will definitely make the paper stronger. I increase my score as a result and recommend acceptance. ----------------------------------------------------------------------------------------------------Comments after rebuttal:I have reviewed the response from the authors, and I decided to keep my score. Although the paper is certainly interesting, for ICLR it is borderline. While I agree that many works in the literature choose to treat their network as a monolithic architecture, and thus do not explore the effects of different components, I would still encourage the authors to add an ablation of the readout method, as that would help assess the interaction between that and the choice of the graph propagation scheme. ##########################################################################Comments after the rebuttal period: I will keep my score. The authors' responses addresses my first concern. However, the differences from previous studies are still not significant to me. In addition, no matter the authors claim the proposal of a complete architecture or a new operator, ablation studies are necessary to backup the designing of each part. Post-rebuttal update---------Thank you for your response.  Now I understand that the algorithm works by smoothing the Gaussian parameters $\mu_i,\sigma_i$ w.r.t. the centered Gaussian rv (as described in my last reply, second part of bullet point (1)), so my original concern regarding the bias _in the Gaussian parameters_ does not hold.  However, I still cannot recommend acceptance at this point, because of a newly discovered issue in the theoretical analysis:The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers.  The text only considers two layers of stochastic latents and the KL part of ELBO, but in the deeper case, the smoothing of $\mu_i(z_{i+1})$ will additionally have influence on the layers below $i$, through the nonlinear functions $\mu_{i'},\sigma_{i'}$ for $i'<i$.  More concretely, consider the following scenario: $\mu_i(z_{i+1})\equiv z_{i+1}, \sigma_i(z_{i+1})\equiv \epsilon$ which is very small. Further assume that $z_{i+1}$ is high-dimensional and approximately follows $\mathcal{N}(0,I)$, so $\\|z_i\\|_2 = \\|\mu_i(z_{i+1})+\sigma_i\varepsilon_i\\|_2 \approx \\| z_{i+1} \\|_2 > 100$ with probability $1-\epsilon_1$, where $\epsilon_1$ is also very small. In this case, it is possible to achieve a low KL in the original ELBO, by using a $\mu_{i-1}$ which only has sensible values in the region $B := \\{z_i: \\|z_i\\|>100\\}$; in the complement set $B^c$, $\mu_{i-1}$ can be "arbitrarily" bad so long as its impact on the ELBO does not outweigh $\epsilon_1$, the probability its input falls there. However, in the smoothed estimator with $\rho=0$, the input to $\mu_{i-1}$ only have norm $O_p(\sigma_i(z_{i+1}))=O_p(\epsilon)$, so the value of $\mu_{i-1}$ on $B^c$ will have a far higher impact, easily exceeding the original by $O(1/\epsilon_1)$.  To summarize, *it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically*.  Moreover, even in the shallow case, $z_i$ will be fed into a final decoder block to generate the reconstruction image, so a similar issue exists, although it will be in the reconstruction likelihood part of the ELBO as opposed to the KL part.A less important issue is that parts of the analysis are written in a confusing way.  Apart from the abuse of notation $U_\rho$ which leads to my original confusion, in Section 3 the $\hat{\mu}_p$'s should have a suffix of $z_1$, to signify the fact that they are coefficients of a function that depends on $z_1$ (see the last response from he authors).  Also it is unclear to me why there is no mention of $\mu_p^4$, in the analysis of the variance of an estimator for $\mu_p^2$.  But given the aforementioned issue, I don't think it is necessary to look further into this case. Updated review:I thank the author for their new experiments during the discussion period. Given the superior performance of GNN over MLP, I am more convinced that the usage of GNNs in this application is justified. I have updated my review rating from 4 -> 6 to reflect this.But just to harass the authors a bit more, I have this curious question:- Is the worse performance of MLP due to generalization or expressive power? In other words, can the MLP fit the training data well? Also, when comparing MLP and GOREN, are the authors controlling the number of parameters when comparing MLP and GOREN? As the authors mentioned in their reply, which I agree, "MLP generally works better than LR due to model capacity". We want to make sure that MLP and GOREN have similar model capacity but GOREN captures better inductive biases.I believe this submission finds an interesting application for GNNs. I encourage the authors to bring out the full potential of this idea by having solid, rigorous empirical studies.  After rebuttal:I appreciate authors' detailed responses and an updated version of the paper. The new version is a lot clearer. After reading other reviews, I agree that the algorithmic novelty is limited, but the model is well-adapted for multi-horizon forecasting problem. Overall, I increase my score to 6. marginally above acceptance threshold. ---------------------------------------------------------- --- **Update:** I would like to thank the authors for their answer. I acknowledge the improvement of the manuscript after the review process:- Added clarifications on the baselines,- Added helpful precisions for reproducibility (even though the code cannot be open sourced),- Evaluation on 2 requested public datasets with good results.However, I am still not confident to raise my score to 6 (marginally above the acceptance threshold) given the missing public manuscripts (or Appendix) to explain the martingale diagnostic tools. This hinder one of the main selling point of the paper that their model reduces the volatility of the forecast. I've looked through the revised paper, and the authors has addressed my comments. Hence, I am happy to recommend an accept. ######Update: After looking at the revised version, I would like to raise my score to 5 since the motivation is clearer. Post Rebuttal Increased the score from 6 to 7. I would have strongly recommended the paper if it had more real-world datasets.  ========= Post-rebuttal update:I thank the authors for carefully addressing my comments, as well as other reviewers'.Ultimately, this is a nice paper with a novel recurrent component, and I can see how it could perform well in practice.However, the lack of stronger real-world experimentation (on datasets such as OGB) unfortunately renders the contribution insufficient -- the synthetic benchmarks being insufficient on their own to pull the weight of the paper.I retain my score, but encourage the authors to carefully revise and resubmit for the next venue should the paper be rejected. Update:The paper has considerably improved:the title is now accurately describing the papercomplex scenes have been added (and the method works there too)comparison with knn has been added and it shows a clear improvement for the proposed methodI think the paper has improved, but I still find the comparison with direct solvers problematic. For small scenes like the one shown in this paper, a direct solver is fine, there is no need to use an iterative one. If the scenes are large enough to require an iterative solver (i.e. a direct one runs out of memory) then it should be shown that the proposed methods provide benefits in that specific setting. It could be that my bar for comparison is too high, as I usually publish in a different community where quantitative improvement against the closest baseline is always required.Overall, I am still mildly positive, but not willing to champion this paper given the many issues raised in the reviews. POST REBUTTALThe updated results make sense. The limitation/assumption mentioned in weaknesses 1 must be sufficiently disclosed in section 3.2.  ======= updated ======The authors response partially addressed my concerns and I would raise the rating to 6. *Edits after author comments and revision:*The authors have greatly improved the readability of the paper. I also appreciate the addition of section 4.4. which seems like a reasonable attempt at supporting the increased interpretability of the architecture. I feel that the paper has improved, but it wasn't quite enough for me to raise the rating from 7 to 8, so I'm leaving it at "Good paper". ========== After discussion =============I have increased my score from 6 to 7. ---------- after rebuttal ----------Thanks for the response and the updated manuscript. I'm raising my score from 4 to 5. I'm still leaning towards rejection since I still find the results quite subtle and I hope to see more empirical justifications.In the updated Proposition 3, the sparsity-inducing property 3 assumes the existence of a time $t_2>t_1$ when the ratio between the two weight norms deviate from $1/c$. However, it seems entirely possible that this ratio will have already converged $1/c$ after time $t_1$; in this case the two weight norms grow at the same rate. It would be good to investigate this more carefully to see which cases are more likely to happen. I'm also concerned that the advantage of EWN for pruning only shows up in extremely small loss value (Figure 7), and therefore the practical relevance shown in the current paper is not very convincing. Post-discussion update: The authors have clarified their work considerably, and I believe the work is probably correct. However, the paper still suffers from poor presentation and poorly-motivated or justified modelling choices. The current version of the paper has not been updated, and all below issues thus stand. The paper overall presents a good idea, but I believe the authors made poor modelling choices, which led the kludgy math. ---- ________________EDIT: Score changed from 6 to 5 during discussion, see comments below. Thank you for your response, which cleared up some of my questions so I increased my score to a 5. I would have liked more analytical / empirical evidence to substantiate some of the claims made in response to the questions numbered 4, 5, 12, and 13 in the comments provided by the authors. My overall opinion remains unchanged but I encourage the authors to continue this line of work on build on these results and incorporate the feedback provided by the other reviewers as well.### ====== POST-RESPONSE UPDATE ======I appreciate the authors' response and the additional experimental results provided. While I do think that they are a step in the right direction (I hence slightly increased my score to a 5), they still do not address my concerns. Specifically:- **Empirical results.** Taking a closer look at Table 1* of the response and the results on randomized smoothing (provided in other responses), I do agree that not all errors increase by the exact same multiplicative factor. At the same time, the discrepancy does not seem to be particularly large---i.e., the ratios have a mean of 2.5 with a standard deviation of <0.5. Clearly, given that this is a real-world dataset, it is natural to expect that the effect of adversarial training is not perfectly linear across all classes.- **Theoretical results.** After reading the response, reading the discussion with Reviewer4, and going through the paper again, I am still not convinced that the increase in class disparity cannot be attributed to a large extent to the gap in clean accuracy introduced by robust training. Perhaps there is a cleaner way of formulating there results or highlighting the key components of the analysis that resolves this. However, unfortunately, I still do not find the analysis convincing in its current state.Overall, I believe that this point is quite nuanced and the existing empirical and theoretical analysis is not sufficient to draw a confident conclusion. Given how this point is at the core of the paper's contribution, I still recommend rejection. [updated after discussion]Thank the authors for their efforts to add ablation study and make the manuscript more clear in presentation, it greatly resolves my questions and thus I raised the score. Overall the paper is in a good shape now. However I do want to point out a couple of things: 1) the performance of UE in base model FM/DeepFM/AutoInt seems to be a bit weired, as in previous CTR paper, deep models should outperform FM significantly. It's not clear to me is this due to different experimental settings or training schemes. 2) if the target is better performance (instead of compression), there is no clever way to choose s, other than mannual picking for each dataset/model. =========Post Rebuttal==========Because no response is provided, I maintain my original rating. Post-rebuttal ReviewAs there is no response submitted by the authors, I would like to stick to my original rating to reject this paper. ## Update The new sparse tasks and comparison to Dreamer + Curious improve the paper and address some of my concerns. Specifically, a sizable improvement due to exploration is now seen on 3 tasks, Hopper, Cartpole Sparse, and Cheetah Sparse. The new maze task is also more challenging than the bug trap task.  --- Final Decision ---After the significant improvements in the experimental evaluation, I believe the paper provides a reasonable case for the proposed latent UCB method. It also provides an interesting discussion on the advantages of UCB-style methods, and an interesting observation that optimistic reward-based exploration can be effectively used even in absence of (positive) rewards. Even though the experimental evaluation of prior work on exploration is still rather lacking, I believe that these contributions are enough for the paper to be interesting to the ICLR audience. I raise my score to 6.--- Remaining weaknesses ---The experimental evaluation in the paper is still quite lacking in terms of baselines, making it impossible to judge whether the paper actually works better than prior work.First, the proposed method contains two improvements, model ensembling, and optimistic exploration, but doesn't go much in-depth analyzing either of these improvements, instead trying to focus on both at the same time. This makes comparison to prior exploration methods hard because the proposed method receives an additional boost due to an ensemble of dynamics (the paper conveniently quantifies this boost in the LVE method, and it is shown to be rather large). For a more fair comparison, the ensemble of dynamics might be ablated (leaving only the ensemble of value functions), or the competing baselines could also be built on top of LVE.Second, the paper only compares against one competing exploration method, Dreamer + Curious. There has been a large amount of proposed exploration methods, and it would be appropriate to evaluate the proposed UCB method against at least a few of them. For instance, the paper could compare against similar value function ensemble techniques (Osband'16, Lowrey'18, Seyde'20), or other cited work (Ostrovski'17, Pathak'17). Burda'18 is not cited, but perhaps should be compared against. All these methods can be relatively easily implemented on top of LVE for a fair comparison.Burda'18, Exploration by random network distillation.--- Additional comments ---Would be great to clarify what is the observation space for the bug trap and maze tasks. For instance, you could add observations and predictions for these tasks to the appendix. I am happy that the authors improved the paper with reviewer feedback. In particular I think the new ablations and comparison and mention of previous work makes the work more complete. The results on new sparse tasks are also interesting. I still think more can be done in terms of experimental validation (in particular my original note regarding early cutting of the curves has not been addressed). However overall I think the paper does meet the acceptance threshold as things stand. **Update (after the author's response)**: During the rebuttal, the authors clarified my major concern, as well as provided additional experiments that verify the main claim of the paper. I am totally satisfied with the author's response, hence I am changing the score 6 $\to$ 7 // Post-rebuttal update:Thank you for replying to my questions. I am still concerned about the sample complexity associated with $\epsilon$-optimality in cross-entropy (even in the trivial case, and perhaps impossible for some low-dim representations) as LMs are over a countably infinite extended alphabet. ---- update after authors' response ----\Thanks for clarification and providing additional experiments. I'm changing my final evaluation to weak accept. Yes, this paper does provide some interesting insights, but I still think that it has a limited potential impact (see above for major drawbacks). # Updates after rebuttal1. The authors have provided more results I concerned, which seems to be accord with their conclusions in the paper.2. Initialization of CNN or other networks is an interesting topic which affects the performance of pruned models. However, there are few papers about the topic. I think the paper is a good example which may arouse more concerns about it.3. I am willing to increase my rating to 6.### After rebuttal:My main concerns are addressed, and I changed my score to 5 accordingly.------ **Comments after Author Response**I thank the authors for their response. My opinion of the potential of this paper to encourage further discussion in this area is unchanged. I can already see from the response on choice of biased and unbiased compressors to combine that there is plenty of scope for future work that builds on this idea. Regarding the comparison with EF, I appreciate the additional intuition provided in the author response on the drawbacks of EF and hope to see improvements to EF or more exhaustive comparisons between EF and the approach proposed in this paper in future work. As I had already recommended acceptance I am leaving my score unchanged.  ==================================Comment after author responses.On reading the author's responses to my (and other) review comments, my recommendation remains unchanged. This is a solid piece of work.  -----------------------After Rebuttal============My main concern was that it is already known that BatchNorm has a great expressive power, and thus the authors should have gone broader and deeper in their search for valuable insights to explain what makes BatchNorm so special. I proposed different ways to do that, such as comparing with other parametric transformations like squeeze-and-excitation, or trying to understand what is the role of normalization. My concerns were shared with Reviewer 4, who proposed an interesting experiment that involves comparing multiple normalization functions such as LayerNorm.The authors have agreed with some of these points, and updated the text to reflect the discussion. However, they believe all these suggestions belong to future work and, although the form of the paper has changed, the content is still the same.Overall, I think this work is interesting and I think studies like this are necessary (although more in depth). Thus,  I have raised my score to borderline.  ***********************After rebuttal: I maintain my rating and think this paper is on the borderline. I think the paper is interesting, but the novelty and significance is limited by previous works along the direction. * I have read the authors' response and also other reviewers. In my view, this paper provides novel insights into batch normalization. **after rebuttal**Some of the issues are clarified. I updated my score to 5. **POST REBUTTAL**I read through the other reviews and the author rebuttal. I thank the authors for addressing all of my concerns in the rebuttal. Overall, this is an interesting contribution in the multimodal VAE space and would make for a good poster at the conference, and I am happy to keep my original rating for the paper. In terms of energy based models here is one recent work which comes to mind which might be useful to extend to multimodal settings: Du, Yilun, Shuang Li, and Igor Mordatch. 2020. Compositional Visual Generation and Inference with Energy Based Models. arXiv [cs.CV], April. https://arxiv.org/abs/2004.06030.(note the quality of generations in Fig. 5) **Update**The errors in the paper were fixed, and the discussion was improved.It is very rare to see a novel result as fundamental as the one presented in this paper, and I believe this puts it in the top 5% of accepted papers, so I have updated my score accordingly. I think the discussion and experimentation still has room for improvement, but I am not too bothered, as there do not appear to be any major errors remaining in the paper. --- EDIT POST REBUTTAL ---I thank the authors for their answer and their efforts in editing the submission. I have also read other reviews and replies. However, my stance on the paper did not really change as I find the contribution insufficient for acceptance. PS: when I wrote "Formulation 1 uses explicitly the proportion of outliers", I was referring to the display equation above eq. (2.1). I did not realize the term "formulation" was already formally used in the paper to refer to another equation. ##########Update##########I'm pleased with many of the changes the authors have made in response to the reviewers' concerns. However, I'll mention a couple lingering concerns.- The authors have updated their paper regarding use-cases for layer-wise explanations, but it remains unclear whether this feature is impactful. While it is possible to perform instance-level pruning with ShapNets, is there any reason to do so? Finding features with low SHAP values requires actually calculating the SHAP values, which takes longer than just evaluating the function. Therefore, it seems that this technique would not lead to saving either time or memory. If the authors disagree, they may consider improving this aspect of the paper.- The authors have clarified that they use the "reference values" approach to holding features primarily because it is convenient. To my knowledge, no existing research actually advocates for this approach. Most research advocates for either the "interventional" [2] or "observational conditional" approach [1]. The authors cite a paper that mentions the "reference values" idea (Baseline Shapley) [3], but not even this work truly advocates for this approach. In my view, this is a rather severe limitation of the proposed approach, and the authors did not suggest that they see a way to overcome it. To call these "SHAP values" is almost misleading because it silently changes one of the core aspects of SHAP. As an example of the consequences of this limitation, any feature that is equal to its reference value will have a SHAP value equal to zero, but it is easy to image how such features can be informative (e.g., black regions in an MNIST digit, such as missing arcs on the left-hand side of a "3" that distinguish it from an "8").While it is very helpful to calculate SHAP values faster, this is a flawed version of SHAP that is not supported by existing research that considers the question of how to model missing features. Unfortunately, the proposed approach apparently lacks the flexibility to work with different notions of missingness (e.g., the interventional or observational approach). For that reason, I'm lowering my score by one point (6).[1] Frye et al., "Shapley-based explainability on the data manifold" (2020)[2] Janzing et al., "Feature relevance quantification in explainable AI: A causal problem" (2019)[3] Sundararajan and Najmi, "The many Shapley values for model explanation" (2019) (I am raising my score by 2 points after the author response)-- Update after the rebuttal:I read other reviews and response from the authors and I decided to keep my score. Overall, I think paper still needs morework. For example, incorporating details on confidence into the main paper and not just a section in the appendix is quiteimportant as otherwise the paper is misleading.================================================== -------------------After reading the authors response, i think that this work would benefit from an experimental comparison of their random method against a method relying on a deterministic crop selection, even if the certification rates of the deterministic method are inferior, because the certificates both methods are yielding are different: The resulting certificates from the deterministic method would be deterministic instead of probabilistic. Hence i will retain my score.  Update after rebuttal:After reading the rebuttal, I don't think my questions are addressed very well, especially about the confidence probability, $p_c$. The certification is defined as a guaranteed yes/no problem but the $p_c$ will relax the certification to a probabilistic problem. Also, PatchGuard with patch transformation is out of scope for the original paper, so I think the experimental results in Section 4.1 and 4.2 are more like a fair comparison. However,  refer to the results in table 3, the proposed method yields worse performance than PG-DRS although the computational cost is saved. Hence, I will keep my rating. ------------------------------------Update after rebuttal-----------------------------The updated version is clearly better than the first one. However, the concernsof the other reviewers regarding the randomness of $p_c$ (and therefore of thecertification method) have convinced me that there are, indeed, some furtherclarifications and discussions needed prior to the publication, which is why Iwill keep my initial recommendation.To be more precise, the root issue here seems to be that the proposedclassifier is not deterministic, which means that the standard definitions ofadversarial examples and adversarial accuracy do not apply and therefore, thatthe problem that you try to solve is unclear and/or not well defined.  Inparticular: what is it that gets certified?  what does it mean to getcertified? and, more generally, is the word "certified" really appropriate inthis context?However, whether an analysis of the distribution of $n_{2to1}$ and $p_c$ willbe needed (as asked by other reviewers) might depend on how the authors willdefine adversarial vulnerability in the random setting, and what they try tocertify. Let me explain what I mean.A reasonable start might be to define adversarial risk as$$    E_{(x,y)} E_{\phi} \mathcal{L}(\phi(x), y) \ ,  \tag{1}$$which is the usual definition, but with an additional expectation over thevariability of the classifier $\phi$.  Adversarial accuracy would then be theadversarial risk for the 0-1-loss $\mathcal{L}_{0-1}$. Then the authors could, f.ex., set as goalto construct a (provably) unbiased estimate of this quantity.The advantage of such a method is that one doesn't forget the fact that, whatwe actually want to certify is this "distributional" robustness (i.e. whereexpectation is taken over the true underlying, unknown distribution), not therobustness on the test set. Even methods that have a non-random certificationprocess (so-called "provable robustness guarantees") will never be able tocertify this quantity: they'll only deliver certificates on test example. The"certified robustness on the test set" that they yield is also just a randomvariable which we hope "generalizes to" (1). Reviewers almost never ask authorsto analyze/certify this generalization gap. Similarly, here, one could see therandomness over $n_{2to1}$ and $p_c$ as just another source of randomnesscontributing to the variability of the generalization gap, in which case, maybeno rigorous analysis could be acceptable, as long as it is clear what theauthors want to certify (unbiasedness of the estimator, f.ex.). Therefore,whether this source of randomness could or should be explicitly captured/usedby the authors' method is, I think, a question of how the authors frame theproblem and their goal.### Minor points:- even the revised version still contains quite a few grammatical errors,  especially in the new/re-worked sections, where many articles ("the", "a")  are missing.- End of p.5, "to maximize number of certified robust images, the randomized  cropping classifier should maximize n2to1, which is equivalent to maximizing  classification accuracy of $g_\theta$": not sure about this equivalence.  Maximizing the classification accuracy is equivalent to maximizing n1, not  necessarily n1-n2.- are DRS and PG also probabilistic certifications (i.e. certifying robustness  with some probability, f.ex. pc>.95)? This should be clearly said in the text  and the captions, especially since it would make the comparison a bit unfair  if their certification were 100% sure. (This issue is obviously related to my  previous major remark on randomness.)- the name "worst-case certified accuracy" in the caption of Table 1 is very  unclear at that point. It becomes clear in Sec. 4.3, but you refer to Table 1  in Sec. 4.1 already. So this term should be clearly explained in the caption,  or there should be a clear reference to the relevant part in the text.- don't always re-cite Levine&Feizi and Xiang et al. every time you mention  de-randomized smoothing and patch guard. Cite them the first time, and then  say that you'll refer to de-randomized smoothing and patch guard as DRS and  PG in the rest of the text.- in the conclusion: "This paper proposes a new architecture for defense  against" -> "This paper proposes a new defense against". (You are not really  proposing a new architecture.) *** Update ***I thank the authors for their very detailed response. Most of my concerns have been addressed and I now recommend acceptance. *** ===========================================Updates:After considering the author's response and updates to the paper, I have bumped the score to a 6. The addition of Appendix H, in my opinion, considerably strengthens the paper's story and case for acceptance. I still have minor concerns about the writing surrounding the use of the Multinomial likelihood - for example, the paper still claims to be dealing with high-dimensional data, but bucketing into 25 buckets immediately reduces the complexity of the observation space to 25-dimensional counts. However, the authors have addressed most of my major concerns. --------------------------Rebuttal: Thank you to the authors for their detailed response. I am happy with the response and will keep the original score. POST-REBUTTAL UPDATE======================The rebuttal and resulting changes have addressed most of my concerns, and expanding the alternative reward-shaping aspect of the evaluation has helped too. I've increased the score.====================== -----Edited after authors responses:I would like to thank the authors for the detailed response and the changes they have made to the paper.  - Regarding contextual data augmentation (Kobayshi et al., Wu et al., etc.): Thanks for pointing out that these method use the labeled data to finetune the LM to make sure that words are replaced with other "label-compatible" words. Note that the comparison is not intended to necessarily show that the proposed method outperforms these baselines. Rather it is intended to guide the reader into making a decision about which method is more appropriate for which problem. If the findings are that the performance is comparable but one method will eliminate the additional finetuning step, that would be a useful finding to share. Also , it is not clear that  these methods would require labeled data in the target language for the finetuning or not.  For example, can the source labeled data be used for the finetuning step?- Regarding translation: Cross-Lingual transfer via Machine Translation does suffer from the label transfer problem for sequence tagging tasks (transferring labels for sentence-level tasks is straightforward). However, there has been several methods to address this in the literature by using unsupervised word alignment  (e.g., Yarowsky et al., 2001; Ni et al., 2017) or attention weights from NMT models (Schuster et al., 2019), heuristic approaches (e.g., Ehrmann et al., 2011) or co-learning alignment and tagging (Xu et al., 2020).A detailed comparison and discussion of the trade-off between the performance of each method and the resources they require would make the paper much stronger  ------Response to author's replies:I am impressed by the detailed response and the changes they have made to their paper and I am happy for this paper to be accepted. I still feel like the XLA framework is quite involved and it would have been good to understand which components of this framework are crucial to its success which is why I do not increase my score.  ---post-discussion update----I would like to thank the authors for preparing the rebuttal and attending our discussion. However, I still think the complexity is a concern of this work. I do not think that Eq. (3) can be implemented within the complexity that the authors claimed. Moreover, if the authors use another way to compute the attention scores, that way should be very clearly stated instead of written in a different form. Given the high complexity, I cannot clearly see the advantage of this work in comparison to [1], as the non-local attention has been proposed in [1] already. Final reviewThe authors have addressed some of my concerns so I am updating the rating. I still feel that the mode's novelty is limited and thus my highest rating will be 6.  After The Rebuttal: I really appreciate the rebuttal and revisions submitted by the authors. They did a good job of detailing domains of interest where their approach may be applicable. I also agree with some of the points they made about the complexity of domains that they considered in their experiments. As such, I have revised my score and now lean towards acceptance of the paper.  I have read the authors response and updated paper and appreciate the discussion of applications that admit factor graphs. Post-rebuttal ------------------I appreciate the response and the manuscript update, but some of the comments have not been addressed such as the code which has not been provided, and the fact that the results are still very inconsistent like R2 mentioned although the arguments made in this work seem clearer to me. But it is still not clear to me how the results can be used to improve current methods for identifying adversarial examples with smaller distortion and less queries. Thus, I will retain my score as is.  ----------- Update after revision ---------------------Dear Authors, Thank you for the revised manuscript. The expanded sections have helped me understand the paper more deeply, and place the contributions in context to prior work better than before. I especially appreciate the new experiments and explanations on the Robust Manifold Defense. A few comments on the revised paper: * First, the success rate plots which were missing are quite revealing: While the biggest benefit of the reduced resolution query search appears to have been in robust models (Madry et al), the benefit is not reflected as clear in terms of the success rates (which is arguably more critical). For example, in CIFAR-10 experiment in Fig 2(a) while in the Natural case, BiLN+HSJA is slightly more query efficient it appears less successful than plain HSJA in Fig 6(a). (Once again, the plots and figures are extremely illegible). * For "Robust" CIFAR-10 models and "Natural" ImageNet , there appears to be a considerable gain in both $L_\infty $ distortion *and* success-rate.  For the "Robust" ImageNet case, while there are *huge* gains in terms of $L_\infty $ distortion, there is essentially no discernible difference in the success rate between reduced-resolution and the original attacks. * by the way there appears to be a typo in Fig 6(a): both BiLN+Sign-OPT and AE+SignOPT are the same color in the plot* In the new Figure 4 with MNIST experiments, in all the experiments, the base attack appears to be more query efficient than the resolution reduced attack. For a few it appears that before 5000 queries the reduced-resolution are better, but claiming this supports the earlier results isn't as convincing.  * The observations from these plots are confusing: >"The hard-label attacks consistently outperform against the Robust Manifold defense": Again not clear how it is claimed that the attack is outperforming the defense without mentioning success rates for this experiment. This also does not clearly outline why the reduced-resolution variants to not match up to the base attacks?* The reconstruction error plots are also very hard to see -- once again all the reduced-resolution suffer in these experiments. The explanation given is that:> "Due to the dynamics of the topological hierarchy discussed before, the dimension-reduction is less effective on MNIST" This is another example of very heavy terminology that makes it hard to understand what the argument is exactly. Overall, I think the paper needs more clarity in its experiments. The story is muddled by several inconsistent results, some showing improvements with reduced resolution (the finding is surprising and genuinely interesting.. but unfortunately doesn't seem to be consistent; especially when looking at it with the success rate metric)I think the authors have a convincing motivation and argument that query-efficient hard label attacks achieved with reduced-resolution can be closer to the manifold, this is clear in Robust CIFAR-10, Natural ImageNet models. The paper does a good job of explaining why these are the case, unfortunately the hypothesis doesn't hold up just as well in the other scenarios, which raises the question as to how these can be used in a more realistic scenario -- and more importantly as one of the other reviewers also pointed out -- what the reader must take out of these results to appreciate query efficient attacks better. I think in its current state the experiments portray a more confusing picture than the arguments in the paper suggest. As a result, I will retain my original score.  Update after rebuttal: After seeing the author response below, no change to my score. Update after rebuttal: the authors have clarified some of my concerns, and I have therefore increased my score.--------------- ~~I'm updating my rating to accept the paper due to the comments and updates on the paper.  The proposed flexible usage of the GMM is novel from the existing literature.  The changes in the paper improved its clarity, and the contribution is better presented in contrast to existing work. - In general, my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient.  - I also agree with AnonReviewer3  and AnonReviewer4 but feel less excited about the prons and more worried about the cons. At this point, I'm not against the acceptance of the paper, although I'm still staying on the rejection side. I'm increasing my score because we are at least talking about a borderline.-------------------------- Update after the author response: I've read the other reviews, and agree with R2 and R3. I think the paper is useful (emphasizes you need to calibrate the final ensemble, not enough to calibrate members), and has some nice conceptual contributions (explaining that if ensemble accuracy > average member accuracy (which is usually the case), and the ensemble is calibrated even in just a global/weak sense, then the members must be uncalibrated). This could spur more research into conceptually analyzing ensembles, and seems interesting. But I understand the other reviewer's concerns that it's not clear what practical impact this will have, so I'm keeping my score at a 6 (instead of raising to a 7). ==== Updates after the response ====I appreciate the authors effort in updating the manuscript. The new manuscript has significant changes, but still many questions are left unanswered. Thus, Im keeping my rating and recommendation. **Post-rebuttal**After reading the rebuttal and the other reviews, my rating remains the same, although the rebuttal addresses some issues. The added videos show good temporal coherence as previous methods, and several writing issues have been resolved. However, I still feel that the contribution of the proposed method is not significant enough. It would be more convincing if the paper can show that the proposed augmentation technique is effective for more applications. Also, I still think that the visual improvement is very subtle. For the six added videos, only fig_a2_label.mp4 shows clear improvement visually.  ====================================================================================================Update: Thank you for your new experiments and detailed feedback. Most of the concerns have been addressed and the experimental results look better. I believe this paper will provide new insight into efficient neural network training. Thus, I raise my rating and recommend this paper to be accepted. =====================================================================================================Added after author response----------------------------------------I believe authors have clarified many things I asked and addressed the issues I and other reviewers raised. Therefore, I increase my score. I believe the idea of using LSH for efficient training has a lot of promise and this paper brings a possible way to do this into light. I read the authors' feedback and appreciate the clarifications. I keep my score.  ----------------------After rebuttal:I thank the authors'  clarification. I keep my rating. ---------------------After reading the author response and the feedback from the others ------------------------Thank the authors for their response.  I still think the assumptions made in the paper are strong.  For example, the assumption in equation 2.3 and 3.3 is hardly true. Even in the revised example 2.6, the missing rates might also depend on other factors. It is hard to identify every related attribute in general. I am still not convinced that "machine-learning practitioners are unwilling to sacrifice the performance of their ML model for fairness". First, there might be many other possible reasons that some machine learning applications do not incorporate these fairness considerations. For example, there is no consensus on what fairness constraints we should achieve. In fact, existing works show that considering fairness might has bad effect on protected groups (e.g. Liu et al Delayed impact of fair machine learning). Machine-learning practitioners might just be confused what to do rather than unwilling to do. Second, I have seem many applications that have fairness incorporated. (e.g. Search Google image CEO, we now see many female CEOs on the top). From my perspective, responsible businesses would like to pay extra money to obtain a fair ML solution for the benefit of the society and the reputation of the business as long as they know what to do and how to achieve fairness. So, I would like to keep my score.  Update:==================================Comments for Area Chair and Reviewers: If I view this work merely by the story conveyed in the paper, my assessment would be more in line with the other reviewers. I am not quite sure if this is the right way to evaluate this paper since I view it as having a broader impact that goes beyond the story in the paper, but other reviewers disagree with my view on its broader impacts. I see this as a sign that the paper is currently not in a good enough state to really convey its potential impact.Comments for authors:I believe you still did good work here on the merits of the "speedup training" story that you convey in your paper. I believe that you have much more than this in your hands though. I think you could go two ways from here: (1) get this paper accepted in this form and work closely on the other angles that this work offers in a new paper, e.g. learning which is in line with biological or efficient parallelization of large networks. The second way (2) would be to rewrite this paper more in line with that view and resubmit. I think (1) might be better for you. I do not think many reviewers would understand a paper that comes from the process in (2). Good luck! ### UpdateThanks to the authors for their responses, particularly to the other reviewers who were more critical.  I still believe this is worth publishing if there is room, with the caveat stated before: this is an implementation paper, and does not introduce new algorithms or analysis.  If it is accepted, it should be accepted on this basis. After rebuttal:The authors' rebuttal addressed all my questions and I upgrade my rating. **After rebuttal**The authors have gone above and beyond in providing additional experimental results. All of the points raised above that deal with methodological issues are completely addressed.The sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions. I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method.  ##### Post-rebuttal updateI've read the rebuttal and updated my score.--------------- _**POST-REBUTTAL**Sadly, I'm decreasing my score a notch because the authors lack a deep understanding of MIR [1] that would make it evident that GMED is *much* closer to the three methods proposed in MIR. Specifically, the authors have responded to my concern about the lack of novelty:    In GEN-MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The generator is indeed optimized for maximizing the forgetting, but the forgetting here is measured for the generator - i.e., the generator retrieves most forgettable examples for the generator itself. The approach does not learn a generator that can generate examples that are more forgettable for the classifier; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.This is simply not true. If you take a look at Equation 2 in MIR, you will see that the generator is generating forgettable examples **for the classifier**. It also uses it for itself, see Equation 3. GEN-MIR is thus a gradient-based memory editing for CL.        AE-MIR (...) It is a hybrid approach of example compression and ER-MIR. There is no online optimization towards more forgettable examples for the classifier like GMEDAgain, just like GEN-MIR, AE-MIR uses gradient-based memory editing for CL for the classifier.In GMED lies somewhere between ER-MIR and [GEN-MIR, AE-MIR]. My guess is that the MIR's author didn't propose the GMED method because it doesn't make a lot of sense to update edit a sample **and not edit its label**. Here is an intuition on the behavior of each method: let's say your models sequentially learning to visually classify objects. The model is now learning about zebras and it's causing some interference on the horse's representation.- ER-MIR will search in its buffer and retrieve a horse for the classifier to do replay on.- GEN-MIR will search inside the latent space of a generative model to find generated horses for the classifier to do replay on.- AE-MIR will search the latent space of an autoencoder to retrieve past horses that appeared in the data stream for the classifier to do replay on.- GMED randomly samples some data in the buffer, e.g. a car, and takes one gradient update on the car such that it resembles more a horse. Then the classifier is fed that modified image (i.e. x) as well as the unchanged horse label (i.e. y)The empirical section shows us that one needs to add MIR to GMED to obtain the best results. This comes as no surprise. Combining methods with each other and increasing computing needs and/or replay will give you a better performance on forgetting._________ ---## Post-rebuttalDuring the rebuttal period, the experiments largely improved, resolving the majority of my concerns. I raise my rating accordingly. I expect the authors to reflect other suggestions as well in the final version. Especially, R1 raised a serious concern about the similarity between GMED and GEN/AE-MIR. A proper comparison should be included in the final version. **Final recommendation**I do not recommend accepting the paper. The results have been greatly improved. They now look decent and I have improved my score as a result. However I think the contributions are still not clearly highlighted. I however encourage the authors to improve their paper.  === Post Rebuttal Responses ===I thank the authors for their replies and the corresponding new revision. After reading them and the other reviews carefully, I changed the rating accordingly. However, I still lean to reject this paper for the following reasons.1. When saying "our method outperforms something," the standard deviation and the mean should be considered (such as confidence interval or statistical test). However, the authors seem only to consider the mean in the paper and the other replies. In this sense, the proposed method does not outperform CQL. Besides, the variances of CQL in halfcheetah-mixed and hopper-mixed are required to conclude that the proposed method outperforms CQL in the mixed setting (e.g., the variance of the proposed method in 'hopper-mixed' is high).2. I agree with the other reviewers common concerns on the novelty / main benefit of this paper.3. The supplementary code can be shared with the reviewers using an anonymous link, as explained in ICLR Author Guide. It would have been better if the authors used this functionality of official comments during the review process for better reproducibility. Update: Thanks to the authors for addressing my comments and releasing the source code. Code is well-structured and easy to follow. It can be definitely used as a supplement for the robustness evaluation. However, judging by the implementation, the rain, snow, and fog attacks are too simplistic. From the paper, it is not clear how well these attacks approximate the respective type of perturbations in real-world scenarios. Therefore, the proposed set of 6 attacks is more the author's heuristic than something that can be used for the evaluation of computer vision systems, e.g. autonomous vehicles. Given all that, I decrease my score from 5 to 4. # UpdateI thank the authors for their thoughtful reply and for incorporating the feedback.  The additional information is most welcome, and so I maintain my score of 8 for the paper. -----update: thanks for the clarification points + figure 9 :) I still recommend acceptance (score of 8). **Response to the last comments made by the authors**The referenced paper (https://arxiv.org/pdf/1906.04933.pdf ) is not answering the question I have. Nor does section 4.2.My question is: does ALS really improve the quality of uncertainty?The paper's answer seems to be no.Look at Tables 1, 2, 3, and 4 in the revised paper. It is great that ALS achieves a lower O.conf (overconfidence), meaning that for wrong predictions, ALS helps to produce lower confidence values. However, this comes at the cost of higher U.conf (underconfidence), meaning that even for correct predictions, ALS makes the model produce low confidence values. A confidence measure that always produces a low value, regardless of whether the prediction was correct or not, is not useful.The authors may say "look at Table 1 - our uncertainty measure produces lower scores for images with objects removed". While this is true and it is a good signal, object removal is only a particular case for introducing uncertainty in an image. Eventually, I believe a good uncertainty measure should first of all produce a good ranking of test images such that the correctly predicted images are ranked first (this paper does not quantify this). Then comes the question of calibration (like ECE and MCE). Then comes the evaluations with specific uncertainty scenarios (like OOD images, object removal, or other controls on image uncertainty). ----------------------------------------------------------------------------------------------------------Post rebuttal:Thank you for your response. After reading the other reviewers' comments and your responses, I think the paper is not yet ready for publication. All reviewers are concerned by the lack of a technical contribution and the limited benefit of the paper. While I appreciate the effort of the authors to answer our concerns, I think the paper needs a major revision that incorporates our shared concerns. Therefore, I retain my initial rating. ====After rebuttal: Thank you for addressing my concerns. - I believe the claim of equivariance leading to improved generalization (weakness 1 above) has now been validated more directly. - Regarding the validation of sample complexity claim: a more direct investigation of performance against training set size, for equivariant and non-equivariant models, would be more convincing. I understand that augmenting data does increase the training set size by a factor of 3, but the training set is then not iid so it's not clear what the "effective size" of training set is. In any case I do believe that equivariance reduces sample complexity.Overall, I vote for accepting. After rebuttal: They authors have addressed my questions. I hope that the authors will carefully review classical literature on weighted minimax linear estimator in their final version. I raise my score to marginally above acceptance threshold, since I am still not convinced in the importance of domain adaptation for fixed design regression.  _[Edit: The authors clarified that the flaw I was talking about isn't there. I updated my review accordingly. The score is based on the fact that the paper can use some polish to improve clarity and the fact that some of the assumptions, even if traditional, are too strong (in essence, sample complexity bound will become meaningless if what is assumed known itself is too sample-intensive to approximate).]_ **After Author Response and Discussion:** Thanks to the authors for their responses. After reading the other reviews and the author responses, I am raising my score to 7 (accept). -----------------Post-rebuttal review:I carefully read through the rebuttal and other reviews and  I would stick to my current rating.1. Cons 1: The rebuttal does not provide sufficient explanation on how the magic numbers are chosen (though it is somewhat explained in the rebuttal, it looks more like a design and lacks experiments to back it up: why these numbers but not other numbers?)2. Cons 2: I would suggest the author(s) further improve the evaluation metrics to make it more objective and convincing.3. Cons 3: I believe the 2.68m difference in detecting cars should be regarded as a very large error (under IoU 0.5 metric, it would be counted as a misdetection) and should be handled properly.I think the approach presented in this paper is interesting, and I encourage the author(s) to do more analysis to make it more solid. Update after rebuttal: I thank the authors for their responses to all my questions. They satisfactorily answer some of my concerns. However, I still have two major concerns: 1) the faithfulness of the proposed approach, and 2) I see the potential contribution of uncertainty saliency maps but without an application/evaluation, their significance is unclear. I disagree that uncertainty/confidence generated from the same mechanism that generated the explanation is more trustworthy than the explanation itself. Hence, I cannot recommend the paper for acceptance. # UpdateThanks for the authors to address my questions. However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept. ### UPDATEI thank the authors for addressing my concerns and confirm my initial rating. ## UpdateThe authors' response addresses some concerns, and I would like to keep the initial scores.  I appreciate the authors' efforts to fairly compare to Zhuang et al. in the rebuttal, and I do find the preliminary evidence sufficient to establish that their approach outperforms LA on clustering metrics. However, the authors seem to miss the the point that LA is not just another consensus clustering approach which they forgot to include into literature review since it does not report clustering metrics. The main contribution of their work is combining consensus clustering with representation learning, which is exactly what the authors of LA had done before. It does seems that the particular approach proposed in this paper results in a better clustering performance, so the submission contains a valid contribution, but the relationship between the two methods needs to be discussed in a lot more detail, and they have to be throughly compared experimentally. I encourage the authors to improve the manuscript in this direction and resubmit to a different venue. ## Post-Rebuttal UpdateSince many of the concerns from my original review would require a major revision to address, I've tentatively left my original review score (3) unchanged. The biggest reason is that I'm still concerned that insufficient hyper-parameter tuning could lead to wrong conclusions; this would need more work to address. I've included some additional notes below.In their response to my review and those of the other reviewers, the authors promise to address a number of issues with the current draft. While I hope these updates will ultimately improve the paper, the authors' current responses don't provide me with enough information merit increasing my ICLR review score. For example:* "We will try to auto-tune the hyper-parameters of different predictors in the future."* "We think we can improve Figure 3c" by measuring mean/variance across multiple runs.On a positive note: I'd like to thank the authors for clearing up my confusion about ranking diff under "additional notes". I also think the authors' explanation of their isomorphic sampling procedure in the response to AnonReviewer2 addresses one of my questions. ### Final Recommendation after Author Response ###I have read the author response and appreciate their feedback. As the authors could not address some of the issues (error bars/statistical significance testing, other hyperparameters and other datasets/benchmarks) in the restricted time of the rebuttal period, I will keep my rating and recommend rejecting this submission for ICLR. However, I also encourage the authors to resubmit a revised version of the paper taking all feedback into account since I see clear potential. # Post RebuttalI thank the authors for replying to my question. Unfortunately, the authors did not present any new results on other benchmarks (e.g Nasbench101-shot).  I will therefore keep my score. == After Rebuttal ==I had previously missed that Lemaire et. al. had some of the key comparisons (simple baselines done for the same architecture/dataset pairs) that are necessary to judge this method. I thank the authors for pointing this out. It is also helpful that these baselines are now included for completeness.C.6 still seems like an indirect measure of optimization stability, but it is reasonable to conclude from it that the differences from BAR are real, given the very low standard deviation in accuracy.I remain on the side that this is still a relatively incremental addition to the pruning literature, but I am now satisfied that it is well-validated. =======================================================================================================After rebuttal : Sorry for the delay. The authors address my concerns and reflect them on the revised version. I'm keeping my rating. ### UpdateThe authors have not sufficiently addressed my comments regarding the choice of alpha in practice. While a possible strategy was proposed, it has not been evaluated empirically, see comments below for more details. I am thus maintaining my score. **Final Update**After reading other reviewers' comments and authors' responses, I decided to choose to reject the paper at this time for a few reasons.1. I am afraid that the impact of the paper won't be good with only one out of the four reviewers understanding a paper meant to present an entry point benchmarking dataset.2. I agree the binary, pseudo sensitive attributes, such as stripes, make the benchmarking and the analysis more manageable. However, as a future user of the benchmarking dataset, I see that there still remains the question of whether the metrics measured against the proposed benchmarking dataset with simple pseudo sensitive attributes can translate well to the model performance measured against some real world dataset with real sensitive attributes. It will help to add the best or chosen model's performance against some real world dataset with real and more complicated sensitive attributes.3. Add something like what is proposed in #2 to help the readers understand how to choose the best model and make tradeoffs for a given scenario using the presented dataset with the proposed methods. The goal would be somewhat similar to that of the "INTENDED USES" section of the LFW dataset presented in http://vis-www.cs.umass.edu/papers/lfw.pdf.Nevertheless, I believe the authors' core work is in the right direction. Just need some way to organize and present it better to make a bigger impact. Thank you for your work! #Post rebuttalI appreciate the authors added a deeper discussion and analysis in their newest uploaded draft, which helps clarify some of my concerns. However, it would be better if the following can be better addressed:- As a benchmark paper the experimental setting is relatively simple/artificial, more realistic datasets/settings would be more useful;- Thanks for adding the stability study. It would be more helpful if the main results can be presented with a better control over the stability. Currently it is still hard to tell the statistical significance of the results.- As the other reviewers have mentioned, it is still unclear how this simulated setting connects to real-world fairness applications. +++ Points after discussion about Meta-RL, HiP-MDPs, and MARL.Like Reviewer 4, my view of MARL is more general than requiring all agents to be learning, even if most work does have one algorithm training multiple seats. I don't think this actually changes anything in the current draft, but should hopefully not appear in future edits.The related works should include discussion of the HiP-MDP paper.These points fit together: there should be a consistent placement of this paper, and related works. Given fixed opponents, the multiagent problem is equivalent to a single agent problem. It doesn't seem relevant whether or not the unobserved, unknown variables correspond to different but similar environments, or different but similar opponents in a single fixed environment.+++ ----------------------I've read the other reviews, the author's responses, and the discussion - thank you everyone.  I'm still in favour of accepting the paper. **Update**: Revised score from 4 to a 6, mainly because the authors rightfully pointed out that they did have an ablation study in their paper which demonstrates the efficacy of their proposed methods. ==================== After rebuttal =======================Thank the authors for their detailed responses, which have addressed my concerns regarding Eqs. (10) and (18).  Thanks for the efforts of the authors. Some of my concerns have been addressed. However, I think that  the paper should test SDL layer by plugging in any hidden layer. So I keep my score._____________________________________________ -----------After the rebuttal: I would like to thank the authors for their response and clarification. The argument that the proposed strategy minimizes an upper bound is, however, somewhat weak, because $\|B(h - y)\|\leq \|B\| \|h-y\| $ is a possible upper bound for any matrix $B$, but using the right-hand side as a replacement for $\|B(y-y)\|$ within an optimization problem can make a large difference. I would expect to at least verify a marginal difference in the results numerically. Moreover, I do not think that the argument that some dictionary learning algorithms are slow is valid for omitting a comparison to a simple baseline denoiser at all: Your denoising baseline could be as simple as thresholding DCT coefficients or applying a median filter - operations that are surely as fast as the proposed approach (and still represent a sequential pipeline of first denoising and then applying a network, which one can expect to work worse than the proposed approach). Therefore, I keep my score and do not recommend this paper for acceptance in its current form.  After response: Thanks for the clarifications especially the piecewise linearity of deep network as a function of input and not as parameters (like I incorrectly suggested in my review). I think the empirical results have some merit, but are preliminary and I fail to see the bigger picture. For example, more carefully designed ablation studies are needed to determine the practical utility i.e., when the approximations suggested are reliable and when they are not. *** Post Rebuttal: Thank you so much for your response and for the codebase link. Your additional experiments are convincing for group influence and overparametrization.  I sustain my recommendation to accept this paper :) *** After reading rebuttal, I think the paper is somewhat improved hence I increase the score.  brief update after author response============================== I decided to maintain my rating due to several key claims in the response are unverified. For example, in Q1 "We propose that NOT ALL users (but only active users) should use transductive embeddings to memorize their history, which is different from existing methods that treat all users in the same way.", however, comparison against them is missing, especially for methods like [1] and [3] that uses both inductive and sequential embeddings; in Q2 "Trivially cutting off the sequence could result in worse accuracy", however, this is not verified: not clear what's the effect of cutting off on baseline models. I believe these are the core research questions that need to be verified in the paper, which are unfortunately missing. [No rebuttal given by the authors] Score unchanged. Comments after the rebuttalThe author's response resolves my concern in part, and I will keep my positive score. After rebuttal: I thank the author for their response and have raised my rating by one after reading the author's response, but my concern still holds unfortunately regarding the whole approach that the author has taken to analyze the effect of data augmentation. Therefore, I'm still not confident about my rating. **Post-Rebuttal Comments.** Thanks for the author's response and their dedication during the rebuttal period, I re-read the updated version of the manuscript and authors did an effort for adding extra content and editing based on the questions and recommendations. In my particular review, they answered and solved the questions that I did. I understand the points addressed by the other reviewers and the theoretical limitations of the method. However, I still have a positive opinion about the paper, and I believe that the aforementioned limitations are well indicated in the paper, something that is valuable. For these reasons, I keep my score. -- post rebuttal --After carefully reading the authors response and the discussions with other reviewers, I am updating my ratings. The authors acknowledged that it is not a theory paper and therefore, the biggest concern is empirical evaluation. The shown performance is not comparable with existing high performing methods and therefore it is hard to judge without any direct comparison. The authors stated that their method can be applied on top of any existing method, which was not shown in this submission and therefore it will not be meaningful to judge just based on this statement. I believe showing this empirically will definitely strengthen this submission. -----------------------------Post Rebuttal ModificationRegarding A1: I agree with R2 that the theory has major concerns and the authors were not able to fix it during rebuttal. I think we need to be clear that whether the method can work empirically and whether the provided theory can explain it are two problems. Now it seems to me that it is clear that the theory is wrong, and the problem is that the authors did not take into account the difference of the class-wise labels and pairwise labels. I suggest the authors to change the theory completely or remove the theory before next submission.Regarding A2: I don't chase SOTAs and I could certainly appreciate works that give nice theoretical insight but limited improvement. Now that the theory is wrong I have to be critical about the experiments. Since the performance is much worse than STOA, it is no longer clear whether the proposed algorithm works or it's just because the baselines are too bad.I adjusted my rating from 5 to 1.-----------------------------Regarding the authors' 2rd and 3rd responsesFirst, please allow me to clarify that my wording "the theory is wrong" means the theoretical justification on why the proposed algorithm can benefit from the transformation and achieve better performances is wrong, as the authors wrote This theoretically justifies why the proposed method works well in their submission. The major flaw/concern has been raised by R1(Q1) and myself(Q1), and the authors responses on these two questions are not convincing. This is the concern that I have been asking, so I assume it is not vague. I dont see any potential way to fix this major concern in the current theoretical justification sketch, so I think this submission needs a major revision. I want to give my apology if my wording "the theory is wrong" leads to misunderstanding to the authors or other reviewers.Second, I would like to see ACs or PCs to step in and let me know if I could rate the submission as 1 in this case. I have temporarily increased my rating from 1 to 3 as it has been questioned by the authors, especially the author who have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR.Whats more, I would also like to request apologies from the authors. The wording angry is unpleasant and misleading. As the author asked, what are you angry for?, Im not angry at all. I simply adjusted my post-rebuttal rating with my expertise after reading the authors responses and other reviewers comments.Finally, I would like to remind the author who have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR, one of the main rules of academic writing is to avoid using second person. I hope this will be helpful.Best ---------------------I thank the author for clarification. I would like to be with my score. -------------------After rebuttal:I thank the authors for clarification. I would like to keep with my score. **Post Rebuttal**I had concerns regarding the clarity, theory, and experiments. During the rebuttal phase, the authors actively discussed various points raised by the reviewers and the AC. Despite its length and breadth, I do not find them addressing the core of the raised concerns. That is except my 2nd point of the major theory concerns, regarding the time complexity and convergence time which is at least partially addressed.  The time complexity is addressed since the length of the first round of training for transition matrix is negligible compared to the main training. The convergence time would also be addressed if the number of epochs for the proposed method is the same as the baselines. The reviewer is not entirely sure that is the case though. Given the outstanding majority of the concerns my final feedback is as follows:The paper provides an original idea for learning with label noise which is positive, to rate the demonstration of the relevance of the idea, I can either consider the paper from an empirical study lens or a theoretical one. From the latter perspective, the concerns above effectively affect the whole theoretical arguments of the paper. The main claim of the paper, even in the latest revision, is based on the noise rate of similarity labels being lower than the noise rate of the corresponding class labels and that this is what can bring improvement in the final performance. This reads absolutely unfounded to the reviewer. As discussed, the similarity rate is mostly influenced by dissimilar labels (in the balanced c>2-classification scenario) and can be made arbitrarily low. Furthermore, the discussion still does not make a clear formal connection between the error bound on the noisy similarity learning and the noisy classification for the general case. Nevertheless, such a connection would require a major revision/addition to the paper that would need a proper round of review.When it comes to the empirical view, the experiments are inconclusive and not thorough-enough for an empirical paper due to 1) the drastic change in the learning setup (which is implemented inhouse including the base transition-matrix methods) in tandem with the fact that hyperparameter optimization is not done per method (e.g., the hyperparameters are taken from the papers for baselines while they are optimized for the proposed method's training). This is especially important since when learning with noisy labels, the choice of hyperparameters are extremely influential in the final results. 2) the improvements are mostly marginal except for CIFAR100. 3) Results are not provided for the original Clothing1M dataset. The shortcoming that led to changing Clothing1M needs to be thoroughly studied for an empirical work since it directly affects the applicability of the paper.On top of these, the final version of the pdf is still lacking on clarity several instances of which were listed in the original review. Thus, considering all the points above leads me to confidently keep the original rating as "clear reject". (Number rating was updated from 3 to 7 in light of substantial expanded experiments and analysis.) #### Update after author response and other reviewers comments:I think after the addition of new evaluation (Appendix A.5) on the aspect of counter-factual and other comments made by reviewers I'll stick to my score. Also, I liked the idea of applying counter-factual to long-tailed distribution IE problem. ## After RebuttalThank you for addressing my questions. Since the performance improvement is still marginal and based on the other reviews I have decided to keep the same score. ----After rebuttal: My main concerns about (a) no counterfactual model and (b) the linear / nonlinear requirements of the method remain. Generally, bias assumptions are made about the data, not the output of a representation learning procedure.  "The nonlinear relationship between raw input with the outcome can be encoded into the learned embedding" yes, but it does not mean H_S and H_V will meaningfully encode anything related to the input bias in any meaningful way. The method needs to precisely describe the structural causal model to be properly evaluated.  ------### Post rebuttalGreat thanks to the authors for the detailed replies. After reading them, I decided to keep my rating. Revised rating to 7 after revision. Update after response: I appreciate the authors making their contributions clearer, and adding details about the training loss and error. I have increased my score accordingly. -------Update after rebuttal:I appreciate the efforts of providing a hyper parameter study. Thanks for the clarification about dataset used in the paper.  I would like to increase my rating from 4 to 5.  Since the proposed method is somewhat ad-hoc (shared concern among other reviewers), either experimental or theoretic analysis is important to understand when and why it works. However, I don't think these analysis are sufficient in current form.  Thank you for the clarifications.I did not change my rating, since I am unclear how the proposed method compares to SOTA beyond CIFAR-10/SVHN.Table 14 suggests that Likelihood Ratios is considerably better than the proposed method.\nFurthermore, neither in Table 1, nor in Section 5.4, I can find any results of the Likelihood Ratio method. ###################################################################Post Discussion Score:The authors did not submit their rebuttal. After reading the comments from other reviewers, I decided to keep my original score for this paper. --- post rebuttal --Thanks for the response. I think the contribution of this work is solid and I vote for clear acceptance.  After rebuttal: It is highly appreciated that the authors provide additional evidence in response to my reviews. My previous concerns about effectiveness and efficiency are addressed to some extent, however, this also means the original submission needs significant modification to address the concerns. I like the idea in general and the problem is well-motivated but it needs more work for a complete version. I would encourage the authors to further improve the quality of the paper. ##Updated Review## I'd like to thank the authors for their response and am looking forward to seeing the result of their edge detection comparison.  I maintain my review of this paper. -------The rebuttal did not meaningfully addressed my concerns.Apologies to the authors for not providing a reference for my comment on approaches for reducing communication in graph-optimization methods being widely known in industry. GraphLab is an example (https://en.wikipedia.org/wiki/GraphLab)- Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin and J. Hellerstein. GraphLab: A New Framework for Parallel Machine Learning. In the 26th Conference on Uncertainty in Artificial Intelligence (UAI), Catalina Island, USA, 2010- Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin and Joseph M. Hellerstein (2012). "Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud." Proceedings of Very Large Data Bases (PVLDB).- Joseph Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, Carlos Guestrin (2012). "PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs." Proceedings of Operating Systems Design and Implementation (OSDI). Post-rebuttal:My concerns are not addressed, so I would like to keep my original score.------------------------------- ** after reading rebuttal**Thanks for clarifications and an improved version. I decide to increase my evaluation to 5.However, I think that the paper needs to take more content to illustrate the practical benefits of the proposed CSG frameworks, for the following reasons: - the framework is proposed based on empirical observations like 'intervening an image by e.g. breaking a camera sensor unit when taking the image, does not change how the photographer labels it', which is not mathematically rigorous; - the principles and assumptions are rather strong (though I understand one generally has to make assumptions in causality), and in practice it is not clear when such assumptions hold and how many applications satisfy these assumptions; - the interesting derivations and theorems are also based on the CSG framework, which means, if the framework is incorrect, then these results may fail; - the experiment settings are rather limited in the current version. I hope the authors to add further content in their next version, regardless of whether the paper gets accepted or rejected.Lastly, I still feel it a bit tricky to change the original formatting in the previous submission. ======Comments after rebuttal: I know the authors develop two components for FSL, my concern is that these components are incremental and have limited novelty. However, I admit this paper is a high quality paper in presenting its idea, organization and empirical evaluation. Hence I increase my score to accept now.  Post-comments to the author's response:After reading the other reviewers comments and the authors rebuttal, I am still concerned with the novelty of the approach, experimental evaluation, and performance improvements over previous work. These issues are pointed out by the other reviewers as well. Hence, I will go with my original decision of rejecting the paper. #### UpdatesIn light of the responses provided by the authors, and the thorough nature of the additional experiments I am inclined to increase the ratings of this paper. The followup responses answer most concerns raised by reviewers and provide supporting evidence. However, the reviewer maintains that this work could potentially be of more interest to a privacy audience who may better be able to appreciate the use case considered in the presented work. --- Updates ---After reading the authors' response, I decide to raise my rating to "accept". Update:Thanks for all the authors' feedback and the make-up experiments as well as revisions. I appreciate the authors' efforts on solving the privacy problem and new input forms, but I really think the paper made no contributions to 'learning representation'.  The VAE and GAN-based model is not problem-specific or novelly designed for the task. Instead, it is a general framework for any reconstruction problems. I do believe this will make a strong submission to other conferences related to information. In this case, I will not change my rating. Update (2020-12-03):  Increasing score from 4 to 5.  -------------Post-rebuttal: After reading the rebuttal and the other reviews, I have decided to lower my rating. The rebuttal appears to state that all baselines and evaluations suggested by the reviewers are irrelevant to the goals of the paper. I see the two main hypotheses clarified in the rebuttal, but I do not see a convincing argument for avoiding comparisons to related approaches to the task at hand.  Post-rebuttal response-----------Thank you for the detailed rebuttal responses.  The rebuttal suggests that there are not many other baselines against which to compare.  If that is true, I would want to see much more detailed analysis of the comparisons offered in the paper.  However, I am still missing details of the latent space generated by the task-unaware approach.  I will leave my score as it is. ------Post rebuttal:After reading the author's response as well as the opinions from other reviewers, I will stick to my original rating. Although the authors resolve some of the concerns in the rebuttal, there are still limitations in the method and task design. --- EDIT POST REBUTTAL ---I thank the authors for their response. I have read other reviews and answers too. I have appreciated the revisions (especially the proof for ReLu with biases). I am still a bit concerned by the applicability due to the assumptions on the architecture/data generation process,  but will not fight acceptance if other reviewers feel strongly about it (I changed my score from 4 to 5 to reflect this position). I understand that authors do not want to present unfinished/ongoing work, but mentioning the parallel with kernel methods (at least as a research direction) would seem fair to me. === UPDATED ===Given that the reviewers have not reached a consensus, I want to add more discussion to my review to facilitate the AC to make the decision. I would also include a few more quick TODOs for the authors and hope they can help add evidence to my argument.1. This submission proposes to replace the propagation in GNNs with heat kernel. The main motivation for this method is that the laplacian filters tend to oscillate, as illustrated by Figure 1. The heat-kernel provides a continuous convergence process and intuitively may address the oscillation process. Importantly, I believe the motivation of this method is *not* to prevent oversmoothing but to prevent over-oscillation. I believe this intuition is sound but encourage the authors to do more to validate this hypothesis. I appreciate the ablation study in Figure 6. As one more analysis, I suggest the authors to add the performance curves of SGC to Figure 6 (under the same setting). If this theoretical intuition is valid, we should expect HKGCN and SGC to behave more differently when the propagation degree is low (within 2-10). My understanding is that both HKGCN and SGC are efficient so this experiment shouldn't take long. Ideally the authors can update this result, at least on 1-2 datasets, within the discussion period.2. I am also very impressed by the efficiency of HKGCN. This submission has experimented with, to my knowledge, the largest publicly available graph dataset (arXiv), which contains more than one million nodes. According to the authors, HKGCN can be trained for this arXiv dataset in 48.5s, which is impressive. Note that here the heat diffusion matrix does not need to be computed/stored explicitly. Following the setup in SGC, HKGCN only needs to compute the propagated features in a preprocessing step.  I suggest the authors to give more concrete numbers to illustrate the efficiency of this method. For this arXiv dataset, what kind of hardware is required? How much actual RAM did you use to compute the preprocessing step?3. After reading other reviews, I now realize that this submission is not the first to introduce heat kernels into GCNs. Among the papers pointed out by other reviewers, I find [1] to be most relevant in that they also proposed the usage of heat kernels. Can the authors also clarify the difference between this submission and [2]?Based on this novelty concern, I have lowered my review score to 6. [1] Xu et al, graph wavelet neural network, ICLR 2019[2] Xu et al, Graph Convolutional Networks using Heat Kernel for Semi-supervised Learning, IJCAI 2019.  ---After rebuttal:Thank you for the revision and the clarifications."no one has made a clear connection between GCN and the heat kernel." For example, in (Klicpera 2019) section 2, the heat kernel is discussed as a special case."We dont simply combine SGC and heat kernel."Clearly, the only difference between the proposed method in section 3.4 and SGC is that the authors used heat diffusion as the spectral filter matrix, while SGC used a polynomial filter (the K'th power of the normalized adjacency matrix).Furthermore, the other reviewers raised similar works such as graph ODE, which further reduces the novelty of this work. Post-rebuttal feedback:Thanks to the authors for the provided extra experiments and clarifications. I feel that my concerns have been partially addressed, thus raising my score to 5. I still think that the proposed method is limited by the classifier, it's ability to capture a long-tailed distribution, which is not so easy to get when trained on imbalanced dataset. This significantly limits the applications of the proposed approach in real-life scenarios. The paper has also experimented only on artificially created imbalanced datasets, which contain small number of classes with the model being trained with the batch size higher than the number of classes. It would be beneficial to see how the model would perform in more realistic setup when the number of classes is significantly bigger than the batch size (e.g. iNaturalist or even ImageNet), to support more the claims of the paper. ----------------------------------------------------Post Rebuttal UpdateWhile I think the idea is interesting, I still think the proposed loss is not consistent as I still think the two terms in the loss collide with each other, its practical value is limited mainly because making a GAN to work on various datasets is a challenging task, and that the experiments now raised more questions than answers. For these reasons I still lean towards rejection as I believe the paper can benefit from a revision. ----Updated:Thanks to the authors for the provided extra experiments and clarifications. Some of my concerns (e.g., how the batch size affected the performance) have been diminished, but I do agree with other reviewers that more baselines should be included.  After reading the authors' feedback, I decided to increase my rating from 5 to 6. The authors convinced me that setting $E>1$ can reduce the number of communications.  After reading the author feedback, I upgrade my rating to 6. See responses in this thread for reasons. ----------------------------------**After Author Response**Thank you for the detailed clarifications. I've raised my score to 5 since I found the random block skipping experiment quite interesting and surprising. However, weakness #2 and #3 are fairly important in my opinion and I don't think the response sufficiently addresses those weaknesses. I encourage the authors to show their method works on longer sequence datasets (where skimming intuitively makes sense) and compare against 1-2 efficient transformers using sparse attention, for better grounding the improvements in existing literature. After the rebuttal, the authors added a section on large-batch training, which shows that the catastrophic Fisher explosion also occurs in large batch training. This makes the paper more convincing. However, the main concern that this paper lacks theoretical contribution still exists. Therefore, I keep the weak acceptance recommendation.  --------------------------After rebuttal:Thank you for the additional explanation.The comments by the authors effectively address my concerns. Although the edge dataset and SR are quite similar to the existing methods, the authors clearly present the usefulness of them as well as their additional contributions. Also, additional training details support the adequacy of the comparison in the experiments. Therefore, I would like to increase my final score to 7: Good paper, accept. -------------------UPdate: the authors' reply address my concerns well, so I raise my rating to the acceptance side. --------------after rebuttal-------------------I've read all reviews and the rebuttal, and thank the authors for their efforts and extra experiments. I think it is ok to be accepted due to provide further understandings about the learning representation with solid experimental results. ##Updated Review##I'd like to thank the reviewers for their responses, and for updating to the much clearer naming scheme for the different methodologies!  Much easier to follow with those names.I maintain that this is high quality novel work that contradicts a widely held belief within the field and as such is a clear accept. --- Post rebuttal ---After reading the other reviewers comments and the rebuttal, I'm keeping my initial score. -----------------------------------------------------------------------------------------------------------------------------------Update: I read the authors' rebuttal. The authors didn't address my concern "The study is conducted on a single dataset: ImageNet. It is unclear whether the conclusions hold for other datasets."  sufficiently. I would like to keep my original rating.  ========================UPDATEThe response says:"With the straightforward use of Transformers, where the model has only seen a single variable in training, theres no information for the model about what to do with the second variable and thus it will not generalize to the two variable case. Training on two variable-polynomials and testing on two variable-polynomials has relatively low accuracies in our experiment. This suggests that training on single variable-polynomials and testing on two variable-polynomials will result in even lower accuracies. With more work, one may be able to design a model with appropriate inductive bias that understands the concept of multiple variables. This is beyond our scope."I am afraid that this makes the study rather insufficient for me. The problem of representing variables, eigenvariables/skolems, and capturing structural similarity between different theories and signatures is ubiquitous in the ML-for-TP area. Practically all useful systems developed so far - both features-based and DL-based - have to address this. The authors' answer is "our representation is unsuitable". The observation that if you have no shared representation of variables, you will get little/no generalization is a no-brainer and there is hardly any need to publish negative papers about it. In particular, in a conference about *representations* and some 15 years after first useful systems dealing with such issues have been developed. There are many fixes to this - see e.g. Gauthier's representation of variables in his Tree NNs, etc.My score will stand, but I would like to encourage the authors to dig deeper and follow the suggestions given in this and other reviews. The general topic of learnability of symbolic rewriting by various neural architectures is certainly interesting, potentially very useful, and far from well understood. ## Post-rebuttal commentsThanks again to the authors for the submission.  My concerns are clarified and so I will leave my rating intact. ## After Author ResponseMy concerns are only somewhat addressed by the authors' response. While the results on extra datasets are encouraging, I still think there is limited novelty in the proposed approach and some of the important details remain unclear. In particular, terminology seems to be used inconsistently, which results in ambiguity when describing variants of the model used in experiments. Review update after rebuttal: The authors have addressed some of my concerns in the rebuttal and the modified version of the paper. They have added results on more real datasets and explained some aspects that were unclear in the first version. However, I am still not convinced that this is a comprehensive enough contribution as it is right now. The results on the new datasets have, in fact, raised more questions. Their proposed method seems to perform worse than the baselines in some scenarios. Though this is not bad and it is important to show negative results, I did not see any discussion or insights into the poor performance. I would recommend the authors run some baselines themselves and compare rather than copying the results from the baseline papers. This would ensure that the experimental setup and environment are similar leading to a fairer comparison, and possibly some insight into their methods' performance. In summary, I still believe that this is an idea with potential but the authors still have ways to go in putting their idea clearly on paper and justifying it completely and comprehensively. I am sticking to the score I had assigned before.  ----------Updates after rebuttal------------The author did not provide a revised version and additional experiments to address my questions. I keep my original score.------------------------------------------ Post-rebuttal:  I thank  the authors' effort for the improvement of the manuscript following the comments of the different reviewers. I think the overall quality of the paper has really improved. But, after many thoughts, I still think there is a limited novelty in this paper. I have increased my score to 5. But I can not recommend this paper for publication.   adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept. ._Post-rebuttal_:I really appreciate the authors adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept ================== After rebuttal ==================Several typos:In Figure 1, to be consistent the X and Z may be written in lower case.In Eq. (6), the transpose should be applied to the right $U_1^{(i)}$ instead of the left one.In Appendix C.1, ``It seems that MAW seems to learn" should be ``It seems that MAW learns" or ``MAW seems to learn".In Appendix D.3:In line 2 above Proposition D.2, ``the ill-posedness of (11) with $\mathcal{R}= W_2$": The $\mathcal{R}= W_2$ should be $\mathcal{R}= KL$.Please rephrase ``the KL divergence fails is unsuitable for low-rank covariance modeling", by e.g., removing ``fails" or inserting ``and" between ``fails is". ----------------------------------------**Update after author response**: I appreciate that the authors answered some of my questions, but they did not address my two main concerns: insufficient baselines and writing clarity. I therefore am inclined to maintain my vote to reject the paper (score = 4). ** Edit (Nov 23): I have slightly increased my score due to the improvements made to the paper (mainly reorganization) & some clarifications made by the authors, but I still don't feel like my main concerns were addressed.  Update post-rebuttal==================- I am happy with the thorough engagement by the authors and their clarifications. So  I am bumping the up the score a notch. I am increasing the score to 6. I appreciate the authors' response to the reviews. They did the additional experiments I asked for. As I stated in the original review, I really liked the unified approach. It is elegant and is nicely presented.After reading the authors' response to R4, it is clarified that the 1D essential homology is because the computation over all threshold is too expensive. I think there might be an opportunity to better justify this paper: we often have to stop the filtration early due to computational concern. This unified representation could potentially be a good solution for this: without computing the actually death time, the unified solution can still 'learn' the real death time of the 1D essential classes. The authors might want to discuss or ideally empirically verify this in the final version. For example, can you show that using the new approach, and stopping earlier during the filtration, the unified classifier can be as good as when we run the whole filtration and compute the real death  time for all 1D classes. Moreover, it will be ideal if the authors can manage to show that the unified approach can actually learn the real death time for these fake essential classes (I do not know how). This way the paper can potentially have a bigger impact.  ** Edit after author response **I feel that the authors have sufficiently addressed the statistical significance concerns I had in the comments below. I now recommend acceptance because I think the work provides strong evidence that regularization is beneficial in continuous control. Although, as other reviewers have pointed out, further analysis of why regularization is beneficial--especially from a theoretical standpoint--would be helpful, I think the empirical contribution of the paper still stands.  ** EDIT **The authors have addressed my concernes and I have increased my score. ======Update======Thank you for the rebuttal, which resolved most of my concerns. I increased my score.  POST-REBUTTAL COMMENTS======== I would like to thank authors for their clarifications. Accordingly, I have increased my score to 6. ===== Edit after authors' revisions ======The authors made some efforts to improve the exposition of the most obscure parts of the paper, but in my opinion this is not sufficient and I am still not able to fully grasp the connection between section 4.4 and 4.5 and the rest of the paper. The added parts (in blue) contain additional typos and, like the rest of the paper, look like they have been type in a rush. For instance:- "First of all, we show that f_i can be regarded as a sum-of-power mapping defined by the coordinate function" -> I don't see where this is showed in the paper- "f_i(v_1, · · · , v_N ) can rewritten as" -> can be rewritten asI'd encourage the authors to resubmit their work using a longer paper format that would allow them to better expose the details of their investigation. Post-rebuttal:I acknowledge reading the rebuttal as well as other reviewers comments. I'm satisfied with the rebuttal, I think that the authors have addressed many of my initial comments and I'm happy to increase the score of the paper to 7. If the paper gets accepted to the conference I would encourage the authors to include and expand a discussion about method limitations in the main body of the paper.---------------------------------------------------------------- # Post Rebuttal:Thank you for the detailed rebuttal. The comment made about being able to use this in a semi-supervised setting is an exciting direction and I encourage the authors to pursue it on larger less-labeled datasets mentioned in the review in a future work/final submission. I am glad that removing VGG improved the results on COCO.Ultimately, I am keeping my score at 7, accept. -------- Post rebuttal updateI appreciate the rebuttal made by the authors, as they have clarified my questions about the paper. It is also good that a new dataset and additional ablation experiments have been added to the paper, however, the empirical section still needs to be improved in my opinion. For example, more datasets and stronger baselines could be used, and I believe the authors could do it if they had more time. Therefore, even though I mantain my score, I encourage the authors to strengthen the empirical evaluation for future submissions.---- ==================== Post-rebuttal update:I would like to thank the authors for providing a detailed reply, which partially addresses some of my concerns. However, there are several issues in the reply and updated manuscript as provided by the authors:- The results on OGBN-arXiv are definitely useful, but there is no GAT-based baseline, which corresponds to the JAT's main comparison point. Therefore it is not possible to make strong claims about the method's effectiveness based on the following baseline choice alone.- The discussion of alternate structural learning approaches is not sufficiently detailed, and has no empirical backing to the authors' proposal. More experimentation is needed, in my opinion, to fully back the authors' claim of: "However, these methods may not appropriately determine which part, i.e., structure or node features is more important in the embedding space."- Lastly, I do not find that the authors have appropriately toned down their claims in the Introduction. Most critically, the authors write: "The experimental results show that JATs achieve the state-of-the-art performance". A lookup of the OGBN-arXiv publicly available leaderboard demonstrates that this is not the case: https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxivHaving state-of-the-art is not the most important thing, but it should not be claimed when it's not achieved.Overall I think the JAT paper has a lot of potential. For now I choose to retain my 'weak reject' recommendation, and hope the authors will take my comments into account for subsequent submissions. =========== POST REBUTTALI have really appreciated the effort placed by the Authors in their rebuttal. Here follows some considerations following rebuttal:* The empirical analysis has been strenghtened with the inclusion of the OGB benchmark as it provides a view of the empirical performance of JAT in the context of a challenging dataset within a standardised setup. At the same time, OGB allows direct comparison with leaderboards and results in literature that highlight how JAT performance are not (as claimed) state-of-the-art. In this sense, it is still quite unclear if there is any practical advantage with respect to GAT.* The revised paper version places the work much better in the context of recent related literature.* Also considering the point above, the novelty of the work is still borderline to me.In summary, the work has good potential, but it is not yet ready, hence my decision to stay on the reject side. The contained originality can be made up in a future submission by providing convincing state-of-the-art performance results on challenging benchmarks, showing substantial differences from related models such as GAT.  === After authors' feedback period ===I read carefully authors' responses to all reviews. The author's addressed my concerns and I guess the ones of the other reviewers too. One limitation that can be raised now is that the method is better suited for low-dimensional data.Hence, I keep my accept score. Post-discussion Update===========================Thanks to the authors for addressing my concerns.However, after viewing the other responses (especially the comments from Ekaterina Lobacheva) as well as the author's explanation, I think this submission missed out some quite important references. In addition, its contribution over previous works appears to be marginal after viewing these references. Therefore, I would like to lower my rating from 6 to 5. ------------------Post-rebuttal comments: Thanks for the detailed answers. Many of my concerns were addressed, and I am increasing my score as a result. A follow-up thought: It would be nice to add some discussion on the runtime of the proposed framework. ================Thanks for the rebuttal. I appreciate the authors' efforts on polishing the presentation. It clears some of my concerns. So, I raise my rating from 4 to 5. However, after reading the rebuttal and other reviewers' comments, I still feel that the contribution of the paper is not that significant,  since the search space just includes a scalar and the empirical improvements are not strong (Table 5 to Table 8). I think the authors may consider further applying their method to multi-dimension HPO problems (e.g. joint searching initial learning rate, momentum and weight decay) to verity the effectiveness on more challenging configurations.  Thanks for your rebuttal. It solves some of my concerns. However, combining with rebuttal and other reviewers' comment, I think only choosing the initial learning rate is not that reasonable. It may be more convincing to me that the whole parameters are chosen together.  AFTER REBUTTAL:The authors have toned down some of their claims and I am increasing my score accordingly.  [Rebuttal update: the extra results clear up some uncertainty about this method. I think the success of this method is interesting and teaches us a thing or two about deep RL.] ### Update during review period- The reproducibility of the paper is now much better. It's great that the authors promised to release the LTA code. I hope that this includes the code for the experiments. - Based on the above, I changed my review score to 7.  _________**Post rebuttal**I am happy with the response. **================================== Update after rebuttal ==================================**It seems that the authors have only provided a general comment for all reviewers, which is understandable since most criticisms from all reviewers are on the same weaknesses of the paper.While I appreciate the author's effort in adding more experiments, I do not think the added experiments and reply properly addressed the concerns shared by other reviewers and myself. For example, it is still unclear what the advantage of the proposed method is or what insights we could gain from this study.I think this paper is not ready for publication. As today is the last day of discussion period, I will maintain my original assessment. Thank you authors for your response. ##########################################################################Comments after the rebuttal period:Pros:First, it is totally acceptable to follow the notations and organization of Xu et al. (2017a), as long as the statements are clear and self-contained. The original submission failed on providing key details. The authors have made revisions to address this concern. Thanks!Second, I appreciate the extra experimental results and visualizations.Cons:However, the authors' responses do not fully address my concerns about the novelty, especially method-wise. I will raise my score to 5, but still recommend rejection. After author response: the authors have sufficiently addressed my questions and also the other reviewers' questions. I am keeping my score of acceptance. ============== Post Rebuttal Comments =================I would like to thank the authors for their insightful rebuttal and clarifications. Sadly, I cannot find the newly added section regarding the non-linearity analysis in the revised manuscript and therefore cannot judge the findings of the authors.Hence, my rating will stay the same. ----After rebuttal:Novelty: my assessment remains the same. It is not non-trivial enough to combine several linear operators into a unified optimization framework. Although the unification is useful, it is not a major novelty.Thank you for the additional experiments on testing the hyper-parameter. As you mentioned instability, it is worth to have some toy example to demonstrate the instability and study the cause of such instability and show how to avoid such instability using the proposed regularizer. Clearly (19) is bounded. When \alpha_3 is large enough, the solution will be trivial.Regarding non-linearity: the authors' framework is for unifying a graph convolution operator (that is one layer in a graph neural network). Nonlinear activation is another operator. This is not a major problem from my perspective.Overall, I think this work has some value (although the novelty is not strong) and still recommend weak acceptance. Update: The author response has not changed my opinion that there is insufficient new material in this paper vs the ISIT paper, and the presentation of the material from the ISIT paper does not note that this material was previously presented there. Without clarity in what is the novel material claimed in this paper it should not be accepted. # Post-Rebuttal CommentsI've increased the rating for the paper from 5 -> 8 as the authors have addressed all my substantive concerns.- Most critically, the authors demonstrated that, even without the tree-based exploration variant, FGP's performance improves significantly over the state of the art particularly for $l_2$ networks. - The authors have also significantly improved the clarity of the algorithm description, made the comparison in the section on generalization to other norms more clear, provided information in the paper allowing readers to understand that FastLin is significantly faster (but also significantly looser), and provided details that make reproducing these results far simpler.  ###########################################Update after discussion with the authors and reading the updated version:The authors have clarified the points that were unclear. I'm happy to raise my score. ---Update after rebuttalI thank the authors for their response. After reading it, the revised version and the other reviews, I think that there's evidence of the effectiveness of the proposed method.As I tried to convey in the initial review, I consider the main weakness of the paper not showing the performance on models achieving state-of-the-art verified robustness. I think the case of $\ell_\infty$ is emblematic, where $\epsilon=0.01$ is used. Since there exist models achieving provable robustness >90% for $\epsilon=0.3$ (on MNIST), the scenario considered in the evaluation doesn't seem interesting, regardless of how FGP compares to the other methods.While I agree with the authors that training for provable guarantees sacrifices clean accuracy in most of the cases, as far as I know that is currently the way to achieve good VRA at meaningful thresholds. Along this line, also the authors used FGP on larger models when those were trained with MMR or RS. In my opinion the most interesting application of a (incomplete) verification method like FGP is to provide better VRA than current methods for training (IBP, CROWN-IBP, Wong & Kolter's method), for example using less heavily regularized models which can retain higher clean accuracy. I think this is the missing piece in the current version of the paper.For these reasons, I keep my initial score. ===I thank the authors for providing the answers to my questions. I will not change my score.  --------Updates after author response--------I thank the authors for the detailed response and appreciate the additional experiment. However, I still believe that evaluation on downstream tasks is essential to demonstrate the superiority of the approach, and I unfortunately do not agree with the authors that it implicit that the approach will yield better downstream networks. Therefore, I cannot raise my score, but would encourage additional experimentation.  Update: "Noisy Student and Assemble-ResNet use without removing overlapping augmentations, yet they test on ImageNet-C." This is a bad practice and I should hope the authors of this paper only have experiments where there is a train-test mismatch (otherwise we're not testing robustness to distribution shift). POST-REBUTTAL:I thank the authors for the response and the revisions to the paper. I appreciate the authors' efforts towards the rebuttal. I am however left with some concerns which did not have a convincing resolution:* Regarding the comment on how the proposed analysis would look for stylized transforms, the authors say in the response that "...we dont expect that perceptual similarity is the only cause of improved corruption error, only that perceptual similarity is particularly salient for predicting generalization to dissimilar corruptions...". The work seems to be one-sided in this regard. If stylized transforms don't look perceptually similar to ImageNet-C corruptions but provide robustness, this counters the proposed hypothesis. It then becomes important to say where the proposed analysis is meaningful and where it is not. This seems to be lacking at this time in the work.* Regarding the robustness of MSD to the choice of feature extractor (as also asked by R1), the revised paper includes results on VGG as feature extractor (thanks to the authors for this), but uses a model that is finetuned for CIFAR-10. In general, perceptual similarity is studied directly taking VGG pre-trained on ImageNet - without finetuning on the target dataset. This leaves this question open, and makes one wonder if the latter features did not support the analysis.* The utility of Imagenet-C-bar as an additional benchmark to check the goodness of performance on Imagenet-C seems a bit convoluted. Would we need a Imagenet-C-bar-bar to check the goodness for corruptions that may be beyond perceptual similarity (such as stylized transforms)? This is not very convincing.Overall, I am still on the fence on this work (and retain my original decision at this time). I think the paper does present an interesting insight, but I am not very convinced it has been studied and explored comprehensively enough. I would have ideally preferred to give a borderline (neither positive nor negative) decision, and will not be disappointed if the work is accepted, considering the interesting insights it offers.  Update after rebuttal: I appreciate the authors' response and clarifications. I maintain my original score. Update after rebuttal: I appreciate the author response. I will maintain my original score. After reading author response ================================Thank you for the response.It's good to know that CRC outperforms random sampling on a couple other image datasets, but would be informative to see the results with respect to other AL techniques.I wasn't quite able to follow the logic of section 3.2, but this seems on the right track.The issue I see with using the final layer is not performance related, it's that it seems to throw out the theory you claim to be using. For instance, the fact that the performance dramatically improves is troubling and makes me wonder if the algorithm is working because of the theory or for a different reason. On the other hand, if you aren't able to make the theoretical justification more compelling, I think it would be fine to say the theory is just for inspiration.I agree there is no batch approximation when G=Q. I might be missing something, but the paper says "All experiments hereon use G = Q/10 considering the speed vs. performance trade-off".  ---**After rebuttal**The responses address my main concerns. I have increased the score to 6. But, I also agree with other reviewers that the novelty of this paper is somewhat limited.  Update after rebuttal:I agree with Reviewer 5 that this paper has good ingredients, and the discussion and update of the draft clarifies the novelty and provides better review on the related work. However, the experiments presented in this paper are not very comprehensive, particularly the baselines and the ablation/alternative studies. I am not fully convinced by the authors response of "because MI-VAE performed worse than $\beta$-VAE in PixelCNN-VAEs ... we expected that a similar tendency would be observed." PixelCNN-VAE uses an autoregressive decoder, which are known to exhibit issues that are not observed from non-autoregressive ones like DSAE. This explanation of why MI-VAE was emitted seems slightly hand-wavy. I decided to decrease the rating to 6 to reflect the insufficiency in experiments, but hope that this experiment can be added if the paper is accepted. **Update after discussion with authors**I want to thank the authors for their incredibly detailed responses and engaging so actively in the discussion. Some of my criticism could be addressed, while other issues are still somewhat open. If the main merit of the paper is to make the "sequential VAE community" aware of issues that have been discussed and addressed before, then I think the paper does an OK job at that (though I'm not entirely sure what community that is, the issues have been discussed before in the fields of vision and VAEs with autoregressive decoders). I want to strongly encourage the authors to be as precise as possible when describing the novelty - maximizing the mutual information that a representation carries w.r.t. some relevance variable while simultaneously minimizing information that it carries w.r.t. to another variable is NOT novel. What is novel is the application of that principle to separating "global" from "local" information in sequential data (and how to actually perform this originally intractable optimization in practice). I also want to encourage the authors to state what's known and what's new as clearly as possible and improve the quality and clarity of the "educational" review of why maximizing mutual information is not enough as much as possible.Viewing the paper as "showing how a known problem also appears when separating global from local information, and how to apply known solution-approaches to the problem in this specific context", shifts the relative importance of the issues raised by me. Essentially, that view emphasizes the paper as mostly an application paper (rather than a novel theoretical contribution). Accordingly (but please make sure that that shift in view is also clear in the final paper), I am weakly in favor of accepting the paper and have updated my score accordingly.--- Post Rebuttal:I'm totally fine with the topic, which is a very minor issue as mentioned earlier. My main concern was regarding the overselling of some presented results.From the technical side, Lemma 1 is a common approach in information theory/channel coding/privacy that one can linearly combine several designs by randomly sampling them, which was presented in the abstract as one of the main results. I expected something beyond a simple application, such as determining an interesting condition where this approach can be used, but the provided statement does not seem to hold after revision. As long as one can append additional messages to the query, the domain of Q could change completely, and the convexity of P_{Q|M}, interpreted exactly as stated, will not provide any guarantee. The condition actually needed by the authors is essentially random sampling does not hurt privacy, making the statement "random sampling can be applied when it can be applied", which is a bit trivial.I carefully double-checked the provided proof steps and found that an additional assumption was needed in the very last step, which further requires something like the metric \rho is invariant under permutation of Q, for which I didn't find in the paper, maybe I missed it. Adding this assumption totally makes sense to me, but requiring convexity+invariant under permutation is a bit over-complication, instead of just directly requiring random sampling does not hurt privacy. The theory part of the paper should be presented in the simplest possible way, without unnecessary complication.  The authors do have a novel contribution in terms of experiments. But the contribution and novelty in the formulation are rather limited given that the considered components have been studied either separately or jointly in cited works.For the experiments: I have a similar concern as mentioned by Reviewer 1, which is about the generalizability issue. A natural explanation of the presented results could be that the hyperparameters in the implemented solution are tweaked for datasets like MNIST or something similar. It is not clear if it would fail for more general datasets similar to synthetic Gaussian. I've read the authors' responses to review 1 and the revised paper. Overall, there is no more direct evidence of generalizability. But this issue itself is fine as it could happen in many other works, so I won't consider it as my main concern. Besides, the author has adequately addressed the issues of the missing conditions in the Theorems. Conclusively, my recommendation remains the same for the above reasons.  --------- After Reading the Updated Paper ----------Thanks for the update. After reading the revised paper, I still have some major concerns:The current experiments performed are not enough to demonstrate the effectiveness of the proposed method. The old experiment results (Table 6, 7, 8) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data, which is not realizable in practice. We should assume that the test OOD data are unknown during learning the OOD detector. The new experimental results where they train the binary classifier using adversarial examples generated on in-distribution data (follow the Mahalanobis method) in Table 1 are limited. For example, on CIFAR10, they only report results for ResNet50 and WideResNet, but I also want to know the results for DenseNet (Mahalanobis method [4] performs very well on CIFAR10/SVHN using DenseNet under the same setting).Some experimental details about their method are missing. The authors mention that they train 12 binary classifiers and then select the best one on the validation dataset. But they don't provide the details about the validation dataset, which is critical for their results. Based on their previous response, it seems they use a subset of test OOD data to select the best classifier, which is not allowed I think. Based on the current description of experimental settings, it is hard for me to evaluate the reported results.The proposed approach needs a lot of hyper-parameters (4 attributes, 12 combinations, the weights of the binary classifier, etc) and it is unclear how to tune these hyper-parameters and how they would affect the results. The current ablation study is limited I think.This paper doesn't have rigorous analysis for why integrating different attributions would improve OOD detection. I think this is an empirical paper but the experiments provided are not sufficient to demonstrate the effectiveness of the proposed method.To clarify, I didn't agree to raise the score previously. What I said was that the previous paper needed significant revision and I could not recommend acceptance. I still have some major concerns after reading the revised paper. Thus, I keep the same rating and think the paper is not ready for publication. I hope the authors could keep improving their paper.[4] Lee, Kimin, et al. "A simple unified framework for detecting out-of-distribution samples and adversarial attacks." Advances in Neural Information Processing Systems. 2018. ##I have read the author response and I still think the paper is limited in terms of novelty, significance and experiments. I would like to keep my current score. ## ====== Post discussion comments ======After reading all reviews, responses, and discussions, I still do not see evidence that the model has learned a "high-level plan", which is the main claim of the paper.I agree that most deep learning models are not interpretable, but most papers do provide some qualitative/anecdotal/generality/strong empirical evidence to support their claims. In this paper, I do not see such strong evidence.My main concerns:1. Is it possible that the 2-stage search simply increases the diversity of solutions? If there is an empirical improvement, I would like to understand the simplest explanation ("Occam's razor"). If the main contribution is diversity, I would expect the authors to spell it out clearly.Further, if the main contribution is diversity, maybe there are much simpler and general ways to achieve diversity (e.g., diversity inducing versions of beam search), that can be applied to different architectures (i.e., not coupled with VQ-VAE).I feel that my question "Can we have an additional baseline that improves diversity in RobustFill, to reject the hypothesis that the VQ-VAE simply increases the diversity of solutions?" was not answered by the authors.2. The 2-stage search is a general approach, but the paper did not convince me that it is useful to settings beyond the FlashFill task, and for models other than the textual approach where programs are generated as text.If the authors claim that their approach allows "high-level planning", I would expect to see that it works across different models / tasks / datasets / settings.Minor: I do not agree with the authors that this is self-supervised. I think that the paper uses the term "self-supervised" incorrectly. Edit: I have increased my score to 7. ---UPDATED After RebuttalThanks the authors for answering my questions. Theorem 1 now looks fine. **It can be said that BC is to maximizes the off-policy expected...**I do understand that the existence of $\epsilon$ makes the RHS of Eq (3) (All labels are in the revised version. ) smaller. However, Eq (3) only means the maximum *guarantee* (the lowest performance) of BC is small, and doesn't mean that the BC's performance can't exceed something. **definitions of $S_e^{\pi_e}$ and $S_{e+*}^{\pi_e}$**Thanks for clarification. It's much clearer now. However, in this definition, how about the states where the policy has always followed the expert policy so far? I guess this is the reason why you have 3 sets before revision. **The first term on the RHS in (7) thus works as a noisy regularizer.** (in the revised paper)In my opinion, it goes a little bit beyond regularization, as the coefficient is as large as 0.4. For example, in the image classification task, can we call corrupting 40% labels a "noisy regularization"? **It could happen that removing $S_e^{\pi_e}$ from the training set changes the performance in such a case where the noisy expert recovers to be in the optimal state after adopting the non-optimal policies.**But with much larger probability, it will go out of the manifold (of expert policy's states), right? Even BC can go out of it easily (otherwise you can't beat it). Overall, I think this is a neat algorithm and it seems to work pretty well. The revision also makes it much easier to understand and I do appreciate the effort behind it. I'll then raise my current score (5) to 6, although I'm still not fully convinced so won't give a higher score.  ## After Rebuttal ##I thank the authors for their response to my question. I think that the comment regarding differential privacy being ineffective is particularly interesting. It would be nice to actually demonstrate this empirically - construct a DP classifier with a poisoned backdoor, and show that the authors' method is still effective. My support for the paper remains unchanged.  **Post Rebuttal**I thank the authors for the extensive experiments and answers. Unfortunately, I still feel that the contribution is rather marginal. I keep my score unchanged.--- ---Post-rebuttal: Raising my score from 5->6 after authors provided a detailed list of changes. ########After rebuttal#########I appreciate the author's effort in addressing my concerns. After reading the rebuttal and other reviews, I am raising my scores to 6. ---Edit: after the authors' response I maintain my recommendation to accept the paper and keep my score of 7. **Update to review**I shall keep my rating at 7. The authors updated the paper and I think it is good enough to be accepted. I still note how Figure 1 is difficult to parse (what are all the variables? Why are there so many arrows and boxes? It is not possible to understand what is going on without reading the text in detail, by which time it does not add much to the reader's understanding); in my opinion it requires starting from scratch again.