 ---------- Post rebuttal ----------After discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.However, I would like to keep my original reject score. The reasons are as follows.Extending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks. The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO. With the development of computer vision techniques, it is natural to try more challenging datasets. So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.In addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO. The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't. This difference is also minor.Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper). I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019 UPDATE: After reading through all other reviews and responses by the authors, I share the concern that the theoretical justification of the paper is lacking as the connection between the truncation error and the improved algorithm performance is not rigorously proven. Therefore, I have reduced my score. **Update after rebuttal:** The author rebuttal clarified some minor issues for me, but it did nothing to address my main concern, which is that very similar methods have been proposed before. I'm therefore keeping my score the same.  --------------------------------------------- Update after author response:The author response is much appreciated. However, my two main concerns remain unaddressed. The authors may add these additional experiments/results to Table 3 and 4 in further revisions for a stronger submission.* Table 3 is the main result of the paper which claims policies learned in TextWorld (TW) environment can be transferred over to ALFRED (ALF) environment under zero-shot setting. This result alone has several weaknesses -- (1) Evaluation done on non-human goals in ALF seem to use same template as that used in TW, so it is not surprising that agents will have non-zero success rates on similar language specifications. (2) When evaluation is done on human goals (which seems to be the real test), the agent's performance is very low. Also, there are no baselines (e.g., random) provided to compare those scores against. (3) Why are the experiments only conducted in zero-shot setting? This actually brings me to the second weakness.* Since the transfer learning is happening from a pure-text TW environment to a physically simulated (with visual input) ALF environment, it is more interesting/relevant to see how the language module pre-trained on text-only TW adapts to multimodal setup in ALF. This adaptation will require further training/fine-tuning on ALF so that visual/control modules can adapt to this pre-trained language module. This experiment was attempted in Table 4, however, as pointed in my initial review, this falls short of proving any claims made in the paper because the agents in Table 4 are learned with an oracle state estimator which means there is no visual input processing during this mode. It can also be noted that the Controller is also a heuristic module with no learning. Which means the setting used in Table 4 reduces learning/evaluating in embodied ALF environment to a pure text-driven environment. ---Additional comments after rebuttal--I have carefully reviewed the authors' feedback regarding their comments on how their proposed method differentiates the existing (AVT and AET). Unfortunately, it did not address my concerns about the novel technical contributions in the proposed paper. Obviously, the authors applied the "Transformation Equivariant Representations by Autoencoding Variational Transformations" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). I am not sure why in the rebuttal, the authors claimed, "Their proposed method distinguishes from AET significantly in two aspects". I would urge the authors to check both papers below, and it clearly defines the Transformation Equivariant Representations learning by Autoencoding Variational Transformations, which could be applied for various types of data. [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data, in CVPR 2019. (AET) [2] Qi et al., AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations, in ICCV 2019. (AVT)Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. For this question, I think the authors should prove the Transformation Equivariant Representations directly on a 3D object (point cloud, voxel, 3D mesh) instead of multi-view 2D images. I am not sure why the authors answered that "3D objects are unavailable at the testing stage." The proof of Autoencoding Variational Transformations for 3D data directly should not depend on the availability of 3D data. EDIT: **post rebuttal. I'd like to thank the authors for their response. As scalability to high-dimensional hyperparameter spaces is presented as a key advantage of the method, direct comparisons to high-dimensional BO techniques would be needed. The fact that using trust regions could benefit HOZOG, or that HOZOG is a better strategy compared to TurBO and REMBO, should be demonstrated empirically. I am keeping my score to 4 as the current positioning of the paper would require direct comparisons with these baselines. ** EDIT: **post rebuttal (could not add a comment readable by authors). Thank you for your rebuttal, I have read it and the other reviews. I agree with the other reviewers that some more baselines for high-dimensional benchmarks are required, and was not convinced by your rebuttal to those requests. I also think the aspect of non-convexity and local optima touched by Reviewer 4 warrants some discussion in the paper. I maintain my score of 4. ** ---After rebuttal---The authors have partially addressed my concerns, however, I am still not quite sure why their method would be better than SimPLE. The authors' ablation study of removing image gradients does not address my main question about where the performance benefit is coming from. I am assuming that the architecture and hyperparameters that the authors use are different than SimPLE. I think one must instead replace predicting the latent state with the image as SimPLE did to see if that makes a difference in performance. Therefore, I will be keeping my review the same. **=========== Update to Review, after Updates to Manuscript during Rebuttal period ==========**Summary of improvements during rebuttal and remaining concerns:- Extended the literature review, with references to an unsupervised clustering method (k-shapes),  and methods that use CAE, LSTM and VAE. This is a significant improvement. The literature review still does not discuss previous losses that lead to clustering within the latent space, which is what the proposed methods here perform, and are hence closely related (see my initial comment).- Rephrased/corrected claims about performance of the method. The corresponding claims about performance is now that the methods can usually improve k-Means clustering performance, improving the baseline CAE. This is now accurate. Yet, I do not think the performance is a sufficiently strong argument unfortunately (especially given the limited evaluation, only against few and basic baselines).- Evaluation has been expanded by employing unsupervised clustering via k-Means, k-Means+PCA, and k-Shape in Table 1. However, there is still no comparison with any other more advanced method, such as SSL based, DL based, etc (as per my initial comment). I think there is a lot of improvement here, before the paper can display improvement over the current state of the art.- No improvement on the ablation study, which is currently not particularly useful, e.g. by investigating aspects of the losses and their behaviour to provide more insights into the method, or performing empirical sensitivity-analysis to meta-parameter values.- Added clarifications about some values hyper-parameters used (Adam optim with default TF2.1 learning rate for all). There seems there was no attempt to find optimal hyper-parameter configuration for each method, in order to perform fair comparisons (e.g. each method may require different weighting/learningrate). The authors seem to acknowledge the issue and defer it to future work. However, I think that without such investigation (e.g. via an empirical study and sensitivity to these hyper-parameters), we cannot be strongly confident about conclusions on the relative performance of methods, as they can be sensitive to configuration.- Provided the code of the work as supplementary material, hence reproducibility is greatly increased. Thank you.- Improved clarity of the paper by improving Fig 1 and some of the math notation problems. Overall, the work has been improved during the revision, hence I will raise my rating (from 3 to 4). However, I believe the remaining problems of limited literature review, evaluation, no sensitivity analysis to hyper-parameters (or effort thereafter to configure them for each method) or other sort of empirical analysis that would give insights to the behaviour of the losses, still keep my score relatively low. UPD:  Thank you for your answer and for addressing the points raised in the review. Still, I agree with the concerns raised by AnonReviewer4 on the small scale of the networks used in the experiments (e.g., LeNet, fc). I am still not convinced by the experiments and the excess of hardly understandable plots given to support the main bulk of the paper's claims. Therefore, I leave my rating the same. Thank you for your response. I have read the revised paper and the discussion with other reviewers. While I still think that the overall problematic discussed in this paper is important, I am not satisfied with the revisions. I think the paper would benefit from more technical content and clear mathematical definitions of the procedures used. I have updated my score to a reject.  ----Update: I am very happy to see the new experiments that validate the implications of the Stackelberg games theory. The main drawback of the paper is that it is not clear that direction homogenization could lead to practical improvements for multi-task learning. The additional experiments in Table 2 are useful, and suggest that much of the benefit comes from the greater expressivity due to task-specific matrices. --------After author's response----------I am not fully convinced by the explanation of the motivation behind rotation matrix, in particular why it is aligning with the single-task learning, which is counter-intuitive. The authors provided more ablation studies, however, the evaluation on datasets is still quite preliminary with some questions remaining (such as why there is a discrepancy between the two versions of Rotograd on the second dataset). Therefore I am keeping my original score. ---Edit after rebuttal:I am sorry for not yet being able to support acceptance of this paper. I appreciate the authors' work during the rebuttal, and I agree that the execution of the paper improved in this version.But the issue remains that the proposed algorithm does not fundamentally lift any of the current obstacles in implementing backpropagation in the brain. Most importantly, the algorithm is very similar to exact standard backpropagation, and it still requires the same coordinated phases as backpropagation: the forward phase and the backward phase. The learning rule is non-local in time and while it does not use activity differences, that is not necessarily a good thing. For example, the connection to spike-timing-dependent plasticity becomes harder to establish. The authors verify that learning still works after important approximations are made (notably to avoid weight transport), but these approximations were already previously reported and are more or less obviously applicable here, given how close activation relaxation is to standard backpropagation.Finally, I really recommend the authors to more explicitly state and discuss the temporal non-locality of the algorithm when introducing the method (and in Algorithm 1), which remains insufficiently clear to me. **Post-rebuttal**The revised version removed questionable or invalid notations/assumptions/justifications, and there are some defences for previous formal justifications in the rebuttal. IMHO, the rebuttal raises further concerns about the technical quality, and the paper still requires stronger justification for acceptance. I'll lower my score instead.Specifically, I find the rebuttal generally confusing, and I disagree with various points in the rebuttal/revised version.- Condition 3.1 doesn't seem right: by definition, a classifier in  has a misclassification probability of at most  on the ID data, but Condition 3.1 states that the misclassification rate is ? Importantly, I don't see why this condition is needed as there is no justification on how it is connected to the disagreement test. In addition, "As a consequence, with very high probability 1  (1  ´)s we cannot fit a set of s random i.i.d. in-distribution points with the wrong label" is quite vauge and doesn't seem right either.- "According to the definition, there exists a class of functions F that is complex enough such that OOD(P, F) is the complement of the support of the training distribution.": I don't think the definition implies this. In addition, if OOD(P, F) is the complement of the training distribution, then ID examples not in the training distribution are included in OOD(P, F).- "If we defined OOD(P_n, F) with P_n the empirical distribution, then the reviewer would be correct and indeed this set could contain ID samples.": in theory, OOD(P, F) CAN contain ID samples, unless additional assumptions are made.- "two-sample test": while I think this is a minor issue to call the test a two-sample test, I still don't think it agrees with the standard usage of the term (which means two random samples are used).- "one may simply pick the time point with the highest validation accuracy, after training for a fixed number of epochs": I doubt that this can be called "early stopping". So I don't think the response addresses my question regarding Fig. 5 (previous Fig. 4). In addition, since "early stopping" is used for regularization, this makes it questionable whether "regularization" is indeed needed.- The test statistic is included in the main paper now, but how about the threshold? In rigorous statistical testing, the threshold can be rigorously calculated, while in this case, it is not clear how the threshold is set. --- Post Rebuttal ---I've read the author response. However, I do not plan on changing my score as my main concerns have not be addressed. In particular:> the framework does not so much provide a novel formulation of successor features but rather presents a specific instantiation... Thus, the claim that this is a novel formulation of successor features with a non-linear reward seem inaccurate since the original successor feature formulation already handles the case where the reward is a non-linear function of the state.The author response is> There is no guarantee that the learned state features are able to find appropriate values. In theory yes, we agree but practically we found this not to be the case if we explicitly test for it. As our environments do, where the reward is a non-linear function of the state.I understand that perhaps existing methods are not capable of learning good state features, and presenting a method for finding better state features (not the weights) would be interesting. However, the current paper simply hard-codes good features, which I do not find compelling. --------------------------------------------------------------------------------# Review UpdateI thank the authors for their thorough response and the additional experiments. Based on these factors I will raise my score from 'clear rejection' (3) to 'okay, but not good enough' (4). I would have liked to score the paper higher, but at this stage I believe the paper is still not ready to be published. The authors acknowledged in their update that the review process helped them to understand their own work better. As a result, some aspects of their approach have been changed (e.g. removing step(ii), changes to step (iii)). I believe changes to the method go beyond the scope of the discussion phase and instead justify resubmission. This would give the authors some more time to get an in depth understanding of their approach as well.## Author comments on step (ii)I thank the authors for clarifying. In their response, the authors claim that they will use a held-out data set with known density parameters. It is then possible to evaluate which models in the ensemble best estimate these density parameters. I have some issues with this claim:1. This is not made clear in the paper.2. The approach assumes that the density parameters are unknown. Adding this assumption will weaken the paper.3. The selected models will be biased toward the held-out set.## Author comments on step (iii), now step (ii)In their update, the authors change the reweighting scheme. Instead of having a model-based weight, the reweighting is now done solely on a per-sample basis. I believe this looks like the right direction to take.## Ablation studyThe ablation study is important. One possible addition would be to make a comparison for different ensemble sizes. ### After rebuttal phase ###Since the authors did not provide a revised version of the paper or addressed my comments in detail, I do not see a reason to change my initial recommendation. I thus recommend to reject the paper. After rebuttal: Authors' responses do not address any of my concerns, and I completely agree with other reviewers regarding lack of clarity, evaluation, and novelty. The current form of the paper is not ready to be published. I decrease my score to reject. -------------------------------------- *Update:* Following the authors' response, I upgraded my rating, but I still think there are critical issues with the paper. The most problematic point, in my opinion, is the only-marginal improvement on the test data, indicating that the suggested training method only improves the specific "failure scenarios", making it is similar to adversarial training methods used to gain adversarial robustness. However, the abstract and introduction indicates that the paper helps in debugging in fixing failures in general, which, I think should have been evident in improved test accuracy. ----------------------After reading the rebuttals-------------------------------Thanks authors for the detailed response to my review.Unfortunately, the responses are not satisfactory for me to increase the score.Therefore, I stand on my initial score (4) as my final score.The below are the reasons for this.1. The authors failed to provide the comparison with other AutoML papers on time-series.2. Imputation/Interpolation is a basic data preprocessing step. Also, the additional complexity for the imputation is marginal (especially compared with AutoML). Therefore, excluding the comparisons with imputation is hard to be accepted.3. There are a bunch of self-supervised learning methods. It is unclear what is the motivation that the authors utilize "contrastive learning" as the self-supervised learning methods. 4. Also, motivations of many decisions in this paper are still missing. I think I am not the only person that raised this problem.5. I asked for "quantitative" analyses on computational complexity. However, I cannot find the "quantitative" analyses in the revised manuscript and rebuttals for the computational complexity. Post rebuttalI would like to thank the authors for their valuable response. The experimental results seem strong, but technical novelty is still limited. My review score remains the same. I believe this paper can make great impact if submitted to a chemistry journal. ###  **Post-rebuttal comments**I read the rebuttal and other reviews. I am still not convinced that there is a big difference between this method and speaker-follower of Fried et al. The speaker model cannot be used for navigation by itself but the learned distribution is used to adjust the actions taken by the follower. The only difference between these two methods is that one uses the entire trajectory and the other one uses partial trajectories up to the current time step (pre-trained on the entire trajectory though). Also, as mentioned by another reviewer, some analysis should be provided about why the generative model works better. Due to these issues, I keep my original rating.  ======= POST-RESPONSE UPDATE ========I appreciate the authors' response and additional experiments. Unfortunately, my criticism still stands:- **Comparison with standard models.** Based on Appendix C, it is still unclear if the class-wise disparity is a unique property of robust models. As we can see in Figure 7, standard models also have disparate accuracies between classes. More importantly, comparing the **absolute difference** between class accuracies is misleading, since robust models have overall lower robust accuracy. A better comparison would be to measure the **relative error** between classes. While it is hard to draw conclusions by just inspecting the graph, it seems that the discrepancy is significantly milder based on this metric.- **Classifier norm.** The additional experiments still do not demonstrate any causal link between classifier norm and robustness. It is thus still unclear what this metric conveys.- **Temp-PGD.** While I appreciate the effort to provide additional intuition about the attack, I still do not find the method fundamentally new when compared to other attacks optimizing combinations of logits (e.g., Carlini-Wagner, Multi-Targeted).Overall, I still think that the original observation is intriguing, yet requires a deeper and more systematic study. ======= POST-RESPONSE UPDATE ========I appreciate the author's efforts for responding my questions and providing additional results and I do find the empirical observations of class-wise properties interesting. However, I still feel that the contribution of the current form of the paper is not strong enough to reach the bar of ICLR, so I remain my previous rating. Beyond exploratory analysis, the paper would be much stronger if it can go deeper with the observed class-wise properties of robust models.  #### Post-rebuttal Update ####I thank the authors for their detailed response and edits to the paper. However, even after reading the rebuttal some of my original concerns stand:[Novelty] As I mentioned in my original review, the discovery of disparities in class-wise robustness is not new to this paper. In the rebuttal the authors mention that their finding is different from [1] because in [1] the measure of class-wise robustness is distance to the decision boundary with respect to every other class. However, this is in my view, is just an alternative and well-established measure of robustness---i.e., distance to the decision boundary and robust accuracy are fairly correlated and not fundamentally different.[Takeaways] - The authors do not perform sufficient quantitative analysis to justify the link between robustness and weight norm. Moreover, without establishing the causality of this link, it is unclear to me how this observation provides any new insight to understand robust models. - Figure 7 (in the revised manuscript) shows that a similar class-wise disparity is present even in the *standard accuracy* of *standard models*. Thus, although I find the observation of disparities in class-wise robustness interesting, I believe further investigation is needed to understand whether this is just an inherent property of the data distribution that hurts both standard and robust models or is specifically tied to robustness. - The authors' comment about improved robustness based on methods adapted from [3] is misleading. As mentioned in Appendix G (I believe there is no Appendix H), this seems to be the case only for a specific attack. The authors themselves demonstrate a different attack under which the improved robustness of vulnerable classes disappears.[Other comments] There is no evidence to suggest that untargeted attacks will find the *closest* adversarial example within an eps ball. The optimization problem for untargeted attacks is set up to maximize the loss (w.r.t. the ground truth label) and not to find the nearest misclassification. Thus, I still assert that to get a better picture of per-class robustness, the authors need to measure the targeted confusion matrix (or distance to per-class decision boundaries as in [1]).Due to these concerns, I am unable to raise my score. =========== Updated Score============Of the three concerns above, the authors satisfactorily addressed the point related to technical novelty, hence I am increasing my score. **POST-DISCUSSION UPDATE**I want to thank the authors for responding to my questions, correcting my misunderstandings, and addressing some of the raised points. Overall, I still believe that the work is not yet ready to be published. One of my main concerns is that a method for context integration should be evaluated in comparison to multiple other methods for context integration (ideally on multiple predictors) in order to see which approach is particularly meaningful and why.  As mentioned in the initial review, the general idea is interesting and worthy of resubmission after the authors address the issues raised in the reviews. =======================After reading all the review comments and rebuttals, I would like to change my score to 4. The paper is interesting, but more detailed analysis and experiments are needed to make the work more clear and convincing. =========================================Thanks for the rebuttal and revision. My first concern has been addressed. However, I still found the proposal lack empirical or theoretical proof, so I am not convinced the contribution is principle enough. I decide to keep my original score. UPDATE:Thank you for clarifying the ethics concern. However, this makes it much more difficult to assess whether I believe your method works as well as you state. After having read the rebuttal and the other reviews, I am more confident that the methodology proposed lacks connection to educational relevance and novelty for publication at this venue. My score stays the same. ----------------------Update After Rebuttal----------------------------------I appreciate the response from the authors, and the authors' efforts in significantly revising the Method section. The Method section in the current version looks much better and clear. Most points from the authors' response are reasonable, and some of my concerns are indeed cleared, such as my questions regarding the use of $||\hat{\omega}-\omega^*||$ instead of $||f(\hat{\omega})-f(\omega^*)||$. However, in the revised paper, I still find a few places questionable, so I still have a few concerns which are still about the first and fourth points I raised in my original review:- In Definition 3.2 of the revised paper, I find this definition of optimal optimizer $g^*$ not fully convincing. I understand that this definition of $g^*$ naturally gives rise to the $F(\theta^*)$ as in the first line of page 5, however, since our problem is an optimization problem (i.e., to minimize $f$), I think the summation in the definition of $g^*$ should be replaced by minimization. The current definition of using a summation over different iterations could lead to problems in some scenarios. For example, imagine we have optimizer B who quickly converges to a local minimum, and optimizer B who explores the entire search space first (encountering many large $f$ values along the way) and finally converges to the global minimum. Then according to this definition, optimizer A is likely to be defined to be better than optimizer B, which is incorrect. Moreover, another problem with the current definition is that the initialization $\omega^1_g$ is not specified. I feel that for the sake of defining the optimal optimizer, the argmin over all optimizers $g\in\mathcal{G}$ should be based on the same initialization for all optimizers, i.e., $\omega^1_g$ is the same for all $g\in\mathcal{G}$. Since I imagine that for different initializations, the optimal optimizer could be different.- Equation (5) on page 4, I think the distribution of the initial point $p(\omega^1_{\theta^*})$ should appear on the Right Hand Side. Because given the optimizer $\theta^*$, the distribution of the trajectory $\mathcal{D}$ clearly depends on the initialization. This may not be a serious problem since $p(\omega^1_{\theta^*})$ could be factored into the normalization constant of $p(\theta^*|\mathcal{D})$.- Section 3.6, I still find the motivation for using the Meta-Training Set heuristic. It is stated here that the meta-training set is introduced here to improve the robustness and generalizability of solutions, which seems unrelated to the main objective of quantifying optimizer uncertainty. I think (not sure though) a better motivation could be related to uncertainty regarding the function $f$.- Although the objective of the paper is to "further" consider the uncertainty regarding the optimizer, I feel that the introduced method ONLY considers optimizer uncertainty, and hasn't dealt with $p(\omega^*|\mathcal{D},g)$ in a rigorous way. If I understand correctly, samples from $p(\omega^*|\mathcal{D},g)$ are obtained by running optimizer $g$ for multiple random initialization points (line 5 of Section 4), and I find this kind of heuristic.Overall I find this paper very interesting mainly due to the novel perspective of considering this additional source of uncertainty, so although I cannot recommend for acceptance this time, I believe it will be a valuable contribution to the community if the problem formulation can be made more rigorous. Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4. After rebuttal:I appreciate the authors for thoughtful response and additional experimental results, which are helpful for further understanding of the manuscript. Especially, the additional experiment on the recent OOD detection method addresses my concern about the evidence that the previous OOD studies are not sufficient for defending the bit-level corruption.Unfortunately, I am still not sure about the technical novelty of this paper. I agree that the paper proposed a new problem setting, but I do not think that the technical novelty is significant, given the proposed approach of just applying the data augmentation simply at a bit level, rather than at a pixel level.Due to this concern, I want to keep my rating of "4. Ok but not good enough - rejection" as it is. ------- after the discussion period ------I would like to thank the authors for the reply, clarification, and additional experiments. Although I do like the perspectives revealed in this work, many points are still quite unclear to me. For example, the theory seems not be able to explain why whitening does not hurt testing when sample size is large, as demonstrated in the paper.  This work also does not draw connection between sample size and generalization error, which may make the claims a bit incredible. I would encourage the authors to work towards this direction and solidify the contribution. =========Post-rebuttal=========I thank the authors for their thorough response, which is well argued. Overall, yes, I agree with the authors that there are some differences between their work and prior work. I do not claim that there is zero novelty here. My concern is whether there is enough for this venue, given the high degree of similarity. As the authors do acknowledge that the hypothesis of "catastrophic forgetting leads to GAN oscillations" was introduced in other work, then the primary contribution here is replacing one continual learning method for preventing catastrophic forgetting in GAN discriminators with another, and despite the rebuttal, I still find the proposed solution to be rather ad hoc. I keep my score. =====Post Rebuttal======I appreciate the authors' responses and the revision of the manuscript. My point about Section 3.2 is not that the correction doesn't make sense, but that the reasoning is not quite convincing from an optimization point of view. I would suggest the authors to simply say what they have replied me instead of trying to link this part to Newton's method.I wonder why the warm up strategy is only applied to the proposed method but not others if that is effective. Isn't it an unfair comparison?I also appreciate the new experiments, but decided to retain the score unchanged. My major concern is that if the additional cost of other solvers is the main issue, then probably it would be better for the authors to directly show the training time as well so that the comparison can be straightforward. ----- UpdateThank you for the update and response. Unfortunately, some of my concerns remain. The plots are now run with 10 runs, rather than only 5 runs in Figure 5. But, they look almost identical (in some cases, maybe they are identical?). That is not possible, unless there is a potentially invalid choice in the experimental design. If nothing else, the standard error should change. The theory itself has some utility, since it is shown that learning in embedding space is equivalent. This is not surprising, considering it is assumed that there is a one-to-one mappings, but its good to be thorough. Nonetheless, this could maybe be shown more simply, and I am not sure Lemma 2 is exactly correct.Lemma 1 is overly complex.Alternative proof:Assume pr(a | s) is pi(a | s) (i.e., action probabilities given s are defined under pi).vpi(s) = sum_a pr(a | s) qpi(s,a) = sum_a int_e pr(a, e| s) de qpi(s,a) = sum_a int_e pr(a | e, s) pr(e | s) de qpi(s,a) = sum_a int_e pr(a | e) pr(e | s) de qpi(s,a) (also using Claim 5 like they do) = sum_a int_{f^{-1}(a)} pr(e | s) de qpi(s,a) = sum_a int_{f^{-1}(a)} pr(e | phi(s)) de qpi(s,a)Lemma 2 claims to show that the gradients are equivalent, but instead it seems to show that the functions themselves are equivalent and so should maybe be stated that way. Gradients are just placed in front of everything. Further the last step replacing d_0(s) with d_0(x) seems incorrect, as s is from a discrete space and x from a continuous space. Maybe you are suggesting that d_0 is some kind of delta distribution, but then it might be better to just sum over the same set of s.I am also a bit unsure about any smoothness assumptions required. Is J_0 even differentiable in theta? The requirements on the one-to-one mappings between discrete state to continuous state make for a piecewise flat function that could be problematic for such gradients.I also appreciate that Figure 2 was added. But, it is a bit hard to interpret. More explanation is needed there. ----------------------------------------------------------------------Update after rebuttal----------------------------------------------------------------------Thanks for the feedback from the authors.Unfortunately, although some parts are clarified, the main issues still exist. Overall, the novelty is limited and the motivation is still not clear enough.   **Post Rebuttal**: The authors did not provide any responses, so I decreased the scores. **AFTER AUTHOR RESPONSE**I have read the other reviewers carefully and the feedback provided by the authors. The reviewers had two major concerns: (i) Theoretical or empirical justification/proof for the following claim (and the motivation) of the paper: the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima. (ii) Lack of comparisons with newer methods from e.g. ECCV2020 etc.For the second, I feel empathy with the authors: In such a rapidly progressing field, it is difficult to integrate comparisons with new methods that are published while writing/submitting a paper. I am sure all of us have experienced similar problems.For the first issue, I disagree with the authors and agree with the other reviewers: This is an important claim that needs to be justified and I disagree with the authors's comment on the current results of the paper being a sufficient empirical evidence for the claim. An ICLR paper should have provided the necessary evidences and justifications. +++ Updates after author response +++I want to thank the authors for their answers as well as their attempts to improve the manuscript. I have read the other reviewers' comments and the updated version of the paper. The latter improves on the clarity of the approach in terms of the formal presentation of the approach, but a concise problem definition is still missing in my opinion. I feel that the paper still needs to additionally quantify who many labels are saved compared to classic cross-validation, as the models still need to be trained from scratch or fine-tuned. Here, it is important to establish when the method is actually (guaranteed to be) sufficiently concise, such that it can be used in practice. I therefore keep my tendency to reject the current version of the paper. -------------------updated comments:I thank the authors to provide responses before the last time of rebuttal. Some of the questions are addressed, but it seems many points still keep disagreement. For example, the contribution is still not clear to me The training cost is increased a lot, this can not be ignored, though I understand in the industry it may be less important. However, for academic research, we should care about this. Besides, it seems that all reviewers agree with the limited contribution and unsatisfied experiments.  ==============post-rebuttal:I have read all the comments from other reviewers and replies from the authors. All the reviewers are leaning to reject this paper due to the limited novelty and unfair and incomplete experimental comparisons. The authors' reply does not address my concerns, so I keep my initial attitude towards this work. Post-rebuttal feedback-------------------------------I thank the authors for their reply.> In contrast, our paper focuses on the following question: how can we reduce the number/accuracy of the samples the agent takes during the test phase? (Here, the test phase corresponds to the agents behaviour after the training is completed.)I agree with the authors that intrinsic motivation is different, and perhaps in my review I expressed this concern too strongly. So I thank the authors for their long and informative answer.> We believe that the reviewer refers to the problems where a possibly large multi-dimensional data such as images in games are used as input to the RL algorithm.Exactly. Experiments on high-dimensional problems would make the contribution of this paper stronger, considering the rather limited  theoretical/methodological impact that it has now. I strongly suggest the authors to work in this direction, perhaps on robotic application if possible.After the rebuttal, I still argue for rejection, although I increase my score from 3 to 4. [Rebuttal update]Thank you for your response.This alleviates some of my concerns about Theorem 1, although I feel like I'd need to see a revised version of the entire proof to make sure I understand it.On Figure 3, I'm not sure you've understood my concerns; perhaps I did not explain them clearly enough. Regularized models do no better than chance, and less-regularized models do worse than chance on test points. This is presumably because of what I mention in my review, which is that the synthetic data is basically noise. Thus the "improvement in test accuracy" isn't really an improvement, but rather that the model is no longer free to extremely overfit.On the interpretation of Dropout you provide, this differs somewhat than the message of your paper. I agree more with this interpretation, although not fully. Either way, the paper doesn't really contain strong evidence for that interpretation, which I think would be great to have.I encourage you to rethink the experimental setup somewhat and to have clear experimental support for the proposed intuitions/insights. I think this is a valuable research direction but I think that a more mature paper would have a much higher impact. _[Edit: The authors do not give any substantive feedback to my review, except for clarifying the hash choices. It is surprising that they object so vehemently to my intuitive description of their method as a heuristic to top-k, when they themselves write "Our proposed negative sampling scheme is a proxy to topK-softmax. It selects the top-k classes via LSH [...]". Also my reading of the sampled softmax is directly from their paper, showing a comparable accuracy-time tradeoff, but I was not refuted on this and instead was given other references claiming the inferiority of that method. I have updated my recommendation to reflect these shortcomings.]_ =======================================================================Update:I thank the authors for providing additional data, however the additional data is insufficient for me to recommend acceptance.   While the approach is certainly novel, it appears to performs worse in the relevant metrics than other methods while working on less standard networks.  As the networks are so far from standard, it is necessary to see how they (and the method) behave on commonly accepted datasets.Furthermore, AnonReviewer4 pointed out the similarities to AdderNet which I had overlooked.   Given these similarities I expect a more thorough methodological and experimental comparison to the original AdderNet. **After Rebuttal**I thank the authors for their detailed remarks and clarification. While it is encouraging to see that the authors have offered some clarifications and assurances regarding the validity of their approach, my main concern is whether the edit distance between the current work and the proposed modifications is just too high. It appears to me that there simply is too much that the authors need to modify in order to obtain an acceptable manuscript (with the other reviewers' concerns and suggestions as well). I believe that the paper can be significantly improved if the authors incorporate the comments from the current round of reviewing.  ***AFTER AUTHOR RESPONSE***I have read the comments of the other reviewers, which revealed that all reviewers identified the same major issues with the paper (novelty and evaluation). The authors did not provide a rebuttal but kindly thanked the reviewers and stated their intention for improving the paper with the reviewer comments and submitting it for a future venue. Therefore, I changed my overall rating to rejection. Update: I thank the authors for their response. Some justifications are provided and for that I will change my score. Overall, the paper still needs work. .###############################################################################I understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD. I still don't think that technically or conceptually, there is much difference. The authors presented many fresh results during the rebuttal (which might have been better presented just as a table in the manuscript, rather than on this thread). The experiments can form a part of a resubmission of this work, that will incorporate the extensive comments presented by the current reviews **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. Overall, I appreciate the clarifications and added experiments given by the authors. My concerns about the low novelty of the presented algorithm and findings remain, however, as I find the OOD attack to be only a slight modification of existing adversarial attacks.I also appreciate that the defense solution claim has been weakened and moved to the appendix, yet these promises are still left to be validated.Lastly, I find all the many added experiments positive, but these have significantly changed the content of the initial submission at this point, which is somewhat out of scope of the ICLR rebuttal phase (see Q4 in the FAQ of the Reviewer Guide: https://iclr.cc/Conferences/2021/ReviewerGuide).For these reasons, I would keep my recommendation to reject this work for ICLR 2021, but I encourage the authors to further improve and re-submit the now extended work to some future venue.##### Update: Thanks for the authors' response. However, I am not convinced on several points, e.g., (3) - (7). Considering the other reviewers' comments, I think the paper needs to be further improved. Thus, I will keep my score. ==============================================================================================Thanks for the authors' response. I am still inclined to reject this paper. Compared to the existing ControlVAE, the contributions of this paper looks incremental. More experimental results might be necessary to make the paper more convincing. =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft, in particular with respect to the new experiment on the Criteo dataset. Regarding the following arguments:1- *"The dependence of regret on the prior applies to any Bayesian method, in the same way that tuning the learning rate for gradient methods is necessary."* I agree that, for batch methods, the prior can be tuned to the data. Nevertheless, in an online prediction setting, the prior needs to be chosen before observing the data and once you start predicting you can't go back and change the prior for the already predicted sequence. 2- *"To the best of our knowledge, none of the literature that proposed practical (online) Bayesian methods proposed regret bounds or even studied regret for these methods"*Regarding both arguments, there is recent literature that proposes a practical nonparametric online prediction method with regret guarantees for a fixed given prior:*Lhéritier, Alix, and Frederic Cazals. "Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees." Advances in Neural Information Processing Systems. 2019.*Therefore, unfortunately, my first two concerns (theoretical guarantees on the regret and strong dependence on the prior) remain so I retain my original decision. ==========Post rebuttal==========The authors' response does not fully address my concerns. I keep the rating as it is.  ========I keep the score after reading the rebuttal / author comments. I thank the authors to conduct a lot of additional experiments. Since the authors also agree that "two nearby states can have very different representations", it doesn't make sense to treat nearby states as positive pairs (and use SimCLR) to learn the representation from the beginning, which contradicts the purpose of the entire paper. If the authors cannot show other benefits of this representation learning (other than the performance boost on a 1-2 tasks), then the performance gain could be just due to other reasons that are not known. The additional experiments (e.g., Appendix F) only compares Random with RIDE-SimCLR, and I wonder what's the performance of RIDE? Overall the authors need to rethink the proposed approach in order to give a consistent story of what is a good representation for RL exploration.  ---Update: The added generalization tests and performance numbers strengthen the paper, and make the aim more clear. The paper seems now mostly focused on accelerating MPS (or potentially other SPH variants). While the new result table shows speedups compared to MPS for model inference, in practice the model still is bottlenecked by NN search, and total runtime of inference+NN is not significantly different to MPS. And if speed is the aim, the appropriate baseline comparisons should be to methods which -- as this method-- use domain knowledge and solver internals to speed up the solver (e.g. Ladicky et al.).On the method side, it does seems like the proposed approach is limited by supervising specific MPS subcomponent separately, which hinders learning richer intermediate representations, applying the model to other systems, or taking significantly larger timesteps. There could be potential benefits of the specific architecture chosen, but their effect is a bit unclear; as for the comparisons shown the most significant effect is supervision on subcomponents.I will therefore keep my score. After author response:See reply comment in the thread below for further score justification.Thank you for the work you have done in running the additional experiments and revising the manuscript.Two major concerns, and why I do not think a direct comparison across all things in Table 1 is warranted still:This is to do with point 1. (ii) of my initial review. The bigger handicap here is the fact that planning/search algorithms are being compared to RL algorithms. These have fundamentally different assumptions, assuming that the simulator is resettable at will effectively to anything but the starting state is different problem than what the RL agents are trying to solve. There's nothing wrong with making the kinds of assumptions that the authors have made (deterministic simulator/hard constraint/etc) but comparing it to those that do not in a single table is a bit of an apples to oranges comparison.The KG-A2C with hard constraint is not a very good baseline given that it was never designed to exploit the hard constraint in the first place. It was designed to account for the additional difficulty in having the soft constraint by filtering actions via the graph - once this is done by the hard constraint there is little separating KG-A2C from vanilla A2C (the original paper indicated that the KG input representation portion did not account for any increase in final score) and in fact some games received an increase in score when the KG/soft constraint combination was relaxed. Once again, there is nothing inherently wrong with any of these methods - just that it becomes an issue when you attempt to compare to it side by side.This being said, the handicap clarifications added in table 1 do help, though the prose still compares them all to each other directly. PUCT-RL and MC-LAVE-RL are the only two directly comparable things there. In appreciation of the authors efforts during the rebuttal phase, I will raise my score though overall still do not think this paper is ready for acceptance. ------------Post Rebuttal----------------I thank the authors for their response. I disagree about epistemic and aleatoric uncertainty. Bootstrapped DQN (Osband et al. 2016), Randomized prior functions (Osband et al. 2019), and several other works show that to get the variance of different possible Q-functions given the data, $p(Q|D)$ or differnet possible MDPs given the data, $p(M|D)$, you need backups consistent in time, i.e. the same dropout mask is to be used for both the main Q-network and the target Q-network for the backup. This is the uncertainty that we trypically need in offline RL and is also used in Theorem A.2 (for the high probability bound which is given the data), and that's why ensembles with Q-functions consistent with themelves are typically used. By merging the target values across different dropout masks, the uncertainty is not timewise consistent. While the paper that the authors point does actually do dropout masks with Q-function at each step, it is discussed in later works including Osband et al 2016, 2018 that not being consistent over time is leading to wide uncertainty estimates.  I would recommend the authors read the discussion on Posterior sampling for RL vs Optimism and Thmpson sampling for a discussion on this.I am a bit disappointed with the rebuttal. I expected a comparison to such metods that perform timewise consistent uncertainty estimation and also to distributional RL, since the algorithm that the authors use can also be drawn similar to a set of particles of Q-functions and performing a backup using target values computed using all of the possible particles, which is essentially what, for instance, QR-DQN does in a way or even IQN does in a way. Even REM would have been fine. Without this comparison, I unfortunately cannot increase my score and I am going to retain my score. ---------------Update:I thank the authors to give responses to my points, especially the discussion about novelty. But I still feel the success of KNN for NMT is similar for LM, that's why a lot of works study on NMT are also work on LM. Since this KNN method only targets at the decoder side, same as LM model. Therefore, I still feel not novel enough.  --------------------------------------------------Post rebuttal:I have read the authors response and appreciate the effort made to improve the paper. But I think the results still need additional work, especially from the theoretical front. So my original score is unchanged. ### UPDATEThe presentation of the paper is now convincing, with the background, contributions and concepts clearly stated. I hence increased my score. However, I agree with reviewer 2 that the library is not properly tested (I understand it can be hard to test the whole computation, but unit tests could be easily provided) and that it would have a higher impact if it were more modular, so that a user could easily add the loss analysis directly in her workflow.Moreover, I also think that the paper should be rejected given that the first submission wasn't anonymized and the paper wasn't in a state of being submitted. ---**After rebuttal**The authors' responses do not address my major concerns (the first two). I do not think the responses directly answer my questions.So, I keep my score unchanged.    #### **Post Rebuttal**I'm afraid the authors failed to answer my main question regarding the results and the applicability of their proposed approximation.Therefore, I decide to keep my score.--- ---Update after rebuttalI thank the authors for the response. After reading it, the revised version and the other reviews, the concerns expressed in the initial review are still valid.Then, I keep the initial score. ===============================================================================================Update: Since the authors did not give any feedback for the reviews, I retain my original decision. Thank you. $\textbf{Final Rating}$Based on the authors' responses during the rebuttal period, I don't believe that the paper makes a sufficient contribution for ICLR. Hence I will stick to my original score. Summary after Discussion Period:-----------------------------------------------After reading the other reviewer's comments and corresponding with the authors, I have become convinced that the author's proposed regularization method is novel and effective, and would recommend this avenue of research be further explored. Yet it has also become clear to me that the author's claims on why their method works are not yet supported by evidence. Further, I don't believe the author's proposed further ablation studies would fix the theory, since such experiments don't address whether their method works by fixing problems with Laskins work (as the author's claim) or because it provides a more direct way of enforcing invariance to transformation (as I claim).So we're left with a difficult situation, the method and the experiments are good while the theory is lacking. In such a situation both acceptance or rejection seem reasonable. Yet, as per ICLR reviewer guidelines, one should answer three questions for oneself: - What is the specific question and/or problem tackled by the paper? - is the approach well motivated, including being well-placed in the literature? - Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.Since the theory is lacking and the approach is not well motivated, and since the theoretical claims haven't been rigorously supported, I feel as per ICLR guidelines the paper is not yet ready for acceptance. -- EDIT: I am thankful to the authors for their insightful answer to my concerns, and for the though work in reviewing the manuscript. At this stage, I would keep my score (also in light of the other comments), but I hope that the authors find the suggestions from all the reviewers useful to re-present a more consolidated work soon. Some references that demonstrate size generalization is a well-known issue.ReferencesMeasuring abstract reasoning in neural networks. Barrett et al 2018.Learning TSP Requires Rethinking Generalization. Joshi et al 2020.Learning Combinatorial Optimization Algorithms over Graphs. Dai et al 2017. ===== after rebuttal =====I'd like to thank the authors for their revisions, which have significantly improved the readability of the paper, and the presentation of the results. The addition of fairness violation/accuracy tradeoffs and also (Kamiran et al. 2012) add a lot of value in putting this paper in the right context. After reading the rebuttal, I still remain unconvinced about the contributions of this paper. From a practical point of view, the performance of the proposed algorithm is on a par with (Kamiran et al. 2012), which is a baseline for demographic parity and has been improved on several times in the past 8 years. On the other hand, the main claim of the paper seems to be theoretical optimality. Unfortunately, although several previous mistakes have been corrected in the proofs, I cannot still follow the proofs of Theorems 1 and 2. New notation pops up all over the proof, and it is unclear how to follow some lines from the others. Given this, I am increasing my rating to 4 in acknowledgement of my previous misunderstanding of the definition of the statistical parity, which was cleared by the authors during the discussion period. However, I am still unable to recommend this paper in the current form for publication in the conference proceedings. =============UPDATE POST DISCUSSIONThank you for your responses.- I am more convinced about the novelty of the proposed methods.- I am still unconvinced about relabeling being the best we can do for out-of-distribution state distributions. Building learned methods that generalize more broadly or many-inner-step meta-learners could help quite a bit.- **I got concerned that a question by AnonReviewer2 "Why for in-distribution experiments, Experience Relabeling was removed?" went unanswered.** Having evaluated MIER-wR, adding MIER to the plots during rebuttal should have been very easy. Moreover, in practice, we shouldn't be able to leverage the fact that we know a task is in-distribution or out-of-distribution and choose between MIER and MIER-wR accordingly (which would also be a bit ugly). Therefore, in my opinion, putting MIER-wR as a representative of the paper instead of MIER is not valid.Since most of my concerns pre-discussion are still there and we cannot assess whether MIER does well on in-distribution (and MIER-wR is not better than MQL for OOD tasks) I, unfortunately, have to decrease my score from 5 to 4.- This prompted me to take a closer look at the actual numbers of the plots and I got more concerned. I didn't take it into account in my rating update because I should have brought this up before the discussion:    1. I feel something is off with PEARL vs MIER-wR when looking at in-distribution vs OOD in half-cheetah vel. In particular, we see PEARL does considerably better than MIER-wR on in-distribution (Fig 2), but its performance starts already much lower than MIER-wR in Fig 6 left.    2. Another concern is whether the numbers for OOD are "decent" in the sense of creating meaningful policies. Half-cheetah-vel-hard has a reward of ~-325, which is lower than MQL with 0 data for in-distribution half-cheetah-vel. This may be because the evaluated tasks are different, but raises concerns that maybe all baselines and MIER are all doing poorly. UPDATE: I read the authors' response. They say "Thus, it is unclear that generalization to unseen viewpoints shown in our results is completely explained by the similarity between the seen and unseen set,..."I agree with this comment. I don't say we can prove that the similarity fully explains generalization to unseen viewpoints. It's my conjecture. However, the authors themselves seem to recognize my conjecture could be right; at least, it cannot be eliminated. In other words, a major issue with the current manuscript is that we cannot derive a firm conclusion from the experimental results. If my conjecture is true, then the results are almost obvious and not interesting. I want to lower the score from 5 to 4, as the authors' response makes me believe my concern is valid.I think the authors tackle quite a hard problem and admire their efforts, though.  -------------------**Post Rebuttal**I have considered the revised article, additional reviews and rebuttal and decided to slightly raise my score but I am still in favor of rejecting the paper. Below is a summary of my reasoning.--------The authors have provided a good rebuttal and I am overall pleased with the detailed response, additional experiments and figures, and overall exhibited transparency. Unfortunately my assumption about $t_k$ seemed correct when considering the additional L-BFGS-B results, which indicate that using standard $t_k=1$ is a really strong baseline that proved difficult to beat.I would suggest finding another set of problems where $t_k=1$ is not so good for L-BFGS or consider adapting another first-order algorithm for which it is clear that the step-length needs to change between tasks and architectures. **Follow-up to author comments**I appreciate the authors' thoughtful response. While I agree on certain points, I am not convinced that the paper, as currently written, is ready for acceptance to a venue like ICLR. (That said, I also still believe the paper's core technical claims to be correct; my question is about significance.)The paper's premise is that reconstruction attacks on InstaHide deserve extensive investigation. I'd like to see justification for that premise. To be absolutely clear: I strongly believe that techniques (like InstaHide) which attempt to provide security against moderately strong attackers deserve discussion and investigation. However, I also believe that the starting point for such work should be a careful attempt to formulate security goals. I don't see how the current paper advances the important parts of that discussion. The focus on reconstruction is narrow; my score reflects that.To add just a little bit to my review: InstaHide is best viewed as a proposed lightweight alternative to multiparty computation (MPC). MPC protocols allow participants to compute on shared data in a way that reveals nothing but the final outcome of the computation (tools like FHE, to which InstaHide's authors compare it, can help achieve that goal but are not qualitatively different). MPC protocols (and InstaHide in particular) say nothing about how much is revealed about the data by the final trained model (the "ideal functionality", in the language of MPC). There is at this point a large literature showing that models themselves leak information in surprising ways (membership inference, to pick an example that received recent attention).Even if we focus on "lightweight MPC" as the end goal, the literature on data privacy suggests a wide range of more sophisticated measures of security than resistance/vulnerability to reconstruction. (That is, it's interesting and potentially important to relax the goal of full simulation that one normally aims for in MPC; but then one should spell out what the relaxed goal is, why it's sufficient for some settings, etc.) This submission reflects none of the past decades' lessons on that count. Responding to specific points: * I was not comparing to the paper of Carlini et al. I assume that this submission and the manuscript of Carlini et al are independent. * (Minor point) I'll stick by my complaint about Page 3: "it is clearly information-theoretically impossible to recover anything about [...] the private dataset." I understand and agree with the math of the rebuttal, but not with the conclusion. To spell out my original objection: Let $V=(V_1,V_2)$ be the random variable consisting of the two private images, and let $W$ be their average. It is not true that the mutual information $I(V;W)$ is 0 (which is the natural meaning of "no information about the data set"). It is not even true that $I(V_1;W)$ is 0.  Concretely, learning $W$ makes certain pairs of images much more likely than they were a priori. Whether that's ok depends on the context, what else is likely to be  known about the images, etc. My point isn't that one released image will lead to a practical attack; it is that information leaks in lots of ways. If you want to claim that leaked information isn't useful for an attack, that requires a clear notion of "useful for an attack" (and possibly a proof, though that's less important than a clear claim). ###Final Recommendation###Based on the discussions with other reviewers and AC, this paper is not ready to publish at this stage mainy due to the following reasons:1. the big claim of causality as also pointed out by R62. the writing should be significantly improved and the experiments lack details as pointed out by all reviewers3. the new problems found during discussion with the AC regarding the ablation study, and seeds, etc. In summary, this paper presents an interesting idea, but the experiments and writing in its current shape make the paper insufficient to be published at ICLR. The authors are encouraged to polish the paper in writing and experiments for future resubmissions. ### Update after discussion period ###Good idea, but the results don't clearly support the authors claims. I lowered my score. Update:I have read the rebuttal and the updated paper. I don't see my issue of relevance addressed. My score remains the same. ---I read and appreciated the response, but my overall rating is still leaning negative. After reading the other reviews and responses, I still think the paper needs further improvement before it can published. ===============After rebuttal ===============================After reading all reviews, considering author rebuttal and AC inputs, I believe my initial rating is a bit generous. I would like to downgrade it to 4. It has been pointed out that many recent works that are of a similar flavor, published in CVPR 2018 and ECCV 2018, have slightly better results on the same dataset. Further, the only novelty of this work is the proposed factorization and not the encoding scheme. This alone is not sufficient to merit acceptance. Update after feedback: I would like to thank the authors for huge work done on improving the paper. I appreciate the tight time constrains given during the discussion phase and big steps towards more clear paper, but at the current stage I keep my opinion that the paper is not ready for publication. Also variability of concerns raised by other reviewers does not motivate acceptance.I would like to encourage the authors to make careful revision and I would be happy to see this work published. It looks very promising. Just an example of still unclear parts of the paper: the text between eq. (3) and (4). This describes the proposed method, together with theoretical discussions this is the main part of the paper. As a reader I would appreciate this part being written detailed, step by step. Uodate: Read the rebuttal. My score remains unchanged. Final Evaluation================I have gone through the other reviews as well as the author response.Firstly, I would like to thank the authors for providing detailed responses to my questions.In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.Moreover, from my understanding the analysis in David McKays book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron). As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper). Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review). In general, I feel this section could use some tighter formalism and justifications.I also remain unconvinced by the response to my issue with the claim Our experiments show that our networks can remember a large number of images and distinguish them from unseen images, where the negative images are also seen by the memorization model, so they are not unseen. The authors address this by saying 3M of the 15 M negatives have been seen. That does not seem like a small enough percentage to claim that these are unseen images.In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating. 1. Re the distribution assumption, the response from the authors is not convincing. The paper you mentioned (https://arxiv.org/pdf/1805.11046.pdf#page=9&zoom=100,0,96) says that, when using BN, "quantization preserves the direction (angle) of high-dimensional vectors when W follows a Gaussian distribution", this has nothing to do with your assumption that W follows a gaussian distribution.The original question was not that "gaussian -> low quantization error -> good performance" (I think this is clear in the past 3 years) but rather "non-gaussian -> high quantization error -> bad performance?". Recent work suggests this may not always lead to bad performance (e.g. there are binary models with good performance and high quantization error). What does Figure 5 show? That quantization error is similar for analysis and simulation. Is this level of error "small"? Clearly, it depends on the number of bits. The gaussian assumption is not true for lower bit networks (the paper you referred uses 8 bits). Overall, the distribution assumption is a weakness.3. The point was about more datasets like VOC, beyond image classification. Thank you for improving the paper, I have increased my rating appropriately. ----Post discussion.After reading some of the author responses I have decided not to update my score. I think the paper needs a bigger revision and more results to be above the acceptance threshold. REVISION:The other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first. --- After rebuttal --- Still not convinced of the value of the work to the community. Will keep my score the same. *Update after discussion period*I remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks. I read the author feedback and decide to keep my score.  The response does not fully address my concerns. After going through the authors' comments and the revised version of the paper, I keep the rating as is. The paper needs a more convincing evaluation section as well as some clean up (e.g., references to figures and tables in the text) Revision: Updated my rating to acknowledge that the reproducibility issue is addressed.   ==== After rebuttal ====The authors' feedback clarified some of my concerns. But my main concern about why minimizing the objective function can reduce both error and the number of parameters still remains. So I changed my rating to 4 from 3. ---------------------------------------------------------------------------------------------------------------------------------------------------------------------Update:Thanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. However, I still found some claims made by authors problematic. For example, it reads in the abstract that "...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters...". Clearly, Li et al. 2018b has a differentiable formulation which falls under your description.Besides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. Also, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.Directly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.Therefore, I would like to keep my original rating. After the rebuttal:I read the authors' comments and understand more the technical results. I raised my score. But I still feel that the techniccal contribution is a bit weak. AFTER REBUTTAL:I think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. ------------ Update: I have read the authors' response, and have decided to keep my score as-is ====== Edit after author response ====I have read authors' responses and have decided to keep my score as is.The additional experiments haven't addressed problem formulation issues (W1, W2, W5) if the paper is positioned as more of a theoretical work; if the paper is positioned as a more of an experimental work, the baselines used (W3, W4) need to be improved with proper hparams settings. ========================================================================I thank the authors for their detailed rebuttal. However, their accuracies are still very below sota. Therefore, I am inclined to stick to my original review and rating. After author response:I would like to thank the authors for providing response. For the second point, I am still a bit confused about spectrum of perturbations vs log-spectrum of adversarial examples. The authors agreed that my third point was correct, i.e., the current results do not give enough insights on how to improve robustness against adversarial examples, in the real-world white box setting. Given the above reasons, I decided to keep my score. ===================Post Rebuttal Update:While I think the proposal is interesting, I still think the proposed methodology lacks a clear presentation of the studied (causal inference) problem and statement of its underlying assumptions, as it has also been pointed out by other reviewers. Despite some clarifications from the authors I still vote for rejection as I believe the paper requires a major revision. Post-discussion update: The authors only partially adressed my concerns in their rebuttal. The paper suffers from lack of comparisons: only 2 baselines are compared, and only on few systems. Crucially the new Navier-Stokes experiment lacks comparisons. The authors also couldn't respond to my questions about research context or scope: it's difficult to assess what this work actually claims in relation to competing methods. For a machine learning paper this is not enough.----- **Post Rebuttal Comments**: The authors improved the paper during the rebuttal but the clarity is not sufficient and the results are still puzzling. I do not fully understand how one can learn the perfect solution with only 3-4 data points. **Acknowledgement of author rebuttal** I appreciate the detailed discussions that the authors have provided. However, my concerns still remain, and I am not convinced to improve my overall evaluation.  Updates: I would like to thank the authors for their response and the updated draft. Unfortunately, I believe the above major concerns are still valid and therefore retain my original rating. Overall, while the subject area of the paper is exciting, unfortunately, the execution (both empirical and theoretical) is weak. I tend to remain to vote for rejection with encouragement for a more thorough empirical and theoretical investigation of the problem.---post rebuttal---After reading the authors' response, the other reviews, and the revision to the paper, I find that my comments are not sufficiently addressed. The author did not even acknowledge the existence of the prior work, REPAIR, in the revised paper. The imprecise mathematical expressions are still in the paper despite feedback from multiple reviewers. From a practical point of view, the developed algorithm is not scalable as it requires to (almost) solve the inner maximization at each iteration (based on the rebuttal), and it only works in the significantly overfitting regime (the authors are yet to show its performance in a more interesting regime). From a theoretical point of view, the applicability of the theory is also extremely limited to the perfectly overfitting regime, which does not capture the real world. In addition, I agree with AnonReviewer4 that the proofs are inscrutable.  I regret to say that despite the fact that the subject area of the paper is exciting, I am adjusting my score to 4 post rebuttal. --------------------Response after rebuttal:Thank you to the authors for their response. I appreciate the detailed answers to each of the prior works. I still do think the paper needs to be more clear in the problem and differentiation with prior work in the writing itself, which will require a non-trivial update to the paper.On the computational results + baselines side, while the authors have run the experiment and have described these qualitative behaviors, this isn't a substitute for quantitative results, especially because this is important to show when comparing with baselines. The authors say "We have updated Section 4 to be clearer about what baseline approaches would do in the environments we have tested". It's important to show evidence that this is what the baselines actually did.Based on these points, I don't think the paper is quite ready for publication yet. ---### RebuttalI thank the authors for their detailed reply. I still consider the contribution to be too high-level and to cover too much ground without going into sufficient depth. I am not sure I can follow the argument about Kolmogorov complexity. Its chain rule is also only equal up to a logarithmic factor. I will keep my score the same. %%% After the Author Responses and Paper Updates %%%I would like to thank the authors for very seriously considering my recommendations and genuinely attempting to implement many of them, even in their proofs. I do think their updates made the paper stronger in general. A minor general note is that many of the newly edited sections have typos and could benefit from proof-reading. Following are my final remarks:After going through the paper again, I realized a step in the proofs which indicates an extra assumption that is not mentioned in the main paper. The proof in lines (14,15) of Section 7 (supplementary material) seems to assume that the exogenous variable has a fixed distribution, i.e., despite two different models inducing the same observed distribution having two different functions f_x and \tilde{f_x}, their exogenous noise \epsilon_x must have identical distributions for the proof to go through. Similar steps are used for the exogenous noise of Y as well. These steps can only be explained by the very strong assumption that the exogenous noise term of every variable has a fixed distribution across different causal models. Such an assumption is not mentioned anywhere in the paper as far as I can see - this has to be definitely addressed. Moreover, I do not think the identifiability result is very useful when obtained under such a strong assumption. Unfortunately, I cannot recommend acceptance due to this. But I encourage the authors to pursue this direction and seek out ways to relax this condition.I had brought up the point that C is not used to induce correlation within a dataset, which takes away from the essence of the practical problem they are trying to address. In order to address this, i.e., to be able to handle the confounding within each dataset, the authors added an extra condition on the effect of confounding: They assume that once p_1(x,y)=p_2(x,y), this implies p_1(x,y|c)=p_2(x,y|c) for all c. This extra assumption allows the authors to use the machinery they developed as is with this additional argument.Unfortunately, this assumption, much like the others, is also presented in between the lines. I think it will be really helpful if the authors could explicitly write down their assumptions in a theorem environment (\begin{assumption} ... \end{assumption}) and make them very explicit rather than only within the theorems or in-text: Assumption 1: ANM Assumption 2: Exponential family Assumption 3: ... etc.This relates to the implicitness of another key assumption made in the paper: \Gamma matrix is assumed to be full rank. This intuitively suggests that the experimental conditions are sufficiently different. But this is an algebraic statement and is hard to interpret. I would recommend the authors to think about how to interpret this condition, i.e., assess how it impacts the conditional distributions - exponential assumption allows them to make an algebraic assumption here, rather than probabilistic; but a probabilistic interpretation would be more intuitive.Some of the typos that I can see: "and functions the prior of distribution of C"->"and functions as the prior of distribution of C"typo in (62): "R"->"r""We generalize the identifiable result in theorem 4.3 "->"We generalize the identifiability result in theorem 4.3 "Many of the arXiv citations are actually published in various venues, please go through the bibliography and update. POST-REBUTTAL:I thank the authors for their response. While some of my concerns have been addressed, a few key questions haven't been answered.* The contribution is limited in novelty.* The general author response mentions that reward shaping is not used in the proposed method. In that case, Sec 3.1 seems a bit pedagogical and misleading, since MDP for AL is described in detail but is not even empirically compared with the proposed method.* Regarding the ablation studies on a different classifier, while I agree that SOTA networks need to perform well, the ablation study on a different classifier was suggested to rule out the effect of the choice of SOTA classifier in the effectiveness of the proposed method. Also, the authors respond to R1 that the gap between the ensembles' performance and each single classifier's performance is small since they chose SOTA models for the individual classifiers. This is perhaps even more reason to show how ensembling works when the base models are not SOTA.* Beyond just a statement in the response, it would have been good to see some empirical comparisons between the proposed method and, say, Q-learning-based RL methods - especially in terms of actual running time complexity.* I also agree with R3 that similar ideas have been explored before (papers cited in R3's review), and it is important to compare with those methods as baselines in the experiments.* Also, the choice of the datasets used is not justified appropriately.I stay with my original decision. Post Rebuttal Comments: Following the discussion in rebuttal phase and after reading all the other reviews (and authors response) , I feel that the paper is not ready for submission. While authors did address some of my concerns, the evaluation strategy seems flawed and the compared methods are not representative (as pointed out by reviewer 1). As a result, I am changing my rating from 6 to 4. Thanks! -- after rebuttal --Thank the authors for their responses.  EDIT: The author response addressed some of my concerns. In particular, it confirms that the experimental results are impressive compared to many baselines. However, I would appreciate the distinction between easy and hard confident examples much more if the authors went beyond illustrative figures and defined this concept more precisely. Without a precise definition, it's difficult to verify the paper's claims about why the method performs well. Based on t-SNE visualizations, the author response offers an alternate definition of "far away from the cluster centroids." The submission would be much stronger if it developed this idea further and analyzed it quantitatively.Next, the authors suggest that methods cannot distinguish hard confident examples from mislabeled examples using the "small loss trick" alone, and that their "momentum trick" is necessary. However, they do not present a principled argument or strong evidence to support the claim. In fact, some recent methods do show a separation between these types of training data using measurable quantities, see Figure 1 of [3] and Figures 1-2 of [4].Finally, the authors claim that reinitialization helps escape bad local optima. However, I do not see how low standard deviation supports this claim.[3] Pleiss et al. Identifying Mislabeled Data using the Area Under the Margin Ranking, Neurips 2020. https://arxiv.org/abs/2001.10528[4] Swayamdipta et al. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics, EMNLP 2020. https://arxiv.org/abs/2009.10795] The authors have not prepared a rebuttal.  #### Update after rebuttal:Thanks for the detailed feedback. While I still find the technical novelty is limited comparing to [Liu et. al.]. For this reason, my score remains unchanged. Additional comments after the rebuttal phase: On the positive side- the authors are clarifying a few things in the rebuttal and also in the paper such as table 1 - for that table results are clearer now by updating the table and caption. On the negative side, however, I am less convinced after the rebuttal than it looked to be prior to the rebuttal. Let me give some of the most important things- claiming that the results in table 2 and 3 are statically significant is hard to believe. The authors claim that they are but it is not clear what the mean by "standard deviation in mAP for each of the datasets... is within .1mAP" in their rebuttal - what is varied to get this standard deviation? Learning a detector e.g. with different random seeds will result in much larger differences than 0.1mAP - thus claiming this it is statically significant is actually not scientifically credible to me - while I understand the arguments the authors make while standard evaluation metrics are potentially not ideal or comprehensive, I am still quite strongly unconvinced about the novel metric mEAP - that seems very specific for the setting used here and does not lent itself for easy understanding and is also dependent on some threshold that is not clear how to choose. In that sense all the experiments using that particular measure are still not convincing to me. Additionally, the authors do not make a real attempt to make this measure more accessible in any way. It is mostly stated that mAP does not work. Even though, as the authors point out, for most labels there is a "joint" label across datasets which allows to evaluate that directly at least for those joint labels (and these are the majority of classes apparently). Also in the rebuttal the authors simply defend their metric rather than to acknowledge that this is not particularly useful. As said - rather unconvincing and I am sticking with that. The rebuttal is making me even less convinced about that metric as no attempt is made to show that the metric is sensible and fair. - I strongly recommend to NOT use the term zero-shot. It is not only confusing as mentioned before but also does not really apply for most labels (the authors mention themselves that for most labels there is a corresponding label in the other dataset) - thus is more of a domain-adaptation or label-adaptation problem than really a zero-shot setting. The authors defend the usage of the term zero-shot which I do not find praticularly unconvincing. minor- the so called "zero-shot cross-dataset generalization" setting is not properly defined. It is mentioned at the beginning of sec 5 without being properly introduced what really is meant. - typo first line sec 4.1: detectpr -> detectorI really would have liked to see a strong rebuttal given the good results for the ECCV challenge and the importance of the problem. However, the rebuttal nearly caused me lowering the score. So overall the rebuttal has made me less convinced about the paper than before. Sorry to say.  Update after rebuttal:Thanks for the response! Combining deep generative modeling with evolutionary algorithms is a very interesting idea in general. I hope the authors can continue to improve the paper (especially on the evaluation part) and resubmit in the future.  ----------------------------------------------------------------------------------------------------Comments after rebuttal:I would like to thank the authors for their response. I am keeping my score, but I encourage the authors to resubmit after improving the things that were discussed (most importantly, using better metrics and comparing to more established baselines); I think this will make the paper much stronger. --------------------------------------**Update**: After reading the assessment of other reviewers and the referenced papers in the intrinsic reward literature, I am reassured that the methods/metrics proposed in this paper are not novel and, as pointed out by other reviewers, have been studied under other terminologies in different prior works. The analysis of these metrics' correlation with human data is still an interesting piece of result but is not significant enough to become the sole contribution of an ICLR paper. Therefore, I move my initial assessment of 6 to 4. Update: Thank you for the feedback and the efforts in revising the draft. Some of my questions were clarified and many existing errors have been fixed. However, I still think the novelty is limited, and more needs should be done to enrich the method with more analysis in terms of theoretical and mathematical aspects. Based on the above reasons, I do not intend to increase my rating. Update: After discussion with the authors, I am inclined to lower my score. While I find the architecture proposed by the authors to be interesting, I do not think they have done enough to motivate the connection with neural networks. I find this especially troubling since the language used by the authors continues to imply that the connection is obvious. I would encourage the authors to look into the literature on scattering networks (e.g. https://arxiv.org/abs/1203.1513) for another approach to explaining networks from first principles that, I think, does a better job of making the connection to realistic neural network architectures.  Edit:### RebuttalRemark 1:  * Figures are much clearer; * about Figure 1, I see that MNIST has been added, which is good;Remark 2: * "We can always normalize inputs such that they are distributed as N(0,1)": this is not true. At best, we can put the mean to 0 and the variance to 1. It does not mean anything in terms of distribution *shape*; * I maintain that experiments with MNIST were missing in Figure 1 in the first version of the paper. This is more important than pretended in the rebuttal, because, in the MNIST dataset, the pixels are not N(0, 1) (even when centered and normalized), so MNIST does no meet the approximation made in Annex A.2. However, the results are not too different from those obtained with the other datasets.The remark about the limitation to piecewise linear NNs has not been correctly addressed by the authors. The setup used in this paper corresponds to "Piecewise linear NNs", which is *really* limiting, since the biases are supposed to be zero. This is very different from "Piecewise *affine* NNs", which are actually widely used (biases can be non-zero). Adding the influence of the biases would break the entire computation made in this paper (or, at least, would necessitate more effort to take it into account). I assert that this limitation is not discussed in the paper, and my attempt to discuss it here has been eluded in the rebuttal.Despite the new and more fashionable figures the authors presented in the rebuttal, I am disappointed by its lack of accuracy and vagueness, especially when discussing the "piecewise linear NNs". So, I lower my rating. ---## UpdateI have read the revision and the rebuttal. I have also re-read the initial submission for comparison.In the revised version, the authors have added "(4) Tune $\lambda_1$ such that $CP_{\mathcal{D}'} > 1-\alpha$  where $\lambda_2$ and $\lambda_3$ are fixed from (3)." after (3) in Algorithm 4, which substantiates their claim about the marginal coverage guarantee. As my other questions under #1 were all in response to the apparent absence of a valid calibration procedure, with the introduction of this line in the revised version, I have no further complaints about the correctness of the procedure itself. I still strongly recommend including Algorithm 1 in the main part of the paper, as a prediction interval is rather meaningless unless the associated coverage level is also known.The biggest reason why I am keeping my score as is that after going through all the reviewing material, some of the recurring questions appear to be pointing at a larger issue with the submission.1. It is repeatedly emphasized that the proposed method "outperforms the state-of-the-art algorithms on high-quality PI generation." This is great, except that it is hard to see *what* about the method is causing this improvement in performance. Is it the $L_{CA}$ component? Is it some non-obvious differences in architecture or in hyper-parameter tuning? Why should there be such a difference in practical performance for the simultaneous training vs a "decoupled" approach, leaving aside the practical concerns such as the computational cost?Now that I have been thinking about this paper for awhile, I suspect that a great deal of the questions that the other reviewers and I have been asking are really about this need for *some* explanation for the improved performance. In my opinion, the current version does not provide enough evidence to *convince* the readers that the excellent empirical performance reported in Section 5 is an inevitable consequence of their novel method. This makes me cautious.2. Throughout the review process, I couldn't escape the sense that the authors themselves have not settled on the central message. On this point, I am with R3. There is a lack of clear messaging on whether the focus is on (a) high-quality PI generation or on (b) estimating conditional coverage or on (c) both. About 3/4 of the way into the paper in my initial reading, I received the impression that the paper was definitely about (b). However, I revised my opinion and switched to (a) after going through the experimental section. After reading the first batch of the comments posted by the authors, I thought that the paper must have been about (b) all along. The last comment posted by the authors threw me into doubt yet again, however, as it seemed to indicate (c) as the correct conclusion.In my opinion, both these issues need to be addressed before this otherwise interesting paper can be ready for publication. After author response:I disagree with the discussion on MSE. For the empirical estimator you mention, we have:$$E[(Y - \hat{P}(X))^2] = E[(Y - A(X))^2] + E[(A(X) - \hat{P}(X))^2]$$Importantly, $E[(Y - A(X))^2]$ is a fixed value regardless of what $\hat{P}$ you use. So while you cant compute $E[(A(X) - \hat{P}(X))^2]$, you can compare whether this is higher or lower for a particular $\hat{P}$ by just comparing $E[(Y - \hat{P}(X))^2]$.By the way, this is directly analogous to classification. In classification, Y | X is stochastic, it is 1 with some probability A(X) and 0 with probability 1 - A(X). Indeed, we cannot measure $E[(A(X) - \hat{P}(X))^2]$ directly - instead we estimate $E[(Y - \hat{P}(X))^2]$, but thats just off by some fixed value (which does not depend on $\hat{P}$).At a higher level, there isnt really a distinction between classification and the setting here. Let f(X) be your confidence interval, and introduce a random variable A given by A = 1 if Y \in f(X) and A = 0 if Y \not\in f(X) be a random variable, then we are precisely estimating P(A = 1 | X). This exactly corresponds to classification, where the label A is either 0 or 1, and we are estimating P(A = 1 | X).As such, its important to compare with standard baselines (e.g. the 2 stage approach). Use the neural network features instead of training the coverage estimation model from scratch in the second stage, and show the MSE and calibration error values.I still think its unclear there is much interaction between the high quality confidence interval and coverage estimation. As the author response says, setting $\lambda_2 = 0$ and turning off the Ca-module, would not affect the confidence intervals produced. After the Response by the Authors ====Thank you for the detailed reply. For the clarifications the authors made to the algorithm description, I will increase my score.The authors state "If the scientific community had waited for deep learning to prove that it could discover the true conditional distribution of outputs given inputs, we would not have had the progress we achieved in the last two decades in AI. We believe that it is important to take into consideration all sources of evidence about the usefulness of a method, and experimental evidence is at the heart of the success of the scientific method and should not be discarded because of an established cultural habit of relying on proofs of identifiability."Note that the objections I (R1), and I believe also R3 and R4, have are not about theoretical vs. experimental research and that the paper lacks proofs or identifiability results. It is perfectly fine to not have a theoretical understanding of a proposed algorithm. But the authors should be able to justify the choices they made in the algorithm design, and especially in light of the prior work. The main justification given by the authors both in the paper and in their rebuttal is that the algorithm performs well. I believe the paper needs an iteration to address these issues.The following is my detailed feedback in addition to my original review in light of the authors' response. I hope this will help the authors in improving their paper.On fully learning the causal graph: I suggest the authors examine and try to identify, in small graphs, what aspect of their method allows it to perform better than the existing methods such as JCI or allows it to go beyond the existing equivalence classes. Without such justification, I do not think the paper in its current form will influence future research.Remark on interventions having variety: This is not sufficient for exact recovery. Imagine intervening on the same node with different mechanisms over and over. This does not allow recovery outside of the local structure around the intervened node for most causal graphs. This also relates to the remark above. Full identifiability is always related to having variety in the intervention targets and not just in interventional mechanisms. This is why some of the datasets where the exact graph is recovered by the algorithm need a detailed investigation.About synthetic experiments: One explanation for full structure recovery in the synthetic experiments could be the following: The authors randomly pick one target variable to intervene on. My guess is that this randomness in the experiment design is sufficient to have diverse enough target sets for the equivalence class to shrink to a single graph. Can you verify/check this?How many interventions do you use in the synthetic experiments? How many samples are collected per intervention? Unless I am missing something, these are not provided until page 19 but then it is not clear if these numbers are kept identical throughout the experiments. x-axis is set to be # of episodes or # of steps in most experiments whereas # of samples would be more informative.About JCI comparison: I did not completely understand why the authors could not run JCI in synthetic data. They say it is due to its complexity. But JCI's complexity comes from the graph degree and not from the number of samples for a small enough state space. It would be very interesting to compare what JCI learns relative to the proposed method in these synthetic experiments. This should test my hypothesis above that the random intervention target is providing enough diversity to reduce the equivalence class to one graph, which should be detected by JCI.Inferring a Markov equivalence class from the adjacency matrix by early stopping is definitely an interesting idea and I would encourage the authors to further pursue and formalize this direction.Sample complexity: The authors mention that their method is "sample-hungry". Given that the method presents significant divergence from the standard literature on causal inference that relies on conditional independence tests, which are known to require many samples, it is especially important to clearly present the number of samples used by the method. The main paper does not present the number of samples used in the synthetic experiments. These should be made explicit.Finally, the title and abstract still state "dependency structure discovery" and learning "Bayesian networks" whereas the authors attempt to learn causal graphs from interventions. I suggest an update to the narrative to clarify the objective of the paper. final recommendations : I maintain my score. I think the other reviewer summarized it best, "interesting but immature". I hope to see this work develop and published in the future. ---------------------------------Post-Rebuttal: Thanks to the authors for the detailed response. I think the idea has potential, however (resonating with the comments of other reviewers) I still think the paper should be significantly improved for better motivation, stronger theoretical results, and more elaborate and diverse experiments that can demonstrate the effectiveness of the method.  __UPDATE AFTER RESPONSE__Hello authors, thanks for your response. After reviewing the updates, I'm still unconvinced. I will raise my score to a 4, but won't be recommending acceptance. I'll provide some suggestions below, but first I didn't note any strengths in my original review which was not right! So I'll start with a list of strengths.Strengths:  - This direction of trying to understand how much value dot product self-attention adds is very interesting. Synthesizing the attention matrix, rather than computing pairwise dependencies is a cool idea.  - The experiments are on a range of tasks including machine translation, language modeling, GLUE/SuperGLUE and more.  - The performance of the random synthesizer is quite surprising, the fact that it doesn't depend on input tokens but can still achieve non-trivial performance is intriguing.  Suggestions to improve:  - I still think the paper could do a better job of reporting a more complete set of experiments/comparisons. Comparing against the variants of synthetic attention is interesting but not enough given that there are quite a few papers that investigate similar ideas. Dynamic convolutions -- Wu et al. report a range of experiments on machine translation, language modeling etc. Why not compare to them on these tasks as well? Comparing only to self-attention just isn't enough since **synthetic attention is not the first attempt to replace it**.  - It seems a bit strange that dynamic convolutions are competitive with self-attention in the original paper, but results on GLUE are so much worse. It might be worth verifying on the sequence generation tasks that results are as expected. For GLUE, Linformer has results in the original paper, why not also compare against it here?  - The paper needs some revision to clarify the motivations -- it starts out by talking about how self-attention may not be necessary, but in some of the results synthetic attention has to be combined with dot product self-attention to achieve reasonable performance. **On GLUE, looks like the deterioration from using synthetic attention alone is as large as 10 points on average.** The fact that it improves performance to use self-attention and add some parameters strategically can still be interesting I guess, but the original motivation of the paper starts to fade.  - Small note: Everywhere, that the baseline is "Transformer" that's a self-attention-only variant (V), so maybe the notation/tables could clarify that point. ==========after rebuttal===========My main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that.  =============== after rebuttal ====================I appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. My main concern is that the benefit of this method is unclear. The main baseline  that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. The proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. Regarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity. ********************After the authors' response:If the main motivation of the paper is to fix the mistakes in [14], then the paper should clearly state so, in addition to explaining why fixing is necessary. While I believe that pointing out other paper's mistakes and correcting them is important, the current state of the paper leads me to keeping my initial score and recommending to reject the paper. Update:Id like to thank the authors for their thoroughness in responding to the issues I raised. I will echo my fellow reviewers in saying that I would encourage the authors to submit to another venue, given the substantial modifications made to the original submission.The updated version provides a clearer context for the proposed approach (phychophysical experimentation) and avoids mischaracterizing GAIA as a generative model.Despite more emphasis being put on mentioning the existence of bidirectional variants of GANs, I still feel that the paper does not adequately address the following question: What does GAIA offer that is not already achievable by models such as ALI, BiGAN, ALICE, and IAE, which equip GANs with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates? To be clear, I do think that the above models are inadequate for the papers intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but I believe this is a question that is likely to be raised by many readers.To answer the authors questions:- Flow-based generative models such as RealNVP relate to gaussian latent spaces in that they learn to map from the data distribution to a simple base distribution (usually a Gaussian distribution) in a way that is invertible (and which makes the computation of the Jacobians determinant tractable). The base distribution can be seen as a Gaussian latent space which has the same dimensionality as the data space.- Papers on building more flexible approximate posteriors in VAEs: in addition to the inverse autoregressive flow paper already cited in the submission, I would point the authors to Rezende and Mohameds Variational Inference with Normalizing Flows, Huang et al.s Neural Autoregressive Flows, and van den Berg et al.s Sylvester Normalizing Flows for Variational Inference.----- REVISION: thanks for the clarification. I have slightly increased my rating (to 4). === Update after rebuttal:Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision. This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.       =======================================================UPDATE: I thank the authors for their responses to our questions. However, I still cannot recommend acceptance, because of the issue both R3 and I mentioned: the comparison to a more traditional VAE-style encoder, which replaces the "aggregator" of this paper with max-pooling plus some number of FC layers to output a mean and variance (as I mentioned earlier, this is almost exactly PointNet with the last layer outputting mean+variance instead of a single feature vector). I definitely do not buy the assertion that VAEs cannot support variable sized point contexts. Just about any shape encoder can be converted to a VAE encoder by modifying the last layer (and training with an appropriate decoder and loss) -- this includes encoders that handle point clouds of varying size like PointNet. In the exchange with the authors, it was not clear that this point was fully appreciated. Hence, while I appreciate the many interesting aspects of this paper, and the additional results provided by the authors during the discussion session, I remain negative and am slightly lowering my rating to make this clear (if this was a journal submission I would mark it as "major revisions" and ask for additional experiments vs VAE baselines which share the bulk of the proposed architecture other than the different aggregator). ----------------------------- Post rebuttal -----------------------------I read the author's rebuttal and I greatly appreciate their efforts. The authors have done many more experiments and I would like the authors to incorporate them into their manuscript and modify their manuscript, even methods, accordingly. I think these new materials can greatly strengthen the paper.1) It seems that ZSL with the original classifier involved is quite strong (this could not happen in ZSL as there is no original classifier for the unseen classes). I would suggest that the authors further investigate this for a detailed comparison. These methods may even simplify the authors' methods, and a connection to ZSL can strengthen the paper. For instance, Changpinyo et al., 2016) showed that their method can outperform [1] and it will be interesting to have some further comparison.2) It's nice that the authors compare the shared and non-shared alpha net. I still have doubt that why non-shared alpha net won't over-fit given that there are only a few labeled data instances. A shared alpha net might be more suitable for robustness.3) There is one difference to Kang's method. Kang's first stage stopped earlier so tailed classifiers have not covered. Did the author do the same thing?4) One method that can simply trade-off the accuracy is Kang's method. I think you can tune their hyperparameter to get a higher tail accuracy. Now the question will be, what will their head accuracy be? Without having a more ground comparison among methods, my question still remains unsolved.5) Besides ImageNet-LT and PlaceNet-LT, there are several CVPR/ECCV papers that outperform Kang's paper on CIFAR, iNaturalist but do not report on these two datasets.I have increased the score to 4, but I think the paper needs significant work to incorporate my comments as well as other reviewers' comments to be ready for being published. Final recommendation after rebuttalThe authors gave a good rebuttal, and the current version of Fig 5 and the new Fig 6 are making the paper stronger in my opinion. However, I will stick to my previous rating as: a) the main weakness, the tradeoff in performance for few vs overall makes the contributions weak, especially since the cRT baseline is as simple as finetuning only the classifers with balanceed sampling. I would expect gains over thatb) Fig 6 further shows the marginal gains over a weight-sharing baseline and makes the basic approach questionablec) there are no experiments on real-life long tailed datasets - a note here about iNaturalist: the argument the authors make about iNat makes some sense to me, and I want to thank them for replying. But this is exactly why we should test on real long-tail datasets, ie they dont behave as the artificially created ones. Seems no response from the authors. --------------------------------------------Revision:I thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns.A more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018). --After reading the authors' response, I still think the way that the contributions are depicted (e.g., a justifying the effectiveness of SGD) are inaccurate/unsupported. Furthermore, although the authors' suggest that they have tested a linear classifier and observed that the data is not linearly separable, more explanations/intuitions are needed about the assumptions that are made throughout the paper. Response to author comments:Unfortunately I am still significantly unclear on why RL is useful here.  The author response attempts to clarify that by pointing me to paragraph 2 of the intro, which states that RL has been used for data selection in other settings in the past.  What would help me (and I believe, the paper) more is a reason why greedy selection isn't sufficient for this particular problem.  Even just a single motivating example would be extremely helpful.  R3 mentioned similar concerns in their review, saying that the paper lacks explanation for why RL would win over non-RL for e.g. sentiment analysis.Likewise, while I appreciate the authors comparing against a stronger baseline in Figure 3, I don't know how to interpret the figure.  Why is Figure 3(b) better than Figure 3(c), and why does using RL cause that difference to arise? ============after rebuttal============I really appreciate the authors' rebuttal, which has addressed some of my concerns.Nevertheless, I agree with the other reviewers about the main weakness of the paper. That is, why the proposed method works and what are its advantages over similar strategies, such as Mixup, AdaMixup and Manifold Mixup, are not clear. Response to rebuttal:The authors clarified the questions. However, I maintain my rating because the contributions are limited and the paper is very poorly written. ===== update =====No response is provided so I am maintaining the score. I hope the authors could address all the issues in a future version.  Considering the answers from the authors, I decided to not change my score.  After discussion with authors----------------------------------------The reviewer thanks the authors for clarifications. With the confusion from the typos resolved, the paper is easier to follow. The technical correctness of the paper is not in doubt any more. But two major concerns still remain: 1.  The diversity enforcement has been reported before, and more comprehensive discussion of related work would be useful.  A detailed empirical analysis would also make the paper balanced and not heavily reliant on the novelty/depth of theoretical contribution. 2.  The main claimed theoretical contribution summarized in Lemma B.1 is tedious but a derivation of an obvious statement (as sketched out in the original review). The reviewer is raising the score to encourage this work, but still holding to the recommendation of not accepting the paper in its current form.  ------------- UPDATE FOLLOWING DISCUSSION -------------The authors were responsive and suggested a series of updates to address the concerns raised here. I find the inclusion of a theory section valuable. Overall, though, my stance did not change: Id like to encourage the authors to pursue their idea, but I dont think the paper is ready for acceptance yet. Id recommend focusing on two aspects:(i) Simulations and numerical results are especially confusing. Please consider a major revision in light of the comments raised by different reviewers.(ii) I find the application to fMRI interesting and potentially impactful. If the authors can include such analysis, I think itd improve the paper significantly. Being a completely different experiment, it would also alleviate the concerns on numerics. (significance, effect size, stability)------------- END OF UPDATE ------------- **I have updated this review after noting the authors detailed response.**This paper focuses on the problem of Neural Text Degenerationwhere text sampled from a language model can either be too repetitive and bland or too random and nonsensical. The authors focus largely on the former problem, proposing a finetuning loss that specifically incentivizes the use of tokens that have not yet been decoded in the given document. The authors test whether this improves repetition and unique token coverage with greedy decoding in open-ended generation. A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT). Similarly good results are obtained on Image Captioning with and without trigram repetition blocking. On Abstractive Summarization BeamSearch is used and again outperform MLE and UT. Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited. The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad. Multiple hyperparameter settings are shown, with some analysis on how gamma can be chosen to get a desired behavior. Finally, the authors analyze why UT may not be as effective: it penalizes gold repetitions too much and does little for other tokens.Strength:- The results are good for greedy decoding- The method is well motivated and well explained- The analysis regarding Unlikelihood Training is interestingWeaknesses- The results shown do not make proper comparisons across models, baselines, and hyperaparameters over all tasks.- Results for stochastic decoding should have been shown across tasks.- Despite citing the need for awkward rules such as trigram repetition blocking as a reason to propose ScaleGrad, trigram repetition blocking still helps significantly.- Some details are hidden away in the appendix, which I had to read thoroughly in order to fully understand the comparison.I recommend to reject this paper, because the experimental comparisons are not quite fair and because of implicit argumentation about what Greedy decoding can or should do that is never made explicit.The following two paragraphs are obsolete, because the authors shared experimental results from a larger set of experiments.> The results in Table 1which show the main metrics of interest on open-ended generationare missing two key points of comparison: ScaleGrad is only show with gamma=0.2, even though gamma=0.5 & gamma=0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings. This is despite the fact that two hyperparameter options for Unlikelihood Training are shown. In a footnote on page 6, for directed generation, the authors state Although UL was originally proposed for open-ended generation, it is applicable to directed generation. We did the same scale hyper-parameter search for UL. Details can be seen in Appendix E. However, in Appendix E two hyperparameter settings for alpha are shown, the same two as used in Table 1, but two hyperparameter settings for gamma in ScaleGrad are shown _neither of which are shown in Table 1_ nor are repetition or uniqueness numbers shown for these hyperparameters settings anywhere in the paper or the appendices. This makes me question whether the improvements shown in Table 1 hold across hyperparameter settings as the authors claim in their analysis of Figure 1.> However, Figure 1 is missing necessary data points and comparison. First of all gamma=0.2 is not shown, though at least gamma=0.1,0.3 are so it can be somewhat inferred. That is suboptimal, but this graph does not even go up to gamma=0.8, which is what is used in the Abstractive Text Summarization experiment! Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1. Luckily, Figure 1 (c) can be compared, and if cross-referenced with Table 1, shows that Unlikelihood Training does better than ScaleGrad with a higher gamma. However, Figure 1 has no data on either Unlikelihood Training or a human baseline. It really should not be necessary to go looking through Table 1, Figure 1, and Appendix F to see that Unlikelihood Training is outperforming ScaleGrad on some metrics. Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.On page 4 the authors write Following Welleck et al. (2020), we apply greedy decoding in our experiments in this section. This allows us to evaluate the modeling capability exclusively. We will get into the matter of comparison to Welleck et al. 2020, but I would like to begin by addressing whether Greedy Decoding is a neutral choice that only tests modeling capability, because it is clearly not. There is a spectrum of generation algorithms between probability maximization and straight-forward sampling. Greedy is closer to probability maximization, but it only maximizes local probabilities (Meister et al., 2020) and inevitably comes-up with lower probability outputs than Beam Search or Bound & Branch (Stahlberg & Byrne, 2019). Welleck et al., 2020 show that Greedy Decoding results in better text along their proposed metrics for open-ended generation.Since Greedy Decoding is not a neutral choice, I do not believe it is appropriate to exclude stochastic decoding baselines from the given comparisons. Stochastic decoding algorithms such as sampling, top-k sampling, and Nucleus Sampling usually do very well on repetition and uniqueness metrics. Indeed, they can be seen to outperform all the other models on Table 16 in Appendix H.In the analysis section, tables are quite limited in their coverage. In Table 6 no comparisons are made to systems that have not been trained with ScaleGrad, and these algorithms were not reported on in Table 1 so no comparison can properly be made even if the reader goes searching for the data. In Table 8, Unlikelihood Training is not included in the comparison even though it does very similarly to ScaleGrad on the same task in Table 5. Finally, Table 5 shows that trigram blocking still helps significantly on ScaleGrad trained systems. This is understandable, but disappointing since getting rid of these kind of rules is described as the reason for proposing ScaleGrad.Altogether, I feel the comparisons made in this paper are not quite convincing and the argument about why Greedy decoding, a deterministic algorithm, should even be able to match the properties of a large, noisy distribution is not properly fleshed-out.Meister, Clara, Ryan Cotterell, and Tim Vieira. "If Beam Search Is the Answer, What Was the Question?." Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.Stahlberg, Felix, and Bill Byrne. "On NMT Search Errors and Model Errors: Cat Got Your Tongue?." Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019. ###update###As no author response, I would like to keep my rating. ------- UPDATE ---------I thank the authors for their response to my concerns/comments. It seems like I have to defend my position for suggesting a rejection of the paper. While the response of the authors has clarified some aspects, some comments have not been adequately addressed.R1: However, the authors could not show how to measure the structural differences between the source and target graphs? Their measurement is not stable and is different between different datasets. How one can evaluate a new dataset is good for transferring or not (R5)? You could see how Ron Levie et al. showed this in their experiments. R2: But embedding using GCN is another definition for the function of the structure. However, it does not work well as node degree. Therefore, the author cannot point out that you only need to construct the features based on a function of the structure.R4: You can somehow use their idea to show how much difference is reasonable to transfer information. R5: So it seems this is one of the main drawbacks of considering \delta. If that is the case, you need to discuss it in the paper. R7: I believe the response is not sufficient. The authors need to add that to show they could positively transfer information. ===============Post-rebuttal:I would like to thank the authors for their rebuttal and addressing some of my concerns. However, after reading the updated manuscript as well as the other reviews, I decide to maintain my current ranting. I would still like to see more rigour in the paper: more of the mathematics need to be fleshed out and how the proposed approach compares with the existing works in complex/quarternion networks in a more mathematical way.  In considering author response:Thank you to the authors for continuing discussion on the points raised in my review, and for further clarifying the nature of the data as a kind of unidirectional ambiguity problem. I understand this better now and can see a contribution in releasing this data / data-generating process for other researchers studying autoformalization. On account of this I'm going to raise my initial scoring.On the subject of methodology, I still think there are reasons to reconsider this work.  As discussed, the translation baselines were not great.  I think it's not really fair to compare those models without pre-training on data that was too small to learn basic tree properties.  It is possible that translation models that perform string-to-tree translation would perform better here(1), though results from natural language translation would hint towards the pre-trained models still performing better.  Translation models used in the domain of programs seem more suitable as well, and there's a good number of these, and there is a natural desire to generate strings that reflect a properly nested tree (2).  There is also work on mapping strings to knowledgebase queries that seems similar in input/output (IIRC, Luke Zettlemoyer's had a number of important papers in this line).But at best these would still be comparisons of mostly off-the-shelf translation models, which doesn't leave the reader with much of a takeaway.So I'm left feeling that if the authors want a useful quantitative comparison, these methods should be explored.  Pre-trained model beats model trained on only in-domain data is not to me a story significant enough to warrant inclusion in the conference, even if it contributes a new dataset (as the modeling is presented as a contribution here).  Even off-the-shelf methods can of course be part of an important contribution when the authors show that they have pushed the field further with an important result (say, GPT) but I do not feel the evaluation in this case supports that conclusion.It seems more natural given that none of these methods are likely to out-perform a vanilla pre-trained translation model, that the problem description and qualitative evaluation are of the utmost importance.  I would really recommend expanding this beyond half a page, to give the reader a better idea of what problems are solved and what are remaining.  It also seems that some of the errors pointed out (like those involving ellipses) would likely be remedied by additional synthetic data.  As I'm the most dissenting reviewer, I would still hope the authors attempt to improve the results section with the additional page upon acceptance.1.Towards String-To-Tree Neural Machine TranslationRoee Aharoni, Yoav Goldberg2.Tree-to-tree Neural Networks for ProgramTranslationXinyun Chen, Chang Liu, Dawn Song ===== After rebuttal =====I appreciate the fact that the authors update their manuscript, taking into account some of our comments (+1 for this). However, I still feel that the content is quite hard to follow. In general, I believe that the motivation is still unclear and the clarity low. Even Fig. 1 (that potentially should help the understanding) is so complicated without any explanation in the caption. Maybe the technical content is interesting, but I think that the paper is not accessible. I have tried to read the revised version submitted by the authors. Unfortunately, it is still very hard for me to fully grasp the proposed concept and to follow the derivation steps and the proofs. In my opinion, the paper is not publishable in its present state. =================================**Update after discussion period**My feeling is still that the proposed method has a lot of interesting potential, but that the paper still needs some improvement. I've bulleted my main remaining concerns below:1. The clarity of the writing still needs to be improved in many places. Notably, the authors' discussion of their metric is mostly discussion-based (rather than providing concrete theoretical claims about their metric and its strengths). While this isn't necessarily a problem, such a presentation really needs precise language and phrasing so that the details of the claims and their supporting arguments can be completely understood. From what I can follow, I think many of the authors' arguments are going in good directions, but the writing should be improved to be sure.2. I still don't completely understand why $\beta$ is being fixed here. The argument in Appendix A.3 seems to say that if $\hat\beta$ varies, then the meaning of $\hat\gamma$ changes, and so looking at the change in $\gamma$ would not be meaningful. I agree, but that does not mean we cannot look at the combined change of $\beta$ and $\gamma$. It seems like this would be straightforward to experiment with on some smaller problems that only have parameters in the tens of thousands; I think it would be better to include such experiments rather than trying to argue verbally that such an approach will not work well.3. I am also not sure I see why we should not be interested in leave-one-domain-out CV. I appreciate the clarification that Figure 1 is actually showing CV's estimate of error, and definitely find this to be a compelling experiment. It is pretty surprising to me that the authors' metric could succeed in this case, given that the influence functions are an approximation to the parameter change as each domain is left out (this is true, whether or not this is the intent of the metric). I think further investigation of this point is needed. Would computing the proposed metric with the parameter changes under leave-one-domain-out CV not detect the OOD issues in Figure 1? Or is it just that feeding these parameter changes into the function measuring test error that is providing a poor assessment of OOD generalization in Figure 1?On a different note, I appreciated the increased discussion of "the shuffle;" I think this is an important part of the paper that didn't come up much during the discussion period. As a side note, I think it is more common to just call this sort of thing a permutation test. And, along those lines, it would be good to actually perform a permutation test (i.e. run over many shuffles and examine the distribution) so that we know for sure the reported shuffles aren't just unluckily high/low.=================================== ------- After Discussion --------The authors agreed that the paper is not ready for publication, yet. Thus I keep my original score. **Update**: Since there's no substantial author response, I'm keeping my score as it is. All the best for your future submissions! **Update after rebuttal** : I want to thank the authors for their rebuttal. However, after reading all responses and all new results presented, I still think that most of the weaknesses of this paper are still present. Two notes that I urge authors to take into account:* Even the updated Figs 2 and 3 are highly misleading: sure, the gains over a baseline ResNet network are big, but the comparisson should be vs other backbones that combine convolutions with attentions. * The authors should clearly say that they do use a convolution for conv1, and their modules start on top of that first spatial convolution.---------------------------------------- Edit: Read responses, bumped score up to a 4, check full response below =====Update after rebuttal=====I have read the authors' rebuttal. Considering the limitations and non-superior performance for larger datasets, I am keeping my score unchanged.  ********* Update ********The reviewer appreciates the efforts authors have made. However, the response does not fully address the issues. In the new experiments the authors used small sketch size to show clearer advantages of the learned IHS (Figure 2 & 4), but at that regime all of the methods including learned-IHS are converging only slowly and not practical compare to the slightly larger sketch size choices. The reviewer believes that such a comparison is not meaningful. In order to truly demonstrate the benefits of learned-IHS (which seems to be robust to small sketch sizes), the authors should choose for each algorithm the best sketch size and then compare them in run time, at least between learned-IHS and Count-sketch IHS.Beside the flaws in numerical experiments, the reviewer found that the theoretical contribution of the current version is incremental. From the reviewer's point of view, the meaningful analysis for learned IHS should certainly be how the converge relates to  the statistics of the training data, to show the benefit of the learned sketch theoretically (i.e. to show how much better the learned sketch SA compare to the unlearned ones in terms of Z_1 and Z_2). The authors have avoided such type of analysis and the main results are "safe-guarded" by the concentration of the random sketch, which are easy to derive.Overall, the idea of combining IHS with learned sparse sketch is interesting, but the reviewer believes that the current version needs significant amount of rework to be publishable in top conferences. Update: I thank the authors for the detailled response. Due to the number of required changes and the feedback of other reviewers, I believe the paper needs a major revision before publication and still recommend rejection.--- ----------------------------**Update after author response**: Thanks to the authors for answering my questions and for incorporating feedback into the paper. Through discussion, I think we got to the crux of the method: distance functions seem to work better than classifiers for imitation learning. I think this is a really neat observation, and potentially quite important; the experiments definitely support this hypothesis. That said, I don't think the paper goes far enough in exploring this hypothesis, either experimentally or theoretically. There are a number of confounding variables, such as data, loss function, and architecture, which each will need to be accounted for. I therefore stand by my previous vote (4) to reject the paper. With more thorough experiments and ablations, I think this will make a fantastic submission to a future conference. ------ After Discussion -------I had an interesting discussion with the authors that clarified some important points. I came to the conclusion that the topic is interesting and the study is valuable, however, not yet conclusive. Consequently, the paper is not ready for publication, yet. Thus I have lowered my score by one. I hope the authors continue this work, since I'm convinced a complete empirical study on the impact of the gradient norm along the training path is insightful and a valuable contribution to the community. ============After rebuttal:I thank authors for their response. I share concerns with others reviewers and I highly encourage authors to consider answering questions suggested by Reviewer 3 at the end of their discussion. I believe a systematic study of gradient norm is interesting but this work does not provide a solid set of answers. As such, I'm reducing my rating to 4. ---------------------------------------------------------------------------------------------------After Rebuttals---------------------------------------------------------------------------------------------------Thanks for your responses. I still have doubts about the proof of lemma 7. You not only change from '2' to '3', you also add a new term (the fourth term). I admit that $1 + 2/k < (1 + 3/k)(1  \mu*\eta)$ holds under your assumption, but you also need: $\mu*\eta> 1/k$, i.e. let the fourth term be greater than zero, to make the inequality hold, however, this contradicts the previous assumption. So I still believe this inequality does not hold.  ====After rebuttal:I read all reviews and rebuttals, especially the argument about Lemma 7. I agree with reviewer 2 that the Lemma 7 is not correct. The assumption of $\mu\eta < \frac{1}{42k}$ contradicts with the requirement $\mu\eta > \frac{1}{k}$ of term 4.  This error needs to be resolved before acceptance.  .###############################################################################Final recommendationI thank the authors for their detailed response. The authors have clarified some technical details and blind spots of their methods, have fixed their evaluation metric which was wrongfully comparing tree sizes on solved and unsolved instances, and have presented an additional experiment in the appendix on the original benchmark from Gasse et al. These changes are going in the right direction. However, I remain concerned about the experimental setup in the paper, and therefore my final recommendation is still rejection.First, I still find suspicious that the main experiment in the paper is conducted on only 3 (modified) benchmarks out of the 4 proposed in Gasse et al. The authors claim they did not run experiments on the 4th benchmark due limited computational resources, but at the same time they present complete results on the 4 original benchmarks in the appendix. Therefore I believe the explanation given by the authors is fallacious. Results on the 4th (modified) benchmark, and even better, on other additional benchmarks, would be much more convincing and alleviate any doubt about cherry-picking.Second, I am not convinced by the argument the authors present to justify their two solver settings: clean and default. I do agree that challenging problems should be solved under the default setting, however I do not see why decision quality should be measured using the clean setting. Decision quality matters in the default setting as well, and one could argue it matters in the default setting only, if the final goal is to improve the solving time of the solver on challenging problems. Moreover, if the authors want to use the tree size as a mean to measure branching decision quality, they must also provide the optimal solution value to the solver at the beginning of the solving process, in order to deactivate side-effects from pruning. See G. Gamrath and C. Schubert, 2018, "Measuring the Impact of Branching Rules for Mixed-Integer Programming".Last, the method proposed by the authors seems to be effective only in the specific benchmark they propose. In the additional experiments they present in the appendix (Table 4), their method does not convincingly improve over the original method from Gasse et al., as the performance gains on the Easy training instances degrade rapidly as one moves away towards the more challenging Hard instances. The very name of the paper, "Improving Learning to Branch via Reinforcement Learning", seems to claim that reinforcement learning improves existing learning to branch methods. However, the improvement observed by the authors is very specific to the "backbone" setting they propose, and does not seem to translate to the original benchmarks of the methods they compare to. The authors justify their choice of benchmarks saying "we focus on a more realistic industrial setting", but I am unsure whether the hypothesis of a "backbone graph" is particularly realistic in industrial applications. I do not think the original benchmark from Gasse et al. is particularly realistic either, however I would not give more or less value to either one of the two. As such, I believe it is crucial that the authors report experimental results on both benchmarks in the main body, and provide a discussion as to why RL seems to bring improvement in the restricted "backbone graph" benchmarks, but not in the original ones which have more variability. This, in my opinion, would have a much higher scientific value than simply presenting both a new method and a new benchmark, while disregarding how the method performs on previous benchmarks from the literature.In light of the changes made by the authors I am willing to raise my rating, however I still recommend rejection for ICLR Post discussion:I read the author's response and other reviews. I will stick to my rating and encourage the author to resubmit a revised version focusing on the antisymmetric case.  # UpdateThank you for the rebuttal. The writing was improved and it is easier to read the paper. Nevertheless, the experimentation remains weak. I increase the rating by one point. Post discussion period:-------------------------------I read the other reviews and the author's response. Thank you for performing these extra experiments - they make the experimental section more complete. I still feel that the technical contribution of this paper is marginal and stick to my original rating.------------------------------- **Update**My impression after the extensive discussion is that the remaining differences are possibly too subjective to come to an agreement:1) Whether the fact that the invertible reparameterization principle does not hold for anomaly detection represents a significant theoretical contribution. To me, I still dont quite see why, but I do believe there are other researchers with a stronger theory background more qualified than me to evaluate this, I will therefore downgrade my experience score by 1.2) Whether the fact that the invertible reparameterization principle does not hold has promise to further understand the practical issues like the CIFAR10 vs SVHN phenomenon beyond things that have already been discussed in the literature - I think quite likely not, the different arguments for and against have been discussed in our back and forth I think.I invite the AC to go through the discussion here to see the detailed arguments/rebuttals.My personal impression, in case the authors are interested, is also the manuscript might be further improved by giving more space to these two questions, i.e. the motivation of the principle and instead moving some of the proofs to the supplementary. I think it is quite straightforward to understand that the principle cannot hold and more difficult to understand why it should hold in the first place. **Post-Rebuttal**: No change in my score, please read my comments in the thread below. After rebuttal:  I appreciate the authors for the response. However, I feel that the conclusions drawn from this paper are not focused, there is no central message that i can get as a clear take-away for understanding CNNs after reading the paper. I do agree with each of the findings the authors make, it is just that the presentations and writings of the paper make me confused about what the authors are trying to convey through this paper. Therefore, i am keeping my score. Thanks for the efforts of the authors. Some of my concerns have been addressed. However, I still think that the experiments are not convincing enough for ICLR. So I keep my score._____________________________________________ =====Update after rebuttal=====I have read the authors' rebuttal. I still believe the novelty is limited, and hence I keep my score unchanged.  -------Update: Thanks to the authors for their response, which helped clarify several of my minor questions and I believe those can be revised with a writing pass. However, I still think this paper has two significant deficiencies:1. The parameter size comparison still seems flawed to me. The authors say that one can discount the entity embeddings, but can we really? Arent they part of the models representation even if the inference does not use all of them in a single forward pass? Several neural net architectures exist (including large-scale LMs) that do not use all inference paths but still count them in the total params. The BERT vs GPT-2 example provided in the rebuttal is only a difference of 26k tokens (still significant!) but here we're talking about a few hundred million parameters!     At the very least, I think the true sizes of the models must be acknowledged and one can add the point on entity embeddings vs 'brain' parameters as a caveat, but it seems scientifically inaccurate to me to claim otherwise. To be clear, I don't think the size issue detracts much from the main contribution of adding entity embeddings here (i.e. this work may still be of interest even if the size of model is larger), but the current version of the paper has several claims about size savings that seem incorrect.2. The analysis of the proposed method (where it helps, where it fails, which hyperparams matter) is still lacking. The authors did mention one ablation in the supplementary that I missed, but I dont think that is sufficient for a reader to understand how to build on this method in this future without re-running all the experiments, doing an extensive hyperparam search, etc. Update after author response: I appreciate the clarifications, but given the lack of comparisons or discussion to related prior methods (at least Liang et al 2020 or some alternative equivalent), I cannot recommend acceptance at this point. The authors did not submit a paper revision as well. I think the idea seems promising, so don't take this as a critique of the research direction. ----------- UPDATE I ----------I thank the authors for their response to my concerns/comments. It seems like I have to defend my position for suggesting a rejection of the paper. While the response of the authors has clarified some aspects, some comments have not been adequately addressed.Still, the experiments and related works are my main concern with this paper. R1: Based on this response, It seems the paper is trying to tackle the negative transferability problem. While this is an important problem, the authors need to discuss that in the related works, for example [1]. However, in a multi-view learning problem, the main issue is that each view includes some features which can help the classification goal. In other words, part of the features in all views are noisy. At least, you need to add this experiment in my point of view.R10: [2,3,4] are a few examples that used DP for classification purposes. When one of the main arguments of the paper is adding uncertainty estimation using DP, discussing other related papers are important. In some cases, you can also compare with them.  Besides that, the authors can compare with HDP similar to what they did for the CCA-based methods.It would be great if the authors could report the classification results based on only using one view for Figure 4 as a baseline. The authors might also want to check the paper [5]. [1] Bayesian multi-domain learning for cancer subtype discovery, NeurIPS 2018.[2] Multi-Task Learning for Classification with Dirichlet Process Priors, Journal of Machine Learning Research 2007.[3] Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification, NeurIPS 2018.[4] Factorial Multi-Task Learning : A Bayesian Nonparametric Approach, ICML 2013.[5] Hierarchical Optimal Transport for Robust Multi-View Learning, NeurIPS 2020.----------- UPDATE II ----------I have read this paper again and went through the author's responses. While I appreciate the authors for their responses and believe there is some novelty in including uncertainty quantification, I'm still not confident with their experimental designs and literature review.Based on my understanding, this method is not a multi-view learning in a classical way. It is better to say that the proposed method is somehow an ensemble approach. Going through the model, the paper deals with each view separately and then tries to combine the classification results of each view by weighting them. There is some novelty here, where the weighting is adapting based on the uncertainty. However, in a classical multi-view, the problem is that you have different views, where features of each view are randomly noisy. So, the goal is how one can combine information from different views to improve the classification results. Still, I cannot see that. In the experiments, the authors make N views to be noisy, and the other N(+1) views are clean. Then they are trying to classify each view (somehow) separately and because they are reducing the weight of noisy views, their performance (slightly) will be improved. For example, please check table 1. For instance, the Handwritten dataset, the performance of DE, the only ensemble model as the baselines, performed almost similar 99.79 vs 99.97 ( 98.30 vs 98.51). This improvement can be due to tuning the other methods, and the authors did discuss this in neither the main text nor the supplement.In Figure 4, when the noise is small, the different models' performances are almost the same. Also, \sigma = 10^9 is not meaningful and with some sort of variance comparisons, one can find that that view is corrupted.The authors did not address if the method can work in real/harder situations when all features and views are noisy. The authors responded they do not have any restriction on that; however, they did not show either empirically or theoretically that the proposed method can handle this situation more suitable. I agree that the authors have tried on the real dataset, however, to see the performance comparison, you somehow need to randomly corrupt different features on different views.Since the method does seem to be a multi-view ensemble learning, I would have rather expected to try a more common approach for achieving the same goal first, for example to use one of the available Bayesian single-view methods for each view and they combine the results and see how the results look like.The authors claim that the negative transfer effect is not related here, which I can't entirely agree with. This term has been used in multi-task learning as well. The reason that no one uses this term in multi-view learning is that this is not a real scenario in multi-view learning. One view is degrading the classification performance because some views are noisy. If we do not use those very noisy views, the performance should be improved.I also want to point out that their authors show that removing each view can degrade their model's performance. And this is desire, especially when the number of views is small, as having three views means three models in their ensemble architecture. It is not clear that this improvement is due to better information sharing or more complex methods.Regarding the CCA-based method, one can use (Optimal) Bayesian Classification on learned space to get the uncertainty as well. Although, I agree that the proposed method is somehow an end-to-end learning method. I would suggest the authors somehow re-organize/write the paper as a multi-view ensemble learning method and compare it with those multi-view ensemble methods as the main focus of the paper. You also need to discuss [5] as it is very related to your work. Although, it is a Bayesian but not an ensemble method.  ++++ updates after discussion period ++++While the initial paper provided empirical evidence  for the claims of the paper, based on the reviews an appendix of about 9 pages was added in the revised version, and the focus now seems on providing theoretical support of the claims in the paper. Also several experimental results were added in the main paper in response to the reviews. I feel like all these (quite major) changes indicate that this paper is not mature enough for publication at this point. So I maintain my current review. Author feedback: The behaviour and performance of the algorithm in single-goal environments should be part of the paper. The paper should show not only where the algorithm succeeds but also where it fails. I appreciated the author's efforts to add diversity to the domains evaluated but they do not go far enough to change my score.  Update after response:While I appreciate the authors' attempt to address my concerns, the fact that model is required to not be fully trained is concerning. It was in this context that I suggested label smoothing - that training on smoothed labels might address a sparse G matrix, but it seems like this point was not communicated clear enough by me and not understood by the authors. The authors suggest some applications of the model centric view of the data, but do not present any experiments regarding these applications. I believe the paper might be more convincing if those experiments are added in the next version of the paper.At this time however I will still have to vote for rejection. Update: Thank you to the authors for clarifying the points brought up in reviews. I acknowledge that I have reviewed their responses. ======After reading the author's comments I still keep the score. After rebuttal/revision, the paper still has a lot of steps, many of them are human designed and are not well-defined. For example, in the author response, they said "their effect distributions look similar, and so are merged into one class type", what is the criterion to merge them together? In the revised paper, what is the procedure "COMPUTEMASK" defined?  In line 38 of the algorithm proposed in Appendix F in the revised paper, what does the sign of "approximate equal" mean?  Overall, this makes it hard to reproduce and it is not clear whether the proposed approaches can be applied to other problems than the specific tasks mentioned in the paper.  ##########################################################################Comments after Discussion:I appreciate the effort made by the authors to elaborate on the causal formulation behind CIQ. However, the additional discussions in the paper are still confusing and raise soundness concerns. Some of the issues are discussed below.1- Rubin's Causal Model: The authors reference RCM in Subsection 3.1 for the causal formulation yet the rest of the work does not seem to use the potential outcome notation. Instead, Subsection 3.3 uses graphical models and the do-operator which follows the causal framework by Pearl. Then, in Subsection 3.4, the authors go back to reference RCM. It is not clear why this alternation between the two approaches is employed.2- If $z_t$ is defined as a function of $x_t$ and $i_t$, shouldn't the CGM reflect that with an arrow from $i_t$ to $z_t$ instead of it being the other way around in Fig.2(a)? Despite the attempt by the authors to elaborate on the causal formulation, I'm unable to map the structural equations such as Eq. (1) and the function of $z_t$ to the given CGM in Fig.2(a).3- The discussion in Subsection 3.3 leading to Eq. (3) sounds flawed to me. Quoting the authors, "the interference model of Eq. (1) can be viewed as the intervention logic with the interference label it being the treatment information". This statement is elaborating on the formulation of $x_t'$ where $x_t$ is intervened on and replaced by an interfered state when $i_t=1$. Alternatively, $x_t$ is kept intact when $i_t=0$. This intervention on the mechanism of $x_t$ happens whether we obtain $i_t$ and train the DQN with it or not. In this sense, the intervention is not happening under the CIQ framework only, but also when we simply train based on $x_t'$. Accordingly, it is not clear to me how "the learning problem is elevated to Level II of the causal hierarchy" due to the presence of the interference labels. To be clear, I'm not questioning the significance of using the interference labels in the training, but rather the causal story and formulation behind CIQ. Post discussion update=========After discussion with the authors, I have calibrated my score upward to 4, since the authors seemed willing to engage in discussion and correct/improve the paper, which I appreciate, but I still recommend not accepting the paper.The authors generally acknowledged (though have not yet fixed) the issue of wrong attribution of the APSP algorithm. This isn't just a matter of citing B instead of A; the paper (still) contains a lengthy discussion of why A is not suitable, so instead they must resort to B, even though in reality they just use A (and B remains unused). This is glaring since these papers are famous classics, widely taught in graduate courses, their content is well known, and it is puzzling how a diametrically incorrect representation of them made its way into the paper.The reason I dwell on this is that it signifies a larger issue with the paper. The original version was peppered with formal statements which were at best inaccurate, and even though the authors fixed (or said they would fix) the ones I pointed out, I remain unable to trust the overall technical soundness of the paper. The review time frame doesn't allow a reviewer to carefully verify every statement (nor would I want to), there must be some commitment of due diligence on part of the authors, that up to a small inevitable fraction of inaccuracies, the formal content is rigorously correct. I'm afraid the current version of the paper is quite off this mark.Putting formal soundness aside, my present understanding of the idea of the paper is the following: The authors observe that many basic computations on graphs can be parallelized into a few computations of small width and depth. Usual GNNs can implicitly implement this if their width is large enough, but this poses a computational burden, and there are obvious advantages to explicitly building this parallelism into the architecture. This seems like a sensible and potentially empirically useful observation, but the experimental section still seems too thin to make the case properly. That said, perhaps I have not fully understood the paper, since its frequent inaccuracies and fuzzy statements made it a bit hard for me to follow. In conclusion. I think the paper should undergo a substantial revision:1. Clean up the theory part and ensure its formal soundness,2. Crystalize the point of the paper (in particular, rather than just presenting GNN+, I hope a revised version would include a more thorough comparison with usual GNNs - not just dismiss them with some citations of prior works which allegedly prove limitations - this leaves doubts about the exact model and assumptions, particularly since as discussed above, the prior work is not always cited accurately),3. Possibly expand the experimental section. ------------I have read the authors' response. I do not think the answers addressed my concerns.On the positive side, I do think the paper is written well and the idea is clear. It is true that a soft-clustering of PDs on the true Wasserstein distance has not been done. The paper also provided proof of the convergence of the algorithm.My main concerns are the following:First and most importantly, the method is not particularly surprising. It basically combines the most classic soft-clustering algorithm and the Frechet mean computation algorithm of Turner et al. Theoretically, the proof is extending the convergence result of the soft-clustering algorithm to the Wasserstein distance of PDs. I would not say the proof is trivial. But it is not that surprising, as we already knew that the Frechet mean of PDs is computable. I would be much more excited if the theoretical result is about the optimality rather than just convergence.Empirically, the paper did not provide comparison with (Latombe et al. 18). Just saying that they did not do it exactly in the PD space is not good enough. A lot of practically powerful methods (persistence image, various kernels, etc) are approximations/relaxations outside the PD space. These approximations/relaxations can bring computational advantage, and sometimes better learning efficiency. Therefore, we need to know how this method is compared with (Latombe et al. 18) in efficiency and clustering performance. (Latombe et al. 18) can naturally have both hard- and soft-clustering versions. A thorough comparison with the different versions can show how important it is to stick with the PD space rather than the relaxation. My guess is that in practice sticking with PD space is not that important, or maybe even worse due to bad local optima of the Frechet mean. But I would be very happy to be proven wrong.Another issue is the limited experiments. The material data does seem to be a good fit. But the authors could use some of the classic topology-friendly data (shape, dynamic data, graph) from existing supervised methods. Any labeled binary/multiclass data can be used to evaluate clustering. An even more ambitious goal is to prove the usefulness of clustering in the supervised task. For example, the authors can show that a bag-of-words approach (using the clustering result) can improve classification performance.Overall, I feel that the methodology is not very exciting to me, and the experiments are insufficient. If the main argument is the algorithm computes on the PD space and the proof of convergence, this paper may better fit a theoretical conference. Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4.------------------------------------------------------ #########################POST-REBUTTAL UPDATE#########################Thank you to the authors for your detailed responses and for uploading your code. (Minor point: your README assumes that the GNOME desktop environment is being used; you may want to make the instructions platform-independent.) My main concerns about the learning process described in the paper remain. The authors indicate in their response that "it is difficult to quantify which is the most challenging part of the learning process". This makes it much more difficult to reason about whether the learning process is primarily about using the Bayesian Stackelberg game solver, and whether the interaction with the environment (given the limited number of trials) provides limited benefit. === After rebuttal ===I am not satisfied with the response from the authors. I can only hardly recognize the effort made by the authors during the rebuttal - most of my points were only briefly discussed in the response without revising the paper. - The suggested baselines were merely briefly discussed but not added to the comparison.- The results of the meta-dataset, which in my opinion is the most suitable dataset for the purpose, are still not included in the revised paper.- The stddev of the classification task is not still not provided, making it hard to justify the performance gain.- Why is RL left to future work?I have read the reviews from other reviewers. With the little revision from the authors, I have decided to keep my original rating and would not recommend this paper to be accepted. __________________________________________________________After rebuttal, the authors reformulate Theorem 2, and then I think it is right. I raised my score.I am still not convinced by their experiments on recovering bandits. ============== Post-Rebuttal ==============The authors' responses to point 1 & 2 do not sound (reflecting a question to another paper does not solve the problem). The authors mentioned "We made this decision based on Mei et al (2018) which proposed our baseline model (the top down approach)", where the reference of Mei et al (2018) cannot be found in the paper, as a critical baseline. This raises a flag on the novelty of the work and completeness of the related work. Therefore, I am lowering my rating to 4. ## POST-REBUTTALI really appreciate the author's engagement and response during the discussion phase to help me understand the paper better, and the revisions incorporated in the paper.But after much thought, I do not think the current form of the paper meets the bar for publication.Here are my main concerns that I hope is useful for the next version or final submission.I think the core idea of the algorithm is uncertainty propagation is necessary for inducing effective exploratory behaviour. This core idea is theoretically motivated from sound strategies for exploration in finite-horizon RL, but as the paper addresses the discounted problem setting with deep neural networks the soundness is traded-in for computational tractability  which is a fine choice.But, currently the choices are presented in a confusing way, and the actual contributions of the algorithm are unclear: most importantly, is it a Frequentist approach to exploration or Bayesian?- under the Frequentist approach setting, utilizing e-greedy seems justifiable just based on the reasoning of this is the widely accepted practice in the field.- under the Bayesian viewpoint, which is the crux of the architecture used here (Bootstrap DQN), the attempted theoretical connections in the paper (Theorem 1) and the practical algorithm proposed (based on the ideas of Optimistic LSVI), do not provide a clear picture.To show theoretical soundness the algorithm is anchored to a Bayesian architecture and theoretical uncertainty connection, but for practical performance purposes the paper leverages reasoning from Frequentist Deep RL methods. Maybe this is a step in the right direction, and the extensive empirical results do seem to suggest it is effective, but the presentation is unclear. From the current draft:- OEB3 relies on the posterior of Q-functions  Bayesian- UBE uses posterior sampling for exploration, whereas OEB3 uses optimism for exploration  FrequentistTherefore, this can be improved and presented more clearly to communicate the idea. %%% post-rebuttal %%The authors replied to my comments related to the diversity condition in 3.2 and Theorem 1. Their answers did not fully clarify my concerns or misunderstandings, and it seems the authors didn't make any changes in that regard in the revised version. Except if I missed something in the review system, they did not answer my other comments.%%%%%%%%%%%%%%% Post-Rebuttal Comment=====I thank the authors for the detailed response and the updated results. While my overall opinion of the work is slightly more positive post-rebuttal, I still maintain that this is a clear reject, primarily for the following reason:The technical contribution (adding a hierarchical layer to SPAIR and demonstrate the hierarchy can also be learned without supervision) is not significant enough to accept purely based on a "proof-of-concept" of a "new" direction.For incremental contributions, I expect the experimental results to be more convincing to be acceptable, a few points that I still really expect to see:comparisons on harder datasets when one doesn't have to go into a specific metric to show an edge over a prior art that is targeting a different applicationresults on decomposition with more significant overlaps (especially in 2D) and on objects where part boundaries are harder to infer (actual 3D objects is still preferable)object-level manipulation (row 3&4 in fig 4 did not match description) and latent space interpolationI would also strongly encourage the authors to highlight the similarity and different between SPAIR and SPACE when introducing the latent code formulation. -------------------------------------------POST REBUTTALThanks for the author's swift responses. I appreciate that. However, the paper still needs to provide more evidence and precise comparisons to clarify the mentioned concerns. Three major concerns are listed below:1. Why does the proposed pipeline perform better than RS? The paper should add the proposed modifications one-by-one into the RS method and show/discuss how these individual design choices affect the performance.2. WTA is worse than rank statistics in Appendix Table 5. Is it a counterexample of WTA? It needs more experimental supports to show WTA is a better method.  The hyper-parameters used in all experiments should be provided in a table and explain how the parameters are selected. Please make sure the same tuning budget is given to all the methods for a fair comparison.3. What are the necessary changes to make end-to-end training possible (or not possible in RS)? The paper should summarize RS's design and explain why it can not do end-to-end training, then explain what modifications are made to make this possible. This comparison is better to be illustrated in figures.Overall, this is an interesting paper, and the community will be benefit from it if well presented. Please consider revising and submitting to another venue. Post-rebuttal feedback-------------------------------I thank the author for their reply and I encourage them to make the suggested improvements to the paper. #############AFTER RESPONSEI would like to thank the authors for the detailed response. I encourage them to keep working on MetaCURE: With some improvements I believe it will provide a valuable contribution to the meta-RL field. ------------------------------------------------------------------------------------UPDATEThanks for your detailed response!A1: I see, that makes more sense to me now. I'm still not 100% convinced that it might not be better to let the policy entirely meta-learn what a good exploration policy is, and only use the inductive bias (terms 1 and 2) for how to do so for meta-training and anneal those terms over time. This would mean that at test time the only thing that the exploration policy should do is maximise the return in the *exploitation* phase - and basically figure out what the best way to do so is (so you'd need the "missing term" I describe above). But that's just a hunch, not sure if that would actually work better.A2-A8: Thanks a lot for clarifying. I'll keep my current score given all reviewers agree that the work is unpolished in its current form, and because the authors plan to propose a new version of MetaCURE soon. I think this is very interesting and promising work and look forward to reading an updated version in the future! Edit: The updated results need consistent baselines. For example, the method of [7] should be consistently compared against. POST REBUTTAL: I think the paper is decent, there are some significant downsides to the method but it could constitute a first step towards a more mature learning-rate-free method. However, in its current state the paper is left with some gaping holes in its experiment section. The authors tried to add experiments on Imagenet, but these experiments apparently didn't finish before the end of the rebuttal period. For that reason, the paper probably should not be accepted for publication (even if the authors manage to finish running these experiments, we would not have a chance to review these results).  [Revision]Greatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one. Update:I feel the idea of this paper is straightforward, and the contribution is incremental. To improve the paper, stronger experiments need to be performed. REVISED:I've revised by review upwards by 1, though I still recommend rejection. The authors improved the scholarship by adding many more citations and related work. They also made the model details and implementation more clear. The remaining problem I see is that the results are just not that compelling, and the experiments do not test any other graph neural network architectures.Specifically, in Table 1 (synthetic experiments) the key result is that their tree-transformer outperforms seq-transformer on structured input. But seq-transformer is best on raw programs. I'm not sure what to make of this. But I wouldn't use tree-transformer in this problem. I'd use seq-transformer.In Table 2 (CoffeeScript-JavaScript experiments), no seq-transformer results are presented. That seems... suspicious. Did the authors try those experiments? What were the results? I'd definitely like to see them, or an explanation of why they're not shown. This paper tests whether tree-transformers are better than seq-transformer and other seq/tree models, but this experiment's results do not address that fully. Of the 8 tasks tested, tree-transformer is best on 5/8 while tree2tree is best on 3/8. In Table 3, there's definitely a moderate advantage to using tree-transformer over seq-transformer, but in 5/6 of the tasks tree-transformer is worse than other approaches. The authors write, "Transformer architectures in general, however, do not yet compete with state-of-the-art results.". Finally, no other graph neural network/message-passing/graph attention architectures are tested (eg. Li et al 2016 was cited but not tested, and Gilmer et al 2017 and Velikovi et al 2017 weren't cited or tested), but there's a reasonable chance they'd outperform the tree-transformer.So overall the results are intriguing, and I believe there's something potentially valuable here. But I'm not sure there's sufficient reason presented in the paper to use tree-transformer over seq-transformer or other seq/tree models. Also, while the basic idea is nice, as I understand it is restricted to trees, so other graphical structures wouldn't be handled. -------------------After reading the author response, I do not think the paper does a sufficient job of evaluating the contributions or comparing to existing work. The authors should run ablation experiments, compare to existing work such as [1], and evaluate on additional datasets. These were easy tasks that could have been done during the review period but were not.If I wanted to build on top of this paper to train higher accuracy binary networks, I would have to perform all of these tasks myself to determine which contributions to employ and which are unnecessary. As such, the paper is currently not ready for publication. UPDATE:I read the revision and stick to my vote. In the discussion, I wasn't able to get my points across, e.g., that bounding the worst case duality gap is not enough to conclude that the observed duality gap does not grow for multiple local optimal GANs, where the duality gap is expected to be much smaller. A simple experiment could be to actually measure the duality gap (flip the order of the players and measure the difference of the objectives, when starting with the same initialization). If the authors were right, the maximum of those gap should stay constant when adding more data generators. To justify a Stackelberg setting, the authors may provide an example instantiation that cannot be cast into a standard zero-sum game with minimax solution. I can't see such an example but I'm happy to be proven wrong. =============After rebuttal:Thank you for improving the clarity. Unfortunately, the following issues are still unresolved to me:- Figure 1: this geometrical argument seems to be at the core of Eq. 1 but I still have a hard time understanding it. You might want to formalize the argument in text.- Experiments are not convincing. Even the original MNIST is not a representative dataset for optimization methods. The theoretical contributions of this work is not enough to justify having limited experiments.- The rebuttal says: "The hyperparameter k in DCL is tuned by trial and error on the test set.". Does that mean mistakenly using the test set as the validation set? -------------------------------Post-rebuttal comments-------------------------------Thank you for taking the time to revise your submission. I will maintain my original score of 4. The main justification for this is that the two main weaknesses I see in this paper (the first two I list in the original review) remain unresolved, and it indeed is unclear whether the second one could feasibly be addressed without significant changes to the core methodology currently proposed. **Post Rebuttal**Thanks for the response. I still didn't get an answer for why the second order term and not the first order term is of interest in the Taylor expansion. Also the paper does not seem to be updated with the promised changes, particularly error bars. So I am leaving my score as is.  [Post-rebuttal update] No author response was provided to address the reviewer comments. In particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern. I've left my overall score for the paper unchanged. [Edit: as mentioned by the other reviewers, this extension isn't as novel, given Ha et al's work, hence that reduces my confidence about accepting this paper further...] I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter "drop rate" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. ====== Updates after discussion/revision period:It appears that the paper has improved. However, the changes appear to be so substantial that the paper is now essentially a different paper which would require a new review process. --------- Opinion post rebuttals:After reading the rebuttal and the other reviews, I remain of the opinion that more work is needed before warranting acceptance. This paper indeed learns prototypes on the fly in contrast to e.g. Mettes et al. 2019. That method was however not designed for hierarchical knowledge, while several recent CVPR papers were. Since the novelty over these papers is limited and direct comparisons are lacking, more research is needed. Regarding the author responses, I have updated my rating. __________Update after the discussions:I would like to thank the author(s) for all their comments. Although most of my concerns have been addressed, some questions remain topics of contention. Before discussing these topics, I will first append to this review my answer to the author(s)' last comments, as it was their wish to keep hearing from me after closing of the discussions:  $\ \ \ $ The assumptions (3.1, 4.1, 5.1, 5.3) made on the function sequence $f_t$ for convergence of the proposed algorithm are unconventional as they require that the expected absolute variations of two function values at the points visited by the algorithm be bounded, or that the squared variations of two function values obtained by Gaussian sampling from points visited by the algorithm be bounded. So formulated, the conditions for convergence involve the algorithm's trajectory $x_t$ as much as the function sequence $f_t$, and they are difficult to verify. In an attempt to identify sufficient conditions for these assumptions to hold true, I made three suggestions: (i) and (ii) were concerned with the boundedness of the sequence of points generated by the algorithm, and (iii) was the case of bounded incremental variations of the sequence $f_t$, e.g. martingales. In their reply, the author(s) were right to rule out (i) and (ii), which indeed were unrelated. This leaves us with (iii) as a possible setting for the proposed algorithm.  $\ \ \ $ In my last comment I argued that the case (iii), where the sequence $f_t$ undergoes incremental variations uniformly bounded in expectation, was covered by the approach taken in Bach & Perchet (2016), where two function queries obtained from perturbations around the same iterate are processed at each step. The Bach/Perchet approach is cited in the paper for comparison, but it is called impractical as it would not apply when $f_t$ varies over time $-$ argument I disagree with and that I attempted to refute in a brief discussion involving martingale-like variations for $f_t$. When the author(s) of the submission object to my regret analysis in the case of martingale-like noise on the basis that the assumptions they make also cover non-zero-mean variations with similar uniform upper bounds on the moments, they do not address the main point of my comment. My intention was to show that it does not take much effort to consider the approach used in Bach & Perchet (2016) in settings where the cost function is changing over time, for as long as the cost variations are incremental with bounded moments. This can be seen by noting that the convergence result derived in the revised version of the supplementary material for the residual-feedback algorithm with unit-sphere sampling can be reproduced for the Bach/Perchet approach under the considered assumptions. I take it that the author(s), who excel at deriving the convergence rates for such algorithms, will not disagree. Although the assumptions used in Bach & Perchet (2016) (uniform zero-mean increments) may look somewhat stricter, they have the merit of being clear and simple, as opposed to Assumptions 3.1, 4.1, 5.1, 5.3, which involve the trajectory of the algorithm and can't be verified easily. They are also sufficient to improve the convergence rates for higher degrees of smoothness compared to the early algorithm by Flaxman et al. (2005), which was the objective of that paper. Higher degrees of smoothness failing which it is difficult to improve the convergence rates, as confirmed by the convergence rates given in the submission. In my sense, one important message conveyed by the submission is that the approaches proposed in the submission and in Bach & Perchet (2016) can both handle bounded additive noise, and both fail in the more general framework of adversarial learning. By calling the Bach/Perchet algorithm impractical for their setting, I believe the author(s) of the submission missed to chance to compare the two approaches from a fair perspective and to answer the simple question that comes to mind when reading their paper: is the residual feedback technique really useful in the stochastic learning framework, or isn't convergence just as fast when the function queries are processed by pairs as in Bach & Perchet (2016), or in the reference paper by Nesterov & Spokoiny (2017) ?-------That being said, the following issues remain in this submission:$\bullet$ The assumptions (3.1, 4.1, 5.1, 5.3) made on the function sequence $f_t$ for convergence of the proposed algorithm are unconventional and difficult to verify, because they consist in properties of the iterates of the algorithm.$\bullet$ In our discussions, only incremental sequences $f_t$ with variations uniformly bounded in expectation have been identified to meet those assumptions. In my sense, this particular setting is also covered by the approach taken in Bach & Perchet (2016), where the function queries are handled by pairs obtained from perturbations around the same iterate. Also, I still find it unfair to call the latter approach impractical for the considered setting.$\bullet$ Since the convergence rates derived in the paper show no clear improvement, compared to the early approach of Flaxman et al. (2005), the arguments of the submission lie in the experimental results, where I don't think the algorithm by Bach & Perchet (2016) is given a fair treatment (for the reason explained in the previous paragraph). Besides, the application considered in Section 6.1 reduces to the unconstrained minimization of a polynomial of high degree that is neither Lipschitz nor smooth, which is a basic requirement for the convergence algorithms. This makes the convergence of the algorithms highly dependent on the initial point, unless optimization is done over a compact set, but I don't think the projection step was implemented for the algorithms.$\bullet$ In constrained optimization, the problem that the proposed algorithm samples function values outside the feasible set has been partly addressed by the author(s), who provided a variant of the algorithm based no longer on Gaussian sampling, but on sampling over a sphere. Partly because only one convergence result for a particular setting was derived, and it remains unclear (as pointed out by Reviewer 4) if all the benefits of Gaussian smoothing and all convergence results would also extend to spheric smoothing. This discussion is missing. In my opinion, the extension to settings where the functions can't be sampled outside the feasible set is not absolutely imperative in all frameworks (the author(s) have provided counter-examples), but it would be useful to know the limits of the proposed technique. All things considered, I would not recommend the submission for presentation at the conference. Independently of the final decision, I hope the author(s) will make the most from the discussions with all the reviewers.I would like to make a last comment about the submission and the discussions that followed. It is natural that the author(s) give the best picture of the algorithm they propose. Yet in the paper the contrast is particularly strong between, on the one hand, the haziness surrounding the assumptions made on the function sequence $f_t$, or the negligence with which the algorithms were applied in Section 6.1 to a problem not actually meeting the conditions for convergence, and on the other hand the severity with which the Bach/Perchet approach was disqualified as a possible method of solution. This contrast gives the reader an overall feeling of partiality, which makes the reviewing task an intricate, contradictory, and unappreciative one. -----Update after rebuttal:I appreciate the detailed answers to my questions and the authors' revisions. I also read the other reviewers' comments. While the new assumptions address my initial concerns, the new versions depend on the algorithm being implemented. As far as I can tell the assumptions might be satisfied for one choice of step-size while not being satisfied for another. Also, I agree with the other reviewers that generally in OCO one considers a worst case sequence of functions. A discussion of this issue in the main body of the paper seems appropriate.After addressing these issues, I think this work would warrant acceptance to ICLR. For now, however, I am not changing my score. Updates:Thanks for the authors' response and the revisions. However, the paper still lacks strong and significant results. Therefore, I keep my previous rating.  Post Rebuttal Update:I appreciate the author response, but I will maintain my score after reading the rebuttal and discussion with other reviewers. It still appears to me that the motivation and clarity can be improved, and so I would recommend focusing on those aspects in future revisions. Additionally, baselines such as "allocating a region for every single test point" should be compared to in a clear way (as opposed to being in the appendix), as such baselines seem natural to compare to. EDIT: Thanks for the clarifications. Unfortunately, none of the responses are enough for me to update my rating.One thing regarding point 1 in particular: the transductive setting seems contrived for adversarial robustness as it does not seem to correspond to a plausible threat model. It's true that in the transductive setting, the examples don't have labels, but since clean accuracy >> robust accuracy, just caching predicted labels on the clean examples is roughly as good (which can be done even if test labels are not available). ##### ======================== UPDATE =========================Thanks for the authors' clarifications.After a careful re-evaluation of the paper, I have many concerns about the performance of baselines on the StarCraft II benchmark tasks. The reported performance is not consistent with those reported in the SMAC benchmark paper (see Figure 4,5,6 in [1]) and QPLEX paper (Figure 5,8,19 in [2]). Moreover, I also evaluate the available GitHub codes of baselines on my own, which is consistent with [1,2].Using results in [1,2], QTRAN++ significantly underperforms the baselines on the StarCraft II benchmark tasks. Moreover, the paper claims that it uses the standard StarCraft II benchmark, the latest version of SC2, and the default baseline codes.Due to these concerns, I tend to lower my rating.[1] Samvelyan M, Rashid T, de Witt C S, et al. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.[2] Jianhao Wang, Zhizhou Ren, Terry Liu, Yu Yang, and Chongjie Zhang. Qplex: Duplex dueling multi-agent Q-learning. ICLR submission. https://openreview.net/forum?id=Rcmk0xxIQV --------### Post-Response UpdateUnfortunately, the authors' response is not satisfactory on multiple issues. Thus, I reduce my rating by one point. ----- post rebuttal ---- The authors haven't addressed my questions. I would keep my score unchanged. One more comment: I suggest the authors compare to a related baseline SimpleShot [6] that is arguably less complicated.[6] Wang, Yan, et al. "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning." arXiv preprint arXiv:1911.04623 (2019). After response: While some of my concerns were cleared after the response, the experimental evaluations presented do not sufficiently support the claim that the method can handle nonlinear constraints.  ### Post rebuttalI would like to thank all the reviewers for valuable feedback. My review score stays the same. ### Post RebuttalI have read the other reviews and the author's response. I also had another look at the revision. While I appreciate the work that was put in the revision, my main concerns remain:* This paper proposes an alternative to VAE-based approaches to learning disentangled representations, which are known to require hyper-parameter selection based on ground-truth factors (as pointed out in Locatello et al., 2019). Hence, to improve over VAEs in this regard, it is crucial that the proposed framework does not suffer from this same issue or least consistently performs better compared to VAEs. Unfortunately this is simply not the case: across Figure 2 and Table 1 there is no variation (among CF, GS, DS, LD) that consistently performs well or consistently outperforms VAEs. Similarly in Figure 3c it can be seen how the GAN-based approach and VAE-based approach perform similarly (especially if the outliers on the top would be included in the mean). Similar fluctuations can essentially be observed across all figures and which does not take into account yet the choice of GAN or other hyper-parameters that were kept fixed (as per my initial review). Further, notice how in Table 7 (when using ProGAN as opposed to StyleGAN), the best performing variation (DS) now performs more than 50% worse according to MIG -- which is not factored in the comparison to VAEs.* The abstract visual reasoning experiment is flawed, since the comparison to van Steenkiste et al. (2019) figure 11 considers a random selection of pre-trained VAEs for which it is unclear whether they were disentangled or not.* DS is a simple heuristic that generally does not outperform other approaches. Indeed, notice how not reporting the results for DS separate for each layer (as was done previously) now results in large fluctuations. In particular, for the four datasets considered in Figure 1 DS outperforms other methods only on 3D shapes. Further, although the authors in their rebuttal argue that DS is not a simple heuristic since they build [DS] on the intuition that singular vectors of the Jacobian provide a set of locally ``independent directions with respect to the perceptual metric, there is no evidence provided that this intuition is correct. I had suggested ways according to which this part of the contribution could be strengthened (i.e. by analyzing it in the linear case, or attempting to automatically select a good layer) but this was not further explored.I do think that there is significant value in a systematic analysis of GAN-based approaches applied for disentanglement in this way, as it could serve as a useful benchmark for existing (VAE-based) approaches to compare against. However, the current set-up falls short at this as it is not sufficiently systematic and certain variations remain insufficiently unexplored (like other kinds of GANs). Further, this is not how this work is currently motivated in the paper or how the comparisons are performed.These concerns are irrespective of whether the reader needs to perform a manual comparison to Locatello et. al (2019). Although I noted in my review that this is far from ideal, I can understand how this may be necessary if Locatello et al. (2019) can not provide a smaller range of hyper-parameters. In that sense, I believe that the authors did a good job incorporating VAE results at such short notice. =============================== UPDATE =============================== Checked the proof again and want to confirm that, is it using the composition with pointwise maximum? If so I believe that it's correct, just be more detailed with the proof. Now I'm only concerned with the technical sophistication of the paper. Would be good to discuss deeper NN. Also good to make clear the advantage over other convex or nonconvex but optimizable formulations. UPDATE AFTER REBUTTAL =========== I have read the author's response. While the case study for industrial applications is important, it would probably be much more impactful if the same study was done on a much larger/realistic scale. For instance, right now it appears that each edge vehicle gets an already available dataset for federated learning, which may have been cleaned and preprocessed properly. For claiming a real industrial deployment/importance, it would have been great if the study was conducted with vehicles receiving real-time data from real vehicles which is prone to be extremely noisy (although the reviewer is not sure if this would be possible for regulatory reasons (e.g., if such learning experiments would be safe enough on real autonomous vehicles as these applications are safety-critical)). Currently, the paper neither has significant enough contributions from novelty side, nor from industrial deployment angle. Hence, as such, the paper cannot be accepted. Perhaps more application-oriented conferences maybe more suitable for this kind of work. I do not think the response addressed my concerns. I would strongly suggest authors reconsider the design choices where I raised questions. Note that these are not only clarification questions, but also fundamentals of machine learning and federated learning. **Update after author response:** I appreciate the authors' efforts to address my comments. Part of the comments on the presentation of technical details have been addressed, such as the meaning of "position" of a vertex and "-" in Equation 9. However, the main concern in terms of the technical depth has not been addressed well. There is a lack of performance guarantees and theoretical guidance on the choice of parameter values. For example, how much worse in terms of the attack impact when the weakest vertex is found by the heuristic algorithm instead of by a full traversal? The results in Section 4.2 and Table 3 are empirical and not theoretical. As such, I could not change to a more positive rating.  ## Update after RebuttalThanks for the detailed reply. You have answered a number of my questions, but many of my main concerns remain, in particular points 3 b) and c) as well as 4). Furthermore, incorporating all the clarifications and additional discussions into the paper would in my opinion amount to a substantial revision beyond the usual scope of revisions after rebuttal. I will thus stick to my original rating and recommend rejection for this paper. I appreciate the response from the authors to my review, as well as to others. My concerns on the intuition are most not solved. Although in this DNN dominating era, we cannot expect the explainability as we had before, I still believe that a solid work should be grounded on a reasonable basis, which could be in a high level, such as BERT and SBERT. Let's refer to the example given in the model architecture. The projection of the sentence vector of "Life is a box of chocolates" is left-concatenated with the masked embeddings of the second sentence. This operation is very much lacking in intuition, how come the projection of a sentence representation can be concatenated with the embeddings? In addition, "The second encoder shares the same weights with the encoder used to embed s1", considering their inputs are very different, weight sharing for the two encoders are also problematic.Another point I just noticed, although the authors claimed that their model is better than SBERT, and did a comparison with SBERT-large, they did not compare with SBERT-base, which makes the conclusion unreliable.--------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------- Post Rebuttal --------------------------------------------------------------------------------------------I carefully read the reviewer's comments and the rebuttal. I think the author did not get my major concern on competing algorithms. I acknowledge this proposed method is a novel problem setting and my goal is not to ask for summarizing the difference in problem settings. However, the technical approach, which is based on (batch-)training sample selection and reweighting, could be validated through comparing against more comprehensive baselines, as I pointed out, under similar settings.Furthermore, I am not convinced by the claim that unseen perturbation cannot be justified if there exists a similar perturbation assemble during training. The reference-based perceptual similarity is a well-studied domain which could be directly adopted to evaluate the overall "difficulty" or "out-of-distribution"-level. And I am not convinced by the author that the experiments you have tried cannot be shown in paper due to page limit: at least you could include in supplementary, as they are important to justify if your experiment setting really validates generalization ability. In sum, I will keep my current rating.  Edit:I have read the authors' response and the other reviews. I still believe that this paper is not ready for acceptance.  ## After Rebuttal and Discussion PeriodI want to first say that I really appreciated the opportunity to review this paper. It was awesome to see the authors willing to respond to the various suggestions and questions that the reviewers provided. The paper has improved over time but some significant areas of improvement remain. There continues to be some discontinuity between the stated scope of this paper as XRL and the proposed CDT approach. During the rebuttal period, I felt that the authors didn't do enough to soften their claims to match the support provided by the contributions present in the paper through both the methods and experimentation. This is not to say that this is not valuable work. As discussed, the concepts and ideas are strong, I however feel that the execution and scoping of this work is a little off from clearly communicating the contributions it makes. One aspect will be through a full evaluation of the utility of their approach as explainable through user studies or other qualitative means. The claims made in this paper regarding explainability are currently unsupported.Additionally, I believe that there is significant room for improvement in the experimental portion of this work. If there were a way to provide some ablations of the CDT approach as additional baselines as well as the SDT/DDT benchmark, it would significantly improve this portion of the paper.As it stands, I have not chosen to adjust my score. I do however urge the authors to continue in this line of work. I believe that there is strong merit with the direction they've begun. I look forward to seeing a future completed version of this work. =====I have read the authors' response and my comments remain the same as above especially the paragraph regarding novelty. I am keen to see a revised version of this paper. #######As no author response, I will keep my rating. --------After rebuttal (updated)---------------Thanks for your detailed response. I would like to apologize for my late feedback. Since your rebuttal is long without proper organization (5 pages without properly using markdown), I may miss several points. I directly commented my feedback on my original review, by the marked textSeveral of my concerns have been surely fixed and the understanding of the proposed approach is much more clear. However, it is still difficult to change my score because of the following reasons:Current paper still requires careful polishing for facilitating reading. e.g. The authors claimed several times they will update the paper with pseudo-code, it is still missing after discussion. The paper and the rebuttal are still dense, which make the reader difficult to understand.The diverse datasets (such as NLP, digits experiment in the original DANN paper) and additional in-depth empirical analysis (not only accuracy) are indeed necessary. I think you do not need to claim a significantly better performance with SOTA. The detailed empirical analysis is more important.I hope my additional comments and feedback can help you improve your paper. ----- Post Discussion -----I appreciate the clarifications made in the discussion regarding additional experimental details and whether the methods were fairly compared. However, given that much of the claims rely heavily on the empirical evaluation, I think further experiments with more rigorous statistical analysis is necessary. Even with the correctly plotted standard errors, the results still largely do not appear significant, suggesting that 5 seeds is just not enough. There are good recommendations by Henderson et al. (2017) and Colas et al. (2018) for the empirical evaluation, and I think it would additionally strengthen the paper to formalize the notion of heterogeneity (e.g., be able to approximately measure it, and convincingly argue that this is what's underlying any differences in performance).  **Update after rebuttal**I would like to thank the authors for the detailed rebuttal, but my feeling now is that the rebuttal is making it even more complicated and sometimes conflicting with itself. I believe the paper needs some careful rewriting and updates to clarify its points and assumptions. Concretely, the paper is built upon the premise that each class manifold is a submanifold with dimension lower than that of the ambient space. I pointed out in my review that this premise may not hold at al, therefore the paper is fundamentally problematic. Then, R2 in one of his/her responses raise the same question, perhaps after reading my question. Then, I see a difference in response to R2 and my comments. For R2, the response is "The intrinsic dimensionality of class manifolds is absolutely the full dimension of ambient input space", which is effectively acknowledging that my critique is valid. However, the response to me is "This is very easily refuted by the ubiquitous and universal existence of adversarial examples". I don't really see why there is a discrepancy here. Besides, the argument that is used to refute my argument, namely existence of adversarial examples implies class manifolds are lower dimensional than the ambient space, is apparently wrong and can be easily refuted. By and large, the existence of adversarial examples only means that the decision regions are thin at every location, I can totally have a fine mesh of the data space that achieves this.  ## Edit after discussionI thank the authors for the clarification. I still like the work and believe it is promising. However, I don't find too convincing the arguments around problem-space and malware experiments, I am afraid. Also, it seems there is still experimental bias in the evaluation (on malware), which hinders a bit the opportunity to assess the actual effectiveness of the authors' approach. I would really encourage the authors to reason about the points raised in the review in a more principled way. ## Post RebuttalI have increased my rating slightly but still don't think the paper is ready for publication.  I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive). Post-Rebuttal Update: The response from the authors addressed several of my concerns and several clarity issues where fixed in the update paper. However, I don't think the results on ModelNet10 provide strong support for this method. While I don't think it is reasonable to expect this method to outperform other works which are specifically designed for mesh/point cloud inputs (given that this method is more general), I think there needs to be some application outside of super-pixel classification where the proposed method shows an clear advantage.   Update: I appreciate the authors response and, in particular, providing more explanations about eq. (3) (now eq. (5)). However, the explanations are still largely heuristic and based on unproven claims. To reflect the authors efforts to improve the paper, I am increasing my rating from 3 to 4. --------Update after the author's response:I was not convinced by many of the authors' responses.1. While the author(s) agree that $F(\lambda)$ is non-differentiable, they keep the gradient updates in the paper with a footnote referring to sub-gradient methods in (Boyd et al. 2004)? Moreover, it is not clear what the sub-gradient would refer too when the loss function is non-convex, what would $H_{\lambda}$ refer to in section 3.2? One possible way to solve this issue might be through a mild smoothing technique. But as presented the theoretical part is not rigorous.2. The positive definiteness assumption in Lemma 1, although will most probably hold when the three functions are convex; this limits the application of the results as typical ML loss functions involve non-convexity.3. My comment on the subscript of the functions $f_i$ and $g_i$ in Lemma 1 was addresses by adding a comment mentioning that the subscript will removed for the simpler notation. I believe one subscript does not make the notation complex. Rather defining two notations might confuse the reader.Despite my tendency to have this novel idea and work published, I believe the authors need to be more careful in writing and dealing with the theoretical section of the paper. I will hence decrease the score to 4. After rebuttal :Thank you for the responses. I'm not sure why the authors didn't perform the experiments on the correlation between the previous factor-based disentanglement scores and the proposed disentanglement score in the limited supervised setting. For example, if there are 5 factors, I propose to evaluate the proposed disentanglement score for every possible pair of the factors (10 pairs) and average the scores. I believe this paper handles the valuable topic but it is not enough to be accepted since the experiments, which are crucial I believe, are omitted (comparison with the other disentanglement score, baselines to Delta VAE, comparison with [2]). Also, I concerned that other readers might be confused with the ("factor-based" disentanglement and "symmetric-based" disentanglement ) and (limited-supervision and weakly-supervision) (a new section should be added to handle these topics if this paper should be accepted). Furthermore, discussion with the related works is not enough.I lower my confidence rate to 3 (5->3) and vote for weak reject (3->4). But, I hope this paper would be accepted after revisions in the future. -------------**I've read the authors' response. I'm still concerned with the novelty of the paper given there are similar results for EG/OGDA. Therefore, I stick to my original rating.** ----------Post-update:Thanks to the authors for your response. I deeply acknowledge the promising results achieved from your work, which is impressive. I still feel hard about the contribution, though I also acknowledge the difference and it is an excellent practical trick.  UpdateFirst I'd like to thank the authors for their detailed rebuttal. I have upgraded my recommendation from 3 to 4. As mentioned in my review I believe this approach is interesting. However, as pointed by reviewer2, the experimental section lacks completeness. I think this experimental section would be suitable for a workshop, but not a conference. I am excited to hear you are considering to use this method as an inspiration for real problems. I'd like to see the paper resubmitted when you have obtained such results. ##########################################################################Post-rebuttal:I have read other reviewers' comments. Since the authors did not provide feedback to our reviews, I would change my score from 5 to 4. ### Updates after rebuttalAbout the contributions of the paper, it does provide some interesting points of view.The authors revise the paper and modify their basic assumption. However, it seems to be a hollow assumption without any guarantee. The concerns are as follows: First of all, why the authors claim it as a diagonal matrix when actually observing it as a block-diagonal matrix? Secondly, considering the updated assumption is only tested on one layer of VGG and ResNet, I am really worried about its generality and reproducibility.Totally, the weight distribution assumption is the cornerstone of the paper, but the assumption seems to be not convicing. Specifically, the assumption in the first version of the paper is a paradox to some extent, and the revised version casually modifies the assumption without too much verfication. I have to decrease my rating to 4. Post RebuttalI have read the authors rebuttal and updated manuscript. The authors have taken some steps to clarify parts of the manuscript, but my main concerns remain and thus my score is unchanged. In particular, the authors did not clarify how the two optimisation problems in Eq. 2. relates and what this means algorithmically. The authors defend their empirical setup by stating that meta-testing is still K-shot. While this is true, it is also true that their method have enjoy a greater amount of meta-training compared to baselines. What I would have liked to see is an ablation that trains the baselines for 2x meta-updates, but this is unfortunately not provided. Echoing other reviewers, if stronger attacks are considered, these should not be relegated to appendix.  Post-rebuttal: I'd like to thank the authors for the rebuttal and for pointing out some additional results that I had initially missed. While they're definitely welcome, it is still not clear to me why the main work is based on FGSM and why simple defense techniques have not been considered, both of which significantly weaken the manuscript.  I also see that very similar concerns have been raised in the other reviews , confirming that these weaknesses of the manuscript are quite salient.  ***** post rebuttal comment *****Thanks for sharing the response. Unfortunately, the very basic issue with the definition used in this paper, and its implications to practice, remains unsolved.To clarify the definition, you just need to focus on the following simple example: what if the model has zero risk/error?  If you perturb a point, it would still be correctly classified. Yet, they still show that adversarial examples are inevitable even in this setting. This already shows something is fundamentally wrong with the definition used.Your response is that the attacker/adversary will not get to change the label, but only the features. But please note: the adversary *is not allowed* to choose the label. The adversary picks the features, and it is up to the model to correctly classify it or not. If an attacker changes the picture of a cat to to a picture of a dog, the neural net (or any other model) should call this dog (and if does still calls it cat it would be a a mistake not the other way around). The ground truth (i.e., the concept function) determines what is correct and what is not. The above issue is not imaginary, it has real affect on the experiments, and as I said, it is important to report in the experiments whether or not they attacks lead to *actual misclassification*.I hope these comments will help improving the paper, since as I said, the topic of this paper is a very important one, and so exactly because of this, it is important to have the basics right. ========================Final RecommendationI have read the rebuttal and decided to keep my score. I think this study needs to be further motivated.I also want to clarify a minor issue in authors rebuttal. In contributions, you say: "We demonstrate that alignment is an invariant for fully connected networks with multidimensional outputs only in special problem classes including autoencoding, matrix factorization and matrix sensing. This is in contrast to networks with 1-dimensional outputs, where there exists an initialization...". My point is that the matrix sensing problem that you study here has 1-dimensional output. ##################################Update after reading other reviews and author responses:Thanks to the authors for answering my questions. However, my main concerns were not addressed, so I will keep my score.  Post-rebuttal: I would like to thank for the detailed responses and the careful revisions made in the paper. Overall, the paper is now definitely improved in certain ways and initiates an important discussion on the value of episodic training & classifier synthesis for few-shot learning, as opposed to typically-simpler metric-learning based approaches. The paper also approaches this problem from an interesting point of view, by focusing on sample utilization in the episodic training of PN.  However, I still find that the the paper remains somewhat weak in its current form for the following reasons:- I maintain my view that NCA vs PN are not direct alternatives to each other, considering that PN allows learning a representation that is optimized for class-average to sample comparisons, whereas, NCA uses a sample-to-sample distance based loss. The fact that the very construction of these two models, despite the similarities pointed out, blurs the strength of the overall NCA vs PN based discussion on the value of episodic training.- The claims about the advantages of non-episodic training of NCA is mainly based on the observation that NCA creates more positive/negative labels. However, it is not clear whether it is the more efficient utilization of training samples or just the differences in terms of the predictive model formulations. (Perhaps, averaging based prototype computation is a bad idea, after all, which may not have directly anything to do with episodic training.)- To this end, Fig. 3 is indeed interesting, but again the results are not very clear. Here, careful optimizer re-tuning specially for each case can be necessary as subsampling degrades the gradient approximation quality, which creates the question how much fundamentally important efficiency in batch utilization is, as long as one uses a proper optimizer. Overall, I think the paper makes a valuable step in an interesting direction but the paper fails to make a strong-enough case. Overall, I  improve my rating by a single level to 4, but find that the paper is not stronger than this in its current form. ***Post-rebuttal***I want to thank the author for addressing my concerns. However, I still have issues with the evaluation and the clarity of the paper. I think the paper requires another round of revision before it is ready for publication. Update: I thanks the authors for preparing a response and for providing additional experiments. I believe these additional experiments will benefit the paper. However, these experiments will require a full re-evaluation of the paper which, in my view, is beyond the scope of a response to a rebuttal. ### Post RebuttalI have read the other reviews and the rebuttal. I appreciate the extensive revision and response of the authors. Indeed, several of the minor issues/clarifications that I had raised have now been addressed.However, as noted in my initial review, my main concern with this work is the highly limited novelty and the significance of the findings:* LORL is essentially a simple application of unsupervised object-centric representation learners (like MONet, Slot-attention) to the language-guided visual reasoning framework proposed in MAO et al (2019), where the pre-trained vision module is interchanged. As I have previously argued, the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules, which is not very interesting. Indeed, this score is not needed for segmentation (which is the primary measure of success that the authors have adopted).* The main finding, which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance, is not very surprising. Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations, the considered type of questions (and DSL) provide a sizable amount of supervision I believe (as is also evident from the observed fluctuations in Table 9).One issue that I noticed in the revision is when comparing the results in Table 9 and Table 10. It can be seen how when training on the visual reasoning task using only 25% of the provided data (i.e. 22.5K as opposed to 90K) actually reduces segmentation performance, i.e. from 83.51 (image only) to 81.01. This is surprising, and perhaps somewhat concerning, since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance. The authors do not provide an explanation for this behavior, while it appears to invalidate the main claim regarding the benefit of LORL in the general case.For these reasons I remain in favor of a rejection. -------------------------------------------------------------------------------**UPDATE from responses**Thank you for your detailed response to my comments.The majority of my comments describing the weaknesses of this paper are related to both the conceptual and experimental components of the corridor example, so I want to recommend not including the example at all. It does not add clarity or provide any conceptual insights. This is in agreement with your stated primary focus, which is on demonstrating the empirical performance of a novel intrinsic reward design. Further, if you removed nearly all of the parts of the paper referring to the corridor example, you might have space to explain the detachment and derailment and the relationship of your method to these problems, as well as the other terminology that is currently not defined. It may be appropriate to include the idea briefly as motivation.**It is not clear to what extent short-sightedness is a serious issue and how much it affects exploration.**None of the papers have the term "short-sighted" in them, so at the very least, you are using a new name for a problem and then not defining it clearly. To the best of my reading, the problem you describe as short-sightedness is not described in these papers. Ecoffet et al. focus on derailment and detachment (which are different from the short-sightedness problem described in this paper) and while Guo et al. do describe myopia, their paper is not about intrinsic reward, so the type of local optima that they get stuck in are quite distinct from the optima created by the intrinsic reward design that the short-sightedness problem refers to.I am a strong proponent of using tabular simplifications to better understand problems, but in this case, the tabular simplification has not allowed for a sufficiently detailed understanding of short-sightedness. For this reason, I think it is important to look to the function approximation setting. In the function approximation setting, shortsightedness isn't clearly a problem. Your results in Table 2 suggest that RND doesn't suffer from short-sightedness in the same way that the tabular count-based method doesit does not spread out between the corridors evenly, which seems to be the behaviour you are looking for, but it doesn't get the short-sightedness effect either.**The pseudo-mathematical analysis in the appendix seems seriously oversimplified and flawed.**When I said that it was over-simplified, I meant that the assumptions that were made are inappropriate. I am not looking for more complex analysis; I am looking for analysis of an equally simple problem but one that is correct. The primary problem with the analysis is that it sort of stops in the middle. There is a great deal of set-up, and then the final point is simply a qualitative statement that if the growth of a quantity is proportional to how big it is, then the quantity that starts out larger will continue to get larger. The differential equations are not needed and not even used to make this point.1. While it is true that you do not state that the second-longest corridor will obtain the second-most exploration, I would like to understand what makes the choice of the second corridor to explore different from the choice of the first corridor to explore. From what you've written, it sounds like the agent is going to become obsessed with the longest corridor, and continue exploring it until it is "exhausted." If the first corridor is never exhausted, why are the remaining three corridors not explored roughly the same amount? If the first corridor is exhausted, why isn't the next corridor that the agent becomes obsessed with the second longest, since it is now the longest corridor that is not exhausted, making it a similar choice to the initial one. In this case, wouldn't the second-longest corridor receive the second-most exploration? While I understand that because the action is sampled from the distribution output by the policy network instead of taking max, the "exhausting" would not necessarily happen neatly, but it still seems that the Q-value of the second-longest corridor should become the second-largest, again resulting the second corridor being sampled the second most.1.3 I am not disputing that this could be thought of as getting stuck in local minima, but I believe that the idea that "alternately feature lower visitation counts" is a completely different problem from the long corridor preference (which may actually have higher visitation counts, but is more preferable due to accumulation).2.5 Since you do not know why your method outperforms the baselines, it is inappropriate to say that BeBold mitigates the detachment problem. 2.6 I'm sorry that I don't understand this response. Can you explain the relationship between reward clipping and dedication?3.3 The response does not address my concern. The primary problem is that the way detachment and derailment are described does not seem to be accurate. The example, "in which the agent gets trapped into one (long) corridor and fails to try other choices," partially addresses the problem of detachment, but does not appear to have any relevance to derailment. However, I am also of the opinion that for the authors to adequately support claims about these concepts, they must be described in enough detail for the reader to understand.4.2 I understand that a difference between RIDE and BeBold is this use of an embedding network, and I can see why you might hypothesize that it might affect performance, but again, I don't think there is any evidence given to show that this is the reason that BeBold outperforms RIDE, so it should be stated appropriately.6.1 While I am familiar with RND, the problem I am trying to point out here is that the terms appear in the paper prior to any explanation of what they are, which severely hinders readability.**Additional Feedback (Here to help, not necessarily part of decision assessment)**I recommend using the terms _target_ network and _predictor_ network for the two networks involved in RND, rather than teacher and student. Since target and predictor are used in the majority of the paper by Burda et al., they're more memorable for readers of the paper, so I think it will be less confusing.New Typos: "The results is averaged"  The results are averaged (p. 5) # UpdateI thank the authors for their comments and answers. While they agree on some of the concerns I raised, others are still left open.I believe they could be addressed in new major revision of the paper and I encourage authors to do so. ################################################################################################Post-Rebuttal Update:I'd like to thank the authors for their updates, the additional experiments are especially welcome. After taking into consideration the responses and the new evidence, I believe the concerns I raised still stand. Therefore, I keep the previous rating. Post-Rebuttal:I want to thank the authors for their responses and clarifications. I think the revision already improved the quality of the submission quite a bit. However, I still believe that there are some aspects which need a better presentation and clearer discussion. For example, a more direct discussion and (empirical) comparison to other approaches like AnnealedVAE is necessary, as also other reviewers pointed out, to justify the points made (qualitatively) in the paper. The added results in figure 6c already provide results in that direction.  I appreciate the clarifications in the notions of action and action sequence. Although I agree that the notions are comprehensible in the toy example and dSprites setting, I still think that the point I raised in my initial review applies. In order to provide a well-defined notion a more formal definition is required. To me it is still unclear what an action sequence in the case of e.g. images of faces should be.I genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion. Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. However, I would like to encourage the authors to do a major revision taking the issues raised by the reviewers into consideration and to submit again.###  ===== Post Rebuttal UpdatesI thank the authors for responding to my comments. The work shows improved generative performance by using a multi-scale architecture, however the approach is the same that used in previous GAN works. Furthermore, the generation quality is not as good those of recent GAN works and I don't believe the added EBM benefits are significant.  In addition, other contributions, such as the use of the Swish activation and domain transfer has also been noted as used in previous work [1]. I also have additional empirical concerns over the experiments.I list additional comments below:1) The diverse inpaintings (Figure 11) do not really look very diverse to me and seems to suggest that the model is not learning a likelihood. To evaluate diverse inpaintings, it would be good to follow past work and evaluated on ImageNet images where only the top half of an image is conditioned. [3]2) The likelihood evaluation are hard to interpret in A.5. An issue with evaluating AIS based likelihood sampling on MNIST for upper and lower bounds of likelihood depends heavily on the large number of steps of sampling required.  Upper and lower bounds depend heavily on the number of steps of sampling run (with unrealistically high likelihoods obtained when running only a few steps of AIS). It seems unlikely to me that the proposed model obtains a significant boost to log-likelihood compared to past approaches, and it would be good to report both the number of AIS transition distribution (and ensure that it is same used in [1]). In particular, I believe that this approach is likely to perform poorly with a large number of gradient steps (as the rebuttal response noted), which is required for proper evaluation of likelihood. 3) The related work is still missing older work in the area of EBM training. Instead of adding additional references to recent work on score based generative modeling, I think the others should cite past works that have used score matching to training energy models. For examples, such works include [4, 5, 6].  4) I didn't find the out-of-distribution results to be a particularly compelling application of the model (although its good that it performs similarly to past approaches). The only results that appear to be better are uniform (which in my experience performance across all models fluctuates) and interpolation. Furthermore, the model from [1] used in section 4.2 is not conditional. 5) I wouldn't say this paper is the first to generate 512x512 samples with EBMs. For example see [3].6) It's difficult to evaluate an open source code release, since the code is not provided at the time.7) Regarding the approach in [1], when doing source translation images are initialized from ground truth images from a seperate class.[1] Yilun Du and Igor Mordatch, Implicit Generation and Generalization with EBMs[2] Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, Pixel Recurrent Neural Networks[3] Tian Han et al. Divergence Triangle for Joint Training of Generator Model, Energy-based Model, and Inferential Model[4] Jascha Sohl-Dickstein et al. Minimum Probability Flow Learning [5] Saeed Saremi. Deep Energy Estimator Networks[6] Hyvärinen, Aapo. Estimation of non-normalized statistical models by score matching. Update:  Thanks for the detailed response.  I appreciate the additional figures and other results that you have provided.  However, it seems like there's still some open questions about the types of improvements being made and what this implies about the LM's attention mechanism.  It seems like more quantitative analysis would be needed to determine how much the LM's attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements.  Additionally, there's some limitations to the way the language model is being leveraged and the types of knowledge it can extract.   I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews. #### Post Rebuttal(Copying from the discussion below)I would like to thank the author(s) for their response. After going over them, I am still not very confident about the paper would stick to my initial assessment. Following are my primary concerns:"We note that there is a distinction between wanting to see something from another point of view, versus wanting to answer a question from another point of view. The former is where re-rendering is appropriate, but we do not make the claim that this alternative (view rendering + VQA) performs better or worse empirically."I understand the distinction. But the issue still remains. Why is the out-of-the-box "view rendering + VQA" solution insufficient? Is there any empirical justification for it? If not its hard to see the value in the current setup. A potential way to address this could be to run a simple out-of-the-box "view rendering + VQA" baseline."(2) R3 and R4s concern about camera information being provided to the model and its potential infeasibility in practice: In real world settings, camera rigs can and do have knowledge about where they are situated in the world, for instance using SLAM or GPS coordinates. In that case, it is not unreasonable for e.g. an autonomous vehicle to answer queries by performing rotations and/or translations of its current viewpoint."The concern was not about the viewpoint of the observer but the new viewpoint from which the question has to be answered. Also, the location of the new viewpoint need not be converted into float and appended to the question. It could be expressed in natural language. For example "viewpoint of the driver in the other car" like in the example provided by the paper. In the current setup the information about this viewpoint is provided in terms of exact coordinates, which makes the setup less interesting and not so practical.Although the authors improved some of the figures, the latest version of the paper does not seem to address other clarity concerns like a clear section for the dataset; organization of text and figures; removing unnecessary equations #########################################################################POST-REBUTTAL RESPONSE:I read the author's rebuttal but have decided to not increase my score. I still have doubts over the claims in this paper.  Update: I appreciate the detailed replies to my questions. Indeed, some of the points I raised were addressed well and the paper updated accordingly. However, some new concerns were also raised by the replies:- Using 3 seeds for the experimental evaluation is an extremely questionable evaluation protocol. There is no way to know if any of the results are going to hold up. - It's also clear now that none of the experiments are comparing to benchmark numbers from other publications. It would have been more confidence inspiring if the method was tested on a set of tasks where external benchmarks have already been established. - This is particularly true for the new results that were added to the paper, e.g. the QMIX results. It's difficult to make sense of them and the instability points towards a potential hyperparameter issue.   - All baselines for the 'prisoners' case should at least compare to the fully cooperative case of adding up the rewards. Comparing to a DQN baseline that maximizes individual rewards is a red herring. - It's odd that all experiments require less than 1000 episodes to train. This is very unusual for challenging multi-agent RL problems. It would be great to understand if the main selling point of LToS is sample-complexity/learning speed or if there is something else going on.I also agree with the concern raised by other reviewers that the paper is currently not positioned clearly. All things considered, I believe my score is still appropriate for the paper. However, I also believe that a future version of the paper with clarified positioning and more thorough experimental evaluation could make for a compelling contribution. I have read the author response and stand by my original score of the paper. [Post rebuttal] The rebuttal has not address any of my main concerns, so my rating stays. POST-REBUTTAL COMMENTS========The authors did not make a good effort to address my comments and failed to update the paper. Therefore, I maintain my original decision. **After Rebuttal**The author's reply partially resolved my concerns, although the diversity of models has not improved, nor has it significantly decreased. Thus I have increased my score from 3 to 4.  ### POST-REBUTTAL UPDATEThe authors' response did not address my concerns. Given that most current evaluations metrics for video generation/prediction have some shortcomings (including human evaluations), it makes more sense to include a wide range of metrics that showcase the strengths and weaknesses of a method rather than to argue against their inclusion in the paper. Additionally, the authors failed to mention very relevant prior work (Latent Video Transformer). Therefore I do not think this submission in its current form should be accepted and I have reduced my rating to a 4. --- post rebuttal update ----I appreciate the authors for their response. However, the arguments on FVD are still not quite convincing (i.e., FVD still has reasonable correlation with actual generation quality; if the perceptual metric is inappropriate, then the authors should have tried other metrics. Also, the authors did not address other concerns such as concerns on computation time, scalability, small-scale human evaluation, etc. I maintain my score to rejection of this paper. ***After author response***Thanks for the response. Perhaps I didn't explain my concern about the motivation. SVM with uncertainty sampling works well, yes; however, it is equivalently several different things including selecting points closest to the decision boundary in Euclidean distance but also selecting points with highest predictive entropy and selecting points with smallest predictive margin. In other words, it seems that you are extrapolating from the SVM case to the neural network case and it's not clear which of several equivalent things will work in the neural network case.Furthermore, this paper doesn't even use Euclidean distance but instead uses a newly defined distance. So I really don't find the motivation or theory convincing. The authors' response addresses a small part of my concerns, so I change my score. However, I still recommend rejecting this submission (Details are shown in my response). ----------------Post rebuttal-----------The issues I raised still persist in my mind. Also, other reviewers have similar issues and also more issues other than what I point out, which seem valid issues to me. I will keep my score as is. ====================================================================================**Post-Rebuttal**After reading the rebuttal and the other reviewers' comments, my concerns persist:- The technical contribution is limited. Adding a pretext task of homography prediction itself brings little insights regarding how it improves upon contrastive representation learning from a different perspective. - The experimental results are not convincing compared to the recent advances. The authors are encouraged to include ImageNet results as well as transfer learning evaluation.Therefore, I would like to keep my initial rating as rejection. Update after rebuttalI want to thank the authors for a long and highly detailed rebuttal. They clarified a lot of my questions and hopefully in the process they were able to improve the paper. However, my listed weaknesses still stand:there is no transfer learning experimentsthere not seem to be any consistent gains with the proposed approach over other methods, as seen in Table 4, after 500 epochs (when the models indeed have probably converged). Gains can be seen for 100 epochs, but from the absolute numbers it is obvious that models havent really convered at that time. Furthermore, as the authors clarified, they do require 30% higher training time, something that make the claim that they learn faster even weakerThere is no analysis It is unclear why/how forcing the feature vectors difference to be predictive (ie encode) the transformation and learning representations that are non-invariant to homography transforms actually helps for learning better representations. In fact, It turns out that simpler versions of the proposed module than homography estimation are the ones giving the best results.I will therefore retain my rejection rating. ####################Post Rebuttal#######################################I meant tiered-ImageNet (https://github.com/renmengye/few-shot-ssl-public). I apologize for the mistake. I think the rebuttal has not addressed my concerns, especially the one related to the 'ill-defined' tasks. The 'ill-defined' tasks detected from the proposed method could be 'hard but good' tasks benefitting the test. The authors didn't deny this. And the proposed method tries to push the model to learn less from such tasks. A natural deduction is that it could affect the classification performance. I think if the authors can take a further step to disentangle uncertainty estimation and classification, this con of the proposed method can be removed in the future.  ** update after rebuttal **Thank you for the detailed clarifications. I still find that the recommended method used in practice, which only encourages robustness to local random perturbations, is too disconnected from the motivation of manifold regularization, and will thus keep my score. ##########################################################################Post rebuttalThe author's response does not address my primary concern and I'd like to keep my original score. ### Overall evaluation: The paper is interesting and tackles an important problem. However, it lacks clarity in many aspects.=====POST-REBUTTAL COMMENTS======== I thank the authors for answering my questions. Their answers did clarify some aspects, such as the ratio of mixed sources, the optimal data collection, and the relation of the proposed work to active learning and multitask. However, the answer provided on my comment about the distinction between excess loss and absolute loss is not sufficient. I believe this is an important point, and calls for a major revision, not just a promise to clarify the point throughout the paper. The same comment applies to the experiment when comparing methods on the basis of computing the excess loss and not showing the absolute loss, which is the actual measure of quality of an estimator. ===== After rebuttal =====As I have mentioned in my review, I am not an expert in the field of neural ODEs. Regarding the technical contribution I think is quite interesting, but probably rather small as far as I understand from the rest of the reviews. In my opinion, the current paper is very much focused to audience which is very familiar with the topic, and its impact on a non-expert probably limited. Therefore, I tend to keep my score and vote for rejection, because I feel that the submission has to become more accessible to general audience. Edit: I have read the authors' response and the other reviews. I still believe that this paper is not ready for acceptance.  ----**Post rebuttal comment**I thank the authors for detailed rebuttal and new empirical results. I also have read other reviewer's comment, and decided to keep my original score. The main concern of this paper, the weak novelty, still remains (also pointed out by Reviewer#1/4).  =========================================================================================================Update:Thanks for the authors' response. But unfortunately, the most significant concern has not been addressed well, which is regarding of the paper's novelty. I thought the contributions of this work are incremental, and the authors' rebuttal did not convince me well. Btw, the authors claimed that [A] used KL-divergence while their work used mutual information maximization, however, minimizing KL-divergence is actually one kind of mutual information maximization. What's more, the authors even did not clearly point out which kind of mutual information maximization they used in either the manuscript or the rebuttal. Thanks again for the authors' efforts, but I choose to maintain my original score. After discussion with authors------------------------------------------Some of the concerns of the reviewer have been addressed and the reviewer is raising the score to reflect it. The paper still has some major concerns preventing the reviewer from recommending acceptance of the paper: - Similarity of the theoretical analysis to https://proceedings.neurips.cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf  and https://arxiv.org/pdf/1804.11285.pdf .  - Hard negative mining is pretty standard in many learning domains. Both of these issues can be addressed by a better review of the related work and more accurate identification of the novelty in the paper. Given the limited theoretical novelty, a more robust empirical evaluation establishing the value of the extreme outliers against state of the art approaches would have been useful. Also, if the authors want to investigate theoretical results on OODs, one challenge is the lack of a formal definition of OODs. The paper is a good work in progress but not yet ready for publication. The reviewer is hopeful that the above suggestions will make the paper stronger.   ----------------------------------------------Update after Rebuttal:I have read the authors' response but do no change my scores.The experimental results alone do not offer a theoretical justification. The paper does not have to contain the latter but then the evaluation would have to be exhaustive.I see that the authors have done a huge number of experiments. However, if basic standards like standard deviation are neglected in order to run just more experiments, the results of the entire evaluation remain questionable. Similarly, related approaches have to be considered adequately to have an appropriate comparison to SOTA. The authors have not made them available yet. # After RebuttalI have read the response to my review and the responses to the other reviews. The summaries of the paper in the other reviews helped to clarify my understanding of the research question the authors were aiming to answer and how they went about doing so. I have re-read the paper and the revisions have helped to make this story clearer.With that said, I remain concerned with many technical aspects of the paper, for example:* The scale of the networks studied.* I still don't understand why this particular definition of utility imbalance is well-motivated.* I generally don't think that two dimensional loss landscape visualizations are informative since they discard an enormous amount of information from the full loss landscape. To show that minima are related, I think it is better to use interpolation (mode connectivity).I am also still concerned about the writing. The revisions, alongside the other reviews, were enough for me to get a sense for the research question and technical story, but I still struggled to make sense of the details.Since the other reviewers appear to have better understood the paper, I have raised my score to a 4 and decreased my certainty to a 2. I suggest that the AC weight my review much less heavily than the other reviewers, who seem to have better understood the technical details. I appreciate the author response but unfortunately they are still a bit vague (1,3), or not supported with experiments (2,4). I still maintain my rating of 4. EDIT: The authors have answered my questions and it clarified a lot. Thus I raise my score by one. However, I am still suspicious about the value of the DP result: for example, it is not discussed why would the residual mapping have an L2-sensitivity G (as stated in the assumptions of the theorem). Also, the reported privacy values in the end of the revised version of the paper (epsilon > 1000) are not meaningful. As far as I see, the experiments give an example where this given membership inference attack works better for DP-SGD protected model than for this residual perturbed version. However the paper does not give any privacy guarantees for the residual perturbation method. Whether the L2-sensitivity can be obtained with e.g. batch normalisation remains unclear to me. I think that the paper would require a careful rewriting. ----------Update after author response----------I thank the authors for the detailed response. I think the fact that the near-linear time JL approach follows more or less from previous work needs to be clearly mentioned in the paper. I also think some experiments would be nice, and it would be reasonable to use some of the heuristics which the authors suggested, and sparse JL can often be reasonably efficient in practice. In light of all this I am keeping my score, but would encourage the authors to perhaps further pursue these directions. ===============================================================================================================\After rebuttal:\I have read the authors' response, but since the actual body of the paper has not changed much from the original submission, I stand by my original rating. === Response After Rebuttal ===I thank the authors for their responses to my comments. After reading the response as well as the other reviews, I still stand by my original rating. I still find the motivation and empirical results non-compelling, given the current version of the paper.  ------post-rebuttal updateI appreciate the authors for the responses. While the other reviewers give high reviews about this manuscript, I would keep my original reveiw for the following reasons. 1) This manuscript is incremental in nature. I agree with R3 that "understanding which techniques can be combined and which cannot (and how to fix it) is important for developing the field and feels like a small step to the right direction", but a somewhat unsurprising result lacks technical novelty. In one of the responses the authors wrote "The spirit of this work is to point out that not all data augmentations can be combined well with ensembles." I think this potentially means that the conclusion the authors made on mixup could not even transfer to other data augmentations. This even further limits the contribution of this manuscript. So I guess the above argument from R3 is not convincing, or one could try to study the effect of batch norm and ensemble and wrote another good paper. 2) The empirical performance of the method is limited and thus whether the proposed method is useful is a question.  After reading the responses from the authors and other review comments, I maintain my previous rating of this paper. I am not convinced that the proposed approach is a simple form of analogy reasoning. Trying to build a relationship between the proposed approach and analogy reasoning is uninformative and misleading. **UPDATE:** I appreciate the authors' responses and the engaged discussion. However, I still think that the claims of the paper are not sufficiently supported by the presented results, and maintain my original rating. ##################################Post-rebuttal:The idea is good, but the experiments and analysis are not enough to validate the proposed idea. The paper is not ready for a publication. *****Post Rebuttal*****I would like to thank the authors for the detailed rebuttal.The authors state: "The main goal of our paper is NOT to introduce novel models, but rather to introduce a NOVEL benchmark and insights/ingredients to study causal induction in model-based RL" and "It is true that the models we use do not learn an explicit structure for causal learning" which corresponds to my original reservations to the novelty of this paper. The authors introduce a benchmark / software for evaluating causal induction in RL models, where the user can specify a causal model and its influence on the environment can be examined. I remain unconvinced that the introduction of a RL evaluation benchmark (even one allowing for the presence of arbitrary causal networks) counts as novel at ICLR.Further, the statement "we used some of the typically common models for this purpose, such as GNNs and modular networks. Though these models do not learn an explicit causal graph, they do learn structure that could allow them to discover causal relationships under certain assumptions" confirms my point that no causal formalism (e.g. connection to the data generating process) is accounted for. It merely means that directed relationships can be modelled.I agree with the authors that (Lopez-Paz et al., 2017) only uses observational data and does not have any connection to RL, but it is an example of an approach to extracting causal relationships from images in a sound way when it comes to causal inference. This is a side comment and does not influence my assessment of the paper.To sum up, my reservations towards the degree of novelty in this paper have not changed (I agree with the authors' summary of their contributions, but disagree as to whether proposing a new benchmark constitutes novelty at ICLR) and I recommend a rejection of the paper in its current form. Post-rebuttal:Thanks for the feedback. The goal of learning causal representation is ambiguous, and it is absolutely a good research topic. However, I fail to see an obvious contribution of the current version. Researchers in this field are usually clearly aware of the limitations for existing methods in causal learning. The problem is how to handle it, e.g., how to give a appropriate definition of the causal variables, how to theoretically show the identifiability and consistency, and then propose a practical solution.  ::::::Post-Rebuttal update::::::After reading the new revision, I decided to keep my initial score. I do not consider the need of groundtruth real data for metric computation as a strong disadvantage. The authors report some numbers on Recall in Table 1 but it only shows that Recall is consistent with RND, being much cheaper to compute. Therefore, I do not see any reason to prefer RND over established diversity metrics. ----------After RebutalI appreciate the authors response to my questions and they addressed some of my minor concerns on the clarity and presentation. The authors confirm that that their work is a pure study on UQ for forecasting. In this case, the methods evaluated is not very specific to the forecasting tasks, and the conclusions are not very surprising either. Therefore it seems there is limited technical merit from this paper. The setting of the COVID forecasting task is also a bit strange and the authors response does not fully address my concern. I would like to keep my ratings unchanged. Update after Reading Authors' Response:I appreciate the authors thorough response and the new results for different crop sizes. Its not too surprising that the performance varies with the crop size and that the best crop size depends on the task and environment (among other variables such as camera resolution, camera viewpoint, and object sizes). And so, I share the concern of the other reviewers: its unclear whether this approach would be similarly successful in other manipulation settings without prior knowledge of the task outside of RLBench. --Post Rebuttal--Thank the authors for the response. I agree with other reviewers that the task is interesting and the submission has great potential, but might need another round of editing. The results show "a single perturbation from a random state of a completely different MDP" is not normally distributed. However, I agree with R4 that applying a single perturbation from a random state of another MDP does not make sense unless the two MDP and the two states have some similarities. =========After rebuttal======After the rebuttal, my main concern remains. Specifically, the paper defines a variational distribution q(z|x0 via a hierarchical construction: z_0 ~ N(0, 1), z ~ N(z_0, \eta I), which is essentially a zero-mean Gaussian. And I suspect the this is a bad variational distribution and it will induce high variance. The author said they didn't the hierarchical construction to define the variational distribution, because they fix z_0 after sampling. I don't think this is a formal way of defining a variational distribution. One reason is that even if they fix z_0, the proposal distribution will be a z_0-mean Gaussian, and the mean is randomly drawn from N(0, 1), which will not match the true posterior distribution (they only optimize the variance parameter). I think this should be make clear and investigated in more details. I will keep my initial score. Post-rebuttalI have read the rebuttal and other reviews. The first version of this submission fails to cite any highly relevant works in long-tailed recognition and generalized few-shot learning. I believe that addressing this issue would require a major revision. The authors argue that their proposed few-shot imbalanced setting is different from the long-tailed recognition problem and generalized few-shot learning setting.  But this is only partially true. It is unclear whether previous approaches for long-tailed and generalized few-shot learning can be already applied to address the few-shot imbalanced problem. Moreover, I am not convinced that the proposed setting is more appealing than those two existing imbalanced settings because the 5-way classification problem seems to be an artificial setting that rarely happens in practice. Finally, I realize that the proposed setting is actually not new. [Lee et al., ICLR 2020] have explored a very similar setting. Thus, I would keep my original review and recommend rejection. **Update following author response and reviewer discussion:**I would like to thank the authors for providing a response, and in particular for providing further justification for their injectivity assumption. However, the main concern remains the lack of empirical validation, even on toy examples, showing that (or whether) the derived theory here goes beyond a hypothetical thought exercise and can be implemented in practice. It shouldn't be difficult to provide such examples, assuming the proposed approach does indeed work as indicated by the theory developed here, and without such examples, the paper seems rather incomplete. Therefore, unfortunately, my score remains unchanged at this point, although I would like to encourage the authors to keep pursuing this direction. ****************************Thank you for the response and updates to the paper. Given the number of changes required, I encourage the authors to resubmit elsewhere with the updated paper, ideally with additional experimental comparisons as discussed. Note that there are many other offline RL works, such as BCQ or BEAR, that you could use (see, e.g., "D4RL: Datasets for Deep Data-Driven Reinforcement Learning" or "RL Unplugged: Benchmarks for Offline Reinforcement Learning" for relevant algorithms).  Additional comments: I have read the authors' response and the other reviews. While my initial concerns have been mostly addressed, there are still concerns from the other reviews and I have revised my score accordingly.In addition, I am not completely satisfied with the authors' response concerning higher-order interactions. In the end, even if you add other nodes, the operations are simply matrix operations and so you can model any higher order interactions with a single matrix. This should be clarified further.______________________________________________________________________________ After reading the rebuttal:I keep my score. The authors should rethink how to claim the novelty in a concrete way. The presentation needs to be improved for readers not familiar with information geometry (they should be the majority). I do not get 6) in the rebuttal. **UPDATE AFTER DISCUSSION:** R4's exceptionally thorough review raised a number of important concerns which I initially did not recognize. Moreover, the most important of these concerns went almost entirely unaddressed by the authors. Since the paper has not been appropriately revised, I have decided to lower my score from a 7 to a 4. Update:I have read over the changes made by the authors, and also the other reviewers responses. I am maintaining my score, because I dont think the current version of the paper is enough of a contribution to get accepted. As mentioned in my responses below, I would be happy to accept a future version of the paper that addresses my comments above.  Edit: After the rebuttal period, I maintain my original rating but am increasing the confidence of my evaluation from 4 to 5. I thank the authors for their hard work and engaging in discussion. I agree with Reviewer 3 that tuning with a fixed seed and the lack of search space refinement is a major weakness. The lack of a larger dataset further limit the applicability of the results. As such, I do not believe the paper in its current form should be accepted to ICLR.  ---Update after rebuttalI thank the authors for the response. I think that the revised version improved clarity. The overall impression I have of the paper is still similar to the initial one.The final proposed loss is reasonable, but just the combination of two existing ones with minor modifications. About this, even in the revised version the authors seem not to discuss and justify (at least in the main part) how the adversarial point $x'$ in computed at training time, which is different from TRADES according to Section C.1.About the experimental part, I thank the authors for adding the new experiments in Section D. However, the baseline in Table 4 seems a bit weak, at least for 10 steps ADV. For reference, the baseline WRN-28-10 in (Gowal et al., 2020) has robustness under AutoAttack + MultiTargeted close to 51% (I see that here a WRN-28-8 is used, but I wouldn't expect such difference).Moreover, although a minor concern, the authors didn't address the question about the step size used on MNIST.Then, I keep my initial score.Gowal et al., "Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples" ## Post-rebuttal commentsI am impressed at the amount of time the authors have spent trying to clarify the different concerns.  But unfortunately my concerns are not fully addressed, and also a new concern arises: if this much space had to be spent clarifying different confusions of the reviewers, I think the paper could do with a complete overhaul.  I would suggest that the authors take into account the general confusions that arose in this review process and rewrite the paper such that those particular confusions are alleviated.  For instance, I struggled for clarity on what makes this contribution non-trivial, whether the contributions are primarily theoretical or primarily empirical (and how I should understand the balance between the two), and the fairness of the empirical comparison.  Rewriting the paper such that those three concerns---and the others pointed out by the other reviewers---are discussed clearly would be a significant improvement. ** UPDATE**Read the author answers, thanks. There is a significant amount of new material added in response to the reviews, including some interesting new findings (e.g. Tab. 3, 4). Unfortunately that did not help the clarity issue. I did not see a new example in Section 3.2.Basically, the stronger points of the paper (the results) are stronger, but the weaknesses (clarity, motivation) are not really addressed. The new version uploaded by the authors is an improvement but I still lean towards rejection. #### Post-rebuttal comments- The paper should further elaborate on the smooth reward predictions and how online learning in the sparse reward setting can be possible with LatCo. It seems the method requires a specific initialization/implementation of the reward predictor, for instance, to overestimate rewards so that the method has to explore the areas where reward is overestimated and pull down the predicted reward. The paper should explain how this was implemented. This kind of exploration would be prone to the curse of dimensionality if the state representation of the environment is high-dimensional. The authors should discuss this limitation thoroughly. This might also explain why the tasks in the experiments are limited to 2-dimensional states.- I wonder about the discretization of the colors in Fig 8. Higher quantization of color should be provided so gradients of the reward landscape can be assessed.- The paper still does not detail the update rule for \lambda_actOverall, the author response has addressed some of my technical concerns, but the main challenges are only addressed partially. The paper is still borderline and might need another thorough round of improvement and resubmission to another venue.  ============= Post-Rebuttal Comments ===============Thanks to authors for their response and efforts in updating the manuscript. Some of my concerns were addressed. However, I still think that the novelty is fairly limited. For example, additional experiments in Fig. 5 produce quite intuitive results in the sense that any scheme that yields smaller t_c will have higher accuracy at a specific time slot. FedMes reduces t_c with the assumption that it is faster to communicate between edge servers. Systems-level experiments to thoroughly study the effect on t_c will greatly improve the contributions.  =======================================After reviewing the response from the authors, I decide to change my evaluation score and confidence score. **Update following discussion:**Following the revision by the authors and the discussion with them, I am updating my score from 3 (Clear rejection) to 4 (OK, but not good enough - rejection). This reflects in great part the revision the authors made to have the main paper (limited to 8 pages) be self contain and present their main results, while using the appendices for complementary and technical information.However, I still maintain the paper is not ready for publication in its current form. The extension of UMAP to implement the optimization via a neural network applied to input data rather than directly assigning coordinates is rather straightforward. The advantages it provides over UMAP in terms of natural inference on new data without the need for separate (more computationally intensive) out of sample extension method are a direct result of this neural network implementation, and they would be true not only for UMAP, but in fact for any method implemented in a "parametric" way via a neural network compared to nonparametric coordinate assignment. Similarly, allowing the addition of reconstruction or classification objectives in training is clearly a direct byproduct of this neural network implementation as well, and not unique to UMAP.Therefore, an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks, for example, to enable visualization or improve various tasks. The authors already look into this to some extent by comparing to parametric tSNE as one alternative approach, but there are many others, as I mention in the initial review, relying on constructions from topological data analysis and manifold learning - most, if not all, of which rely on some graph construction on the data and then ensuring the coordinates provided by a hidden layer in the network match the relations encoded in the graph, similar to the proposed UMAP loss term. How are reconstruction and classification affected by using such other regularizations compared to the UMAP one? Is inference speed the same for these other approaches? How does training speed compare between them? One can clearly expect some tradeoff between such properties and the geometric information encoded by different methods (UMAP and tSNE emphasize clusters, while other methods may emphasize other patterns), but this should be discussed and demonstrated clearly rather than just ignoring the vast amount of related work on parametric approaches to capturing intrinsic geometry in data.Now, beyond the described lack of relevant comparisons for autoencoding and semi-supervised classification, even simply as a parametric implementation of UMAP (which would be a rather narrow scope, which is not very enticing as a motivation on its own),  I am not sure this work is sufficient to establish the presented approach. First, for the inference or embedding speed - this is essentially and out of sample extension task. As such, even if one insists on only comparing to UMAP-based methods, there are multiple OOS methods that can be used, such as Nystrom, geometric harmonics, etc. Some analysis of the tradeoff between extension quality and speed seems warranted here, but as I said previously - I think a comparison should also be provided to other parametric embedding methods beyond just OOS of UMAP (and tSNE for that matter). Second, as the authors clarified in discussion - their approach relies on the suitability of the UMAP loss to be incorporated directly in the network optimization, essentially comparing activations to the UMAP graph. However, an alternative approach presented in related work is to provide a loss term between activations and a UMAP embedding. This second approach is more general, since other embeddings can also be considered there, but also probably has some disadvantages (for example, the a priori fixed dimensionality, as the authors suggest). The differences between these two approaches should be addressed better in the manuscript, and importantly, since previous work exists already on the embedding loss approach, the authors should present a comparison establishing the benefits of the graph-based loss one, in addition to discussion regarding them. To conclude, the idea behind this work seems reasonable, albeit rather straightforward since it's a reimplementation of the UMAP optimization. However, as it currently stands, I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones. I would like to encourage the authors to invest the time in adding such comparisons and clarifying not only how they are different from other methods, but also how they are better, and why choose UMAP to begin with as the basis for their proposed loss terms (compared to various other approaches - not just tSNE).--- Response to authors:The authors have largely responded well to my original concerns. However, after reading through the discussions with other reviewers, I agree with reviewer 2 that more work is required to make this publishable. In particular, this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function. Given this, I have downgraded my score accordingly. Update after rebuttal: The authors have added a recurrent SAC baseline to one set of experiments. The results indicate that the recurrent SAC is a much stronger baseline, and the variance of results is high enough that I am not convinced of the benefits of the proposed method.The authors argue that "many state-of-the-art RL algorithms" "are non-recurrent", and "frame stacking" is "largely untouched since its inception and is used in most state-of-the-art RL architectures", "recurrent architectures" have "additional overhead from training and implementation". I do not believe this is true. Recurrent architectures are commonly used in RL algorithms (for example RSSM in Dreamer) and are widely available in open-source implementations (for example https://github.com/openai/baselines/blob/master/baselines/common/models.py). There are some prior papers which use the frame stacking heuristic for a fair comparison with DQN, but this heuristic or the non-recurrent model architecture is not a part of the RL algorithm itself. Since this paper proposes a method for extracting temporal information, LSTM/GRU are very natural baselines in my opinion and should be added to all experiments.The authors argue that "it is very reasonable and common to have a new method improving a majority of the environments but not all", I agree with this, but the examples given by the authors such as Rainbow DQN, Dueling DQN, Dreamer etc perform experiments in many more environments and performance improvements are larger. I believe a much larger scale study is needed to compare Flare with recurrent baselines and make conclusive statements about performance gains. *Post rebuttal comments*: I appreciate the author's efforts for the rebuttal, however, the feedback did not adequately address my questions. I am not changing the score. After reading reviewer2's comment, I realize that there is literature proving much stronger results that I was unaware of. I still think these results should be used as a standard baseline for certified poisoning defense, but due to the lack of novelty, I have to downgrade my score.  ==========================================================================================Update after rebuttal:Thanks for the detailed author response. I think the paper is interesting in providing an analysis on how optimizing the linear relaxation in CROWN (although the verification method itself seems to be similar as the one in Fastened CROWN in AAAI 2020) can lead to better loss smoothness and tightness, which seems to improve the performance of certified training. The author replies have addressed some of my concerns in my initial review. However, there are still some outstanding concerns:1. After more consideration, I think the More favorable landscape paragraph is still insufficient to address the second point in my above cons. The author response argues that some $(p/q)$ have looser or tighter bounds, but these are considered for relaxation *locally*, not the tightness on the final output, while the relaxation optimized in the paper is to tighten the final output. Thus it remains unclear why tighter final bounds with improvement from the unstable ReLU neurons makes the model favor unstable neurons less.2. AnonReviewer2 has reminded me that the Fastened CROWN work had a similar method about optimizing the lower bound of the linear relaxation in verification, which seems to be very similar to the method proposed in this paper, in terms of the verification part in certified training. Although this paper focuses on certified training and has some different analysis, the major modification on the method is still on the verification part, and thus I agree that a discussion on the comparison with Fastened CROWN should not be missed. The authors did not add it in the discussion period.3. It is promising that the proposed method outperformed the modified CROWN-IBP ($\beta=1$) and IBP, and there seems to be a significant margin. But the proposed method fails to make a significant improvement compared to the original CROWN-IBP (the 1->0 one), e.g., the improvement on CIFAR-10 eps=8/255 or 16/255 is negligible. Overall, I am keeping my recommendation as rejection for the current version of the manuscript.  **Update after response of the authors**The authors partly corrected the points I mentioned, and I thank them for extensively addressing my comments. However I still believe that the paper oversells its results and analysis, although in a much less strong manner (e.g. in the last sentence of the abstract) and that many concerns remain. For instance, my concern on the tensor initialization has been partially addressed by the authors which showed that it performed as well as a random initialization close to the solution. The very small dimension (d = 5) for which these simulations were performed is however still a concern for verifying this analysis, which would need to be more rigorous and much more extensive to justify replacing the tensor initialization with the random one.As a consequence of all the changes made by the authors, I have raised my grade from 3 to 4 (and I would grade the current state of the paper between 4 and 5). **After rebuttal:**Thanks to the author for providing additional experimental data. But without the results of imagenet, it is difficult to judge the effectiveness of this method on complicated data. So I decided to keep the original score. #### After rebuttalQ1: Thanks for running the additional experiments. Unfortunately, the results are not strong enough to convince me to use the searched architecture instead of BiFPN. It will also be interesting to see how BiFPN works under similar FLOPs with the searched model (e.g., the structure of EfficientD4 or D3).Q2: Please do run the ResNeXt101-DCN experiments for the revision or next submission. These are critical to make people use the proposed method.Q3: True. However training from scratch requires 6x longer training time according to [1], and is considered as a drawback.Q4: Thank you for the clarification, this makes the contribution clearer. However my concerns on changing the backbone remains (Q3).Q5: Thank you for considering. I agree the ranks in the leaderboard is a main factor for design choice. This also highlight the importance of Q2.  =================Post-discussion:After reading the authors' response and the changes to the paper, I must unfortunately stick with my current score. I applaud the authors for taking my feedback on board, and certainly many changes have been made to improve the paper, but my main concern of novelty regrettably still remains. The paper offers a very simple improvement over Jung et al (effectively scaling the importance measure), which I believe is quite incremental in this setting. While I believe simple advances can often have broad impact (eg. dropout, batchnorm, etc), in this case it is not clear that the proposed change offers any benefits outside of the very specific area of importance-based continual learning. === Post rebuttal ===The inclusion of runtime comparisons with search based methods is a welcome addition, as it showcases that the method provides an interesting avenue to utilize GPUs for this kind of search task. Effectively trading up-front training time for deployment computational time. However, the question regarding the ability to handle larger state spaces is only touched on in the appendix and it is not clear from that description what the state space was. If the proposed approach works for planning in 6D environments or control of 7DoF manipulators then this is very interesting and should not be hidden away, so to speak, in the appendix but be highlighted in the main paper and showcased.In light of the considerable amount of work that has been put into the revision I changed my score from 3 to 4. While I could see improvements and clarifications I could not see the main concern I had, handling of high dimensional problems, being addressed. _Post-Rebuttal_:Unfortunately, the authors neither did update their paper nor addresses my comments. Therefore, I'm keeping my recommendation of rejection. -------------------UPDATE AFTER RESPONSEThanks for the response and the additional experiments. The comparison between ACE and these other techniques is nice to see, although I'll note that both SWAF and voting shouldn't make totally independent predictions in tasks like NER, but should at least respect constraints in the label space (not sure if there were applied or not).In the end, my opinion of this paper largely comes down to the practicality of this technique and its likelihood to be adopted more generally. This results in a large, complex model, and while I am now convinced that the authors have a better ensembling/combination technique than some others, I think it still falls short of a real "neural architecture search" contribution or a really exciting result. Post-rebuttalThanks the authors for the clarifications! I appreciate it. However, some of my concerns are not addressed. - The main concern I have is about the new insight on local update methods. Basically, the author obtain the insights based on a newly proposed algorithm (L2GD, let's call this algorithm B) and a new problem formulation (let's call this formulation B). However, they want to apply the insight from algorithm B and formulation B to algorithm A (original local update methods) and formulation A (original FL formulation). It is obvious that one cannot draw this conclusion because both the algorithm and the formulation are different.- Second, as I stated in the original review, I don't think one can obtain the insights from the analyses in this paper. The author didn't directly answer my question and just said "they didn't expect people to interpret it in this way". But it is still unclear how to correctly understand their insights.Based on the above two points, I strongly feel that their main insights about the effects of local updates should be further and carefully examined. The current version could be misleading. Besides, I also have the following minor concerns:- The authors claim that [Yu et al. ICML 2019] didn't consider the heterogeneous setting. This is not true. Although [Yu et al. ICML2019] assumes that the gradient dissimilarity is uniformly bounded (which is widely used in literature), their setting is still non-iid setting. It's unfair to say that they only study the IID data setting. So the second motivation of this paper does not make sense to me. The authors oversell their contribution. More precisely, their contribution is not the first proof under data heterogeneous setting but should be the new proof without data similarity assumption.- "non-local cousins" is unclear and hasn't been properly defined in the paper. For local SGD with mini-batch size ,  local steps and  clients, there are two non-local cousins: (a) SGD with mini-batch size ; and (b) SGD with mini-batch size . It seems that the authors misused these two algorithms. In the response, they agree that [Yu et al. ICML 2019] proves "with data dissimilarity assumption, local SGD can improve the communication complexity of classical SGD". Here, classical SGD refers to algorithm (a). In the updated paper, they cite two papers from Woodworth et al. to support their claim. However, the non-local methods in Woodworth et al. is algorithm (b). The authors should formally define which non-local algorithm they want to compare with.- In the paper, the authors claim that they prove for the first time local methods can improve the communication complexity of the non-local cousins. However, this statement is overselling. The more precise version is that they prove that the variance-reduced version of local methods can improve the communication complexity of the vanilla non-local version algorithms.- It seems that the authors want to claim a lot of contributions in this single paper and they didn't organize these contributions well. Hence, it causes difficulties for readers to understand their true novelty. I recommend the authors to rewrite the paper and carefully consider the paper structure. For example, if I understand correctly, the main contribution of this paper should be the insights on local updates. However, the authors didn't show any experiments on this insight in the main paper (they put them in the appendix). Instead, they just validate the effect of variance reduction in the main paper, which is just a minor point. Also, in the introduction, there is a long paragraph to introduce L2GD as one of the main contributions. However, as discussed in the responses, L2GD is not a new algorithm. The authors don't need to give it so much emphasize or should not claim it as one contribution.- Also, in [1] EASGD does use multiple local steps. The authors should compare L2GD with EASGD, as they both are designed to minimize the new formulation.##  ## POST-REBUTTALThank you for rebuttal which has helped me, to some extent, to understand your model. I still believe the formulation needs to be clarified before the contributions can be assessed objectively.For example, on page 2:> Given a realization of timestamps $\mathbf{t}_1, \mathbf{t}_2, \ldots ,\mathbf{t}_N$ with $\mathbf{t}_i \in [0, T]^D$ from an inhomogeneous (multi-dimensional) Poisson process with the intensity $\lambda$. Each $\mathbf{t}_i$ is the time of occurrence for the $i$-th event across $D$ processes and $T$ is the observation duration.This is confusing: only a single one of the dimensions will likely represent "time". Other dimensions will be space, etc. Furthermore, why would every dimension need to be within $[0, T]$? Different dimensions might use different units and be in different ranges.I am upgrading my score slightly, because I sense that your contributions might be interesting once properly explained, but in the present state I believe the paper is not ready for publication. #############################################################After author response: I thank the authors for their answers. However, as noted by other reviewers, experimental results and comparisons with related work are lacking, and I cannot increase my score without major changes to the paper.  ---**Final review**After reading the paper, other reviews, and author responses carefully again, I decided to remain on the rejection side because- I think the proposed dataset does not really guarantee the robustness against "real-world distribution shifts" because  - This paper did not rigorously define what the "real-world distribution shifts" are. In the final response, the authors mentioned that *"It is clear that temporal, hardware, geographic, and rendition shifts occur in the real world."*, but to me, it is not clear whether they are really common and representative in the real-world deployment scenario and really threaten deep models.  - Because the real-world distribution shift is not well-defined here, I feel the "robustness" is also ill-defined too. According to the author response, robustness is defined as the accuracy gap between "in-distributed" samples and "out-of-distributed" samples (not critical, but OOD is usually defined as the same data distribution, but unseen class. I think this terminology need to be polished). However, here OOD (distribution shift) is ill-defined, and the robustness test is heavily dependent on the test dataset.  - Thus, if we just test the "robustness against real-world distribution shift" on the proposed dataset only, it can lead to wrong conclusions, e.g., assume we have a model can be specifically better in a specific shift, e.g., rendition shift, but not generalized to other shifts, then ImageNet-R benchmark cannot measure how this model is vulnerable to the other shifts. It will confuse researchers in this field. Hence, I think this paper needs more justification for the new dataset (e.g., why the chosen shifts? why 200 classes for ImageNet-R? why different three datasets?), and need more human studies (e.g., humans can correctly classify the shifted images and non-shifted images) such as [3, 5, 7, 8].- This paper is not clearly presented. After reading the paper, I am still confusing about how to understand the experimental results. To me, the benchmark results cannot answer these questions well. I think R2 has a similar opinion on me in this criterion.- It is not mentioned in my previous reviews, so I lower the weights for this part to the final decision, but there are already some datasets benchmarking the dataset distribution shifts, e.g., PACS [9], NICO [10]. It may not be true that this kind of distribution shift is only measurable by the proposed datasets. But, as my first words, I noticed that I did not mention these datasets in my previous reviews, and these datasets will not affect my review a lot.  - https://domaingeneralization.github.io/  - http://nico.thumedialab.com/[8] Shankar, Vaishaal, et al. "Evaluating machine accuracy on imagenet." International Conference on Machine Learning. PMLR, 2020.[9] Li, Da, et al. "Deeper, broader and artier domain generalization." Proceedings of the IEEE international conference on computer vision. 2017.[10] He, Yue, Zheyan Shen, and Peng Cui. "Towards Non-IID Image Classification: A Dataset and Baselines." Pattern Recognition (2020): 107383.Of course, building a new dataset is a non-trivial effort, and measuring real-world robustness is not an easy task (maybe it even can be an impossible task). However, I think this paper can not clearly present how the proposed benchmark can solve the real-world distribution shifts and how can we move forward in the next directions.To sum up, I think this paper is okay, but not enough to be accepted to ICLR main conference paper. However, I will respect all decisions made by AC. **Response after author rebuttal period**I highly appreciate the detailed explanations and discussions the authors have provided during this period. It indeed clarifies my concerns and helps me better understand the setting and proposed solution. However, my major concern about the generalizability of the proposed solution still remines, as there are too many design choices depending on domain knowledge. I would keep my original recommendation; and if the paper could be accepted, I would like to encourage the authors to discuss the limitations of the proposed solution.   After the rebuttal.The authors partially addressed my concerns. I have read other reviewers' comments. I decide to remain the current score. ---I have considered the rebuttal provided. Particularly the aspect that data-augmentation would result in inconsistent supervision is an interesting point and experimental analysis of the same would be useful. However, I am not convinced that the paper provides a broad enough solution. In view of this I raise my score from 3 to 4, but maintain my view that the paper is presently not good enough for acceptance. ---**Post-rebuttal update**My main concerns in the initial review were three-folds:- Potential flaws in the analyses based on VAE and adversarial attacks- Unclear connection between the MI analysis and the proposed method- Small performance gap, and even sometimes worse performance, compared to the baseline methods (Mixup, CutMix)After having discussions with the authors, I will keep my initial score because:- I am still confused about the MI-based analysis conclusion. The authors mentioned *"We make no claim that increasing or decreasing the mutual information measure will have a strong impact on performance. Instead, we contend that MixUp works by forcing the model to ignore sample specific features (thus learning compressed representations  the reason for discussing the information bottleneck theory) and that CutMix works by mimicking the real data whilst preventing example memorization."* in the rebuttal, but these two conclusions are not trivial to me (by the MI analysis).- Even if we ignore the first part, my second concern still remains. The authors mentioned *"That is the problem FMix tries to solve by removing the horizontal and vertical edge artefacts from cutmix. Our belief is that cutmix biases models towards these edges as they are a guaranteed feature of the data and learning about them would reduce the loss since these edges can tell you how much of each source image is present in the input (a key part of the objective)."*. But if this paper assumes that the rectangle masking strategy of CutMix makes bias, then I think other CutMix variants such as AttentiveCutMix or PuzzleMix should be considered as the comparison methods. Hence, I disagree with this statement *"A comparison to masks generated using additional models (and, thus, significant additional computation) does not seem fair to us."*- For my last concern, the small performance gap, the authors claimed that this method *"was also used by the second place team in the BengaliAI Kaggle competition"*. It is good evidence that FMix can sometimes offer benefit to real-world applications, but I think more evidence that FMix can really solve problems of previous MSDA in a certain scenario, e.g., the edge bias as pointed by the authors. POST-REBUTTAL COMMENTS========I would like to thank the authors for their response. The authors have clarified the method in their response. I appreciate all the experimental details in the appendix. I tend to agree this is a promising idea and worth explored.However, this paper clearly cannot be accepted in its current form. The paper is poorly organized and poorly written. The method in the paper needs to be clarified. Their theory goes nowhere and proves nothing. In terms of the experimental results, the authors choose unclear baselines (which they claim state-of-the-art) and report improvement in terms of percentage increase (percentage over percentage). None of this is convincing to me.I slightly increased my score. == Update after author response ==Thanks for your response. I believe it is widely agreed upon that black box evaluation is not meaningful in security settings, and that we should use white box attacks. Therefore, I don't find the justification for omitting PGD and CW convincing. I also still do not feel that counting images in pixel space is a meaningful metric. Therefore, I have kept my original score. -------------- Post-Rebuttal Comments -----------------Thanks to the authors for their response, and for updating the manuscript. Some of my queries were clarified. However, updated Theorem 1 seems to raise more questions. In particular, Assumption 4 looks very restrictive to me. If adversaries manage to produce large values of \Gamma, they can inflict a large error as per (7). The paragraph after Theorem 1 does not mention how entropy and loss based filtering methods can achieve small \Omega_{max}, but only says that "if the entropy-based filtering method successfully filters out the model poisoned devices, and the loss-weights \beta_i(k) of the adversaries are significantly small for data poisoning and backdoor attacks... ... then we have a small error term". It is not clear what guarantees the filtering schemes yield. Due to these reasons, I still think the paper is not yet ready for publication. ******** After Rebuttal ************I carefully read the authors' response and unfortunately they do not address my concerns. Based on my research background in adversarial robustness and uncertainty estimates, I would keep my original rating unchanged as this work has very limited contribution to these two areas. In the rebuttal the authors have argued that their method studies a different problem from that of unsupervised video segmentation, and thus a comparison to those methods is unnecessary. Moreover, they mentioned that no established metrics exist for evaluating the degree of disentanglement of a representation, thus no new experiments can or should be added to the paper. I can see that there is a difference between image-based scene decomposition and unsupervised video segmentation. Once you move to video-based methods, however, the difference start to elude me. At least in the classical works, such as Brox and Malik, ECCV'10, the problem is defined in exactly the same way - decomposing a video into object/background regions in a fully unsupervised way. The only difference I can see is that in those works decomposition was the end goal, whereas this paper attempts to use it as a surrogate task for representation learning. This would be a valuable contribution if the authors could show that the resulting representations are superior to those learned with other unsupervised objectives (say, contrastive learning) at least for some tasks (say, object detection). Unfortunately, such evaluation is missing from the paper, thus I still find find that the benefits of the proposed approach are not convincingly demonstrated. Updates after Reading Authors' Response:Thank you for the detailed clarification and the new results. From the response, it seems that the proposed modifications to SAC do not and are not meant to solve the exploration problem in sparse-reward tasks. Instead, its aim is to improve sample efficiency on standard dense-reward tasks. The impact of the work then feels quite limited: the proposed modifications are specific to SAC and do not meaningfully improve performance on the sparse-reward task. For these reasons, I will keep my original evaluation. *Post-rebuttal*All reviewers agree that this paper is not up to the mark. While the revision does include several additional related works, they are not very well integrated with the rest of the discussion on the paper. For example, how would some of these memory networks perform? How would Neural Turing Machine do? Considering this, I am hesitant to improve my rating for the paper, even if the collection of related works will certainly help in the re-submission. Update 1: I appreciate the authors' response. I think including the "on-going research" points as the authors brought up  and improving the clarity will greatly strengthen the submission. I still think the current form is not ready yet and would like to keep my score as is. ### UPDATE:Per my response in the thread, I appreciate the authors' replies and updates, but I am keeping my score because- Even with the new 2-layer results, I find this experimental setting still too limited;- Even within the conducted experimental setting, the benefit of likelihood-guided initialization over He init is not robust (notably on small dataset sizes, which is contrary to the expectation that a good prior will be more beneficial in such cases). UPDATE AFTER REBUTTAL:I would like to thank the authors for their responses. After reading the updated version I still would recommend to reject the paper. The reason is that the paper is written for a very narrow audience and is hard to understand for readers who are not familiar with this area of research. Also I  feel like some of my concerns where not properly addressed in the updated version (e.g., issue 2 and 6). After reading the authors' response, I still think that the novelty of this paper is very limited. I decided to keep my score at 4. --------------------------POST DISCUSSION UPDATEThe central part of this work about how the extra network weights only affect the curvature still confuses me. But I'm more motivated by the proposed objective function that borrows idea from non-Bayesian robust learning literature. #################Post-Rebuttal Reviews: Thank the authors for the detailed responses. The proposed approach is an interesting add-up for the Laplacian approximations. However, I think the paper still deserves more works. As far as I am concerned, applying the approach to multi-layers instead of only the output-layer is important. I will keep my score for now.################## **Final update:** The main criterion used by the authors to search invariant predictors is not correct and is in fact not satisfied by the invariant predictors. For this my suggestion to authors is to modify their criterion in a way that it is at least satisfied by the ideal model you want to learn. ---Post rebuttal---Thank you very much for the response, and I understood that some of the concerns I raised can be resolved empirically under appropriate conditions.   After the response, however, I think I am still comfortable with the original score, because this is (as I understood it) a theoretical paper and I felt that it is necessary to make a judge based on theoretical solidness.  **After rebuttal**I'd like to thank authors for their efforts to address my concerns. I didn't change my initial rating, due to the two main concerns below:(1) To me, the main argument of this paper sounds "when a large (and maybe diverse) OOD is given, adding an OOD class to the classifier is better than baselines." Since the large OOD setting has already been proposed by [Hendrycks et al.], the only contribution of this work is on the empirical observation that the proposed method is better than baselines. While the observation is interesting, I think the contribution is not enough as a full ICLR paper at this point.During the rebuttal period, R3 corrected it that "the main question investigated by the paper is how to best use the outlier exposure set," and this sounds better. However, authors didn't emphasize the setting but their method, such that their main argument is (if they intended to say as like what R3 understood) misleading. Training with a large OOD dataset like [Hendrycks et al.] is not common, and the observation in this paper is limited to this setting. However, the only statement about the setting I could find in the intro is that "as in Hendrycks et al. (2018), we uses additional samples of real images and text from non-overlapping categories to train the model to abstain, ..." i.e., rather than elaborating/emphasizing the setting (together with their method), they just cited a prior work.In short, I recommend authors to rewrite abstract/intro as suggested by R3, to properly emphasize their contribution.(2) The comparison is unfair, as authors didn't re-evaluate baselines in the same setting (they had to make it the same as much as possible) but just pasted numbers from original papers. Even the comparison with the closest prior work [Hendrycks et al.] is unfair, as the prior work fine-tuned the model while the proposed method trained the model from scratch.Regarding the performance of similar methods evaluated in [Lee et al. (a)] and [Dhamija et al.], I think the main reason why they didn't get the same observation is on the size of OOD dataset, i.e., they didn't train their model with a large OOD dataset like [Hendrycks et al.] or this work. As this work claims, it might be true that when a large OOD dataset is available, adding an OOD class to the classifier is simply good enough. [Post-rebuttal]I have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4's concern on the significance of results is valid. Therefore, I will keep my initial justification as is.  ###### Post-Revision ########################Thank you for revising the paper and addressing the reviewers' concerns. The updated version reads much better and I have updated my score. Unfortunately, I still think that the experimental analysis is not enough to warrant acceptance. I would encourage the authors to have a more detailed set of experiments to showcase the effectiveness of their method and have ablation studies to disentangle the effects of the different moving parts. ###### Post-Revision ######################## ------After rebuttal:I gave detailed responses to each part of the rebuttal below. Here is the summary:Although the response addresses some of my concerns. There are still major issues with the experimental study. 1) there are existing, relevant and well-studied multi-task setups with negative interference. Method should be experimented with some of those setups. 2) Multi-task baseline in the paper is naive and far from state-of-the-art. Paper need strong baselines as discussed. Hence, I am keeping my score. Paper needs to be improved with a stronger experimental study and need to be re-submitted. 2. "DefenseGAN is broken"You are right about DefenseGAN not being broken.3. The evaluation methods used in our work are standard methods which are widely used in all other previous adversarial defense works. FGSM and PGD are indeed widely used, but many previously proposed defences used additional attacks (like transfer-based, score-based, decision-based). Please check https://arxiv.org/pdf/1802.05666.pdf for an in-depth discussion of this issue.4. "Gradient masking"You are right that the gradient masking effects visible in the graybox attack doesn't necessarily indicate gradient masking in the white-box setting (but still means that hyper parameters of the attack have not been tuned properly).Given the discussion I will increase my score by one point, but the lack of a reliable robustness evaluation and the reduced novelty compared to DefenseGAN still puts it below the acceptance threshold in my opinion. ===============After Reading Authors' Response ================The reviewer would like to thank the authors for their detailed response and careful revision of the paper to address the reviewer's concern. However, the reviewer is not persuaded by the authors' response. Specifically,1. the reviewer is not satisfied with the explanation and modification to address the scalability issue stemming from both the cause-specific subnetworks and the output layer. Simplifying the structure and parameterization of cause-specific subnetworks when many are present seems like a comprise rather than a principled approach to address the issue. The same is true for the exponentially distributed parameterization of the output layer.2. It is the reviewer's impression that for point process neural networks, it is possible to use the covariate information for prediction, as opposed to the claim given by the author. ----Post-rebuttal reviewI appreciate the authors' efforts in clarifying some of my concerns. However, I am still not convinced the comparison has been made fair. Many numbers from Table 1, such as ZOO, Opt-attack, QL-attack and AutoZOOM seem to be directly adapted from the papers rather than implemented and reproduced based on the same setting as the proposed attack. In particular, given that QL-attack is a published work, one of the state-of-the-art method and its codes has been released, I would really love to see a direct comparison using the same data samples and threat model. I would also like to emphasize that implementing all attacks under the same setting is crucial, since different attack methods may have a different criterion to determine attack successfulness. For example, QL-attack has some pre-defined distortion (L2 or Linfinity) for determining an adversarial example is successful, in addition to a different predicted class. **Update to review**Thanks to the authors for responding. They did clear up point 5 (above) for me. However, I shall keep my score of 4. Unfortunately I cannot see the new revision of the paper that the authors refer to, meaning I cannot change my score.  update: Thanks for the response. However, there is no updated revision in the revision history of this paper. Based on the flaws that I have previously pointed out, it is impossible for me to validate if my concerns were actually adequately addressed without seeing the updated version. I will keep my score unchanged.  --REVISION--I thank the authors for their rebuttal and clarifications on the threat model and end goals of their attacks. I remain somewhat unconvinced by the usefulness of extracting architectural information. For most of the listed attacks (e.g., building substitute models for adversarial examples, or simply for model extraction) it is not clear from prior work that knowledge of the architecture is really necessary, although it is of course always helpful to have this knowledge. As I mentioned in my review, with current (undefended) ML libraries, it should be possible to extract much more information (e.g., layer weights) using cache side channels. I can see what you're trying to do with Eq (6) after the change but if it's related to the conditional GAN and GAN+class, it's better to cite them and discuss the differences/similarities in the paper. With additional experiments, the evaluation seems to be more complete. However, the results in Table 1 and Table 2 do not show consistent improvements of the proposed models.  For example, if we only look at the human evaluation, it's not clear why phredGAN_a has the highest score among hredGAN, phred, phredGAN_a, phredGAN_d for TV datasets but has the lowest for UDC dataset. It's better if we can see more concrete analysis. I increase my score because of more complete evaluation. However, due to the overall limited novelty/improvements and unclear presentation, the paper is still under my expectation for ICLR. #update: I've read the authors comments but unfortunately my main concerns about the contributions and novelty of this work are not answered. As such, I cannot increase my score.------------------  Extended review of Update 17 Nov:I would like to note that I liked the fact that you used several optimization algorithms in your comparison. To my best understanding, several algorithms shown in Figure 3 (e.g., BOBYQA and L-BFGS) would benefit from restarts and it is fairly common to use restarts when the computational budget allows it (it seems to be the case for Figure 3).The results shown in Figure 4 are hard to trust because it does not seem that we observe mean/median results but probably a single run where the results after 1 iteration are drastically different for different algorithms. For instance, after one iteration BOBYQA only tests its second DOE point. Here, again, the issue is that one iteration for BOBYQA is 1 function evaluation while it is several (10) function evaluations for other algorithms. In that scenario, it would be more fair to run BOBYQA with 10 different initializations as well. I don't understand "Due to the restriction of PyBOBYQA API, we can only provide the function evaluation of the final solution obtained by BOBYQA as a flatline in Figure 4". At the point when your objective function (which is not part of PyBOBYQA API) is called, I suppose you can log everything you need. # Post-rebuttal RecommendationThanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I  also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result.Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., "de facto standard in RL"), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work.As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of  just observing  / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience.  This might also require adding a third game to ensure more generalizable experimental insights. ### UPDATE AFTER THE REBUTTALMany thanks to the authors for their reply, and my sincere apologies for forgetting to include my bibliography in the earlier response (you can find it above)! I also appreciate the information about the comparison to [1], as well as the promise of a clarifying statement about the appropriate experimental context for the use of quantum phase estimation.While I agree that the comparison to previous QNN models is favorable, I still think that the use of quantum phase estimation in the proposed method is problematic, as it would significantly delay any potential deployment of the proposed method on a quantum computer. I don't disagree with the authors that their statement regarding the ability to demonstrate quantum supremacy is technically correct, but they haven't addressed my question about _why_ there would be any advantage to carrying out a quantum supremacy experiment on such a model. Studying QNNs for the sake of QNNs isn't a compelling justification in my eyes, and without any strong practical advantages (either definite or potential) provided for the model, my rating unfortunately remains the same. As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.* Different motivation/derivation of NES.* Concept of adversarial distributions.* Regression network for good initialization.* Introduction of accuracy-iterations plots. ---Thank you for responding to my comments and updating the paper. I have slightly raised my score to reflect this effort.There are new claims in the results section that do not seem to be warranted given the human evaluation. The claim is that the human evaluation results validate the use of the automatic metrics. The new human evaluation results show that the proposed abstractive model performs on par with the extractive model in terms of conveying the overall sentiment and information (Table 2), whereas it substantially outperforms the extractive model on the automatic measures (Table 1). This seems to be evidence that the automatic measures do not correlate with human judgments, and should not be used as evaluation measures.I am also glad that the title was changed to reflect the scope of the experiments. I would now suggest comparing against previous work in opinion summarization which do not assume gold-standard summaries for training. Here are two representative papers:Ganesan et al. Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions. COLING 2010.Carenini et al. Multi-Document Summarization of Evaluative Text. Computational Intellgience 2012. I have read the discussion from the authors. my evaluation stays the same.-------- Comments After rebuttal==========Thank you for adress my concerns.The response and the revision resolved my concern (1). However, the most important part, the possibly problematic loss is not resolved. It is true that sometimes (10) can achieve good results with good regularizers or a good set of hyperparameters. However, theoretically, the loss is ]only pushed down the desired answer, which may make the training procedure quite unstable. Thus I still think that a different loss should be used here.  ---------------------------------------------Thanks for the update. But are they fair comparisons (evaluation only in terms of accuracy)? Different methods expand the network different amount. Hence, they should be compared on this metric too. **After Rebuttal** I would like to thank the authors for their rebuttal. I agree that it is not fair to assess the merits of the current work based on papers that were not available at the time of submission (or that, strictly speaking, have not been published at the time of submission). Indeed, to an extent, pointing out the ArXiv paper encourages authors to simply submit their works there to get a "publication" stamp, which on a community level is undesirable (papers on ArXiv aren't reviewed and citing them as scientific sources is problematic to say the least). I suppose the only point is that there exist works that do similar things in a more compelling fashion. It's encouraging to see that the authors checked for robustness of their method, and I appreciate the efforts. With these two issues resolved to a certain extent, I am willing to increase my score.  **Update after author response:** I appreciate the authors' efforts to address my concerns. Thanks for the correction on QBC, I appreciate it. I still believe that the paper needs to accompany a more comprehensive evaluation and qualitative insights to highlight the effectiveness of the proposed method. For instance, it is common practice in Active Learning to report mean accuracy over multiple runs of the same experiment as data is sampled based on a particular heuristic which isn't always deterministic. Furthermore, the choice of warmstart samples could also influence the results, which is why it is recommended to conduct multiple runs of the same experiments and report mean performance. In such a scenario, any claims that arise from only one run of the experiments (as in this paper) should be taken with a grain of salt. I also appreciate the pointer to Equation 4 but how does it translate in practice is another important piece that is missing. BERT based models produce highly confident predictions and to visualize the distribution from your empirical investigation would help bridge the divide between the equations and the empirical results (how they actually turn out in practice). I am also not convinced by the authors' response to why the difference in performance of the BERT model trained on randomly sampled data vs the model trained on data sampled via ALBUS stays within the 1-2% range, often increasing with more data. I believe there are critical questions regarding this paper that need to be addressed before the paper is published, hence, my score remains unchanged.--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## Post-rebuttal commentsThanks to the authors for the comments.  I will leave my review unchanged for this one.> The term suspicious: we agree with the reviewer that the word suspicious is an informal and subjective term, and there is no universal notion of what it means for adversarial examples to be unsuspicious. In fact, the core contribution of our paper is to address this issue.To be clear, what I mean is that you should choose a *different* term than "suspicious".  While it may not sound as good to use a more specific technical term describing your actual measure that you are introducing, it will prevent the inevitable confusion that arises when an informal, vague term is overloaded to have a specific, quantitative definition. ### UpdateI have read the author's rebuttal and the comments of the other reviewers. Sadly, the comments of the other reviewers seem to align with my own thoughts and the rebuttal doesn't resolve those questions. I would like to maintain my initial rating of 4. More generally using the same model to measure saliency as well as to construct adversarial attacks is not properly justified and secondly, I am not convinced that putting a large perturbation on the "background" as determined as DeepGaze  is a proper side-by-side comparison with AT where AT is allowed  much smaller perturbation. ---post-discussion update---I greatly appreciate the authors' effort to actively attend the discussion. In general, I like the idea to leverage the graph sparsification technique to accelerate GNN training. However, I think there are still several fundamental questions that should be well addressed before this paper get accepted.  First, I do not see clear reasons to emphasize a specific model GAT. One concern arises during the discussion, where the sparsification technique here only works for GAT instead of GCN or other GNN smoothing models, which brings me some concerns about the technique. Second, the fundamental difference between this method and GDC [1] is still not clear. Both methods emphasize the low-frequent section of the graph connection. Why does the sparsification method here work while GDC does not work, though the sparsification method here can also be viewed as a type of graph diffusion (GDC)? Post Rebuttal Update:I have read the author's rebuttal, and appreciate the additional experiments. I will maintain my score due to the importance of extensive evaluation for showing empirical robustness, and a lack of proper baselines. In particular, I think that some version of "adversarial training with coarse labels" should be better than standard adversarial training in terms of hierarchical robustness; only after that can we evaluate the benefits of HAR in comparison to "adversarial training with coarse labels." ======== after discussion phase ==========I still think that the merit of the method is unclear due to reasons: 1) It is not clear how the method behaves without Lipschitz Hessian assumption. 2) The method only obtains the state-of-the-art complexity of $\epsilon^{-3.5}$ with large mini-batch sizes and the complexity with small mini-batch sizes (section 2.1) is suboptimal (in fact drawbacks such as this needs to be presented explicitly, right now I do not see enough discussions about this.). 3) Adaptive variance reduction property claimed by the authors boils down to picking "small enough" $\beta$ parameter, which in my opinion takes away the adaptivity claim and is for example not the case in adaptive methods such as AdaGrad. 4) The comparison with AdaGrad and Adam with scalar step sizes is not included (the authors promised to include it later, but I cannot make a decision about these without seeing results) and I am not sure if Adam+ will bring benefits over them. 5) Presentation of the paper needs major improvements. I recommend making the remarks after Lemma 1 and theorems clearer, by writing down exact expressions and the implications of these (for example remarks such as "As the algorithm converges with $\mathbb{E}[\|\nabla F(w_)\|^2]$ and $\beta$ decreases to zero, the variance of $z_t$ will also decrease" can be made more rigorous and clearer, by writing down exactly the bound for the variance of $z_t$ by iterating the recursion written with $\mathbb{E}\delta_{t+1}$ and highlighting what each term does in the bound. This way will be much easier for readers to understand your paper).Therefore, I am keeping my score. #####################update:  I have read authors' response to my comments and also read other reviewers' comments and discussions.  The main concern of my comments is still not clear. I will keep my rating unchanged.  ##########################################################################Comments after rebuttal period: I appreciate the authors' efforts on increasing the clarity of this paper. However, the answers to "How do you order those m clusters?" are not convincing (as well as the responses to R4's 2nd concern, which mentions the same problem). It is a key point in generalization across different graphs. According to the experimental results, I believe there is an implicit mechanism in the proposed method that achieves the ordering, but it is not clearly explained and analyzed. I will keep my score. Post-Rebuttal:I would like to thank the authors for their rebuttal.The updated version of the paper has addressed some of my comments.However, I still fail to understand why the method should 1) converge in general, and 2) converge to a good solution. I have updated my score accordingly.===================================================================================================== Post-Rebuttal Evaluation [FINAL] I would like to thank the authors for their response and for updating their paper based on the reviewers feedback. Following these updates, I'm changing my recommendation to Rejection for the following 2 reasons: 1- the technical novelty of this paper is moderate and there's a significant gap in the model performance as compared to state of the art, 2- the authors failed to provide convincing answers to many of the reviewers concerns, including motivation for not using semantic embeddings during the training process, and not comparing their approach to  transductive ZSL ones which achieve a higher performance. =====Update after rebuttal=====I have read the authors' rebuttal. However, my concerns are not well addressed. 1. There are a lot mask based methods for interpretation in different domains [1] [2] [3] [4]. Existing methods [1][2][3] are providing post-doc explanations for a pretrained model. I still believe "the connection between the proposed method and data compression is not convincing". 2. I still believe the novelty is limited. Hence, I am keeping my score unchanged. [1] GNNExplainer: Generating Explanations for Graph Neural Networks, NIPS 2019[2] Real Time Image Saliency for Black Box Classifiers, NIPS 2017[3] Learning to Explain: An Information-Theoretic Perspective on Model Interpretation, ICML 2018[4] Rationalizing Neural Predictions, EMNLP 2016 **Update after rebuttal:** I appreciate the detailed responses by the authors. I'm willing to increase my score based on the responses, but unfortunately I'm still not ready to recommend acceptance. In my opinion, the paper is simply not mature enough yet for publication (the significant amount of revisions required during the rebuttal period attests to this, I think; a mature conference paper should not have to require this much revision during review). In particular, the following fundamental issues still remain for me even after the revisions: 1. The misleading language about "temporal smoothness" in real-word data remains throughout the paper despite the fact that the paper doesn't address temporal smoothness as it exists in real-world data.2. The authors promise some new experiments on more realistic stimuli, but as it stands the paper still only includes experiments on static images with mostly toy data and I have no way of knowing whether any of their results would generalize to more realistic data. The experiments with multi-scale stimuli suggest that that generalization may be non-trivial (e.g. in that experiment, the baseline model with no memory or gating mechanisms actually performs the best).3. Which brings me to my final point: I still don't think the authors have adequately explained why and how the proposed mechanisms work. For example, the authors say: *"Our working hypothesis is that averaging across multiple members of the same category increases (in some datasets) the proportion of variance in the hidden units that is associated with category-diagnostic features."* Why the hedging *in some datasets*? The experiments with multi-scale stimuli clearly demonstrate that the proposed scheme doesn't work in all cases, but what exactly are the conditions under which it would work better than the baseline model? The authors need to make these a lot clearer.------------------------------------------ After reading author replies:I would like to thank the authors to respond to my doubts on some of the results. But I decide to keep the review and the score, because Theorem 1 and Claim 1 are still not well explained. In particular, the explanation like "if the 2nd inequality in Eq. 11 is violated, the network can not capture the amount of information measured by the entanglement entropy " still looks like a conjecture or intuition rather than a mathematical statement. --------------------------------------------- ---I appreciate the thoughtful rebuttal provided by the authors. My main concerns are on Q2, i.e., the practical usefulness of the algorithm. I do think that the authors provide a convincing argument on "we can only understand what we can understand," hence we should set up a hypothesis and see if it aligns with the explanation. However, I think the usefulness is not well-supported in the current state of the paper. The authors can come up with (a) a stronger example of such a hypothesis and (b) a better measurement of how the hypothesis aligns with the explanation to strengthen the paper. Regarding Q3, I still think that it is not correct to provide the same explanation for different algorithms when they produce the same output. Hence, the proposed algorithm should be modified to consider this aspect.  ##########################################################################Comments after the rebuttal period: I will keep my score. The authors' responses addresses my first concern. However, the differences from previous studies are still not significant to me. In addition, no matter the authors claim the proposal of a complete architecture or a new operator, ablation studies are necessary to backup the designing of each part. Post-rebuttal update---------Thank you for your response.  Now I understand that the algorithm works by smoothing the Gaussian parameters $\mu_i,\sigma_i$ w.r.t. the centered Gaussian rv (as described in my last reply, second part of bullet point (1)), so my original concern regarding the bias _in the Gaussian parameters_ does not hold.  However, I still cannot recommend acceptance at this point, because of a newly discovered issue in the theoretical analysis:The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream'' nonlinear layers.  The text only considers two layers of stochastic latents and the KL part of ELBO, but in the deeper case, the smoothing of $\mu_i(z_{i+1})$ will additionally have influence on the layers below $i$, through the nonlinear functions $\mu_{i'},\sigma_{i'}$ for $i'<i$.  More concretely, consider the following scenario: $\mu_i(z_{i+1})\equiv z_{i+1}, \sigma_i(z_{i+1})\equiv \epsilon$ which is very small. Further assume that $z_{i+1}$ is high-dimensional and approximately follows $\mathcal{N}(0,I)$, so $\\|z_i\\|_2 = \\|\mu_i(z_{i+1})+\sigma_i\varepsilon_i\\|_2 \approx \\| z_{i+1} \\|_2 > 100$ with probability $1-\epsilon_1$, where $\epsilon_1$ is also very small. In this case, it is possible to achieve a low KL in the original ELBO, by using a $\mu_{i-1}$ which only has sensible values in the region $B := \\{z_i: \\|z_i\\|>100\\}$; in the complement set $B^c$, $\mu_{i-1}$ can be "arbitrarily" bad so long as its impact on the ELBO does not outweigh $\epsilon_1$, the probability its input falls there. However, in the smoothed estimator with $\rho=0$, the input to $\mu_{i-1}$ only have norm $O_p(\sigma_i(z_{i+1}))=O_p(\epsilon)$, so the value of $\mu_{i-1}$ on $B^c$ will have a far higher impact, easily exceeding the original by $O(1/\epsilon_1)$.  To summarize, *it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically*.  Moreover, even in the shallow case, $z_i$ will be fed into a final decoder block to generate the reconstruction image, so a similar issue exists, although it will be in the reconstruction likelihood part of the ELBO as opposed to the KL part.A less important issue is that parts of the analysis are written in a confusing way.  Apart from the abuse of notation $U_\rho$ which leads to my original confusion, in Section 3 the $\hat{\mu}_p$'s should have a suffix of $z_1$, to signify the fact that they are coefficients of a function that depends on $z_1$ (see the last response from he authors).  Also it is unclear to me why there is no mention of $\mu_p^4$, in the analysis of the variance of an estimator for $\mu_p^2$.  But given the aforementioned issue, I don't think it is necessary to look further into this case. ========= Post-rebuttal update:I thank the authors for carefully addressing my comments, as well as other reviewers'.Ultimately, this is a nice paper with a novel recurrent component, and I can see how it could perform well in practice.However, the lack of stronger real-world experimentation (on datasets such as OGB) unfortunately renders the contribution insufficient -- the synthetic benchmarks being insufficient on their own to pull the weight of the paper.I retain my score, but encourage the authors to carefully revise and resubmit for the next venue should the paper be rejected. --- EDIT POST REBUTTAL ---I thank the authors for their answer and their efforts in editing the submission. I have also read other reviews and replies. However, my stance on the paper did not really change as I find the contribution insufficient for acceptance. PS: when I wrote "Formulation 1 uses explicitly the proportion of outliers", I was referring to the display equation above eq. (2.1). I did not realize the term "formulation" was already formally used in the paper to refer to another equation. Update after the rebuttal:I read other reviews and response from the authors and I decided to keep my score. Overall, I think paper still needs morework. For example, incorporating details on confidence into the main paper and not just a section in the appendix is quiteimportant as otherwise the paper is misleading.================================================== ---post-discussion update----I would like to thank the authors for preparing the rebuttal and attending our discussion. However, I still think the complexity is a concern of this work. I do not think that Eq. (3) can be implemented within the complexity that the authors claimed. Moreover, if the authors use another way to compute the attention scores, that way should be very clearly stated instead of written in a different form. Given the high complexity, I cannot clearly see the advantage of this work in comparison to [1], as the non-local attention has been proposed in [1] already. Post-rebuttal ------------------I appreciate the response and the manuscript update, but some of the comments have not been addressed such as the code which has not been provided, and the fact that the results are still very inconsistent like R2 mentioned although the arguments made in this work seem clearer to me. But it is still not clear to me how the results can be used to improve current methods for identifying adversarial examples with smaller distortion and less queries. Thus, I will retain my score as is.  **Post-rebuttal**After reading the rebuttal and the other reviews, my rating remains the same, although the rebuttal addresses some issues. The added videos show good temporal coherence as previous methods, and several writing issues have been resolved. However, I still feel that the contribution of the proposed method is not significant enough. It would be more convincing if the paper can show that the proposed augmentation technique is effective for more applications. Also, I still think that the visual improvement is very subtle. For the six added videos, only fig_a2_label.mp4 shows clear improvement visually.  === Post Rebuttal Responses ===I thank the authors for their replies and the corresponding new revision. After reading them and the other reviews carefully, I changed the rating accordingly. However, I still lean to reject this paper for the following reasons.1. When saying "our method outperforms something," the standard deviation and the mean should be considered (such as confidence interval or statistical test). However, the authors seem only to consider the mean in the paper and the other replies. In this sense, the proposed method does not outperform CQL. Besides, the variances of CQL in halfcheetah-mixed and hopper-mixed are required to conclude that the proposed method outperforms CQL in the mixed setting (e.g., the variance of the proposed method in 'hopper-mixed' is high).2. I agree with the other reviewers common concerns on the novelty / main benefit of this paper.3. The supplementary code can be shared with the reviewers using an anonymous link, as explained in ICLR Author Guide. It would have been better if the authors used this functionality of official comments during the review process for better reproducibility. Update: Thanks to the authors for addressing my comments and releasing the source code. Code is well-structured and easy to follow. It can be definitely used as a supplement for the robustness evaluation. However, judging by the implementation, the rain, snow, and fog attacks are too simplistic. From the paper, it is not clear how well these attacks approximate the respective type of perturbations in real-world scenarios. Therefore, the proposed set of 6 attacks is more the author's heuristic than something that can be used for the evaluation of computer vision systems, e.g. autonomous vehicles. Given all that, I decrease my score from 5 to 4. **Response to the last comments made by the authors**The referenced paper (https://arxiv.org/pdf/1906.04933.pdf ) is not answering the question I have. Nor does section 4.2.My question is: does ALS really improve the quality of uncertainty?The paper's answer seems to be no.Look at Tables 1, 2, 3, and 4 in the revised paper. It is great that ALS achieves a lower O.conf (overconfidence), meaning that for wrong predictions, ALS helps to produce lower confidence values. However, this comes at the cost of higher U.conf (underconfidence), meaning that even for correct predictions, ALS makes the model produce low confidence values. A confidence measure that always produces a low value, regardless of whether the prediction was correct or not, is not useful.The authors may say "look at Table 1 - our uncertainty measure produces lower scores for images with objects removed". While this is true and it is a good signal, object removal is only a particular case for introducing uncertainty in an image. Eventually, I believe a good uncertainty measure should first of all produce a good ranking of test images such that the correctly predicted images are ranked first (this paper does not quantify this). Then comes the question of calibration (like ECE and MCE). Then comes the evaluations with specific uncertainty scenarios (like OOD images, object removal, or other controls on image uncertainty). ----------------------------------------------------------------------------------------------------------Post rebuttal:Thank you for your response. After reading the other reviewers' comments and your responses, I think the paper is not yet ready for publication. All reviewers are concerned by the lack of a technical contribution and the limited benefit of the paper. While I appreciate the effort of the authors to answer our concerns, I think the paper needs a major revision that incorporates our shared concerns. Therefore, I retain my initial rating. -----------------Post-rebuttal review:I carefully read through the rebuttal and other reviews and  I would stick to my current rating.1. Cons 1: The rebuttal does not provide sufficient explanation on how the magic numbers are chosen (though it is somewhat explained in the rebuttal, it looks more like a design and lacks experiments to back it up: why these numbers but not other numbers?)2. Cons 2: I would suggest the author(s) further improve the evaluation metrics to make it more objective and convincing.3. Cons 3: I believe the 2.68m difference in detecting cars should be regarded as a very large error (under IoU 0.5 metric, it would be counted as a misdetection) and should be handled properly.I think the approach presented in this paper is interesting, and I encourage the author(s) to do more analysis to make it more solid. Update after rebuttal: I thank the authors for their responses to all my questions. They satisfactorily answer some of my concerns. However, I still have two major concerns: 1) the faithfulness of the proposed approach, and 2) I see the potential contribution of uncertainty saliency maps but without an application/evaluation, their significance is unclear. I disagree that uncertainty/confidence generated from the same mechanism that generated the explanation is more trustworthy than the explanation itself. Hence, I cannot recommend the paper for acceptance. # Post RebuttalI thank the authors for replying to my question. Unfortunately, the authors did not present any new results on other benchmarks (e.g Nasbench101-shot).  I will therefore keep my score. #Post rebuttalI appreciate the authors added a deeper discussion and analysis in their newest uploaded draft, which helps clarify some of my concerns. However, it would be better if the following can be better addressed:- As a benchmark paper the experimental setting is relatively simple/artificial, more realistic datasets/settings would be more useful;- Thanks for adding the stability study. It would be more helpful if the main results can be presented with a better control over the stability. Currently it is still hard to tell the statistical significance of the results.- As the other reviewers have mentioned, it is still unclear how this simulated setting connects to real-world fairness applications. -----------After the rebuttal: I would like to thank the authors for their response and clarification. The argument that the proposed strategy minimizes an upper bound is, however, somewhat weak, because $\|B(h - y)\|\leq \|B\| \|h-y\| $ is a possible upper bound for any matrix $B$, but using the right-hand side as a replacement for $\|B(y-y)\|$ within an optimization problem can make a large difference. I would expect to at least verify a marginal difference in the results numerically. Moreover, I do not think that the argument that some dictionary learning algorithms are slow is valid for omitting a comparison to a simple baseline denoiser at all: Your denoising baseline could be as simple as thresholding DCT coefficients or applying a median filter - operations that are surely as fast as the proposed approach (and still represent a sequential pipeline of first denoising and then applying a network, which one can expect to work worse than the proposed approach). Therefore, I keep my score and do not recommend this paper for acceptance in its current form.  ----After rebuttal: My main concerns about (a) no counterfactual model and (b) the linear / nonlinear requirements of the method remain. Generally, bias assumptions are made about the data, not the output of a representation learning procedure.  "The nonlinear relationship between raw input with the outcome can be encoded into the learned embedding" yes, but it does not mean H_S and H_V will meaningfully encode anything related to the input bias in any meaningful way. The method needs to precisely describe the structural causal model to be properly evaluated.  ###################################################################Post Discussion Score:The authors did not submit their rebuttal. After reading the comments from other reviewers, I decided to keep my original score for this paper. Post-rebuttal:My concerns are not addressed, so I would like to keep my original score.------------------------------- -------------Post-rebuttal: After reading the rebuttal and the other reviews, I have decided to lower my rating. The rebuttal appears to state that all baselines and evaluations suggested by the reviewers are irrelevant to the goals of the paper. I see the two main hypotheses clarified in the rebuttal, but I do not see a convincing argument for avoiding comparisons to related approaches to the task at hand.  ------Post rebuttal:After reading the author's response as well as the opinions from other reviewers, I will stick to my original rating. Although the authors resolve some of the concerns in the rebuttal, there are still limitations in the method and task design. ----------------------------------------------------Post Rebuttal UpdateWhile I think the idea is interesting, I still think the proposed loss is not consistent as I still think the two terms in the loss collide with each other, its practical value is limited mainly because making a GAN to work on various datasets is a challenging task, and that the experiments now raised more questions than answers. For these reasons I still lean towards rejection as I believe the paper can benefit from a revision. ========================UPDATEThe response says:"With the straightforward use of Transformers, where the model has only seen a single variable in training, theres no information for the model about what to do with the second variable and thus it will not generalize to the two variable case. Training on two variable-polynomials and testing on two variable-polynomials has relatively low accuracies in our experiment. This suggests that training on single variable-polynomials and testing on two variable-polynomials will result in even lower accuracies. With more work, one may be able to design a model with appropriate inductive bias that understands the concept of multiple variables. This is beyond our scope."I am afraid that this makes the study rather insufficient for me. The problem of representing variables, eigenvariables/skolems, and capturing structural similarity between different theories and signatures is ubiquitous in the ML-for-TP area. Practically all useful systems developed so far - both features-based and DL-based - have to address this. The authors' answer is "our representation is unsuitable". The observation that if you have no shared representation of variables, you will get little/no generalization is a no-brainer and there is hardly any need to publish negative papers about it. In particular, in a conference about *representations* and some 15 years after first useful systems dealing with such issues have been developed. There are many fixes to this - see e.g. Gauthier's representation of variables in his Tree NNs, etc.My score will stand, but I would like to encourage the authors to dig deeper and follow the suggestions given in this and other reviews. The general topic of learnability of symbolic rewriting by various neural architectures is certainly interesting, potentially very useful, and far from well understood. Review update after rebuttal: The authors have addressed some of my concerns in the rebuttal and the modified version of the paper. They have added results on more real datasets and explained some aspects that were unclear in the first version. However, I am still not convinced that this is a comprehensive enough contribution as it is right now. The results on the new datasets have, in fact, raised more questions. Their proposed method seems to perform worse than the baselines in some scenarios. Though this is not bad and it is important to show negative results, I did not see any discussion or insights into the poor performance. I would recommend the authors run some baselines themselves and compare rather than copying the results from the baseline papers. This would ensure that the experimental setup and environment are similar leading to a fairer comparison, and possibly some insight into their methods' performance. In summary, I still believe that this is an idea with potential but the authors still have ways to go in putting their idea clearly on paper and justifying it completely and comprehensively. I am sticking to the score I had assigned before.  ----------------------------------------**Update after author response**: I appreciate that the authors answered some of my questions, but they did not address my two main concerns: insufficient baselines and writing clarity. I therefore am inclined to maintain my vote to reject the paper (score = 4). ** Edit (Nov 23): I have slightly increased my score due to the improvements made to the paper (mainly reorganization) & some clarifications made by the authors, but I still don't feel like my main concerns were addressed.  ===== Edit after authors' revisions ======The authors made some efforts to improve the exposition of the most obscure parts of the paper, but in my opinion this is not sufficient and I am still not able to fully grasp the connection between section 4.4 and 4.5 and the rest of the paper. The added parts (in blue) contain additional typos and, like the rest of the paper, look like they have been type in a rush. For instance:- "First of all, we show that f_i can be regarded as a sum-of-power mapping defined by the coordinate function" -> I don't see where this is showed in the paper- "f_i(v_1, · · · , v_N ) can rewritten as" -> can be rewritten asI'd encourage the authors to resubmit their work using a longer paper format that would allow them to better expose the details of their investigation. =========== POST REBUTTALI have really appreciated the effort placed by the Authors in their rebuttal. Here follows some considerations following rebuttal:* The empirical analysis has been strenghtened with the inclusion of the OGB benchmark as it provides a view of the empirical performance of JAT in the context of a challenging dataset within a standardised setup. At the same time, OGB allows direct comparison with leaderboards and results in literature that highlight how JAT performance are not (as claimed) state-of-the-art. In this sense, it is still quite unclear if there is any practical advantage with respect to GAT.* The revised paper version places the work much better in the context of recent related literature.* Also considering the point above, the novelty of the work is still borderline to me.In summary, the work has good potential, but it is not yet ready, hence my decision to stay on the reject side. The contained originality can be made up in a future submission by providing convincing state-of-the-art performance results on challenging benchmarks, showing substantial differences from related models such as GAT.  **================================== Update after rebuttal ==================================**It seems that the authors have only provided a general comment for all reviewers, which is understandable since most criticisms from all reviewers are on the same weaknesses of the paper.While I appreciate the author's effort in adding more experiments, I do not think the added experiments and reply properly addressed the concerns shared by other reviewers and myself. For example, it is still unclear what the advantage of the proposed method is or what insights we could gain from this study.I think this paper is not ready for publication. As today is the last day of discussion period, I will maintain my original assessment. Thank you authors for your response. --------Updates after author response--------I thank the authors for the detailed response and appreciate the additional experiment. However, I still believe that evaluation on downstream tasks is essential to demonstrate the superiority of the approach, and I unfortunately do not agree with the authors that it implicit that the approach will yield better downstream networks. Therefore, I cannot raise my score, but would encourage additional experimentation.  --------- After Reading the Updated Paper ----------Thanks for the update. After reading the revised paper, I still have some major concerns:The current experiments performed are not enough to demonstrate the effectiveness of the proposed method. The old experiment results (Table 6, 7, 8) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data, which is not realizable in practice. We should assume that the test OOD data are unknown during learning the OOD detector. The new experimental results where they train the binary classifier using adversarial examples generated on in-distribution data (follow the Mahalanobis method) in Table 1 are limited. For example, on CIFAR10, they only report results for ResNet50 and WideResNet, but I also want to know the results for DenseNet (Mahalanobis method [4] performs very well on CIFAR10/SVHN using DenseNet under the same setting).Some experimental details about their method are missing. The authors mention that they train 12 binary classifiers and then select the best one on the validation dataset. But they don't provide the details about the validation dataset, which is critical for their results. Based on their previous response, it seems they use a subset of test OOD data to select the best classifier, which is not allowed I think. Based on the current description of experimental settings, it is hard for me to evaluate the reported results.The proposed approach needs a lot of hyper-parameters (4 attributes, 12 combinations, the weights of the binary classifier, etc) and it is unclear how to tune these hyper-parameters and how they would affect the results. The current ablation study is limited I think.This paper doesn't have rigorous analysis for why integrating different attributions would improve OOD detection. I think this is an empirical paper but the experiments provided are not sufficient to demonstrate the effectiveness of the proposed method.To clarify, I didn't agree to raise the score previously. What I said was that the previous paper needed significant revision and I could not recommend acceptance. I still have some major concerns after reading the revised paper. Thus, I keep the same rating and think the paper is not ready for publication. I hope the authors could keep improving their paper.[4] Lee, Kimin, et al. "A simple unified framework for detecting out-of-distribution samples and adversarial attacks." Advances in Neural Information Processing Systems. 2018. ##I have read the author response and I still think the paper is limited in terms of novelty, significance and experiments. I would like to keep my current score. ##