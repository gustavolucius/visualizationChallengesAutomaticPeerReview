 Post revision update-------------------------------The authors have been very helpful and addressed many of my concerns, and I think the revised paper is a substantial improvement. I have rated the paper as a 7, although I do have some lingering concerns.Most crucially, it's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior, since, for example, both models significantly outperform humans at the tree rule. However, the authors do point out that humans perform better at these tasks than the null distribution. Still, I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ. However, I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard, at least future researchers will be able to see the full pattern of results to draw their own conclusions. Updates after the rebuttalI find the revised manuscript to be clear and more transparent. After reading the reviews, the response, and the extended appendix, I am increasing my score and vote to accept this paper. ------Updates after author response------I thank the authors for the response and the new experiments. In light of the clarifications and additional evaluations, I have increased my score to 7. *** After reading the authors' responses ***I have raised my score to a 7. I felt that some of the key questions, e.g. regarding generalization to mathematical expressions that are qualitatively different than the training data,  were answered well. This paper should not be reviewed as being methods-driven. It's about demonstrating a new way that deep learning could be transformative for engineering, by allowing engineers to screen proposed designs for stability, etc. Final rating- I am satisfied with the author's response and updating my ratings.  *Post-Rebuttal Evaluation [FINAL]*I have thoroughly inspected the revised manuscript and read the response provided by authors. I was pleased in registering that my two requests were taken into consideration by authors who provided a solid sensitivity analysis and additional experimental evidences on ImageNet. Therefore, taking this into account, I am now convinced in raising my original score (5: Marginally below acceptance threshold) to a full acceptance (7: Good paper, accept).  ### Post-rebuttal updateI thank the authors for their response -- this helps. Having read the other reviews, I am still leaning towards acceptance. --------------------Post Author ResponseThank you for addressing my concern about complexity and adding Fig. 6. It seems that it is still better than baselines in terms of complexity. ## After rebuttalThe authors have addressed my main concerns and I've decided to raise my score from 6 to 7. Edit: After reviewing the author response, I leave my score unchanged. I believe this paper should be accepted.  Edit after rebuttal: I have read the author response and I am thankful to authors for answering my questions. I keep my rating as it is and believe that the paper should be accepted. I hope this will be widely adopted in NLP community. Edit: score modified after reading the authors' reply.Edit 2: Regarding the issue of optimization getting stuck in local optima or finding global minima (moved my comment here for visibility):I thank the authors for their flexibility on this issue, but I'd like to weigh in on this saying that from my perspective those statements were actually accurate and useful, and should be kept in the paper as they existed originally. They are properly backed up by citations, unlike the comment of "getting stuck for sure in a local optimum", which is very much dependent on the optimization problem. I am willing to follow up with the other reviewers, should they consider I am mistaken.I have also increased my score after reading the author's response, and I agree that the final decision should not depend on this specific issue. If the paper is accepted, I'd like it to include the original statement, which can be very informative for some readers. -------Thank you to the authors for their response and update. I have read the response and am keeping my current rating for the paper. ---I thank the authors for the response, which addresses my questions. I still believe that the paper provides valuable contributions and insights. Update: I've updated the score given the authors' response, see my comment below. ---------- Update on the revised manuscript ----------I have read the new version of the paper and it reads a lot better. The new expanded methods section, and the definitions for different variations of GONs makes the paper much stronger and easier to understand. I appreciate and like the new experiments that show GONs capabilities on LSUN, comparisons with VAE on ELBO. Most of my concerns have been addressed in this version. I think this paper makes an interesting and novel contribution and I will raise my score accordingly.  ==============================Edit after author response: we thank the authors for their response and providing some more empirical information. Overall I feel that this paper presents a neat idea that could be of interest to some people in the community, and I have modified my score from 6 to 7. It would be great for the authors to discuss the importance of initialization, as in particular, it seems to me that the sign of $m_2$ can never change  (from its initial value), indicating perhaps that practitioners should try initialization at either and select the better performing model. #########################################################################Edit after author response: We thank the authors for their response to my concerns and those of other reviewers. I have updated my score from 6 to 7 based on their changes. Post rebuttal: I thank the authors for their detailed response and paper revision. My concerns have been addressed, and I particularly appreciate the experiments presented Table 3, Figure 5 and Section 5.4. I am happy to maintain my rating and recommend acceptance.  -------- post-rebuttal comments --------I have read the concerns raised by other reviewers as well as the author response. I still feel that this is an interesting work and its findings are of potential value to the community: There has been a considerable amount of recent work in open set recognition for deep models, and this paper calls into question the need for sophisticated techniques by showing (fairly rigorously, in my opinion) that simple strategies and the right choice of feature engineering works better. I agree with the authors that not using ImageNet pretraining is an unrealistic and unnecessary constraint  moreover, the method generalizes even when evaluated on datasets such as MNIST and SVHN, which are distributionally very different from ImageNet. Further, I think the paper acknowledges prior work appropriately and did not find any of the claims made to be unreasonable. I agree with R1's concerns about overlap with two recent papers, but found the author response to be satisfactory. Overall, I will retain my accept rating. POST-REBUTTALThank you for your answers and incorporated modifications! I think you've succeeded in addressing my major concerns, so I'm raising my score and recommending accept as promised. # After rebuttalThe authors did a good job of answering my concerns. I keep my original positive rating. .Update after Rebuttal:An additional experiment on real data was added which I find a valuable addition to the submission. AR2 points out a similarity with AET which I did not notice in my initial review. I feel this does limit the contribution somewhat. As the rebuttal points out there are differences between the proposed method and AET but the core idea is very similar. In my opinion there is enough difference to still recommend acceptance Post-rebuttal:I believe the authors have done sufficient work in their revision to address my concerns. Thus I'm leaning towards acceptance and raising my score to a 7. Edit: As mentioned in my comment below, I believe the authors have done sufficient work in their revision to address my concerns and am revising my score to an acceptance. ## After rebuttalThank you for answering my questions! It address my concern and I revise my recommendation.- Please clarify in the next version how the checkpoint is selected (e.g. which epoch is selected?) given a product of seed and hyperparameter. **Update:** The authors clarified all my questions very well. They also added an extra plot for the double descent experiment on a different architecture (ResNet). Although I feel a bit lukewarm about the added plot (in that the double descent phenomenon is only somewhat weakly reflected by their empirical measure), I've increased my confidence score from 3 to 4 to appreciate their efforts in addressing my concerns. Good luck to the authors! **The score does not represent the initial review. It was updated following the discussions below.** =====POST-REBUTTAL COMMENTS======== Initially I had only minor comments and the authors addressed all of them. I will keep my score. ---### RebuttalI thank the authors for their reply. I'm more confident this is a good paper now. ## Second ReviewThanks for taking all my comments seriously. After carefully reading other reviews and quick modifications introduced by authors, I believe this work is richer and has shown some potential towards building scalable and robust alternative to BP. Thanks for including angles as suggested, this further supports hypothesis proposed in this work. I really appreciate results using CNNs and cross entropy loss, therefore I increase my score to 7. It seems that other reviewers do not appreciate that training a network using hebbian like updates and without BP requires some nontrivial engineering tricks and theoretical considerations which are now well described in this paper (updated manuscript). It is difficult to match BP gradients, and many popular alternatives including FA, DFA, DTP, EP struggle to match BP performance when tested on complex benchmarks (Cifar, imagenet, etc) with complex architectures (CNNs, RNNs, etc). Beside this given approach is robust even when backward weights are trainable. However I agree storing backward and forward synapses challenges the bio-plausibility of approach, which I think can be handled if local representation are handled differently. Nonetheless, changing few strong words (optimal BP, optimal gradients) and derivation, supports major hypothesis proposed in this work. A better justification on non-local updates as raised by other reviewers is required to further strengthen this work. But i liked the results with activity relaxation and how close gradients are with respect to BP. Combining current approach with other bio-inspired approaches might solve some key aspects of current learning algorithm. Current approach is still heavily dependent on backprop, and partially gets closer to bioplausible approaches (mainly hebbian like update rule). Testing this on deeper architectures (Resnet, student-teacher etc) might further strengthen your work.  ## Minor commentsFigure 4 c) change angls--> anglesIn appendix, change caption of fig 5a) from MNOST to MNIST, I believe it is a typoAdd results with FA or DFA with fixed and learnable weights, will further support robustness and closeness of BP claim in this work. UPDATE-----------the author response has addressed some concerns (efficiency concerns; unclarities about some ablations studies; reason for not comparing to Edunov for EN-DE) well, and I have slightly increased my rating.I maintain that the description is needlessly obscure in places, even in the revised version (while it is technically correct that a denoising autoencoder could be described as a crossover encoder where the two parent outputs are identical, this is needlessly convoluted), and a major concern about the framing of the paper was not resolved in the response (if back-translation is self-supervised learning, then some of the claims about limitations of self-supervised learning are untrue, and the findings lose novelty. If back-translation isn't self-supervised learning, then it's misleading to imply that self-supervised learning led to a new SOTA).  Update:  Thank the authors for the detailed feedback. I decide to keep the score.------ #####EDIT#####I agree that the author's disagreement with my second comment and thank you for the update. I change my rate to 7. %%%%% EDIT %%%%%%I am satisfied with the author's response and given the proposed changes will raise my score to a 7. ########## Edit ##########The authors have addressed all my questions. Thanks. ----- post rebuttal -----The authors addressed most of my concerns and the revision is better than before. I would like to increase my score and would recommend an acceptance. *******************After rebuttal period: thank you for answering my questions and for updating the paper. I still think it is a good paper and would like to keep my score. === Post rebuttal ===The clarification and additional experiments are greatly appreciated and answered some of the points which were not entirely clear to me. =======================================**Post-rebuttal Comments**===============================I appreciate the revisions and additional results reported by the authors. The authors have addressed the concerns raised by me in the revision. While I agree with R1 that novelty of the method is limited, after considering the reviews collectively, I believe this paper presents very impressive results given the fact that the problem is challenging. Therefore, I would like to improve my final rating to 7. Edit: I am happy with the author responses and have raised the score accordingly to 7. **Post-response update**Thanks authors for extra effort on semi-supervised experiments. I decided to increase the score to 6.  After reading other reviews and the authors responses to all of the reviewers, I recommend this paper by acceptedextensive results show that the CALM objectives offer more signal from data than current pretraining methods.This paper suggests a number of cheap-to-compute corruptions of the input data that, when used to reconstruct the input, enrich the underlying model. These objectives certainly improve over the original T5 base and larges models that are used as initializations, and especially outperform the base model in the low-data regime. The authors use objectives which capture both generative and discriminative information, which some have suggested contain mutually beneficial signal but have not been unified in a single training method.Below are two paragraphs from my original review. The authors have done further experiments and show that there are still gains on these tasks when model sized is increased significantly. Furthermore, they have clarified that on the key metric of CommonGen they achieved SoTA with only slightly more than half the parameters of the current SoTA model. I therefore believe that these results show merit.I still feel that the authors use of concept and commonsense is vague, when their method can be defined more clearly with more mundane terminology. In practice, the authors use nouns and verbs as their concepts, which is fine in terms of pretraining objectives, but surely does not capture the generality of concepts. The authors have somewhat clarified in this in their updated version.Finally, the CALM intermediate objectives share many properties with all of the datasets tested on and are likely calibrating the model to the kind of correlations they should expect to predict in advance of finetuning. One way this can be seen is that the slopes of the T5 and CALM lines are very similar after an initial bump which T5 likely needs to calibrate to the new distribution. This makes claims of learning commonsense hard to verify, though I do agree that something relevant to solving these problems is clearly being learned.Altogether, I think this paper makes an interesting contribution to the question of: How can we get the most pretraining signal from unstructured data using off-the-shelf tools? I recommend this paper for acceptance, though I encourage the authors to revise their paper to make this the focus of the story, rather than the vaguely defined notion of concept. **Update after author response**I appreciated the authors response to the scalability and raw computation times.  Thank you also for the additional comparison to a non-potential function method.  This will be a good comparison.  My main concerns were answered, and I still think this is a good paper. ######## Post-Rebuttal Updates:The authors have addressed most of my concerns, and I appreciate their effort. I am not convinced that early-stopping alone can help the model learn to utilize relation information, when the target ground truth does not require the model to do so. However, I recommend accepting this paper as it introduces a new task and presents strong results.  ### Post Author ResponseI thank the authors for their hard work and I'm happy to see that the analysis has improved. ----Post Rebuttal Update:The authors addressed and clarified many of my concerns, therefore I updated my score.  ----Post rebuttal update:I  read the response and commented on it. The authors clarified my questions and updated the paper accordingly. So I think my score of 7 is supported. ---------------after rebuttal----------I've read all reviews and the rebuttal. I think overall this is a good paper and would like to keep my score. -----------------------------------Rebuttal: Thank you to the authors for considering the comments and for the changes. I am happy with the response, and have increased the score accordingly. Fig. 5 could be changed similarly to Fig. 1 for visual clarity. The authors mention in the response that both GRU-ODE-Bayes and Rubanova et al. 2019 can be used interchangeably, but there is no reference to the latter.  ## Post-rebuttalI thank the authors for their hard work, and for incorporating my suggestions into the paper. I believe the paper has improved. **Update**Thank you to the authors for the detailed responses and revisions. I have adjusted my score upwards. I think this is a useful exploration of an alternative means of quantifying feature importance, with intriguing results: somehow, optimizing for adversarial robustness also seems to optimize for the score on insertion/deletion games. Further exploration of that issue (or at least, elevating some of the many appendices on that issue to the main text) would, in my opinion, increase the impact of the paper. .**Update**: The authors' update is comprehensive, well-thought out, and demonstrates significant improvements to the paper; I have raised my score to reflect this ------Added after the discussion period: Thank you for answering my questions and the lively online interaction. You explained a few things to me, and I could see that you understood my point about VAEs. That was a great outcome. I hope that that you will address the  points that we discussed, where showing an honest link with VAEs will be highly desirable for your future readers.  -- EDIT:I would like to thank the authors for the nice work during the review process. I am pretty satisfied with that and I feel serene to increase my rating to acceptance. Update: I really appreciate the authors' efforts to address my original concerns. I believe that this work is a nice application of transformers to image colorization. The paper is well-written and the performance of proposed transformer architecture is strong. I think that this work is above the threshold of acceptance. Update: Thanks for the additional ablation studies. I would like to keep my original evaluation which is acceptance.-------------------- =============================UpdateWhile I agree in principle with Reviewer 1 that this paper has jarring flaws in writing and the rebuttal version does not adequately address it, I disagree that the writing warrants such a low score. I have seen worse papers with outrageous claims (e.g. try to claim significance with p=0.1) and I would not give those a 2. I would also disagree with R1 that there is no interesting result in this paper, because there is no prior work I know that even considers how distilled models generalize like their teacher.If I were to grade this paper based on different aspects, the originality and significance would be both 9's, quality a 6 due to experiment issues and careless generalization, and clarity a 3-4 due to unclear motivation in the abstract/early intro and poor differentiation from prior work in terms of experiment design and analysis.That said, the rebuttal did not change my mind that the writing probably will not be improved enough post-rebuttal, I would thus not be able to consider this a top paper despite the interesting observations. ==========Edit after author comments:I've read the author comments and the updated version of the paper.Although the authors claim that they have shortened the introduction of the paper by 1 page, this doesn't actually seem to be the case in the last version that was uploaded, where the section titled "Introduction" is almost unchanged compared to the original upload (I've used the diff tool between the latest and original version).Maybe there was a misunderstanding and the authors have shortened a different part of the paper?Although it would have been nice to shorten and streamline the introduction to make the paper easier to read, it's not critical to my rating.The added ablation experiments demonstrate that each of the different attention modules proposed in the paper improve results, which I think really improves the paper. I was originally not sure whether the complexity of the model was justified, but the new experiments demonstrate that each of the components seems to be needed.I've also carefully read the rest of the paper and the author comments explaining details of the tasks under study and now feel that I have a much better understanding of what was done and how the model could be used for other tasks.I agree with R4 that the novelty of the paper might not be groundbreaking, but I believe the paper could be relevant and interesting for other researchers who want to incorporate attention mechanisms into their architectures, so I recommend accepting the paper.I've increased my rating from 6 to 7. #### Post Rebuttal-UpdateI thank the authors for responding to my concerns. I enjoyed reading the paper and maintain my rating. Update: The authors thoroughly addressed all the questions, the experiments demonstrate an improvement, the theory coincides with experiments. From my perspective, that would be useful for the neural ODE community to know more about the proposed log-signature-based technique. I increase the score.  *Final Evaluation [Post-Rebuttal]*I am thankful to the authors for their clarification regarding ANOVA decomposition and I am inclined in confirming my initial score. Update (Nov 30th) In light of the author's responses and the other reviews I increase my score for this paper to 7: Good paper, accept. I raise my rating based on the additional experimental results given. ### UpdateThanks to the authors for their clarifications (and to the other reviewers).  I am more comfortable with my accept recommendation now, and have updated my confidence accordingly. ======After RebuttalThanks for the detailed reply and additional experiments. I increased my score accordingly and I hope the authors could further address following issues:- While the results in C.3 shows default $\rho$ improves over SGD on most experiments (may also add SVHN and Fashion), I can still see its sensitivity to datasets, architecture, noise level and number of accelerators as shown in Table 6, 7, 8 and Fig. 3. For example, 0.05 is not close to optimal with labe noise 20%~60% in Table 8. It is unclear whether $\rho$ is robust to other hyperparameter changes (e.g., weight decay that controls weight scales).  So an ablation study on the sensitivity of $\rho$ and further explanation would be necessary and much valuable for practitioners. - It would be also helpful if the authors can provide more details about how to get the flat minima of Fig .1 (right) when optimizing deep non-residual networks, such as $\rho$ and other hyperparameters.- Minor: Table 8 should be validation errors rather than accuracy. Updated review----------Given the unanimous support for acceptance amongst the reviewers, I don't think it is really necessary for me to provide a detailed update. The details of how the authors have addressed the concerns expressed in my initial review can be found in the follow up discussion. I now support acceptance without reservation. EDIT: the authors have clarified that the hardware area results take into account the need to support multiple formats, which addressed my biggest issue with the paper. I have raised my score to a 7 (accept). ___After reading other reviews, authors' comments, and checking the revised manuscript I decided to slightly improve my rating for two reasons. Firstly, my concerns were answered during discussions, secondly, I do not agree that the concerns raised by other reviewers could justify a rejection. I believe this is an exemplary empirical study presenting novel information and insights about sampling sensitivity of point cloud encoders and point cloud evaluation metrics. Summary after discussion period:----------------------------------------------The authors have done a good job in toning down their claims to match what the evidence supports. After reading the other review's comments and the updated version of the paper, I feel both my comments and most of the reivewer's comments have been addressed, allowing me to recommend this paper for acceptance. =====================Update after author response: I thank the authors for their responses. I broadly agree with the points brought up by the other reviewers, and despite some weaknesses brought up by various reviewers, this paper is a good contribution to the community. I have brought my score from 6 to 7. -------------Thank you to the authors for their response. It has helped clear some questions I had in mind. I am keeping my rating. #####################################I have considered the rebuttal as well as other comments in my final recommendation. ----Update:I do agree with the other reviewers that the paper may be difficult to read, especially for non experts. Nevertheless I still think the paper makes a nice contribution, so I will keep my rating. The authors have tried to answer all issues and questions raised in the reviews. At least, I can say so for the questions and issues raised by me. I therefore tend to keep my positive opinion on this paper.  I have read the author rebuttal and appreciate the changes made to clarify the distinctions and handicaps used by each of the algorithms. Additionally the exploration into ranges of the delta parameter was appreciated. --------------Thanks for the response and I've increased my score. I am satisfied with the response but still not convinced about the algorithmic novelty on the intention semantics built into the method, even after reading B.1.  In particular, it seems that the loss functions do not drive mu's represented by NNs to the fixed point solution of Eq (3); psi shows up in Eq (3) but does not play a role in the following development of the method.  ------------------------------------------------------------------------------------------I have read the authors rebuttal. The authors have addressed most of my concerns. The JL random projection baseline has been added and the heuristic method of reusing the eigenspace for some iterations has been explored, which I appreciate a lot. From the current experiments, the proposed method seems effective, though it will be more convincing if the method can be tested on larger models or harder datasets. I think the topic of the paper is quite interesting and the idea of bounding $M_t - \Sigma_t$ uniformly is also interesting. The theory indeed manifests the effectiveness of the proposed method. As a result, I increase my point to 7. **UPDATE AFTER THE REBUTTAL:** The new material in the paper clarifies things quite a bit, especially the intuitive explanations appearing below Equation 2 and at the bottom of page 4. Thank you for adding that, I have changed my score accordingly :) POST REBUTTAL UPDATE: I am increasing my confidence in this paper from 2 to 3 - I still believe the paper can use some more clarity but enough points have been explained and updated in the draft for me to feel more confident in my evaluation.  I think the ideas in this paper are quite interesting - for this reason I continue to recommend acceptance. **Post Rebuttal Update** The authors address many of the concerns, 1) [a] is properly acknowledged in the revised version and novelty is not claimed on additional label noise in the text, 2) while quantitative studies are still absent for claims on sharp vs. flat minima, qualitative results are provided for convergence to "flatter" minima  3) connections between smooth functions and generalization is discussed 4) answers and updates regarding complexity and convergence are *somewhat* convincing. Thus, I am willing to increase the score from 6 to 7 and confidence from 3 to 4 as I believe the paper provides relevant and interesting theoretical arguments. ################################################Post Rebuttal Update: the authors have well addressed my concerns, in particular (1) the additional visualization gives a good qualitative empirical evidence supporting the claim that SLN helps escaping sharp minima. (2) the search process for the hyperparameter $\sigma$ is very reasonable and makes the usage of SLN practical. I will keep my initial assessment and vote for accepting this paper. EDIT: Thanks to the authors' for their clear responses, I'm happy to raise my score to a 7. EDIT: The response is very helpful and it addresses my concerns. I raised my score to 7 after reading the response. ====Post-rebuttal: I am happy to accept this paper after seeing my concerns are mostly addressed. ----------------------After the rebuttal: I'd like to thank the authors for their answers, particularly for resolving the confusion about the term "contraction". I believe this is a good paper and stick to my rating of recommending its acceptance.  ##### Updated reviewer #####The authors have addressed my main concerns satisfactorily, in a clear and concise matter. I have updated my recommendation to reflect this. Edit: I have updated my score based on the new experiments added during the rebuttal process.  After rebuttal:I am satisfied with the reply of the authors and I have raised my score to 7. ***************************************************************************POST REBUTTAL UPDATEThe authors provided a detailed rebuttal which addressed almost all my concerns and answered most of my questions. I wil therefore update my rating from 6 (marginally above acceptance threshold) to 7 (good paper, accept). I believe this paper should be accepted. --------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:Thanks to the authors for providing relevant details and fellow reviewers for nice discussions. Original rating is maintained.  **Update after authors' response**I am very happy to see the additional results on CIFAR-10, and the layer-wise ablations and other control experiments. To me, these results have shed a lot of light onto how the observed hashing effect can be explained. These explanations mostly confirm intuitions. Nonetheless I think it's worth reporting the empirical verification of these intuitions that tie in with earlier results reported in other papers. To me, the most interesting aspect of the work is how the hashing properties change over training (and across layers). The paper could still be improved by experiments on e.g. CIFAR-100, ImageNet and non-vision tasks, as well as more mathematically sophisticated definitions of some of the measures (e.g. average stochastic activation diameter). I personally think the results are now sufficient (and sufficiently backed up) for a publication and most of the criticism raised by the other reviewers has been addressed sufficiently (for me). I would now rate the paper as a 6.5 - but to facilitate the reviewer's discussion I will take a clear stance and have thus raised my score to 7.--- Edit: I have read the author's response. They have responded to the questions I had, and they address many of the concerns I had. I am now raising Edit: updating score, and recommending acceptance as per my response to the author's rebuttal. =====================Post-rebuttal comments:Thank the authors for the detailed responses and revised submission. My concerns have been adequately addressed and I raised my score to 7. (Edit: the authors have clarified) ------------------------------------Update: the major concerns above have been addressed in the appendix of the updated manuscript. I'm moving my initial rating of 6 to 7. Post-Rebuttal----------I thank the authors for their response. Both of the sections are now more clear, although the authors should make an effort to polish the narrative of the paper and the clarity of exposition throughout. The discussion of epistemic versus aleatoric uncertainty in the appendix is also interesting. I have increased my score from 6 to 7.  Updates:Thanks to the authors for addressing my concerns and responding to my questions. The newly added experimental results make the paper stronger and addressed many of my concerns. I recommend this paper to be accepted. ==============After rebutall:I thank the authors and appreciate addressing all my concerns. I'm glad that the additional experiments with smaller mini-batches and smaller training set size provide more supporting evidence for the method. I encourage the authors to point to these results in the main body.One more minor suggestion is regarding the sentence in Remark 3. The way I read the sentence the "equivalence of minimizing CE loss and MLE loss" is the reason we can remove the KL divergence term. But the authors' response seems to say it is for the 3rd part of the sentence. The authors' might want to rewrite that sentence to make it clear. UPDATE:I do not believe that Merity 18's results are not reproducible. Just look at the vast amount of work after the publication that builds on top of the results. I don't doubt that your way of evaluating the performance of language models can lead to hypothesis testing, but it is not what the field employs thus your results are not comparable to others, which is why I cannot accept this paper. If your method really does work as well as you argue it should be no issue obtaining an improvement over the actual baseline.UPDATE 2:The new results with Merity's original benchmark leads me to increase my score from 4 to 7. I appreciate the effort in reproducing Merity's results. --Update after rebuttal--The reviewer thank the authors for the response. Most of the core issues have been addressed and scores have been updated accordingly. ## Post Rebuttal I thank the authors for their response. I have two responses: 1) I still don't find the answer to my Q1 convincing; in particular, the 'filtering effect of distillation' mechanism requires more rigorous discussion, and 2) with regards to the ICCV2019 paper, I think the authors may have misinterpreted my point; my point here is that one could design backdoor attacks that do not affect the attention maps substantially and was wondering if the logic would hold for such attacks. However, I agree with the authors that the points go beyond the scope of the current paper and would be interesting for potential future work. In any case, I think the paper is a good contribution to the field and would still vote for accepting the paper.  == comments after discussions and the paper update ==I appreciate the authors' efforts to improve the clarity and provide additional results. I believe that the proposed method is now clearly presented and the claims are properly supported by experiments. I raise the score to "accept".  #######################################Update after reading other reviews and author responses:I am happy to keep my score and support accepting this paper. I agree with Reviewer 4 that conducting a more detailed ablation study of the impact of regularizing both the policy and value function (i.e. a comparison with an algorithm like that of Kostrikov et al. (2020)) would improve the paper and hope that it will be included in the final paper.  update: Thank you to authors for their response to reviewer comments. I acknowledge I have read and reviewed their rebuttals. ** Update after rebuttal **I appreciate that the authors addressed my comments. After reading the authors response and other reviews, I still believe that it is a good paper that should be accepted to the conference. Update: the authors went out of their way to address my concerns about the absence of the unbalanced class setting: they added a new datasets (SVHN), new results (table 4) and updated some of their explanations. All these additions seem satisfactory. I was also pleased with the feedback about computational cost (R3). I improved my rating. While I agree with the concerns of reviewer 4 (those I could understand), they would apply to every publication I have read about calibration, and I think the authors addressed these concerns to the best of our current knowledge. Update after the rebuttal: The authors have answered my concerns. I believe the paper should be accepted and would be a nice contribution to the current research.=============================================================== -- UPDATEThanks for the clear and exhaustive response.Minor, regarding the ablation study in A.1, with S=3 you get the best results, why not try with more? Post Rebuttal: Thank you for the response. I have read the discussion with other reviewers. A small comment: while I agree that it is reasonable to keep the classifier the same for all the models (softmax with cross entropy) for a fair comparison, I disagree that the activation function for the first layer should be kept as ReLU in the KH model. In fact KH explain that this is a suboptimal choice in Fig. 4 of their original paper. Using powers of ReLUs should increase the KH accuracy. Overall, I think that this is a nice paper, and I am inclined to keep my initial score.  UPDATE:I have read the author feedback and other reviews & discussions. I have updated my rating from 6 to 7. The authors responded to my comments with updated analysis to further prove their claims. ------After rebuttal: Thank the authors for their efforts in the rebuttal and revision. The authors' response to the Maxmin question (along with the revised discussions) sound convincing to me. I would like to keep my original evaluation and would lean towards acceptance for this paper. Post Rebuttal:I think the paper is much improved, and I'm bumping up my score accordingly. Small point - in corollary 3.2, to say 'with high probability' still requires that explicit conditions such as $|\beta|/\sqrt{d} \to \infty$ are stated. Thm 3.1 makes no claim of _high_ probability - it instead has an explicit bound, and does not impose conditions needed to make this bound 'high'. As a conseuqnce, the conditions of Thm 3.1 do not suffice for corollary 3.2. ---- Thank you for the updates! The paper is much improved. I have raised my score. I still have some specific concerns, below:In the new Figure 1a, could you talk about how you search over optimization and initialization hyper-parameters for the local and global loss cases? I have a suspicion that the better performance of the local network may be due only to hyperparameters being better tuned for local rather than global training.re "they differ from our method in that their layer activations z_k are calculated using backpropagation-based optimization."I'm pretty sure this is not an accurate statement about the method of auxiliary coordinates? At least, it is my understanding that, due to the quadratic coupling between z_k in adjacent layers, the gradient with respect to z_k only depends on the z_{k-1 ... k+1}, and so there is no backpropagation through multiple layers in the network. Similarly for the gradient with respect to the weights in a given layer.More minor questions/comments:re: "Even at random initialization the gradient alignment S(d_L1/d_phi1, d_L2/d_phi1) is in general NOT zero-mean.  For a randomly initialized network, d_L2/d_phi1 and d_L1/d_phi1 are both zero-mean random variables (for the reason you mention - that d_L2/d_s2 is equally likely to have either sign), but they are not independent - they are both functions of phi1, and this dependency induces alignment.  Weve changed Figure to demonstrate that the initially weak alignment becomes stronger as training progresses (and phi approaches phi*), and added a derivation of the alignment result in Appendix B."Can you say more about why you would expect the gradients to be aligned at initialization? Is it just that both gradients are expected to have a non-zero projection in the phi1 direction (because the distance between two random vectors will tend to shrink if either vector is moved towards the origin)?Another thread of references that comes to mind, for biologically plausible local learning rules in machine learning, is the use of meta-learning to learn those rules. A seed paper is:Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Université de Montréal,Département dinformatique et de recherche opérationnelle, 1990. * EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. * Edit: change score to 7 in light of revisions and new experiment. After rebuttal:=============Authors have addressed many topics that not only I but rev 3 address and hence I score this paper with a 7 and recommend it for publication. ****Reply to authors' rebuttal****Dear Authors,I greatly appreciate the effort you have put into the rebuttal. The changes you have made have addressed most of my concerns and I believe that the few outstanding ones can be fixed without significantly affecting the main message of the paper. I will thus be recommending acceptance of the paper.Best wishes,Rev 3 -------# Post-discussionI increased my rating: even if novelty is not high, the results support the incremental ideas proposed by the authors. AFTER REBUTTAL:This is an overall good work, and I do think proves its point. The results on the TaxiBJ dataset (not TatxtBJ, please correct the name in the paper) are compelling, and the concerns regarding some of the text explainations have been corrected.----- Update:The score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now. ******************Update after author response:Thanks for the clear response and Figure 3, and nice paper. My score is updated.PS: I still think that the (tiny) error bars are obfuscated because the line connecting them is the same thickness and color. I read the authors' response. The paper should in its final version add the precise explanation of how the two states interact and how a joint state definition differs from the current one. -----------------------------------------------------------Post-Rebuttal---------------------------------------------------------The authors have fully addressed my concerns. I changed the rating to a 7. ##### added after author response #####I appreciate the authors effort in trying to make their contributions precise and appropriate. The connection between ZO-signSGD and adversarial examples is further elaborated, which I agree is an interesting and potentially fruitful direction. I commend the authors for supplying further experiments to explain the pros and cons of the proposed algorithms. Many of the concerns in my original review were largely alleviate/addressed. As such, I have raised my original evaluation. EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference. (After the first revision) I have raised the score after the very detailed author response (thanks for that!), but this is also conditioned on the authors making the actual revisions promised in their response. I am still quite interested to check how well the method works in a setup with distant language pairs.  Revision: After the authors revision, I change my score since they addressed my main complaint about results using pseudogradient attacks ------------------------Post Author ResponseThank you for adding the ablation study and the attention based models. I enjoyed reading your work and it has answered some of the questions we wanted to explore in Capsule Networks.  === EDIT: post-rebuttal ===Thanks for the additional explanations. Thank your for your response and the revisions. The revised version of the paper includes many improvements. The results presented in Figures 1, 2, & 3 are much more compelling with the additional trials. I also appreciate the inclusion of the simple MDP example in the appendix, and the editorial changes made; the paper reads much more smoothly now.In light of the updates, I have changed my score from a 6 to a 7. ==========================================After discussing with the authors, they provided better evidence to support the conclusions in this paper, and fixed bugs in experiments. The paper looks much better than before. Thus I increased my rating. UPDATE:I've read the revised version of this paper, I think the concernings have been clarified.------- Edit: After paper additions I am changing my score to a 7. Revision:The approach of using robust features is interesting and promising, as well as the idea of training on multiple languages. Overall, the authors response addressed most of the issues, therefore I am not changing my rating. Updated Review: The authors have updated the appendix with new results, comparing against HER, and provided detailed responses to all of my concerns: thank you authors.While not all of my concerns have been addressed (see below), the new results and discussion that have been added to the paper make me much more comfortable with recommending acceptance. The formuation, while straightforward and not without limitations, has been shown in preliminary experiments to be effective. While many important details (e.g. robust baselines and ultimate performance) still need to be worked out, HPG is almost certainly going to end up being a widely used addition to the RL toolbox. Good paper, recommend acceptance.Evaluation/Clarity/Originality/Significance: 3.5/4/3/4Remaining concerns: - The poor performance of the baselines may indeed be due to lack of hindsight, but this should really be debugged and addressed by the final version of the paper.- Results throughout the paper are shown for only the first 100 evaluation steps. In many of the figures the baselines are still improving and are highly competitive... some extended results should be included in the final version of the paper (at least in the appendix).- As pointed out, it is difficult to compare the HER results directly, and it is fair to initially avoid confounding factors, but Polyak-averaging and temporal difference target clipping are important optimization tricks. I think it would strengthen the paper to optimize both the PG and DQN based methods and provide additional results to get a better idea of where things stand on these and/or possibly a more complicated set of tasks. ====Raising my score after the authors responded to my questions and added the HER results. === After rebuttal ===Thanks for adding the additional experiments (particularly with fully random embeddings) and result analyses to the paper. I feel that this makes the paper stronger and have raised my score accordingly. Reading thread and authors response rebuttal decision:=================================================I consider that the authors have perfomed a good rebuttal and reading the other messages and the authors response I also consider that my issue with clarity is solved. Hence, I upgrade my score to 7 and recommend the paper for publication. Based on the revision, I am willing to raise the score from 5 to 7.==========================================  ===After rebuttal: I thank the authors for addressing the comments in my review. It clarifies the questions I had about on the 2D3DS dataset (panorama vs. 3D points). Overall I feel this is a good model and have solid experiments. Therefore, I raise the score to 7. # [Updated after author response]Thank you for your response. I am happy to see the updated paper. In particular, the added item in section 1.3 highlights where the novelty of the paper lies, and as a consequence, I think the significance of the paper is increased. Furthermore, the clarity of the paper has increased. In its current form, I think the paper would be a valuable input to the deep learning community, highlighting an important issue (CF) for neural networks. I have therefore increased my score.------------------------------------------  -----------------------Update: Apologies for the slow response. The new version with more baselines, comparisons to CPC, discussion of NCE, and comparisons between JS and MI greatly improve the paper! I've increased my score (5 -&gt; 7) to reflect the improved clarity and experiments.  ////////////I would like to thank authors for providing detailed answers to my questions. After reading their feedback, I am now willing to change my score to accept. UPDATE AFTER REBUTTAL:I am still torn about this paper. On one hand, I still think that the topic and discourse provided by this paper is extremely important. On the other, the results - even after the revision - do not completely convince me. I might update my score after some discussion with the other reviewers.2ND UPDATE:After giving it some more thought, I find myself convinced that this paper has a contribution important enough to be accepted. I increase my score to 7. [addressed in the revision][addressed in the revision][addressed by adding WSJ experiments][addressed in the revision].  [short comment added in the revision][short comment added in the revision][addressed in the revision]. [fixed in the revision] [fixed in the revision][fixed in the revision][fixed in the revision][remains in the revision] After the discussion with authors, I am happy to recommend acceptance.  Begin revision comments:-----Given the revisions to this paper, I am more confident that it will be of interest to the community. The major contributions here I see is the removal of target networks given their approach. Given this I still have concerns on clarity and still am unhappy with the lack of confidence intervals in the main experimental section. I've increased my score to 7 to reflect my increase in confidence. ********************After authors response:Thanks to the authors for a detailed response. The introduction led me to believe that the paper solves a different task from what it actually does. I still like the algorithm and, given that the scope of the paper is limited to a few-shot learning, I tend to change my evaluation and recommend to accept the paper. It was a good idea to change the title to avoid possible confusion by other readers. The introduction is still misleading though. It creates the impression that APL solves a more general problem where it would be good enough to limit the discussion to a few-shot learning setting and explain it in greater detail for an unfamiliar reader. Some details also seem to be missing, e.g. I didn't get that the memory is flushed after each episode and could not find where this is mentioned in the paper. Update:I appreciate the through error analysis the authors have done in the revision, which addressed my major previous concerns. I've updated my score accordingly. UPDATE: Given the authors' rebuttal and the clear improvements to their paper, I've increased my rating of the work.======================= === after rebuttal ===The authors have addressed my concerns and I have updated my score accordingly. [UPDATE]. The authors addressed my concerns stated in my review above. I think the bibliography has improved and I recommend acceptance. ===after rebuttal===All my concerns are addressed. I will upgrade the score. I want to thank the authors for addressing my concerns.  I understand that their focus was not exactly the same as in previous work, but want to thank the authors for nevertheless adding the additional motivations and extra analysis.  I believe that this will help situate this work better within this area, and also allow better comparison with other studies.I have changed my overall rating from a 6 to a 7. I have read the authors' detailed rebuttal. Thanks. --- Update ----Given the author's response and the discussion, I'm going to raise the score a little. Although there are some valid concerns, it provides a clear improvement over Allamanis et al paper, and provides an interesting approach to the task. EDIT: I think dealing with the lower bound and including plots for all 55 games pushed this over the edge. It would've been nice if there non-zero scores on Montezuma's Revenge, but I know that is a high bar for a general purpose exploration method. In general I think this approach shows great promise going forward score 6--&gt;7 -------The rebuttal and revisions addressed some of my concerns so I am increasing my score to 7 _____________________________________________________________________________________________________________________________________________I read the answers of authors. I increased my rating. (score raised from 6 to 7 after the rebuttal) Edit: The authors have made a significant effort to address my concerns, and I'm updating my score to 7 from 5 in response. Thank you for the clarifications. I feel that the material is now much more convincing after seeing the architectural presentation. It is illuminating to note that one can break up content and style to capture their essence as can be seen in figures 2, 3, 4 and 5 in the appendix. Fig 2 uses multiheaded attention to compute similarity between ref. embedding and randomly initialized tokens - this seems to be a new addition to the previous GST works (Skerry-Ryan et al 2018 and Wang et al 2018). Overall, This work exhibits a very high level of application - attention based seq2seq modeling with Tacotron setup, and manipulating content and style with instructive use of techniques from the formulation to the architectures used . I rule this as a clear accept ---Edit after rebuttal: Increased score from 6 to 7. (increased score from 6 to 7) ## Post-rebuttalI've read the authors' response and other reviewers' comments. The response and the updated version clarify my concerns. So I slightly increase my score. -------Based on the author response, the rating has been updated (see comments below). Edit:- the authors meaningfully addressed the above points. I have raised my score accordingly. *******************************************Final decision: I would keep my score unchanged. As for Principle 1, the authors said that upholding Principle 1 is impossible with no supervision, and they proposed to uphold Principle 1 approximately. This is acceptable to me as they build on ideas from positive-unlabeled learning.This paper is clearly written and well organized. Both empirical and theoretical analysis is provided. The feedback addressed my concerns well. After rebuttal:Thanks for your response. I read authors response to my question and as well as other reviewers feedback. I will keep my rating as it is.* no effect on the rating: a point on the question on the *generalized* zero-shot learning as a downstream task, is if employing this approach, improve the performance on unseen classes while is not negatively impacting the performance for seen classes.  After rebuttal- I read the response of the authors. The response addresses most of the concerns raised in the reviews. #########################################################################POST-REBUTTAL RESPONSE:I found the author's further work on (1) the mode-collapse experiments (2) quantitative comparisons with other slicing methods interesting and convincing. I have decided to increase my score. I reiterate that I have not checked the mathematical content of this paper in detail. [Post rebuttal] I have increased the score of my review to 7. Below is a copy-pasta of my comments post discussion:While my original concern about how much sampling affects FSL is still not fully addressed, I think it is not a trivial question to answer and a full exploration of the topic could constitute its own paper. So although I'm not fully convinced about the motivation of this paper, I think the thorough experimental evaluation along with the strong empirical results together warrants publication. From my perspective, a particular important strength of this paper is its ablations. I'm fairly convinced that the presented implementation is likely the best way to implement the idea of cross-episode attention + distillation. I think the baseline proposed by the AC makes sense. It would be great if that could be incorporated into the final version of the paper. A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity. Results on wider datasets could corroborate this hypothesis. ---I was the only reviewer who happened to imagine their threat scenario had some importance.After reviewing the authors' changes and comments, I feel that the threat scenario in the revision still is insufficiently motivated/explained.  I'm downgrading to "good paper". [Post Rebuttal Comments] Authors have done a good job for addressing my concerns, especially the additional ablation studies regarding the performance of the expert assignment module. I'm updating my score accordingly for recommending acceptance. UPDATE:After extensive discussion with the authors, I'm raising my score to a 7.  I believe their revision will adequately address the concerns I've raised.I think this paper clearly identifies and illustrates the qualitative advantages of assistance, and that this is a novel and significant contribution.In particular, I do *not* believe, as other reviewers seem to, that any of the following are sufficient reasons for rejecting this work:* The wealth of prior work on variants of reward learning and assistance * The lack of a comprehensive survey or categorization of such work in this submission* The lack of further resultsAfter reading the other reviews and responses, I am more confident that this paper makes a valuable contribution, although I stand ready to be challenged by other reviewers.  This is because the authors have argued that the qualitative benefits they describe in sections 4.1/2/3 have not been available to any of the many previous works reviewers mentioned, and no reviewers disputed this.  Furthermore, I did not find the reasons provided for rejection to be very relevant to the goals of this work.  So overall, I do not believe that other reviewers have made a strong case for rejecting this work.In my mind, the best argument would seem to be simply that the contribution is insufficient.  I think this is a common criticism of papers that do not adhere to a conventional format or "type", but in this case, it seems unfair.  I believe the intellectual contribution of this paper is rather modest, but nonetheless novel and significant.  And I found this motivation for the work quite compelling (emphasis mine):> This existing literature is exactly why we wrote this paper: almost all of the existing literature on learning without reward functions can be captured by the reward learning paradigm as we formalize it. But (as we show) the assistance paradigm can enable significantly better behavior from the agents we train! **We are hoping to influence researchers to put more effort into algorithms for the assistance domain, in order to realize these qualitative benefits, instead of continuing to work in the reward learning paradigm as they have done so far.**I would encourage the authors to explain this goal in their revision, and make sure their claims about the superiority of assistance are appropriately modest.  Overall, I think the qualitative benefits of assistance presented provide a compelling argument for more work in the assistance paradigm, *given the paucity of such work*.  But I think the overall message of the paper should be: "Given these advantages, and the lack of work on assistance, there should be more work on assistance, since it seems promising and neglected", and not "Assistance is better, so why would you do reward learning?"  And my first impression of the paper was closer to the latter.  END UPDATE ##########Post-Rebuttal Feedback########I appreciate the author's thorough response and I think the additional experiments on robustness/stability make the paper stronger, so I decide to raise my score to 7. For the future version of the paper, it would be great to see more comprehensive experiment results that show the improved robustness/stability in the main text. Modifications after discussion:Increased score by one since the revised paper clarifies the missing details on inference and also improves the motivation. ### After author responseI thank the authors for their detailed response to my comments. I do not agree with the complexity of FGW being solved in $n^4$ which should be more related to $n^3$ for the type of distance considered in the paper (see analysis in [1]), but yet the point is still sensible for considering the minibatch version. My other comments have been adressed, and I am changing my note to the score of 7.  Edit: I have raised my score from 6 to 7 after the rebuttal. **Update**I think the manuscript has been further improved now and I improve my score to 7. Also since I think results on public datasets in new domains other than images may be helpful for the wider research community. Keep in mind, as written below, that other reviewers more familiar with this research field may be more  able than me to judge the evaluation quality and whether the evaluation contains too substantial flaws to allow publication.  ----Update and final recommendation. I still recommend acceptance of the submission. The paper is well written, results stand on its own and the numbers improve in the way described. In light of the missing comparisons to other works pointed out by the fellow reviewers I have lowered my score because I think better calibrates with the significance of the work. Combination of downstream tasks is not novel but this combination I have not seen and so even bearing similarity with other approaches the paper still stands on its own. Thanks to the reviewers and authors for their responses. *Updated score based on below discussion**Updated score again based on below discussion* ***********************************Post rebuttal: The author has addressed most of my questions, and the SQuAD v2 test result is on par with the state-of-the-art, partially indicating the proposed method is effective. So I am happy to increase my rating and champion for the acceptance. **After rebuttal**:  I've read the authors' feedback and my score remains the same. edit after rebuttal: The idea of the paper was interesting to me but some motivations or choices were unclear. After reading the rebuttal and other reviews, the authors have addressed most of my concerns. Therefore, I am ready to increase my score.====== _Post-rebuttal_: I do appreciate the authors addressing my comments and updating their paper. Furthermore, the authors also addresses a lot of concerns of my fellow reviewers. As this was a good paper to begin with, I am keeping my score of recommending an accept. Following the author response, I have updated my score to an accept. The key contribution is likely to be of interest to the ICLR community and the paper is well executed. --- *************I want to thank the authors for their detailed comments and explanations. I agree with (3) that a global floating representation is not required, but different settings of bitwidth may require additional data paths designed in the hardware (therefore a unified bitwidth for exponent and mantissa is recommended). Given revised related work and additional practical implications, I would like to champion this paper.  =========================== Post-Rebuttal ===========================My questions are well addressed in the rebuttal. I acknowledge the novelty in the paper and the convincing empirical results. The authors have committed to include more datasets to enhance the result completeness and add relevant but missing related work. Taking all these into account, I am adjusting my final rating to Accept. ## Post rebuttal commentThe authors have successfully answered my concerns and added the necessary experiments. I think the paper should be accepted as the support set idea is an important one to improve upon the simple contrastive idea (in particular to soften the set of positives).  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------After reading the rebuttal, I think the authors have well addressed my concern, thus this paper is good to be accepted for ICLR and I raise my rating from 6 to 7.   **Post-rebuttal**> I want to thank the author for addressing my concerns. Overall, this is an exciting paper with comprehensive ablation studies and analysis. It provides an effective multi-task training method for multilingual tasks. The authors also support the method with a strong mathematical foundation. Thus, I would like to keep my positive rating.  I change my rating after looking at authors response.  ### Post rebuttalMy previous rating still applies. If accepted, I encourage the authors to more clearly state in the final version that their method is not applicable (without additional - and arguably inelegant - random augmentation) to graphs with degenerate eigenvalues and in particular symmetric graphs. The necessity of taking the absolute value to ensure invariance to the sign of the eigenvector should also be more clearly stated.I share reviewer #1's concerns about corollaries 2.5 and 2.6. These should be clarified or removed from the final version. Nevertheless, I think the novelty of the approach justifies acceptance. It is a simple modification that may bring the expressive power of graph networks closer to that of pixel CNNs. ### Post rebuttalThe authors addressed all my concerns and strongly improved their paper. I think it is now a good candidate for acceptance, as it provides an interesting alternative to / variation on tensor field networks. I raise my rating from 4 to 7. **Post-Rebuttal**After reading the rebuttal, the updated manuscript and the other reviews, I became *less* convinced about this paper. While I remain positive about the conceptual idea of learning a unified label space. The authors did not successfully convince me in improving the understanding of the method. The pseudo-code alone does not make the  algorithm better to understand. It seems that the space T is insanely big (100x100x100 when three datasets of 100 labels are used). Also, while the authors do add a requested baseline, it is hardly compared (afaik only in the sentence that the expert human obtains 659 labels, while the learned space contains 701). Also note that the 'human expert' is a *"best-effort" mapping, which can be a good starting point.* according to the RVC dev kit. So, this might be an overclaim.Finally, please carefully proof read the paper, main typos remain.  ---------------------------------------------------------------------------------------------------------------------------I believe that the authors have adequately addressed my concerns, and have made a good effort to improve the quality of their work and so I am raising my score.  ----------FINAL-RATINGI read the other reviews and authors' replies carefully.First of all, I would acknowledge the effort of the authors in replying to reviewers' concerns.About my points, I agree with other reviewers that the quantitative analysis is a bit limited (I think it is also the main criticism), but the introduced ablation and the new Figure 6 (numerically comparing against other SOTA methods) are convincing. The authors solved also my other concerns, and so I raise my score; I think the study of this representation is interesting, and well fit the audience of ICLR.Thanks to the author for their availability, and best of luck with their work! **Post-discussion update:**The authors have addressed all my comments. It is unfortunate that a comparison with diffusion graph augmentation could not be added, but I understand the reason provided by the authors and this anyway does not significantly detract from the presented results. Since my original score already marked this as a good paper recommending acceptance, it is left unchanged. -----------------EDIT after rebuttal: Thank you for the response.  After responses: I now understand the paper, and I believe it is a good contribution. ================================================ Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7 Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes. ----Update: after all the discussion, I'm lowering my score a bit while still hoping the paper will get published. I'm satisfied with the results and the improvement of the paper. I still find it a bit surprising that the pairs of literals/leaves in the tree are a good approximation for the program itself (as shown in one of the ablation study). The discussions with the two other reviewers have shown me that my enthusiasm for the matter at hand may have clouded my judgement.Although I maintain that the paper may be helpful for practitioners, in particular because of its identifying and comparing different kinds of architectures, the other reviewers do see the lack of clear novelty, the writing and some over-simplifications as important issues. Therefore, although I still believe the paper to be a good paper, I was probably wrong in my first assessment and changed it based on the elements presented in the discussions. REVISED: I am fine accepting. The authors did make it a bit easier to read (although it is still very dense). I am also satisfied with related work and comparisons Revision: Score updated from 6 to 7. Update following author and reviewer discussion: I agree with others regarding the weakness of the empirical comparison to pseudo-counts in particular, but still believe that the paper deserves to be accepted due to the fact that (1) some of the results are really good, and (2) this is a simple original idea that has the potential to drive further advances (hopefully addressing the empirical and theoretical limitations of the current work) --------------------------------Update--------------------------------The authors have provided a detailed response to my concerns and have fixed many of them in their revised version. I verified parts of the proofs in the appendix (Theorem 3.1 and its Corollaries). I congratulate the authors on their work and recommend acceptance! The authors have updated the paper and clarified some things, and now my impression of the paper has improved. It still feels a little incremental to me, but the potential application areas of these sorts of models are quite large and therefore incremental improvements are not insignificant. This paper suggests some natural follow-up work in exploring Hellinger distance and other variations for these models.  Update after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper. Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.------ Update==========Upon reviewing the paper revision and the author comments to my and the other reviewers' comments, I will revise my suggestion to that of acceptance. As I said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified. They have also increased the rigor of their experimental results by providing variances in the plots.I think this work will be of interest to the community. The previous version of the paper was not clear enough in the motivation and uniqueness of the work. After a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work. The current paper is much better, so I would like to raise my score to 6. My revised review is: [orginality and significance]+ The paper deals with a challenging navigation problem where natural language instructions can be underspecified and the environment is complex---thus a correct reward function being extremely hard to craft. + The paper proposed to use a &lt;instruction, state&gt; discriminator D to compute a pseudo reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in an adversarial way---with expert supervised data. The idea is neat, and its effectiveness is empirically supported by extensive experimental results.  [clarity]+ The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1 and -2). The experiments are also well-documented, including training and testing details, results and analysis.   [quality]+ The paper was not clear at certain points but the authors had helpful discussions with me and the paper was revised accordingly. + The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL.  Overall, it is a good paper. # Update post author responseThanks to the authors for the detailed response. The authors have satisfactorily responded to my main criticisms of the paper (primarily about the non-standard evaluation regime, and secondarily about the motivation and strength of the theoretical results), so Im raising my score to a 7, though I do think the paper would be further improved with references to Appendix G in the main body of the paper. # UpdateThough the theoretical analysis is a bit weak, I think the experiments are quite good. The code can also run without any issue, which is a significant contribution in my opinion. ===================Post Rebuttal Update:After the discussions and reading the other reviews, I lower my score by one point. I could not find the changes to the manuscript announced by the authors during the discussion, especially the additional intermediate results necessary for the theorem's proof pointed out by reviewer 1. # Edit after author responseI thank the authors for their detailed response and for taking into account many of my comments. One issue that remains is the disconnect between toy and natural scenarios. There is some added discussion, which is helpful, but actually sketching out and conducting experiments, including intermediary steps moving from a full toy example to the naturalistic case, could be very helpful here.  --------------------------I agree the comment from the author mostly for my concerns.(1) From the interpolated images, a point in the latent space seems to be matched to corresponding image in the image distribution, which means that it does not simply memorizes the images.(2) By seeing the figure 8, I think this work can be tested in image generation task, either. In final version, I strongly want to see the Pure Image generation result. Based on the comment, I changed my previous rating. ---------Revision------------I have read the author's response and other reviews. I am not changing the current review.I have one technical question. In the new generalization bound (Proposition 3.1), the authors claim that the product of Frobenius norms is replaced with a sum. However, I don't see any sum in the proof. Could the authors please clarify this? * I have revised my score upwards due to the authors response to my concerns --- particularly the addition of new results on graph classification. The original review remains here, and I respond to the author's response below.  Updated in response to author response: the inclusion of experimental comparisons with linear algebraic sparsification baselines, showing that the proposed method can be significantly more accurate, strengthens the appeal of the method. --------------After seeing the authors engage at least a little with the related semantic parsing literature, I've increased my score to a 7. ================After rebuttal:Thanks the authors for clarification. I have read the author's responses to my review. The authors have sufficiently addressed my concerns. I agree with the responses and decide to change my overall rating  Post-rebuttal update:The paper was substantially improved. New experiments using real objects have been included, this clearly demonstrates the merits of the proposed method in robotic object manipulation. Revision:The addition of new datasets and the qualitative demonstration of latent space interpolations and algebra are quite convincing. Interpolations from raster-based generative models such as the original VAE tend to be blurry and not semantic. The interpolations in this paper do a good job of demonstrating the usefulness of structure.The classification metric is reasonable, but there is no comparison with SPIRAL, and only a comparison with ablated versions of the StrokeNet agent. I see no reason why the comparison with SPIRAL was removed for this metric.Figure 11 does a good job of showing the usefulness of gradients over reinforcement learning, but should have a better x range so that one of the curves doesn't just become a vertical line, which is bad for stylistic reasons.The writing has improved, but still has stylistic and grammatical issues. A few examples, "therere", "the network could be more aware of what its exactly doing", "discriminator loss given its popularity and mightiness to achieve adversarial learning". A full enumeration would be out of scope of this review. I encourage the authors to iterate more on the writing, and get the paper proofread by more people.In summary, the paper's quality has significantly improved, but some presentation issues keep it from being a great paper. The idea presented in the paper is however interesting and timely and deserves to be shared with the wider generative models community, which makes me lean towards an accept. EDIT: I still think the compositions under consideration are the simpler ones. Still with the new experiments the coverage seems nicer. Given the authors plan to release their source code, I expect there will be an opportunity for the rest of the community to build on these, to test TRE's efficacy on more complex compositions. I updated my scores to reflect the change. Thanks for the feedback and the revisions to the paper. I am satisfied to the response. Congratulations on the great work! Post-Rebuttal:I would like to thank the authors for the insightful rebuttal! The authors were able to address my concerns adequately and I believe that the revision improved the quality of the paper quite a bit. Therefore, I stand with my initial recommendation and due to the reasons stated above, I endorse accepting this paper. ###  POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. I think the paper is stronger and should be accepted. --REVISION--I would like to thank authors for their effort to improve quality of images. In my opinion the paper is nice and I sustain my initial score. xxxxxxxxxxxxxxxxxxxAppreciate the authors' rebuttal, updated my score. # Update (2018-11-29)Given the substantial author feedback, I'm willing to raise my score. The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7. Update 11/27/2018Thanks for the authors for updating the paper. The updated paper have more clear comparisons with other models, with more &amp; stronger experiments with the additional dataset. Also, the model is claimed to perform multi-step interaction rather than multi-step reasoning, which clearly resolves my initial concern. The analysis, especially ablations in varying number of iterations, was helpful to understand how their framework benefits. I believe these make the paper stronger along with its initial novelty in the framework. In this regard, I vote for acceptance. ************ Revision ***********The authors' updates include further quantitative comparisons to Fader Networks and ablation studies for the different types of losses, addressing the concerns I had in the review. Hence I have boosted up my score to 7. Update: I thank the authors for their response and additional experiments. I am increasing my score to 7. If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments.--- = Revised after rebuttal =I thank the authors for their response. I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from. However, I do encourage further work to1) Provide stronger empirical results (these are not too convincing).2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators. ########RevisionI would like to thank the authors for a thoughtful revision and response. I have updated my score to a 7 and think this paper is a worthy contribution to ICLR. The new drawback section is well written and informative. [UPDATE] the authors address my concerns in a detailed way, and the updated revision is rather robust, therefore, I decide to change my score to accept. Response to author comments:I would like to thank the authors for answering my questions and addressing the issues in their paper. I believe the edits and newly added comments improve the paper. I found the response regarding the use of your convergence bound very clear. It is a very reasonable use of the bound and now I see how you take advantage of it in your experimental work. However, I believe the description in the paper, in particular, the last two sentences of Remark 1, could still be improved and better explain how a reasonable and computationally feasible n was chosen.To clarify one of my questions, you correctly assumed that I meant to write the true label, and not the output of the network.  ****Post-rebuttal update: The authors have made significant revision to their work, which sufficiently addressed all my concerns. I have upgraded my score accordingly and I am willing to support the acceptance of this paper. ==== Update following rebuttal and discussion ====The AC has raised an important point that the paper does not discuss its reliance on infinite precision. While using infinite precision is not necessarily wrong, it does make the paper's result less relevant in practice, narrowing its contributions to being mostly about theoretical aspects. More importantly, by hiding this technical detail deep within the proofs, the authors missed the opportunity to discuss the difference between the number of bits used by the parameters and the number of edges (operations) necessary for memorization. It seems to me that the number of bits is linear with the number of examples, even as you can reduce the number of edges, which does have practical implications about the utilization of the memory bought by depth. Moreover, if the authors had discussed this topic, it would've been accepted more easily.Despite the above flaws, I still find value in this article, even if its contributions are mostly theoretical. Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly. Post-rebuttal update: The review process has identified several issues such as missing citations and lack of clarity with respect to aims of the paper. Although the authors have failed to update the paper within the rebuttal period, their responses show an understanding of the issues that need to be addressed as well as a broad appreciation of work in EC that would be included in a final version, making it a useful resource for the wider ML community. On top of this they will include an even larger amount of empiricial data from the experiments they have already run, which is a valuable resource considering the amount of compute needed to obtain this data.--- ******************************************************After reading authors' responses, I decided to change the score to accept. It got clear to me that this paper covers broader models than I originally understood from the paper. Changing the expression to general forms was a useful adjustment in understanding of its framework. Comparing to other relaxation technique was also an interesting argument (added by the authors in section H in the appendix). Adding the experimental results for N=3 and 4 are reassuring.One quick note: I think there should be less referring to papers on arxiv. I understand that this is a rapidly changing area, but it should not become the trend or the norm to refer to unpublished/unverified papers to justify an argument.  ---- Post author feedback comment ---- I raised my rating to 7 as the paper itself is solid, main concern as another reviewer points out is it may be a bit too specialist for ICLR. If the AC decides to reject based on this fact I am ok with that as well. I think it would be helpful to add more ablation (deformation-only results for all cases) and experiments with different numbers of bases in the final version. If that's added it will strengthen the paper. --- --- After rebuttal Authors addressed most of my concerns. The paper has merit and would be of interest to the community. I am increasing my score. [Revision] Thanks for the replies. I still believe experiments on more tasks would be great but will be happy to accept this paper. Response to rebuttal:It is good to know that the authors have a new modified VAE posterior distribution for the stochastic model which can achieve significant gain over the deterministic model. Is this empirical and specific to this dataset? Without knowing the details, it is not clear how general this new stochastic model is.I agree that it is worthwhile to test the model using the 45 minute dataset. However, I still believe the dataset is very limiting and it is not clear how much the experimental results can apply to other large realistic datasets.My rating stays the same. Revision:After reading the author's response, I think most of the points were well addressed and that the repulsive loss has interesting properties that should be further investigated. Also, the authors show experimentally the benefit of using PICO ver PIM which is also an interesting finding.I'm less convinced by the bounded RBF kernel, which seems a little hacky although it works well in practice. I think the saturation issues with RBF kernel is mainly due to discontinuity under the weak topology of the optimized MMD [2] and can be fixed by controlling the Lipschitz constant of the critic.Overall I feel that this paper has two interesting contributions (Repulsive loss + highlighting the difference between PICO and PIM) and I would recommend acceptance. Revision: after reading rebuttal (as well as to other reviewers), I think they addressed my concerns. I would like to keep the original score. ====update: The bound comparison added value to the paper. It strengthens my opinion that this work deserves to be published. I therefore increase my score to 7. Revision:The authors have thoroughly addressed my review and I have consequently updated my rating accordingly. Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this. Update 2018-11-23: I am reducing my rating to 5 (from 6) due to the absence of author response regarding a potential revision addressing my comments/questions as well as those from other reviewersUpdate 2018-11-27: I am increasing my rating to 7 (from 5) after the authors responded to reviewers' comments and uploaded a revised version of the paper Update:I have updated my review to mention that we should accept this work as being concurrent with the two papers that are discussed below. Update: Incorporating the AC/PC decision to treat the paper as concurrent work.  ======================= after rebuttal =======================I appreciate the authors' efforts and am generally satisfied with the revision. I raised my score. The authors show advantage of the proposed ProxQuant over previous BinaryConnect and BinaryRelax in both theory and practice. The analysis bring insights into training quantized neural networks and should be welcomed by the community. However, I still have concerns about novelty and experiments.- The proposed ProxQuant is similar to BinaryRelax except for non-lazy vs. lazy updates. I personally like the theoretical analysis showing ProxQuant is better, although it is based on smooth assumptions. However, I am quite surprised BinaryRelax is so much worse than ProxQuant and BinaryConnect in practice (table 1). I would encourage the authors to give more unintuitive explanation.-  The training time is still long, and the experimental setting seems uncommon. I appreciate the authors' efforts on shortening the finetuning time, and provide more parameter tuning.  However, 200 epochs training full precision network and 300 epochs for finetuning is still a long time, consider previous works like BinaryConnect can train from scratch without a full precision warm start. In this long-training setting, the empirical advantage of ProxQuant over baselines is not much (less than 0.3% for cifar-10 in table 1, and comparable with Xu 2018 in table 2). UPDATE (Dec 4, 2018):I have read the author response and they have addressed the specific concerns I have brought up. I am overall positive about this paper and the new changes and additions so I will slightly increase my score, though I am still concerned about the significance of the results themselves. ######Update after revision: I have looked over the revised paper and believe the authors have addressed most issues that were raised by the reviewers, especially by describing in more depth the relation of their approach with autoregressive models and self-attention and mentioning the runtime differences of their model compared to a normal CNN.I have, therefore, raised my score to 7. #############After reading the rebuttal I confirm the initial rating. ### VerdictThank you for addressing all my concerns. This gives me the confidence to slightly increase my score. One more small thing, for future work, you might also consider incorporating clipping the metrics to the input range. ---------------------Update: The authors have adequately addressed my questions, and I am happy to maintain the rating of 7. --update---I am upgrading to 7: Good paper accept; as my concerns have been addressed by the additional experiments.The proposed method is not yet a drop in replacement for BatchNorm in general, but it can be useful in specific circumstances, i.e. small batch-size training. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------UPDATE: The author has addressed most of my concerns, but regarding the motivations and the benefits for the community, I still keep my score. ##########################################################################Updates are appreciatedHi, Authors,I appreciate the updates you've made to the paper and the responses to my questions.  You're quite correct that RN50 and ImageNet is sufficient to illustrate deficiencies.(I'd still want to see broader experiments for claims of some new method overcoming these deficiencies, though!  When broadening scope to other tasks, I'd expect the authors of prior methods designed for vision tasks would be okay with use of their methods as baselines if there are no methods designed specifically for those new tasks.)With this in mind, I'll update my rating to a 7. ##post-rebuttal##I share the concerns of others reviewers that this paper has a limited novelty and narrowed scope but I think the authors have addressed other issues/concerns quite well. In my opinion, the key contributions mostly come from the insight and experimental results, and less so on the heuristic itself. Thus, I would adjust my rating to 7. ##############Post Rebuttal###############My concerns are addressed by the authors. I'm keeping my original rating. -----------------------------------------------------------------------------------------After reading the authors response to all reviewers, I believe all questions to be sufficiently addressed. I will therefore (happily) raise my score. ___### After author response and paper revisionFirst, I would like to congratulate the authors for their amazing work during the rebuttal period.My main concern, the comparison against [1], has been perfectly addressed in appendix G (both "conceptually", by highlighting the differences between both methods, and showing the advantages of the proposed SGDP and AdamP, but also empirically on ImageNet, where a fair comparison with proper hyper-parameter tuning has been performed).The authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of SGDP and AdamP. Finally, they also added the experiments with higher weight decay, showing that indeed 1e-4 was the best value.With all these changes, I think the paper is good and I recommend its acceptance. ##########################################################################Post-rebuttal: I have increased my score to a 7. I have laid out my reasoning in response to the author's comments. POST-REVISIONThanks for the revisions made to the paper, particularly for elaborating on the heuristic to speed-up the inner subroutine, and the clarification on the generalization bound.Also thanks for the running time numbers. Would be great if you could report them in the paper (if you haven't already).I'm revising my score for the paper to 7. Post rebuttalThe authors addressed the concerns around the clarity of the paper and added useful additional experiments. I will increase my score to 7. *Post-Rebuttal Evaluation*I have carefully read the response provided by authors and checked the revised manuscript. I confirm my preliminary acceptance rate. *** after reading rebuttal *** All my concerns are addressed, and I decide to improve my evaluation. **The revision answered my concerns about the experiment by enlarging it, which is why I upgrade the score I gave. I think it is a very good and interesting paper.** **Update From Author Reply**I am grateful for the author's replies, edits, and additional evaluation, all performed within limited time. This helps me feel confident in my accept (7) recommendation. My reason for not giving a higher score remains the limited experiments (which is likely not something to be addressed in two weeks), but even so I think the work is quite worthy of being accepted. The methods are a significant contribution and the experiments are sufficient to demonstrate they work. ======After author response: I have read other reviews and the revised version. I think the paper's overall quality has improved. I decided to change my score to 7. **Update** : Since most of my issues have been addressed, I have changed my rating from 6 to 7 ============================================The authors address most of my questions and concerns in the response, and update the manuscript. I decide to increase my score. ### Post-rebuttal updatesGiven the answer of the authors. I will raise my reviews to 7 assuming the authors will add more about comapring with current sota methods, as discussed on this review. ======== score changed ==============My major concern has been addressed by the reply from the authors.  The revised paper has been improved.    **Post Rebuttal**I thank the authors for the quick replies and updates to the paper. I keep my positive score.--- -----------------------I thank the authors for their detailed reply. The revision after the initial review solves the concerns and questions I had. Very good work. I vote for accept. update after rebuttal: the authors answered all my questions to my satisfaction. The rebuttal provide valuable information that strengthens the original paper. While some of my concerns are not resolved in the rebuttal, they require additional experiments and may be beyond the scope of rebuttal. ---Thanks to the authors for the clarifications. I have read the other reviews and responses and still believe that the paper is a good contribution. Therefore, I am keeping my score. ________________________Thanks! The authors' update was well-received. I like the way how ARM formulates the unlabeled adaptation. Though "generalizing" meta-learning from labeled to unlabeled adaptation is natural and simple, ARM (proposed in this work) is crucial to highlight this and present the results to the community. I increased the rating to 7. Post-rebuttal comments: The authors have addressed my concerns in their rebuttals. All reviewers give positive comments to this paper. So I would like to give an accept to this paper. =====updates========Most of my concerns have been addressed by the rebuttal and all the other reviews are positive. l will remain my original recommendation. ==============I have read the authors' rebuttal information. The authors have addressed my concerns with additional ablation studies and experiments to verify the effectiveness of the proposed multi-hop transformer. And also some experiments are provided  to illustrate whether the most attended object is the most important one. Therefore, I am still standing on the previous justification for accepting the submitted manuscript. update 1: I thank the authors for the rebuttal. My questions and concerns were appropriately addressed. However, other reviewers raised some concerns regarding the experimental set-up that I have missed. Thus, I will keep my score as is -- 7: good paper, accept. Edited score after author comments. -- After rebuttalThank you for revising the paper. I've read the revised section, and stand by my original evaluation.  ###update###I have read the other reviews and the author feedbacks. I would like to keep my rating. ---------------------------------------------post-rebuttalI would like to thank the reviewers for their efforts to improve the draft. Most of my concerns were resolved. However, I agree with other reviewers on the fact that the proposed method only present advantages in certain limited networks and scenarios.  To calibrate this weakness, I downgrade my score to 7. Overall, I think the paper explored an interesting and promising direction to improve network robustness using second-order regularization and solid progress was made. I will recommend accepting the paper for publication. ----- Post Discussion ----I have updated my rating for the paper after the authors have provided additional discussion and experiments to address my concerns. Edit: Updated score to reflect the changes from the revision. ############################# Update after rebuttal ###############################The authors response addresses my concerns and I would like to update my score to 7. The paper presents insightful findings about FBF, and proposes a simple and effective method for stabilizing single-step adversarial training. This is useful not only for FBF but also for other single-step defenses. Although the gain in robustness is marginal, stabilizing single-step training is useful. The authors also propose a computationally efficient method of achieving robustness similar to PGD training.  Update:Thank you for the answers to my questions and additional experiments! ----*Update as of December 5th. I would like to thank the authors for clearing up my concerns and would like to raise my score from 6 to 7. I think this paper would be a good poster that can provide complimentary insights through different experiments to the recent paper of Hermann et al. NeurIPS 2020. ########################################################################################The rebuttal addressed my concerns - I increased the score Edit: The authors have not responded to any of the reviews, i lower my rating to 4 Edit2: Oh there was a misunderstanding, i probably was not logged in and didn't see any comments and reviews. I raised the rating and will read the answers and will rate again.Edit3: After reading the rebuttals, i raise my rating to 7  I have read the rebuttals and other comments and maintain my rating of the paper.  ## Post answerMy questions are appropriately answered and I appreciate the addition of section 3.4. I think my current score accurately reflects my evaluation of the paper, with the remaining concern being the magnitude of the contribution. =======I read through all the reviews and rebuttals and I could better understand and evaluate the paper. I updated my score to 7. Given that this replay scheme works fairly well (intuitively, empirically), easy to understand and implement, fairly sufficient amount of empirical experimentation, I would like to see the paper accepted (and adopted and improved by others).One more comment about staleness.I think staleness is a proxy measure for the (unmeasured) score of the 'current' policy on that level. So I would like to see (in future or revised version) some experiments that measure how well staleness measure correlate with such score. Further, the way staleness is designed properly reflects how the score degrades as the level isn't played.Some idea.It would be nice to make a connection to multi-task learning where tasks share some similarities. Currently, level is somewhat 'linearly' defined. If an agent plays level x, then staleness for level x' (something similar to x') doesn't have to be updated a lot compared to another task which might be dissimilar to level x. Hence, some similarity measure can be further employed (or learn a metric). **POST-DISCUSSION UPDATE**I want to thank the authors for correcting my misunderstandings, answering my questions, and providing additional material. As a consequence of this, I have raised my score to "Accept". To answer your question about what would be needed for a higher score: For a strong accept recommendation, I would have expected a mix of several additional things such as a clear impact outside of own subfield, code availability at time of submission (to evaluate how easy it is to reproduce the results and re-use the code), or more additional theoretical justification (in the sense of new formal guarantees for at least certain aspects of the proposed method). While not directly working in this subfield, I still think this work is solid and worthy of publication. Post Rebuttal:Clearly I was too cavalier in suggesting alternative bin selection methods, as the authors have clarified. I think the paper is much improved by the comparison to other techniques such as ECE_debiased. The rebuttal also made me re-read with the view of seeing the demonstration of how bias varies with sample size as a major part of the contribution. In addition, the concrete demonstration of the relevance of the discussion via fig. 2 is nice.On the whole, my opinion has shifted to be much more positive. I have been rating the paper somewhere between 6 and 7, and in such a case I would rather err on the side of acceptance. Two asides:1. I only realised this now, but I had misentered my confidence - it should have been 3 and not 5! I have corrected this now.2. Of course, actual constants matter in practice, but one way to select number of bins by considering errors in $\overline{y}_k$ is that the squared error in each of these scales as $B/n,$ which, when added over $B$ bins induces error at scale $B^2/n$. With the same logic as the maximum number of bins in ECE_sweep, this suggests one way to set $B$ to be as $B = \lfloor \sqrt{n} \rfloor,$ which controls this error term. Of course, this is only a heuristic, and the analysis is incomplete because it's not accounting for correlations between these errors and $(f(x_i) - \overline{y}_k).$In a larger sense, I still think that just looking at $15$ bins because that's all that has been looked at in the literature is not quite the right thing - basically I think that when studying an autotuning method, such as ECE_sweep, it is important to illustrate how it performs with respect to simple strategies based on some basic heuristics. This was the reason I wanted to score the paper at $6$, but I didn't bring this up when the authors asked for references and so felt it would be unfair to penalise on this basis.----- UPDATE: I thank the authors for their detailed feedback. The authors have addressed a number of concerns and I've increased my score to accept. I continue to think that the empirical section could be easily improved with additional domains, sensitivity to noise, etc. and a careful running example (Fig 1 unfortunately isn't). However, the current draft seems sufficient for publication due to the interesting algorithm (which is novel, as far as I can tell). **EDIT **The authors have addressed my concerns, and I have increased my score. After author response:Thanks for the response! After reading other reviews, I feel the novelty of this work is somewhat limited and it would be useful to highlight the differences with respect to previous work. Also a discussion on when your method is preferred over existing proposals. I am also decreasing my confidence score after reading other reviews. ##########Update##########The authors have addressed many of my concerns. Moving Table 1 into the main text and adding the new columns is an important change, and it seems that the authors have improved various aspects of their presentation. I'm raising my score to a 7. --- Edit: After rebuttal ---I thank the authors' for their detailed response to my questions and for clarifying so many aspects of the work! I also appreciate the time taken to make the minor corrections that help with the presentation. My main concerns have been sufficiently addressed - specifically the references to the dips in performance have eased my concerns with the validity of the experiments. The ablation with the beta parameter is also quite important I think and at least helps clarify that the issue for kickstarting may indeed be one of reaching a local optima.  Given the substantial effort (including implementing a new baseline) that went into the rebuttal I am willing to increase my score and recommend the paper be accepted at this venue. Post rebuttal update: The authors have addressed my concerns and the revised submission is much clearer, so I'm increasing my score to 7. # post rebuttalI have no further concerns and increase the rate to accept. ***Post-discussion period comments***I am pleased with the revision the authors have posted during the discussion phase. I am also satisfied with the authors' response to my comments and to the comments of the other reviewers. I therefore recommend that this paper be accepted into proceedings of ICLR 2021. The authors addressed my concerns and it seems my co-reviewers didn't bring new concerns to light, so I increased my score to 7 The questions above are well addressed in the response, and I would like to increase my score.    **after-rebuttal:** The authors go over the definitions and now make more clear statements in section 4, which makes the argument more concise. The current version is more enlightening to the reader. UPDATE AFTER AUTHOR RESPONSE:The authors have addressed all my queries and made the changes that I requested. I am increasing my rating reflecting this. The paper has novelty, good experiments and improved performance. I would like to see the paper accepted.  *** After rebuttalThank the authors for the detailed explanation. After reading other reviewers' comments and the author feedback, I would like to keep my rating unchanged. EDIT:The revision addressed my concerns, I'm raising my evaluation to 7. Post rebuttal:The authors have addressed most of my main concerns and provided detailed experiment results, hence I increase my score to 7. after author responseI thank the authors for providing sensible answers to my questions, and analysis of their method compared to the minibatch version. I am still not sure about the fact that the rate of convergence of  SW is indeed indepedent of the dimension, as the dimension is indeed hiddent in the scalar product. Nevertheless, I believe this question is out of the scope of the paper, and I changed my final rating accordingly to the new version of the manuscript. Post-Rebuttal Comment: I would like to thank the authors for thoughtfully answering my concerns and questions. I think the small modifications made in response to my comments have made the paper much easier to understand and I think the work is well presented and positioned. Ultimately, I have increased my score to Accept on the basis that I no longer have any major criticism of the work. I do hope the authors can make a more prominent note about appendix F1 in the main text as I think it is an interesting and important thing to highlight for those who may be skeptical.  ## Post-Rebuttal CommentsI thank the authors for their detailed feedback. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting!I am not sure whether it is standard to show the learning curves with the "max achieved return so far" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm. Potentially, the stability could also be increased by adding a learning rate decay schedule?Overall, the rebuttal addressed my questions and incorporated many of the suggestions, therefore I am increasing my score and vote for acceptance. Post-rebuttal comments--After reading the authors' response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns. The inclusion of a clearer motivation, more discussion w.r.t. TPIL, and comparison to TPIL, all enhance my understanding of the contributions of the paper beyond my original review enough for me to increase my score from a 6 to a 7. ----------Post-rebuttal commentsThank you for answering my all questions and updating the manuscript with some of my suggestions. The additions in section 4 clarify the motivation much better and also highlight the differences with prior work. I've increased my score from 6 to 7. # Rating and comments after the rebuttalI share the concerns of fellow reviewers regarding the practicality of the assumptions needed for launching the attack presented in the paper, however, based also on the discussion with the authors, I think that the paper is interesting regardless as it can lead to better insights regarding the development of suitable defence mechanisms for securing the architecture and the information carried by a Deep Learning model.I share also the concerns raised in the other reviews that the paper might be better appreciated by an audience focusing on cyber-security. However, I think that the subject can also be of relevance to ICLR as the paper made an effort to highlight the aspects more relevant to the ML community.Hence, leaving the aspect of relevance to the ACs' discretion, I think that overall this is a clearly written paper based on well executed research that presents some interesting results that are potentially impactful in the aspects concerning security of systems employing Deep Learning models. . I was convinced by further introspection that this is a significant enough contribution --------------------------After reading other reviews are rebuttals---------------------After reading all the reviews from other reviewers and corresponding rebuttals, I think this paper is a good paper and enough to be accepted in ICLR. 1. I think it has a clear difference from f-GAN. It can provide a new loss function for the generative models which can further extend the success of generative models in the future.2. Experiments are not super interesting but at least it has some intuitions corresponding to the authors' claims.3. General theoretical results for the generative models (such as when should we use which loss) is a very difficult problem to solve. Maybe this paper can provide some intuitions for solving that large problem. But it seems too much to ask this thing to the authors of this paper. Without that, I think this paper is still worth to present to the ICLR readers and participants.Therefore, I am standing on my original score (7). ----------------------------------------------------------Post rebuttal: I have read the authors response and will go with my original score for this paper. ==== update ====I thank the authors for providing such detailed response. All my concerns are addressed and reflected in the revision (though some are much better done than the rest). I congratulate the authors on their spirit of maintaining a high standard on the theory, experiments and descriptions, and therefore significantly raise my score. I hope to see this paper accepted.  Update: I would like to increase the score to 7. The authors have improved the paper and addressed my concerns. ------------------Update:I've raised my score in light of author response and new results.  -------------------# UpdateAfter reading the updated version of the paper, I think my main issues with presentation have been addressed. I now see this work as introducing a relatively niche problem and solving it elegantly. The text is clear enough for an average ICLR attendee to understand and contains additional background information in the appendix. I have increased my score to a 7.------------------- #### Thank you for the authors to having answered my questions and having addressed all my comments. My main concern was about the generalisation of the proposed architecture and the results at the current revision are convincing to me. I believe this direction of the research together with the approach taken make a nice contribution to the conference. In consequence, I have raised my score from 5 to 7.   ------Authors' response addressed properly my request. Thank you for the response! Most of my questions are addressed (except for comparisons with other high-precision text generation methods, and error analysis, which I'd like to see but not sure if possible). I think the discussions and clarifications make good sense to me. Indeed, I agree that there are a lot of cases where focusing on quality at the cost of "diversity" is desired, which justifies the motivation of this work. Update: I appreciate the authors addressing my concerns. I have increased my score accordingly.  After rebuttal:See the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning. Edit: Following response, I have updated my score from 6 to 7. ***EDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7. +++++++++++++++++++I have updated my rating after reading author's responses Revised Review:The authors of this work has taken my concerns, and concerns of other reviewers, and revised their paper during the rebuttal period. They have increased the quality of the writing / clarity, restructured the presentation (i.e. put many details in the Appendix section), and committed to open-sourcing the platform post publication. For these reasons I believe this work is now at a state that should be published at ICLR, and I revised my score from 5 to 7. I hope other reviewers can reread the work and post their updated comments.I'm excited about the work, because it incorporates good ideas from A-Life / evolution / open-endedness communities, to introduce new paradigms and new ways of thinking to the RL community. I look forward to using this environment in my own research going forward, regardless of whether this work gets accepted or not. Good luck!Minor comment: On page 4, the section 5 Experiments, I think "Technical details" should be in bold font before the sentence "We run each experiment using 100 worlds." so it is distinguished from being part of that sentence. After revision:The authors have addressed all points in my review. Although I will not be increasing the score, these fixes certainly increase the confidence of my evaluation and I think it deserves to be accepted.==================== ===========  comments after reading rebuttal ===========I appreciate the authors' feedback. I raised my score for Fig 7 showing the conditional images, and for experiments on ImageNet. I think WC is a reasonable extension to BN, and I generally like the extensive experiments. However, the paper is still borderline to me for the following concerns.- I strongly encourage the authors to shorten the paper to the recommended 8-page. - The motivation of WC for GAN is still unclear. WC is general extension of BN, and a simplified version has been shown to be effective for discrimination in Huang 2018. I understand the empirically good performance for GAN. But I am not convinced why WC is particularly effective for GAN, comparing to discrimination. The smoothness explanation of BN applies to both GAN and discrimination. I actually think it may be nontrivial to extend the smoothness argument from BN to WC.- The motivation of cWC is still unclear. I did not find the details of cBN for class-label conditions, and how they motivated it in (Gulrajani et al. (2017) and (Miyato et al. 2018). Even if it has been used before, I would encourage the authors to restate the motivation in the paper. Saying it has been used before is an unsatisfactory answer for an unintuitive setting.- Another less important comment is that it is still hard to say how much benefits we get from the more learnable parameters in WC than BN. It is probably not so important because it can be a good trade-off for state-of-the-art results. In table 3 for unconditioned generation, it looks like the benefits come a lot from the larger parameter space. For conditioned generation in table 6, I am not sure if whitening is conditioned or not, which makes it less reliable to me. If whitening is conditioned, then the samples in each minibatches may not be enough to get a stable whitening. If whitening is unconditioned, then there seems to be a mismatch between whitening and coloring. ====== second round after rebuttal =============I raise the score again for the commitment of shortening the paper and the detailed response from the authors. That being said, I am not fully convinced about motivations for WC and cWC. - GAN training is more difficult and unstable, but that does not explain why WC is particularly effective for GAN training. - I have never seen papers saying cBN/cWC is better than other conditional generator conditioned on class labels. I think the capacity argument is interesting, but I am not sure if it applies to convolutional net (where the mean and variance of a channel is used), or how well it can explain the performance because neural nets are overparameterized in general. I would encourage authors to include these discussions in the paper. comments after reading response ===========The authors make a good response, which clarifies the unclear issues from my first review. I remove the mention of the concurrent submission.Specially, the new Appendix D with the new Fig. 4 clearly explains and shows the benefit of WC over W_zca. Post-Rebuttal================I thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods,  clarifications of the proposed approach, adding references to related work, I will  increase my score and suggest to accept the paper. A novelty of this work is that it is the first paper that methodologically analyses FRIs for DNNs a reasearch area which might shed new light on the understanding of how vision systems work and the source of misrecognitions and the limitations of recognition systems.In light of the changes of the paper and the clarification on the novelty aspect of this research, I suggest this paper to be accepted as it constitutes novel research in understanding how DNNs recognize image content and its similarities and differences to human vision. ================================Thanks for the clarification and revision! It addressed some of my concerns. I would stick to the current rating, and vote for an acceptance. POST REBUTTAL EDIT: I thank the authors for providing detailed answers regarding my concerns. I have updated my score in light these comments. Below are some additional comments in response. Response to comments regarding C1) and C2): While early stopping with pre-conditioned updates differentiates this work from (A. Rudi et. al 2019), the analysis still requires the knowledge of the population covariance. Indeed, while the authors have included a section (Appendix A.3) showing that the operator norm of the population and the inverse regularised empirical covariance can be controlled, it would be insightful to discuss to what extent this allows the analysis for the pre-conditioned gradient descent to be extended to an approximated population covariance.  Response to comment regarding C3): I am inclined to agree with reviewer 3, in that the manuscript is difficult to read due to the larger number of fragmented results. In this regard, I feel the authors should focus on a single phenomenon that is supported by both the parametric and non-parametric aspects of the paper, for instance, how pre-conditioning helps against misalignment.      Response to comment regarding different prior on ground truth (point 4. third bullet point): Note that some concurrent works have studied the case of different priors on the ground truth [2,3], which are likely relevant in this case. Minor Comment: The pre-conditioned updates for non-parametric regression (4.1) use notation $\alpha$ where as Appendix D.8.1 uses notation $\lambda$, with the discussion then switching back to using $\alpha$ and $\lambda$ being used in reference to the regularisation used within FALKON. The switching of notation is possibly confusing here. [2] - D. Richards, J. Mourtada, L. Rosasco "Asymptotics of Ridge (less) Regression under General Source Condition", arXiv:2006.06386 (2020) [3] - Wu, D. and Xu, J. "On the Optimal Weighted $\ell_2 $ Regularization in Overparameterized Linear Regression" NeurIPS 2020 Update:I thank the authors for the response. Unfortunately, the response does not mention modifications made to the paper according to the comments. According to pdfdiff, modifications to the paper are very minor, and none of my comments are addressed in the paper. I think the paper shows good results, but it could very much benefit from improved presentation and evaluation. I do recommend acceptance, but if the authors put more work in improving the paper, it could have a larger impact.------ -----------Edit: most of the issues listed in "cons" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7.  ** Post-rebuttalScore increased to a 7 following rebuttal and paper revision. Thank you for your explanations and the associated addition to your paper. This resolves most of the concerns I had noted in my initial review. The fact that LTVAE seems to learn a latent structure in which the Y_i are highly dependent on each other is quite an interesting point, and it indeed matches the intuition we can have about the data in general. I'm raising my rating following these improvements.I am however not really convinced by your approach to empirically compare the performance of LTVAE wrt an equivalent model where the Y_i are independent. The immediate reason is that your reported test log-likelihood (-120) is much worse than that of a regular VAE as per your paper (-84.9), which is quite surprising as the regular VAE would be a special case of both LTVAE and this independent approach. This leads me to believe that your approach of first training using LTVAE and then amputating it has an issue. This likely makes the model fall into some local minimum that it would not have reached if trained from scratch, especially given TLVAE seems to learn a structure where the latent variables are highly dependent. Revision post-discussion: The paper's notation and model has been clarified, and my concerns about the paper have been addressed. Proposing a latent tree structure on the latent space of generative models is a strong contribution, the model performs well and seems to find meaningful and interpretable structure in the latent space. 1) With regards to transfer, it would be useful to see a comparison with a purely neural network baseline: one can imagine pre-training the neural network baseline on the go-to-pose task, and then fine-tune the network in the collect-wood task. This may be a fairer comparison.2) The authors reasoning makes sense, and I believe that this is a strength of the preceptor gradients framework.3) I see. Thank you for clarifying.7) Agreed, expressing the program at the right abstraction level would be crucial. Although writing a controller program whose inputs live in a relatively simpler abstraction level is easier, a major challenge I potentially see with how the perceptor gradients would scale. For example, in the Minecraft tasks, the outputs of the preceptor network are categorical variables over the agents x and y position. However, presumably for a task with this simplicity it may be possible to use conventional non-learning computer vision methods to obtain the x and y position. Beyond merely increasing the complexity of the preceptor network, there seems to be a conceptual issue: how would the perceptor gradients approach scale to scenarios where the state is not that easy to symbolically specify? This is presumably the motivation for learning from pixels in the first place. For example, one can imagine manipulating a non-convex object like a cup, or a soft object like a stuffed animal. Would the output of the perceptor network be in these cases, and would the output space of the perceptor network need to be custom-designed for each differing object geometry? I think the perceptor gradients approach is a good step towards learning systems that generalize better, but it seems that future work would need to address the challenge of scaling the approach to domains where the output space of the perceptor network is more complex than categorical positions.Overall, the revised version of the paper has addressed some of my concerns, although it still seems to me that more future work would need to be done with respect to point (7) for the perceptor gradients approach to have more impact. Despite these concerns, I believe the paper is a good first step towards tackling an ambitious goal, so I would recommend acceptance. Thank you for the answers to questions regarding the LadderVAE and the choice of prior distribution. The outcome of both the node perturbations and the linear conditional density experiment is in line with I expected. Overall, I the contribution of the work is bit clearer now and I'm raising my score to reflect this. The primary actionable recommendation I have is to add visualizations of the data under different node perturbations to the supplementary material for the best model learned on MNIST/Omniglot so that readers may have a better intuition for what low-frequency structural changes and high-frequency elements correspond to.  Post-rebuttalThanks for addressing my questions. With the new comparisons and discussions wrt the most relevant methods, I believe the contributions of the paper are clearer. I'm revising my score from 6 to 7. After rebuttal: The authors have nicely addressed my comments. I have increased my rating to 7.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ The rebutal and the revision of the paper solve my comments.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Changes after author responseThanks for addressing the concerns in the review with the new revision. I am revising the score to 7 from 5 based on the reply and the revisions to the paper.--- **After Rebuttal**I have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami. I understand that the authors have taken a pessimistic approach to compute the lower bounds of risk in the minimax setup of meta-learning. However, I believe the paper is an important step in this direction. Theorem 3 and Theorem 4 are important additions to the paper, which show a margin between the pessimistic lower bound of the risk and the actual risk with the introduction of structure to the learning setup. Overall,  I enjoyed reading the paper, and I have increased my score after the rebuttal. ######################################################################Post-Rebuttal Updates:6. Thank you for running the experiments on small datasets and for clarifying my additional concerns.  In light of your clarifications, I do find this work to be a practical extension of Shampoo and am in favor of accepting the work.  I am still surprised by the fact that delayed pre-conditioner computation still leads to improved convergence over 1st order methods, but the presented empirical evidence demonstrates a consistent benefit in wall time across a number of settings.  UPDATE:I think the rebuttal addresses some of my concerns. I am especially glad to see improvement on en-fr, too. Thus I raised my score from 5 to 7. Addendum:  You know, what I really love about ICLR is the effort authors make to refresh their paper and respond to reviewers.  You guys did a great job.  Really impressed.  50x2D now clarified and some of the hasty/unexplained bits fixed. Updated rating after author response from 8 to 7 because I agree that Figure 1 and some discussions were confusing in the original manuscript.-------------------------------------------------------------------------- After the rebuttal and the authors providing newer experimental results, I've increased my score. They have addressed both the issue with the phrasing of the auxiliary loss, which I'm very happy they did as well as provided more solid experimental results, which in my opinion make the paper strong enough for publication. ##### Update:I appreciate the clarifications and the extension of the paper in response to the reviews. I think it made the work stronger. The results in the newly added section are interesting and actually suggest that by putting more effort into training set design/augmentation, one could further robustify the agents, possibly up to the point where they do not break at all in unnatural ways. It is a pity the authors have not pushed the work to this point (and therefore the paper is not as great as it could be), but still I think it is a good paper that can be published.----- ----------Apart from one concern (refer to comment), the authors have responded to most of my other comments. Based on this, I think the paper does offer an interesting analysis of the STE approach used for training binary networks, hence I'm increasing my score. ------------------------After reading the author response, they have sufficiently addressed my main concerns. I think that this is a good paper that will be of interest to those concerned with understanding the training of activation-quantized / hard-threshold neural networks. I have thus increased my score. After reading the authors feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? This paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss?After reading the authors feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community. In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this? This paper uses multi class hinge loss as an example for illustration. Can this approach be applied for structure prediction, for example, various ranking loss? update: the authors have assured me in comments that the model is trained end to end - changing rating to good.. Thanks for the detailed responses. After reading the author response and the updated paper, I am satisfied on several of my concerns, many of which were due to the writing in the earlier submission. The updated results on various comparisons are also good.  I have updated my score accordingly. Some qualitative analysis of the results would have been nice -- examples of protein pairs where they do well and other methods have difficulty as they don't use the structural similarity info / global sequence info used by this paper. But maybe those can be in a journal submission.My only remaining concern is on the lack of reporting average performance on the test data (which used to be the norm until recently for papers submitted to ML conferences). Updated score from 6 to 7 after the authors addressed my comments below. PS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve &gt;96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score. **Update after authors' response**I want to thank the authors for their response, and I am happy to see additional results on shared sets of weight values (which allows to easily relate the work to methods for training low-bit-width networks). To further increase impact and significance of the work it would be necessary to really flesh out the advantage of the proposed method over other, similar methods ("it is not too surprising that the method works, but why would I prefer it over other methods?"). Nonetheless, the paper presents novel empirical analysis that adds to the body of work on non-standard training of neural networks. To make a clear stance for the reviewer discussion I have therefore increased my score to 7, though I would rate the paper at the lower end of score-7 papers. --- **Update: As noted elsewhere in the discussion, the authors have addressed my primary concern. I will therefore increase my score from a 6 to a 7.** Edit: I have read the other reviews as well as all author responses. The other reviewers noted meaningful concerns, but I believe the authors have clearly addressed most of these points. I still believe this work is an "accept". -------Post Rebuttal-------Thanks for the feedback from the authors. I am glad to see that the new theorem (Theorem 2) has successfully tackle the problem about the prior knowledge of the variation budget. I have updated the score accordingly. UPDATE:The authors have consistently improved their argumentation over the course of the review and addressed my concerns sufficiently. I have increased my score and recommend acceptance. POST REVISIONFollowing our discussions and taking the changes made to the manuscript into account, I have decided to increase my score and recommend acceptance. My concerns have been adequately addressed. I believe the paper has been clarified, results are more carefully evaluated and claims are sound. I believe that this work consitutes a well-selected application that addresses a relevant research question with important clinical implications. In my opinion, this deserves to be aknowledged and may be of interest to others in the ICLR audience. My hope is that this application stimulates more work in EEG-based machine learning and also encourages others not to shy away from difficult application domains such as psychiatry, where we really are in need of new solutions to old and - unfortunatley - quite persistent problems.Thank you very much for your hard work!--------------------------------------------------------------------------- My enthusiasm for this paper remains fairly high after reading through the other reviews and responses: I think the results (in the main body of the paper) look strong. In particular, this approach is competitive with MixUp, and I prefer this simpler label-only approach to MixUp. I don't think this work is duplicative with the ICML 2020 deep knn paper cited by another reviewer (that paper filters out noisy examples, rather than smoothing labels). And I think the theory is novel in its synthesis oof KNN theory with label noise.However, I am unnerved by something that Rev 2 pointed out about the ablation results in the new Table 3 in the appendix: in Definition 3, the weight given to the assigned ("true") label is $(1-a)$, so we'd expect to see higher (or at least competitive) accuracy for lower values of $a$ when label noise is not too severe. For SVHN, at least, this seems like it should be true since we know the SOTA accuracy is in the 90s. Instead we see the highest accuracy is for the highest values of $a$. That makes me suspect the order is reversed. However, if this method works, we should expect to see lower churn for higher values of $a$, which we do, which would indicate that things are in the right order. This apparent contradiction is disconcerting since some of these numbers are cited in the comparison in Table 1 in the main paper.In an ideal world, it'd be nice to have one more round of discussion with the authors about Table 3. Because it was added to the appendix late in the review process and no further discussion is possible, I am electing to leave my score as is. However, I urge the authors to take a look at Table 3 and verify that there is no error. If it is possible for the authors to post a public comment and/or contact the AC with an explanation, they should do so.I'm also lowering my confidence to reflect my uncertainty about Table 3.----- .+++ Post-rebuttal +++I've read the rebuttal and am content with the response. I'm maintain my recommendation to accept this paper ==================================UPDATEThank you for replying to my questions and clarifying in the document. ########## EDIT ##########The authors have addressed all my questions. #### Post-rebuttal comments:The author response has well addressed most of my concerns on technical details and experiments. Edit: I have read the authors' response as well as the other reviews. Based on the additional results and added feature selection details, I now agree that ESP is generally applicable.  Addressed Concerns:- corrected the typos.- Majority of the weaknesses are addressed. UPDATE TO REVIEW FOLLOWING AUTHOR REVISIONS AND COMMENTS---------------------------------------------------------------------------------------------------I thank (and commend) the authors for their detailed, point-by-point responses.The authors have made a good effort in their revisions to improve and clarify the exposition, and rectify the other comments made by the reviewers. The paper could still benefit from a deeper rewrite -- there's just so much that can be packed into a conference paper with limited real estate, and the authors are seeking to make several contributions under the umbrella of one submission (as the title suggests). So clarity suffers, and impact will suffer as a result. But, in my mind that shouldn't necessarily be a show stopper at this stage, in light of the revisions. I am therefore upgrading my recommendation. ---Edit after rebuttal: I am satisfied with the changes to the paper and increase my score to an accept. ###########Updates: Thanks for the authors' response. The modified version improves clarity. I think this paper provides nice observations and initial analysis to the community and can be beneficial to future work, so I recommend this paper to be accepted. -------------UPDATE-------------I thank the authors for their response. The revised version certainly looks better. I'm still happy to recommend this paper for acceptance.  -----------------------------------------------------------------------------------------------Compared with the latest related work, the author added the qualitative and quantitative comparison against "Recurrent Feature Reasoning for Image Inpainting" in the image completion task. And it shows the outstanding performance of this paper. They have also reformatted the paper so the paper becomes clearer for readers to read. Consider the above all, I think its a good paper and is worthwhile to be accepted.So I improving my rating to 7: Good paper, accept ******************After rebuttal:Glad to see this was just an error in the decription of the algorithm! Score increased from 4 to 7. We'd even increase our score to 8 if the authors added an analysis similar to the one of Figure 3, just showing which OF is used. As stated in the original review: The experiment could be to add/remove objects, and observe the number of object files, or to corrupt specific files and see which object becomes unpredictable, thus identifying which OF corresponds to which object. after rebuttal: The authors have addressed some of my major concerns in an updated version. for this reason I raise my score to a point where I can recommend acceptance. I now add after-rebuttal comments at the end of each item of my original review.**after-rebuttal**: the authors have changed the lower bound to a true lower bound. ----- Update: Updated score based on the author response. That being said, it would still be good to see a plot of time complexity of the decoding scheme as a function of m. ******* After RebuttalI have read the author's response # Update after author discussionMy original review mentioned an apparent similarity to VideoFlow and a weak experimental setup as the main reasons for my score. Through discussions with the authors, I am now convinced that VideoFlow is indeed a quite different model, which significantly adds to the novelty of the proposed model. As for the experiments, I had misunderstood the setup. In terms of probabilistic multivariate methods, my experience lies in the multi-output Gaussian process domain where it is common to consider tasks where only some outputs are considered missing (i.e., part of the test set), thus testing the models' ability to make use of partial information at a given input. This was the setup I had in mind, but the proposed model is not easily adapted for this. This is not a limitation of the proposed model as much as an inherent difficulty of handling missing data with neural networks. The experimental setup used in this paper instead considers a training set and a test set entirely separated in time, which is not a setup where GPs usually perform well, so the additional experiments I had requested do not make much sense. Indeed, the baseline methods chosen for comparison in the paper are both reasonable and strong.In conclusion, the authors have satisfactorily addressed my concerns with the submission. I still think the proposed idea is good (although not groundbreakingly novel), the paper is well-written, and the model thoroughly discussed and tested. I view the submission as a significant contribution to time-series modelling and is therefore recommending acceptance.---------------------------------------------------------- -----------------------------------Post revision update:All concerns addressed. Score updated. ---### Updates:Thanks for the authors' response. My major concerns were related to the comments (1) and (2) in Cons, but it is now clear why the authors consider only the Gaussian copula in the paper. My other concerns are addressed, too. I upgraded my rating after reading the authors' response. ++++++++++++++++++++++++++++++++++++++++++++Edit after author response: We thank the authors for their response to my concerns and those of other reviewers. **Update after rebuttal** I would like to thank the authors for the additional details provided in the rebuttal and revised version. All my concerns have been adequately addressed. Given the importance of the topic and the development is reasonably solid, I would like to recommend for its acceptance.  *Update after discussion*: The authors have addressed all the points that I raised during the discussion. I appreciate the effort, and I'm increasing my score accordingly. Update: I am happy with the changes and am keeping my score. ---The authors' comments strengthen the argument for the method.   I (perhaps liberally) conceptualizetheir approach as one of tethering "close to a rotation" but still don't have a simple understanding ofwhy/when this should be better than tethering "close to original parameters". My original rating is unchanged. Update after reading the response from the authors:I would like to thanks to authors for answering my questions and revising the draft. I agree that the main strength of this work is to explore data augmentation methods which are modality agnostic. Since most of the data augmentation research is domain specific, I think this paper will increase awareness for modality agnostic data augmentation methods. I think it's a good paper, hence I am revising my score.   # Update after author responseThanks to the authors for their response. I appreciate the inclusion of Figure A13,  though more details (specifically, whether the plot for LR=0.2 is both LR_find and LR_eval, as I suspect but can't confirm) and more runs (specifically, the decoupled LR lines in Fig 4a that outperform LR=0.05, probably the LR_find=0.05 and LR_eval=0.2) would be appreciated. I don't agree with the authors' claims about standard LT (rewinding to iteration 0) being equally as valid of an object of study as rewinding to later iterations on these large-scale networks, because by the original definition of lottery ticket (a sparse randomly initialized subnetwork that matches the accuracy of the full network in the same amount of training time), there do not exist standard lottery tickets on ResNet-50 at nontrivial sparsities using the standard learning rate schedules, and the resultant "lottery ticket" network trains little better than a random subnetwork [1, Figure 10], implying that LR_find may not actually be a relevant hyper-parameter in this large-scale rewind-to-0 context. I still think more discussion of this point is also warranted when discussing results on ResNet-50 when rewinding to iteration 0.Regardless, I believe that the paper presents and thoroughly validates an interesting hypothesis, and I maintain that the paper should be accepted. EDIT: The authors clarified the presentation, gave a nuanced response to my concerns about the relative scalability, and promised to discuss the relationship to the actions-in architecture in the final version (it's not as central as I first thought given the limited role played by universal mixers). Solid work in its current state. --------After discussion:After reading the author's response as well as the opinions from other reviewers, I will stick to my original rating. The authors resolve most of my questions and concerns and I look forward to a revised version of the paper. After reading the authors' responses and other reviews, I would like to stick to my original rating to accept this paper. Though there are minor problems that I still have concerns (e.g., why not using re-sampling for all methods which is a better strategy than using fixed points), I am satisfied with the responses to my primary concerns about the paper.  -----------------Post-rebuttal reviewI carefully read through the rebuttal and other reviews and I would like to keep my original rating. The author(s) addressed my concerns and I think it is a good paper for the community. UPDATE: The authors have removed the attention calibration study. My new score is a 7. This is a good paper and I heartily recommend its acceptance. **Update after the authors' response**The authors have provided a comprehensive response that address most of my comments, and clearly clarified the novelty and key differences with prior work such as mBART (which also seems to be a key concern in the other reviews). Given the satisfactory authors' response, I am therefore raising my score to "7". Overall, I believe this paper presents a good contribution to the field. UPDATE: I thank the authors for their detailed replies and running additional experiments. This resolves my questions and I'd keep my recommendation to accept the paper. Post Rebuttal: I like this paper and would vote for it to get accepted on the merits of: (1) Their finding about how MDL can be an indicator of inductive biases of the models; (2) introducing an experimental framework for studying inductive biases of the models. I'd also like to appreciate the authors' efforts to address reviewers concerns in the rebuttal. I agree with reviewer #5, that the paper can be better contextualised in the related research area but I think the paper is improved from this aspect a little bit during rebuttal and this is something that in general can potentially be fixed for the camera ready version.  ########################################UPDATEThe paper provides important and novel empirical observations about the use of machine learning to perform mathematical reasoning. The authors have addressed and clarified some doubts about the originality of their idea and the reproducibility of their experiments in the discussion phase. I believe that the paper is now ready for publication and I'm happy to recommend for its acceptance. # Post-rebuttal updatesI've tentatively updated my review score from 6 to 7 for the following reasons:* The authors clarified the relationship of their proposed method with Squeeze-and-Excite, and promised to add an explanation to their paper.* The authors clarified the relationship between their method and CondConv in OpenReview comments, and promised to update the submission accordingly.* The authors reported additional experiments on ResNet-18 and MobileNetV3(small). These new experiments help increase my confidence that the proposed method is broadly applicable.* The authors provided inference time numbers for MobileNetV2 on CPU. AnonReviewer5 raised a very good point: that reducing the number of trainable parameters may lead to much faster models because of memory bandwidth considerations. I'm enthusiastic about this line of work, and think the inference time measurements provided in the rebuttal are a promising first step in this direction. ***Post-discussion period comments***The authors have done an adequate job of responding to my queries and have also revised the paper in light of the comments of all the reviewers. While the paper could always be improved, I believe it is now above the threshold of acceptance and it should be accepted into the program, if possible. I am raising my score for this paper in light of the discussion and the revised paper. Update after author response:Most of my methodological concerns have been address (except for the ablation studies).  The scientific application here is not super well-motivated.  It would improve the article greatly to show a greater utility (or at least, clearly describing future utility for answering scientific questions).  It is mentioned that "A full application of the method to study the dependence of contextual signals in mouse visual cortex will be the focus of a follow-up publication."  That's vague; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments. --------------------**In the rebuttal, the authors resolved most of my concerns, therefore I increased my score to 7 as promised.** === EDIT: post rebuttal ===Thanks for the response and addressing the issues for a more serious research paper. === update ===The provided response is reasonable and helps address some of my concerns. I would keep my rating as acceptance. ### Recommendation after Author Response ###I have read the author response and appreciate the effort spent by the authors on this response. My main criticism was addressed and the author's feedback is very convincing. The authors have not yet added this additional content to the paper. Assuming they will include it in the final version,  I am confident that this paper will meet all standards of ICLR and recommend acceptance. I increase my score accordingly to 7. Update after Rebuttal:The authors provided clarifications and improved the manuscript. In particular, the authors now detail the two special cases (beta=0, beta=1) and how it relates to EWC and VCL. I am no longer sceptical that the claims regarding the equivalence to EWC in case of beta=0 is correct. Based on this, I changed my evaluation and now suggest acceptance.  Update: Not all the experiments are particular thorough and the novelty less than expected. ----------------------------------------------------------------------------------------------------------I have read the revised manuscript and the comments of all reviewers. For concerns that the experiments are not sufficient to validate the proposed method, I am leaning to the author's rebuttals that the datasets have been chosen to meet the heretical assumptions of context-agnostic learning. The given datasets seem to be sufficient to demonstrate the effectiveness of the proposed learning method.Therefore, I will not change the rating. [Post rebuttal]After more discussion and reading through the revision, I think this is a good paper and will be useful to the community for an instance of test-time defenses. ---------Post-rebuttal commentsThank you for adding the additional experiments and analysis. The results with the basketball dataset (Table 4, Figure 6) and the visualization of the latent distribution (Figure 5) address my inital concerns and showcase the versatility of VDM. I've increased my score from 6 to 7. AFTER REVIEW UPDATE: I find the revised version much improved explaining the inference model much more clearly. The lack of clarity was for me the main reason for evaluating the paper as below the acceptance threshold despite the fact that otherwise I found the paper to be good and useful for the community. As the lack of clarity has now been, in my view, resolved, I increase my score to 7 - Good paper, accept. ----Review update after revision: my concerns have been addressed. *****   Post Rebuttal  *****Thank you for your clarifications! After reading the rebuttal and comments of other reviewers, I am increasing my score.   --------- After rebuttal ---------Thanks to the authors for the response and updated paper. I keep my original score and recommend acceptance for this paper. ---In the revision, authors provided detailed clarifications on their approach and practical utility of the proposed method. Therefore, I am changing the rating of my review. EDIT: The authors addressed my main concerns, fixed a crucial bug in the claim/proof of Theorem 2, and added many more clarifying details to a revised submission. Therefore I will keep my rating the same -------------As a result of the feedback, the part I was concerned  became clear, so I would like to raise the score. Update:Thanks the authors for their response. All my concerns are addressed and I decide to increase my score from 6 to 7. -----------------------------------------------------------------Post Rebuttal:Many thanks for the authors to update their original paper addressing my questions and concerns.I have now updated the score. UPDATE:After the reviewers clarifications and some further explanations of the implications of Theorem 2 (in Appendix E) I think now that the paper tells an interesting story and thus I will vote to accept. *** Post Response Comments ***I thank the authors for addressing the points raised. I am raising my score accordingly to 7.Nit: The y-axis labels on Figure 7 and 8 should probably say "Clean accuracy" instead of "Robust test accuracy". Note: After rebuttal, I increase my overall score from 6 to 7. ==============Update after rebuttal:Thanks to the authors for their response. I enjoyed this paper, and I'm keeping my score unchanged. In terms of multiple datasets/diverse models, I appreciate the various models/dataset/baselines currently included as a proof of concept. However, I'd be very interested in a more systematic study with significantly more models and datasets to understand better precisely when excess error as function of data composition and size can be reliably extrapolated and the extent to which the trends observed are "universal." ====== POST-RESPONSE UPDATE ======After reading the authors' responses and the rest of the reviews, I still stand by my original score.That being said, I do agree with the other reviewers that the framing of certain discussion points could be improved to avoid misconceptions. Specifically:- There are places where the narrative feels accusatory towards earlier attacks (e.g., words like "flaws").- It would be good to explicitly state the scope of the paper relatively early (e.g., "clean-label attacks on deep learning for images") and explain this choice.- Clarify that this is not meant to be the ultimate poisoning benchmark but rather a first step that allows us to make progress in terms of attack development and evaluation.It would be great if these could be incorporated in the next revision of the manuscript. Updated recommendation after major changes to the submission: thank you for addressing my comments. Updates: Thanks for the authors' response. I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted. -------------------------------------------------------- Post Rebuttal --------------------------------------------------------------------------------------------The author did a great job in terms of addressing most of my concerns and answer all the questions. I also like the updated paper. The update reflects most major concerns from the reviewers. Thus I would like to raise my score to 7 and would like to champion this paper. I do think the paper would potentially provide great value to the community not only due to its open-source effort but also as a general approach to improve simulator efficiency for RL, despite it's not "novel" methodology-wise.   **Update post-rebuttal**The authors addressed most of my concerns, especially the one about the confounding factor. I am updating the score from 6 to 7. ------------Post-rebuttal response:The authors addressed most of my concerns so I continue to recommend acceptance of the paper. Specifically, they answered my question about whether the approach will work in static environments and how prior data can be used to improve sample efficiency. They also conducted additional experiments to verify some of my questions. _____Post author rebuttal:I appreciate the authors response and overall the authors have addressed my concerns. I am thus raising my score. The only point that I believe still stands is #7, though I should have updated earlier. My issue with claiming this as the first use of end-to-end learned subgoals in navigation is that there have been many recent works from goal-conditioned hierarchical RL that use end-to-end learned subgoals, e.g., https://arxiv.org/pdf/1712.00948.pdf, https://arxiv.org/pdf/1805.08296.pdf, https://arxiv.org/pdf/1909.10618.pdf. Navigation to a known goal is a version of this problem and indeed in these works, the approaches are shown navigating between states. Others have applied end-to-end to navigation and manipulation, http://proceedings.mlr.press/v100/li20a/li20a.pdf. Overall, application of end-to-end HRL to the navigation problem is an interesting area to study, but to claim it as a major contribution I believe the paper should thoroughly examine the tradeoffs as applied to that problem, which I believe requires a detailed and standalone work. [Post Rebuttal]Thank you to the authors for addressing my question. The paper presents a simple and elegant approach to the AudioGoal task, backed by extensive experiments and good writing. I'd like to maintain my positive rating. *****Post Rebuttal*****I would like to thank the authors for their comments. They clarified most of my doubts regarding the paper, most importantly about learning the basis $L_i$. While I agree with reviewer 3 that there is room for improvement when it comes to experimental evaluation, I appreciate the changes made during the rebuttal period and keep my original score. **Update Post Rebuttal:** The revised manuscript addresses my main concerns, and is stronger than the original, so I am raising my score to accept. For the final manuscript, I urge the authors to better integrate the added content from this discussion period into the main text, instead of relegating it to the Appendix.   [Edit: Score updated, see discussion below] Update: taking authors comments and improvements into account I've updated the rating from 6 to 7. Post rebuttalThe authors addressed the concerns around having a deterministic gating only baseline. I will increase my score to 7 ------------ After Discussion ---------I am still of the opinion that the manuscript is not without flaws: the theoretical result is quite messy, the empirical evaluation is still limited, and the generalization bound could still be vacuous.However, the authors provided new experiments that indicate that the bound might not be vacuous in the considered setting, though. The authors also provide a preliminary runtime analysis that suggests the costs for using the surrogate loss do not explode (please include a proper runtime analysis in any future version of this paper). Since the authors furthermore did address my main points in my review during the discussion, I have increased my score to 7. --EDIT-- updated score to 7 after the author's response to questions and changes to the paper.  Post-rebuttal comments:My concerns are resolved. I have changed my vote to acceptance. (7). Update: see comment below for rationale behind change from 5 to 7. Post-Rebuttal Update:I acknowledge having read the author's response and I also glanced over the new experiments.  after discussion with authors ======== During the discussion phase, the authors addressed my concerns and improved their results. Therefore I increase my score to reflect this. ***During rebuttal: I thank the toy examples provided by the authors.  ### Post-discussion feedbackThank you for the valuable discussion and revisions. I believe the paper has improved and I've updated my score to reflect that.The new analysis in Figure 4 and its surrounding discussion offer some useful insights. I think the authors could still improve this analysis a bit in a final version of the paper (if only just to make the plots a bit easier to parse visually), but the TD-error instability of the baselines seems reasonably clear and I'm inclined to agree with how the authors frame this as an example of the problem they solve. In addition, the added discussion around discounting is also valuable; and the simplification of the algorithm and added ablation experiments improve the quality of the contributions. **Post-Rebuttal**The authors have addressed my comments and revised the manuscript accordingly. Recommending an accept. After Discussion:I think this is a good paper and I would like to see it presented at ICLR2021.  ---- Post-rebuttal ----Thank you for addressing my concerns and providing the additional experiments and baseline which I think make the paper stronger. I have updated my score as a result. POST-REBUTTAL=============="First, we would argue that there are not many examples of planning approaches outperforming non-planning approaches when using imperfect learned models. Providing such a demonstration in challenging image-based benchmarks is one of the contributions of this work."- I disagree with authors. The idea of planning using imperfect models have been explored as early as 1998 (e.g. Dyna algorithm in Reinforcement Learning: An Introduction by Sutton and Barto)."computational analysis"- Thank you for adding this section. It addressed my concern."non-determinism"- Thank you for adding more clarity. It addressed my concern."Statistical significance"- Great to see the extra runs. Please include the bars for Figures 5,7 in the final submission. === Response After Rebuttal ===Thank you for what I found to be an impressive and convincing rebuttal, both in terms of the detailed responses to every comment---and not to mention the extensive rewrite of this paper in the limited amount of time available. In short, I think that the paper is significantly improved, and is now at a level of quality that I feel warrants acceptance. As such, I am choosing to raise my initial score of 4 to 7 (good paper, accept).As the revised version introduces many changes, I have some new (minor) comments:The last line of the abstract claims "sets that are often factors of 5 to 10 smaller". This is true for APS and NAIVE, but not adaptive top-k. You might wish to qualify this statement as to which comparison you are referring to.Thank you for clarifying the introduction w.r.t. problems with NAIVE. The third "problem" seems a bit funny to me; it doesn't identify a problem with NAIVE, but rather that other methods are better (which seems to stem from problems 1 and 2). Can you clarify this point to make it more precise?Typo, related work: "[...] data-splitting version known as split conformal prediction THAT enables conformal prediction [...]"Related work when talking about equal coverage per class, you might want to cite the earlier reference of mondrian conformal prediction: Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian confidence machine. Technical Report, 2003.Theorem 1's remark on "this result is not new". Not to nitpick, but I'm not sure which aspects of Thm. 1 makes it "first appear" in Papadopoulus et. al. and Hechtlinger et. al? This result can likely be found in (or if not explicitly in, then derived from) from late 1990's work by Vovk and co. on Ridge Regression Confidence Machines, Transductive Confidence Machines, etc. Or the journal/book papers (i.e., Algorithmic Learning in a Random World, Hedging Predictions in Machine Learning, etc).(It goes without saying, however, that this revision is much better in terms of proper citation than before!)Proposition 2 seems to have some funny crowding around the \subseteq operator.Nice work on adding section 4. In the proof of proposition 3 I believe you accidentally left out set size (| |) around C(X) on the r.h.s.In the discussion, you mention improving efficiency via prediction cascades. You might be interested to refer to a concurrent ICLR submission (https://openreview.net/forum?id=tnSo6VRLmT) which does precisely this, and to which your work on larger label spaces seems quite complementary. ## UpdatePlease see my reply to the authors' message. ---**[After rebuttal]**Thanks for the clarifications. My score remains the same after reading through all the other reviews and the replies from the author. During the rebuttal, the authors attempt to answer many of the questions raised by the reviewers. While some of the replies are less satisfying, I still find this work to be worth to publish. I would encourage the authors incorporate the changes in the revision if the paper was accepted.  Added after reading author response:Authors have sufficiently addressed my concerns and I'm planning to maintain my generous score based on my initial understanding. However, other reviewers have raised many important concerns and I encourage authors to improve the paper based on those. Update: score updated after author feedbackThis has been fixed in an updated version and is now not a problemThis has been fixed in an updated version and is now not a problemThe authors argue in rebuttal that ReLU networks on spike-free data is still different from a linear classifier, some experiments were addedAuthors have answered this in the revisionAuthors have answered this in the revisionUpdate After author feedback many of my concerns have been addressed, in particular the optimization problem templates are much easier to understand now as well as the derivation of the algorithm. Also some important differences between linear classifiers vs ReLU networks on 'Spike-free' data have been clarified. These were my main concerns and thus I am inclined to raise my score. =====POST-REBUTTAL COMMENTS========As I mentioned earlier in my review, I like the result and I feel it could be interest of the TCS community. However, as correctly raised by other reviewers, ICLR may not be the right venue for this paper and also it would be beneficial if authors improve the presentation of their result and the motivation of their work further.  [Update after reading authors' comments]Based on the authors' and other reviewers' comments,  I keep the score unchanged. UpdateI have increased my score to 7 after reading the authors' response and the updated paper.###  ---**Update**---Increased score 5 -> 7 thanks to clarifications from authors. Post Rebuttal =====Thanks for your response and for revising the paper. I have increased my score accordingly. Some Typos/Mistakes:p. 1: In paragraph 3, introduce  T .p. 1: Introduce  5K[5M5V].p. 2: there exists functions -> existp. 3 (and elsewhere): for factored Markov chain -> & chainsp. 3: Knapsack setting -> the knapsack settingp. 3: & is more concise -> are p. 3: We use & estimation of  5IÆ  -> & estimation of  5IÆ , respectively.p. 4: do not harm to the & -> remove "to"p. 4 (and elsewhere): omits higher order factors -> I believe you mean here that  5  collects some higher order terms in the expression.p. 5: all possible value of -> values p. 6:  5K[5M5V]  -> Did you mean 5K[5M5V]p. 7: for a  5[¯¯  -> for a factor of  5[¯¯ p. 7: focus -> focuses p. 7: & but counts the cumulative cost & -> "counts" has made the statement rather unclear.p. 8: from state to & -> from states and budget to actions (or state-space & to action-space)p. 8: With prob. at least -> shortening probability to "prob." does not seem to provide any gain in the space. p. 8: To be more formally -> formalp. 8: in interesting future work -> an interesting & Based on author feedback, here are some additional comments:I would like to thank the authors for their response to the reviews. As noted in my original review, the proposed method is a well-engineered method for a particular type of document recommendation problem. Empirical performance on the chosen data sets is impressive compared to the baselines. The claim on gain in training speed is suspect as there is a hyper parameter tuning step that has been not taken into account while reporting the speed gain over the baselines. Label correlations are not taken into account for label embeddings and might hurt the performance when there are not many documents in the training data for the long tail of labels in many real-world applications. Random embedding puts unrelated labels into the same bucket. Though this doesn't seem to hurt retrieval performance in the experiments reported in the submission thanks to filtering, it is not clear how training will be affected by this non-semantic bucketing of labels. Overall, I think the submission has several things going in its favor though there is substantial scope for strengthening. I've updated the rating.  After the discussion, my concerns were fixed. The paper explores the interesting relations of Mix Up and Uncertainty, which is useful and will be the right fit for the conference. EDIT: The statements about ERO clarify the contribution considerably. 6-->7 Update:After seeing the author response below, no change to my score. EDIT: The author response addressed all my concerns and answered all my questions, in particular that the exponential running time is a worst-case bound that is indeed loose in practice. I believe that the revised version will be much clearer and am therefore increasing my score POST-REBUTTAL COMMENTS========I thank the authors for the response and the efforts in the updated draft. Most of my concerns were addressed. This is a simple, but nice idea. After reading the rebuttal and the other reviews I am recommending to accept the paper. From the discussions, it seems that there are quite a bit of concerns raised about the experimentation process. On the other hand, the responses, and presentations in the paper, are also quite convincing to me. So I believe this result is ready to appear in the conference, if anything for the further discussion/interest it will generate, and would still like to recommend acceptance of this paper.         ---- update:Thanks for the update. I guess the "intuition" is driven mostly by empirical results, which I suppose is ok but may be worth digging into a bit more. I have updated my rating. ===Post rebuttal===I would like to thank the authors for the detailed response and clarification of my questions. I believe that this paper will be valuable for the ML community. **Update** : Since nearly all of my issues have been addressed, I have changed my rating from 5 to 7. Best of luck :) ### Final Recommendation after Author Response###The authors have addressed several of my main concerns. It would have been helpful to study transferability to tasks beyond image recognition, but overall, I think the paper has been considerably improved. I increase my score to 7.Two remarks regarding new content: * I find it misleading to denote PGD as a targeted adversary and additive Gaussian noise as a random adversary Section 7. "Targeted" usually refers to an adversary that aims at achieving a specific misclassification (target class). Gaussian noise is not really an adversary, rather a distortion/image corruption. I would recommend clarifying the naming to avoid confusion of readers. * Is there any particular reason to use PGD(3) in Table 1b for evaluation? Would the effect hold also against stronger attacks (more iterations etc.)? EDIT: Updated recommendation to acceptance (7) following author response. ----------------Post-rebuttalI appreciate the authors' response and additional comparison against previous work. I do think that proper comparison with previous work is important, as it allows us to know better when and where the proposed approach is beneficial.  Edit: Markdown problem with nested listsEdit: Upgraded rating due paper improvements------------Thank you for addressing my concerns.Figure 1 is not updated (it looks the same w.r.t. the GP, but the change from variance to std should be noticeable?). # Post Rebuttal: I thank the authors for taking the time to address my concerns. The paper is well written and the proposed approached is promising. I therefor recommend acceptance. Added after reading author response:I believe authors have addressed the issues I raised about clarity in certain parts of the paper in their response sufficiently. I agree that in this paper, the convergence rates' dependency on m has been improved significantly compared to previous work. Thus I increase my score. I encourage authors to investigate m's effects on the convergence rate more to see whether there is a structural limitation in federated learning settings, perhaps better left for future work. ### Update 1 after discussion:Score raised. ======= Review edit after authors' revisions ====== Most of my concerns have been resolved in the significantly-improved revised version of the paper. After rebuttal: I would like to thank the authors for paying attention to the comments and providing additional experiments and results. I updated my score. ================================== ### After rebuttal phase ###The comments by the authors and the revised version of the paper successfully address my concerns. I thus recommend to accept the paper. ---- Final Rating ----The authors' response has resolved my concerns. I would keep my positive rating. ----------------------------------------------------Post-rebuttalThanks for the new ZSL and GZSL results on ImageNet. The results are convincing to me. My other concerns are properly addressed too. I read the concerns from R4 and R5. To my knowledge, using common sense knowledge graph and the GCN with self-attention are novel in the zero-shot learning literature. I decide to increase my score to an accept. ==============Update after rebuttal:Thank you for clarifying the numbers in Table 1 should match the main text. I enjoyed this paper, and I'm keeping my score unchanged. _[Edit: I'm glad the author clarified some of the tensions between the different definitions. The ERM bias proof seems also okay, but it's still not clear how ERM performs with all the additional assumptions of the IRM result. I'm still not on the same page regarding $\kappa$, but I'm willing to take a leap of faith on the bits that I'm uneasy about. I would suggest this paper is shared with the community, since I believe it has enough good insights.]_ --- EDIT POST REBUTTAL ---I thank the authors for their detailed answer(s) and the efforts they put in editing the revision (in particular I agree with other reviewers that Prop. 17 adds clarity). Overall my stance on the paper has not changed and I still recommend acceptance. ==============Update after rebuttal:Thank you for clarifying that aux-outputs is itself a contribution and not simply a baseline for comparison. I also appreciated the additional experiment showing examples where In-N-Out can outperform aux-outputs. I'm raising my score from a 6 to a 7 accordingly. Update after rebuttal:The authors have addressed my concerns. Contrary to my initial understanding, the paper builds off of prior work in a methodical way, and the pseudolabeling stage of In-N-Out makes more sense now. I have raised my score from 6 to 7. Final review:The authors addressed most of my concerns. Just a nit that it could be helpful to add the total FLOPs into the table (together with # of params) just for completeness.  ---------------------------------------------------------------------------------------------------------------------------The authors have addressed my previous concerns. The experiments on a larger dataset are a plus. Therefore, I would recommend an acceptance of the paper. The authors answered my questions so I am increasing my score to 7. ----- ** Post Rebuttal **To best of my understanding, the authors have addressed all my questions and suggestions with the appropriate revision of their paper. Specifically, the necessary discussion of hyperparameter selection is added and presentation of the runtime&solution quality results (i.e., raised in point iv)) have been improved with the inclusion of important details, additional discussion of related work is added (i.e., raised in point i)) and questions are addressed (i.e., raised in point ii) and iii)). As such, I have updated my rating accordingly. ## After author responsesBased on the revision of the draft and the authors' responses to the review, I am raising my score to 7 from 6.--- After authors' response to revisions, I reconsidered my evaluation and updated the score. === Update After Rebuttal ===I commend the authors on a through rebuttal and active rewrites/experimentation. I still think the work is good, and can warrant acceptance. However, I still find the empirical results to be only moderate at best (though I appreciated the authors' rebuttal and significance testing). I am keeping my score the same. ## Post RebuttalI thank the authors for their response.  The addition of the recurrent SAC baseline helps the paper.  I disagree with R1 that it is a stronger baseline as FLARE outperforms it in all tasks and stack SAC similar or better three (arguably four) of five tasks. Instead it shows that recurrence isn't common in off-policy RL because it doesn't always perform better.  While recurrence is considerably more common in embodied 3D environments and this work may be less applicable there, I don't foresee DM control style RL benchmarks going away anytime soon and this believe this method will be useful. Update after rebuttal: I thank the authors for their responses to my questions. They satisfactorily answer most of my concerns. Overall, I agree with the concern that the proposed approach is specific to RPM and it's unclear how well it (or parts of it) would generalize to other problems but I think the approach is quite interesting, novel and achieves state-of-the-art results. Hence, I think the contributions of the paper are significant for acceptance.  ******Update after author response: The authors have addressed my comments and my recommendation remains unchanged.****** ----updating in light of author's response-----I am upgrading to 7 as my concerns have been addressed.Other reviewers have questioned if ICLR is the right venue for a paper about a library.The paper does describe novel algorithms that lower the big-O cost of computation, similar to how Strassen's algorithm is a non-trivial modification of the naive matrix multiplication algorithm. After reading the rebuttal from the authors, as well as the updated draft, I agree that this style of presentation and framing is much more approachable to people not familiar with sig/logsig transforms. Thus I vote to accept this as a *library* paper. _Update after rebuttal_: The authors have been very forthcoming with further analyses in the rebuttals. I believe the findings deserve to be published and that bringing attention to these issues is a good thing, and accordingly I have bumped my rating up a bit. --------------------------------------------------------------------------#### Update after rebuttalThe authors have explained the reasons for the comparing experiments in Appendix E and updated the result of time complexity, which I think worth including in this paper. The authors did a fantastic job of answering questions, revising their manuscript in accord with reviewer feedback (Sec 3.4 title), and even adding new experimental results based on reviewer suggestions (mid-training hierarchy) and reflecting best practices in interpretability research. I was really impressed by their nimbleness and responsiveness. I will raise my score to a 7: I think this is a very solid paper and excellent research effort around a nascent idea. In particular, I think its impact is limited by- its close coupling to naturally hierarchical problems, e.g., multi-class classification with a taxonomy- its close coupling to image data and tasks- its heuristic nature: fully train neural net, infer hierarchy via clustering, retrain neural net, then map a priori labels onto inferred hierarchyThe "10" version of this paper (maybe future work?) would propose a way to infer the hierarchy on the fly and show how to apply it onto other kinds of data and problems with different structures.----- Update:The authors have addressed many of my comments below. As such, I am increasing my score. ---------------------Edit after author's responsesMy first concern is addressed by the authors' response. My other two concerns were not really addressed, but I think these concerns should not preclude this manuscript from getting accepted. So I'm updating my score. [Update after author's responses]I appreciate the responses provided by the authors. I think they answer my questions. In consequence, I update my review to favour accepting the paper.--- After rebuttal:I appreciate authors' detailed responses and an updated version of the paper. They mostly clear my concerns and doubt. I increase my rating to accept. -------------------------------------- Thank you for your answers.------ ## Post RebuttalI thank their authors for their response.  I have decided to maintain my rating, overall I still think this is a good paper that presents an interesting set of results.  The result that image prediction accuracy correlated better with asymptotic performance than reward prediction accuracy continues to intrigue me.  My fellow reviewers have some concerns that while I don't agree, I think they could be avoided with some changes in the presentation to the paper:* The models used.  I understand the decision to move the model description to the appendix (not everything is going to fit in 8 pages), but the main paper would benefit from paragraph or two describing why which models were chosen and then referring to the proper places in the appendix for the full details.* Reward-only model.  There was considerable concern about how much smaller that models is than the others.  Additional results showing that one with a comparable number of parameters does as poorly if not worse would be beneficial.  The 0% line in Fig 6 and Fig 8 is very similar to this hypothetical (if it exactly) so I believe this will pan out as expected.* Presentation of results.  In my read, the most interesting result, anti-correlation between reward prediction accuracy and asymptotic performance, comes at the end.  Without that result, there is a way to read the paper as "yes image prediction helps, it increases the amount of supervision given to the model".  That result and the difference between online and offline shows that there is something unexpected going on here so perhaps leading with those and/or highlighting them more would help. update after rebuttal: the answers were convincing, and the paper improved.  ## Post Rebuttal commentThe authors have clarified the contributions of their work and improved the manuscript accordingly. Given this and the other positive points about the paper I am willing to increase my score to accept. Added after the author response:The authors adequately addressed my questions and concerns. I appreciate that the authors provided the error bars for their experiments and tried out the idea of modifying epsilon_k, but I wish the authors left more time for the discussion. I think the paper is a valuable contribution to the domain of irregular time series. I am increasing my score to 7. ---update: Thanks for the clarifications. I've read the response and other reviews and have updated my rating. #####################    After Rebuttal   ####################I thank the authors for responding to the comments and have read them carefully.The authors have addressed all my concerns in the rebuttal and I vote for acceptance of this submission. ###################################Update:The authors have addressed the concerns I had in my initial review. I have raised the score from 6 to 7. ---------------------------Update after author response:I thank the authors of the paper for significantly improving the prose of the paper, and I agree that the changes make the paper more self-contained and approachable. I have kept my ratings as my score was for primarily for the strong experiment results (and the score was also conditional on the paper being more polished). I am happy to support this paper for acceptance, but I am a little concerned about the degree of changes in the final version versus the initial submission, given the number of concerns the other reviewers had. ---------------------------------------------------------------------------------------------------------------------------------------------------------------------Post Rebuttal update:I would like to thank the authors for providing relevant details and a thorough rebuttal to all the issues raised by the fellow reviewers. Original rating is maintained. **Post-Rebuttal**My main concern was regarding the clarity of the paper. I believe it is improved in the revised version, so I increase my rating. [Addendum after review: most of the concerns have been addressed by the authors with a large set of additional experiments.][Addendum after review: As the authors point out, this is only partially true, as each feature can be selected for multiple groups.][Addendum after review: the authors have added a random forest to the experiments. However, this is the only method whose parameters are not fine-tuned (which might be a smaller problem for random forests). No time comparison is given, and some accuracy deviations appear well within the standard deviations. Still, this is a very good addition to the paper.][Addendum after review: This point has been partially addressed.] ===============================After response ======================Thanks for stressing my concern, the additional experiment makes the empirical result more convincing. Overall I think this is a good paper, I prefer to keep my score and rate it as accept. ------------------------Update after author response:I have increased my score in light of the substantial improvement to the manuscript and experiments. ==== Post discussion Update ====I am updating my score to accept after the discussion. ----Update: thanks for the additional human evaluation results! These help and the results on excluding unimportant entities seem strong to this reviewer. Perhaps it might be more helpful for the annotators themselves to try to interact with the summarizer in some way, but that's a more minor point.Anyways, I bumped up my score from 5->7. ------------------------------------------------------Edit after author response:The authors have sufficiently addressed my concerns with the additions that have been added in Appendix D. As such, I will increase my score from a 5 to a 7. ##################################################################After author response: I thank the authors for the additional experiments. It partially addresses my concerns. However, the qualitative results do not always show improvements in image quality, and most of the low-quality samples are still generated after cleansing. Its also hard to clearly notice differences before and after cleansing in Tables 9 and 11 (2D-Normal). It would be better to use a multi-modal Gaussian with some modes being more likely. Overall, I am still concerned about practical applicability of the proposed approach. I keep my score of 7.   Post-author response: I appreciate that the extra experiment I asked for was conducted and the equations were fixed. While I think Review5 raised some interesting discussion points which should be included in the final version, I still think the paper has merit, even if larger-scale pre-training would have improved the results. Thus I raised my score to 7. -------------------------------------------------------------------------------------------------------------------------------------------------------------------------Update: After going through all the discussion, I'd be happy to raise my score to 7. The authors did a great job in clarifying all the concerns raised by the reviewers. This make the paper a much stronger publication.  UpdateThanks for the rebuttal, it addressed my main concerns. The new results on CIFAR-10 and CIFAR-100 with fine and course labels match the results on the checkerboard task. This increases the generality of the main claims. The new experiments also confirm that the level of noise in SGD has a key role in finding minimal representations. Furthermore, they show that when training with SGD with enough amount of noise, the usable information with fine labels increases initially and then decreases. This improves our understanding of the phenomenon introduced by [2], which was later debated by [1]. For these mentioned reasons, I updated the rating from "5: Marginally below acceptance threshold" to "7: Good paper, accept".[2] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR,  abs/1703.00810, 2017. **UPDATE:** Most of my questions/comments are addressed in the revised version of the paper and the author responses. I maintain my support for acceptance. POST DISCUSSION UPDATEI like the proposed method and I will keep my score.END OF UPDATE --------------------------------------Updates after author response:---------------------------------------I think the revisions and the responses did address all the concerns I had, in particular towards assuring that the method and baselines leverage the same information for inference. Additionally, the experiments where one can 'search' for the optimal subtype instead of assuming known subtype at inference also showed encouraging results.Overall, I think the paper writing and presentation is much improved, and I would argue for acceptance as the paper presents a simple and intuitive idea which is shown to work (rather surprisingly!) well. **Post-RebuttalThe authors have addressed most of my concerns. I have increased my score. Although the additional experiments on the variance/checkpointing are helpful I would still like to see more discussion in the paper itself.  === post rebuttal ===I am satisfied with the response and i will keep my score. **Update**: Thanks to the authors for clarifying most of my clarification questions.  I'm not sure I was able to fully follow your argument about why these results can't be adapted to a two-color dataset (see above), but to indicate that you've clarified many of my questions I've increased my confidence score to a 4.  Good luck to the authors.**Further updates**: I'd like to elaborate on my thoughts a bit more with the hope that the authors may find it useful for future versions of the paper. - First, I really appreciate the authors for performing additional experiments with EIIL during the response phase. I wish to emphasize that, after a long discussion with R3, I've some strong disagreements with their review regarding the "simple fix":   -   I personally think EIIL is out-of-scope as it is a recent algorithm. It doesn't sound like a "simple fix" to me. However, it's great that the authors were able to show that their algorithm works better.    -  I don't think pooling all datapoints and splitting it will work. You'll end up with a dataset with label-color correlation in both domains, and both domains will be identical. So IRM won't work here.  - One can always come up with some hacks like "pool all environments and carefully split them back" that work under the assumption that there's $\Lambda$-spurious correlation. But those hacks would be sub-optimal if there were no $\Lambda$-spurious correlation.  Therefore, this is an unfairly powerful "overfit" hack, and does not make a good baseline. You want an elegant solution that works whether or not there's $\Lambda$-spurious correlation as you won't know whether that sort of a spurious correlation exists in practice.  - As a side note, in light of the above point, I think it's important that the authors also demonstrate that the CDM constraint added preserves the performance of IRM on the original CMNIST dataset.   - Hopefully the authors can keep the EIIL results for future versions of the paper as it only makes the paper stronger.- I don't think the paper should be heavily penalized for the lack of a realistic dataset, because it's hard to verify $\Lambda$ spuriousness on realistic benchmarks. However:  - I'd strongly encourage that you consider trying similar experiments on a dataset like say Rotated-MNIST (or Rotated-MNIST+ to be more precise, if at all possible).   - Even better, you could consider whether similar experiments can be done with Celeb-A where you have access to image attributes like hair color etc., (See Fig 2 https://arxiv.org/abs/2005.04345) and you could try creating different environments by sampling differently in each.  - If you think that's it's impossible to create a $\Lambda$-spurious dataset, you might want to explain in future versions of the paper as to why that's not possible.- I understand R3's main concern which is that the algorithm in this paper requires that the distribution of the causal features $X_{causal} | Y$ to be the similar across all environments. It seems like IRM doesn't expect this sort of invariance, while algorithms prior to IRM do require something of this sort (including CDM, DANN etc.,). I think one actionable way to address this concern would be to show that there are datasets where IRM + CDM does better than CDM (just like how IRM+CDM does better than IRM in CMNIST+). This way we can see why combining IRM and CDM offers something unique.- Finally, I want to appreciate your efforts in trying to clarify all the reviewers' concerns (at least I found them helpful) and also to update the paper accordingly, add experiments etc.,  ## Post response updateThe author's response has clarified most of the missing details in the paper. I still have an issue with reporting results for 3 runs --- even if the variance is small for 3 runs, that does not imply that there won't be outliers when one does more runs. Nonetheless, the proposed method is insightful and the paper has significant pedagogical value. I'm moving my score from 6 to 7 and I hope the paper is accepted.  Update======I have looked through the response and edited version. The updated evaluation looks solid and provides a potential solution to a robust overfitting problem. Although the work is primarily empirical in nature, it may inspire directions for future work to look into more theoretical explanations of robust overfitting.  Final Evaluation (Post Rebuttal)---The author response and accompanying paper revision clearly and effectively addressed each of the 3 main weaknesses I pointed out, so I raised my rating. Updates:Thank you for the clarifications, new study, and theoretical justification. The new results demonstrate the utility of PSEs even when policies are not exactly optimal. My concerns were thoroughly addressed by the authors response, and Ive updated my score accordingly. final recommendation:I maintain my score. I think this is an interesting piece of work that can easily be used as citations whenever the discussion of "why don't you just relax your program to be differentiable" comes up, and I can cite this paper and say "no that does not work theoretically". **Comments after Author Response**I thank the authors for their response. Queries 1,2, and 4 have been adequately addressed. Regarding Query 3, I appreciate the addition of the Deep Ensemble results though I find that the text of Section 5.5 has not been changed to reflect the same. Specifically the paper still says "For the IMDB age prediction dataset, results show that PIVEN outperforms both baselines across all metrics". This is now incorrect since there is a third baseline, DE, which appears to outperform/match PIVEN for this dataset. However the explanation that this is because the population age is approximately Gaussian makes sense to me and so keeping in mind the good performance of PIVEN on the other datasets, and the improved explanation and added ablation study for hyper parameters, I recommend accepting the paper as long as the relevant corrections are made in Section 5.5. ----[post rebuttal]I thank the authors for their revision and clarifying many of my questions and adding results based on that. I have read other reviewers concern and while I agree some room for improvement on clarity, I still think the paper brings in value. Unless there's technical flaws spotted by other reviewers that has not been resolved, I'm still leaning towards accepting (increased score from 6 to 7).  ----After reading the author responses and updated manuscript, I have raised my score from 6 to 7. Thanks for the detailed response! The new revision addresses my presentation concerns and answers my questions, so I've increased my score to a 7. I still think it would be useful to present a slightly more targeted set of ablations in the main paper rather than the kitchen sink in table 2: e.g. just - "co-grounding" / nothing- progress monitor / progress inference / both / neither- best model + data augmentation / best model (no data aug)Haven't proofread the new draft carefully but "state of the arts" in Table 1 is wrong, so you should do another copyediting pass before the final version if the paper is accepted. Revision after rebuttal:The new version is definitely clearer and easier to read, hence I support the paper for acceptance and change my rating to 7. There are still minor improvements that can be done in Section 4 to improve the overall clarity:* About the metrics, the "Average attack success rate" and the "Target command recognition rate" should be clearly defined, probably under the description of the attack methods.* The Adaptive attack approach could be introduced unter "Attack methods" in 4.1.* Table 4 is not easy to read, the authors should improve it.* The first paragraph in Section 4 ("The presentation flows ...") is very interesting, but almost reads like a conclusion, so maybe the authors could move that to the end of Section 4 or to Section 5. ==================After rebuttal==================The authors provided new experiments supporting the proposed method. I am happy to increase my rating. Revision:I thank the authors for the all the extra experiments they performed. The paper looks good to me, and increased my evaluation accordingly. ---------[UPDATE]Regarding the comment "Our paper resolves the question posed in ICLR Best paper 2017 "Understanding deep learning requires rethinking generalization"", I don't think that analyzing networks with explicit regularization resolves the questions stated in Zhang et al paper. As other reviewers mentioned, there are a number of other papers that formally define quantities that correlate with the generalization error, and are larger for random vs true labels. There are also other papers showing that one can tune some parameters of the optimization algorithm to avoid overfitting on random labels (while it is a modification to the algorithm, it is still similar to explicitly regularizing the Lipschitz constant of the network) (see e.g., Dziugaite et al work on SGLD).Therefore, the claim in the abstract "A second result resolves a question posed in Zhang et al. (2016): how can a model distinguish between the case of clean labels, and randomized labels?" needs to be toned down a bit.In my opinion, the work presented in this paper is a valuable contribution to learning theory. The new version of the paper is easy to read. Therefore, I recommend acceptance if the authors change the claim about resolving  the questions posed by Zhang et al.Another typo: - for convergence, we require that the network also grow(s), --- In their rebuttal, the authors satisfyingly addressed my concerns. Hence, I am upgrading my overall rating. Post-rebuttal update ===The authors' rebuttal provided many of the details I was seeking. I asked a few additional questions which were also recently addressed, and I encourage the authors to include these clarifications into the final draft of the paper.Hence, I've increased my score for this paper. -------------FINAL RATINGI have carefully read the other reviews and authors' replies.I agree with other reviewers that the rebuttal is satisfactory, and raised the score accordingly. Thanks to the authors for their effort. Regarding the answers provided by the authors, I increase my score  UPDATE: I've upped my score to a 7, and would like to see the paper accepted.  **Post-rebuttal update.** Thanks to the authors for their response to all my questions and comments. I also read the updated version of the manuscript, which is clearly improved and the rest of reviews and comments by the AC. Looking to that, I agree with the rest of reviewers about the quality of the paper, so I raised my score and I recommend to accept it. Apologies for my mistake about prior work on applying transformer networks to music: while reading other papers on music generation, I had encountered a few citations of a 2018 paper that directly applied transformer networks to music generation. After going back and inspecting, I found that the paper being cited was in fact the arxiv version of your paper, effectively blowing my mind! This changes my opinion. Originally I felt that even as an application paper, the technical novelty was thin since transformers had been applied to music in the past. But given that these results are in fact the first on applying transformers to music, I think they do make sense at ICLR. I have changed my rating accordingly.Further, thank you for your diplomatic response! Thanks for your responses. Yes, I was referring to use the gated attention approach to use as another baseline and use different metric (rather than data efficiency ) to evaluate this framework.Most of my concerns were addressed. As a result, I've updated my score. That said, I am hoping that the authors will add more tasks and metrics to evaluate this platform. Data efficiency metric is good but not enough. EDIT: the concerns were mostly addressed in the revision. ---I have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.    ------------------------------------------------I have read authors' comments. I raised my rating. After the rebuttal.- the authors address most of my concerns.- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.-------------------------------------------------------------  --edit:The authors have sufficiently addressed my questions and concerns and have performed additional analysis.  My biggest concern of weather or not the RFM-augmented agent was capable of learning without a pre-trained agent has been addressed with additional experiments and analysis (Figure 8). Based on this, i have adjusted my rating to a 7. --------- review after rebuttal----------#1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. #5 fine-tuning with enough epochs. I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that training from scratch is better when the number of epochs is large enough. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates fine-tuning is faster to converge.  However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of scratch B, as ResNet-56 B just reduce 27% FLOPs. The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training.Questions:- Are both methods using the same learning rate schedule between epoch 160 and epoch 320?- The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5,  but it turns out to be 92.68(±0.19) in reply#8.- It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison.#6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. The authors find that when the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning.  This could be due to following reasons:       1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning.       2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios.-------- update ----------------The authors addressed most of my concerns. Some questions are still remaining in my comment Review after rebuttal,  specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version.However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7. **** EDIT *****I have read the response of the authors and appreciate their clarifications and the additional information on the runtimes. See my response below for the concern that remains about the absence of the estimate of the log likelihood for the VAE experiments. Besides this issue, the other comments/answers were satisfactory, and I think this paper is of interest to the research community, so I will stick with my score. ### After rebuttal phase ###The answers provided by the authors and the revised version of the paper sufficiently address my concerns. As such, I recommend to accept the paper. I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines. Still, I feel that in the current form, the paper can be accepted. The rebuttal provides valuable information that was missing in the original paper and improves the readability. Therefore, I recommend accepting the paper. After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. I believe this is a strong paper and it represents an interesting contribution for the community.Still things to fix:a) a dataset used in 4.2 is not statedb) missing articles, for example, p.5 ".In practice, however, we need a weaker regularization for A small dataset or A large model"c) upper case at the beginning of a sentence after question: p.8 "Is our Adv-BNN model susceptible to transfer attack? we answer" - "we" -&gt; "We"==================================================================================== Update after author response: thanks for your response; I think the revised paper largely addresses my comments and those of the other reviewers, and I continue to hope it is accepted. Here are two small notes on the related work section of the revised paper:- In distinguishing OCD from DAgger, you note that the optimal policy is computed rather than provided at training time. In fact, structured prediction applications of SEARN (Daume III et al., 2009, which should also be cited) and DAgger often have this flavor too, such as when using them for sequence labeling (where optimal continuations are calculated based on Hamming distance).- Please include a reference to Goldberg and Nivre's (2012) dynamic oracle work.  -----------Revision------------------------I am not changing the score. I disagree with AnonReviewer2 regarding the significance of the results.  The assumption that the states are observed is indeed a weakness of the paper. However, understanding non-linear dynamical systems is extremely challenging and this paper provides strong convergence guarantees. Furthermore, there are several insights in the analysis that may be useful in future work. --------------I would be maintaining the same score. I agree that the paper has nice convergence results that could possibly be building steps towards the harder problem of unobserved hidden states however, there is more work that could be done for unstable systems and possible extension to ReLU and other activations to take it a notch higher. ***** EDIT ******I thank the authors for their clarifications. They have sufficiently answered my questions/comments, so I will stick with my score. *********************Update after author response:I think the Fashion-MNIST experiments and comparisons with ICA are many times more compelling than the original experiments. I think this is an exciting contribution to dually learning component manifolds for demixing. Updated Thoughts=================I was primarily concerned about a lack of analysis regarding the technical contributions moving from AQM to AQM+. The revisions and author comments here have addressed the specific experiments I've asked for and more generally clarified the contributions made as part of AQM+. I've increased my rating to reflect my increased confidence in this paper. Overall, I think this is a good paper and will be interesting to the community.I also thank the authors for their substantial efforts to revise the paper and address these concerns. ------Rev. In light of the rebuttal and the following discussions I have updated my rating to 7. Update: the new version of the paper addresses most of my concerns. There are a lot more experiments, and I think the paper is good for ICLR. However, I wonder if the reasoning for PR2 is limited to "self-play", otherwise Theorem 1 could break because of the individual Q_i functions will not be symmetric. This could limit the applications to other scenarios.Also, maybe explain self-play mathematically to make the paper self contained? * RevisionI took into account the discussion and the newly added experiments and increased the score. The experiments verify the proven effect and make the paper more substantial. Some additional comments about experiments follow.Training loss plots would be more clear in the log scale.Comparison to "SGD BN removed" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017). Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm. Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1? ========================================Revision: I have read the reply from the authors and it clarified several matters. I adjusted my rating of this paper (from 6 to 7). [COMMENTS AFTER AUTHORS' RESPONSE] After the rebuttal provided by authors, all raised questions and criticisms have been fully solved. Therefore, I recommend for a full acceptance. ===Update: The authors have made a good effort to address the concerns raised, and I believe the paper should be accepted in its current form. I have increased my rating from 6 to 7, accordingly. ------ Updating score based on authors' response. ## Edit after RebuttalI thank the authors for their engagement with my review. Many of my comments and questions have been resolved and, consequently, I have increase my score and **recommend accepting this paper.** After reviewing the authors' response:The authors have agreed to include missing sparsity level-results and have commented that such results are in line with the trends observed in other experiments.  Furthermore, the authors' response addressed all my questions and concerns, for which I'm raising my score.======================================== -------------------Update:Most of the weak points were appropriately addressed by the authors and I have increased my rating accordingly. -- bumping down my score because the misleading title was not addressed by the author response.-- bumping it up again because the authors have reacted. Comments after rebuttal:The  paper has clearly improved. It leaves a few questions open though. For example, it is surprising that h-detach doesn't work on language modelling since Dropout-LSTM and BN-LSTM clearly improve over vanilla LSTM in this case (if not every case). In the new version, the authors only reference it in one or two sentences but don't discuss this in detail. When dropout is mentioned, one should also mention that dropout is a variant of the old stochastic delta rule:Hanson, S. J. (1990). A Stochastic Version of the Delta Rule, PHYSICA D,42, 265-272.  See also arXiv:1808.03578 Nevertheless, we now think that this is a very interesting LSTM regularization paper that people who study this field should probably know. We are increasing the score by 2 points! Response to Authors-------------I've read all other reviews and the author responses. Most responses to my issues seem to be "we will run more experiments", so my review scores haven't changed. I'm glad the authors are planning many revised experiments, and I understand that these take time. It's too bad revised results won't be available before the review revision deadline (tomorrow 11/26). I guess I'm willing to take the author's promises to update in good faith. Thus, I think this is an "accept", but only if the authors really do follow through on promises to add uncertainty quantification and include some complete comparisons to KL annealing strategies.   ===Update: I am happy with the clarifications and the changes to the manuscript, and have increased my rating accordingly from 6 to 7.  I incline to my current score after reading the response and other reviews.  * RevisionThanks for your response,  the paper new  version address my main concerns, I appreciate the new experiment looking at the eigenvalues of the  end-to-end Jacobian which clearly shows the advantage of the AntisymmetricRNN. ------------UPDATE: Score changed based on author resposne------------ ==========================================================================REVISIONThe authors' additional results and responses have addressed most of my concerns, and I've raised my rating from 6 to 7.&gt; We remark that the identity mapping loss L_idt is already used by the authors of the original CycleGAN (see Figure 9 of [2]). Thanks, you're right, I didn't know this was part of the original CycleGAN. As a final suggestion, it would be good to mention in your method section that this loss component is used in the original CycleGAN for less knowledgeable readers (like me) as it's somewhat hard to find in the original paper (only used in some of their experiments and not mentioned as part of the "main objective"). **UPDATE**I acknowledge that I have read the author responses as well as the other reviews. I appreciate the clarifications and improvements made during the rebuttal phase, which I think have further strengthened this work.I find the key contributions of this work to be (i) demonstrating that recent methods that include labeled anomalies into training can suffer from unfavorable biases, and (ii) providing a framework for a theoretical analysis of this setting.Though I see that the presented results are somewhat what one would expect, to my knowledge such an analysis hasn't been carried out in the existing literature.Since weak forms of supervision (here few labeled anomalies) appears to be a promising research direction for anomaly detection, I find this critical and rigorous analysis to be worth circulating the community.For these reasons, I would keep my recommendation to accept this work (score: 7)##### [original score: 3 (clear rejection)]11/24: Updated score based on updates from the authors. The addition of FLOP counts and more baselines in the experiments section greatly improved the paper. The proposed approach appears to achieve excellent FLOP-accuracy tradeoffs relative to existing approaches.[2nd score: 6 (marginal acceptance)]11/30: Updated score based on updates from the authors. The discrepancy with some baseline numbers has been resolved and the authors added clarifying information to the paper regarding the counting of FLOPs. EDIT: Lowered score by one point after reviewer discussions. The evaluation could certainly be better and the difference to earlier mask-based methods more clear. Update during rebuttal: I have read other reviews and authors' answers to them. My main concern about the approach to be potentially overcomplicated has been address and I was convinced. I am raising the score for the paper. /============================================================================================================  -----After rebuttal:I think the authors well-addressed my comments. Thanks for their efforts. I would suggest that the authors include some results in their response in the paper or supplementary results if they have not done it. ## After author feedback ##Thanks for the paper update, and now I have a better understanding of the proposed approach. I have updated my review to the following:Previously Widrich+ (2020) showed that integrating transformer-like attention (or equivalently modern Hopfield networks based on softmax)  into deep learning architectures outperforms existing methods (kNN and logistic regression) for massive MIL such as immune repertoire classification. More specifically a pooling layer can be formed by attending over a repertoire of instances with a fixed (but learnable) query vector.This work provides theoretical analysis of such a layer for its energy function, convergence of updates, and storage capacity, and points to directions of how such a layer can be understood and controlled. It extends the previous experiment:1) apply HopfieldPooling (attention with fixed learnable query Q) to more MIL datasets (animal image and breast cancer)  and achieve state of the art results. 2) apply Hopfield (attention) to 75 small UCI benchmarks replacing feedforward nets. Here Selu units (Klambauer+ 2017) are used to map input to storage Y  and query R. The result is quite positive beating previous approaches including SVM, random forest, and SNN (Klambauer+ 2017)3) apply HopfieldLayer (attention with fixed training data Y as storage) to 4 drug design tasks  acting as an instance-based learning approach.The result seems quite interesting indicating that general purpose layers such as  feedforward, pooling and nearest neighbors can be improved (in terms of robustness, learnability, or controllability) by adding attention like operations.I think the paper can talk less about existing results, and focus more on the new results and their analysis:- remove [Immune Repertoire Classification] result since it is from previous work.- move the Drug Design experiment details to the main text, and add some comment about under what condition Hopfield outperforms/underperforms RF.- for the UCI benchmark experiment the transformer layer (Vaswani+ 2017) seems to be a natural baseline and should be compared to. Suggestions for the presentation:- Should only in the future work section state that Hopfield can potentially substitute LSTMs or GRUs, since it is all hypothetical with no experiment result at this point.- The word "implemented"  in Section 4 seems misleading as there is nothing changed in the Bert model structure? "Transformer and BERT models can be implemented by the layer Hopfield."  - Can be more specific in descriptions. For example in the description of (2) Layer HopfieldPooling and (3) Layer HopfieldLayer in Section 3, R and  W_K can be  referenced again for "state (query) patterns " and "The stored (key) patterns" respectively.- It is probably more informative to replace figure 1 with a table to directly compare the energy function and updating rules of different Hopfield nets--i.e., classical, exponential and attention.- Avoid using "x" in equation 1, since the symbol has already been used for the stored patterns.- "HopfieldLayer" seems to be a very strange name. **Update**I have updated my score to 7.One of the points that was not explained in the original paper was that (ignoring function approximation effects) an optimal solution for $J_b$ (the OffPAC objective) will be optimal also for the original off-policy RL objective $J$ (i.e. estimating the on-policy objective in an unbiased manner from off-policy data). From this point of view, I agree that optimizing $J_b$ directly is an interesting question, despite the fact that the exact gradient for $J_b$ may be less similar to the gradient of $J$ compared to the usually used approximate gradient of $J_b$ that drops the $\nabla_\theta Q$ term. It still remains unclear which of the two methods has a theoretical advantage over the other in the function approximation setting (in terms of optimizing for $J$); however, because it is unclear, it is interesting to evaluate the method proposed here and to perform experiments as done in the paper to try to find out which method performs better.The results were mixed; however, the evaluation is fairly thorough and some potential advantages of the new methods such as generalization in the $\theta$ space and zero-shot learning were explained.The discussion in the paper is much improved compared to the original version. Also, additional ablation studies such as testing what happens when the $\nabla_\theta Q$ term is dropped were added (when $Q$ includes $\theta$ as an input). Moreover, LQR experiments for $Q(s,a,\theta)$ and $V(s,\theta)$ were added in the appendix (the results here do not give as good a match as the $V(\theta)$ formulation gave, but they are reasonable).______________________________________________________________ I've read the authors' feedbacks and other reviewers' comments. R5's main concerns are the clarity and the motivation of Bayessian classifier and off-policy learning. That should have been resolved from authors' feedbacks.  -------The authors have addressed most of my concerns. I believe the paper is a good contribution to the literature on normalizing flows; therefore, I firmly vote for acceptance.  ***** Post Rebuttal *****I thank authors for the clarifications! After reading the rebuttal and comments of other reviewers, I am increasing my score.  UPDATE: The authors have addressed my concerns and I have therefore increased my score. **============  Summary of improvements and revisiting reviewers score after rebuttal ==============**Summary of main points of improvement related to my comments after the revision:- The authors have significantly extended the analysis within the paper with more experiments to investigate the proposed measures and the deep nets behaviour in question. This adds a lot of value to the paper, offering more insights and support for the main claims.- The authors have added many in-text clarifications about certain design choices (and improved some, e.g. by average k-max instead of max), which makes the study much clearer and adds confidence to the reader about interpretating the investigation and its results.- The authors have added a study of the influence of the k parameter in the measures, which simultaneously offers confidence in the conclusions (conclusions hold for a significant range of values for k) and offers new insights about how separability happens (1 neuron vs multiple).- Reproducibility is greatly improved, both by improved clarifications of the experimental settings within the text, and by providing the code in the supplementary.- The authors improved discussions about the results with respect to related literature, taking into account and linking them also to papers that at first glance seem to be contradicting (e.g. references [1,2] I provided above that show tight clustering improves generalization). These links and discussions further improve my confidence in the conclusions.Overall, the authors have addressed sufficiently most of my concerns, significantly improving the manuscript. The updated manuscript provides significantly more insights, which should be of interest to a part of the community. The extra analysis provides better support of the studys hypothesis and claims. Remaining weaknesses of the paper include the CIFAR-only experimentation (we do not know if conclusions hold beyond it). I am happy to increase my reviews score from 5 (marginally bellow acceptance) to 7 (Good paper, accept).= Minor =In case the paper gets accepted, I would suggest the authors to try to complete Fig 4 with the layer-wise investigation, which they said was too expensive for the rebuttal period (I cannot estimate how expensive that can be. Hopefully it is possible in longer time period, and would complete nicely Fig 4). ## Response to commentsI thank the authors for their comments and their revision, which have clarified the aim of this work. Having read the authors' responses to this and other reviews, I realize that in my initial assessment I had misjudged the nature of the manuscript. After careful consideration, I have therefore increased my rating. ########### UPDATE #########I thank the authors for their responses and for updating the paper. I think this work introduces some new and valuable ideas for generating videos conditioned by an action graph and I recommend the acceptance. EDIT: the authors have removed the claims I was concerned by, as well as adding a requested baseline. The new quantitative results on stabilization also make their method much more compelling. The new hopper results are a bit noisy (would it stay stabilized if training continued?), but are still quite promising in that they show an ability to handle more complex dynamics.4-->7, thanks for the great revision effort! **Update after revision**The revision improved the paper. Thanks for taking care of my comments.Justification of sine activations, generalization to unseen speakers experiment are nice additions.The new title is a bit better and I think it may be OK since the goal is to perform a moving source simulation for single speech sources. Multiple speech sources can be simulated separately and added together as mentioned. The authors may consider possibly a better name: "Neural binaural synthesis from mono speech" which emphasizes that the synthesized target is "binaural speech" from a single speech recording.Just a few more points.1. I think it is essential in wavenet to apply the model in an auto-regressive fashion over samples. Just using the network architecture and the loss function from wavenet is not equivalent to "using a wavenet model" since an essential part of the model is the autoregressive sampling which makes sure the samples are dependent and coherent. Without auto-regressive sampling, the resulting sound is poor as observed by the authors. So, I suggest to emphasize that "autoregressive sampling" is not performed in the paper to avoid misleading the readers.2. More explanation of 2.5D is appropriate. One wonders if using a larger STFT window size would improve its results. ------------------------------------------------------------------The authors sufficiently addressed all questions in their rebuttal and I will therefore increase my score. --------------------------------------------------------------------------------------------Thank you for addressing my comments, I have updated my score accordingly. Author response: the updated paper is much clearer, and its abstract and introduction allow to clearly see what will be the contributions of this paper. I thank the authors for having followed my advice about this point. With this problem addressed, I recommend accepting this paper. -------------------------Update after author response:Thanks to the authors for addressing my questions! Post Rebuttal Response: I would like to thank the authors for considering my review and for making some of the suggested edits. I think this paper provides a valuable contribution and point of discussion for those interested in the interplay between robustness and uncertainty in deep learning. I do consider the experimental evaluation in this work to be sufficient given that the authors consider many applications which already exist in the literature, and in my view it is out of the scope of an evaluation/methodology paper to necessarily advance the state-of-the-art in applications of the method they seek to evaluate. As I have no major standing criticism of this work, and believe that it provides a useful and interesting contribution to the conference I have increased my score. ---- Post-rebuttal comments----The author rebuttal + revised draft adequately addresses most of my concerns  in particular, the experiments on online adaptation and semantic segmentation are strong, and the additional context on the DA results is helpful. I would still have liked to see DA results on more challenging benchmarks but nevertheless think that the paper proposes an interesting approach and is worth accepting. ---- Post-rebuttal comments----The rebuttal and the paper revision address my concerns. I fully recommend acceptance. Edit post reviewer responses:- I had some misunderstandings in the review above, which have mostly been cleared up. Ive raised my score accordingly =====Updates: Thanks for the authors' response. I carefully read other reviewers' comments and responses. My concern on the missing study of empirical or theoretical support of claim "framework can filter out domain-specific information while preserving the amount of domain-shared information" was also raised by other reviewer. Overall, I still believe this paper provides new insights for this field. ***********After the discussion period, I have increased my score to 7 as the authors have provided a strong set of ablations and changes to address my concerns and those of the other reviewers. ------------------------------------------------------------------------------------------------------------------**After Rebuttal and Discussion**I appreciate the changes and additional experiments added by the authors, and am recommending accept. For the final version, I would strongly encourage the authors to make the representation change/representation reuse argument clearer. Specifically, the authors might want to first present the BOIL algorithm, highlight key aspects of the algorithm (frozen head), present performance results, and then the ablation results in Appendix N to highlight the importance of freezing the head. After this, the paper could switch to a discussion on representation reuse/change and present the analysis results, making clear what is happening at a layer level vs at the algorithm level.Right now, I think it's still a little confusing that representation change often refers to the algorithm not the layers, and this actually reduces the impact of what is a very striking result!I hope the authors can make these changes, and with the clearer messaging, this should be a very interesting paper for the community, and provide many interesting directions for future work! =========================Response after author rebuttal: The authors answered my concern about evaluating the interpretability of the approach. They evaluated the method with a few clinicians, and I'm glad to see that they preferred the authors' method.Adding these results + clarifying the points I included will definitely make the paper stronger. I increase my score as a result and recommend acceptance. I've looked through the revised paper, and the authors has addressed my comments. Hence, I am happy to recommend an accept. Post Rebuttal Increased the score from 6 to 7. I would have strongly recommended the paper if it had more real-world datasets.  *Edits after author comments and revision:*The authors have greatly improved the readability of the paper. I also appreciate the addition of section 4.4. which seems like a reasonable attempt at supporting the increased interpretability of the architecture. I feel that the paper has improved, but it wasn't quite enough for me to raise the rating from 7 to 8, so I'm leaving it at "Good paper". ========== After discussion =============I have increased my score from 6 to 7. [updated after discussion]Thank the authors for their efforts to add ablation study and make the manuscript more clear in presentation, it greatly resolves my questions and thus I raised the score. Overall the paper is in a good shape now. However I do want to point out a couple of things: 1) the performance of UE in base model FM/DeepFM/AutoInt seems to be a bit weired, as in previous CTR paper, deep models should outperform FM significantly. It's not clear to me is this due to different experimental settings or training schemes. 2) if the target is better performance (instead of compression), there is no clever way to choose s, other than mannual picking for each dataset/model. **Update (after the author's response)**: During the rebuttal, the authors clarified my major concern, as well as provided additional experiments that verify the main claim of the paper. I am totally satisfied with the author's response, hence I am changing the score 6 $\to$ 7 // Post-rebuttal update:Thank you for replying to my questions. I am still concerned about the sample complexity associated with $\epsilon$-optimality in cross-entropy (even in the trivial case, and perhaps impossible for some low-dim representations) as LMs are over a countably infinite extended alphabet. **Comments after Author Response**I thank the authors for their response. My opinion of the potential of this paper to encourage further discussion in this area is unchanged. I can already see from the response on choice of biased and unbiased compressors to combine that there is plenty of scope for future work that builds on this idea. Regarding the comparison with EF, I appreciate the additional intuition provided in the author response on the drawbacks of EF and hope to see improvements to EF or more exhaustive comparisons between EF and the approach proposed in this paper in future work. As I had already recommended acceptance I am leaving my score unchanged.  (I am raising my score by 2 points after the author response)-- --------------------------Rebuttal: Thank you to the authors for their detailed response. I am happy with the response and will keep the original score. POST-REBUTTAL UPDATE======================The rebuttal and resulting changes have addressed most of my concerns, and expanding the alternative reward-shaping aspect of the evaluation has helped too. I've increased the score.====================== ~~I'm updating my rating to accept the paper due to the comments and updates on the paper.  The proposed flexible usage of the GMM is novel from the existing literature.  The changes in the paper improved its clarity, and the contribution is better presented in contrast to existing work. ====================================================================================================Update: Thank you for your new experiments and detailed feedback. Most of the concerns have been addressed and the experimental results look better. I believe this paper will provide new insight into efficient neural network training. Thus, I raise my rating and recommend this paper to be accepted. =====================================================================================================Added after author response----------------------------------------I believe authors have clarified many things I asked and addressed the issues I and other reviewers raised. Therefore, I increase my score. I believe the idea of using LSH for efficient training has a lot of promise and this paper brings a possible way to do this into light. Update:==================================Comments for Area Chair and Reviewers: If I view this work merely by the story conveyed in the paper, my assessment would be more in line with the other reviewers. I am not quite sure if this is the right way to evaluate this paper since I view it as having a broader impact that goes beyond the story in the paper, but other reviewers disagree with my view on its broader impacts. I see this as a sign that the paper is currently not in a good enough state to really convey its potential impact.Comments for authors:I believe you still did good work here on the merits of the "speedup training" story that you convey in your paper. I believe that you have much more than this in your hands though. I think you could go two ways from here: (1) get this paper accepted in this form and work closely on the other angles that this work offers in a new paper, e.g. learning which is in line with biological or efficient parallelization of large networks. The second way (2) would be to rewrite this paper more in line with that view and resubmit. I think (1) might be better for you. I do not think many reviewers would understand a paper that comes from the process in (2). Good luck! ### UpdateThanks to the authors for their responses, particularly to the other reviewers who were more critical.  I still believe this is worth publishing if there is room, with the caveat stated before: this is an implementation paper, and does not introduce new algorithms or analysis.  If it is accepted, it should be accepted on this basis. After rebuttal:The authors' rebuttal addressed all my questions and I upgrade my rating. **After rebuttal**The authors have gone above and beyond in providing additional experimental results. All of the points raised above that deal with methodological issues are completely addressed.The sole significant weakness that remains is the lack of the kind of ablation/component studies that would justify individual design decisions. I do not disagree with the authors that this will be difficult for this work, but I still feel they would have been helpful for researchers who will be building upon this method.  ====After rebuttal: Thank you for addressing my concerns. - I believe the claim of equivariance leading to improved generalization (weakness 1 above) has now been validated more directly. - Regarding the validation of sample complexity claim: a more direct investigation of performance against training set size, for equivariant and non-equivariant models, would be more convincing. I understand that augmenting data does increase the training set size by a factor of 3, but the training set is then not iid so it's not clear what the "effective size" of training set is. In any case I do believe that equivariance reduces sample complexity.Overall, I vote for accepting. **After Author Response and Discussion:** Thanks to the authors for their responses. After reading the other reviews and the author responses, I am raising my score to 7 (accept). ### UPDATEI thank the authors for addressing my concerns and confirm my initial rating. ----------------------I've read the other reviews, the author's responses, and the discussion - thank you everyone.  I'm still in favour of accepting the paper. *** Post Rebuttal: Thank you so much for your response and for the codebase link. Your additional experiments are convincing for group influence and overparametrization.  I sustain my recommendation to accept this paper :) *** **Post-Rebuttal Comments.** Thanks for the author's response and their dedication during the rebuttal period, I re-read the updated version of the manuscript and authors did an effort for adding extra content and editing based on the questions and recommendations. In my particular review, they answered and solved the questions that I did. I understand the points addressed by the other reviewers and the theoretical limitations of the method. However, I still have a positive opinion about the paper, and I believe that the aforementioned limitations are well indicated in the paper, something that is valuable. For these reasons, I keep my score. (Number rating was updated from 3 to 7 in light of substantial expanded experiments and analysis.) #### Update after author response and other reviewers comments:I think after the addition of new evaluation (Appendix A.5) on the aspect of counter-factual and other comments made by reviewers I'll stick to my score. Also, I liked the idea of applying counter-factual to long-tailed distribution IE problem. Revised rating to 7 after revision. --- post rebuttal --Thanks for the response. I think the contribution of this work is solid and I vote for clear acceptance.  ##Updated Review## I'd like to thank the authors for their response and am looking forward to seeing the result of their edge detection comparison.  I maintain my review of this paper. #### UpdatesIn light of the responses provided by the authors, and the thorough nature of the additional experiments I am inclined to increase the ratings of this paper. The followup responses answer most concerns raised by reviewers and provide supporting evidence. However, the reviewer maintains that this work could potentially be of more interest to a privacy audience who may better be able to appreciate the use case considered in the presented work. --- Updates ---After reading the authors' response, I decide to raise my rating to "accept". --------------------------After rebuttal:Thank you for the additional explanation.The comments by the authors effectively address my concerns. Although the edge dataset and SR are quite similar to the existing methods, the authors clearly present the usefulness of them as well as their additional contributions. Also, additional training details support the adequacy of the comparison in the experiments. Therefore, I would like to increase my final score to 7: Good paper, accept. ** Edit after author response **I feel that the authors have sufficiently addressed the statistical significance concerns I had in the comments below. I now recommend acceptance because I think the work provides strong evidence that regularization is beneficial in continuous control. Although, as other reviewers have pointed out, further analysis of why regularization is beneficial--especially from a theoretical standpoint--would be helpful, I think the empirical contribution of the paper still stands.  ** EDIT **The authors have addressed my concernes and I have increased my score. Post-rebuttal:I acknowledge reading the rebuttal as well as other reviewers comments. I'm satisfied with the rebuttal, I think that the authors have addressed many of my initial comments and I'm happy to increase the score of the paper to 7. If the paper gets accepted to the conference I would encourage the authors to include and expand a discussion about method limitations in the main body of the paper.---------------------------------------------------------------- # Post Rebuttal:Thank you for the detailed rebuttal. The comment made about being able to use this in a semi-supervised setting is an exciting direction and I encourage the authors to pursue it on larger less-labeled datasets mentioned in the review in a future work/final submission. I am glad that removing VGG improved the results on COCO.Ultimately, I am keeping my score at 7, accept. === After authors' feedback period ===I read carefully authors' responses to all reviews. The author's addressed my concerns and I guess the ones of the other reviewers too. One limitation that can be raised now is that the method is better suited for low-dimensional data.Hence, I keep my accept score. [Rebuttal update: the extra results clear up some uncertainty about this method. I think the success of this method is interesting and teaches us a thing or two about deep RL.] ### Update during review period- The reproducibility of the paper is now much better. It's great that the authors promised to release the LTA code. I hope that this includes the code for the experiments. - Based on the above, I changed my review score to 7.  _________**Post rebuttal**I am happy with the response. After author response: the authors have sufficiently addressed my questions and also the other reviewers' questions. I am keeping my score of acceptance. ============== Post Rebuttal Comments =================I would like to thank the authors for their insightful rebuttal and clarifications. Sadly, I cannot find the newly added section regarding the non-linearity analysis in the revised manuscript and therefore cannot judge the findings of the authors.Hence, my rating will stay the same. ###########################################Update after discussion with the authors and reading the updated version:The authors have clarified the points that were unclear. I'm happy to raise my score. ===I thank the authors for providing the answers to my questions. I will not change my score.  Update after rebuttal: I appreciate the author response. I will maintain my original score. Edit: I have increased my score to 7. ## After Rebuttal ##I thank the authors for their response to my question. I think that the comment regarding differential privacy being ineffective is particularly interesting. It would be nice to actually demonstrate this empirically - construct a DP classifier with a poisoned backdoor, and show that the authors' method is still effective. My support for the paper remains unchanged.  ---Edit: after the authors' response I maintain my recommendation to accept the paper and keep my score of 7. **Update to review**I shall keep my rating at 7. The authors updated the paper and I think it is good enough to be accepted. I still note how Figure 1 is difficult to parse (what are all the variables? Why are there so many arrows and boxes? It is not possible to understand what is going on without reading the text in detail, by which time it does not add much to the reader's understanding); in my opinion it requires starting from scratch again.