 ===During discussion period, I noticed import missing references of this paper as written by Nikunj Saunshi. Besides, the authors do not respond to any of the reviewers' questions. Hence I change my score to strong rejection.  ## Post-rebuttal responseI have read the other reviews and the authors' responses, and do not wish to change my review. The proposed model seems quite complex with somewhat unclear conceptual motivations, and does not clearly demonstrate impressive performance gains despite the complexity. I would suggest that the authors attempt to change one of these things in a later paper, either by revisiting the model design, or task choice and evaluation (to better motivate the model). ----Below is based on the revision---Thanks to the reviewers for making the paper much clearer. I have no particular issues on the items that are in the paper. However, subsections 7.2.1 and 7.2.2 are missing. ------------Update after rebuttal:"It appears that the comment made by the reviewer may stem from an assumption that two models which are compared for PD can be different in the operations they perform to generate the predictions."This is incorrect, my review does not mention such assumption and my statements hold without it. As stated in my review, I consider models with different weight magnitudes, making no assumptions on the underlying cause."PD, as we defined in Section 2, is aimed explicitly at measuring differences between predictions of a set of models that are supposed to be identical in all their components"Indeed, and my point is that comparing the PD of two sets of models that are not identical is also problematic **even if all models within each set are identical**, except for the PD in its Hamming form. More details below."Changing calibration between such models violates this assumption."Please check the celebrated work of Guo et al., "On Calibration of Modern Neural Networks": calibration does not necessarily consist of an explicit, additional component that modifies the model, and the same model trained in different ways can present distinct calibrations. More specifically, two sets of models can have not only the same accuracy, but the exact same predictions (i.e. there is a 1-1 mapping from each model in one set to a model in the other set that has the exact same predictions for all data points) but vastly different internal calibrations, which will result in vastly different PDs (to be overly specific, the scalar PD of a set will be different from the scalar PD of the other set) even though the two sets agree "point-wise" in terms of predictions."If one changes something about one of the models (including how calibration is done), one would expect them to predict differently, and have different accuracies."This is incorrect. First, I'm not assuming models are explicitly calibrated, only that they have distinct internal calibrations (confidences in terms of predicted probabilities, which depend mostly on the parameters' magnitudes). Second, "scaling the output layer weights by positive scalars" (quoting from my review) will not change a model's accuracy: while it changes the class-wise predicted probabilities, the rank of the logits is preserved. If the authors remain skeptical of this fact, let $\phi(x)$ denote the activations of the previous to last layer of a model, and let $\langle w_i, \phi(x) \rangle > \langle w_j, \phi(x) \rangle$, where $w_i$ and $w_j$ are the weight vectors of output units respective to classes $i$ and $j$ (i.e. $p(y_i | x) > p(y_j | x)$ for probabilities produced by a softmax over logits). Then for any $\alpha \in \mathbb R_+$, we have trivially that $\langle \alpha w_i, \phi(x) \rangle > \langle \alpha w_j, \phi(x) \rangle$ (hence $p'(y_i | x) > p'(y_j | x)$, for probabilities $p'$ computed from the new logits). Again, note that it is --not-- necessary for an external, explicit calibration factor $\alpha$ to be employed: training the network differently, or even adopting a different activation function -- just consider $\max(0, 10x)$ for clarity, which will scale $\phi(x)$ by a positive factor and yield the same observation as above."Specifically, if one flips the predictions of a binary classifier, the flipped model will have much worse accuracy from the actual model of interest, and measuring PD at this point is irrelevant."The fact that two classifiers with vastly different accuracies can have zero PD is worrying and shows that PD is not a trustworthy metric: claiming that such evaluation is 'irrelevant' and should not be done does not address the issue.Since the authors remained unconvinced that the PD is sensible to positive scalings of a model's parameters, and hence comparing the PDs of two sets of models with different activations (one activation per set) is not sensible, here is a more detailed explanation of this fact.Assume a fairly trivial example for clarity: two 1-d data points, $x_1 = +1, x_2 = -1$, and binary classification models $f_1, f_2$, where $f_1(x) = \sigma(w_1 \cdot \phi(x))$ and $f_2(x) = \sigma(w_2 \cdot \phi(x))$ are the assigned probabilities for the positive label, and $\phi: \mathbb R \to \mathbb R$ captures some notion of activation function and/or scale of weights before the final classification layer. For simplicity, let $\phi(x) = \alpha x$, for some $\alpha \in \mathbb R_+$, and feel free to think of $\alpha$ as a 'magnitude' of an activation function instead of some notion of internal calibration.Then, we have $P_{1,1} = (\sigma(\alpha w_1), \sigma(-\alpha w_1))$, $P_{1,2} = (\sigma(\alpha w_2), \sigma(-\alpha w_2))$, $P_{2,1} = (\sigma(-\alpha w_1), \sigma(\alpha w_1))$, and $P_{2,2} = (\sigma(-\alpha w_2), \sigma(\alpha w_2))$. The PD of the set consisting of the two defined models, after simplifying the 8 relevant terms, ends up being simply $\Delta_1 = |\sigma(\alpha w_1) - \sigma(\alpha w_2)|$. Let's pick some numbers to make this crystal clear: let $\alpha = 1, w_1 = 1.0, w_2 = 0.1$, so we get $\Delta_1 = \sigma(1) - \sigma(0.1) \approx 0.2$ (note that w.l.o.g. we can assume that $y_1 = +1, y_2 = -1$ so that for these weights both models achieve 100% accuracy).Now, take ANOTHER set, consisting of models $g_1, g_2$, defined similarly to $f_1, f_2$, but with $g_1(x) = \sigma(w'_1 \cdot \phi'(x)), g_2(x) = \sigma(w'_2 \cdot \phi'(x))$, where $\phi'$ (not the derivative of $\phi$) captures the the activation function and/or weight magnitude of layers preceding the classification head. Let $\phi'(x) = \beta x$ for simplicity. Consider the case where $\beta = 0.1, w_1 = 1.0, w_2 = 0.1$, i.e. the weights of $g_1, g_2$ are *exactly the same* as the weights of $f_1, f_2$, but $\phi'$ is a 'scaled-down' $\phi$ (e.g. a different activation function): in this case (note that both $g_1$ and $g_2$ achieve 100% accuracy as well), **for this new set of models, consisting of the pair $g_1, g_2$**, we get $\Delta_1 = \sigma(0.1) - \sigma(0.01) \approx 0.02$, a value around 10 times smaller than the PD of the first set of models, **even though the second set predicts the exact same labels for each data point**, and claiming that the set $\{g_1, g_2\}$ is 'more robust' than the set $\{f_1, f_2\}$ in terms of reproducibility is simply factually wrong. If the idea of having $\beta \neq \alpha$ sounds a bit of a stretch since the proposed activations are not simply 'scaled down' ReLUs, consider instead the case $\beta = 1.0, w_1 = 0.1, w_2 = 0.01$ and note that we again get $\Delta_1 \approx 0.02$ for this second set of models: the discrepancy in terms of magnitude of weights can be caused by different optimizers, different strength of $\ell_2$ regularization, or, as my original review already mentioned, smaller variance of gradients w.r.t. activation function.To reiterate, in the above example we did **not**, at any point, compute the PD of a set of models that had different components: both $\{f_1, f_2\}$ (the first set) had the same 'activation function' $\phi$, while $\{g_1, g_2\}$ had $\phi'$.Going a step further, which shows how problematic the PD is as a metric, consider an arbitrary set of binary classifiers $S_1 = \{f_1, f_2, \dots, f_M\}$, where $f_i(x) = \sigma( \langle w_i, \phi(x) \rangle)$ is the probability assigned by the $i$'th model of $x$ belonging to the positive class. Now, take *another* set of binary classifiers $S_2 = \{g_1, g_2, \dots, g_M\}$, with $g_i(x) = \sigma(\langle w_i, \phi'(x) \rangle)$, where $w_i$ is the **same** weight vector that model $f_i$ has (i.e. except for $\phi'$, the set $S_2$ is 'point-wise' identical to the set $S_1$). Finally, let $\phi'(x) = \beta \phi(x)$, where $\beta \in \mathbb R_+$, and feel free to check that for any $\beta$, every model $g_i$ from $S_2$ will agree with the model $f_i$ from $S_1$ in terms of predicted class (i.e. although the class probabilities will change, the rank is be preserved for any $\beta$). This means that $S_2$ produces the **exact same** predictions as $S_1$ for **any possible data point**. Taking $\beta \to 0$ yields in $g_i(x) \to 0.5$ for any $i \in [M]$ and possible $x$, hence **the PD of $S_2$ will go to zero, even though the PD of $S_1$ can be arbitrarily large and the two model sets $S_1, S_2$ agree point-wise in terms of predicted classes**. In other words, taking an arbitrary set of models with ReLU activations, copying its weights and replacing the ReLU by $\phi(x) = \max(0, \frac{x}{10^{10}})$, will yield a second model set with PD close to zero. Hopefully the authors agree with me that this trivial replacement of activation functions does not 'solve' any reproducibility problem in machine learning.With the above in mind, I urge the authors to re-evaluate PD as a metric. As mentioned in my review, the Hamming form does not suffer from this issue, but the reported numbers in this case seem to indicate that there is little to no reproducibility challenge for the adopted tasks. ## Post-rebuttal updateI have read the other reviews and the authors' rebuttals, and do not wish to change my review.I strongly believe that numerical task improvements are not in themselves a conceptual contribution. I look forward to the results of the analyses the authors mention in response to R3-Q2, to better understand what exact interaction between language and low-level visual input is being modeled.Along with R4 I remain unconvinced of the strength of the empirical results. The authors' response is not helpful here. I can't understand where the numbers (mean 60.74 IoU, std 0.06) come from -- taking stats across table 2 and table 1, I get very different results, so I must be misunderstanding where they come from.Significance tests would not take too much time -- it's not absolutely critical that you retrain the models for this. You can use data resampling methods instead. For example, on each individual dataset, run bootstrap tests comparing the predictions of your model and others on random resamples of the evaluation data and corresponding predictions.Pooling IoU results across datasets within model and then comparing between models can yield misleading results and should be avoided.  =====updates======== After reviewing the other reviews and rebuttal, I will remain my original recommendation. ---## UpdateI thank the authors for responding to my questions and revising the paper.As indicated in the first sentence of my review, the submission's biggest flaw is in poor presentation. I have read the revised version, but I am afraid to say that I remain disappointed. Although I can see that the authors have tried to address some of the common concerns, the changes do not go far enough. Given that the current version is a version in which "all [...] comments and suggestions have been taken into account," I do not see a reason to give the authors a benefit of the doubt, and have decided to keep my score as is. However, because my evaluation appears to be quite different from those of the other reviewers, I think it may be possible that they are seeing something in this paper that I am incapable of. Since I cannot profess to have a complete understanding of the content, I have decided to downgrade my confidence score.I will give two examples of ambiguous / confusing language. They are both from Section 2 after the paragraph heading **Definition of constrained black-box uncertainty modeling**.1. (The last sentence of the 1st paragraph) "Given this context, the pointwise forecasting system mentioned above is a function $\beta:\mathbb{R}^D \to \mathbb{R}$, which tries to approximate a certain conditional summary statistic (a percentile or moment) or $p(y | x)$.- My understanding of this sentence is that $\beta$ is a _statistic_ (i.e., a function of $\mathcal{D} = (\mathbf{x}_i,y_i)_{i=1}^{n}$) such that $\beta \approx \beta^*$, where $\beta^*$ is some functional of interest of the conditional distribution $p(y | \mathbf{x})$. For example, $\beta^*$ could be the conditional mean $\beta^*(\mathbf{x}) = \mathbb{E}[Y | X = \mathbf{x}]$, in which case $\beta$ is usually obtained by minimizing the MSE, or $\beta^*$ may be the conditional $\alpha$-quantile, in which case $\beta$ is some data approximation of the conditional $\alpha$-quantile.- Now, compare the above sentence to the following sentence from the author response: "For instance, $\beta(\mathbf{x})$ can be the conditional mean of $p(y | \mathbf{x})$."My point here is that throughout the paper, $\beta$ is used to refer to both a functional of the conditional distribution or a data-based approximation for the said functional. I find the lack of distinction puzzling, if not downright confusing.2. (The 5th paragraph. I think this is supposed to address my objection about the lack of a precise problem statement.) "The overall goal of the present article is, taking a pre-defined black box $\beta(x)$ that estimates a certain conditional summary statistic of $p(y | \mathbf{x})$, to model $q(y | \mathbf{x})$ under the constraint that if we calculate the summary statistic of this predicted conditional distribution, it will correspond to $\beta(\mathbf{x})$."- What is $q(y | \mathbf{x})$, and what is its precise relationship to $p(y | \mathbf{x})$? The first occurrence of $q(y | \mathbf{x})$ is followed by the description that it is "a conditional density model," but this can mean many things. Judging from the remainder of the article, $q(y | \mathbf{x})$ is either a model defined through a location-scale family or a model defined via a specification of quantiles. The proposed method is exclusively concerned with the latter kind of models. If this is the case, it would help the reader to have this said explicitly when the symbol $q$ is first introduced.- The usage of the term "statistic" does not appear to be standard. A _statistic_ is a function of the data $\mathcal{D}$, and is therefore a random quantity. A percentile or a moment of $p(y | \mathbf{x})$, on the other hand, is a functional of the conditional distribution $p(y | \mathbf{x})$, and would be non-random.- Even ignoring the previous points, the sentence made little sense to me. I will say more on this after the following paragraph.More on the subject of clarity, notation-wise, I am bothered by the proliferation of $p$'s. Aside from the occurrence in $p(M | \mathbf{x})$, which is rare and hence not a cause for concern, $p$ is used to represent the conditional density (distribution) $p(y | \mathbf{x})$, as well as a generic Chebyshev polynomial approximation $p(\tau,\mathbf{x};d)$. It is possible that I am failing to see some deeper connection here, but if none exists, I would prefer a more clear system of notations.Returning to the problem statement, after re-reading the paper several times and with the help of the author response, I have arrived at the following interpretation:Find an estimate / model $q(y | \mathbf{x})$ for the conditional density (distribution) $p(y | \mathbf{x})$ such that $q$ and $p$ have the same $\beta$. Here, $q$ is given by specifying the quantiles, and is approximated by a Chebyshev polynomial of degree $d$. The usual QR methods are inadequate, because the model they output will not have the same conditional percentile / moment / etc. as $\beta$.However, in situations in which $\beta$ is also being approximated, what is the point in constraining the functional at $\beta$? As pointed out by the authors, if $\beta$ is inaccurate, then this error is propagated to the quantiles, and the performance is worse. If this is the case, why is it of practical interest to enforce the constraint? Had the authors given a concrete, illustrative example, this question may not have arisen. However, absent a practical motivation, it is hard to understand why the constraint needs to be enforced at all.In any case, suffice it to say that I am still experiencing significant difficulty pinning down the exact goal of the paper.At the time of writing my initial review, I had hoped that the issues with clarity would be fixed in the revision, leaving me free to evaluate the paper based on the merits of the proposal. However, this has not been the case. Now that I am more familiar with the paper, I realize that the biggest issue about the clarity has much to do with the organization, ambiguous language, etc. Some of it has been addressed in the revision, but the effort did not go far enough. Even leaving aside any reservation I have about clarity from the point of view of methodology and/or theory, I have to say that I am deeply dissatisfied about the lack of practical motivation. I believe the authors intended the paragraph after Section 2 to be a response to concerns about poor motivation (which have been raised by other reviewers, not I): "The present article was motivated by a real-world need that appears in a pointwise regression forecasting system of a large company. Due to the risk nature of the internal problem where it is applied, uncertainty modeling is important." It would have been nice if the authors had chosen to respond by giving a _concrete_ example, complete with a real data set and an actual task to which their method is an adequate solution. However, the example given in the added paragraph is far too generic and vague to be useful as an illustrative example.Finally, when I asked for theoretical justification, I was looking for some actual proofs about e.g., consistency guarantees, approximation quality, etc.---## MinorI find the size of the text in the plots to be much too small and hard to read. Also, please label all axes in Figure 3. This paper applied an object detection network, like SSD, for optical character detection and recognition. This paper doesn't give any new contributions and has no potential values. # No RebuttalSince there is no rebuttal, I have not modified my score. ####post rebuttal####The contribution of this paper is marginal only. The link between adversarial robustness and calibration has been explored previously: Snoek et al. NeurIPS 2019 have shown that adversarial training as part of deep ensembles leads to better calibration under domain shift. Unfortunately the authors do not compare their approach to these deep ensembles, but only an ensemble of differently initialised vanilla networks without adversarial training, which they call deep ensembles (section 5.1). Also in terms of label smoothing the contribution is marginal: in their rebuttal the authors show that MixUp training - a different implementation of label smoothing combined with input smoothing - has a better performance than their method (ECE of 1.8 MixUp vs 2.3 their method for CIFAR-100); they do show that further post-processing improves their method, but this is likely true for MixUp too (results not shown). For ImageNet results for MixUP are not shown, nor for calibration under domain shift where MixUp is likely to perform well too. Taken together, this suggests that the link between adversarial robustness and calibration is mainly a link between OOD samples and calibration: generating OOD samples with input smoothing in MixUp works very well compared to the proposed approach, as does adversarial training in deep ensembles (both of which was shown in prior work). In summary, the proposed approach lacks novelty and performs worse than baselines for complex datasets.Lack of code during the reviewing phase means it is not possible to review reproducibility of results. ## After rebuttal update.Given there is no rebuttal, there will be no update. ############ Updated Review #################I have read the author(s)' rebuttal. My decision stays unchanged. In my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for ICLR. Additionally, the author(s) did not update their submission to reflect the changes. I thereby recommending rejection to this submission. ########################################## ------------------------------------------POST-REBUTTAL COMMENTSThanks for your comments.Re: A2, time augmentation in finite-horizon settings increases the size of the state space you need to keep in memory by at most a factor of 2... But in any case, further discussion of this issue will have to wait until the revision with the additional experiments that you mentioned.Re: A5, regarding "D" being analogous to "gamma" -- fair enough, but this approach is still a meaningful baseline to compare against.Regarding the statement "contrary to this, initializing state values pessimistically requires task-specific knowledge", this isn't accurate. In a MAXPROB instance, initializing the the value function at all states to 0 is a problem-independent pessimistic initialization. Of course, task-specific knowledge may help design a better one, but it isn't _necessary_.In conclusion, as the original review mentioned, I believe that the presented "loop penalty" idea may well have conceptual merit, but I encourage you to think more carefully how you "sell" it, because so far neither the original submission nor the rebuttal present convincing arguments that it is better than the alternatives either theoretically or empirically.--------------------------------------------------- ==== Updates after the response ====I appreciate the authors effort in updating the manuscript. The new manuscript has significant changes, but still many questions are left unanswered. Thus, Im keeping my rating and recommendation. Update: The author response has not changed my opinion that there is insufficient new material in this paper vs the ISIT paper, and the presentation of the material from the ISIT paper does not note that this material was previously presented there. Without clarity in what is the novel material claimed in this paper it should not be accepted.