 This paper explores the idea of using state and action embeddings for more scalable learning. The idea has merit so I encourage the authors to continue working in this direction, but unfortunately there are a number of technical issues with the paper that I detail below.The main issues for me are:1. **There are a lot of relevant references missing.** With regards to planning over latent spaces here are some:  - "Recurrent World Models Facilitate Policy Evolution", David Ha and Jürgen Schmidhuber, NeurIPS 2018.  - "Model-Based Reinforcement Learning for Atari", Lukasz Kaiser et al. ICLR 2020  - "Learning Latent Dynamics for Planning from Pixels", D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, ICML 2019  - "Mastering Atari with Discrete World Models", D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv 2020    The authors seem to be learning equivalence relations over the states and actions in MDPs (this is basically the result of Assumption 2). There is a large body of work on this, and here are some important references:  - Givan, R.; Dean, T.; and Greig, M. 2003. "Equivalence notions and model minimization in Markov decision processes". Artificial I Intelligence 147(1-2): 163223.  - Ferns, N.; Panangaden, P.; and Precup, D. 2004. "Metrics for finite Markov decision processes". In Proceedings of the 20th conference on Uncertainty in artificial intelligence, 162169. AUAI Press.  - Li, L.; Walsh, T. J.; and Littman, M. L. 2006. "Towards a Unified Theory of State Abstraction for MDPs. In ISAIM.  - Taylor, J.; Precup, D.; and Panagaden, P. 2009. "Bounding performance loss in approximate MDP homomorphisms". In Advances in Neural Information Processing Systems, 16491656.  - Castro, P. S.; Panangaden, P.; and Precup, D. 2009. "Notions of state equivalence under partial observability". In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09).  - Castro, P. S. 2020. "Scalable methods for computing state similarity in deterministic Markov Decision Processes". In Proceedings of the AAAI Conference on Artificial Intelligence.  - Zhang, A.; McAllister, R.; Calandra, R.; Gal, Y.; and Levine, S. 2020. "Learning Invariant Representations for Reinforcement Learning without Reconstruction. arXiv preprint arXiv:2006.10742 .2. **Lemma 1 is not correct.**  - Consider the following simple example: an MDP with two states ($s, t$) and two actions ($a,b$). Action $a$ always transitions deterministically to state $t$, while action $b$ always transitions deterministtically to state $s$. In state $s$ taking action $a$ gives a reward of 1 while taking action $b$ gives a reward of 0; in state $t$ taking action $a$ gives a reward of 0 while taking action $b$ gives a reward of 1. The optimal policy is to take action $a$ from state $s$ and action $b$ from state $t$, resulting in $V^*(s) = V^*(t) = \frac{1}{1-\gamma}$. Now, take $X = \lbrace x_1, x_2\rbrace$ where $\phi(s) = x_1$ and $\phi(t) = x_2$ (this satisfies Assumption 2). Now take $E = \lbrace e\rbrace$ where $f(e) = a$, so $f^{-1}(a) = \lbrace e\rbrace$ (this satisfies Assumption 1). Letting $g:A\rightarrow E$ be the action embedding function, we can see that $g(a) = g(b) = e$, which means that any internal policy will be suboptimal, as it will choose the same latent action for both $x_1$ and $x_2$.  - Part of the issue may lie in the way the proof is structured. The authors start by using $G$ which they define as "denotes the return, which is a function of $s$, $a$, and $s'$". However, the way they've included it in the definition of $v^{\pi}$ seems like it does _not_ depend on $s'$ and is really just $Q^{\pi}(s, a)$. This seems to be the case as that is where the authors end up at the end of the proof. If it _is_ supposed to depend on $s'$, then $G$ likely needs to be decomposed into the one-step reward $R(s, a)$ and the expected value at the next state $\mathbb{E}V^{\pi}(s')$.  - The other bug I found in the proof is at the top of page 12. The authors went from $P(a | s, a')$ at the end of page 11 to $P(a, x | s, a')$ at the top of page 12, which is most certainly not a valid equality.  - (Minor) The jump from $P(s, a)$ to $\pi(a | s)P(s)$ is probably ok, but requires a little more justification.3. **The losses are incorrect**. Below equation (4), the authors state "the denominator does not depend on $\phi$, $g$, or $T$, and so they get rid of the $P(S_{t+1}|S_t, A_t)$ term in their loss, but this means there is no target for the loss! This means that the loss in equation (5) is just trying to maximize the probabilities uniformly, independent of whatever the true probabilities really are. This problem is also present in the loss in equation (6).Medium issues:1. The authors make a number of claims about their method working for continuous spaces, but this requires more details than simply replacing integrals for sums. For example, in the background the transition function T needs to be defined with respect to Borel sets, the summation in Lemma 1 doesn't "just work" by switching to integrals, etc..2.  Given that the authors motivate their work by claiming that existing algorithms don't work well outside of "simple" tasks, they should include larger scale experiments than the ones they are currently including, which are rather small.3. Doesn't Assumption 3 defeat the purpose of dealing with large state spaces? It's basically just converting the original state space into an isomorphic one.Minor issues:1. In the Introduction the authors refer to the ALE as "comparatively simple tasks", but they are not simple and are in fact more difficult than any of the environments evaluated in the paper.2. In equation (3) it should be $\hat{T}(X_{t+1} | X_t, E_t)$ instead of $\hat{T}(S_{t+1} | X_t, E_t)$ The authors describe an anomaly/novelty detection method based on handcrafted features + VGG based features. I think the paper is out of the scope of the conference (the only part dealing with learned representations uses VG), plus it addresses a problem whose relevance is not correctly motivated. Finally, the method is quite basic, and is not compared to any state of the art method for novelty detectiobn.  In "... the detection of visual anomalies perceived by human observer is an open challenge& " can you provide references of people working in this particular problem?The review of related work seems obsolete, can you provide more recent references (in addition to "historical" ones). More importantly, please provide references of anomaly detection from textures This paper aims to improve on Hindsight Experience Replay by removing the need to compute rewards for reaching a goal. The idea is to frame goal-reaching as a shortest path problem where all rewards are -1 until the goal is reached, removing the need to compute rewards. While similar ideas were explored in a recent arxiv tech report, this paper claims to build on these ideas with new loss functions. The experimental results do not seem to be any better compared to baselines when measured in terms of data efficiency, but the proposed method requires fewer reward computations.Clarity:While the ideas in the paper were easy to follow, there are a number of problems with the writing. The biggest problem is that it wasnt clear exactly what algorithms were evaluated in the experiments. There is an algorithm box for the proposed method in the appendix, but its not clear how the method differs from the FWRL baseline.Another major problem is that the paper does a poor job of citing earlier related work on RL. DQN is introduced without mentioning or citing Q-learning. Experience replay is mentioned without citing the work of Long-Ji Lin. Theres no mention of earlier work on shortest-path RL from LP Kaelbling from 1993. Novelty and Significance:After reading the paper I am not convinced that theres anything substantially new in this paper. Here are my main concerns:1) The shortest path perspective for goal-reaching was introduced in Learning to Achieve Goals by LP Kaelbling [1]. This paper should be cited and discussed.2) I am not convinced that the proposed formulation is any different than what is in Hindsight Experience Replay (HER) paper. Section 3.2 of the HER paper defines the reward function as -1 if the current state is not the same as the goal and 0 if the current state is the same as the goal. Isnt this exactly the cost-to-go/shortest path reward structure that is used in this paper?3) This paper claims that the one-step loss (Equation 8) is new, but it is actually the definition of the Q-learning update for transitioning to a terminal state. Since goal states are absorbing/terminal, any transition to a goal state must use the reward as the target without bootstrapping. So the one-step loss is just Q-learning and is not new. This is exactly how it is described in Section 3 of [1].4) The argument that the proposed method requires fewer reward evaluations than FWRL or HER seems flawed. HER defines the reward to be -1 if the current state and the goal are different and 0 if they are the same. As far as I can tell this paper uses the same reward structure, so how is it saving any computation?Can the authors comment on these points and clarify what they see as the novelty of this work?Overall quality:Unless the authors can convince me that the method is not equivalent to existing work I dont see enough novelty or significance for an ICLR paper.[1] Learning to Achieve Goals LP Kaelbling, 1993. The works proposes a generalization of MMD-squared distance. However, the submission seems to be an incomplete one.Major comments:1. Definition 2 seems to be the key definition in the work. However, there are multiple issues:       a. It is not clear why it is called a kernel? Should not it be called distance? After all, it generalizes MMD-squared!       b. "$K$" seems to be mixed up with "$k$". "$K$" seems to be the gram matrix and not the kernel.       c. $k(y_i, y_j)$ is from $Y \times Y\rightarrow R$, and not from $R^n \times R^n \rightarrow R$.       d. why should "\phi" belong to RKHS of k? Recall that RKHS of k will contain functions from R^n\rightarrow R.2. I agree with the write-up which states  that results in section 4.2 are trivial.3. section 4.3.1 are known results and need to be skipped.4. Proof of theorem 5 seems to be completely missing. How is sup over f removed?5. In Definition 6, what is $f$? Is a sup over f missing? Because of this and the previous issue, section 5.1 seems very incomplete.6. Simulation section seems to be completely missing.7. Connection with Bernstien polynomials highlighted in intro etc. seems to be missing. This is a well-motived paper that considers bridging the gapin discrete-time continuous-state/action optimal controlby approximating the system dynamics with a convex model class.The convex model class has more representational power thanlinear model classes while likely being more tractable andstable than non-convex model classes.They show empirical results in Mujoco continuous-controlenvironments and in an HVAC example.I think this setup is a promising direction but I havesignificant concerns with some of the details and claimsin this work:1. Proposition 2 is wrong and the proposed input-convex recurrent   neural network architecture not input-convex.   To fix this, the D1 parameters should also be non-negative.   To show why the proposition is wrong, consider the convexity of y2   with respect to x1, using g to denote the activation function:       z1 = g(U x1 + ...)       y2 = g(D1 z1 + ...)   Thus making       y2 = g(D1 g(U x1 + ...) + ...)   y2 is *not* necessarily convex with respect to x1 because D1 takes   an unrestricted weighted sum of the convex functions g(U x1 + ...)   With the ICRNN architecture as described in the paper not being   input-convex, I do not know how to interpret the empirical findings   in Section 4.2 that use this architecture.2. I think a stronger and more formal argument should be used to show   that Equation (5) is a convex optimization problem as claimed.   It has arbitrary convex functions on the equality constraints that   are composed with each other and then used in the objective.   Even with parts of the objective being convex and non-decreasing   as the text mentions, it's not clear that this is sufficient when   combined with the composed functions in the constraints.3. I have similar concerns with the convexity of Equation (6).   Consider the convexity of x3 with respect to u1, where g is   now an input-convex neural network (that is not recurrent):       x3 = g(g(x1, u1), u2)      This composes two convex functions that do *not* have non-decreasing   properties and therefore introduces an equality constraint that   is not necessarily even convex, almost certainly making the domain   of this problem non-convex. I think a similar argument can be   used to show why Equation (5) is not convex.In addition to these significant concerns, I have a few otherminor comments.1. Figure 1 hides too much information. It would be useful to know,   for example, that the ICNN portion at the bottom right   is solving a control optimization problem with an ICNN as   part of the constraints.2. The theoretical results in Section 3 seem slightly out-of-place within   the broader context of this paper but are perhaps of standalone interest.   Due to my concerns above I did not go into the details in this portion.3. I think more information should be added to the last paragraph of   Section 1 as it's claimed that the representational power of   ICNNs and "a nice mathematical property" help improve the   computational time of the method, but it's not clear why   this is and this connection is not made anywhere else in the paper.4. What method are you using to solve the control problems in   Eq (5) and (6)?5. The empirical setup and tasks seems identical to [Nagabandi et al.].   Figure 3 directly compares to the K=100 case of their method.   Why does Fig 6 of [Nagabandi et al.] have significantly higher rewards   for their method, even in the K=5 case?6. In Figure 5, f_NN seems surprisingly bad in the red region of the   data on the left side. Is this because the model is not using   many parameters? What are the sizes of the networks used? This paper presents a framework for understanding why seq2seq neural response generators prefer "universal"/generic replies. To do so the paper breaks down the response generation probability into the probability of selecting the set of tokens (reflecting the topic of the output) and then selecting an ordering of the tokens (reflecting the syntax of the output.)The results presented in this paper are not technically sound. E.g the derivation in Eq(2) derive a meaningless bound. Here is why:1. The first equality assumes that the words in a set are independent which is not true.2. In the second equality, the authors incorrectly replace the summation of word probability in each sentence with the summation of word probabilities over all unique words (the set) overall sentences. This is simply not true if there are common words shared between sentences.3. Perhaps the biggest issue is the incorrect application of Jensen's lemma. JL is often used as log(\sum_i a_i x_i) &gt; \sum_i a_i log x_i if \sum_i a_i = 1. Instead what authors have used is log (\sum_i x_i) &gt; \sum_i log(x_i), which is not always true, and is trivially true for all x_i &lt; 1. In fact, this bound is not even tight (unlike Jensen's lemma) and the *worst* part is that the LHS increases if we add more x_i (&lt;1) and the RHS decreases. This means this bound is far from being meaningful and as such should be summarily ignored.Similarly, in section 2.3, the technical content is quite poor. Why is this true -- "the amount of possible queries M of y... 1 &lt;&lt; M \propto N"? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in "non-universal" replies? I am not going more into the details in this section because I think the problems with section 2.2 are themselves dealbreakers.Overall, given the problems this work is not technically sound to be accepted. The title is misleading, since only two particular manifolds are studied in this work. In addition, the proposed methods cannot be applied to a larger or a general class of manifolds. Therefore, you should update the title.There are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. In addition, some of the proposed problem definitions are incorrect, as explained below:You should be more precise about the definition of the manifold you consider in this paper. For example, in equation (1), please define your manifold of interest more precisely checking some standard textbooks.Please define intersection of manifolds, what do you mean by which intersection of which type of manifolds?In the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.In the contribution (2): It is not clear why and how the proposed method can be applied to optimization on manifolds with momentum (what do you mean by use of momentum here?), and regularization (what do you mean by regularization?). There are many problems with this claim, but you can simply consider that applying momentum and regularization will affect the geometry of loss landscape.Definition of retraction is not precise, please fix it.What is L in equation (2)?Please define neighborhood in U_x in Lemma 2.1.What is || ||in Lemma 2.1?As you also noticed on page 3, an intersection of manifolds may not be a manifold. Then, your proposed first problem (1) fails. Therefore, you should completely change your claims on your problem definitions and contributions.What do you mean by We add a drift which contains information from the other manifold to the original gradient descent on manifold? What is the information from the manifold? In equations (3) and (4), you just apply optimization on manifolds individually. How do you compute/determine a_k^(1) and a_k^(2)? How do they affect the theoretical and experimental results?In your claim From the construction of bk, we can see that the smaller the correlation between gradf(xk) and hk is, the smaller effect the information from M2 brings, it is not clear how the information from M2 affects? First, again, what is the information? Second, b_k^(1) and b_k^(2) are computed for individual manifolds separately. Then, how the information make an effect? In Theorem 2.2, what do you mean by then xk convergence to a local minimizer?What is &lt;,&gt; in Theorem 2.2?What is ^ in Theorem 2.3?What is v in proof 6?What is an engine value?What does P (1) xk gradf(yk) denote in computation of h_k? For example, gradf(yk) is a vector on tangent space of the second manifold at yk. Then, how do you project orthogonally this projected vector to the tangent space of the first manifold at xk? They may be completely different geometries, and such an orthogonal projection may not exist in general. Then, how do you compute and calculate that projection?All the theoretical results given in the paper are not about convergence of parameters on a manifold at the intersection or product of manifolds but for an individual manifold. For example, x and y belong to manifolds M1 and M2, and convergence results is about x. How are they related to parameters at the intersection or product of manifolds?The statements regarding batch normalization are confusing and also sound incorrect:Do you apply batch normalization on weights on BN(w)?Please explain what you mean by BN(w) has same image space on G(1, n) and St(n, 1). There are not such results in the papers Cho &amp; Lee (2017); Huang et al. (2017) you cited for these results.What do you mean by applying optimization on manifold to batch normalization problem?In your statement However, the property of these two manifold implies that we can actually optimize on the intersection of two manifolds. Please explain how does this property imply this result more precisely?Please define Grassmann manifold G(1, n) more precisely. In your notation, together with explanation of the notation for St(n,p), G(1,n) is like a set of 1xn dimensional row vectors, while St(n,1) is an nx1 dimensional column vector, Then, their intersection is an empty set and your proposal for optimization on a vector on their intersection is wrong. Notation and definitions used in (9) are wrong and confusing. Please check and revise them.In the whole paper, the problem, method, solutions, theorems, and contributions are proposed for optimization using parameters which belong to intersection of some manifolds. Then, suddenly, you start considering optimization on product manifolds, and give the results for that;What does the statement Then we apply Algorithm 1 to update parameters, which means we optimize on a product manifold mean?What do G(1, k1) × · · · G(1, kn) and St(k1, 1) × · · · St(kn, 1) denote?Dont you perform optimization on intersection of manifolds? Why do you ignore your original problem and methods, and consider this problem? In addition, how do you use your Algorithm 1 for optimization on product manifolds? Optimization on intersection on manifolds and product manifolds are completely different problems. If they are same or related to each in particular cases in your specific definitions, then you should provide these definitions more precisely.What do you mean by optimization on product manifold of weights of all layers? If you compute a product manifold for spaces of all layers, then you simply perform a shallow optimization on a huge matrix containing millions of dimensions according to this definition. First, how do you do that? Second, how can you train a large network using this approach?In the experiments, please first give variance of errors. These results are statistically insignificant.Which problem is solved to perform these experiments is not also clear (see above).The results reported in the paper are also not good, may be due to the mathematical and algorithmic  problems and errors mentioned above. Please clarify them, and provide additional results, especially using other datasets (small scale mnist and large scale imagenet), and networks (mlp, vgg, resnet etc.)Related work is also incomplete, such that many traditional and recent work on optimization on multiple manifolds are omitted. This paper lacks any novelty/contribution as it just applies well-known and standard architectures for object detection (SSD) and image classification (LeNet) trained with standard algorithms and losses.Moreover, I fail to see what is the purpose of the proposed pipeline and it is not clear at all how it may help improving existing OCR engines in any particular scenario (handwriting recognition, printed text, historical documents, etc.). No demonstration or comparison with state of the art is provided. The authors claim This work is the first to apply modern object detection deep learning approaches to document data but there are previously published works. For example:Tuggener, Lukas, et al. "DeepScores--A Dataset for Segmentation, Detection and Classification of Tiny Objects." ICPR 2018.Pacha, Alexander, et al. "Handwritten music object detection: Open issues and baseline results." DAS 2018.Actually, in my opinion Music Object Detection in musical scores would be a much better test-bed/application for the proposed pipeline than any of the datasets used in this paper.  The datasets used in the experimental section seem to be created ad-hoc for the proposed pipeline and do not come from any real world application. Finally, the presentation of the paper is marginal. Data plots have very bad resolution, there are no captions in any table or figure and they are not correctly referenced within the text. There seem to be also missing content in the last sections which makes them impossible to read/understand. The authors experiment with building an SSD-like object detection network for text detection in documents, by replacing the usual VGG or ResNet base architecture with a light weight model inspired by the original digits classification CNN from [LeCun et al 1999].This paper is a pure technical report with no novel contribution: all the authors do is replace the "body" network in the well-known SSD architecture with a simpler model (taken from existing literature) and evaluate it on two synthetic benchmarks of their creation.The idea of employing object detection CNNs for OCR is not novel either, as pointed out in the related works section.Beside the absence of novelty, the paper also suffers from several other serious flaws:1) One of the main motivations provided by the authors for this work is that existing "classification [...] detection [...] or segmentation networks, cannot be applied directly, even with finetuning".However, no experimental results are reported to justify this claim.In fact, in the experimental section the proposed network is not compared against any existing baseline.2) The text has serious clarity and formatting issues, in particular:- most tables and figures have no caption, and the few that have one are not numbered- the text exceed both the 8 pages limit and the extended 10 pages limit allowed in the case of big figures- the experimental section is very confusing, in particular the way the authors refer to the various network variants using long code names makes it really hard for the reader to follow the ablation studies- given the absence of proper captions and numbering, it is quite hard to understand which table refers to which experiment- most of the graphs seem to be in the form of low-resolution bitmaps, which are quite hard to read even on screen- many entries in the References section are either missing the venue, or point to an arXiv link even when a proper conference / journal reference would be available3) Some important details about the network are missing, in particular the authors do not mention how labels are assigned to the network outputs, and only give a vague indication about the losses being used.Similarly, there's no mention about the use of NMS, which is also an important component of the two architectures (SSD and YOLO) that inspire this work.Assigning labels and performing NMS are actually some of the most crucial components in the training of object proposal / object detection networks, often requiring numerous meta-parameters to be properly configured and tuned, as testified by the meticulous descriptions given in previous works (e.g. YOLO and Fast / Faster / Mask r-CNN).4) The experimental section is very poorly organized and formatted (as mentioned in (2) above), and completely lacks any comparison with other state of the art approaches.A lot of space is devoted to presenting a detailed ablation study which, in my opinion, doesn't contribute much to the overall paper and actually reads more like a report on meta-parameter tuning.Finally, starting from Section 5.3.1 the text seems to be copy-pasted without a second read from some differently formatted document, as entire phrases or possibly tables / figures seems to be missing.In conclusion, in my opinion this paper does not meet the conference's minimum quality standards and should definitely be rejected. This paper propose to use important sampling to optimize VAE with discrete latent variables. Basically, the methods proposed by this paper is rather simple and trivial. There are some discussions on why important sampling is not a good choice for VAE. Please refer: https://stats.stackexchange.com/q/255756Moreover, if you focus on VAE with discrete latent variable, you should compare at least with Gumbel-Softmax: https://arxiv.org/abs/1611.01144 This paper proposes an architecture for learning representations on molecules. The paper contains a number of typos, and presents results and an entire paragraph that is a near duplicate copy from prior work (which is not cited). In particular Table 1 is a near copy of Table 2 appearing in [1]. I find it particularly suspicious that the authors have a near copy of this prior table, which reports the ratio of MAE to chemical accuracy, and not MAE directly. The authors have the exact same numbers as this prior Table 2 but claim they are reporting MAE directly. Despite this table being a direct copy, it conveniently omits the columns from [1] which outperform the results presented here.Also the paragraph describing this table is nearly identical to the paragraph in [1], that is the authors write"These baselines include 5 different hand engineered molecular representations, which then get fed through a standard, off-the-shelf classifier. These input representations include the Coulomb Matrix (CM, Neese (2003)), BoB, Bonds Angles, Machine Learning (BAML, Huang &amp; von Lilienfeld (2016)), Extended Connectivity Fingerprints (ECPF4, Rogers &amp; Hahn (2010)), and Projected Histograms (HDAD, Faber et al. (2017a)) representations. In addition to these hand engineered features we include the Molecular Graph Convolutions model (GC, Kearnes et al. (2016)), the original GG-NN model(Li et al., 2015) trained with distance bins and DTNN."In [1] it was written"These baselines include 5 different hand engineered molecular representations, which then get fed through a standard, off-the-shelf classifier. These input representations include the Coulomb Matrix (CM, Rupp et al. (2012)), Bag of Bonds (BoB, Hansen et al. (2015)), Bonds Angles, Machine Learning (BAML, Huang &amp; von Lilienfeld (2016)), Extended Connectivity Fingerprints (ECPF4, Rogers &amp; Hahn (2010)), and Projected Histograms (HDAD, Faber et al. (2017)) representations. In addition to these hand engineered features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from Kearnes et al. (2016), and the original GG-NN model Li et al. (2016) trained with distance bins."The footnote in the table is also a duplicate copy of the footnote in [1].Theirs: "As reported in DTNN. The model was trained on a different train/test split with 100k training samples vsabout 110k used in our experiments."[1]: "As reported in Schutt et al. ¨ (2017). The model was trained on a different train/test split with 100k training samples vs 110k used in our experiments."The proposed method itself is a variant of the MPNN framework introduced in [1], yet underperforms the original results in [1], as well as improved MPNNs (or GNNs) shown in [2,3]. 1. https://arxiv.org/pdf/1704.01212.pdf2. http://papers.nips.cc/paper/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions3. https://arxiv.org/pdf/1806.03146.pdf The paper is very poorly written. It is hard to understand what the real contribution is in this paper. The connection of the model with HMM is not clear. The literature review has to be rewritten.To the reader, it sounds that the authors are confused with the fundamentals itself: mixture model, Bayesian models, inference. &gt; Mixture models can be based on any of the exponential family distributions - Gaussian just happens to be the most commonly used.&gt; Again if this is a Bayesian model, why are #clusters not inferred? The authors further mention that in their Pystan implementation K clusters were spun too quick. What was the K used here? Was it set to a very large value or just 3? Did the authors eventually use the truncated infinite mixture model in Pystan?&gt; The authors mention their model is conceptually similar to EM but then end up using NUTS. &gt; Why is a url given in Section 2.3 instead of being given in the references? &gt; Provide a plate model describing Section 3.2. This paper proposes an approach to zeroth order optimization based on the Banach fixed point theorem for contractive maps. They define a "weak contraction map," argue that it will have a unique fixed point, and use this to propose a zeroth order optimization algorithm which iteratively identifies sublevel sets of the objective until convergence to the optimum.At each iteration $t$, the $f(x_t)$-sublevel set of $f$ is found using a root-finding algorithm, and the next point $x_{t+1}$ is calculated by averaging a collection of points on the boundary of the sublevel set.My main concern about this paper is that the optimization algorithm works neither in theory nor in realistic practical scenarios. There are two main issues:(1) Identifying the sublevel sets requires solving equations of the form $f(x) = L$, which is just as hard as optimizing $f$ in the first place! In many realistic scenarios, e.g. machine learning problems, you know what the minimum value of the function is, so you could just solve $f(x) = f^*$ and be done in one step! Even if you don't know the optimal value, you could do some version of binary search. Also, for the two or three dimensional problems with relatively simple expressions that the authors experimented on, finding roots might be possible, but for higher dimensions or more complicated functions, finding these roots would require numerical optimization--which is the problem we are trying to solve in the first place.(2) The authors seem to imply that this algorithm would work for any $f$, however, consider the function in 1 dimension $f(x) = 1$ for all $x \neq x^*$, and $f(x^*) = 0$. For this function, the $f(x_t)$-sublevel sets are the entire domain until $x_t = x^*$. It is unclear what "points on the contours" would mean in this case, but whatever those contours are, the algorithm would never converge on this function because the function value of $x_0, x_1, ...$ would all be the same, so the contours would remain the same. This function might seem a little ridiculous, but continuous or even Lipschitz version of this counterexample could be constructed by smoothing things out around $x^*$, and a function such as this could be obfuscated by writing it down with a long, complicated expression making it hard to identify $x^*$ by inspection.There are some typos/typesetting issues. It seems that all of the theorem and lemma statements in Section 2 are missing the bold "Theorem" and "Lemma" heading. In the paragraph after equation (2), "weak contraction mapping" is defined twice, I believe the first definition should be just a "contraction mapping." This paper proposes the dual formulation of a two layer neural network, which makes the loss for training convex. The convexity guarantees the global optimality of training step compared to training on the primal loss function. The norm regularizer is applied to ensure the generalization. Then the paper gives explanation and experiments regarding the dual nn. However, I'm not sure what is the novelty of the work. On the theoretical side, I think Thm 1 is a straightforward computation of the dual function, and I cannot see if there's any novel technique beyond the common textbooks. I think if there's any dual computation in Sec 4.3 for nn with any number of layers, it could be more interesting. So far I don't think it's sufficient for ICLR.Despite that, even if the majority of contribution is practical performance -- I cannot see what's the goal of the experiments. If this approach is better than the usual nonconvex nn formulation, at least we should see a clear difference. The solid and dash lines are really close to each other, in terms of computation time/epochs or error/loss I cannot see the advantage. The claims in experiment section are vague. E.g., "Zero duality gap", there should be a proof to justify it. I find the proof in appendix eq (12) that $p^* = d^*$. However, to use Slater's condition, is the primal problem convex? I cannot find the proof. (if this is proven I will definitely raise my score) If it's empirical, even the loss looks similar in the experiments, I'm not sure what setup guarantees a zero duality gap (what's the distribution of training samples, what's the underlying relation between x and y, what's the size of nn, etc.) This is really a theoretical claim, otherwise please say "small duality gap observed empirically".Also it would be great to compare with neural tangent kernel works where GD finds the global optimum of nonconvex primal formulation with high probability. Based on NTK I'm not sure if convexity is so important. Regarding convex NN, this paper is also interesting.https://openreview.net/forum?id=H1MW72AcK7 review:This paper addresses the effects of gradient descent methods onto compositionality and compositional generalization of models. The authors claim that the optimization process imposes the models to deviate compositionality, which is defined with conditional independence among random variables of input, predicted output and the ground-truth. Since compositionality is one of important features of human intelligence, it has been interested widely in the field of AI/ML such as vision, language, neuro-symbolic approaches, common sense reasoning, disentangled representation, and the emergence conditions of compositionality. As it has been not much focused on the relationship with optimizers, it is fresh and interesting. However, it is not easy to figure out the position of this paper from two reasons: (1) the definitions on compositionality in this paper are not so compatible with recent related works, which mostly consider certain structures in models [ICLR19, JAIR20] or representative problems such as visual reasoning [CVPR17] and Raven progressive matrices [PNAS17]. (2) The authors do not consider quantitative approaches such as compositionality [ICLR19] or compositional generalization [ICLR20]. In this paper, the main claim is very broad argument. To verify this claim, the authors provide supports of both theoretical and experimental aspects. Theoretically, they try to show that reducing loss values in the optimization process induces utilizing other input variables including useful information based on mutual information. Experimentally, they show the gaps between several settings of accuracy curves with the MNIST dataset (vision) and the SCAN dataset (language). With both aspects, theoretical steps are vague and weak, and the experimental results are little persuasive and convincing.Some steps in theoretical derivation seem to be wrong.I recommend trivial and wrong for this paper.Pros:They deal with the relationship among compositionality, compositional generalization and gradient descent. It is interesting and novel question as far as I know.Concerns:-It is not clear the assumptions on models is covered in the main claim. Some arguments have readers guess the claim only on neural networks. Currently, it is not explicit. What if a model is naïve Bayes classifier which assumes conditional independence? Does it have compositional generalization? If the classifier is trained with gradient descent, the key argument of the paper has counterexamples, which becomes wrong.-Theorem 1 should show more clearly Markov chain structure among X, Y and Z. X -> Y -> Z (as written in Cover 1999 p.34)-What is the relationship between Y and X in Proposition 1? -The proof in Proposition 2 seems not valid. Is the Markov chain among Y hat, X, and Y still valid? Without any constraints of X and Y, the equation in the middle of Proposition 2 seems not an identity (consider joint probability models with discrete values), and the derivation process is not trivial. The validity of this result is a factor that also affects subsequent verification.-There is no quantitative analysis with measurable cases as mentioned above.[CVPR17] Johnson et al., CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning, CVPR 2017.[PNAS17] Duncan et al., Complexity and compositionality in fluid intelligence, PNAS 2018.[ICLR19] Jacob Andreas, Measuring compositionality in representation learning, ICLR 2019.[ICLR20] Keysers et al., Measuring compositional generalization: a comprehensive method on realistic data, ICLR 2020.[JAIR20] Hupkes et al., Compositionality decomposed: how do neural networks generalise?], JAIR 2020. This paper proposes a neural model called Span-Image Network that allows the model to output multiple spans as answers (existing BERT-based QA models usually only output two endpoints of the answer span). The model is evaluated on SQuAD (top-K answer prediction) and an internal Amazon dataset.Unfortunately, it is clear that this paper doesnt meet the standard of the ICLR publications. I cant recommend the acceptance of the paper.First of all, it is mentioned in the paper that To the best of our knowledge, a multi-span QA architecture has not been proposed. This is certainly incorrect. There have been many models proposed recently to tackle the multi-span QA problemm.  Some examples include:- (Hu et al., EMNLP 2019): A Multi-type Multi-span Network for Reading Comprehension that Requires Discrete Reasoning- (Segal et al, EMNLP 2020): A Simple and Effective Model for Answering Multi-span Questions- (Andor et al., EMNLP 2019): Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension => This one is not straightforward but they support merging single spans at the endIt is very natural to cast a multi-span QA problem as a sequence tagging task and this paper doesnt have such a baseline to compare with.Second, I think the model is not motivated well and the writing of the paper needs to be improved. Why is it called a span-image network? Is it because there is a CNN layer applied? What is the rationale behind that? It is really not an image of pixels. It is just an N by N matrix, and each entry is an elementwise-multiplication of the BERT hidden vectors.Third, the evaluation of the paper also should be improved. SQuAD is a single-span QA dataset and using top-K prediction looks quite artificial and unnatural. Also, the performance on SQuAD 2 is also lower than expected. For a BERT-base-uncased model, it is expected to achieve at least ~75 F1 on SQuAD 2 so even the baselines dont seem to be right. The internal Amazon dataset lacks details so I cant comment much on that.  There are indeed several multi-span QA datasets (e.g., DROP, Quoref, Natural Questions) and I think the paper should experiment with those datasets and compare to previous approaches.  Summary: The authors propose two approaches for pruning: (a) "Evolution-style": start with K random masks associated with the weights, update weights on gradient descent corresponding to those active in the fittest mask, and overtime throw away all but one masks which are less fit. (b) "Dissipating-gradients: Here those weights are removed which are not being updated as much, measured by their sum of gradients over a number of iterations. This is shown for elementary networks on MNIST datasets without any serious experiments or comparisons or even presentation. Pros:-It is quite possible that these nuggets of ideas proposed in this paper could be useful, in some shape. But right now their form is *as rough as a draft can possibly get*. -Another thing which is mentioned rightfully in their paper is the Limitations section, of which are plenty.Cons:Empirical analysis-No serious experiments or comparisons to any baseline apart from random pruning. Even so, all networks used comprise of a few layers on MNIST & Fashion MNIST. -The performance of their proposed methods is basically on par with random pruning, sometimes even worse (Figure 2)!-Why do you sum over gradients? Doesnt it make more sense to sum the absolute value of gradients? (There could be a weight which is changing a lot, but at the time when you decided to prune, its sum of updates was very small.)-What happens in Figure 3a and 3b if you use all the samples from MNIST?-Given the extremely rough shape of the paper, I dont think if the other results should even be taken seriously!Presentation:-The authors keep saying post-training dropout, instead of post-training pruning. Dropout is just used as a verb in place of pruning. Its not that authors do not know that Dropout and they cite the corresponding paper. Yet, it is used by the authors as if dropout and pruning are the same things.-The equations are so poorly written, why even bother?-The writing is absolutely dismal, I dont even think I could enumerate over all the issues here. -There are several vague statements: "It is also averse to retraining losses., "which can be only 2 if the image is very monochrome, etc.-There is one paragraph on the connectivity factor, which is trivial. Its basically what fraction of weights are pruned. Funnily, this concept is accompanied with diagrams. Sorry if this whole review sounds rude, but to be honest I am amazed that authors thought about submitting a paper in this state to ICLR. The paper proposes a defense against adversarial examples. The idea of the defense is to counteract transformation generated by PGD attack. This is done by running one step of PGD on network input several times (and using different target labels) and then average the result.As described below, I think there are multiple serious flaws in the evaluation and the defense likely wont work. Thus I recommend rejecting the paper.Issues with the paper:* Experiment section is lacking rigor which is necessary to properly evaluate defense against adversarial examples. Points below explain it in more detail.* One clear indication of a problem with evaluation is the fact that accuracy under attack is increasing (see table 2) when seemingly stronger attack is used. [i.e. PGD with larger number of steps]* The whole premise of the defense idea is to counteract very specific thing which PGD does, thus its unlikely to help against more sophisticated attacks or simply different attacks.* Evaluation procedure implies that the attacker has no knowledge about the defense and even no ability to query the defended model. Which is very strong restrictions on the attacker, moreover they are impractical from security standpoint [even in black box case attacker usually has an ability to query the model]* Authors cite (Carlini & Wagner, 2017) to justify oblivious-box attack setup. However  (Carlini & Wagner, 2017) actually proposes white-box attack and provides justification why white-box should be used.* https://arxiv.org/pdf/1802.00420.pdf (which authors cite) shows how to break defenses based on input transformations. Nevertheless authors do not address why the proposed defense (which is also input transformation) is not broken.* Authors use Guo et al.s (2018) as one of the baselines, despite that this is a broken defense (per https://arxiv.org/pdf/1802.00420.pdf )* No comparison with adversarial training, in particular authors should consider comparing to  A. Madrys paper https://arxiv.org/pdf/1706.06083.pdf which also performs experiments on CIFAR dataset.Feedback on how to improve paper:* The evaluation should be performed in the assumption that adversary is either aware of the attack (white-box case) or able to query the defended model (black box attack).* Use https://arxiv.org/abs/1902.06705 as a guide on how to properly evaluate models for adversarial robustness. And redo evaluation following this guide.* In particular, consider running gradient free attacks on the whole model (baseline model + defense on top of it) and try to make an attack which will break the proposed defense. While the paper is easy to follow, I found all the results in this paper trivial and already-known.Pros:1. The paper is well-written and easy to follow.Cons:1. The entire Section 3 is not novel. It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1. Moreover, Theorem 1 is not rigorous. What if walk-based proximity is just a constant function? What if a given graph does not contain any automorphism?2. Preserve walk-based similarity is not rigorously defined. It seems that it just means all nodes have different embeddings.3. The proposed model is not permutation-equivariant after adding Gaussian noise. It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored (because the model just reduces to an ordinary GNN).4. The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other.5. The idea of using node identifiers (essentially equivalent to the Gaussian noise) to make GNNs position-aware is not new. In fact, this idea is clearly mentioned in the P-GNN paper already (Section 6.2 of [1] for inductive tasks, augmenting node attributes with one-hot identifiers restricts a models generalization ability).6. In Section 5.3, it is unclear why the OGB node classification datasets are not used.[1] https://arxiv.org/abs/1906.04817 It's a pity but authors submitted their paper to arXiv, breaking the anonymity:https://arxiv.org/abs/2010.05295 This work proposes a defense that combines prior work on learning features that are compact for samples from the same but dispersed for samples from different classes (MMD by Pang et al.) with (a) a method to find better class centers, (b) a gradient-norm regularization and (c) an adversarial training regularization.Unfortunately, the reported results on the robustness of the defense are clearly wrong. For one, the core part of this defense by Pang et al. was broken by [1] which is not mentioned here. More importantly, the adversarial attacks employed here are not suited for finding minimal adversarial perturbations against the proposed defense. This can be seen most clearly in Figure 2 (or Table 10 in the appendix): If we allow a perturbation with L-infinity norm of 0.5 on MNIST, then we can always find an adversarial perturbation simply by setting the whole image to a flat gray value of 0.5. In turn, any effective adversarial attack should drive network performance down to at least random baseline performance (10%) for epsilon = 0.5. Instead, the paper reports > 99% accuracy for this value under a PGD attack, which means that PGD is totally ineffective against the given defense and a very different adaptive attack would be needed to accurately measure its robustness. Similarly, in Table 3 the attack success of targeted attacks is often higher than for untargeted attacks, again a clear sign for ineffective attacks. The work also uses an adaptive attack which works better for some versions of MAT but performs similar to PGD in other cases. Hence, the adaptive attack employed here are not good enought.The reason why the proposed attacks fail against the defense are probably simple: for one, the attacks optimise a different classificatioon loss then what is actually used by the model. Second, both auxiliary losses may give rise to gradient masking, the most common issue for gradient-based attacks to fail against a defense. I highly suggest the authors study [1] to get familiar with how to engineer strong adaptive attacks.[1] On Adaptive Attacks to Adversarial Example Defenses, Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry, NeurIPS 2020, https://arxiv.org/abs/2002.08347 In Appendix I.1 it says "We are machine learning scientists from the University of Munich...", which violates the rule of anonymity. Please let me know if I should continue on reviewing this paper. Thanks! The authors try to claim that Lipschitz continuity of the discriminator is a fundamental solution of GANs, and that current methods do not satisfy this approach in principle.There are several false statements in this paper. In particular, sections 2.3 and 4.4 are wrong (and most of the paper is based on statements made there). The necessary constraint for the Wasserstein distance is NOT f(x) - f(y) &lt;= d(x, y) for all x ~ Pr, y ~ Pg. It has to actually be 1-Lipschitz in the entire space. See Chapters 5 and 6 of [1], for example remark 6.4 or particular cases 5.16 and 5.4. Indeed, this is how it is written in all of the literature this reviewer is aware off, and it's a fact well used in the literature. Indeed, all the smoothness results for optimal transport in [1] heavily exploit the fact that the gradient of the critic is in the direction of the optimal transport map, which wouldn't be the case in the situation the authors try to claim of 'f not being defined outside of the support of Pr or Pg'.Furthermore, the relationship between Lipschitz continuity and having a gradient is elaborated in [2] https://arxiv.org/abs/1701.07875, for example figure 2 clearly show this. Furthermore, and contrary to what section 4.5 tries to claim, the idea that most conclusions of wgan hold *without* the Wasserstein distance, but with Lipschitz continuity are already elaborated in the wgan paper. See in fact, appendix G.1 [3], where this is described in detail.[1]: http://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf[2]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf[3]: http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf The author study the expressive power of neighborhood aggregation mechanisms used in Graph Neural Networks and relates them to the 1-dimensional Weisfeiler-Lehman heuristic (1-WL) for graph isomorphism testing. The authors show that GCNs with injections acting on the neighborhood features can distinguish the same graphs that can be distinguished by 1-WL. Moreover, they propose a simple GNN layer, namely GIN, that satisfies this property. Moreover, less powerful GNN layers are studied, such as GCN or GraphSage. Their advantages and disadvantages are discussed and it is shown which graph structures they can distinguish. Finally, the paper shows that the GIN layer beats SOTA GNN layers on well-known benchmark datasets from the graph kernel literature.Studying the expressive power of neighborhood aggregation mechanisms is an important contribution to the further development of GCNs. The paper is well-written and easy to follow. The experimental results are well explained and the evaluation is convincing.However, I have some concerns regarding the main result in Theorem 3. A consequence of the theorem is that it makes no differences (w.r.t. expressive power) whether one distinguishes the features of the node itself from those of its neighbors. This is remarkable and counterintuitive, but not discussed in the article. However, it is discussed in the proof of Theorem 3 (Appendix) which suggests that the number of iterations must be increased for some graphs in order to obtain the same expressive power. Unfortunately, at this point, the proof is a bit vague. I would like to see a discussion of this differences in the article. This should be clarified in a revised version. Moreover, the novelty of the results compared to the related work, e.g., mentioned in the comments, should be pointed out.Some further questions and remarks:(Q1) Did you use a validation set for evaluation? If not, what kind of stopping criteria did was use?(Q2) You use the universal approximation theorem to prove Theorem 3. Could you please say something about the needed width of the networks?(R1) Could you please provide standard deviations for all experiments. I suspect that the accuracies on the these small datasets fluctuates quite a bit.(R2) In the comments it was already mentioned, that some important related work, e.g., [1], [2], are not mentioned. You should address how your work is different from theirs.Minor remarks:- The colors in Figure 1 are difficult to distinguish[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190[2] https://people.csail.mit.edu/taolei/papers/icml17.pdf The paper focused on the issue of learning a policy for a given task using the learned representations a pre-trained VAE. The authors visualize that using a learned latent space of a pre-trained VAE is not good enough for learning policies and propose a solution for this problem: back-propagate gradient policies through the VAE encoder. The authors proposed two versions on this method, one with pre-training and one fully online.This paper suffers from fundamental issues. The biggest one in my opinion is its poor literature review. There is a bold statement in the introduction which shapes up the problem that the paper is trying to address and I quote: "Recently, many such approaches employed the VAE framework which aims to learn a smooth representation of its domain. Most of these approaches follow the same pattern: First, they build a dataset of states from the RL environment. Second, they train the VAE on this static dataset and lastly train the RL mode using the VAEs representation." But the authors do not provide any reference for this. The related work section is also more focused on VAEs rather than its combination with RL. And as one would expect, there is no comparison with previous work either. Joint training of a VAE with a reward predictor or a policy network is not new. There is a massive body of research doing this. PlaNet, Dreamer by Danijar Hafner et al and their variations being the latest ones. The experiments are also done on a single Atari game (Breakout) which is not sufficient to demonstrate the capability of the proposed method in a variety of tasks. And again, with no comparison to ANY of the previous work.Overall, the paper is suffering from multiple fundamental issues which makes it hard to be accepted as a scientific contribution in a top conference.  ##########################################################################Summary:This paper proposes a new method and two algorithms for solving kernel k-means. The contribution is that the algorithms converge to the optimal solution. The downsides are 1) the method is not better than a simple baseline (i.e., random features + distributed power method) and 2) the main theorem (Theorem 3) is wrong.##########################################################################Reasons for score:I vote for rejection for two reasons. First, the proposed method appears not useful. The same problem can be solved in a much simpler and faster way. Second, the main theorem, Theorem 3, is wrong.##########################################################################Pros:+ This paper develops a new method of distributed kernel k-means. The method is new, although I do not find it very useful.+ This paper proves that two algorithms can correctly solve the trace-norm regularized problem in Section 4.1.##########################################################################Cons:1. First and foremost, I do not see a good reason for using the proposed algorithm. The goal of the algorithm is to find the top singular vectors of the random features, $A$.    - The solution to the trace norm regularized problem, $Z^*$, has the same singular vectors as K. By finding $Z^*$, you can find the eigenvectors of $K$.    - However, the same goal could be achieved in an easier and less expensive way, i.e., random features + distributed power method. Random features are naturally distributed among the clients. Their truncated SVD could be found by the distributed power method or Krylov subspace methods. Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD.2. I am very surprised that Theorem 3 does not reply on $D$ (the number of random features). So I checked some of the proofs. I found Theorem 3, which is the main theorem, is wrong.    - $Z^*$ has the same top eigenvectors as $\xi$. But $Z^*$ may not have the same as $K$.    - The proof of Theorem 3 relies on that $Z^*$ has the same eigenvectors as $K$. This is wrong.3. The description of the algorithm is difficult to follow. Id suggest splitting algorithm description into 3 paragraphs: 1) Client-side computation, 2) server-side computation, and 3) communications.Typos:17th page: "The following two lemmas will be used in the proof of Theorem 4. Do you mean Theorem 3?##########################################################################