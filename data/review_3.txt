 This paper proposes to explore whether meta-learning approaches can exploit a compositional structure in their tasks to generalize, and compares this ability to humans. To do so, the paper introduces a grid dataset consisting of generative grammars for generating compositional grids, as well as a null task distribution which is non-compositional but matches on certain low-order statistics. These tasks are interesting and new. They have humans and agents perform a task to reveal rewarding squares on the grid, and compare to agents that meta-learn this task. Human subjects perform better at the compositional distribution, whereas models perform better at the null distribution. They conclude that "compositional structure remains difficult for these systems and that they prefer other statistical features" and that this "highlights the importance of endowing artificial systems with this bias." While the topic is timely, and the tasks are interesting, there are a number of limitations to the model and training which I think seriously limit the conclusions. I think that the task is really only compositional for a model which is able to fixate on different locations on the grid. Thus, I recommend rejecting for now, although I think a revision with a more sophisticated model and more thorough discussion could be a valuable contribution.Strengths:* Interactive tasks for humans and models are a great improvement over prior toy datasets on compositionality, e.g. SCAN.  * I think the tasks are very interesting, and offer directions that aren't really address by prior compositional generalization datasets that mostly rely on composition of words, or on composition of visual properties like color and shape.* It's great to see actual comparisons to human performance, and careful thinking about how to evaluate compositional generalization vs. other types (though this could be developed further, see below).Areas for improvement: * Is generalization equally good for humans under each rule type? If not, this might affect the conclusions. For example, perhaps humans would be very good at inferring chains, but not the more complex structures (like trees). If so, it could be that "compositionality" per se is not the construct underlying their performance, but rather "chains" or some other, simpler construct. Because performance is not presented broken down by structure type, it is difficult to determine whether some pattern like this could explain the results. Thus, it is difficult to conclude that compositionality is the factor underlying the results. * There are a number of features of the agent that confound the comparison with the humans. These limit the ability to draw a strong conclusion about e.g. "the importance of endowing artificial systems with [a compositional] bias."     * Why are the main comparisons run using the non-convolutional model, when the convolutional one is clearly more closely matched to humans (as the discussion acknowledges)? It seems like most (although not all) the difference in performance is due to spatial bias, rather than compositionality.     * Indeed, this spatial bias is partially addressed on the input (by the convolutional experiments), but it is *never* addressed on the output. Humans know that there is a spatial structure to the tiles they are clicking on, the agent can access this information only implicitly.    * To address this, one could build a recurrent attention model (e.g. Mnih et al, 2014; Gregor et al, 2015) which can make visual saccades around the grid before deciding whether to reveal the square at the current point of fixation, or whether to fixate to another location. This would likely match the human process better, since the humans are likely fixating their gaze on the locations they are considering, rather than fixating in the center of the grid without moving their eyes. It's also motivated by the observation that agents generalize better if they receive ego-centric input rather than visual input fixed on the grid (c.f. Hill et al, 2020; Ye et al, 2020). This is an important issue to the claims at stake. For an agent which could fixate on each location it was considering, the compositional rules would be much more consistent than for an agent that perceives the whole grid from a fixed perspective. For a fixating agent, the compositional rules would also be much more consistent than the null distribution. In fact, I would suggest that it is *only from the perspective of a model which can fixate that this distribution can be considered compositional at all.* How can we tell that the difference between the humans and the model isn't due to the human ability to fixate, rather than some abstract bias toward compositionality?    * The paper may not be able to address all the ways in which the model's experience of the task is unlike humans, but then the there should be a *corresponding tempering of the conclusion that the comparison to humans says something specific about the difference between the model and humans.* That is, given the current experiments, the discussion of this paper should focus at least as much on the limitations of the present model as on general conclusions about failures of the model class and the need for additional inductive biases.* Furthermore, exploring compositionality in toy tasks can be misleading. Hill et al. (2020) show that compositional generalization is significantly improved in more realistic settings (for example an RL agent that executes actions over time achieves 100% compositional generalization on a task that a feed-forward classifier only achieves 80% generalization on). They argue that toy stimuli remove one of the most important elements for training deep models  the rich environments in which humans, also, are trained. Even if the input and output of the model and humans were better matched on this dataset, it may be misleading to conclude something as general as "the importance of endowing artificial systems with this [compositional] bias" without giving these models training on a distribution of stimuli and tasks that more closely match the rich variety which humans experience over development. Of course, it is not feasible in practice to do so (yet). But this limitation and its relevance to the conclusions should at least be acknowledged in the discussion. * The paper could use some more discussion of the distinction between statistical patterns and compositional rules. This seems like an important point, but it wasn't entirely clear to me. Compositional rules correspond to a certain statistical distribution over grids. The fact that up to 2nd order Ising statistics are matched does not mean that the distributions of outcomes are matched "statistically," it merely means they match in certain low-order statistics. It's not clear if these are the right statistics by which to compare the distributions (especially for the non-convolutional model, which has no spatial awareness). The paper would be strengthened by justifying the choice of these statistics more carefully, and articulating a clear distinction between what counts as a "statistical" pattern vs. a rule. References ---------Gregor, Karol, et al. "Draw: A recurrent neural network for image generation." arXiv preprint arXiv:1502.04623 (2015).Hill, Felix, et al. "Environmental drivers of systematicity and generalization in a situated agent." International Conference on Learning Representations, 2020.Mnih, Volodymyr, Nicolas Heess, and Alex Graves. "Recurrent models of visual attention." Advances in neural information processing systems. 2014.Ye, Chang, et al. "Rotation, Translation, and Cropping for Zero-Shot Generalization." arXiv preprint arXiv:2001.09908 (2020). This paper proposes to represent system logs at five levels of abstraction (including log sequence, parsed log, field embedding, log embedding, and sequence embedding). The representation at each level can be computed from the previous level. Transformer Network is utilized for time encoding. The paper also describes various log-related applications based on the proposed log representation. Some experiments were conducted to evaluate the proposed approach. Logs are useful for understanding and diagnosing software intensive systems. It is good to see that this paper proposes a new neural representation of log data. The authors also suggested various applications of the proposed log representation.In section 4, the authors only described the proposed time encoding technique, while other parts of the log representation (such as encoding a log entry) were not described. Also, it is not clear if the proposed time encoding technique is better than the related methods (there are many related methods for encoding/representing time). The evaluation of the proposed approach is very weak. In section 5, the authors mentioned the Radio datasets. However, the use of Radio dataset is not described. The authors only evaluated the anomaly detection model on the HDFS log dataset, which is not enough. Also, the obtained results on HDFS were not very different from the results of the related work (DeepLog). Furthermore, no experiments were conducted for the causal analysis task. Therefore, the effectiveness and generalizability of the proposed approach are not clear. The paper only compared with a few related methods for log-related tasks. Actually, this area has been widely studied and there are a lot more research work (some also utilized deep learning and language models). The authors could discuss and compare with them. Just a few examples:Zhang et al., Robust log-based anomaly detection on unstable log data. In Proc. ESEC/FSE 2019, 807-817.Zhu et al., Learning to log: Helping developers make informed logging decisions, in Proc. ICSE 2015. pp. 415425.P. He et al., Characterizing the natural language descriptions in software logging statements,  in Proc. ASE 2018, pp. 178189. Summary--------------The paper proposes a trainable way to re-order or recover the ordering of features from sets of examples, and use it as a way to build a common feature space (or embedding) for a neural net, the (initial) parameters of which can be trained by Reptile.Experiments show that such initial parameters enable faster training (inside of an episode) than untrained weights.Pros------- The paper shows it is possible to recover information about the identity of coordinates in the input space, through a learned transformation, on several unstructured datasets. The similarity between such representations of individual coordinates can help identify similar features, either in a given dataset or across datasets.Cons--------The paper is overall really hard to follow, statements are often confusing or misleading. For instance:- The introduction suggests a multi-modal learning paradigm, where different tasks could have access to data in different input spaces, some of them common. However, the paper then seems to consider individual coordinates in the input space only, and focuses on mapping shuffled subsets of these coordinates back to their initial position.- There is confusion about the "tasks", which sometimes correspond to one of the OpenML datasets, and sometimes to individual few-shot episodes from one of these datasets.- Concepts like "schema" and "predictors" are never properly introduced or defined.- The description of the "chameleon" (alignment) component mentions "order-invariant" and "permutation invariant" several times, but it is quite unclear whether it refers to the the order of the examples within the data set (or episode) or the order in which the features are represented.The paper uses few-shot learning vocabulary and techniques, including Reptile, but the methodology seems completely different from the few-shot learning literature. In particular:- There does not appear to be a split between meta-training and meta-test classes within a dataset, or meta-training datasets and meta-testing ones, except for the EMNIST experiment. Even then, the pre-training of the "chameleon" alignment module seems to involve using examples of the meta-test classes.- The reported evaluation metric is really unusual: they report the improvement (and sometimes accuracy) after 3 steps of gradient descent from within an episode, which is somewhat related to the quality of the meta-learned weights, but no other metric that would be comparable to existing literature, which makes it especially hard to assess the results.The principle of the alignment module seems similar to (soft) attention mechanisms, in that there is a softmax trained to highlight which parts of an input vector should be emphasized (or selected) at a given point in the processing (here, in the aligned feature space). However, the literature on attention is not reviewed. Many design choices are not addressed clearly, neither in how they were made, or the impact of these choices, especially regarding the architecture of the alignment module:- It is a linear transformation (before the softmax), though parameterized by 3 matrices. An alternative would have been a 3-layer neural net, similar to attention networks.- The parameterization of the first matrix makes the number of parameters depend on N, the number of examples in a given task. This could be quite limiting to be restrained to tasks of exactly N examples, especially if both the support (mini-train) and query (mini-test or valid) parts of an episode need to have exactly N examples.- There is also no discussion of the  value or impact of or K, the size of the chosen embedding space).Recommendation--------------------------I recommend to reject this submission.Arguments------------------The main idea in the paper, learning alignments of various input spaces into a common embedding space through an attention mechanism, has merit and may  work reasonably.However, both the algorithm and the experimental set up are described in a quite confused way, and not well justified or grounded. The reported results are not comparable with few-shot learning literature, nor multi-modal training or feature imputation, and do not make a convincing case. Questions---------------As I understand it, the "Chameleon" architecture itself simply consists in 3 matrix multiplications (Nx8, 8x16, 16xK), which would be equivalent to the length-1 1D convolutions, is that correct? It may be more straightforward to explain that way, as $enc(X) = X M_1 M_2 M_3 X^T$.Also, should the 2nd and 3rd convolutions be labeled "8x16x1" and "16xKx1" respectively? As far as I can tell, only the first Conv1D should have a dependency on N.Additional feedback---------------------------In Figure 2, the "reshape" operation should be "transpose" instead. Summary:This paper aims to perform meta-learning across tasks that have different input data types by learning separate task-specific encoders, and then aligning the features produced by these encoders before making predictions.Pros:Sharing information across tasks with different input types is a relevant problemCons:Precise problem statement and method very unclearExperiments are only on toy datasetsDetailed Comments:It is not clear from the abstract / introduction what is meant by schema. From the abstract: for example, if the number of predictors varies across tasks, while they still share some variables. Does this refer to the number of classes in a few-shot problem? What variables are shared? Classes, or input features? Later in the intro: training a single model across different tasks is only feasible if all tasks share the same schema, meaning that all instances share one set of features in identical order. These definitions of schema do not seem to be the same. Schema also does not seem to be defined in Section 3. At the beginning of that section it says, every task has to share the same schema of common size K which seems to indicate schema is the number of features and then a few lines later,  tasks with varying input schema and feature length F which seems to indicate schema is *not* the number of features.In the related work section, few-shot learning did not begin in 2017 as might be suggested by the citations. It would be good to recognize the earlier works in this area, such as Fei-Fei, L. et al. A bayesian approach to unsupervised one-shot learning of object categories. 2003A Bayesian framework for concept learning. PhD thesis, Massachusetts Institute of Technology, 1999.For few-shot learning with deep learning, Matching Networks should arguably be cited: Vinyals, Oriol, et al. Matching networks for one shot learning. 2016.The original MAML paper actually proposed the first-order version of MAML, Nichol et al. was not the first to propose this.I dont understand how the method works when the features are learned and not given. For example, the encoder for EMNIST-Digits produces 32 features, while the encoder for EMNIST-Letters produces 64 features. If the meta-training tasks are drawn from only EMNIST-Digits, then how can the re-ordering matrix be learned from EMNIST-Digits such that it can re-order features from EMNIST-Letters? At the most basic level, based on Figure 2, the matrix \Pi would have to have different dimensionality for each dataset. Even if they were the same dimensionality, how is the feature ordering supervision performed in this case?In the main results, if you sub-sample features, how do you know that the sub-sampled features have enough information to perform the classification task? It would be helpful to have an experiment on a less-toy dataset, both to demonstrate that the problem of mis-aligned features exists in more complex data, and that the method can address it. Overall, this paper is extremely confusing. I do not understand the problem statement or how the method is trained in the learned feature case. In my view, the clarity of this paper needs to be significantly improved to consider acceptance.  This paper is based on an interesting observation that previous data augmentation tricks cut&mix may select regions that do not contain useful information. Instead, this paper use saliency models to detect the salient regions first and then cut and mix these salient regions in a source image.This observation is interesting and worth trying. However, the contribution might be too limited for a ICLR conference paper:1. as cut&mix cuts bigger blocks in an image, the target object is more likely to be selected. Also for most image classification dataset, the images are quite iconic, so the improvement on classification tasks are limited (as shown in Tab. 1, C10+ and C100+). This might help with detection as it may train models to focus on the most discriminative part of the image, but recent works show that there is no direct correlation between between the performance of the same backbone on detection tasks and classification tasks. In addition, the author didn't provide analysis on what causes the 1.8% improvement on detection tasks (better on smaller objects?) So it's not clear how helpful this trick is.2. While the method is simple, I expect either some mathematic proof or this method works well on various tasks. The paper didn't have any proof or statistical analysis. This paper didn't either show if the proposed method will work on more tasks (for example segmentation or GAN? the detection provided in this paper is using the backbone initialized from classification, during training faster-rcnn it seems that the trick is not used). Strength:The paper notices the problem in pseudo-labeling methods of semi-supervised learning, which is the erroneous prediction of pseudo-labeling. Pseudo-labeling is an easy-to-implement method for semi-supervised learning does not require constraints needed by consistency regularization methods, so improving pseudo-labeling methods can promote the practical use of semi-supervised learning.The paper is well-written and easy to follow. The experimental results on datasets of different domains such as image, video show that the proposed method outperforms previous semi-supervised learning methods.Constraint:The paper proposes a new method to predict pseudo-labels of unlabeled data by confident threshold. However, confidence threshold is a widely-used method to decide pseudo-labels. And the threshold of confidence is hard to decide since different backbone network and different datasets have different confidence levels.Furthermore, using both positive and negative labels for classification is also used in (Kim et al., 2019). The authors fails to discuss the difference of the usage of positive and negative labels between their method and (Kim et al., 2019). The authors also need to discuss what is the special challenge of using positive and negative labels in pseudo-labeling for semi-supervised learning.The paper argues that the calibration error is greatly reduced with more certain predictions. However, the paper only empirically shows the relation but fails to demonstrate the claims or give intuition on the relation. Also, confidence itself is also an uncertainty measure, but the authors do not use the confidence for uncertainty but use a new uncertainty measure. Could the authors explain what uncertainty measurement do they use and why using the new measurement instead of the confidence?The paper uses a fixed set of threshold hyper-parameters for all the experiments. However, the confidence-level for different datasets should be different. For example, the confidence for a real difficult dataset should be much lower than an easy dataset. The authors need to show that why using a set of hyper-parameters is enough and how to select the hyper-parameters. For example, showing the relation between the accuracy of pseudo-labels and the confidence. Authors introduce a new meta-RL algorithm based on SAC. It uses a context variable $c$ that they condition the Q-function on and the adaptation mechanism which is based on the values of the value function (ie. $\mathbb{E_a} Q(\dot, a)$) instead of the true returns. Authors claim their method reduces variance and bias of the meta-gradient estimation, is closer to human learning, encourages the agent to learn to explore, is more data-efficient in test-time and has competitive performance among gradient-based algorithms.I found many of the claims of the paper are unjustified:1. Authors claim "we reformulate and propose the K-shot meta-RL problem to simulate the real world environment". The formulation that follows is the standard one (cf. MAML (Finn et al. 2017): "In K-shot reinforcement learning, K rollouts from $f_\theta$ and task $T_i$ (...) may be used for adaptation on a new task $T_i$". It is unclear how does the authors' definition relate to the real world.2. They suggest their method is able to "learn where to explore", but nothing in the method specifically addresses this part (compared to eg. Learning to reinforcement learn (Wang et al. 2016), which also has a learned context that influences the policy).3. They suggest their method learns more "human-like", which is understood as "not using batched sampling" (as, arguably, humans learn more sequentially). However, their method also uses batched sampling (see eq. (17)). In another part of the paper human-like learning is associated with the usage of LSTM, which is also neither novel (see Wang et al. (2016) again) nor grounded.4. They claim their method decreases bias and variance. Gradient estimation in a typical meta-RL method is unbiased, so that's hard to decrease. On the other hand, I believe the variance of the proposed method is increased compared to E-MAML and others due to approximating rewards with V, which, due to the fixed-capacity of the model, will make their adaptation procedure inherently biased and thus estimating gradients in incorrect places.5. Their method "makes the meta-policy more interpretable", yet no interpretation attempt was presented.6. "Agent can learn how to explore environment and how to utilize the transition data, which is a more structured learning scheme". The learning scheme is basically the same as in PEARL (Rakelly et al. 2019). It's not clear what exactly makes the learning scheme "more structured" nor what it means.7.  "Each trail we sampled 200 steps for total 2 trails, the data used to do adaptation in our algorithm is 10% of in PEARL and 5% in ProMP.". According to Rothfuss et al. (2018), app. D.2, they used a single trajectory of 200 steps for adaptation for HalfCheetahFwdBack, which is half of what the authors' method used. On the other hand, PEARL achieves much better results than the authors' method, what is not mentioned in the paper.Details of the method are not very clear, I assume they follow SAC with online/target networks, but am not really sure nor what $\mu$, $\phi$ and $\eta$ mean. Mechanics of meta-testing also weren't described in enough detail.The writing of the paper is terrible, to the point it's not always clear what the authors mean. Most of the problems are simple grammar errors, which are easily solvable by a tool like grammarly or google docs correction. As such, I consider sending the paper to the review in its current state a disrespect for the reviewers who have to spend the time to decipher the writeup.Grammar problems (first page only):1. However, in many real world tasks2. Derive a new policy **which** maximizes3. The experiment results suggest that4. Utilize sampled data **to** some extent.5. **While** obtaining environment data6. Human gradually understand**s** where to sample data7. Stochastic trajectories to do p**o**licy adaptation8. "This causes agent (...) and only learns how to utilize data" - not clear what was meant9. That have high cost o**f** obtaining data Summary----------This paper presents an approach to meta-RL based on combining gradient-based updating with recurrence-based meta-learning. The approach is based on combining gradient-based policy updating with recurrence-based updating of the value function. The authors evaluate the method of simple standard meta-RL benchmarks. Comments----------Overall, the direction of the paper is interesting but the paper has numerous shortcomings. First, the proposed method is a fairly straightforward combination of existing techniques. In particular, the approach consists of a fairly simple combination of gradient-based and recurrence-based methods. While this is not necessarily a limitation, it necessitates a thorough set of experiments to justify the combination of approaches. This is the second limitation of the paper: the experimental evaluation is very limited. The authors test of very simple benchmark problems compared to recent work in meta-RL. Moreover, there are few comparisons to baseline methods (especially PEARL) and the authors should include ablation experiments in which they examine the performance of their method relative to strictly gradient-based and strictly recurrence-based methods, using the SAC algorithm as an underlying algorithm. Finally, the writing of the paper is extremely sloppy. Trials is misspelled as trails throughout the paper. There are also numerous typos, such as "gardient", "meta-reinforce", "Substitue", "apdated", etc. The presentation of the algorithm is quite unclear and the discussion of related work is quite limited. Overall, the paper should be tested on a wider range of environments and against more competitive baselines to warrant acceptance. The authors should also improve the presentation of the paper to improve clarity.  In this paper, the authors proposed a new method to train DNN (partially) with fixed point weights and approximated natural gradients. The contribution of this paper comes in three folds. First, they propose to use an exponential map to encode the curvature information instead of using the inverse of Fisher information matrix. Secondly they propose to use this technique to compute an approximation of the natural gradient. Lastly, the authors proposed to train low precision DNN using a combination of this approximated natural gradient and an additional loss term which enforces consistency across a full precision and the low precision model weights. Empirically, the authors demonstrate that the proposed method can attain better validation accuracy than plain SGD when training fixed point low precision convolutional models.Strong points: empirically the proposed partially low precision training method in general attains meaningfully higher validation accuracy than plain SGD and other baseline methods (the plain SGD and baseline methods use a pure low precision fixed point model representation).Concerns: I have 3 major technical concerns and questions which lead to my initial rating to reject.1. In the high level, is the proposed method targeting at low precision training with less training compute resource, or is it just aiming at to attain a low precision model that can be efficiently deployed? This is not clearly elaborated in the paper.      If it is for training with low compute resource, the proposed training algorithm will involve forward and backward evaluation of a full precision model. The additional compute from the low precision model weights can only increase the training overhead compared to a pure full precision training algorithm. If the proposed method is just for attaining a low precision model for efficient inference, then the experiment protocol is not convincing to me. Specifically, the authors does not compare to post-training quantization after full precision training methods. Instead, the major baselines in the paper is training methods with low precision compute during training directly.2. The definition of straight through estimator in this paper is not correct for general low precision fixed point training. The definition in the paper is only for 1 bit (binary) model weights; however in the experiment the authors considers 4 bit fixed point number. The straight through estimator for non binary model weights will involve indicator functions depending on where the nearest quantization step is.3. In the ImageNet experiment, the authors use an already well trained full precision model to jump start the proposed partially low precision training algorithm. Is the full precision validation accuracy numbers directly fro them model weights which jump-starts the proposed method? If yes, then the fairness of the comparison needs to be enhanced. This is because the proposed methods is using additional training steps to build on top of the well trained full precision model weights.Comments for improving the paper:1. The vector notations in equation (8) needs to be clarified in the paper.2. Citation formatting needs to switch to \citep instead of \cite in the second paragraph in intro section.4. In equation (14) and (15), it might need to explicitly define which weights is updated in each training phase. ## Summary The paper proposes a neural network quantization approach with a tanh-based quantization function and claims that it is a good approximation to the natural gradient method. In addition, they augment the cross-entorpy loss with a knowledge distillation type loss (between the logits of the real network and quantized ones while training) to improve the performance. The results seem to show marginal improvements over compared methods in cifar and imagenet datasets.## Strengths1. The idea of using a knowledge distillation type loss between the logits of the real network and quantized ones while training is interesting and could be one of the reasons for performance improvement.2. Overall the paper is clearly written.3. Reasonable experiments are conducted (even though comparison with some recent methods are missing) and the results show marginal improvements over compared methods.## WeaknessesThe main weaknesses of the manuscript in my opinion are as follows:1. The final method seems to be not based on natural gradient and the claimed connection is not explained: - In fact, the final method simply uses a tanh-based quantization function (Eq. 10) and simply uses SGD with STE approximation. There is no explanation of why this approach closely approximates the natural gradient method, given that this the main claim of the paper and even the title contains "natural gradient". Definition 2 might be coming from a textbook (even though no reference is given) and it simply provides an existence condition and it does not say anything about the proposed exponential map corresponds to the manifold induced by the Fisher Information Matrix. This is critical for the paper and without this connection the whole discussion about natural gradient is redundant and could be deemed as "decorative maths". Please explain (with supporting theory) how Eq. 10 relates to FIM.- Given that the method is not based on natural gradient, there are many unsubstantiated claims. Examples include: 1) "unique limitaions of the QNNs to construct curvature information" 2nd last para in the introduction, 2) "demonstrated feasibility of training low-precision models using natural gradients" first para of Sec. 3.2, and 3) "our method .. avoids the risk of neural networks falling into the local minimum" in conclusion.2. Misinterepretations and inconsistencies in mathematical writing/notations:- Eq. 8 has many issues. First, what is $vec(dL/dw_i)$? If $w_i$ is a scalar, $dL/dw_i$ is a scalar. So what does it mean to vectorize it?- To my understanding $F_{ij} = g_i g_j$ where $g_i = dL/dw_i$. There is no vector representation and no need for Kronecker product etc. Please clarify.- Please clearly mention the dimensions of each variable and define them appropriately for all the equations. Also, do not use same symbol to denote two things. Examples include $Q$ (Eq. 5, 10) and $\alpha$ (Eq. 10, 14).3. No references given for the statements related to Riemannian geometry:- For both definitions 1 and 2 no reference is given. It seems like there are existing results and they should be cited so that someone can check/verify their applicability in the stated problem setting.4. Recent quantization techniques are not compared:- For cifar experiments only comparison is done against the first quantization method BinaryConnect (Courbariaux, 2015) which is very old. Especially, comparison against other relaxed quantization methods [a,b] are required.- Even for imagenet experiments important comparisons are missing such as [c,d].## Minor Comments1. Please provide the equation for FIM before Eq. 4.2. Second para in page 4: what is meant by "closely related"?3. Please provide all the experimental details required for reproduction (eg, any layers kept floating point in imagenet experiment?, what initialization used?, starting from scratch or pretrained network, etc.)4. After fig. 2, please tone down. The figures only show a marginal improvement in test error not "significantly preventing overfitting".## References- [a] Bai, Y., Wang, Y.X. and Liberty, E., 2019. Proxquant: Quantized neural networks via proximal operators. ICLR.- [b] Ajanthan, T., Dokania, P.K., Hartley, R. and Torr, P.H., 2019. Proximal mean-field for neural network quantization. ICCV.- [c] Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B., Huang, J. and Hua, X.S., 2019. Quantization networks. CVPR.- [d] Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W. and Cheng, K.T., 2018. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. ECCV. In this paper, the authors aim at generating the right questions based on the textual answers and corresponding visual regions of interest (ROIs). The core innovation of the proposed method is leveraging the region information to supervise the question generation, which helps to mitigate the ambiguity of the answers. Correspondingly, a simple method is designed to generate the noisy annotations of the ROIs using the pretrained Masked-RCNN and the questions. This core part is mainly divided into two components: 1) aligning the object features with the answer word embeddings using attention mechanism; 2) constructing the object graphs and getting the GCN-refined object features; 3) refining the question embeddings by attending the refined object features and image ones. Experimental results show that the proposed method outperforms the existing approaches significantly. In addition, ablation studies prove the effectiveness of the proposed components. Although the visual regions of interest can help to guide the question generation, the one-to-many mapping issues still exist when generating the visual regions of interest (no additional information is given), which may lead to the same problem in the question generation stage. Nevertheless, the proposed approach is still valuable. Although it helps little in mitigating the one-to-many issues, it helps to learn a better question generation network, as the additional region information can mitigate the issue during the training stage, which will help to avoid learning a generator that prefers to give general questions. Therefore, I think authors should make that clear in the paper that it helps in learning a better question generator instead of generating the specific question because the ROIs are also selected based on the answers only. Although great improvements are achieved, the proposed method is not novel. The main novelty is leveraging the object features in generating questions. However, neither the feature alignment nor the GCN part is new. In addition, the graph construction part does not make sense to me, and no ablation study show that pruning the non-hints objects can help to improve the performance.  In addition, the confusing equations and poor writing of the paper cannot make this paper an unaccepted one:* In Eq. (2), \beta X^{a} seems like a matrix, which cannot be concatenated with a vector. * In Eq. (3), \beta_{j} should be \beta_{i, j}.* In Eq. (3), what I_{j} means? Why the image feature has a subscript?* In Eq. (3), what does the operation * means? The product signs are different in (2) (3) and (4)* In Sec. 3.4, it lacks detailed explanations of how the model will attend the image and graph to get a better representation of the question embedding between the two LSTMs, which is one of the core components of the proposed methods. * Typos like size information should be side information in the top paragraph on Page 2, question tokens should be word tokens in the bottom paragraph on Page 3.  #### Summary- The authors analyze a self-supervised learning framework for downstream (supervised) few-shot classification. The self-supervised stage is a simplified version of MoCo (He et al. 2019) and relies on class-invariant augmentation of unlabeled data to produce samples for a contrastive loss. This produces two encoder networks that are used in the subsequent few-shot learning stage via a distance-based classification scheme similar to that used by Snell et al. (2017), [1], [2], and Chen et al. (2019). - The authors show that the method minimizes an upper bound on an oracle supervised distance-based classification loss. They then further analyze the looseness by decomposing the self-supervised loss into contributions from false-negative and true-negative samples. They relate these quantities to key methodological considerations, such as the level of diversity in the meta-training/base data and the number of negative samples to use during contrastive learning.- The authors assess this method on the Omniglot and miniImageNet few-shot datasets, following the setup proposed by Hsu et al. (2019) in which the meta-training (aka base) split is treated as unlabeled. The results are strong, though are curiously relegated entirely to the Appendix.#### Strengths- The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution as it draws heavily from prior methods. Unlike previous works that consider unsupervised/self-supervised pre-training for few-shot learning, this work provides some theoretical justification for its method. - Due to the judicious choice of considering contrastive learning and distance-based classification, the resulting analysis is relatively straightforward.#### Weaknesses- This submission is overall poorly written. It was very difficult to parse due to a copious number of grammatical errors. In numerous instances, I can't quite discern what the authors mean. Aside from this, there are many vague statements unsupported by reference or argument.- The organization leaves much to be desired. For example, results of an ablation take center stage in the main text, while key experimental exposition and benchmark results are left entirely to the Appendix.- Comparison to CACTUs (Hsu et al., 2019) is not entirely fair as the method (like most modern contrastive learning methods) requires the specification of instance transformations that are class-invariant for test tasks. This should be noted. (Though comparison to UMTRA (Khodadadeh et al., 2019) is fair.)#### Recommendation- I currently recommend rejection (3), as the submission's poor writing severely hampers clarity and thus prevents it from meeting publication standards. If the writing were fixed, I would probably rate it around a 6. #### References- [1] Qi et al., Low-Shot Learning with Imprinted Weights, CVPR 2018- [2] Gidaris et al., Dynamic Few-Shot Visual Learning without Forgetting, CVPR 2018 This paper consider the decentralized adaptive algorithms. At the first glance,  I am really happy to that the adaptive methods are used for the decentralized optimization. However,  after I read the  main document, I do not think  paper actually    analyzes the decentralized adaptive algorithms.In line 9 of Algorithm 1, the   denominator is $\sqrt{\hat{v}_{t,i}}$. However, in Algorithms 2 and 3, it is changed as $\sqrt{u_{t,i}}$. In  the proofs, the authors proved the convergence based on $u_{t,i}\geq \epsilon$. This is actually the DSGD. The proofs can be quite simple.  And the restriction $\alpha=O(\sqrt{\epsilon})$ can be easily removed.This paper does not present any insights for the decentralized  adaptive methods. It only depends on $u_{t,i}\geq \epsilon$. The numerical results show that the proposed method is similar as DSGD. As mentioned before, it is actually DSGD but with slightly modification. This paper studied the decentralized adaptive gradient methods and provided convergence guarantees. Experiment on MNIST is conducted to show the effectiveness of the proposed approach.1. The theoretical result is weak. The linear speedup result is not proved as in (Lian et al. 2017), the benefits of adaptive gradient methods are also not illustrated in the bound in Theorem 2 and Theorem 3.2. The learning rate scheme is not practical and does not hold in practice. As illustrated in Theorem 2 and 3, the learning rate $\alpha$ is set to be less than $\epsilon^{0.5}/16L$. 3. The LHS of Theorem 2 and Theorem 3 are not the standard gradient squared norm but the scaled version. It is unclear what is the bound if the LHS is the standard gradient squared norm as in (Lian et al. 2017). It is important to use the same measure as in the previous literature for fair comparison.3. The experiment is weak. Doing distributed training only on a tiny dataset on MNIST is not sufficient. I would like to see results on larger datasets such as CIFAR and ImageNet. This paper empirically demonstrated the effectiveness of neural networks for learning to predict different mathematical properties of dynamical systems, which achieve high accuracy on synthetic datasets generated by the authors.Pros:+ the paper is clearly written and easy to follow+ the dataset seems to be carefully generated and is large.Cons:- The authors do not clearly state their methodology, including the concrete architecture of the transformer-based model, and the training loss for optimizing the model. As shown in the experiments, the accuracy of the proposed model is much higher than the baseline, but it is not clear what the major reason is, and why.- As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. Besides, the authors do not describe clearly how they generate the dataset and what is the problem distribution. For some distribution (e.g., those with smaller variance), maybe 50 million is large enough and there won't be a generalization issue. As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.Overall this paper does not have enough technical contributions, so I vote for a clear reject. Pros:1. A stage-wise approach to train GANs for video is defined to reduce the computational costs needed to generate long high resolution videos.2. The authors provide some quality results of the proposed approach.Cons:1. The contribution of this paper is very limited. The authors just do some incremental improvement based on current GAN models, and the theoretical analysis for the stage-wise training approach is not enough. 2. The experiments are not convincing. The authors only compared the baseline methods in the experiments. Besides, the proposed training strategy should be applied in different generation models based on GAN to show the effectiveness in different cases. 3. This paper aims to reduce the computation cost of the model training, but do not achieve significant effect, which takes 23 days for model training. Summary:The paper proposes a stage-wise training pipeline for training 128x128 resolution videos of up to 100 frames. It starts by generating low resolution and temporally downsampled videos, and upsample the results in a stage-wise manner. Experimental results on Kinetics-600 and BDD100K demonstrate that the network is effective in generating higher resolution videos.Strengths:The idea is easy to understand. The paper is well written and easy to follow. Quantitative results show that the proposed method is superior than existing methods under some circumstances.Weaknesses:1.The novelty is very low. Stage-wise and progressive training have been proposed for such a long time, they have been used everywhere. The way the authors use them dont really exhibit anything novel to me.2.The resolution of the outputs (128x128) is lower than prior works (e.g. DVD-GAN has 256x256 outputs). Since the paper claims the computation cost is lower, one would expect the model can generate higher resolution and much longer duration videos, but in fact its quite the opposite. To prove the effectiveness, I feel the authors need to show something higher than 256x256, say 512 or 1024 resolution. On the other hand, the hardware requirement is still high (128 GPUs) instead of some normal equipment that everyone can have, so I really dont see any benefit of the model. If the authors can train DVD-GAN using only a handful of GPUs, that might also be a contribution, but its not the case now.3.Output quality is reasonable, but still far from realistic. Recent GAN works have shown amazing quality in synthesized results, and the bar has become much higher than a few years ago. In that aspect, I feel theres still much room for improvement for the result quality.Overall, given the limited novelty, low resolution output and still high hardware requirement, Im inclined to reject the paper. This paper proposes a new type of separable convolution to improve ConvNet efficiency. Based on a few assumptions (receptive field condition, channel condition, group conv condition), it mathematically calculate the optimal configurations for separable convolutions. Experiments are mostly done on CIFAR and ImageNet.======== Strengths1). Comprehensive study on important separable conv building blocks (e.g. Table 1 is quite informative)2). The idea of split the groups in both stages of separable convs is somewhat new and interesting.======== Weaknesses1). My first concern is about the unrealistic assumptions. For example, Eq (5) channel condition requires g1 * g2 = C2, which doesnt make sense to me: there is no intuition, and most existing convs doesnt satisfy this assumption: (1) regular conv g1=g2=1 != C2 doesnt satisfy this; (2) spatial separable conv g1=g2=1 != C2. This assumption is critical to arrive equation (7) and (8), but is unclear where this assumption comes from.  Due to these unrealistic assumptions, the term optimal is also questionable.2). Second, the CIFAR results show the new layers are not much better than others. As shown in Figure 3, the largest gain is <1%, and sometimes the o-ResNet (~88%) is slightly worse than d-ResNet (which indicates the propose layers might be not "optimal"?) The improvements on ImageNet in Table 4 seem to be promising, but as discussed in DARTS+ and other recent works, the search process of DARTS is often unstable and could potentially have high variance.3). My another main concern is about the weak baseline. As this paper is study separable convs, it should compare to separable conv based models like MobileNet/FBNet/EfficientNet, rather than the full conv based ResNet. For example, by leveraging depthwise and seprable convs, MobileNetV3 achieves 75.2% ImageNet top-1 accuracy with 219 FLOPs, which is a much stronger baseline than the on in Table4. I highly recommend the authors to conduct their experiments on these baselines.======== Suggestions1). Instead of formulating it as a mathematically optimal solution based on unrealistic assumptions, I recommend the authors to conduct more empirical studies on these design choices. For example, the paper only shows the performance results of optimal (g1, g2) computed by equation (7), but it would be helpful to show the performance for different (g1, g2) values, and compare them with the optimal (g1, g2).2). I recommend the authors to use the latest MobileNet or EfficientNet (or other separable conv based models) as baseline, and replace their separable convs with the proposed optimal separable convs, and compare the performance gains.  The paper introduces a first-order algorithm for nonconvex-nonconcave min-max optimization problems. The proposed algorithm terminates in time polynomial in the dimension and smoothness parameters of the loss function. The points (x*,y*) returned by the algorithm satisfy the following guarantee: if the min-player proposes a stochastic gradient descent update to x^*, and the max-player is allowed to respond by updating y^* using any path that increases the loss at a rate of at least \epsilon with high probability, the final loss cannot decrease by more than \epsilon. Then the algorithm is tested in GANs settings on mixtures of Gaussians, MNIST and CIFAR-10 datasets against compared against gradient/ADAM descent ascent and Unrolled GANs. The algorithm is shown to be significantly more stable than GDA (e.g. less mode collapse, cycling, and more stable digit generation).  The paper works on the hard and ambitious problem of general non-convex non-concave optimization which has multiple AI applications such as GANs. On the proposed approach is novel considering a new solution concept and the paper provides some theoretical and experimental results. On the negative side neither the theoretical nor the experimental results seem particularly strong. My main issue on the theoretical side has to do with the solution concept itself. The solution concept seems unnatural to me. Definition 2.1 about \El_\eps(x,y) a critical notion about the ``path" that the max-agent is allowed to use is non-constructive and obtuse. The only closely related solution concept seems to be in a recent unpublished manuscript by Mangoubi and Vishnoi. Given the novelty of the solution concept I think the authors should have spent much more time building intuition about what this concept corresponds to especially in simple settings such as bilinear zero-sum games. It seems that effectively all states satisfy the definition of the provided solution concept in a bilinear game. E.g. suppose that we are arbitrarily far from the max-min equilibrium, the min agent suggests a small improvement step now the max agent can move in the direction of the gradient for arbitrarily long distance negating any gains by the small move of the min agent. This is clearly unnatural and explains why this algorithm can terminate fast, it is because it is willing to accept arbitrarily bad states as solutions. I think that this is a major shortcoming of the solution concept. The authors seem to agree that the solution concept is rather bad at times by explicitly allowing the dynamic to escape from these points with some small probability. The theoretical analysis is definitely non-trivial but if the proposed algorithm fails to solve even simple bilinear zero-sum games then the theoretical guarantees are not particularly strong. On the experimental side, the algorithm is being compared against weak benchmarks such as GDA. As the paper itself presents in the related work there have been a lot of recent developments on variations to the standard GDA techniques such as extra-gradient, optimistic methods, different types of averaging, etc which are known to significantly and robustly outperform GDA both theoretically and experimentally across numerous datasets. The reported FID scores are far from the state of the art and even the visual samples seem of relatively poor quality.Overall, I believe the paper attacks a very hard problem and pursues an interesting idea but both the theoretical and experimental results seem to suggest to me that the proposed approach is not very promising. ##########################################################################Summary:Instead of back-propagation, the authors consider a randomized search heuristic to train the parameters of neural networks. The proposed work is based on the hypothesis that the initial set of neural network weights is close to the final solution. The authors identify the problem that existing randomized search methods update all the parameters of the network in each update. Thus the proposed method updates only a single weight per iteration. Experimental results on MNIST and CIFAR10 show that the proposed method delivers competitive results.##########################################################################Reasons for score:  Overall, I vote for rejecting. Indeed, investigating alternative learning methods for deep architectures is highly relevant. My major concern is about the novelty of the paper and formal presentation (see cons below). I do not expect that the authors can address my concern in the rebuttal period.  ##########################################################################Pros: STRONG1. Investigating alternative learning methods for deep architectures is highly relevant.  2. Nice practical implementation details are provided like parallel computation or clever caching. 3. This paper provides experiments on well-known benchmark data sets. The results suggest that randomized search heuristics can work well for training the weights of deep neural networks.  ##########################################################################WEAK 1. Highly relevant theoretical work in this field is not referenced or discussed, e.g., Nesterov's "Efficiency of coordinate descent methods on huge-scale optimization problems" or work on randomized search in general, like "Evolution strategies - A comprehensive introduction" by Hans-Georg Beyer and Hans-Paul Schwefel.  2. The novelty is unclear to me. Maybe the presentation is sub-optimal, but I do not see any novel methodology here or insight. To make this more clear: it is well known that randomized search heuristics work very well on a wide variety of optimization problems (both, combinatorial and numerical). The real open question in this field is to *prove* under which conditions these methods will work, and under which conditions these methods will not work. What is the expected number of objective function evaluations? What is the probability that a solutions that is epsilon-close to a "good" solution is found in polynomial time? For me, answering any of these questions would make the paper at hand acceptable. 3. The formal presentation regarding classic an recent results can be improved (see below). ##########################################################################Some statements are unfortunate: e.g., on page 2, the authors state "the research community has started questioning the commonly assumed hypothesis if gradient based optimizers get stuck in local minima.". However, this is far from being a "commonly assumed hypothesis". It is a matter of fact from numerical optimization that gradient based optimizers *will*for*sure* get stuck in local minima. The "assumption" is, that these local minima are bad solutions. The authors must be careful with such statements, since many researchers spent decades to reveal insights into numerical optimization which shall not be ignored by today's scientists. This impreciseness in statements appears for recent works as well: On page 3, the authors state that "weight agnostic neural networks (WANN) also searches for architectures, but keeps the set of weights fixed.". This is, however, not correct. WANNs evaluate the expected performance of a model over various parameters which are shared by all connections. Computing the expectation over multiple parameters is far from keeping weights "fixed".  ######################################################################### This paper proposes a new type of generative models with a new inference method of latent variables. Specifically, the gradient of latent variables with respect to zero vector is taken as the inferred latent variables. Based on this, the authors generalize the propose model to implicit and variational versions and demonstrate the models on image datasets.Pros: the proposed method is easy and straightforward to implement. Cons:1. The model assumption that the one step gradient from zero vector equals to latent vector is quite limited and greatly constrains the model expressiveness. A justification that such assumption is reasonable is badly needed.2. Formulation needs to be carefully checked. For example, Eqn 2 is not entirely correct to me. The second term should not be binary cross entropy as there is no categorical variable involved. Also, please avoid using abbreviations (L^BCE, L^CCE) at the first time to introduce them, which are confusing. 3. Experimental results are not sufficient to demonstrate the efficacy. Need more quantitative analysis and experiments on more challenging datasets. 4. The claim that it saves parameters compared to VAE is confusing. In the variational version, parametrizations of mu(x) and sigma(x) are also required. A principled way to very this claim is to show that with the variational version, the method could use much less parameters compared VAE while has the better synthesis quality. Overall, the method proposed in this paper is new and promising. However, given the current unclear formulation and lack of strong experimental results, I recommend a rejection.  This paper describes a parameterized family of "graph shift operators", defined for any graph on n vertices as an n x n matrix where the i,j-th entry is 0 whenever edge (i,j) does not appear in the graph. The paper studies some spectral properties of the parameterized family, and experiments with using them as components of graph neural networks.This paper appears to me to be a hammer in search of a nail. It was not clear what problem is meant to be solved here. Nor does it seem that the topic is fundamental, scientific investigation. The paper is generally poorly written and was difficult to follow. I suspect much of that relates to the lack of clarity in the problem. However, the writing is also unfocused at the paragraph level.In short: this paper needs substantial work before it's ready for publication.  *Summary:This paper investigates stochastic methods for finding an approximate stationary point for a non-convex function that can be written as a finite sum. The authors consider the combination of adaptive (Adagrad style) methods in conjunction with a random shuffling.*Significance: The authors have missed a very important aspect of stochastic optimization.In stochastic optimization the right way to measure progress is to present the error versus the number of stochastic gradient computations.The authors present the error as a function of the number of epochs, where in each epoch we go over the whole dataset.Moreover, the suggested method does not have any benefit over full GD.Basically, when m=1, the method in the paper is equivalent to GD and obtains the same optimal $O(1/\sqrt{T})$ rate. When $m>1$ the authors prove a rate which is worse by a factor of $O(m^{5/4})$ compared to GD. Therefore, they actually show that GD (m=1) is optimal for their algorithm and that there is no benefit to stochasticity (m > 1).*Summary of review:The suggested method  does not show any benefit over full GD, so I don't see what is the contribution here. Methodology:The paper tackles the so-called open-set classification where query examples outside any of the classes in the training set should be detected at inference. By combining the feature extractor based on fashionable deep models and the classical clustering methods (k-means, GMM, etc), the paper empirically shows that this pipeline can address many open-set problems in realistic scenarios.                      Pros:Unfortunately, none.Cons:The paper basically reinvents the wheel. The so-called open-set classification problem is precisely the anomaly detection problem that has been studied for decades (see V. Chandola, A. Banerjee, and V. Kumar, Anomaly detection: A survey, ACM Computing Surveys, vol. 41, no. 3, p. 15, 2009), and the use of classical clustering methods (k-means, GMM, etc) was among one of the first efforts for anomaly detection. Applying the identical strategy on top of the fashionable deep features is the routine treatment for anomaly detection 101 in 2020, and brings in no novelty to the community.  Summary of Paper: The main claim of the paper is that out of distribution (OOD) detection can be done by use of pre-training and appropriately deriving a feature space from SOTA activations  via pooling, PCA based dimensionality reduction, L2 normalization.  Classical methods such as GMMs, k-means etc. can then be used to estimate the probability density function of features for use in OOD detection.  Several alternative schemes are compared against many OOD detection schemes.  Key contributions claimed are that pre-trained nets have information about open-world statistics and off-the-shelf net features along with appropriate choice of a low-dimensional representation helps in outperforming conventional OOD schemes.The paper builds upon recent work on OOD method benchmarking method by Shafaei et al (2019) that argues that most OOD schemes are not able to pass a less unbiased test designed ( wherein a Source data set is used for training using standard methodology, Validation data set for estimating a decision function between source and validation,  and finally the probability of outlier detection on other datasets and their variability is estimated to get robust view of OOD.).   1. The general approach is, as I think, quite misguided: the idea is to use pretrained datasets, extract features from it and apply a Gaussian mixture model. This means though, that such an approach will be able to recognize only classes for which features were made available (hence, the closed-world problem is just extended by additional knowledge about more classes from selected datasets, no open-world problem is solved).2. The fact that pretraining helps, is a generic statement which depends on the statistics of the dataset used for pretraining and then the dataset on which it is tested.3. the setup in 4.1 (split a single dataset into open and closed sets w.r.t class label) is questionable - as the image statistics in the partition are same.  4. in 4.2 the authors use an 'open' dataset for validation/tuning - this makes this dataset not open per definition. It may be true that it helps in the model generalizing better, but the terminology is still misguided.  In my view, the paper applies the testing methodology of Shafaei et al (2019) incorrectly to design an OOD algorithm and claim its superiority.   None of the tables in the paper provide error bars.  In order to convince that the insights are correct I would expect that the experiments need to be run with a larger sampling of outlier datasets (as done in Shafaei et al).    The authors describe a method for representing a continuous signal by a pulse code, in a manner inspired by auditory processing in the brain. The resulting framework is somewhat like matching pursuit except that filters are run a single time in a causal manner to find the spike times (which would be faster than MP), and then a N*N least squares problem is solved (which makes it slower). The authors claim that their method will perfectly reconstruct signals of finite innovation rate, however there appear to be mathematical errors in the proof.First, while any function in the class G defined in section 4 has finite innovation rate, not all finite innovation rate functions are of this form. For example, suppose there was only one kernel K, which was band-limited, having no power above a frequency f. Then no signal with power above f can be represented as a finite sum of shifted copies of K (or even an infinite sum). Yet these signals can still have finite innovation rate (for example if band-limited to 2f).Second, the use of the representer theorem appears to be incorrect. The L^2 norm defines a Hilbert space, but not a reproducing kernel Hilbert space as required for the representer theorem.  The representer theorem does not concern convolutions of L^2 functions with a kernel K, but an optimization over members of an RKHS whose norm is defined by K, whose objective function is defined by values at a finite set of points.As a counter-example to the claimed perfect reconstruction theorem, consider the single boxcar kernel K(x) = {0 if x<0; 1 if 0<=x<=1; 0 if x>1}. Let the signal X(x) = K(x), which belongs to the class G with N=1, t_1 = 1, and alpha_1 = 1. Let the threshold T=0.5. Then the first spike will come at time 0.5; depending on the refractory period there may be any other number of spikes, but there will in general not be a spike at time 1. Thus the original signal is not in the class defined by equation (3).In summary, this paper describes a potentially interesting biologically-inspired algorithm, but at least some of the claims appear to be incorrect.   This manuscript explores a mathematical framework and theory for a signal encoding/decoding scheme that shares many similarities to sparse deconvolution algorithms.  They demonstrate that they can learn a compressed representation of this signal, and there are several interesting theoretical properties.Pros:Some theoretical properties show intriguing propertiesSeems to work well empiricallyCons:The paper needs a significant revision for clarity.I am not confident in all of the theoretical analysis.I have some concerns about the theory.  First, there is a mismatch between the model definition in (1) and the theoretical analysis in Section 4.  The theory only holds if delta_j->0, which isn't discussed at all, and isn't what was used in the experiments. Or at least this is what is shown by the definitions given in (3) and (4).  This must be discussed at a minimum.The "Perfect Reconstruction Theorem" is achieved by limiting the class to signals produced by the decomposition.  It is unsurprising that the system can decompose signals produced by it.  Equation 5 is fairly well-known from deconvolutional models.  The relationship to existing work needs to be more clearly defined, so that the contributions can be considered in context.I'm confused by the claimed innovation in Lemma 3.  $X^*$ is defined at the minimum MSE for a number of spikes N.  Thus, it is the minimum energy and any other solution has equal or greater error.Shouldn't equation 2 look at reconstruction error and not simply the minimum? This paper considers the problem of secure aggregation for federated learning, where the goal is to design a protocol that allows the server to aggregate models from clients without learning anything about any individual model. The paper builds up on the secure aggregation scheme of (Bonawitz et al., 2017), and proposes a scheme that requires smaller communication and computation costs. The main idea is to use a sparse random graph as a communication graph as opposed to the complete graph used by (Bonawitz et al., 2017).Strong points:1. Reducing communication and computation costs in secure aggregation is indeed an important practical challenge. The idea of using a sparse communication graph is interesting.2. The paper gives experimental results on CIFAR-10 and compares with (Bonawitz et al., 2017). On the other hand, (Bonawitz et al., 2017) only present results on synthetic vectors and do not consider any machine learning task in their experiments.Major concerns:1. My main concern is that the paper lacks the mathematical rigor required in a theoretical security/privacy paper.(a) In Definition 2, the eavesdropper model (or threat model) has not been properly defined. In particular, what messages of the protocol the eavesdropper can observe is not explicitly defined. Further, it is not mathematically rigorous to simply say that an eavesdropper cannot obtain any information on the partial sum. This needs to be quantified, e.g., by using an information-theoretic or computational expression. (b) The proof of Theorem 2 in supplementary material simply shows that the eavesdropper cannot compute the sum of local models $\sum_{i\in\mathcal{T}} \theta_i$ from the sum of the masked models $\sum_{i\in\mathcal{T}} \tilde{\theta}_i$. How does this guarantee that the eavesdropper cannot obtain any information about the sum of local models? The proof is not rigorous (partially due to the ill-defined privacy requirement).(c) Sufficient details are not provided in experiments. Specifically, what encryption scheme is used, what PRG is used?2. From the proof of Theorem 2, it is assumed that the eavesdropper has access to masked local models of a subset $\mathcal{T} \subset V_3$ of nodes. This is a much weaker adversary model than (Bonawitz et al., 2017). It does not seem fair to compare costs of protocols that give security guarantees for different threat models. At the very least, it should be explicitly mentioned upfront that  the proposed protocol is secure against a weaker threat model than (Bonawitz et al., 2017).3. Similarly, the attack considered in experiments (Sec. 5.3) is fairly simple. Secure aggregation schemes in (Bonawitz et al., 2017) and (So et al., 2020) provide security against much stronger attacks  privacy is guaranteed even if a subset of devices collude with each other, or the server colludes with a subset of devices.4. The following recent paper uses a very similar idea to reduce communication and computation costs of secure aggregation. (This paper gives precise privacy definitions and rigorous mathematical proofs for security.) James Bell, K. A. Bonawitz, Adrià Gascón, Tancrède Lepoint, Mariana Raykova, Secure Single-Server Aggregation with (Poly)Logarithmic Overhead, Jun 2020. (https://eprint.iacr.org/2020/704)Even if one considers CCESA as a parallel and independent work, it will be helpful to acknowledge the above paper. Overall, the paper proposes an interesting idea of using sparse communication graphs for secure aggregation to reduce communication and computation costs. However, the paper seems to lack the mathematical rigor and details. ##########################################################################Summary:This paper proposes two families of methods for integrating external knowledgeinto neural networks aimed at classifying instances of online grooming. Thefirst family focuses on incorporating knowledge of word semantic similarity intotheir representations. The second, on different attention mechanisms forincorporating knowledge about theoretical stages of online grooming.The authors perform a solid amount of experiments to assess the differencesbetween their suggested methods and baseline models.##########################################################################Reasons for score:However, the way the paper is written and structured make it difficult tounderstand. While looking at the results, it is hard to really assess thecontribution of each suggested strategy, and how they compare to each other.Finally, the paper presents some serious conceptual gaps that undermine itsoverall credibility.##########################################################################Pros:- Fair amount of experiments for assessing impact of each proposed strategy.- The ideas for modeling different word variants could be built upon. I  especially like the idea of Elastic Pulling for combining the semantic  information of word representations. This idea would be useful to the research  community focusing on combining word representations.##########################################################################Cons:- Some serious conceptual gaps, and wrong claims. .- The paper is overall unclear and difficult to understand.- The task the paper is addressing is not well specified.- The methods are also not clearly explained.- Results are difficult to interpret and analyses are lacking.- Lack of ethical considerations for a system that could be used in law  enforcement. I would have liked to see a more detailed discussion on the  societal implications that systems aiding law enforcement could have, and in  particular, which measures should be taken for avoiding the prosecution of  innocent people by systems like this.#########################################################################Comments and suggestions for the authors:- The term "text normalisation" is spread throughout the paper, but is never  concretely defined, and is not immediately inferable.- I was not familiar with the "word semantics representation" noun phrase. After  reading the paper I am pretty sure that you mean "vector space model", or word  embeddings. Why not use these terms that will probably be more familiar to  potential readers?- In Figure 1 (left), you wrote Embedding; at the right you wrote WSR. Are these  equivalent?- I understand that you are doing classification at the conversation level, but  in page 3, in the "Base models" paragraph, you mention that "with the WSR's  embedding provided as input to the OG classifier in place of a sentence  embedding". In order to classify a conversation, you need a vector  representation of it. How is this obtained? In other words, how are you  aggregating the contextualized word representations (i.e., the output of the  LSTM or the XLNet encoder), into a single vector representation of the  conversation? Are you using the last hidden state of the LSTM or a pooling  method? Are you using the [CLS] token of XLNet or something else?- You mentioned that your dataset contains full conversations with an average of  431 messages per conversation. Are all the conversation turns separated by the  [SEP] token? What is the average message length? What max input length did you  use as a hyperparameter? Did you use the same text input for Model 1 and Model  2?- Saying XLNet is the SoTA for NLP is a false statement (p. 3 second-last  paragraph). First of all, NLP encompasses several tasks and there is no single  model superior to all the others in every task. Second, XLNet has already been  beaten in several tasks. See the Glue benchmark  (https://gluebenchmark.com/leaderboard) for a few examples. You mention that  XLNet is the SoTA of NLP again in p. 6; sec. 5.1; second paragraph.- Saying that XLNet iteratively refines word embeddings from a WSR similar to  that of LIU et al. (2017) is only tangentially true and is misleading, in my  opinion. Recurrent models such as those relying on LSTMs, are profoundly  different to those based on transformers such as XLNet.- I am not sure that there is a clear correspondence between using recurrent  models such as LSTMs and better handling of class imbalance. How is a  two-layer LSTM going to help handling class imbalance?- You mentioned several times that "text normalisation" was a strong point of  your contribution, but in the last paragraph of section 4 (p.4) you say that  you do not apply "text normalisation". I understand that you might be  referring to different kinds of normalisation, but I think this terminology is  confusing. I suggest using more precise language to clearly differentiate  what you are referring to.- What do you mean by "intersection tests"?- I suggest using the word "representations" rather than "coordinates" for  referring to the vector representation of a word (p. 4; elastic pulling  paragraph).- The correspondence between rows in Table 1 and the strategies discussed in  section 4.2, are not immediately apparent.- When not using GloVe embeddings, how did you initialize your word  representations?- You mention in Table 1 that bold are improved results, but improved with  respect to what?- You report precision and recall in Table 2, but not in Table 1. I think it  would be valuable to include these in Table 1, rather than the distance  reduction and average resulting distance metrics which are only valid for a few  strategies.- You could check the following papers for more context on combining  representations of different word variants:  * [Attention-based Conditioning Methods for External Knowledge Integration](https://www.aclweb.org/anthology/P19-1385/)  * [Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics](https://www.aclweb.org/anthology/P13-1149/)  * [Composition in Distributional Models of Semantics](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x)#########################################################################Some typos:- p. 4; Manifold Learning - "by building, using manifold learning, a new space"  could be better written as "by building through manifold learning a new space"  or "by building a new space through manifold learning"- p. 4 - "it is possible reduce dimensionality" -> "it is possible to reduce  dimensionality"- p.5, sec. 4.2, first paragraph - "collocates and std the span". Not sure what  std is supposed to mean here. This work proposes to apply zeroth-order optimization to hyperparameter tuning. Zeroth-order optimization techniques use function evaluation only to approximate the gradients, thus they are applicable to black-box functions for which the gradient is difficult to compute. For hyperparameter tuning this is the case in general. Existing hypergradient-based methods are not applicable to generic tuning tasks, and have scalability issue. Existing generic methods such as Bayesian optimization have weak scalability in number of hyperparameters. I agree with these observations and the goal of having a flexible, efficient, effective, simple and scalable solution.While the idea of using zeroth-order optimization for HPO makes sense, the paper has not reached a sufficient depth in exploring this idea. The paper proposes to use (Nesterov & Spokoiny 2017) without modification. There are several issues: (1) The paper concludes in page 5 that "Because the A-based constrained optimization problem f($\lambda$) is continuous, wecan use the zeroth-order hyper-gradient technique to optimize f($\lambda$) (Nesterov & Spokoiny, 2017).Nesterov & Spokoiny (2017) provided the convergence guarantee of zeroth-order hyper-gradientmethod when f($\lambda$) is Lipschitz continuous as defined in Definition 3." With continuity only, the algorithm can converge to local stationary points. Straightforward application of the algorithm could violate the 'effectiveness' and 'efficiency' goal. The situation is worsened in the high-dim case when the variance of the gradient approximation is large. The experiment in Sec 3.3 supports this concern, where HOZOG underperforms REV in the long run. (2) The algorithm still has several hyperparameters to tune, including q, $\lambda$ and $\mu$. From the experiments reported in the paper, each task uses different values of q, $\lambda$ and $\mu$. That raises the question about the practicability, especially, if one needs to tune these hyperparameters, whether it can still be efficient, scalable and simple. The paper has not discussed how to tune them for a given task. The procedure of tuning is required to be discussed because 'flexibility' is indeed a goal of this paper. (3) The same question can be asked about how the initial point of hyperparameters is chosen, and the paper does not discuss that. In addition to these issues, the paper makes a strong claim "forhigh-dimensional hyperparameter optimization problems which have no customized RFHO algorithm,HOZOG currently is the only choice for this kind of problems to the best of our knowledge." in the end of Sec 3.4. For example, there are other methods designed for high-dimensional problems:David Eriksson et al., Scalable Global Optimization via Local Bayesian Optimization, NeurIPS'19.N. Hansen. The CMA evolution strategy: A comparing review. In Towards a New Evolutionary Computation, pages 75102. Springer, 2006.M. J. Powell. A view of algorithms for optimization without derivatives. Mathematics Today-Bulletin of the Institute of Mathematics and its Applications, 43(5):170174, 2007.Since the paper emphasizes the importance of HOZOG in the high-dim scenario, some of these stronger baselines need to used. A final comment is that the phrase 'automated learning algorithm' in the title is not very informative, especially as the selection of hyperparameters of HOZOG is not automated. Summary:This paper proposes a new approach for multi-task learning that estimates the individual task weights through gradient-based meta-optimization on a weighted accumulation of task-specific model updates. Evaluations are performed in a multi-task learning setup on tasks related to computer vision (Multi-MNIST) and natural language understanding (tasks from GLUE and SuperGLUE).Pros:1) The paper is easy to follow. 2) Empirical evaluation is performed on vision and NLU domains.Cons:1) I am not completely convinced with the proposed alpha-Variable Importance Learning algorithm. It is not very clear in the discussion how the alpha is different from task-specific weights. For example, in algorithm-1, if you replace deltas in line-10 with line-6, then there is no need to have separate alphas and task-specific weights, where line-9 can calculate the task-specific weights directly. 2) In general, for a multi-task setup, I would expect to show the multi-task learning with multiple auxiliary tasks (thats the main motivation of this paper as well). However, the choice of the experimental setup is convincing, especially for the vision domain there is only one auxiliary task. 3) Both results in Table-1 and Table-2 suggest that the proposed algorithm is not superior over the baselines and previous approaches. The improvements are minor and sometimes lower, and I believe most of the results fall within the statistically insignificant range. Overall: I think the paper can be made stronger with more thorough discussion on the algorithm and its properties. Further, the experimental results suggest that the proposed algorithm performs more or less similar to previous methods. Hence, there is a lot of scope for further improvement and I would suggest rejecting this paper. I would also suggest the authors to perform more experiments and ablations. Questions:1) How is the alpha different from task-specific weights. Please discuss more on this. In algorithm-1, if you replace deltas in line-10 with line-6, then there is no need to have separate alphas and task-specific weights?2) Please provide statistical significant scores for all the results. 3)  What's the reason behind choosing a multi-MNIST dataset with only one auxiliary task? Arent there other datasets in a MTL setup with more auxiliary tasks? 4) Table-2 results for the development set are based on the average of multiple runs, but for test you reported the ensemble, so why dont you report ensemble for the development set as well?5) Can you also present some ablations/discussion on the learned importance of an auxiliary task (based on task-specific weight over the training trajectory) vs. any intuitive reason that makes sense of this importance of the axillary task for a given primary task? If there is not such correlation, then also it's good to discuss. Other comments: 1) Please try to expand the introduction section. 2) Please provide some more ablations.  Disclaimer: I am not an expert in the time-series domain, although I did some literature review while performing this review.#####################################Summary:The work investigates semi-supervised (SS) clustering of time-series data (i.e., clustering with few labelled points, which can also be seen as semi-supervised classification here). It specifically investigates how to train a convolutional autoencoder (CAE), which is applied on the time-series data creating an embedding for each timeseries, so that CAEs embedding clusters samples appropriately. It investigates 3 semi-supervised (SS) losses for this purpose: 1 is the loss proposed in (Ren 2018), and 2 novel losses, the Silhouette loss and the DB index loss, inspired by the corresponding internal clustering metrics. The work evalutes on 3 databases whether these losses improve clustering of the CAEs embedding, over vanilla training of the CAE (just reconstruction loss). The work also performs a study of whether the size of the convolution filters affect results and whether performing the gradient updates by the SS loss every batch or every epoch makes a difference.######################################Reasons for score:I am recommending a rejection, because of limited literature review, problems with writing/clarity, limited evaluation and analysis of the experimental setup (configuration etc), and the results are not too strong either. I would not mind much about the performance, if the other weaknesses were not there, as at least the Silhouette loss seems reasonable. But overall, the weaknesses overcame the good points.#########################################Pros: + Exploration of semi-supervision for clustering is interesting as its of practical importance, and rather timely, as there is general interest in the community on SS.+ The work explores 3 different losses for inducing clustering in the embedding space, 2 of which seem novel. The 2 new losses are quite intuitive as they are inspired by well established metrics. They are generic (not Time-series specific) and could potentially be interesting to parts of the community that look into how to create more discriminative models by clustering the embedding space of deep networks better, such as in semi-supervised learning.+ Performance of Silhouette loss seems to be interesting, as in all three databases it performs ok. (although I am not 100% convinced due to some unclear points about experimental setup and not too extensive evaluation, see below).#########################################Cons:  Con.1:The work is not well positioned within the literature. The review is limited both from the scope of unsupervised clustering, as well as from the point of view of semi-supervised learning (both in general and specifically to time-series). Details on literature review:One one hand, the work discusses only shallow unsupervised clustering methods, but misses deep models for unsupervised clustering. As a result, it considers as the *only* baseline a vanilla Auto-Encoder (AE). However, for unsupervised clustering, AE has been long surpassed by more appropriate models for clustering (AE has by itself no incentive to cluster the latent space). Examples include Contrastive AEs, extensions of VAE and GANs, etc. I below give a citation to a recent work that extends VAEs for clustering [1], which also contains references to other models, which the authors can consult. Also, some old and recent works that apply such models to time-series are [2, 3, 4]. Hopefully references therein will be of interest to authors. I wouldnt expect all of them to be evaluated, but at least some discussed, to give a good picture of where the field is on time-series clustering. (A suggestion: Perhaps the authors could improve the work by arguing that they chose AE for its simplicity, to focus specifically on the effect of the loss. But at least this discussion should be made.)[1] Dilokthanakul et al, Deep unsupervised clustering with gaussian mixture variational autoencoders, 2016.[2] Rifai et al, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction, 2011.[3] Fortuin et al, SOM-VAE: Interpretable Discrete Representation Learning on Time Series, ICLR 2019[4] McConville et al, N2D: (Not Too) Deep Clustering via Clustering the Local Manifold of an Autoencoded Embedding, ICPR 2020 (arxiv Aug 2019)Within the scope of the time-series application, authors mention deep approaches based approaches (via LSTM and CNN) but they dont cite any. I would expect some important works to have been cited.From the point of view of semi-supervised learning (SSL) in deep-learning, the literature review is also limited, constrained to Ren et al (2018). This should be improved. I below provide a reference to a work that proposed an SSL loss for improved clustering of latent-space via label propagation, which is in the same spirit as the losses discussed in this paper and hence should be discussed [5]. I also provide a reference to a more recent work with cluster-inducing properties, in case the authors find it useful to discuss [6]. ([6] is not SSL, but it may be of interest, according to your judgement).[5] Kamnitsas et al, Semi-Supervised Learning via Compact Latent Space Clustering, ICML 2018.[6] Kenyon-Dean et al, Clustering-Oriented Representation Learning with Attractive-Repulsive Loss, AAAI 2019 workshop.Finally, I below provide a reference to a recent work on SSL in time-series, where the references therein can also help identify important related work [7].[7]  Jawed et al, Self-supervised Learning for Semi-supervised Time Series Classification, PAKDD 2020.Con.2: Evaluation shows that no method is consistently better than the baseline AE. Which diminishes a lot the value in the technical contribution (e.g. the interest in the 2 novel losses). Even more, central claim of the paper about performance of the methods does not hold.Specificially, the claim from the abstract our methods can consistently improve k-Means clustering is not True. In Fig 2 and 4 from evaluation, we clearly see that no method consistently improves over baseline AE across all settings. For example, DB index is clearly worse than all on ECH5000. Prototype loss also falls short, especially in UWave and Fig 4 ECG500. Silhouete loss also falls below AE on UWave in the little-labels regime in Fig2.a and across all settings in Fig.4.b. This actually shows that no model consistently improves AE.Similarly, these claims about performance are accompanied in Abstract & Sec 1 with comments about the maximum increase in performance. But the maximum increase does not really convey the actual picture, as these are only the extrema. Instead, the authors should try to derive some statistic that represents the overall performance better.Con.3: Evaluation is limited:Method is only against unsupervised CAE and Ren 2018. This is a result of limited literature review I guess (weak point 1). The authors could expand the evaluation by comparing to:* other shallow approaches that are already applied in time-series. The authors mention some of these in the related literature, arguing they are slow. The authors could at least try some of them, and report performance and time-for-running. This is very important, as the time-to-run of these approaches should be compared with the time-to-train of the proposed approach, which is never mentioned. (to my understanding the authors train and evaluate on the same samples, ala transductive SSL, hence training time plays the role of time-to-run of the traditional approaches)* other related losses for SSL in deep models (e.g. [5], or other similar easy to apply ones like VAT, see references in [5]), or other baseline generative models (e.g. VAE/GAN variants). Con.4: The hyperparameter study in Sec 4.2.2 is not really useful. The authors explore whether different number of layers in the AE affect performance, and whether updating the weights of the AE with the gradients of the SSL loss at every batch step or only every epoch makes a difference. This is not insightful about the proposed losses themselves.Con.5: There is currently no mention of an attempt to configure the model and training configuration for each loss best. E.g., learning rate, number of steps till convergence, etc. So, I am guessing, same learning rate etc is used in all settings, which is unclear at which settings they have been configured. But, I think that the losses may have gradients at different scale (I m not sure, but seems so). For example, L_proto in Eq.3 has the form ylogp and may have gradients that are at a different scale than Eq 5 and Eq 6 that directly take the gradient of the distances. This means that for optimal convergence, and hence fair comparison, gradients of each loss should be scaled differently. E.g. using different learning rate. Or, better, using a different weight multiplier in front of L_supp, so that the same learning rate can be used for the reconstruction loss across all settings and only scale the L_supp gradients independently. What I would normally expect is a study of the performances sensitivity on such as hyperparameter.Con.6: Reproducibility is not high. There is no code, and various parameters for training (learning rates, number of sgd steps, etc) are not included.Con.7: Clarity of the paper, especially Section 3 (method) is not good. It is largely due to not good mathematical notation, which could help define certain things well, coupled with a problem in Fig 1:Sec 3.1 describes the CAE architecture, which is a very small model (2 layers of conv+pool) but still I dont think it does it adequately. I am particularly confused about the description of the 1D conv filters, their width, the dimension along which the conv is applied etc. This description is done in natural language and I think its currently ambiguous. The authors could introduce some more formal definitions of the input tensor (X \in ??), weight tensors, the output tensor of a convolutions, define dimensionality of these tensors, which would clarify better the models operation.Math notation should be improved. E.g. in Eq.1, C_j is not formally defined. Eq.2, p_j undefined (I think its the prototype for class j, where in Eq.1 is C_j, while p has been previously defined as a probality).Eq 3, L_proto, is a semi-supervised loss (L + U). Fig 1, shows only a loss that is called semi-supervised and symbolised as L_sup, while the figure shows that this loss is only dependent on embeddings of labelled data, not those of the unlabelled data. Hence it confuses, as it looks like a supervised loss.j is sometimes a class (Sec. 3.2.1), sometimes a sample (Eq4), and vice versa for C_j. Also, C_j in 4.b. represents a set of samples, and not just a class number, as in Eq.1. Please, repass math notation and define carefully each term. The current state makes for a difficult reading experience.\bar{C} in Eq.6 undefined.##############################################Questions for rebuttal period:  Please address the weak points I raised above, as well as the following (some are related to above anyway):Can you please clarify whether all the time-series within a database (e.g. FacesUCR) are of the same length? I guess so, but I would like a clarification. Also, please clarify this in text, e.g. in 4.1.3, as it is important for the reader to understand whether all samples have same dimension.Can you state explicitly in the text whether you are using any weighting of the SSL loss VS the reconstruction loss? I think not, but please state it explicitly. If not, please add a short discussion of whether you think the gradients of the different losses are at comparable scale, and whether such weight is needed or not.Can you explain how were configuration parameters for the methods been performed? Since there is no weighting for each loss, I am particularly interested in things like the learning rate. Was it configured on the CAE? On the CAE+SSL loss? On each one separately? On which set, since there is no validation fold? Please add to text.Sec. 4.2.2  Since autoencoder updates& for comparison: Can you please clarify/rephrase what this means? Why at each epoch only? Do you mean specifically due to the SSL loss? Because in previous sentences you say that the autoencoder *still* gets updated at each batch via the reconstruction loss.#############################################Minors, or additional feedback for improving the work in the future (not subject to rebuttal):Sec. 4.2.1. This also contributes to the poor KNN performance on this dataset.. I have not noticed any mention of KNN here (at least not until later in Fig.4). Where is this comment based on? If it refers to experiments later on (Fig 4), please clarify/refer to them in text.Sec. 4.2.2. The findings here also support & Prototype loss model.: I think these results do not support previous results, as the experiments are made on the database the same database that showed the trend in the first place. If they were made on ECG5000, they would support another trend. I would remove this argument. What these results support is that the filter-size is not important on this database.The authors could try to derive metrics that summarize performance across multiple settings. (e.g. average performance across all settings in each database, or average performance across databases on a particular setting (e.g. 4 labels)). Summarized in a table? It could help define clearly which method does better.It would be interesting to explore how the amount of unlabelled data affects results of training.It would be interesting to have 2 sets of unlabelled data, one for training and one for evaluation. To test whether the embedding generalizes and provides good clustering on unseen data, not just the ones it was trained on, and how the 2 cases (seen unlabelled vs on unseen unlabelled) compare.Sec. 4.2.1: The authors provide a description of how training evolves (During training, the clusters& ground-truth data) in this case of sparse labels. Perhaps in future work it would be interesting to actually provide an empirical analysis of this behaviour (e.g. showing how clustering changes between epochs? Perhaps even on a toy-dataset, in a low-dimensional embedding, to observe how the losses behave).It would be nice to have statistical significance tests between model results in Fig 2, especially about the main claims. (E.g. to prove that Silhouete is significantly better than others)Typos and other minors:Sec.1: and is not always segmented: Unclear to me what this means, perhaps rephrase a bit.Some typos: Sec. 1 have been show => shownSec.1 an semi-supervised => aSec. 1 model semi-supervised modelSec. 1 a unsupervised => anSec 3.2.2 the a partitioning This manuscript presents a method for generating protein sequences conditioned on protein structures. The core idea is to represented protein structures by their secondary structures in 3D space. This voxel grid is then encoded into a vector representation and decoded to a distribution over sequences. The authors propose to learn this model jointly with a sequence encoder, combining the sequence and structure representations to decode the sequence during training. For inference, the sequence encoder component is not used. Learning to generate protein sequence conditioned on structure is an interesting and important problem and has been attracting increasing attention from the ML community. Representing structures as voxel grids is an approach worth exploring and flexible structure representations could be promising. However, it isnt clear to me that this work achieves those goals and comparisons against key baselines (namely Ingraham 2019) are missing. Furthermore, the authors make many unsupported and unsubstantiated claims about their method. Specific comments and questions follow below.1.The claim that this method allows flexible fold representations seems to be undermined by the need to specify a specific structure for which sequences are decoded. How does this method allow for more flexible structure representations than Ingraham 2019 or other methods that require 3D coordinates of each residue? It isnt clear that this representation is more flexible than that required by RosettaDesign either. The authors need to justify this claim or remove it.2.The authors also claim that their method preserves spatial secondary structure information in contrast to others. However, all backbone-based protein structure representations preserve secondary structure information as this is defined by the backbone angles. The authors also claim that their method does not need predefined rules or structures. However, their structure representation is derived from a predefined structure, so this claim makes no sense.3.How does the density model loosen the rigidity of structures? The density model only represents a single structure and the authors do not use structure ensembles or other means to represent structural flexibility. I fail to see how this claim is supported by the presented method or experiments. In principle, any structure->sequence model can account for flexibility by ensembling over possible 3D structure.4.Have the authors compared their method with Ingraham et al. 2019? Ingraham et al solve the same problem with, arguably, an even more flexible representation of protein structures, because the graph based representation is invariant to rotation and translation of the structure in 3D space. A comparison against Ingraham et al is critical to understand if the proposed voxel-based structure representation is better than other methods. At face value, Ingraham 2019 reports better perplexities with a difficult train/test split and fewer training examples than this work. In fact, Ingraham et als out of distribution perplexity surpasses this methods in distribution perplexity calling into question the utility of this method. The authors should compare their method on the same train/val/test split used by Ingraham et al.5.For the in distribution test, how do sequence models fit per fold perform? For example, how well do profile HMMs fit to each fold model the heldout sequences from those folds?6.Why are the val and test splits so small (only 2.5%)? Given these splits, how many sequences occur in each?7.Because this method encodes structures into a voxel grid, it is not invariant to rotation and translation of proteins in 3D space. The authors align their structures based on center of mass and orient the protein such that the first residue points in the negative direction of the z-axis. How sensitive is the method to misaligned proteins? Is this alignment scheme robust to alternative conformation of the same protein? What happens when structures are rotated? The described alignment scheme also has ambiguity in the rotation around the z-axis. How is this addressed?Things that would improve my score:1.Provide support for unsupported claims in the introduction.2.Compare against Ingraham et al 2019.3.Examine the sensitivity of this method to rotated structures. In this paper, the authors propose a learn2weight framework to defend against similar-domain adversarial attacks. Experimental studies on Amazon dataset are done to verify the proposed learn2 weight. The paper is not easy to follow. The presentation and organization should be further improved. Here are the detailed comments:(1)Adversarial attacks are widely used in various application domains, e.g., computer vision [ref1] and reinforcement learning [ref2]. It is necessary to discuss with these related works, and highlight the difference and importance of adversarial attack methods on NLP tasks.[ref1] Adversarial Examples that Fool both Computer Vision and Time-Limited Humans[ref2] Minimalistic Attacks: How Little it Takes to Fool Deep Reinforcement Learning Policies(2)The authors highlight domain adaptation theory several times. Please give a clear description on what it is. (3)Where is Table 1 used in the main content? (4)Regarding definition 2, the following two points are unclear: (1) is f_S(A) the true label of A. Based on the figure 1 (a), only correctly classified source samples are used while the definition does not show this. (2) why f(A,W_T) = f_S(A)? f is the target classifier, are you generating the domain-invariant samples? (5)The rationale of similar domain adversarial attack is confused. It is more reasonable to use source data to help generate target adversarial samples X which confuse the classifier to deviate the label f(X) \neq f(X) where X is the original target sample. However, the paper generates source adversarial samples, which naturally may confuse the target classifier due to the domain divergence. It is unclear why and how these source adversarial samples can contribute to the robustness of the target classifier. (6)Regarding the accuracy drops in Table 2, it is highly possible caused by the data shift between different domains. How to differentiate the importance of the data shift and adversarial in the accuracy drops? (7)The technical part is not easy to follow. The sections 5.1 to 5.3 are not linked well. It is necessary to give more contents on the motivation and flow of these algorithms instead of just putting them in algorithm charts. (8)Why target data is used in Algorithm 2 and also transfer loss optimization? In the introduction, target domain information is assumed to be unavailable. Moreover, algorithm 2 is to reduce the domain divergence (if I understand correctly). I am quite curious how the proposed method differentiates from other transfer learning methods.  This manuscript proposes a GAN-based approach to generate calcium traces of neurons by using adapting methods from Wasserstein GANs and WaveGAN to produce the traces.  Using OASIS to extract spikes gives similar patterns to real data on a variety of metrics. This is confirmed over a relatively large number of neurons.The big pitfall of this manuscript is that the authors have not demonstrated the scientific utility of their method.  The purported rationale, "Generative models of neuronal activity hold the promise of alleviating the above problem by enabling the synthesis of an unlimited number of realistic samples for assessing advanced analysis methods," is not shown.  The motivation about how generative approaches can provide additional data to find higher-order relationships is fraught: perhaps the advanced analysis techniques would find more patterns by augmenting fake data, but it doesn't really increase the scientific confidence nor address the fundamental uncertainty due to limited data.  While there are some cases where research groups have used GANs to help advanced analysis techniques, such as in domain adaptation and semi-supervised learning, those cases are not shown here, nor is it clear how to build them in.  This paper needs to be revised with a clear use case to motivate it.Pros:The manuscript is clearly written, and shows an effective use of GANs to mimic scientific data.Significant exploration of the results are shown.Cons:The metrics shown compare derived properties of the traces, not the traces themselves.  This is limiting.  In Figure 2, the real data and synthetic data have clearly different noise levels and background patterns.  If the results are visually different in a relatively small figure, it points that more exploration of the traces themselves and the generative quality needs to be explored.The title suggests that "calcium imaging data" is being generated, but that's not really true.  Instead, the calcium traces on individual neurons are being generated, which is a derived product from calcium imaging data.  Perhaps this is just semantics, but it's misleading and should be clarified up front.As in the major point, no clearly elucidate use case.The lack of conditioning (on trials, behavior, unlying spike train) is a major limitation, since for most scientific applications we care about recovering these properties. This paper discusses an artificial life approach to artificial intelligence, through 1) first discussing some general principles that may be useful in artificial life/chemistry systems from the perspective of open-endedness; 2) sketching an experimental system aiming to implement these principles and showing preliminary qualitative results. While the paper discusses a number of ideas that are highly relevant to AI and should be better known by the AI community, the paper has also some weaknesses and is not currently in a form that is ready for publication.Strengths:+ This paper's topic is a nice opportunity to enable the AI/ML community to know better about Alife approaches to artificial intelligence+ The particular topic of open-endedness is also highly relevant, including in the perspective taken in this paper where one aims to model complex dynamical systems that do not assume a pre-defined concept of agent/individuals. This is useful in helping researchers in the AI community to deconstruct some of the assumptions of the models they are working onWeaknesses:- The paper does not explain clearly the relevance of the Alife approach to AI, in particular it does not explain well the historical roots of this idea that could enlighten AI readers (e.g. discussing foundational work such as Brooks and Steels, 1995), and does not analyze what fundamental contributions where made in the past, and what were the limits.- Related to the point above, the paper does not position itself clearly in this vast literature: both the general/theoretical ideas and the experimental framework discussed overlap significantly with the large body of work in Alife. Clear statements of contributions would be useful.- It is not clear what is the concept of "(general) intelligence" considered here, and whether there is a real value/utility in using this word, as opposed to saying the family of systems discussed in the paper is for studying the "self-organization and evolution of complex structures". Formulating the general problematics around theoretical questions on the origins and evolution of complexity would also enable better linking with the literature in the sciences of complexity and theoretical biology, which is not sufficiently made in the current version of the paper.- It is not clear what is the logical rationale of the listing of principles considered in the paper: what is the scientific objective of this list of principles? how can one measure/evaluate their utility? where do they come from? Also, there seems to be some (at least apparent) inhomogeneity between them, e.g. point 1 says "there should be no built in notion of an individual", but point 2 says "the evolution of new emergent individuals create novel opportunities..." --> point 1 reads like an assumption while point 2 reads like a desirable emergent property.- The proposed SIM system lacks formal definition (e.g. precise pseudo-code enabling precise understanding and reproducibility)- While the paper aims for a general system with "general neural network controllers inside much fewer elements", in practice the system appears to have a lot of specific elements (e.g. physical/enzymatic systems) that are less generic than several recent work generalizing cellular automata as grids of neural networks with multiple kernels, e.g. Chan, 2020; Mordvintsev et al., 2020; Gilpin, 2019. Thus, it is not clear which properties are more general/differ/more useful than these other works- There is no proposed methodology for evaluating the success/failure/progress of the approach: e.g. how could one characterize the self-organized structures that are qualitatively observed in the preliminary runs presented in the paper? What are precisely the scientific questions addressed by experiments, and how can experiments answer them?I recommend the author(s) frame more precisely their work in terms of scientific objectives and links with the conceptual history of alife approaches to AI. The framing of the scientific objectives would benefit from a clear formulation of the scientific questions addressed, and a strategy for measuring progress towards answering them (in addition to the strategy for developing experimental tools presented in the paper, which requires more details to enable better understandability and reproducibility). References:Bert Wang-Chak Chan. Lenia and expanded universe. arXiv preprint arXiv:2005.03742, 2020.Brooks, R. A., & Steels, L. (Eds.). (1995). The artificial life route to artificial intelligence: building embodied, situated agents. L. Erlbaum Associates.William Gilpin. Cellular automata as convolutional neural networks. Physical Review E, 100(3):032402, 2019.Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, and Michael Levin. Growing neural cellular automata. Distill, 2020. doi: 10.23915/distill.00023. https://distill.pub/2020/growingca. In this paper, the author studies the bias problem in race classification task with face data. Specifically, it first analyses the influence of kernel regularization and batch normalization to categorical cross-entropy loss and proposes a maximum categorical cross-entropy loss. Experiments on two face datasets colorFERET and UTKFace demonstrate the effectiveness of the proposed method. From the ethical aspect, the topic of this paper is important and interesting. But it seems the proposed loss is not specially designed for the racial bias problem, please consider evaluate the proposed loss on general image classification tasks. There have some losses for imbalanced training can be used in this topic (e.g. Focal loss, GHM-C loss). In this paper, the author only compares their approach with the traditional CCE loss which is not convincing. And also, to show the proposed approach can mitigate the bias problem in the race classification task, the author should show the accuracy for different races rather than an averaged accuracy. Overall, I think this papers topic is important but the approach seems not make sense and less relevant to the racial bias problem.Pros.1.The bias problem that this paper studied is an important problem for image classification, especially for race classification.2.The results in the experiment section could partially demonstrate the effectiveness of the proposed MCCE loss.Cons.1.The writing of this paper is bad and hard to follow. The author uses several subsections (Section 2.1~2.3, 3.2) to introduce the cross-entropy loss, kernel regularization, batch normalization, and linear interpolation which are redundant.2.Algorithm 1 is not aligned with the paper. The variable \mu is not used in the algorithm but seems to be very important to the method (see Section 3). 3.Accuracy, the key evaluation metric in the experiment part, cannot fully demonstrate the effectiveness of the proposed method. Please consider adding confusion matrix or per-class accuracy.4.The discussion section (Section 5) seems not clear. The figures in that section (ME measures, loss curve, training accuracy) cannot support the conclusions.5.There are some formatting problems in the paper (Page 7 Line 1 & 5). The figures in the paper look like screenshots from Excel which are not very clear. Please consider inserting the figures in a vectorized format.Overall,  I think this paper is far more below the ICLR acceptance bar. In this work, the authors train a model on a subset of architectures (~60k) in the DARTS search space and use this model to predict the performance of architectures outside of that subset in DARTS. They then use this as a surrogate for having to perform network training for evaluating NAS algorithms.NAS benchmarks are a good thing, given that they facilitate NAS research outside of labs with lots of resources. Saying this, I think there are serious issues with this paper.NAS-Bench-301 is a model that predicts the performance of networks in the DARTS search space. We know from https://arxiv.org/abs/1912.12522v3 that there is a significant lack of variety in the performance of models within this space, and this work compounds that. Additionally, this paper only considers CIFAR-10. This is a step back from NAS-Bench-201 which despite its small size, did contain multiple datasets. The compute used by the authors (training 60k networks) has gone into differentiating a bunch of networks that are all quite good, within a few percentage points of error. Im not sure how this is of practical use.A problem with NAS-Bench-101 and 201 is that the search spaces are small (423k, 6k) as the authors point out. NAS-Bench-301 encapsulates the DARTS search space which is much bigger (10^18). However, from https://arxiv.org/abs/1912.12522v3 we see that randomly sampling within this space gives networks between 96.5% and 97.5%. I would argue that it doesnt matter how large a space is if it is lacking in variety; Every possible network works well enough. In 101/201 we see networks across a much larger range (typically between 80-95% although there are some much lower). I believe this is more interesting from a research perspective as we would like to apply NAS to situations where networks can break (and avoid this happening). The DARTS networks do explore a slightly higher accuracy range but it is not at state-of-art levels, so the added value is not clear.The authors are critical of random search, stating (i) `random search stagnates and cannot identify one of the best architectures even after tens of thousands of evaluations`, (ii) NAS-Bench-201 (Dong & Yang, 2020) only contains 6466 unique architectures in total, causing the median of random search runs to find the best architecture after only 3233 evaluations.  However, (i) makes random search sound like it is failing, where on Figure 4 we can see at 10^4 (s) random search is doing very well  - matching or beating all the other techniques.  We know from https://arxiv.org/abs/1806.09055  that random search works well on the DARTS  space (probably due to the lack of variety). (ii) Random search does work well on NAS-Bench-201 but the DARTS algorithm fails, even though it works on this space. The statement in the abstract that using previous benchmarks  leads to unrealistic results that do not transfer to larger search spaces. does not tell the whole story, as there appears to be more going on than just the size of the search space. Some results on this large DARTS search space, do not transfer to the smaller search spaces. It seems that there is more to a search space than the number of architectures within it.In terms of presentation, this paper is well written, although the figures could be larger. I appreciate that this is problem to keep within the page limit.NAS benchmark papers are important and shape the direction of research in the field. This paper doesnt present a benchmark. It provides a model that represents computationally-efficient means of getting network accuracies from the DARTS search space. I appreciate this endeavour and it could be of use outside of this space  being able to map a 10^18 space with 50000 points indicates that the effective size of the space is much smaller, and is highly predictable.This space, although large, has very little variety in terms of network accuracy. I believe NAS-Bench-101 and 201 despite their sizes represent more varied search spaces. 201 also covers multiple datasets, whereas 301 only has CIFAR-10 (which it feels like we are saturating on).  I recommend rejection for this paper, as I do not believe it represents a step forward in the way we benchmark our NAS algorithms. We need to develop more interesting search spaces,  rather than advocating exploration of uninteresting ones. Summary: This paper systematically presents a large-scale empirical study on the disentangled representation learning when the underlying factors are possibly entangled. From the results of purely unsupervised settings, the authors have discovered the shortcomings of the existing metrics of disentanglement as well as the poor learned representations (in terms of disentanglement). However, with the help of small amount of factor labels or other weak supervision signals, recent approaches could learn fairly perfect representation.First of all, it is worthy to pay attention to the possible correlation between factors when you intend to learning a disentangled representation. And the whole empirical results are carried out by very large number of experimental batches, which to some extent could well support the conclusions displayed in the paper.However, there still exist several limitations in my opinion:(1)The design of induced correlation is too simple. As you have mentioned in related work, there were some papers noticed the too ideal assumptions of traditional VAE-based models which may not be held in practice. IMO, the linear dependency between only two variables is far from reality as well. More complicated settings should be involved.(2)In line with the former limitation, diagnostics of the potential entanglement should also not be limited to pairwise level, which cannot scale up to high dimensional latent factors.(3)The novelty of Section 4 is somewhat limited as all the correction methods and even some conclusions were proposed by the previous work. [Summary]In this paper, a graph view-consistent learning framework (GVCLN) is proposed. Specifically, two view learners are used to give predictions for the input. Then, a consistency loss is employed to force the two viewers giving the same predictions. Moreover, a co-training scheme is proposed to alleviate the label sparsity problem.[Pros]+ This paper is easy to follow.[Cons]- The novelty of this work is limited. It seems that this work is a simple combination of [1] and [2], with slight modification. Also, the authors are suggested to include more baselines, especially augmentation-based methods, e.g., [3].- Three studied datasets are of small scales, which are well know to have unstable results.- It is not clear how to select high-confidence pseudo labels. More experiments, e.g., parameter sensitivity analysis wrt the confidence threshold, ablation studies of the two loss, are needed.[Evaluation]Overall this paper presents a simple yet effective framework to semi-supervised node classification. However, the novelty of this work is limited and more experiments are necessary.[Ref][1] A Simple Framework for Contrastive Learning of Visual Representations, in ICML, 2020[2] Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes, in AAAI, 2020[3] NodeAug: Semi-Supervised Node Classification with Data Augmentation, in KDD, 2020 This paper tries to address the limited training data problem by exploiting the consistency between different views of the graph data. However, important details and justification are missing. The major problems are:(1)The proposed model lacks detailed explanation and justification. How do you generate the view for the graph from its features? How and why do you obtain three head representations using graph convolution or graph attention? Why using convolution and attention for the two views respectively? How do you do dropout? Why using the same graph convolution layer later for the two views? After seven lines of introduction of the model, the paper is focused on training, and leave all the above questions behind.(2)For training, how do you judge ËËhigh confidence predictions for generating pseudo-labels? What do you mean by ËËthe same prediction representations, ËËverification set and ËËlabel rates? Moreover, with limited training data, how do you obtain ËËwell trained two-view networks? What is the ËËstop condition, simply max epochs?(3)The authors give some settings of parameters and say that the other parameters are specified in the appendix. However, after references there is no appendix.To summarize, without justification, the proposed model is not convincing. Without details of the model and implementation, this work is difficult to reproduce. The authors propose a biological model for loss function differentiation which reuses a single pool of neurons to compute predictions and prediction errors (i.e., derivatives of a loss function with respect to the neural acitivity variables) in two sequential phases.I have a major concern that I need to see addressed by the authors:- In my understanding the proposed method is not local in time and this is not at all clear, the way the paper is written. This is best seen on the last line of Algorithm 1: the apparently innocuous term $\frac{\partial x^l}{\partial W^l}$ should be evaluated at the _feedforward activation values_, not using the equilibrium $x^l$. I wonder whether the authors are aware of this, as such a detail could be somewhat easily overlooked when using automatic differentiation.Note that this is very different from dropping $f^\prime$ (as per footnote 5) from the gradient, something that has been experimented with in some of the feedback alignment implementations in the past (for example, some of the simulations in Lillicrap et al., 2016, do not backpropagate $f^\prime$, although the local ${f^\prime}^l$ is still used when updating $W^l$).Stemming from this issue, the advantages of the proposed scheme against standard predictive coding implementations using two populations of neurons (predictions and errors; e.g., Whittington & Bogacz, 2017) aren't clear. As I understand the algorithm, prediction activity is overridden by error activity (in a way that still requires coordination/phases), but since prediction activity is necessary to compute the aforementioned term, the resulting learning rule becomes nonlocal in time.A proper evaluation of the significance of the advance brought by the method requires clarifying this issue.I leave some additional comments for the authors:- "Given that backprop provides an optimal solution to this problem (Baldi & Sadowski, 2016)"The claim that backprop provides an optimal solution to the credit assignment problem is a very strong one, and I'm not sure that it is supported by the cited reference. Optimal gradient approximation is different from optimal credit assignment.- "Unlike contrastive Hebbian methods, AR does not require the coordination and storage of information across multiple backwards phases, which would pose a substantial challenge for decentralised neural circuitry."While AR does not require storage of information across multiple backwards phases, it requires storage of information across forward and backward phases.This is related to my major comment above, and this issue/potential misunderstanding reappears in multiple points of the paper.- In (1), why is $x^L$ and not $x^l$ appearing?- In the equation below (2), should $x_i$ be $x^l?- Figure 3: these curves are rather hard to see, perhaps consider presenting mean +- error bars over many runs? - I could not follow several of the steps in the arguments presented in the appendix titled "The energy function". How is (11) related to the dynamics of the algorithm?- There are a few typos that should be corrected, e.g., "is designed to converges to the minimum". - Summary: A multi-agent market simulator using GANs are proposed.- Quality: I see many inappropriate layouts that make it difficult to read and hard to focus on the technical aspects.- Clarity: The paper is not clearly written and not self-contained.- Originality & Significance  - Sec. 2 is supposed to describe the proposed algorithm, however, it is hard to see any novel contents. If the novel contents are written in another section, then it should be rearranged so that the readers would easily catch them.- Cons: Hard to find out due to unclearly written paper. I see no comparison to conventional works.- Layout: I see many inappropriate layouts. I recommend the authors to fix it.  - I recommend that every figure has a title even if there are subtitles (Fig. 1 and 4)  - Fig. 1(a) and Fig. 1(b) are independent contents. I recommend that the subfigures be separated. (I guess that that's why the authors didn't write the title of Fig. 1). Also, the two figures in Fig. 1(b) can be shown as two subfigure.  - Please modify the figure layout properly (size, location, width, etc).  - I see the bad layout in Figure 3.  - The contents of Figure 4(c) would better be shown in Table not figure.  - When I print the paper, some part of Figure 5 (c) is not printed since it is outside of the page. The size of the figure should be adjusted.- Typo: I also see some grammar errors. Any grammar checking program would find those errors. I recommend the authors check the grammar of the whole text again.  - In the first paragraph of Sec. 1.3, please add space in front of "Both".  - What is the blank in the last paragraph of Sec. 2? (simulation parameters out of the ___________) This work proposes a golden symmetric loss (GSL) for the noisy label learning problems. Compared to previous weighted symmetric cross-entropy losses, the proposed loss estimates the trade-off parameters using the transition matrix. Empirical studies demonstrate that the GSL method is better than some baselines.Comments:1. The proposed GLS is simple. My main problem is that why should we estimate $A(\hat{C})$ and $B(\hat{C})$ simultaneously? As they are trade-off parameters, estimating one leads to the same results, but the trainable parameters can be reduced. Moreover, it seems that GSL loss only works when the noise data are generated via a transition matrix. Would it be better if we consider the instance-dependent case and relate the trade-off parameters to the feature $x$?2. There are some flaws and informal presentations in theoretical proofs.- Eq (7) basically follows Proof 2 in (Ghosh et. al. 2017). But, in the first equation, the law of total expectation should be $E_{x}E_{y|x}E_{\tilde{y}|y,x}$ instead of $E_{x}E_{y|x}E_{y|\tilde{y},x}$.- In the fourth equation, the expection should be equipped to $l_{ce}(\cdot)$, since there are $f(x)$ in $l_{ce}(\cdot)$. Hence, it is also desirable to take expection to define $\mathcal{A}(\hat{C}^Tf(x),y)$. Otherwise, $\mathcal{A}(\cdot)$ depends deeply on the instance $x$. - The biggest problem is $\Delta\mathcal{R}$ need not be non-positive. The reason is that $f*$ is the optimal solution to $\arg\min R(\cdot,C*)$ instead of $\arg\min R(\cdot,\cdot)$. The non-positivity requires a detailed discussion, and I think this property does not hold in some conditions. - The lower bound of $\Delta A / \Delta R$ is not discussed. If $\Delta A / \Delta R < 1$, then the algorithm is able to learn from 100% noise data, which is not realizable.- What is the definition of noise-tolerant? The authors need to give a mathematical definition.- It is weird that Theorem 4.1 merely analyzed the vanilla cross-entropy loss. The authors need to bridge the results of Theorem 4.1 to GLS loss.Minor comments:1. The references are informal. Please put the author's names into the brackets.2. The experimental results seem to be far away from state-of-the-art noisy label learning methods, such as DivideMix [1]. While the SOTA performance is not the most essential issue for me to judge a paper, I highly recommend the authors to explore better model architectures that provide competitive results. 3. The related works are not thorough for me. Some state-of-the-art noisy label learning methods (in 2019/2020), e.g. DivideMix, are not discussed. [1] Li J, Socher R, Hoi S C H. DivideMix: Learning with Noisy Labels as Semi-supervised Learning[C]//International Conference on Learning Representations. 2019. Summary and contributions  The paper takes advantage of NAGO and proposes to search for a family of student network architectures instead of a single architecture, aiming to be more sample efficient. This reformulation of the NAS problem makes it possible to search in an expressive searching space, at the same time, avoid to waste time in comparing similar architectures.Strengths  This KD-NAS approach further develops the benefit of NAGO and makes it convenient to search for a family of student architectures. The optimization objective of finding a family instead of a single architecture helps to speed up NAS process or more sample efficient.Weaknesses  There have already exist KD-NAS approaches and the main difference of this work is to search for a family. This objective mainly takes advantage of the generator in NAGO, so the contribution and novelty should be reduced accordingly.  The purpose of some experiments in this paper is a little confusing. From my point of view, this paper aims to utilize NAS to benefit KD, at the same time, make NAS more efficient. Maybe you should compare AutoKD with earlier KD-NAS approaches instead of NAGO to show the impact of KD on NAS.  The title shows the fact that the macro-structure of a network is more important than its micro-structure, which has been studied in previous work. But this fact doesnt logically lead to idea of searching for a family. Also, I think it isnt clearly articulated that how this family of student architectures can benefit knowledge distillation. Maybe the performance gains result from the ensemble of networks.  Finally, I think it doesn't make much sense to have the comparisons showed in Figure 7. Besides, the logic and results shown in the visualization of Figure 6 are not clear enough to me.[ Detailed comments]1. In Chapter 3.2, the original meaning is unclear: The hyperparameters ... that it represents.2. In Algorithm 1, there are many unmarked sentence endings. In addition, where are the definitions of functions f and ±, and what does D refer to?3. In Figure 6, the specific meanings of various arrows and various colors need to be marked. This paper applied knowledge distillation (KD) on network architecture generator optimization (NAGO) which is one of NAS. Specifically, KD has used in the procedure of searching and the parameter of KD has involved in the search space. As a result, the proposed method (AutoKD) seems to improve NAGO.Pros)- Extensive experiments are performedCons)- Some figures are not clearly shown. Please refine the figures (e.g., figure 2) for clarity.- Applying KD into NAGO seems to be naively done. It seems that the proposed method is incremental and the contribution is limited and the differences are not highlighted. - The result comparison in Table 1 looks not fair:  - KD used in this paper used better teachers following the convention, but the competitor KD-LSR and SKD in the table are self-distillation methods, so the comparison is meaningless.  - On CIFAR100 dataset, CRD in the original paper used WRN-40-2 as a teacher and trained the student of WRN-16-2, which has only 0.7M parameters with an accuracy of 75.64. However, this paper reports CRD used ShuffleNetV1 which have more parameters   - On MIT67 dataset, VID used an ImageNet-pretrained model for transfer learning, but AutoKD used the fine-tuned teacher which is much beneficial to KD in terms of performance.  - On CIFAR10 dataset, the compared models (WRN 16-1 and two WRN 40-2s) have fewer parameters than that of NAGO for AutoKD. Therefore, it is hard to say that AutoKD outperforms them.- Experimental results are somewhat unconvincing:   - As weight is zero in Figure 2, the accuracies in the table should show consistent performance but are deviated w.r.t temperature. The authors should clarify this.   - Why the accuracies of NAGO in Figure 4 look low compared to the other results in the paper? - Using KD on NAS leverages additional computational cost, but it is not clearly compared quantitativelyComments)- The method is incremental, and the novelty is limited. The experimental results comparing with other methods are biased to the proposed method, where the competitors' performances are not fairly compared, so it is hardly convincing the results and the effectiveness of the proposed method.  ##########################################################################Summary:The authors proposed CNV-Net, a deep learning-based approach for copy number variation identification. They encoded mapped DNA sequences into a pileup image that captures reference sequence, sequencing coverage, and mapped reads. Then, they used CNNs to classify it into deletions, duplications, or non-breakpoints. They benchmarked CNV-Net with two whole-genome sequencing datasets and claimed to obtain more accurate and efficient results than current tools.##########################################################################Major comments:While the paper has its own merits, unfortunately, it has several issues that need to be addressed.- Although the authors claimed that CNV-Net is the first tool to use a CNN to detect CNVs, this is not true. Several previous works used CNNs for CNV detection. Furthermore, I think other machine learning and deep learning-based works should also be acknowledged. Id recommend authors properly cite and compare previous works with the proposed method. Some of the previous works include the followings:(1) Zhang, Yun Xiang, et al. "DL-CNV: A deep learning method for identifying copy number variations based on next generation target sequencing." Mathematical biosciences and engineering: MBE 17.1 (2019): 202-215.(2) Cai, Lei, Yufeng Wu, and Jingyang Gao. "DeepSV: accurate calling of genomic deletions from high-throughput sequencing data using deep convolutional neural network." BMC bioinformatics 20.1 (2019): 665.(3) Hill, Tom, and Robert L. Unckless. "A deep learning approach for detecting copy number variation in next-generation sequencing data." G3: Genes, Genomes, Genetics 9.11 (2019): 3575-3582.(4) Pounraja, Vijay Kumar, et al. "A machine-learning approach for accurate detection of copy number variants from exome sequencing." Genome research 29.7 (2019): 1134-1143.- The main contribution of the paper would be using a pileup image of mapped reads and a CNN to detect CNVs. However, in my view, the novelty of the paper is quite limited. As stated in the introduction, the pileup image encoding and CNNs have already been used in a couple of previous works for SNV detection. I could find any significant methodological differences in CNV and SNV detections; it seems like rather a straightforward application of previous methods on another similar problem. Otherwise, please clarify the differences between the two problems and what authors have done to overcome the new obstacles.- The core issue I have with this paper is that I do not think the experiment settings are realistic. As stated by the authors, CNV-Net must know the candidate CNV positions. I think this is a serious issue that must be handled rather than leaving it as a limitation of the work. Currently, the CNV-Net is evaluated with mapped and pre-preprocessed reads with CNV centered breakpoints. Compared to the experiments conducted in the previous works, the experiments of the proposed work seem limited, unrealistic, and biased in favor of the proposed method. - In my view, the authors left out too much information. For examples, it is quite difficult to understand how they used other tools for the experiments; Do they only use the CNV breakpoints that passed the quality control filter as CNV-Net? Or do they use all the mapped reads? What tool-specific arguments did the authors use for each tool? Currently, it is extremely difficult to reproduce the results presented in the paper.##########################################################################Minor comments:- How did the authors choose the specific numbers to encode individual base into R channel (e.g. A with 250, G with 180)- In the results section, the authors stated that they only used CNVs passing the quality control filter. Please provide more details for the filter explaining the filtering criteria and how they chose them.- In Table 2, how did the authors obtain the metrics for the multi-class problem? Please state whether they are macro or micro averages of scores.########################################################################## The paper builds on recent revival in control-theoretic approaches to deep neural networks by proposing an adaptive controller that projects intermediate representations in the network to their "manifolds" and consequently makes the neural network robust to input perturbations. Pros:+ Active controller-based projection of intermediate features is an interesting idea and the utility of Pontryagin's maximum principle to address the challenge of high dimensionality of the state (features) is a good observation.Cons:- The manifold-based defense has been shown to be broken previously. For e.g. please see section 4 of https://arxiv.org/pdf/1712.09196.pdf . Manifold/GAN/VAE based defenses can be easily broken by just attacking the projection network and the original network together. The paper considers manifold based defense at all layers using an active controller doing the projection. While comparison with pixeldefense is useful, it would be good to launch an attack similar to the reference above and then observe the effectiveness of the approach. The arguments in the paper are not sufficient to convince the reviewer that this defense is practical. - The paper is severely lacking in comparison with existing defense approaches. The state of the art for the used dataset in the paper is significantly better than the effectiveness of the presented approach. For e.g. see https://github.com/MadryLab/robustness At this point, the presented control theoretic approach appears to be a technique deploying standard approaches from control theory to deep learning with an attempted application for adversarial robustness. In summary, the paper is a good initial effort to exploit the use of active controllers in deep learning. But firstly, the use of manifold-based projection as a cost function is itself a non-robust defense against adversarial examples. Second, the experimental evaluation in the paper is significantly lacking and does not meet the standards of a venue such as ICLR. The reviewer will strongly recommend reviewing the advices in https://arxiv.org/abs/1902.06705 on this topic. At this point, the paper is interesting but it needs significant development and is not yet ready for publication. Questions for the author:1. How critical is Pontryagins Maximum Principle to the presented approach? What prevents one from using projection to a lower dimensional embedding space followed by a state space control method? In particular, if one is building on the manifold assumption, then isn't it reasonable to not worry about high dimensionality for designing the controller too?2. Is it realistic to assume the "input perturbation to be a random vector" in Section 5 for theoretical analysis when we are considering adversarial attacks such as PGD, CW? If not, then isn't theorem 1 not relevant to the primary topic of the paper? This submission presents an extension of SkipRNN, Skip-Window, that splits input sequences into windows of length L from which only K samples can be used. This guarantees that the computational budget is never exceeded. Skip-Window implemented this inductive bias by predicting L updating probabilities in parallel at the beginning of each window. L needs to be set prior to training, whereas K can be modified at test time. The model is evaluated in two tasks, namely a synthetic adding task and human activity recognition. Authors report latency and energy consumption in small platforms, showing the impact of this research direction in real applications.My main concern regarding the proposed architecture is that it limits the types of skipping patterns that can be discovered -- whereas the original SkipRNN can in principle learn any such pattern. For instance, Skip-Window cannot produce the skipping patterns shown in Figure 6 in Campos et al. (2018) unless K=L or L=1 (in both cases, Skip-Window reduces to SkipRNN). This can even be seen in the results for the adding task, where Skip-Window is actually *not* solving the task for most values of K (c.f. Figure 4). Recall that the output distribution has a variance of 0.166, and Campos et al. define solving the task as achieving an MSE two orders of magnitude below such variance. The reason for this is that Skip-Window with 5<K<L will miss the second marker in some sequences, as it needs to guess its position -- which is random within the second half of the sequence. For K<5, theres a chance that Skip-Window will miss the first marker as well. As a sanity check, I suggest that authors report the percentage of first and second markers that are missed in the adding task as a function of K. Plotting the MSE of an LSTM or GRU that skips inputs randomly, and varying the fraction of skipped inputs as in Campos et al., would also provide context for the presented error rates.Despite the imposed constraints in terms of skipping patterns, Skip-Window can be useful in tasks where the input signal can be downsampled more uniformly. After all, the adding task is a challenging problem for RNNs that skip input samples as missing one of the markers will massively increase the error rate. This potential is shown in the human activity recognition (HAR) results. However, I believe that more experimental evaluation is needed before this paper can be published at ICLR. First, the input sequences for HAR are extremely short (32 timesteps), which makes it difficult to draw strong conclusions. Second, since the Skip-Window architecture limits the types of skipping patterns that the model can discover, it is difficult to claim that Skip-Window is a generic RNN architecture unless experiments on more domains are reported (e.g. sequential MNIST, NLP). This paper presents an approach to training recurrent neural networks with the mix objective of minimizing cross entropy (or something similar, which is not clearly defined) and minimizing minimum description length (MDL) by using a binary representation of the network which is optimized with a genetic algorithm. The model is evaluated in contrast to RNN, GRU and LSTM  baselines on modelling a number of formal languages.- (+) MDL is a powerful inductive bias which has been explored extensively in Machine Learning and Pattern Recognition, and it is interesting to see it revisited in the context of neural networks and in contrast to deep learning models.- (+) The paper exhibits numerous examples in which the presented system produces simple hypothesis to model (also relatively simple) data.- (-) Nonetheless, the studied tasks are good for a start, but insufficient to motivate the approach because actually neural networks can deal with them quite well [e.g., 1]. Moreover, the claims about the binary addition experiments are notably flawed. In particular, the task that is considered in this paper involves two bit sequences that are fed *bit by bit* in parallel to the system. This poses no serious challenge to no RNN, and in fact you can find example code directed at students that solves this task: https://github.com/mineshmathew/pyTorch_RNN_Examples. (I have personally tried this code on the same setup of using sequences of up to 20 bits and evaluating on a test set of up to 250 bits with 100% accuracy -- It only required updating a few lines to more modern versions of python/pytorch and changing the loss to BCE). The version of binary addition which is actually more challenging for RNNs is when the output is produced _after_ all operands are given [2]. Nonetheless, other neural network architectures deal with this task effectively [3]. I can see two directions in which this work could be improved moving forward:   - If the goal is to improve interpretability of models, the authors could aim at tackling problems in gradient-based neural network remain obscure (e.g. see https://blackboxnlp.github.io/)   - If on the other hand the goal is improving generalization, the authors could consider tackling tasks in which neural networks have been shown to be deficient in their generalization skills [2].- (-) Furthermore, RNN baselines might not be properly trained across the paper. First, as mentioned in the previous point, RNNs can in fact learn the binary addition task, and thus it is unclear why the authors report that they fail. Second, the authors report that the RNNs perform worse than chance on some other tasks, which could be explained by divergent training. Since no code was provided it is not possible to assess how the models were trained.- (-) The authors enumerate some works related to theirs, but they do not comment in which ways they are similar or different from theirs.  Also, given that whole books have been written on the MDL principle, the related work section could be considerably more detailed. The relation to neural architecture search (NAS) is missing too.- (-) It is also unclear how the training objective is quantified. It is only mentioned that |G : D| relates to cross-entropy, but no precise definition is given, nor it is detailed in which ways it differs from the former.**Questions for the authors**1) Do the addition RNN models reach 100% accuracy on the training data?2) The cross-entropy numbers are quite high for binary classification problems. Could you report how you computed them?3) For the test data you mention that you test the model on "an unseen sequence of length X". By "an unseen" you mean 1 sequence?4) Regarding footnote 7, why not properly quantifying the number of operations needed to train each type of model?**References**[1] Weiss, Gail, Yoav Goldberg, and Eran Yahav. "On the practical computational power of finite precision RNNs for language recognition." arXiv preprint arXiv:1805.04908 (2018).[2] Joulin, Armand, and Tomas Mikolov. "Inferring algorithmic patterns with stack-augmented recurrent nets." Advances in neural information processing systems. 2015.[3] Kaiser, Aukasz, and Ilya Sutskever. "Neural gpus learn algorithms." arXiv preprint arXiv:1511.08228 (2015).[4] Lake, Brenden, and Marco Baroni. "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks." International Conference on Machine Learning. PMLR, 2018. The paper proposes a technique to detect OOD data using disagreement among an ensemble of classifiers. It assumes a transductive setup where OOD data is available during training time.1. The application setting for OOD as stated in the paper is incorrect:Abstract: "For settings where the test data is available at training time.."Introduction: "..Most prior work approaches OOD detection inductively, trying to infer OOD samples after only observing the (ID) training data,.."The main property defining OOD data is that it is never seen during training. The problem fundamentally is that of detecting unknown-unknowns. This is what makes the problem hard and sets it apart. The proposed approach violates this and hence is not justified as an OOD detection algorithm. The paper text (including title) should be changed.The proposed technique should instead be positioned as one that helps in discriminating between two known distributions. In that respect, the baseline/competitor algorithms should be selected appropriately. The current choices (except Malalanobis-T) are biased towards the proposed algorithm.2. Section 2, ID data.: It is assumed for simplicity that a deterministic function f* maps ID data to correct labels. However, this assumption looses generalizability. Considering that real world data has a lot of labeling noise, most such functions as f* would end up memorizing the train data and would have bad performance on ID data in the test set. The paper is not clear what would happen if the train data is noisy.3. Section 2, Transductive OOD Detection: Here the assumption is that there is lot more correctly labeled train data than test data. The basis for this assumption is not clear and clearly limits the applicability. In most transductive cases, the opposite is true: there is more unlabeled data than labeled, and semi-supervised techniques are used to reduce human labeling effort.4. Section 2.2, Regularized ensembles: "..points in T for which at least two models in the..." -- Does that not make the model fragile? In case the ensemble has 20 members and only one member disagrees with 19 others, would it be justified to label the data as outlier/OOD?5. Section 2.2: "...our method indicates that using the regularized function class suffices for many hard OOD detection scenarios." -- This is an overstatement considering that results were presented on only a couple of datasets (CIFAR variants, SVHN)6. Section 3.2, Statistical test for OOD detection: "The null hypothesis is accepted for high values of Tavg-TV." -- I am missing something here: if there is a lot of disagreement among ensembles, then d_TV would be high and as a result for such data Tavg-TV would be also high. Hence, high value of Tavg-TV should probably result in rejecting of null hypothesis instead of accepting. This paper proposes a GAN-based method to remodel the privacy information of an MRI scan, i.e., the face. They first propose a probabilistic solution to construct a surface representation and then compute the convex hull of the head. The convex hull was used to instruct the GAN to generate a synthetic face. The main contribution of this paper is that they use GAN to generate a 3D volume MRI in which the face has been remodeled rather than removed. Experiment results showed the proposed method outperforms several existing removed-based methods.Cons:My main concern about this paper is the purpose of remodeling the face. In my opinion, removing the original face is enough for the de-identification of an MRI scan. Why spent so much effort to generate a new one? Is the new face provides extra information to the downstream medical analyses? The authors point out that their method did not alter any content of medically relevant data. Therefore, I think remodeling the fake face is unnecessary and the method may have little practical uses. The paper is well written, and seems to solve the proposed problem well. I am very confused however about the point of the problem you are trying to solve. From what I understand you want to take a voxelized version of an MRI scan and distort it such that the observed face is no longer identifiable with the original patient, but the useful information in the MRI is preserved. Part of your method requires or extracts the brain from the MRI. In either case you claim that that is information is available and sufficient for downstream tasks, and everything can be removed. Would a superior method here not just be to take this brain information directly.  The De-Identification Quality metric which you use would obviously get the optimal 20% accuracy here as the we have no way of associating images of brains with images of faces, and even if we do, you have not demonstrated that you method would perform well on it either. I see no point in altering the face of the scans as the face information will be useless for any downstream task, and all other information is apparently preserved, so why not just take this preserved data as the privacy preserving MRI information? While this may fail to perform well under SIENAX, I would say this is a problem with using this software as a metric, as the brain would clearly be preserved. If I am mistaken here, I apologies, and am happy to change by rating and comments given sufficient reasoning. However, from the knowledge I posses in the field I do not think the approach has merit due to superior, trivial baselines. I also have an issue with the experiment for assessing privacy preservation. I would say that the mechanical turk experiment is valuable, but only if you also demonstrate that deep learning based approaches cannot identify the associate the faces as well. Without this I have no way of knowing if your method fails to a simple CNN.  This paper examines the positive-unlabeled (PU) learning setting, and recommends the usage of the area under the lift curve, or AUL, as an unbiased estimate of the AUL under the fully labeled setting. This justifies proposing to use an AUL-optimization algorithm to train binary classifiers.Although I am not entirely sure this is fitting for a conference focused on representation learning, this is a question of interest in machine learning at large.I think this is an interesting proposition, supported by the experimental results. However, I do have a number of concerns that make me think this work is too premature and I lean towards rejection.1) I do not understand where the bias of $AUC^{PU}$ comes from in the equation below equation (3). Indeed, it is not obvious how it is derived from (3), and Jain et al. (2017) obtain $$   (\beta-\alpha) (AUC - \frac12) + \frac12. $$2) I disagree with the bound on $P(|AUL - AUL^{PU}| \geq \epsilon)$. Indeed, applying Chebychev's inequality, I think this should be upper bounded by $\frac{\text{Var}}{\epsilon^2}$ and not $\frac{\text{Var}}{\epsilon}$. Hence the final bound should be$$    P(|AUL - AUL^{PU}| \geq \epsilon) \leq \frac{1 - \beta}{4 n^L \epsilon^2}.$$This is unfortunate, because taking $\epsilon = 1\%$, and for example $\beta = 0.5$, $$   \frac{1-\beta}{4 \epsilon^2} = 1250,$$so the sentence "Hundreds of labeled samples can reduce the error to an acceptable level" does not hold any longer -- it's now "tens of thousands of samples".On the numerical experiments of Table 1, you can indeed check that for the Airfol data set, with $\beta = 0.1$, $$    \frac{1-\beta}{4 n^L \epsilon^2} = \frac{1-\beta}{4 \alpha \beta n \epsilon^2} \approx \frac{1.8 \times 10^{-3}}{\epsilon^2},$$so $P(|AUL - AUL^{PU}| \geq 10^{-2})$ can only be bounded by 18, and indeed $MAE_{AUL}=0.019$.3) Although some of the papers that are cited also use UCI data sets, I am wondering whether the values of $\alpha$ that are being tested are realistic. Indeed, it is my impression that PU settings occur mostly when the relative proportion of positive examples in the data is really small (fraud or outlier detection, malicious URL detection, drug discovery), which is why it makes sense to treat the unlabeled as negative, but here $\alpha$ is between 0.3 and 0.9.5) The AUL-optimization algorithm is an interesting proposal, and I think the paper would greatly benefit from having more details about this algorithm, even if it is "only" an adaptation of the algorithm of Sakai et al. (2018).6) On Figure 3, while for some of the data sets (Pageblock, Concrete) it is obvious that PU_AUL is performing much better than PU_AUC, it is not clear for all of them (in particular Landset, Anuran, Abalone, Airfoil) whether the difference in performance is significative. I would recommand plotting error bars on multiple cross-validation runs and/or computing statistical tests.In addition, I have several minor comments.1) The work is conducted under the assumption that labeled examples are selected completely at random among the positives and I think this should be mentioned explicitly in the abstract and Introduction. Indeed, this assumption may not hold in practice, for example in biological applications (the molecules or interactions labeled as positives are those that have been biologically investigated, and these investigations did not occur uniformly at random). 2) I don't think the proof that $\sigma^2$ is less than 1/4 is correct. The result is known as Popoviciu's inequality on variances, and if you want to prove it you should use $$   \mathbb{E}[(t - \bar{t})^2] \leq \mathbb{E}[(t - \bar{t})^2 - t(t-1)],$$which is correct because $t(t-1) < 0$. The equation that is given in the paper,$$   \mathbb{E}[(t - \bar{t})^2] \leq \mathbb{E}[(t - \bar{t})^2 - (0-\bar{t})(\bar{t}-1)],$$is not true because $(0-\bar{t})(\bar{t}-1) > 0$.3) There are a number of typos in the text. "online advertise", "Select Completely at Random", etc.The definition of fpr in Section 2, paragraph "ROC", should be $$   \frac{n^{FP}}{n^N}.$$4) In Section 4.2, I find it odd to compare $|AUC^{est}-AUC|$ to $|AUL^{PU}-AUL|$, because $AUC$ and $AUL$ don't take the same values. I think these two quantities should be normalized by $AUC$ and $AUL$ respectively, reporting $|AUC^{est}-AUC|/AUC$ and $|AUL^{PU}-AUL|/AUL.$ The same holds, of course, for $AUC^{KM1}$ and $AUC^{KM2}$. I note that it does not seem to change the conclusions that can be drawn from Table 1.5) In biological and drug discovery applications, it has become rather mainstream to evaluate the performance of PU-learning algorithms using the cumulative distribution function of the ranking of positive samples among all test samples in a leave-one-out setting as in Mordelet and Vert (2011). I think it would be warranted to discuss this method as well.Another relevant reference seems to be Jiang et al. (2020).ReferencesMordelet and Vert, BMC Bioinformatics 2011, 12:389 http://www.biomedcentral.com/1471-2105/12/389Liwei Jiang, Dan Li, Qisheng Wang, Shuai Wang, Songtao Wang. Improving Positive Unlabeled Learning: Practical AUL Estimation and New Training Method for Extremely Imbalanced Data Sets.  https://arxiv.org/abs/2004.09820 The paper proposes to use texturization of the periphery in images as a proprocessing step for scene classification and claims "greater iid generalization, high spatial frequency sensitivity and robustness to occlusion" for such models. The model is essentially a concatenation of the synthesis procedure for metameric images by Deza and colleagues (2019) followed by AlexNet as a classification head.Strengths:+ Tries to address an interesting question: why does human vision treat the periphery mainly as texture?+ In-depth investigation of the model and three controlsWeaknesses:- Unclear if the premise of the paper is well-conceived- No evidence provided that "foveated" images are actually metameric- "Matched-resource" controls are somewhat questionable- Definition of "foveation" seems strangeDetails on major issues (all of which would need to be addressed in a convincing manner for my score to change):My main issue with the paper is that I don't understand its logic. The results by Rosenholtz, Simoncelli, Wallis and others show that some information about the image is discarded in human peripheral vision and, to some extent, peripheral vision "cares" only about the texture statistics of an image rather than the detailed composition. The hypothesis is that this is the same effect that has been described as crowding.Now, the paper seems to put forward the hypothesis that distorting (texturizing) images in a way that they remain indistinguishable (metameric) for humans somehow enhances their discriminability. I don't understand what's the reasoning behind this logic. Why would throwing away information and distorting an image be useful? The original image is by definition part of the equivalence class of metameric, so why should a distorted image somehow be easier to classify by a neural network than the original image? Both images are perceived in the same way by humans. It seems like a strange proposal to me, but it's possible that I'm missing something. I'd like the authors to explain their reasoning better.Having said that, even if we put these fundamental concerns aside, there are a number of practical issues:(1) The presented images don't appear metameric to me, and the paper does not provide any psychophysical data showing that they are. For instance, for the three images labeled "Metameric" in Fig. 5A, I have a very hard time believing that they are metameric to human observers. Could the authors provide psychophysical evidence that this is the case?(2) I am not sure about the purpose of the "matched-resource" control and why SSIM is the right metric to use here. If it's about information content in the image, I think it should be evaluated in terms of mutual information between the image and the labels (which is hard); if it's about information available to human observers, then a perceptual metric is the right approach, but SSIM is a very poor one that doesn't correspond well with human perception. Therefore, I am skeptical that these baselines are really useful.(3) The claim greater iid generalization, high spatial frequency sensitivity and robustness to occlusion emerged exclusively in our foveated texture-based models seems to be an overstatement. There appears to be basically no difference between foveated and standard net. In all Figures (58) the differences are mostly within the range of 1 SD and the order of the two models changes sometimes. The only effect that might be significant is the difference in the scotoma condition for intermediate to high occlusion conditions. The only robust difference is between standard+foveation and MatchedNet+AdaGauss, where the latter seem to carry much less information (see points (1) and (2) above).Minor comments- Sec 3.4: The connection between Gaussian blur and scale invariance is not clear- Sec 3.4: The following statement is trivial, since the images were low-pass filtered: We found that Foveation-Nets and Standard-Nets were more sensitive to High Pass Frequency information, while Ada-Gauss-Nets and Matched-Nets were sensitive to Low Pass Frequency stimuli- Fig 8: It is unclear how it argues for shape bias. The paper presents a study on regularization methods for the feedforward fully connected neural networks.The study is formulated as hyper-parameter optimization task, heavily using Auto-Pytorch library. The paper claims as contribution (sorry for copy-pasting):1. We demonstrate the empirical accuracy gains of regularization cocktails in a systematicmanner via a large-scale experimental study;2. We challenge the status-quo practices of designing universal dataset-agnostic regularizers,by showing that an optimal regularization cocktail is highly dataset-dependent;3. We demonstrate that regularization cocktails achieve a higher gain on smaller datasets;4. As an overarching contribution, this paper provides previously-lacking in-depth empiri-cal evidence to better understand the importance of combining different mechanisms forregularization, one of the most fundamental concepts in machine learning.***I am highly sceptical on the paper usefulness for the community. In general terms, the benchmark/empirical study type of paper typically can have one (or more) of the following contributions:a) New knowledge, which was obtained as a result of a study. E.g. surprising results, practical recommendations, so on. For example, A Metric Learning Reality Check by Musgrave et. al (ECCV 2020) revealed surprising knowledge about metric learning methods. b) The methodology of such study, which was not used before. E.g. Visual Object Tracking Challenge, which become the benchmark for the tracking methods since 2013. c) The software and/or dataset, which were developed for the study. E.g. OpenAI Gym.(a) is not the case IMO, because all the recommendations are known to the practicioners, e.g. check the any Kaggle winning solutionhttps://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions(b) I see no novelty in using hyper-parameter optimization for the study. The paper agrees with me on this  (see Related Work, "Positioning in the realm of AutoML"(c) Neither software, nor dataset is proposed -- the paper uses existing ones. ****Now I will go over contributions.1. It is well known that the regularization/augmentation/... need to be tuned to archieve the best results. One can publish a CVPR paper about such good combination, e.g. He et.al. (CVPR2019), Bag of Tricks for Image Classification with Convolutional Neural Networks2. I don't see the support for that claim in the paper. Yes, the specific combination of regularization techniques, which performs the best on the given dataset is, perhaps, unique. But the techniques are applicable broadly, which is supported by the paper (Fig. 1), e.g. DropOut, MixUp, BatchNorm, are pretty universal.3. It is also obvious, that the less data you have, more regularization and design bias are needed for better results, see OpenAI Image GPT, or more ViT paper vs. ConvNets. 4. See (1)Overall, if the paper spend some space on particually interesting regularization combinations/interplay of components/analisys, it might be quite useful for researchers. For now it seems as a lot of experiments were done, but no analisys is really performed.E.g. abstract says: "there is no systematic study as to how different regularizers should be combined into the best "cocktail"". But I don't see the answer of how should they be combined.********Small things, not contributing to the score:- AutoPyTorch is cited twice, as arXiv and as CoRR- While skip-connections and BN can be seen as "regularization", I would rather call them "architecture". Anyway, that is just matter of naming.  Summary:This paper provides an empirical study of combining different regularizers. Fourteen regularizers including batch norm, weight decay, etc. are considered. The authors use BOHB (Falkner et al. 2018) to optimize for whether each regularizer is active, and additional regularizer-specific hyperparameters. Using 40 tabular datasets, they show mixtures nearly always outperform tuning a single regularizer, and that the benefits of regularization improves for smaller datasets.Strengths:- To my knowledge, this is the first paper that does an empirical study of combining regularizers. - The list of regularizers considered are extensive.- The paper is very well written and easy to follow. The figures are nice and easy to understand.Weaknesses:I think the biggest weakness of this work is that it is not very useful practically. - All experiments are run on tabular data. I think its fair to say that most practitioners who deal with regularization are interested in non tabular data, given the fact that the regularization methods were individually developed for training on non-tabular data. I find it hard to imagine extrapolating results on tabular data to images, or text.- The paper frames itself as a methods paper more than an analysis paper, where the main claims revolve around the superiority of the regularization cocktail. In fact, the take-home messages given in the conclusion explicitly recommends the use of the regularization cocktail. I find this advice not very useful because: 1) the experiments were run on tabular data, 2) regularization cocktail is too expensive to justify the improvement (if any).- The conclusions are trivial. Its quite obvious that a more general method always does at least as good as the method it subsumes, as long as the tuning of the parameters can be done sufficiently. The fact that the regularization cocktail does better than individual methods, or a combination of a few, is very believable in the tabular setting, since the tuning can be done sufficiently. The fact that more regularization is needed for smaller datasets is also well-known. The paper would be more useful with a similar experimental protocol applied to a non-tabular dataset (CIFAR-10, Fashion MNIST, SVHN, etc), but with a focus on analyzing trends (rather than highlighting the mixture method), or comparing with SOTA human designed cocktails. I recommend reject because of the lack of practicality of the paper.Comments:- For the experiments corresponding to Figure 5, I wonder what the gap would be like for the best performing individual regularizer. I am curious because I think for a small dataset, the need for a state of the art regularizer diminishes, and just one good regularizer suffices. This paper proposes a numerical method to approximate 2d Wasserstein barycenters, that relies on a convolutional network. Namely, the proposed architecture is composed of1.Contractive paths, made of series of 2 convolutional layers, relu activation and a pooling step, characterized by weight sharing.2.An expansive path, symmetrically constructed, with upsampling by NN interpolation, and a final softmax activation.Though fixed during training, the varying number of contracting paths in the test setting is able to predict the barycenter of a varying number of measures. The KL loss is used to train the network predicting the Wasserstein barycenter.The method is assessed on the Quick-draw dataset, to produce barycenters of two measures and its generalization to barycenters of n measures.Strong points of the paper include:1.The writing of the paper is clear.2.The goal to propose a speedup for the computation of Wasserstein barycenters with a varying number of input measures is pertinent.Weak points include:1.The paper proposes an architecture tailored for very specific setting of 2d images. Its extensions to higher dimensional or non-image settings are limited and are not discussed.2.Though mentioned as closely related works, the paper is missing important baselines, for instance (Claici et al.,2018).3.The fact that only one dataset is considered undermines the paper, which focuses on numerics.4.The figure at the end of page 6 (which is lacking both title and legend) does not provide a relevant metric to assess the benefits of the proposed method. 5.The paper does not provide other metrics (such as inception scores) to assess the performance of the methods other than visually.I recommend a reject on the aforementioned grounds. This paper aims to study how GCN will behave under spectral perturbations/manipulations. The empirical numerical analysis on three benchmark datasets (cora, citeseer, pubmed) show that most of the necessary information is contained in the low-frequency domain. Based on that, the author propose to expand the node feature matrix with the eigenvectors corresponding to low-frequency domain and apply MLP on this new feature matrix. Experimental results show that the proposed method outperforms vanilla GCN and achieve comparable results on pubmed with other baselines.I think the paper has a pretty good start point: understanding how the spectrum of adjacency matrix will affect the behavior of GCN. But I think the manuscript is loosely written and I can hardly follow it. I do have a lot of confusion about this paper and hope that the authors can clarify them.- In general, there are too many typos and grammar errors which make the manuscript hard to read.- Sec 1: in the 1st paragraph, what does 'graph principles' mean? I cannot recall a clear definition of this term. I think the authors could clarify it before using it formally.- Other contributions in Sec 1:  * (a), I think the empirical results in the paper show that retaining a small portion of low-frequencies is enough for achieving good classification accuracy. From the results and your analysis, I can hardly find a clear conclusion that links to it (i.e., the very first eigenvector is most informative);   * (b) and (e) are somehow connected since both are mentioning about GCN's behavior over manipulating high frequencies;   * (c), I actually did not quite understand where the clear link is. I would suggest the authors clarify it more clearly in the manuscript;  * Just a minor thing, what exactly does 'informative' mean, greatest change in loss function or greatest change in test accuracy? Better make it clear.- Sec 3:  * Notations are quite messy. A matrix can be denoted using italic lowercase letter, italic uppercase letter, calligraphic uppercase letter, bold italic uppercase letter. It is very confusing when reading the paper, especially when some scalars are also denoted using the same convention.  * I did not find any framework in this section, which contradicts the last paragraph in Sec 1.- Sec 4:  * Pubmed should have only 1 connected components. In the following sentence, what does 'not necessarily fully supported on ...' mean?   * In Sec 4.1, is there any intuition or theoretical justification for the projection operation used in band-pass filter?  * In Figure 2, it seems that Cora has a very sharp performance drop (~0.82 -> ~0.5) when x-axis is nonzero, is there any insightful explanation on that?  * Does the content of 'MLP ablation' correspond to your proposed method? This is the only place I can find clues about your method. I understand that the goal of this paper is show a MLP can perform better or comparable with GCN that perform message passing. But I believe you still need topology information to get those eigenvectors. Is there any insight or theoretical concern about simple concatenation instead of linear transformations like $e^T e x$? This paper presents a set of numeric normalization features from the Automunge open-source python library, and evaluates them in two machine learners (one neural, one SVM) on a data set.  In general the paper discusses standard normalizers rather than introducing novel techniques, and the experiments are too limited to evaluate the effectiveness of the approaches.I did not understand the family tree primitives, unfortunately.  I think I need a concrete example.  What do upstream and downstream mean in this context?  What is a generation?  Why are the primitive names (the familial relations) appropriate to describe each row in Table 3?The experiments also do not show that the transformations described in the paper offer benefits for neural models.  In fact, on the one data set and one neural architecture evaluated, using the best normalizations results in essentially the same performance as using raw unnormalized data.  Even if the experimental results had been positive, the limited scope of the investigation would make it impossible to draw general conclusions.  A substantially expanded set of experiments that looked at more neural architectures and many more data sets might give practitioners more useful guidance, especially if it led to concrete recommendations about which normalizations to try for which type of data.Minor:classical or quantum computation may require non-negative feature values?  Why mention quantum here?Tables 8 -> Table 8The author refers to Automunge as our library at one point, breaking anonymity. Summary:The paper describes a library (Automunge) for pre-processing tabular data to prepare the data for downstream machine learning tasks. The paper also describes how to use the said library and the various options (including some new forms of normalization) available in the library. Experimental evaluation on Higgs Boson interaction dataset is provided, following the experimentation in Baldi, Sadowski and Whiteson 2014. Some improvements over the published results in Baldi 2014 are shown. Key strengths:The paper is well written for the most part, apart from a few typos. The appendix provides examples of usage of the proposed library. The supplementary material contained the code and an extensive readme for the code. Suggestions for improvements:1. The primary novelty of the library is to ease the burden of tabular data pre-processing. However, it seems that the user has to specify the configuration for the pre-processing unless the default options are acceptable. For most machine learning tasks, the choice of pre-processing can be heavily influenced by the context and the domain expertise. It is unclear how a library can automatically derive these options correctly, without the knowledge of the downstream machine learning task (or indeed the domain). 2. The paper provides evaluation on one dataset (Higgs Boson interactions) to support the claim that the library is of value for many machine learning tasks. The setup of the experiment is slightly different from the original paper (Baldi 2014): i.e. learning rate schedule is different, but network setup is the same. Looking at the results in the original paper (Table 3 of Supp material in Baldi 2014), the results presented in this paper are in the same range as the original paper (but also within the variance of results in the original paper), so it's difficult to extrapolate how significant the improvement due to retain normalization (this paper) is, and how it might generalize to other machine learning tasks, or even other datasets. Also, some results improve upon the original paper, whereas others don't, which makes it harder to say how statistically significant the effect of the pre-processing presented in this paper is. 3. Retain normalization: The paper claims that retain normalization is better because it retains the sign of the original datapoints, but it's unclear why this is important if it doesn't affect the eventual result. Perhaps this could be clarified? 4. Software review: The entire library is written in a single Python file. The naming convention is not Pythonic. From the usage examples and reviewing the code, it's clear that the library makes little use of encapsulation of data structures, leading to unwieldy usage. All documentation is provided as a single Readme, rather than as Sphinx documentation or API documentation. There don't appear to be any tests or continuous integration included. The code is not modularised. There are no docstrings with param/return descriptions or type annotations. Overall comments:For what is mainly a description of an automated data pre-processing library, the usage of the library is not sufficiently automatic (i.e. setting up the options amounts to feature engineering, which the paper claims to circumvent). The new normalization technique presented in the paper does not yet demonstrate a clear improvement over current options.  This paper proposed an adversarial training framework for reducing the predictive ability of the new outlet from the news recommendation system. While deep learning-based news recommendation systems work very well, these systems tend to recommend contents of the same site, and are likely to develop extremist tendencies.First, this paper contains many typos and flaws, e.g., the missing references in Section 2, the fluctuation of reference format style, and the different AUC-ROC metric values for the end-to-end setting in the text and table. These flaws make this paper hard to be read. If this paper will be published, I strongly recommend that authors fix these flaws.Second, while this paper starts with a good motivation, I was not sure that preventing the same outlet from adversarial learning procedures. If we want to recommend news from a large variety of sites, the recommendation service may be able to limit the maximum number of news articles from the same site. Therefore, I was not sure the decrease of the AUC-ROC can be compensated by the diversity increase of sites only. It is better if authors can provide evidence where the proposed adversarial settings can actually prevent filter bubbles, for example, by recommending different opinions than before. The authors use adversarial training in an attempt to diversify recommendations of news articles. The effect on filter bubbles is not clear.The authors address the issue of filter bubbles that according to their rationale result from recommendation engines for news articles. Their assumption is, that a news outlet constantly reports the same opinion and thus users should be recommended articles from a more diverse set of outlets to increase the diversity of opinions consumed by an user. To achieve that, the authors propose adversarial training to generate feature representations that encode outlet-specific information less. They evaluate if their obtained representation is less predictive of the outlet and also how this affects the recommendation performance.A strong point of this paper is the motivation and application area since it tackles a problem with direct societal impact, something that is typically not attempted in general machine learning papers. The technical novelty is minimal, since only well established techniques are applied. No technical novelty would be acceptable if some insights for the application area are generated. Unfortunately, several naive assumptions are being made and the evaluation goals are insufficient to obtain any valuable results:1. Outlets are not necessarily biased to one-sided opinions.2. Forcing recommendations from a variety of outlets can be achieved much simpler. No reasonable baseline is implemented and evaluated.3. The assumption that content from different sources has higher diversity in embedding space is not necessarily true. Different outlets commonly publish identical texts (for instance copied from the same press agencies).4. Embedding similarity is not equal to semantic similarity. Bert embeddings capture also syntactic characteristics, which does not necessarily translate to opinions.6. The data used to measure the recommendation performance is likely biased towards similar outlets. Thus, a performance drop is obvious.While I really like the general application area and the attempt to tackle such relevant issues, I think the paper is not ready. It needs to be clearer about it's goals, limitations and assumptions. The experimental design needs to be redone to allow to gain any valuable insights. I assume that also a manual analysis is required to confirm any of the hypothesis underlying this research. # Paper SummaryThis paper addresses the problem of estimating the distribution parameters of features extracted from a set of high dimensional observations, a problem that is common in the physical sciences. To solve this problem, the authors present a deep learning approach that utilises a combination of (i) deep ensemble training, (ii) post hoc model selection, and (iii) importance weighted parameter estimation. First, a deep ensemble is trained to solve a regression task (observation -> feature). During testing, this ensemble is frozen and used to generate feature samples from unseen observations. Using these feature samples, it is possible to estimate the distribution parameters using maximum likelihood estimation. The authors evaluate their method on X-ray polarimetry, and compare it with two other approaches, one of which is also a deep learning approach. On all tasks, the presented method outperforms both baseline approaches.# Assessment SummaryThis paper presents a flexible, data agnostic, and easy to implement approach to *parametric density estimation*. The paper is clearly written, giving a good understanding of the different components of the approach. I would feel confident to implement this approach myself.However, I believe that the presented methodology, in particular steps (ii) and (iii) achieve the opposite of the authors intentions (see negatives below). Further, the use of an ensemble model is not properly motivated. The experimental section does not give justification to each of the model components. I therefor cannot support the acceptance of this paper.For a future manuscript, I recommend the authors to add an ablation study (see below).# Positives- Flexible approach: The approach is separated into two stages: (1) Training a deep ensemble on a regression task, and (2) maximum likelihood estimation of distribution parameters under the ensemble and an unseen test set. This two stage process makes the approach very flexible. The ensemble is trained once on a data set, and then exploited on multiple test sets - even with different likelihoods - for ML estimation.- Data agnostic: The approach does not make any further assumptions about the data, apart from the fact that the ensemble can be trained in a regression task.- Easy to implement: The presented approach chains a number of simple components together (deep ensembles, sample reweighting, ML estimation). Each of these components can be found or easily implemented in most common learning frameworks. Advancements in each of these components can have a trickle down effect on this approach.- The paper is clearly written.# Negatives- Step (ii) of the approach is used to select a sub-set of models from the ensemble that was trained in step (i). Step 3 & 4 in algorithm 1 gives a hint of how this selection is implemented: On a new data set the features $y$ are estimated from each neural network of the ensemble. The density parameters are then fit to approximate the distribution of features from each network under a given likelihood (I believe it should say maximize instead of minimized in algorithm 1: 3). The models for which the parameters can be best fit will be selected for step (iii). The authors claim that this removes the models with highest bias from the ensemble. However, at least in the way I understood it, this approach will in fact select the most biased models. Imagine a network that always output the same feature values. It will be easy to fit these values using the density parameters.- Step (iii) will give highest importance to the most confident model prediction, disregarding whether that prediction is correct or not (we cannot know during test time). A confident but wrong predictor can therefore dominate the loss for a given sample, essentially eliminating the benefit of the ensemble.- The experimental evaluation does not give any insights into which components of the approach actually help in performance. A proper ablation study should be carried out (see recommendations below)# Minor Comments- Figure 2 does not follow the formatting guidelines. Figure and table should be separated.- Wrong citation command (citet <-> citep)  - Section 2.2, 2nd paragraph  - Section 2.2, 4th paragraph  - Section 2.3, 2nd paragraph  - Section 2.3, 4th paragraph  - Section 2.3, 5th paragraph  - Section 2.4, 2nd paragraph  - Section 3.1, 1st paragraph  - Section 3.1, 5th paragraph  - Section 3.4, 1st paragraph  - Section 4, 1st paragraph # Recommmendations- Ablation study: The authors present 3 components to their approach: (1) A deep ensemble, (2) model selection, and (3) importance weighting. Whether each of these components is necessary to achieve the performance as presented in the results section is not clear. To test that, the authors should carry out an ablation study by varying the size of the ensemble, varying the number of M top performing models, and by adding or removing the sample reweighting from step (iii). This paper proposes a sampling free technique based on variance propagation to model predictive distributions of deep learning models. Estimating uncertainty of deep learning models is an important line of research for understanding the reliability of predictions and ensuring robustness to out-of-distribution data. Results are shown using synthetic data, perplexity analysis for a language modeling task and out-of-distribution detection performance using a convolutional network.Overall I vote for rejecting the paper. The paper proposes an upper bound to the variance estimate of predictive distributions. However, the paper does not explain how the upper bound can be ensured. Furthermore, the experiments based on real data are conducted not using the upper bound approach but assuming strong independence assumptions (\rho = 0). In my opinion, the independence assumption needs extensive experiments to validate its performance under different scenarios. Furthermore, for further adoption of the upper bound approach, I think the authors need to provide extensive experiments to showcase the tradeoffs. For instance, as a reader of the paper I would like to get a sense of how much I will be overestimating the variance under typical scenarios. For this, I would recommend a formal analysis of the uncertainty estimates by inspecting the confidence intervals through coverage properties. My specific comments related to this approach:-The authors propose two approaches in Section 3.1 for estimating the variance in predictions and the remaining subsections in Section 3 build on this. oThe first approach (a) assumes independence in the input. In my opinion, this is a very strong assumption. For instance, multi-collinearity in features in DNNs is a very common usage pattern. In domains like images, by construction of the problem, you expect a spatial correlation structure. Furthermore, the multi-layer perceptron architecture by construction is susceptible to correlation among variables. I would highly recommend adding a discussion on the validity independence assumption in general.oThe second approach (b) will be necessarily true if `\rho >= \max_{j,j:j!=j} \rho_{j,j}`. Only then this will guarantee that you will obtain an upper bound to the variance but it is very likely that you will overestimate the variance since your estimate is going to be bound by the highest correlation in your system. Again, in my opinion, this is a significant limitation of this approach and I recommend that authors highlight these points. Another minor thing to note in the manuscript is that when \rho=0, option (b) reduces to option (a).oI would suggest adding another summation term in Var(y_i) related equations to denote the double summation (indexed by j) happening over the covariance terms (in Section 3.1).oRelated to the above point, I could not follow the mathematical derivation from line (1) to line (2) in Equation (1). Could the authors provide an explicit derivation to ensure that the derivation is correct?oFurthermore, authors claim that distribution of y_i is well approximated by the univariate Gaussian distribution if the correlation among x is small (Section 3.1) needs justification. To my knowledge, Wang & Manning (2013)s Gaussian approximation holds due to central limit theorem as the number of samples approaches infinity but I do not think this will be applicable in this sample-free setting.-Regarding the results:oThe results shown in Figure 1 demonstrates that choosing an appropriate `\rho can be challenging. We see underestimation of uncertainty with \rho <=0.15 and overestimation with \rho = 1. However, since these are based on synthetic data, I would suggest that the authors formally assess the fit using metrics like confidence interval widths and coverage.oThe authors note that Estimation of Á is possible by observing the outputs of middle layers several times under the approximate predictive distribution.. I am not convinced that this is an easy problem, I would recommend that the authors provide an example and elaborate this in depth.Minor comments:-Please indicate the best results in the tables by highlighting them. -The uncertainty in deep learning literature typically employs distinction between aleatory and epistemic uncertainties. I think the manuscript can benefit how this proposed approach maps to the different sources of uncertainties.  ## Detailed Comments- "There are problems with supervised learning and machine learning in general." - This statement is so general, it is essentially vacuous.- "machine learning requires huge amounts of data" - unclear what this means. and whatever it means it's not true in general.- "severe labeling issues were found" - what issues?- "we provide the an overview"- "(AL)," - missing space. this happens at several places in the text- "continue iterative" - iteratively- "other stopping criteria" -> "criterion". also, why "other"? there was no stopping criterion mentioned so far. - "If a learner does not choose his strategy" -> their strategy- eq at bottom of page 2: what is "u"? This paper aims to evaluate the performance of seven automated labeling algorithms in terms of accuracy. The authors conducted a set of experiments on six datasets from different domains under two typical settings where 10% and 50%of labels in the datasets are available. Experimental results show that the algorithms label spreading with KNN perform better in the aggregated results, the active learning algorithms  QBC and query instance uncertainty sample perform better when 10% of labels available.Overall, this paper cannot meet the high-quality requirements of ICLR.  First, active learning algorithms such as QBC and uncertainty sampling is not automated labeling algorithms. They are only strategies for the selection of unlabeled instances. The selected instance either can be labeled by human experts or automated labeling algorithms. Second, when evaluating an automated labeling algorithm, merely using accuracy is not enough. For example, when the underlying class distributions are imbalanced, the accuracy is not sufficient to characterize the generalization performance of a learning algorithm. Third, two settings of 10% and 50% of labels available are also insufficient. Many papers of the empirical study investigated the performance under more complicated settings. Finally, the number of investigated methods is two small and the paper should cover more state-of-the-art algorithms. **Summary of paper**The authors introduce an algorithm called VBSW to re-weight a training data set in order to improve generalization. In summary, VBSW sets the weight of each example to be the sample variance of the labels of its k nearest neighbors. The nearest neighbors are chosen in the embedding space from the second-to-last layer of a pre-trained neural network. The last layer of the pre-trained model is then trained with these new weights.This approach is quite simple in practice and seems to be theoretically justified.The authors demonstrate that VBSW achieves better test accuracy than not using VBSW on 3 toy datasets and 5 real-world datasets.**Conclusions**Quality: The authors did not experimentally compare VBSW to any alternative algorithms. I find this omission inexcusable. The problem of re-weighting examples to achieve better accuracy has been studied for decades; there are many other algorithms to compare against.Clarity: The paper is generally well-structured and well-written, although with a few typos and grammatical errors.Originality: I am not familiar enough with the related work to say whether this idea is novel. However, seems quite simple and potentially very similar to existing published techniques.**Comments**Section 3.1 seems to assume that the labels have no noise. For example, if two examples have the same input features, their labels seem to be generated by the same function f, which would always produce the same label for both examples. This seems unrealistic.Section 2 describes the author's VBSW algorithm as being applied "prior to the training", but the algorithm actually requires a neural network to be pre-trained before the reweighting procedure. I felt the beginning of the paper was misleading in this regard.**Minor comments**Table 1: The difference between the mean accuracies of VBSW vs. basline on the BC dataset seems statistically insignificant; I believe VBSW should not be bolded. This paper deals with the interesting question of how to simulate 3D shape of trees: given sets of 2D input images taken from drone cameras, the process consists of building point clouds, and textured triangle meshes, and creating a skinned cylindrical articulated rigid-body 3D shape. It is an important problem in artificial life, and has been investigated in the past, by e.g. L-system. In this work, a systematic pipeline is proposed and reasonable 3D shape result is demonstrated. I also enjoy the demo video and appreciate the amount of efforts devoted to annotate the tree data manually.One the other hand, there are two main issues that prevent it from being considered in this conference venue. The first concerns the sparse result. With only one 3D shape being presented, it is difficult to understand the capacity of the proposed method; second, no or very weak machine learning and related technical contribution exist in the paper. Most of the text is devoted to 3D graphics and computer vision pipelines. Overall it seems unfit to ICLR audience.  There are a few other comments that I hope would be useful for the authors:1. lack of technical contributions. Currently every module in the pipeline is off-the-shelf. The authors are encouraged to conconsider developing a new learning based method to tackle (maybe part of) the problem. The could dramatically change the feel of the current work. 2. more empirical studies. The current dataset seems to consist of only one single tree. It would be much more convincing to show multiple trees acquired at very different location/season/species, etc. The paper succeeds in developing diversity metrics that correlate better with ensemble accuracy than the original diversity metrics. However, this makes one wonder why one cannot just use ensemble accuracy directly. One can also use a combining scheme along the lines of (Freund, 1995) where it adds models that focus on the examples that will increase accuracy and allowing errors on examples where most of the models so far have either classified the examples correctly already or incorrectly (where there is no hope of recovery and so effort is not worthwhile). Additionally, the appendix has the algorithms and other substantive content that is central to the paper, which is not supposed to be the case.Here are additional comments.1. Bagging does not necessarily (or typically) aim to train weak models to form a strong ensemble. In fact, by default, it wants the base models to be much more accurate than random, but with large variance among them.2. The Random Forest reference is incorrect. Please use Breiman's Machine Learning Journal paper from 2001 as the reference since that is the original one.3. In appendix B, note that $\delta = 0$ does not mean that the base models are negatively correlated, but rather they are uncorrelated. For negative correlation, ideally, $\delta = \frac{-1}{S-1}$ since that would lead to $E_{avg}^{add} = 0$. The major issue with this paper is that the definitions of the new diversity measures HQ are not properly laid out, and an explanation/intuition why they would correlate with accuracy is not provided. I understand that the algorithm is provided in the Appendix, but it comes with barely any explanation still. The explanations and motivation in 2.2. are vague, contain undefined terms and concepts, and they are completely unclear. The idea of focal model (which seems to be crucial) is never explained. Furthermore, it's unclear why this paper focuses on deep learning models. Do the proposed diversity measures work only for deep learning models? Why? Would they work also on other models, and regardless of the models used for the ensemble components? What is about these measures that make them correlate with accuracy?- The authors should clarify early on in the paper that the focus of this work is on ensembles for supervised learning. Work has been done also on selective ensemble techniques for unsupervised learning (e.g. clustering or anomaly detection), but this is not the scope of this work.- I find the discussion in Section 2 quite lengthy and trivial. It's well known that enumerating all subsets of an ensemble of a given size is unfeasible. I would shorten it to leave space to core explanations.- Results of Table 1: the authors state that the mean threshold is computed on 1013 candidate deep ensembles. Which deep models were used? The authors need also to clarify that M=10 (this becomes clear only later) in this experiment, and how the individual subsets were generated (randomly?). State upfront what's the purpose of this experiment.- Figure 1: I don't think (c) is showing much of "a sharper trend in terms of the relationship between ensemble diversity and ensemble accuracy" as claimed by the authors. For a given diversity value below the threshold, there is still a range of accuracies being obtained by the ensembles with that diversity level. So I am not sure what's the added value of plot (c). I understand the authors are trying to motivate their choice of comparing ensembles of the same size, but this plot does not serve the purpose well.- When introducing the concept of focal model in 2.2: "..., we introduce the concept of focal model to obtain the set of negative samples for computing the diversity scores of ensembles by taking each member model in turn as the focal model." This statement is totally obscure to me. What are the negative samples? How is a focal model selected. The explanations in this paragraph are not useful. - The HQ (alpha) method does not work that well. It's comparable to standard diversity measures on CIFAR-10, and worse than standard diversity measures on Cora. This raises the question whether it's basically the extra step of clustering that gives an advantage to HQ (alpha + K). The paper proposes a method to identify and correct regions on the data manifold in which a trained classifier fails. The *identification* phase is based on clustering classification failure regions in a GAN latent space and the *correction* phase is based on fine-tuning the classifier with additional synthetic samples from the GAN.The proposed method is strongly based on Zhao et al 2018 (Generating Natural Adversarial Examples), a method to generate on-manifold black-box adversarial examples using a GAN. The authors of the current paper describe some differences of their identification step from Zhao et al (end of section 3.2.1), but in my opinion they are minor.The main contribution of the current paper over Zhao et al seems to be clustering the adversarial examples (using GMM) and using them to fine-tune the classifier. This, in my opinion, is potentially an interesting idea, however, the authors do not show sufficient evidence of its success. Specifically, the authors claim to "achieve near perfect failure scenario accuracy with minimal change in test set accuracy", but they do not provide any details (e.g. table of accuracy values on the train, test and adversarial sets before and after the fine-tuning). I would also expect to see an ablation study comparing the proposed method to simply including the adversarial examples found using Zhao et al (w/o GMM fitting and sampling) as additional training example - a standard adversarial defense approach (see e.g. [1]).Perhaps more importantly, the objective of the proposed method is not, in my opinion, clear. The title and abstract describe the goal as "debugging" a classifier and correcting fail regions, however the described method seems like a defense against on-manifold adversarial attack. If the method, as claimed, helps debugging and correcting the classifier, I would expect to see an improved accuracy on the (natural) unseen test set - not just on the synthetically generated adversarial examples.The quality and clarity of the writing can be improved as well. A lot of space is allocated to describing well-known methods (e.g. VAE, GMM), however, critical information about the experimental results are missing. I'm also not sure all the formally defined algorithms and equations actually help in the understanding (e.g. algorithm 1, equation 2). Some of the mathematical notations are not standard.Minor comment: The norm in definition 3.1 is a regular vector norm (l2?) and not a matrix norm.To summarize:pros:- interesting idea (clustering on-manifold failures, labeling them and then using them to improve the classifier)cons:- contribution over Zhao et al not well established- insufficient and inaccurate experimental results- general quality of writing- not sure actual work and experiments match the stated objective- significance[1] Zhang, Hongyang, et al. "Theoretically principled trade-off between robustness and accuracy." ICML 2019 This work proposes an AutoML framework for multivariate irregularly sampled time series. To achieve this, the proposed framework integrates different modules: data-augmentation self-supervised loss (Equation 7), an anomaly detection loss (Equation 5), and a reconstruction loss. Besides, hyperparameters and models configuration is optimized by using AutoML (including Bayesian optimization). The model is evaluated on the well-known time-series datasets UCR and UAE. Comments:***motivation***  this works combines different techniques including self-supervised learning and hyperparameter optimization. But I cannot clearly find whats their main contributions in this work since all of these techniques used in this work seems not to be new. I encourage authors to clarify their contributions formally. ***irregularly sampled ts*** since this work is for irregularly sampled time series (see title), this work should consider how to model the irregularly sampled time series. But I cannot find any related mechanism to model or deal with irregularly sampled time series in the model section. Besides, as I know, both of the datasets UCR and UAE are not standard datasets for irregularly sampled time series. And in your experiments, you construct that time series by using an irregularly sampling rate \beta. This may be problematic since it should be missing data, but not true irregularly sampled time series. There may be two different topics in the time series community.  ***representation learning*** since representation learning aims to learn some good representative features that can be easily transferred to other downstream tasks. But in this work, it seems that a single feature is learned from a segment of time series. I wonder whether the representation learned from this framework can be used for other more popular downstream tasks such as classification or prediction. Besides, since in the objective function, there are clustering loss and anomaly detection loss, I think we cannot say it is an unsupervised representation learning approach. It is more like a clustering model or an anomaly detection model. To demonstrate that the proposed framework can produce a good time-series representation, other downstream tasks such as classification or prediction need to be considered.  This paper aims to provide an effective augmentation strategy without the need for a separate search. The resulting method is called NOSE Augment, which is presented as a substitute for the previous AutoAugment type methods (e.g. Fast AutoAugment, Population Based Augmentation, RandAugment, Adversarial AutoAugment etc.) In parallel to this goal, the authors propose adding the mixing-based augmentation operations Mixup, Cutmix, and Augmix into the list of operations used in AutoAugment. Finally, authors also employ a curriculum of augmentation strength during training. While the cost of search for AutoAugment-type policies has been significantly reduced (e.g. PBA and Fast AA), it is still a worthwhile goal to want to remove the search phase completely, for further reduction of computational cost as well as convenience. While RandAugment does not use a separate search phase, it still has two hyperparameters that need to be optimized, similar to learning rate or weight decay, as the authors of this submission correctly point out. Thus I find the goal of wanting to remove the 2 hyperparameters that RandAugment requires worthwhile. However, I do not believe that the results shown in the paper indicate that NOSE Augment has achieved this goal, for reasons detailed below. Thus, I do not believe this work presents an improvement over previous work.1)  RandAugment paper found that the optimal strength of augmentation depends on model size and dataset size, and found the optimal strength to be especially different for small ImageNet models such as Resnet-50 vs. large ImageNet models such as EfficientNet-B7. Since NOSE Augment is only evaluated on models that similarly sized to each other, it is hard for us to know if NOSE Augment would also do well on larger models such as EfficientNet-B7, or much smaller datasets that were explored in RandAugment. Looking at the optimal magnitude that was reported in RandAugment for the models NOSE Augment was evaluated on, it would not be surprising that random AutoAugment policies would do well. The authors should evaluate their method on a larger model such as EfficientNet-B7 or on a small dataset such as a small subset of CIFAR-10 to see the performance of NOSE Augment on different model sizes and dataset sizes. 2) This paper adds several different components to their method at once, without any ablations on where the improvements are coming from. Previous work has seen that combining AutoAugment and Mixup can be helpful (e.g. see  "Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network" by Jungkyu Lee et al.) In the case of NOSE Augment, it is not clear how much of their performance results from adding different methods on top of each other vs. other factors such as the curriculum. 3) It is also not clear to me what the main contribution of this paper is. Random AutoAugment policies have been evaluated both in AutoAugment and RandAugment papers. AutoAugment paper found that 25 random subpolicies did somewhere between Cutout and AutoAugment. RandAugment paper found that uniformly randomly sampled magnitudes do as well as constant magnitude on cifar-10. If the main contribution of NOSE Augment is the curriculum they use for augmentation, the authors should provide experiments that show that the curriculum alone gives improvements, and not the addition of new operations such as Mixup, Cutmix, and Augmix. PBA and RandAugment papers have both tried curriculum for augmentation, and it seems like RandAugment without curriculum can get as good results as PBA or RandAugment with curriculum.4) For a paper on augmentation strategies, the results should only focus on improvements due to the augmentation strategy, and not any other model decisions such as training protocol or architecture. Looking at Table 1 of the paper, it is not clear to me if the authors have run their own baselines for each architecture, and what accuracy their own baselines got. The baseline column in Table 1 matches the baseline results that were reported in the AutoAugment paper to 4 significant figures for every row. This makes me think that the numbers reported as baseline are not actually the accuracies that the authors would necessarily get if they ran their experiments with just the baseline augmentation. For example, we know that ResNet-50 can get much higher accuracy than 76.3% on ImageNet, without any advanced augmentation, if other regularization methods are employed (e.g. label smoothing etc.) I would urge the authors to report the results of their own baseline models in all of the relevant tables, so that the reader can directly see how much of the reported performance is due to the proposed augmentation strategy. Summary:This paper provides an approach for causal inference in observational survival dataset in which the outcome is of time-to-event type with right-censored samples. The method consists of a representation learning component to reduce selection bias and a survival analysis component which is modeled with normalizing flows. Pros:- The paper addresses an important and interesting question.- The manuscript is well-written and easy to understand.Cons:- My main concern is the minimal originality and significance of this work. That is, the representation learning component is directly taken from (Shalit et al., 2017) and the objective function for the survival analysis component is directly taken from (Chapfuwa et al., 2018). Also, the use of normalizing flows for modelling time-to-event kind of targets is not well motivated. - The literature review only points to several publications but does not go in depth into why/how the proposed method differs from those. There are also some important references that are missing. For example, Miscouridou et al. (2018) also use normalizing flows for survival analysis and it is necessary that the authors discuss how their work differs from theirs.Miscouridou, X., Perotte, A., Elhadad, N., & Ranganath, R. (2018). Deep survival analysis: Nonparametrics and missingness. In Machine Learning for Healthcare Conference- the paper suffers from many inaccurate statements; a few examples follow:1. In the third paragraph of the Introduction section, the authors mention that the treatment assignment mechanism is not known a priori. Therefore, there may be variables, known as confounders, affecting both the treatment and survival time, which lead to selection bias. This is wrong; even if we know the treatment assignment policy a priori, we still might have selection bias & these two are independent.2. In the last sentence of paragraph four of Introduction, the authors say that the methods (cited above) that account for confounding bias (by re-weighting) lack a counterfactual prediction mechanism. This is wrong because trivially, all methods have a prediction mechanism in place, and the ones that do account for confounding bias can predict counterfactuals accurately as well.3. The authors should note that representation learning does not **remove** confounding bias; it might only **reduce** it. Also, reweighting does not **remove** confounding bias either; it just **accounts for** it.Minor:- In section 2, under *Estimands of Interest*, the authors state that $\lambda(t | x)$ is defined below but its never defined.- In section 3, under *Accounting for selection bias*, the authors state that we can go from Eq. (1) to (2) because identifiability holds (since $X$ is a sufficient set from $A$ into $T$.). Could you please explain how this is different from Ignorability? I.e., $\{ T_0, T1 \} \perp A | X$ ? This paper makes a key observation: exposure bias is blamed for many of the issues with Neural Language Generation but it lacks both a concrete definition or any obvious evidence that it truly exists. The authors begin by defining exposure bias as the decrease in quality and relevancy (to the conditioning text) in generations as the model conditions on its own output. A limited qualitative study fails to find evidence of exposure bias, and the authors propose two metrics, EB-M and EB-C, to measure the quality and relevancy degradation, respectively. Quantitative results find that degradation on these two axes as the model conditions on itself are either minor or non-existent. To calibrate our understanding of these metrics, the authors do a human study as well as comparing two GAN frameworks. In the discussion the authors discuss limitations of the work, mostly that the given metrics are not a complete notion of evaluation and connect their work to related literature.Strong Points:- The authors correctly call out the lack of scientific work on actually defining and understanding exposure bias.- Their intuition that exposure bias should mean quality and relevancy are highly dependent on the prefix is good idea.Weak Points:- The quantification of exposure bias is largely unjustified.- The qualitative evaluation is ad-hoc.- The discussion section is rushed and does not make a strong argument that the authors interpretation is supported.- The choice of wiki-103 as a testbed is strange.I recommend rejecting this paper, as the way exposure bias is quantified is dubious and the authors do not make a strong argument that they would have detected exposure bias if it is present.I want to start by saying I think that the authors correctly point out a deep flaw in the literature around text generation: exposure bias is a casually defined issue that is nowhere well-defined or proven to exist. Even more importantly, I think the intuition that this paper gives around sampling different kinds of prefixes and looking at the resulting generations is exactly right. The main issue I have is that the formalization of exposure bias is not well-founded and I do not believe a convincing argument is made the claims of relatively little (or even no) exposure bias are actually supported by the experiments. While the ways the authors try to quantify exposure bias are intuitive, there is really no evidence that these metrics measure what they claim to. Corpus-BLEU has been used in many papers, though I dont know of any showing actual correlation with a human metric, but we can at least say that it has precedent being used cumulatively. However, using the corpus-BLEU of _individual_ samples in a fraction creates the potential for completely inaccurate estimates. Even assuming corpus-BLEU works, there is no guarantee that it acts as an estimate of individual samples, in the exact same way that BLEU is known not to be accurate for individual sentences (Burch, 2006). Furthermore the EB-C metric is basically proposed from thin air. It makes sense to compare the distributions, but how big of a difference should we expect? GANs do badly on EB-C, but they are known to be significantly worse even on normal metrics, so its not clear what this proves.The qualitative evaluation is just the narrative of what the authors think. No intuition is given, simply we didnt find exposure bias when we looked at the data. It especially shouldnt be described as failing to show the significance of exposure bias. The authors refer back to these thoughts as experiments, e.g. on page 5 with This agrees with our observations in the qualitative experiments. This seems inappropriate.The discussion is extremely rushed and it is not very clear what we are supposed to conclude from it. The authors show that their metric is not complete via a toy example, but talk about how MLE does not produce these kinds of solutions. Thats fine and I believe that part of their argument, but I still feel that there is not much evidence that the metric would actually show significant differences if the quality of the writing went down.  Quality is a tricky thing to measure. To the authors credit they conduct a human study.  However, it is unclear to me whether humans that are exposed to a generated prefix (which may already deviate somewhat from human language) would feel that the mistakes made due to exposure bias match the mistakes in the prefix.  If the generations were truly horrible, I agree we would see more deviation in this score, but the paper attempts to disprove the very existence of exposure bias. A more appropriate reframing would be to talk about the kinds of effects exposure bias couldnt possibly be having, e.g. that exposure bias may only result in strange word choice but not total degeneration. This over-claim, and the lack of smaller claims to build-up to it that might have been more appropriate, make it difficult for me accept the described conclusions.Finally, previous works on open-ended text generation have used the portion of the corpus released by OpenAI. Its hard to know how comparable these results are, and since both GPT-2 and a significant number of validation text examples from WebText are freely available this seems like a flaw.  Why was wiki103 chosen instead of WebText used alongside the pretrained GPT-2?Callison-Burch, Chris, Miles Osborne, and Philipp Koehn. "Re-evaluation the role of bleu in machine translation research." 11th Conference of the European Chapter of the Association for Computational Linguistics. 2006. The paper studies the exposure bias in auto-regressive neural language models. This problem is known to cause incremental performance degradation, and attempts to mitigate this problem have received significant attention in the community (using, e.g., RL and GANs). The paper claims that prior work has mostly focused on addressing the problem rather than measuring how severe the exposure bias problem actually is. Despite extensive previous work on mitigating exposure bias, the paper suggests that the exposure bias is not large enough [to] induce drastic performance loss during generation (e.g., a human evaluation controlling for exposure bias show relative differences of only < 3%).On the positive side, I agree with the paper that it is worthwhile to study the extent of the exposure bias problem, and the approach of the paper (comparing generation with model-based prefixes vs. data prefixes) is quite intriguing, as it attempts to compare generation with exposure bias again without such a bias. That said, I have several concerns that make me question some of the claims of the paper:1) The human evaluation shows very little performance difference between generation with data prefix (D-prefix) and model prefix (M-prefix), suggesting exposure bias is not a problem. But I would argue this is mostly an artifact of the experimental setup, as prefixing generation with D-string (of length L) only eliminates exposure bias up to position L, and then both evaluated systems (according to the generation process defined Section 4.1) use a standard auto-regressive LM generation process that makes them both subject to the exposure bias problem. Since these generated strings are of length 30, there is plenty of room for exposure bias to crop in. (Indeed, Zhang et al., (2019) and Holtzman et al. (2019) have shown concrete examples of exposure bias artifacts appearing in much shorter sequences). So, the small performance difference between a preference M-prefixed and D-prefixed generation may very well be due to both setups being almost identical (same model, same auto-regressive inference algorithm, and the only difference is the prefix  which human raters are not even asked to judge directly). If exposure bias is indeed a problem, then it would affect both systems almost the same way, so this human evaluation cant be used to either affirm or deny exposure bias is a significant problem. 2) I also have concerns regarding the automatic evaluation based on BLEU. What the authors call corpus-BLEU is actually not the standard version of BLEU (see Other Comments below), as the version of the paper looks at n-gram (n=1 to 3) matches between the generated sentences and a large set of references that is *not* specific to a particular context. As the papers automatic evaluation is done in a completely context-agnostic way and relative to a large pool of references, it essentially only measures whether the model (with or w/o exposure bias) is able to generate plausible trigrams, but that sets the bar very low as we already have plenty of evidence showing neural language models are quite capable of generating reasonable trigrams (whether there is exposure bias or not), and in fact often much longer n-grams. The authors claim that auto-regressive models appear to have self-recovery ability that mitigate any exposure bias, but I would say that it is hard to claim anything about self-recovery (at least in terms of automatic evaluation) when the evaluation metric operates over such a small window (<=3 words). 3) Abstract: Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. I find this claim a bit misguided, as the numerous papers addressing exposure bias are *empirical* ones. They have shown improvements in various tasks such as machine translation, image captioning, and other generation tasks thanks to techniques aimed at reducing exposure bias. Now, one could claim that significant improvements shown in these papers are due to reasons other than exposure bias, but if that is the case then the submission doesnt do a much better job at isolating exposure bias from other factors (given my concerns in (1) and (2)). Note that Zhang et al. (2019) actually does include a short section attempting to quantify the effect of exposure bias. 4) For a paper that attempts to challenge the current understanding of a problem that has received very significant attention (i.e., exposure bias), it is quite thin in terms of related work (2 paragraphs). The paper omitted important related work (e.g., Schmidt, 2019; Tan et al., 2019; Rennie et al., 2016), including an ACL best paper on the same topic and whose findings appear to be at odds with the current submission.In sum, the paper tries to improve our understanding of exposure bias and its impact on open-ended language generation, and this is totally a worthy goal. I also recognize that isolating (i.e., ablating) the effect of exposure bias is difficult. That said, the paper makes rather strong claims (e.g., that performance gain is minimal if exposure bias is supposedly eliminated) that I find unfounded given my points in (1) and (2). In both evaluations, the setups evaluate a given model sequence W prefixed by a string sampled either from the data or the model, but that does not eliminate the fact that exposure bias is bound to appear within the generated sequence (length 30), which is the sequence that is evaluated in the end.Other comments:Corpus BLEU: Note that BLEU, as originally defined in (Papineni et al., 2002), is in fact a corpus-level metric, as it aggregates n-gram statistics over an entire (test) corpus. So corpus in corpus BLEU is redundant and possibly confusing. The term corpus BLEU or corpus-level BLEU is generally used to contrast with various versions of sentence-level BLEU. Now it appears that corpus BLEU in this submission refers to the version of BLEU used in SeqGAN (Yu et al.), which is therein not called corpus BLEU. That distinction should be made clearer, considering that the use of a large number of sentences from ground-truth data as references is a significant departure from how BLEU was originally designed to work. Indeed, (corpus-level) BLEU (Papineni et al., 2002) doesnt allow matching hypotheses and references across test instances, as opposed to the submission. The authors justify their use of BLEU as it is a well-established [metric] in the NLG literature, but this is rather misleading as their specific version of BLEU is not well established and not what is commonly used in MT and NLG.Table 1 doesnt actually illustrate what the experimental setup of the paper does (i.e., Section 4.1), as the latter doesnt include a shared prompt of 20 words that is supposed to make the two generated strings more comparable. Since this prompt supposed to make the two strings more comparable is absent from the actual evaluation setup, I gather from the authors own words that they implicitly admit that string comparisons in their evaluation setup are not so comparable. * Zhang et al., 2019: https://arxiv.org/pdf/1906.02448* Schmidt, 2019: https://arxiv.org/abs/1910.00292* Rennie et al., 2016: https://arxiv.org/abs/1612.00563* Tan et al., 2019: https://arxiv.org/abs/1811.09740* Holtzman et al., 2019: https://arxiv.org/abs/1904.09751* Chen and Cherry, 2014: https://www.aclweb.org/anthology/W14-3346/ Summary: This paper proposes to use the squared Bures distance in discriminator feature space to match the generated and real distributions. This proposed method does not require any modification to the network architectures, and is easy to implement. The proposed method produces good empirical results with simple generator architectures in synthetic and real datasets. Reason for score: While I find strong interest in the proposed method, the way in which it is presented in this paper does not permit me to fairly judge the merits of this method. The experiments are not thorough and the quality of writing is subpar. Thus, I vote for reject on this paper.Pros. This method introduced in this paper is theoretically sound. The idea of imposing additional distribution matching in the feature space of the discriminator is an interesting direction that should warrant more studies. Extending beyond the current scope, the use of the adversarial network as both a discriminator and a feature extractor has great potential as a research direction.Cons1. The choice of synthetic experiments may be too easy to discern difference between methods. Most GANs get similar results on the 2D grid experiments as the bottleneck is mostly in the modelling capacity of the generator at expressing non-linearities. Previous works have also achieved perfect mode coverage on stacked-mnist with similar network architecture (https://arxiv.org/pdf/1712.04086.pdf). 2.The Bures metric is one among many metrics for comparing covariance matrices. Ablation on the choice of distance metric should be performed. 3. Higher performing methods with similar network capacities have been left out in the performance tables. This is counter-productive to the purpose of establishing context. Very few recent methods evaluate DCGAN on cifar10 or STL as the architecture is too limiting for these complex datasets. Only inception scores are reported on resnet experiments. Thorough experimentation and evaluation with a modern architecture on RGB datasets would greatly help the case for this paper.4. The writing in this paper requires significant rework to reach publication quality. Grammatical mistakes and convoluted sentences are too frequent and significantly detracts from the idea being presented.Minor comments...images, although, the training... run-on sentence; and a discriminator what follows a semicolon must be a full sentence.issue  the mode collapse  appears -> remove thecomplemented by a additional term -> an additional term; I stopped tracking grammar mistakes after this point.MDGAN (Li et al., 2017), -> MMD-GAN, MDGAN is (Che et al., 2017).In table 1, does time column correspond to total training time of 25k iterations? Why is BuresGAN so much slower than MDGAN here but faster in iteration time according to appendix D table 16? Reject.SummaryThis paper works on the problem of create new recipes. It uses recommendation approaches on it. The paper use two approaches for explainability and conducted experiments on two real-world datasets.Stregthens1. The explainability in recommendation is an important research problem.2. The recommendation for food pairing is reasonable.3. This paper demostrate the proposed approach on two real-world datasets.Weakness1. This paper's writing can be greatly improved. It needs proofwriting. There are a lot of typos and also unfinished sentences. (See minors for details)2. The novelty of this paper is low. Most of the techniques are not proposed in this paper. For example the memory network in already used for explainability in recommendation system. [Huang 2018]. 3. The compared approaches are very simple and outdated. There is no related work section of position this paper in the literature. There are many advanced recommendation approaches. The authors can do a better literature Survey. A few example below- He, Xiangnan, et al. "Neural collaborative filtering." Proceedings of the 26th international conference on world wide web. 2017.- Cheng, Heng-Tze, et al. "Wide & deep learning for recommender systems." Proceedings of the 1st workshop on deep learning for recommender systems. 2016.4. The paper mentions "While both our Implicit and Explicit models outperform all counter parts on all metrics usually with a clear margin". However, from figure 1, NMF is the best performing one on HR@10 on CulinaryDB.Minors:In introduction line 2: "food preference modelling", -> modeling,All the "paring" should be "pairing"Right above Section 2. Missing peorid at the end of the sentence."At the embedding layer,this", add space after comma"In this task we" => In this task, we"eventhough" => even though"This result is some good " need to change.Questions"where the only concern is whether a user has interacted with an item and the system ..." Why this is a concern? Is it an approach?For section 3.5, it has a three steps for generating a mini batch.It mentions sample additional postiive examples as necessary. Why sampling addtional positive examples are needed?"Note that although we also trained our models without the recipe restriction, " What does it mean by without recipe restrictions?The "Artificial Food Pairing Task" is a bit confusing. Does a pair mean two ingredients or mean one ingredient set and one ingredient? This paper presents a general approach to embed high frequency information into low-frequency data with a particular focus on improving the performance of virtual clothing. To address over-smoothing issues in the predicted meshes, authors proposed the texture sliding method that changes texture coordinates on each camera through the deep networks. The texture sliding neural network (TSNN) is trained using the ground truth offset computed for each camera and pose. * Pros1) The proposed TSNN leads to the performance gain in the virtual clothing* Cons1) The current manuscript requires major revisions by addressing the following concerns.- It is hard to understand what to convey in Section 3.1. Especially, the following sentence just states the problem without how to address it.This assumes linearity, which is only valid when the triangles are small enough to capture the inherent nonlinearities in a piecewise linear sense; moreover, folds and wrinkles can create significant nonlinearity.- It is difficult to follow what authors intend to convey in Figure 2.- Two methods in Section 3.2 are hard to interpret. - It is needed to mark both camera view and pose in Figure 4.- Algorithm stating the overall procedure of the proposed method would improve the readability.- What is 'UV space' in Section 5?- How did you estimate the joint angles theta_k?- What data did you use for training/validating/testing the networks?2) Comparative study seems to be insufficient. It seems that a more thorough performance analysis is needed. This paper proposes a new objective to learn representations that allow for efficient and accurate search with the Multi-Bernoulli Search data structure. The motivation for such a scheme is strong and the preliminary empirical results demonstrate the utility of using the proposed representation learning objective and the data structure. However, I am currently leaning towards a reject because of two main reasons: The first being the clarity of the presentation in the paper that makes it hard to identify (i) novel contributions, (ii) key algorithmic and technical details -- the main paper is supposed to be somewhat self-sufficient; with the current version, it was significantly hard for me to follow through the presentation even when repeatedly referring to the supplement. The second reason is the lack of positioning of the proposed scheme/objective/data structure to a long line of research on the use of machine learning (with novel metrics/objectives and data structures) for search, including the learning of space partitioning trees [A, B, C], locality sensitive hashes [E] and representations [D] -- it is quite possible that the proposed scheme is solving a different and/or more general problem but I believe it would be useful to connect this "Search Data Structure Learning" to the existing work on "Learning to Search".Beyond the aforementioned high level comments, please find the following specific comments/questions that should be addressed:- The set of relations $\mathcal{R}$ and the per-database $\mathcal{R}_D$ could use further motivation as to how they relate to search and/or relate to the task of learning search data structures. - It is not clear why the distinction between "absolute" and "relative" is necessary in the context of the problem being targeted in this paper.- The SSWR definition is unclear to me and can use more exposition. It is not clear that $\delta_t, t = 1, \ldots, T$ is a sequence of sets. Also, it is not explained why the oracle costs are only incurred for the final $\delta_T$ and not all intermediate ones (unless $\delta_{t-1} \subseteq \delta_t$) or to the complete $\cup_{i=1}^T \delta_t$.- At the end of section 3, after presentation of SSWR, it is not clear why we are minimizing for a search sequence generator $\mathcal{G}$ that is aggregated over database-query pairs $(D, q)$ -- wouldn't we learn a data structure per database (as is done for data structures used for nearest-neighbor search)? Can this be clarified, and if it is deliberate, please motivate the need and advantage of learning across databases.- The algorithm description is very hard to follow -- there are M back-end data structures and also the learned representation is used to generate M probable back-ends to insert in and T probable back-end to search from. It is not clear how the search involves $T$ passes over the $M$ back-ends. It would be good to clarify in the main paper that the "outcomes" are keys for each backend and all $T$ keys are tried in all $M$ back-ends.- How are we leveraging multiple databases as shown in the previous loss function?- The halting mechanism requires better presentation and clarification. It makes intuitive sense. But the presentation of the training algorithm (at least in the main paper) seems insufficient to provide enough context about the specifics of the halting mechanism.[A] Li, Z., Ning, H., Cao, L., Zhang, T., Gong, Y., & Huang, T. S. (2011). Learning to search efficiently in high dimensions. In Advances in Neural Information Processing Systems (pp. 1710-1718).[B] Cayton, L., & Dasgupta, S. (2008). A learning framework for nearest neighbor search. In Advances in Neural Information Processing Systems (pp. 233-240).[C] Dong, Yihe, et al. "Learning Space Partitions for Nearest Neighbor Search." ICLR 2020.[D] Sablayrolles, A., Douze, M., Schmid, C., & Jegou, H. (2018, September). Spreading vectors for similarity search. In International Conference on Learning Representations.[E] Wang, J., Liu, W., Kumar, S., & Chang, S. F. (2015). Learning to hash for indexing big data -- A survey. Proceedings of the IEEE, 104(1), 34-57. This paper presents a new method, PrBO, and it incorporates user knowledge on the location of potential optimum to the prior used for Bayesian Optimization. The authors claim that BO can be solved more efficiently using their method and showed promising empirical results on some low-dimensional benchmarks.Disclaimer: I reviewed the same paper at Neurips. While I was happy to see that the authors have resolved some of term misusage issues and wordings, I felt somewhat disappointed that many good points raised by the reviewers there before were not addressed in the new version of the paper. IMHO, it still remains to be true that this paper needs a bigger surgery to be accepted, not only for Neurips or ICLR, but also other ML venues. And I hope the authors will consider the reviewers' opinions before submitting somewhere else.Pros:- Writing is mostly clear. Good clarity and good illustrations.- I still think this is a very interesting idea and a quite promising direction to incorporate human knowledge in our used-to-be black-box machineries. This paper could set a good direction if it's done properly.Cons:- This wasn't the case of the previous version, but I found it unsettling that the 1st contribution point was "For the first time, user prior knowledge can be combined with standard BO probabilistic models, such as Gaussian Processes (GPs), Random Forests (RFs), and Bayesian Neural Networks." That is just bold and wrong. User prior knowledge is always considered in those models when selecting which kernel structure to use, how many trees would be sufficient, or if we need a convolution layer or not. Even in the narrowest definition of prior, this paper is a perfect demonstration of how to incorporate human knowledge in GPs: https://papers.nips.cc/paper/2015/file/4462bf0ddbe0d0da40e1e828ebebeb11-Paper.pdf. I strongly suggest not to ever claim one's the first, and when it's really needed, be very very specific. Otherwise there will be bad consequences. (See an extreme case related to Christopher Columbus.)- It is still not clear what different probabilities mean to the authors. The authors described the prior P_g(x) as a prior on good points and P_b(x) as prior on bad points. What do those mean exactly? Could you define them correctly in the paper? And how does the GP-induced good/bad point probability interfere with the proposed ones?- Footnote on Page 3: "P b(x) is not a probability distribution". What do you mean? In the text it says "prior distributionPg(x)". - P_g is not independent for each x. Will that cause any issues to the method?- If both the "prior" and "posterior" are both pseudo, what do they mean exactly? Or what are even approximating?- Can the authors describe a coherent generative model of what they proposed?I strongly suggest the authors carefully read the reviews from Neurips again.  ########### Summary:The paper considers the question of quantifying the uncertainty that arises from the optimiser used to perform inference in a given model. Taking a Bayesian approach, the aim is to deduce the posterior over the space of optimisers. The form for the posterior is chosen to be a Boltzmann distribution which is then approximated with a multivariate Gaussian using a KL divergence. The parameterisation for the posterior is defined using an LSTM neural network.###########Reasons for score: The high-level idea in the paper is intriguing but I found the logic in the methodology hard to follow and interpret, and it is not clear to me how this work is able to estimate the uncertainty or why it would improve the performance of existing approaches to optimisation. ##########Pros:- The main high-level idea seems clear.- The experiment section is quite extensive and the results provided suggest the approach may have some merit.##########Cons:- The presentation is not clear, especially in the method section. It is rather strange that instead of defining the prior over possible optimisers, the paper only talks about the posterior. - It seems to me that this work relies heavily on the work of Ortega et al (2012), and if that is the case, a clearer presentation of the main arguments of that work would be very useful. - The arguments in the problem statement seem quite scattered. For example, I'm not sure I see the relation of Eq. (3) to the rest of this work. Could you explain this statement from page 3 in some more detail:"It is straightforward to first model the posterior over the global optima (p(w|D)) and then sample from the posterior to obtain..."- Some ablation studies may clarify the effect of the various design choices. For example, (1) using an LSTM as a way to parameterise the model, (2) the choice of a local region for the local posterior. - One of your criticisms of MC estimates of optimiser uncertainty is that "it heavily relies on the pre-defined distributions (discrete grids) over the hyperparameters and(or) the start points". However, in your case, the model seems to be constrained to a local neighbourhood of the input parameters, which seems even more limiting to me. - The overall quality of the writing could be improved (for example, Sec. 1 and 2 include a lot of repetition). This paper proposes Jumpy Recurrent Neural Network, an RNN model with non-uniform time steps. To train this model, the authors propose to use a greedy supervision to determine optimal time intervals. The experiments on linear dynamics prediction and planning show comparable performance of proposed model against standard RNNs. The advantage is that the proposed model can significantly speed up RNNs under relatively linear dynamics prediction tasks. Even though non-uniform time step RNNs have been studied in many literature, this proposed training supervision method seems novel.+ves: + Overall, the paper is well written. In particular, the proposed model and its training methods are clearly explained.+ The results section is well structured. The hyperparameters, architecture, and experimental settings are well demonstrated.  Concerns: - The proposed method, non-uniform time interval RNN, is not novel. For example, Che et al 2018, Lipton et al 2016, have very similar contributions. This paper needs to clarify the difference. - The proposed greedy training method seems have significant flaw. Clearly it will have Delta=1 at the beginning of the training. Then it can only improve corresponding Delta one by one at each timestep. The loss corresponding to Delta is highly non-continuous and it will definitely not learn to find the real optimal Delta.- The model does not seem to learn meaningful Delta. See Figure 3 and 4. For such clean data, the model should learn to predict only turning points but it seems it just predict random intervals. Also, there are no verification of learning meaningful Delta. - The experiment is clearly not fair for a standard RNN. Since Delta=1 is always true for standard RNN, clearly it will be slower. The speedup ratio will only be the average of predicted Delta in the proposed model. However, what if the RNN just sets a larger Delta, what if the RNN just set a random predicted Delta?  - The last experiment is not convincing. First, it's a simple supervised task, not real "model-based planning". Second, Figure 6 seems not converged. No experimental detail is given for this task. Third, twice as fast mean the model generally just predict Delta=2. - All the experiments have internal bias towards linear dynamics. This favors the model with corresponding inductive bias. To prove the model is useful, I would recommend to run standard RNN synthetic task like copying task, adding task etc, to prove its basic functionality. Minor comments: * Figure 1, in basic RNN, it should be an arrow instead of a line from h_i to GRU.* Equation 3, people usually don't write output as x, otherwise it can only be a sequence prediction task like language modeling. Please write it y. Also, it creates confusion to equation 7 whether it's input or output.* The "Jump Bootstrap" seems very important technique for the proposed method. But there's no experiment or discussion on it. The paper proposes the IMA model, a scalable model that learns modality importances and robust multimodal representations through a novel cross-covariance based loss function. The proposed model performs unimodal inference in absence of modalities and also addresses the problem of detecting important subspaces in each modality through weighted cross-covariance loss terms, which are minimized by unimodal importance networks. Results are shown that the IMA model is able to distinguish digits from uncorrelated noise, and word-level importances are learned that correspond to the separation between function and emotional words. The multimodal representations learned by IMA are also competitive with state-of-the-art baseline approaches on downstream tasks.Pros: + Importance-based Multimodal Autoencoder is a very interesting topic, Importance Network Training also would benefit to other fresh ideas and new approaches.+ I really like the idea of learning modality importances and robust multimodal representations through a novel cross-covariance based loss function. I agree with the authors that this should help a lot for unimodal inference in absence of modalities and also addresses the problem of detecting important subspaces in each modality. Cons: - While I like the premise of the paper, I feel that it needs more work. My main concern is that seeking to learn weights $y_{ij}$ (the importance of each sample $x_{ij}$) should not be equivalent to the degree to which $x_{ij}$ does not belong to $R_j$. In other words, if the goal of *importance network* is only filtering uncorrelated noise, how is the importance reflected? Perhaps this should be called correlation. My understanding is that the importance network should be quantified to a weight range, similar to the attention mechanism. If not, the author should make a more clear explanation of the *importance network*. - The text is quite hard to read, there are many places in the paper that are not explained clearly, especially the related work. This section more like a pile of other related works but lacks coherence and puts the proposed method into context. - The model description section would be easier to understand if the author can add an overall framework figure, especially *Importance Network Training* need a more detailed description.- At the *Importance Priors* section, what is the $L_{local}$? At *Dataset IEMOCAP* section, the author stated "We also remove the MFCC features, and thus most of the acoustic variability within the utterance is removed resulting in factors of variation (such as emotion) which are more global in nature." What is *global in nature*? What is the difference between defining local and global? - The experimental part is a bit confusing and needs more analysis, why is the result of Table 1 worse than those two models: JMVAE-KL and MVAE?- This paper hardly cited references from 2019 and 2020, and it is recommended that the author pay attention to some updated work.-  The labels of the figures and tables in the paper should preferably appear and quote in order. This paper simulates network and file corruptions at multiple corruption levels, and explore corruption-agnostic and corruption-aware defenses. The presented Bit-corruption Augmented Training enhances the robustness of the video machine learning models. Experimental results show the effectiveness. Pros: The experiments are very extensive. The effectiveness of the presented method compared with the corruption-agnostic is clearly validated.Cons:1.      The novelty is limited. Considering the attack in the encoding and decoding process have been studied, and sophisticated methods [1] have been proposed. Compared with [1], I wonder about the advantage of the proposed method.2.      The experimental results are not convincing. Baselines such as [1] is not included in the experimental results for fair comparison. I think the baselines in the manuscript is too weak.3.      I think the idea is too ad-hoc, which just generates adversarial samples in the space of encoder and decoder instead of the space of the classifier. I think the difference is not significant. [1] Rakin, A. S., He, Z., & Fan, D. (2019). Bit-flip attack: Crushing neural network with progressive bit search. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1211-1220). **Summary:**The authors evaluate the effect of bit-level corruption, including network packet losses and bit corruptions, on video models such as action recognition and multi-object tracking. They found that the model performances drop significantly under severe corruption levels. To overcome this issue, they propose a defense method named Bit-corruption Augmented Training (BAT) to enhance the robustness of the model by embedding corrupted video samples in the training process. Results show that BAT is able to improve the model robustness over other methods such as Out-Of-Distribution (OOD) detection and Adversarial Training (AT). **Strength:**- Explores an interesting and new problem space for video model robustness.- A range of realistic corruption rates was evaluated.**Weakness:**- The threat model for the defense is unclear.- No comparison with naive fault tolerance methods at the network and video file level.- The proposed BAT method is trivial and the performance is not good.- Packet loss should not be categorized into data corruption.**Detailed comments:**This paper explores a topic that is both realistic and interesting for the increasingly popular video model applications. I appreciate that the authors evaluated their approach under realistic packet loss and bit-corruption rates that would occur in the physical world. Yet the paper has several points which I think significantly weaken its value. I will detail them as follows.- The authors claim the proposed BAT method as well as OOD detection and AT as *defenses*. However, I can hardly find any evidence that the authors evaluated these methods under an adversarial setting, i.e., the paper does not assume the existence of an attacker. It seems to me that this paper is more on the side of fault tolerance than defenses for attacks.- For detecting or correcting network packet losses and bit-corruptions in the files, a straightforward solution is applying existing network and memory/file-level fault-tolerance methods. For example, checksum or error-correction codes are two easily applicable solutions. The benefit of applying these methods is that they are model-agnostic and would not affect model performance under normal conditions.- The BAT method proposed by the authors seems rather trivial to me. How is this different from data augmentation? In addition, the reported performance of BAT is not good. For example, would a 27.3% accuracy under 10^-4 random file corruption bring enough utility for the model?- Lastly, I do not think it is a good idea to categorize packet losses as bit-level corruption since data loss is fundamentally different from data corruption. In other words, data corruption considers cases when data got changed but data loss only considers when data is missing. This paper studies the variance of stochastic gradient in SGD conditioned on the initialization point. It shows that the variance of stochastic gradient is a decreasing function of minibatch size for linear regression and deep linear network. Compared with previous works that show similar the results for one step in SGD, the results in this work only rely on initialization point. Although the technique in this paper is different from previous works,  the setting in this paper can only handle linear model which is simple and limited. Besides, only showing that the tendency of gradient variance as minibatch size changes is not enough,  and it is better to give the decreasing rate. So the impact of the refinement on the estimation of gradient variance is not clearly stated. If the authors can provide some examples to show that the analyses in previous works are too coarse to give the wrong direction, the impact of this paper can be enhanced. In current version of the paper, I can not get any new insights from the theoretical and experimental results because there have been many existing works that can tell the relation (or tendency) between gradient variance and minibatch.  Detailed comments: 1. The notation "var()" is not rigorous because the gradient is high-dimensional. It should be replaced by the covariance matrix of a random vector.  Given that the gradient is a high-dimensional vector and the "variance" is a matrix, it is not clear that what the meaning of "the variance is a decreasing function of minibatch size" is. Does it mean that all the diagonal elements in the covariance matrix is a decreasing function?2. It must be clarified that which distribution the expectation and variance are taken over. For example, var(\nabla L(w_t)) should be written as var_{w_t}(\nabla L(w_t)).3. In the setting of deep linear network, the distribution of data is assumed to be normal distribution, which is strong. Is this condition necessary in the proof? 4. The experiments are not sufficient to support the claim. Can you provide results for more selections of minibatch sizes, more datasets and more NN architectures?5. In future works, the authors point out many interesting research problems, but it is not clear that why the techniques and results in this work can help to solve those problems. Can you provide connections between the main results and the future work? For example, which technique can help design better variance reduction method? Clarifying this will be helpful to evaluate the impact of this paper.  This paper presents a constituent-based transformer model with auxiliary relation embeddings and labels to enhance the performance of aspect-based sentiment analysis. The constituent-based transformer modifies the original transformer by re-weighting the attention weights with constituent-based similarities. The auxiliary relation predictions helps to differentiate the relations that are useful for sentiment prediction. In general, this paper is readable and has a clear motivation. The strengths include:1. The idea of modifying the basic transformer to a constituent-based transformer by incorporating constituent similarities without any supervision is interesting. 2. The authors propose three model variations progressively, namely ConsTrans, RelConsTrans and RelConsTransLG to clearly demonstrate the motivation and effect of each variation.However, the paper still lack the following aspects:1. Some parts of the description is unclear with details missing. From (9), the generated $l_{ij}$ is a continuous value, how do you make it a relation label? Why do you take the L2 norm on $r_{ij}$ in (10)? What is the intuition of such computation to obtain the MSE? What is $L(\hat{y},y)$ in (11) and how to compute it? If $y$ is the sentiment label of the aspects, how it could be used to update the label generator $l_{ij}$? What is the intuition of training $\theta_{main}$ and $\theta_{aux}$ using separate datasets? Could you show what is the difference in terms of the performance?2. Given the complexity of the model, the improvement compared to the baseline models are relatively trivial. And it is somewhat insufficient to limit the application only to aspect-based sentiment analysis. The contribution is thus limited.3. How do you sample the meta-train set? Is the result over one meta-train set or averaged over different meta-train set?4. Minor points: caption is missing for figure 2. The first row in (3) should be $i\leq j$. ### SummaryThe paper proposes a method to jointly learn: (a) a latent state embedding; (b) a latent action embedding; (c) a state transition model; and (d) an RL policy.  The latent models should allow for better generalization over states and actions, and therefore result in improved learning, particularly for discrete action domains. The method shows improved performance over vanilla policy gradient on a grid-world task, a slot machine task, a recommender system, and half-cheetah locomotion.### Strong points- important problem: learned latent models of the world are key to making progress in RL- evaluated on several tasks, with improvements on some- it is surprising that the all of the components can be trained, in a stable fashion, without further regularization or conditioning### Weak points- there is a large and growing literature on leveraging learned latent models of all kinds, particularly for continuous systems. E.g., world models and many variants. It is unclear to this reader how to situate this work in relation to those.- the evaluation is weak:  two of the main examples are toy proof-of-concept; half-cheetah shows no benefit; the recommender system needs to be considered in the expansive volume of recommender system algorithms- the method is only compared against vanilla policy gradient for a number of the results### RecommendationsCurrently recommend to reject.  The approach needs to be discussed and evaluated in the context of other latent "world models", and to show benefits on more challenging problems. I also currently remain unclear regarding the potentially underdetermined nature of learning all the given components in the absence of further regularization or constraints.### QuestionsQ1. 4.3.1 component (iii): function g:  How is g well-posed, given that it is inverting a possibly many-to-one mapping?Q2. Eqn (5):  Given that this is the only loss function that involves three of the models, it is unclear to me how they are fully determined in the absence of further regularization, or additional loss terms.Q3. Algorithm 1:  Where are the models for g and T updated?Q4: uniqueness of state embedding:  Why would this emerge as a property? How has this been tested emperically? Doesn't this also mean that some of the benefits of the latent space are lost, if working from possibly-redundant state observations?Q5:  Figure 2e has comparisons with PPO and SAC. How would these algorithms fare for the other problems? ### Additional feedbackThis reader found the phrase "joint action-state embedding" to be ambiguous.  Upon first reading, I interpreted it as learning a unique embedding for the state-action space, e.g., for Q(s,a) for example. Instead, it refers to separate state and action embeddings, which are jointly trained during learning, along with the policy. "Jointly-trained action and state embeddings" would clarify this ambiguity.Figure 1b: label the transition model, T4.2 Assumption 1: Given an _action_ embeddingConcluding sentence for the paper:  this makes a very strong statement that is not really supported by the results. ##########################################################################Summary:The paper proposes a new objective function to prevent underestimating uncertainties. The authors claimed that the new form leads to state-of-the-art performance in several numerical experiments.##########################################################################Pros: - A new objective function is easy to understand.- The motivation for the work is sensible.##########################################################################Cons: - Although the motivation of the SML loss in Section 3 is sensible, but the theoretical grounds for the SML loss looks somewhat weak. MC dropout (Gal & Ghahramani, 2016) is to maximize the evidence lower bound and learns Bayesian neural networks. What are the properties of the optimal solution of the proposed objective function?- Following the question, is it restricted to the dropout networks? How can this new loss function be applied other than the dropout networks?- The authors use the (gaussian) negative log-likelihood with the mean and the variance of the sub-network outputs as one of the evaluation measures. In case of MC dropout, the authors use posterior mean estimates $E(E(Y |\theta, X))$ for $\mu$, but a conditional posterior variance estimates $Var(E( Y | \theta, X))$ for $\sigma^2$. Thus, it makes sense the MC dropout might underestimate uncertainties because $Var(E( Y | \theta, X)) \leq Var( Y | X)$. However, Kendall & Gal (2017), which the author cited in the manuscript, addressed this issue and provided a better uncertainty quantification method. But this method is not considered in the experiments.- The paper's writing makes it very hard to understand the results. For instance, implementation details for Figure 1 or its pointer are not provided. Also, Figure 3 should be improved. The current presentation is confusing and it's hard to recognize which is better.##########################################################################I vote for rejection. I may well have missed some points in my reading, so clarification is welcome. This paper defines a new scoring function for efficiently selecting between different network architectures, without requiring the expensive training of the models.  This scoring function is based on the expected dot-products between the stochastic gradient vectors corresponding to different pairs of training examples.  The paper shows that this method can much more quickly select between different architectures than previous network architecture search (NAS) algorithms, while attaining relatively similar performance.Strengths- Intuitively, it makes sense that higher correlations between the stochastic gradient updates will lead to less noisy updates, and thus hopefully result in a smoother optimization process and a better final model.Weaknesses- The main weaknesses of the paper in my opinion are lack of clarify and unconvincing empirical (and theoretical) results.- The scoring function used for network selection is never clearly defined.  The text in section 5 simply says average of the Gram matrix of gradient, i.e., P in Eq 1, which is vague because P is a 3-dimensional tensor.   Furthermore, in the second paragraph of Section 5 it states that the gradients are normalized by subtracting the average of sample gradients before calculation, which is a very meaningful modification to the above-mentioned definition, which requires justification and explanation.  Lastly, it is claimed in the intro and throughout the paper that the metric unifies gradient correlations and gradient values, but this is never really explained.- The proposed metric is not normalized in any way.  This suggests that larger models should automatically expect to have higher metric values. - It seems the central argument of the paper is that there is a strong positive correlation between the proposed metric and the final test accuracy of the model.  However, the empirical validation of this result is unconvincing; the dots in Figure 4 seem very scattered/random, and the choice the zoom in at two different granularities for the x-axis in this plot is very mysterioushow were the x-axis ranges chosen?  It seems arbitrary.  And even with these hand-picked choices of x-axis, the positive correlation looks quite bad.  To make this convincing, it would be important to compute the actual correlations, and corresponding p-values, to see if these trends are significant.- The take-aways from Figure 2 are quite unclear in my opinion.  Also, the figures are not clearly labeled.- The claims that GT-NAS is universal to different architectures and that it is universal to different initialization approaches seem way too strong, given the limited empirical evidence.- It was unclear to me how the math in Section 4.1 explains why the proposed metric is a good idea.  In order to better justify this metric choice, I would have hoped for a theorem formally analyzing in a simplified setting why this metric was a good idea.Overall, due to the lack of clarity and unconvincing results, I recommend rejection for this paper. <Summary>This paper addresses the problem of unsupervised learning of class representation using data augmentation. Its key idea is to encourage the learned representations to have low MI while maximizing the original augmentation-driven MI objective. It reports the improved performance for the benchmarks of Ji et al. 2019  classification on some easy datasets (e.g. CIFAR-10, CINIC-10, SVHN and STL-10). <Strengths> 1. The proposed approach proposes an MI minimization strategy for unsupervised clustering that can obtain more informative representation than previous methods.2. The proposed DHOG method shows better performance than some baselines reported in  Ji et al. 2019 for some easy classification tasks on  CIFAR-10, CINIC-10, SVHN and STL-10.<Weakness>1. This work summarizes the two contributions in sec 1.2. However, both of them are not fully supported. (1) This work claims that the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima. However, there is neither theoretical justification nor empirical results that show why the proposed approach does not suffer from this issue and why it is subsequently better than other works. (2) This argument may be based on that the proposed DHOG adopts distinct head that encourages different solutions between heads one another and a simple-to-complex hierarchy structure to the heads. Conceptually, they could be reasonable solutions to the issue, but there is no theoretic and empirical evidence that supports this argument. (3) The other contribution that this work claims is strong performance of unsupervised clustering on benchmark datasets. However, the empirical comparison is only carried out with baselines in Ji et al., 2019. (4) More critically, this work fails to cite the following recently published papers that reported much stronger performance on the same datasets. a. W. Van Gansbek et al, SCAN: Learning to Classify Images without Labels, ECCV 2020.b. G. Shiran & D. Weinshall  Multi-Modal Deep Clustering: Unsupervised Partitioning of Image, ICLR 2021.c. N. Astorga et al. MPCC: Matching Priors and Conditionals for Clustering, ECCV 2020.<Conclusion>My initial decision toward this submission is rejection because the main contributions of this work are not fully convinced. Moreover, this work ignores key related papers that show better performance on the same benchmarks. The study starts from the fact that DNNs have been around and popular for a while for modeling the visual system, but that they are not realistic because they are trained via supervised learning approaches with a very large number of parameters and that this is not a feasible model of the development in the visual system.In general, although the manuscript presents some interesting ideas, it makes many assumptions without providing clear bases for these assumptions (e.g. compressing the weights of a pretrained network to sample new weights is posed as a realistic approximation of the infant visual brain) and lacks a theoretical foundation for the claims and experiments that are presented. The authors acknowledge that this study is intended as a proof of principle, but given the arbitrary nature of the choices made, I do not see the added significant value of the results.While DNNs are indeed commonly used as models of the primate visual system, in my view, the current study is addressing a somewhat inconsequential problem. This is because to the best of my knowledge, no neuroscientist is claiming that a deep neural network is a complete and accurate model of the (development of the) primate visual system. Furthermore, it is well-known and acknowledged that deep neural networks are not biologically plausible models of (how learning occurs in) the brain. They are currently one of the best computational tools to use to study the sensory (and especially the visual) nervous systems, and that is all that they are. It is not clearly explained why it is necessary to claim that the learning in these models and the development of the brain has to be similar for them to be good models of vision. Of course, we should thrive for better and more accurate models of the brain, but in my view the current study does not serve to this goal.In section 4 authors describe an initialization protocol for the network weights which involve compressing a trained models weights into clusters and then sampling from these clusters. What is not clear is why the authors assume that this can be a valid model of the infant visual system. At this point their approach sounds like arbitrarily selecting a set of criteria to make the networks perform worse than fully trained networks, and then training them. I could be missing something, but I do not see the relevance or necessity of an approach such as the presented one. A main concern is that no theoretical basis has been established in the paper besides some superficial ideas. For instance, why would an infant brain be made up of a DNN with connections whose weights are initialized with the method authors came up with?Much of the methodological details are only included in the appendix. I found it rather odd to not find any information about, for example, the proposed weight initialization method in the paper.It is not clear to me what is presented in Figure 1 and why. Why are the authors showing how models from another paper trains?Another concern is that nowhere in the results seems to be a test for significance. The improvements of the results could be a coincidence, since the results are heavily dependent on one experiment. The authors propose a novel method, called Consensus, using an ensemble of deep learning architectures, to study the interpretability of models, when ground truth of interpretations is not available.The high-level idea of the paper is promising, i.e. evaluating interpretability without the need for human-labelled groundtruth, which is tedious and expensive to collect, and for this task, can be subjective. The proposed method consists of three stages: forming a committee of deep models, aggregating the results in quasi ground-truth, and ranking models of the committee based on the similarity with the quasi ground-truth.However, it is not clear from the paper how the committee voting is performed (is it averaging, majority voting), and what does the method bring in addition to just an ensemble of multiple CNN architectures.Great points of the paper:- using pretrained models to overcome the lack of ground-truth (which would be subjective and costly to collect).- comparison of the consensus results with several architectures, and consensus vs ground truth (via mAP).Points on which the paper can be improved:1. Clarity of description of the method - there is no clear formal definition of how the committee voting works; adding formulas would improve readability.- no explanation on how RBF is used to address the dimensionality differences when comparing scores; - it is difficult to understand from the paper how the numbers (interpretability scores) are obtained (Sec. 3.3.); please add equations and reference them in the text - especially for correlation score and aggregating scores.- please clarify how the significance tests were performed- Since the figures are cluttered, it might be worth considering reporting the per CNN scores in a table (few models in the main paper) and the rest in the appendix. 2. Experimental comparisons with other methods - the proposed method compares on object domain only, ImageNet and CUB200-2001 (birds). The method compares with Network Dissection (Bau et al, 2017), but no results on Broden dataset are reported. This dataset is particularly interesting, as it is unifying several datasets, spread across multiple domains (objects, scenes, objet parts, textures and materials).- Why is it necessary to have the models trained from scratch? (Sec. 2, committee). What would change if the models would be just finetuned?- What does the current method bring in addition to LIME and SmoothGrad aggregated over a large number of architectures?Datasets as MS-COCO / LVIS provide ground truth labels for object segmentation, which are more diverse than CUB / ImageNet.3. Style / visuals:- axis labels and text on the plots are too small;  pick a smaller number of models, the figures are too cluttered. The full comparison could be in tables, in appendix / supplementary material.- significance tests instead of significant tests.- are the 4 decimals in the Pearson coefficient necessary? (Fig. 1 caption) - please report the exact p-value.- Fig. 5 - please overlay the contour of ground truth over the other visualizations.The overall idea is simple - ensemble of CNN architectures, to generate quasi-ground-truth for interpretability. Not clear what "interpretability" is -- and how this would be different from object segmentation masks.The paper would greatly benefit from a more formal explanation of how the models are scored, and how the consensus is computed.It would add value to emphasize more how the proposed method could leverage existing trained models for datasets on which such labels are non-existent;It would also prove the effectiveness of the method, if at least one more datapoint would be reported, on datasets with object labels (e.g. MS-COCO / LVIS). **Summary**  This paper presents a method of knowledge distillation, which showed better results than KD and RCO in experimental results. The difference between this method, ProKT and RCO is in the teacher model. In this method, the snapshot interval of the teacher model targeted by the student is more frequent (every training step) than the RCO (authors called this property as *continuous*), and the parameters of the teacher model are also updated during *student* training (*dynamic*).**Strong points**- The proposed method was evaluated for two modalities (visual and textual). As far as I know, most KD papers evaluate only one modality (image classifier or language model). Experimenting with both modalities increases the experimental robustness of ProKT.**Weak points**- This method is not traditional KD, and the way the teacher model learns simultaneously with the student is called mutual learning, as the author briefly mentioned in the paper. There is a terminological problem that arises because of this: if the teacher model is also trained with the student, then neither model can be called a "teacher" or "student". Thus I think that most of the terms used in the paper need to be rewritten based on mutual learning terminology.- In advance to the previous point, sharing knowledge between the big models and small models during the training is not new. To name a few, see [1, 2, 3].- Aside from everything, the performance improvement that ProKT alone brings seems limited compared to CRD, and it is difficult to trust because there is no information such as the number of runs or standard deviation in the results of the experiment.[1] Zhang, Ying, et al. "Deep mutual learning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.  [2] Anil, Rohan, et al. "Large scale distributed neural network training through online distillation." arXiv preprint arXiv:1804.03235 (2018).  [3] Park, Wonpyo, et al. "Diversified Mutual Learning for Deep Metric Learning." arXiv preprint arXiv:2009.04170 (2020).  **Recommendation**I vote for rejection as the paper lacks in novelty and results in small performance improvement.**Supporting arguments for the recommendation**The recommendation is based on what was said in the weakness section. In addition, RCO does not provide any information from the student to the teacher during the training (just as Hinton KD or CRD does), so for a fair comparison, it seems necessary to compare the performance with RCO by separating only *continuous* facet among the two properties of ProKT (*continuous* and *dynamic*) claimed by the authors. This paper investigates if neural network parameters, trained for a standard task such as image recognition, can be used as a cover medium in steganography. The authors contend that neural networks are a good choice of cover medium, mainly because the less important fractional bits of parameters follow a uniform distribution. This is empirically demonstrated in Figure 1 where the probability of least significant bits taking values 0 or 1 are 50% for VGG, ResNet, DenseNet (I assume these are ImageNet models judging from later experiments). The authors experiment with replacing either the least or most significant bits from the fractional part of parameter values. The motivation behind replacing the most significant bits is that it will be more difficult for an attacker to remove secret information; bits of lower significance could be removed or replaced with little effect on test accuracy, while removing more significant fractional bits will have a larger impact on accuracy. The downside of this approach is that replacing more important fractional bits with secrets will also have a larger impact on accuracy, but the authors show fine-tuning can somewhat alleviate this drawback. The authors then show simple MLPs trained to distinguish between standard and stego networks are only slightly better than a random coin flip. Overall, I thought the paper was generally well-written and contains some interesting ideas, but these positives are also accompanied by an unclear motivation, lack of positioning with respect to related work, and poor experimental evaluation.My primary concern is the practical motivation for this idea. In steganography, it is normally assumed that both the cover and steg image/audio/network cannot both be revealed, otherwise a trivial comparison will reveal secret information is present thus breaking security of the scheme. Does this mean a new model needs to be trained each time two parties want to communicate? If so, this seems to represent a serious limitation, since training these models requires a non-trivial amount of effort, in comparison to say, generating an image or audio file as the cover. Furthermore, a problematic scaling law then appears, where larger models need to be trained (on presumably more complex tasks to recover some plausible deniability e.g. it would be suspicious to see a ResNet-50 trained on MNIST), to hide larger messages. These larger models are much more expensive to train, and again generating high-resolution images is a much cheaper option. Sharing large resolution images will generally consume less memory than very large neural networks. I wonder how the authors view the practicality of this work? It would have been great to compare this scheme with some standard steganography schemes on images or audio across desideratum such as bandwidth, robustness etc.The authors may not be aware, but there has been work on information hiding in previous work. Song et al. (2017) [1] also investigate how information can be imperceptibly embedded and recovered from neural networks. In addition to experiments on embedding secrets in fractional bits, they have experiments in black-box settings. Could the authors comment on relationship between this work and Song et al. (2017) [1]? As far as I can tell, they are quite similar ideas.I was a little disappointed that the authors didn't include any practical demonstrations, such as successful recovery of secrets as shown in Song et al. (2017) [1]? Steganographic scheme are evaluated by security, capacity and robustness, as the authors comment in Section 2.1, but most of the experiments in section 4 concentrate on capacity. How does the scheme stand-up against robustness attacks such as down-stream task fine-tuning, parameter pruning etc.? I was also confused by the experiment set-up in Section 4.4. For a fair experiment of distinguishability, I had expected a large number of cover and stego models to be trained (for some notion of confidence), and then use a steganalysis tool to distinguish between the two. I'm not sure training a steganalysis model on a single stego network allows for a fair interpretation of performance. [1] Song, Congzheng, Thomas Ristenpart, and Vitaly Shmatikov. "Machine learning models that remember too much." Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017. This paper aims at studying an optimized way of collecting samples from an environment, discarding the ones for which the accuracy of the observation is high. This way the agent focuses on collecting only the samples that improve the knowledge of the state space.This paper could be presented better, as the motivations of the work and the description of the method lack clarity and effectiveness. First, the title is somewhat misleading, as we cannot say that the agent is "learning to observe", which can remind something more related to feature extraction in representation learning. Indeed, the agent is learning to explore states under a certain criterion, i.e. minimizing the accuracy of the observation, closely reminding all the literature about intrinsically motivated exploration, that in this paper is only cited in the related works. After all, it looks to me that this paper is exclusively proposing a form of intrinsic reward, but it fails to explain it thoroughly. In particular, only a small subsection, namely 3.4, is dedicated to this description, moreover referring to "reward shaping", which is not the same concept as intrinsic motivation. The experimental section is weak, as it only analyses two simple RL problems, more problematically not comparing with any method in the literature.Pros------* The paper addresses an interesting problem that can potentially improve sample-efficiency in deep RL problems.Cons-------* Poor description of the methodology, in particular explaining its connection with intrinsic motivation;* No deep RL problems considered;* No comparisons with methods in literature.I recommend the authors to substantially restructure the paper to include a better analysis of how their method compares with intrinsic motivation, include deep RL problems where the problem of exploration and accuracy of observations is more accentuated, and add comparisons with representative baselines, e.g. Pathak et al (2017), Bellemare et al (2016), etc.. The paper provides theoretical analysis and numerical experiments to characterize the structure of hidden layers and a set of optimal solutions. After warm-up with two-layer neural networks, the paper provides main theoretical results for deep linear networks and deep ReLU networks. Finally, numerical results are presented to verify theoretical analysis. The paper cites the previous results in the literature of gradient dynamics of deep linear networks, and compare the contributions with this paper. However, the comparison is not done appropriately in the following sense. These previous results analyze the gradient dynamics of deep linear networks, whereas this paper analyses the set of optimal solutions. The set of optimal solutions is more well studied in the literature of loss landscapes of deep linear networks, where stationary points, saddle points as well as the set of optimal solutions are well studied. What this paper proves would be interesting if these results apply for the characterization of stationary points, saddle points, local minima, or gradient dynamics, instead of the set of globally optimal solutions. This is because, in terms of global optimal solutions, deep linear networks and linear models are the same. Therefore, these results about global optimal solutions in this paper are trivial, whereas the results would be interesting if these are about stationary points, saddle points, or gradient dynamics. For example, alignment of the layers through gradient dynamics is studied in the previous work https://arxiv.org/abs/1810.02032, which is interesting because it is implicitly done by gradient dynamics. However, in the present paper, the alignment of the layers is explicitly imposed by the constraints or the regularization on the norm of each layers weight. This is trivial since with this constraints, the norm is minimized with the alignment, of course. There is no need to use duality, which is superficial for this type of results and is unnecessary. The results on deep ReLU networks are also trivial because of the unrealistic assumptions on the data. For Theorem 4.1., the author assumes that X = c a_0^T, where each entry of c is nonnegative. This assumption makes ReLU to be trivial by the following observation. For any vector v in R^d, (c v^T)_+ = (\diag(c) V)_+ where V = [v & v]^T, which is a n by d matrix. Because c is nonnegative, (\diag(c) V)_+ = \diag(c) (V)_+. Therefore, (c v^T)_+ = \diag(c) (V)_+ = \diag(c) V^+ where V^+ = [(v)_+ & (v)_+]^T. This means that the activation (on or off for ReLU nonlinearity) is exactly same for all data (over the dimension of n). This makes the results trivial for optimal solutions. Again, this setting would be interesting if the results are about gradient descent dynamics or loss surface, instead of optimal solutions. In that case, the activation is still the same for all data in this setting, but changes during time or vary in loss surface. For Theorem 4.2, the assumption on XX^T = I_n makes the result trivial. In the theorem, the optimal solution essentially memorizes the label at the first layer. Again this might potentially be interesting if this is about stationary points, gradient descent or local minima, because this is nontrivial for those points. But, for the optimal solution with XX^T = I_n, this is trivial. This paper does not consider the case without XX^T = I_n. But, even if it considers the case, if d >= n, then optimal solution is trivial, because we can memories the label at the first lawyer in a similar way. The assumption of XX^T = I_n is a strictly stronger version of the assumption of d >= n. Therefore, the results on deep ReLU networks are also trivial.I would recommend the authors to carefully read the papers in the literature of implicit regularization versus explicit regularization, and in the literature of gradient dynamics and loss landscape of deep linear networks. Those papers are motivated to study the problem beyond the optimal solutions. Understanding optimal solutions is also interesting but not in the same or similar setting and the setting where optimal solutions are trivial. For example, characterizing optimal solutions would be interesting for deep ReLU networks if we cannot easily construct optimal solutions, which is the not case in the setting of this paper as I described above. Summary:The paper presents a system for collaborative training where two central servers compute the model update using two-party computation. There are two variants, one where the update are not checked, and one where the server use algorithm (Aggr) to weight the updates such that outliers are excluded.Pros:The basic ideas are presented succinctly, and the overall setup solves a relevant problem.Cons:- The main issue I have is that the authors claim in Section 4.3 that they "can" use full-precision (real) values in the computation but the underlying techniques (secret sharing and Beaver triples) have only been proposed for integer/quantized values. In particular, if the actual value is small, adding a large random value will override it due to rounding in floating-point representation. I cannot find any treatment of this issue or any reference to works to have tackled floating-point secure computation such as Aliasgari et al., NDSS '13.- Similarly, the claim that the paper does not rely on cryptographic primitives seems exaggerated given the use of secret sharing and Beaver triples.- The cost of generating Beaver triples is ignored. The paper does not estimate how many are needed.- Claiming Byzantine robustness seems too strong when the two central servers have to be semi-honest.Conclusion:I recommend rejection because of the irrealistic approach regarding real-valued computation. The authors should either present credible secure floating-point computation or change their claim to quantized computation.Minor issues:3.1: x_i +- \xi_i is not an additive secret sharing of x_i but 2*x_i.3.1: sum of (the) other share3.1: "sum of other share" should be \sum *p_i* x_i^(2)?3.1: It is not surprising that S2 does not learn anything in secure computation.3.3: In particular, (unfinished sentence)3.3: don't face4.2: vetor (twice)6: median and trimmed-mean -based (odd positioning of dash) This paper proposes a neighbor search method for negative sampling in extreme classification. The core idea is to use hash tables to select neighbors of a query.I agree with the authors that:i) The proposed hashing method can quickly retrieve neighbors for arbitrary queries in nearly constant time.ii) Selecting hash functions is important for potential gain.However, I think there are still missing pieces in this paper that need to clarify:i) How do you calibrate the negative sampling results?ii) How do you tune K and L in your experiments. To my understanding, using either too large or too small K and L is suboptimal in your setup.iii) You mentioned that you are using CPU or computation. But the CPU and GPU precisions may contribute to the final result. Have you studied that in your experiments?There are still some points that I disagree with:i) There is no consensus that whether easy negatives or hard negatives should be selected in extreme classifications. For a recent survey on this topic, see Embedding-based retrieval in Facebook search by J. Huang et al. So I wonder if the hashing algorithm has the capacity to choose both easy and hard negatives?ii) You claim you are using an O(1) algorithm independent of negative classes N. But this is simply not true and not reasonable because your K and L may need to increase when N grows in order to keep a constant collision probability.iii) You mentioned you propose two schemes: LSH Embedding and LSH Label. But those two methods only differ in query representations. It would be interesting to compare these two carefully in experiments.Overall, I think this paper over-claim its contribution even though it proposes a tractable solution to extreme classification. This paper focuses on the problem of multi-agent reinforcement learning (MARL) for CTDE scenario which is well studied in recent literature.  The work discusses shortcomings of actor-critic methods for MARL and proposes a solution using linearly factored critic. The paper is somewhat difficult to read and can be made better by deferring the details about previous methods to appendix. However my main concern is with the problem of centralized-decentralized mismatch (CDM) motivated in the paper and its proposed solution itself. 1. How exactly is a regular critic bad? As such a critic is supposed to be "true" to the policy, the requirement of decentralization has little bearing on the variance of policy gradients. Gradient noise increases with number of agents irrespective of whether there is centralized or decentralized execution. 2. The so called problem of CDM seems rather redundant (see 1 above), for example the authors say in page 3, 3rd para from bottom in line 3 that if the critic expectation under policy is negative, then individual policy performance is hurt. Such problem can easily be fixed using baselines, see Sutton and Barto, 2018 for example.3. How is a linear factored critic compatible with an arbitrary joint policy? In general this not true and requires many strong assumptions, see for ex. Bhatnagar, 2009. While the authors acknowledge this, bypassing the actual complexity for modelling a joint critic with a linear one will in general render it insufficient to model inter-agent interactions. This puts into serious question, whether coordination is required in the experiment domains in the first place and if the performance improvement is just coming due to a biased but albeit easier to learn critic. 4. There are some unsupported claims which need better explanation like "This becomes problematic because a negative feedback loop is created, in which the joint critic is affected by the suboptimality of agent i, which disturbs policy updates of other agents" How is that so? The updates in principle can affect the policies of already suboptimal agents, which might fix them?5. "Learning the decomposed critic implicitly realizes multi-agent credit assignment, because the individual critic provides creditinformation for each agent to improve its policy in the direction of increasing the global expected return" again how so? claims like this need to be well supported.6. Expectations are usually sampled so in principle even the $O(|A|^n)$ can be estimated with fewer samples incurring some variance, it might not be necessary to bias the critic drastically for this.7. The authors need to shed more light on when the precondition $Q_i(\tau, a_i) > Q_i(\tau, a_i') \iff \beta_{a_i, \tau}\geq \beta_{a_i', \tau}$ in Prop. 1 holds beyond tabular settings. It seems a rather strong assumption to hold for all trajectory and $O(|A|^n)$ inputs. Right now it seems rather grab bag to show policy improvement.8. Why isn't comparison on SC2 done against more recent baselines like QTRAN, MAVEN, ROMA etc.? High-level view:I dont think this is necessarily a bad paper, but I think its unacceptable for ICLR in its current form. I currently lean heavily toward rejection. After thinking over the concepts in the paper more, I might lean more strongly toward rejection or toward acceptance (if the authors can address the issues I raise below). I provide details below examination of how Ive come to my evaluation rating below.Summary:This paper principally focuses on the idea of decompilation. Decompilation can mean many things, but the general idea as I understand it, is to take a representation of a software program from one level (e.g., program binary) and then lift it to a level that is higher in abstraction (e.g., from binary to assembly, from assembly to C, from C to a lambda calculus, etc.). As I understand it, its called decompilation because it tends to do the opposite of what a compiler does. Compilers tend to lower a representation of a software program into something that is closer to the hardware and therefore potentially more efficient. The benefits of decompilation are numerous. One major benefit is in the ability to perform programming language  to  programming language transformation. Another, which is the focus of this paper, is for reverse engineering purposes of a binary. There are many others. As such, in my opinion this is unquestionably an important subtopic for the field of machine programming and the authors approach also seems satisfactory to me for ICLR (described below).The authors present a new approach called: neural-based binary reverse engineering framework (N-Bref). N-Bref has a number of components that it relies on to perform its decompilation. They consist mostly of components from the programming languages community (e.g., assembly code, abstract syntax trees for encoding and decoding, etc.) and the machine learning community (e.g., deep neural networks for learning structural transformations, etc.).The authors empirically evaluate their N-Brefs accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high-level source and low-level source which are broken into training (60%), validation (20%), and testing (20%). LeetCode problems tend to be fairly simple, self-contained, and, to my knowledge, are coding problems that are meant to help train new programmers or prepare software developers for coding interviews, amongst other things. An emerging use of LeetCode is to use it as a baseline for machine programming (MP) in a variety of different ways. In this case, the authors are using LeetCode coded solutions in MP to compiled the source code into a lower level form (assembly I believe) and then see if N-Bref can return the assembly back to the original form or some semantically equivalent form. Their empirical approach seems sound to me.Overall, the authors show better accuracy for their tested problem set against REWARD, a baseline system (a transformer), lang2logic, and Ins2AST across two dimensions: data type recovery and abstract syntax tree (AST) generation.High-level concerns:There are several reasons Im not positive about this paper. Perhaps the biggest reason is I cant seem to understand what is novel about the system. That is, unless Ive just missed something, it seems that all of the core components of N-Bref are lifted from prior work with perhaps some minor augmentation. This feels largely incremental to me.On the other hand, one could argue that N-Bref is novel because it combines a number of existing components in a unique way to achieve better performance that prior work. I can see this perspective. However, if we considered this view, it seems like the problem they are solving should be more impactful than type recovery and AST generation. Im not saying these problems arent important  especially type recovery (I think this problem is deeply important)  but that it should go further to demonstrate more dimensions of decompilation.The second major concern I have with this paper is the small dataset they are using. Consider, for a moment, that they are using only 25,000 input/output pairs for their training/validation/testing. Now consider a prior accepted ICLR 2020 paper, Hoppity (Dinella et al.), which trained on nearly 300k code change commits in GitHub. This looks like an order of magnitude difference in dataset empirical evaluation to me. On top of that, the only data is coming from LeetCode. We have no empirical demonstration that this approach will work on other datasets outside of LeetCode.If the authors can address these two primary concerns by the time of decisions, I will likely slant toward the positive. If they do not (or will not), I will likely champion this papers rejection, as I do not believe in its current form its up to ICLR standards.Low-level concerns:The language in the paper seems to use many strong and ambiguous claims: N-Bref outperforms previous neural-based decompilers by a large margin. First of all, what is a large margin? Theres not quantitative measurement in the word large. Large could mean 1%, 10%, 100,000%. This kind of language is not what I expect from tier-1 publications.Another example is: However, key challenges remain: where they then summarize two problems. I agree that the two problems they highlight are important. But I absolutely do not agree that those are the *only* two problems that stand in the way of decompilation.Also, there seems to be some lack of understanding of the field of machine programming, from my perspective. For example in the abstract the authors claim decompilation aims to reverse engineer binary executables. I 100% disagree with this definition. As I stated above, I believe, the more general space of decompilation is actually the idea of lifting a software program representation from one format to a higher-level format that increases the level of abstraction from the hardware. Moreover, I know of many decompilation systems (e.g., verified lifting is one), that has an entirely different goal than reverse engineering. Verified lifting is principally focused, as I understand it, is focused on language to language translation.Perhaps the grossest overclaim the authors make is in the introduction Our work pushes the performance of neural-based decompiler to a new level and presents a new baseline and stand dataset for future developers. I find that sentence simply unacceptable. I could never give an accept rating to a paper that makes such an outlandish claim with such a small body of evidence. Moreover, other people have used LeetCode as a baseline, so its not the first time people have done this. So it seems wrong to me on many levels.This continues throughout the paper &That said, these are minor nits that the authors, if they so choose, could probably fix with little effort.I would hope that in a later version of the paper the authors would tone the language down, move away from the number of strong claims they make in the paper, and provide measurable data points when making claims about performance: Our system is more accurate than <list the systems youre comparing against> from X% to Y%. Right now the only way to figure that out seems to be to deeply study the experimental evaluation, which is a bit inappropriate in my opinion. I believe it could (and should) be listed directly in the abstract and in the introduction. By hiding these details, it creates a perception of overclaiming  at least it did for me. The paper proposes a new defense against adversarial examples created through query based attacks. The defense is based on adding a small amount of random noise. The authors provide some theoretical arguments and experimental evaluation of their approach.Overall evaluationWhile the presented idea might be promising neither the theoretical nor the experimental evaluation are sufficient to validate this claim. The research on adversarial robustness is very active and many attacks and defenses have been proposed. To ensure good quality of research, proposals have been made on how to correctly evaluate a proposed defense (see below). The paper falls short of these standards. The main shortcoming is the lack of a robust evaluation against an adaptive attack (i.e. an attacker aware of the defense). The majority of the analysis in the paper is restricted to evaluating  the defense against existing attacks, robustness against an attacker that does not take the defense into account is insufficient. The paper provides a short analysis of an adaptive attack, however, this new proposed attack is just a slight variation of an existing attack. This type of defense against a "patched" attack is still insufficient. Especially because from the paper it is unclear why this specific attack was chosen, if it is optimal and if it addresses the shortcomings of the original attack. One clear oversight is that the objective of the attacker changes, as it is not looking for a point that gets misclassified, but for a point that gets misclassified even under noise. The shortcoming of the analysis goes beyond the lack described above and I recommend the authors to go through the given guidelines.ReferencesCarlini, Nicholas, et al. "On evaluating adversarial robustness." arXiv preprint arXiv:1902.06705 (2019).Athalye, Anish, Nicholas Carlini, and David Wagner. "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples." arXiv preprint arXiv:1802.00420 (2018).Concrete comments about the unclarity"the above observation implies that clean images have a relatively long distance to the decision boundary" -- the existence and prevalence of adversarial examples implies exactly the opposite of this statement, if this would be true there would not be any adversarial examples. Section 3.2 is very unclear. eta which is defined in Section 3.1. a independent of x is suddenly dependent on x. The sentence "this function is very noisy" is just a claim. It is unclear why the random function (?!) l* becomes discontinuous for large Var. Section 3.3 is just saying that existing attacks don't take into account the newly proposed defense.The analysis in Section 3.4 is for a quadratic function. Most current CNN architectures don't have any quadratic component and in fact the Resnet architecture used in the experiments is a piecewise linear function with a final softmax layer. I fail to see the relevance of the presented argument. This paper proposes an adversarial defense method against black-box attack by adding random noise on adversarial examples. The method is simple and empirical results verify that the proposed defense can be used to decrease the attack success rate of both optimization-based and local search-based attacks. My main concern of this paper is the simply use of randomness for adversarial defense, rather than directly improving the robustness of the target model itself (e.g. adversarial training). In the discussion about the Obfuscated Gradient [1], the authors criticized the randomness-based defense method: Stochastic Gradients are caused by randomized defenses, where either the network itself is randomized or the input is randomized before being fed to the classifier. Although this paper is aimed at defending against black-box attacks, it is still essentially a defense by adding random noise to the input data. In addition, the proposed method is not useful to defense against white-box attacks and transfer-based attacks, since basically it does not change the predictive model.Another problem is that while the defense mechanism in this paper is simple, there are equally simple ways to circumvent it. An example is that when the randomness of model output is found by the adversary based on Boundary Attack, the attacker only needs to change the judgment condition of successful attack to most of the attacks are successful in the same input for multiple queries to avoid the decrease of attack success rate.[1] https://arxiv.org/abs/1802.00420 This paper proposes to obtain multi-scale features by `resize -> convolution -> resize (inverse). Extensive experimental results on COCO validate the effectiveness of the proposed approach. Pros:Experimental results on the widely used benchmarking dataset.Comparison with recent approaches.Paper is easy to understand, simply because the proposed method is simple.Cons:Lack of novelty. The use of multi-scale features is not new [a, b]. The formulation in this paper is not very different from that in [b]. It has been used for lots of applications. In object detection, dilated conv in TridentNet is not the only approach using multiple branches. Approaches like [c, d] also used multi-scale-multi-layer features by resizing. [a] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Spatial pyramid pooling in deep convolutional networks for visual recognition." IEEE transactions on pattern analysis and machine intelligence 37, no. 9 (2015): 1904-1916.[b] Yang, Wei, Shuang Li, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. "Learning feature pyramids for human pose estimation." In proceedings of the IEEE international conference on computer vision, pp. 1281-1290. 2017.[c] Gidaris, Spyros, and Nikos Komodakis. "Object detection via a multi-region and semantic segmentation-aware cnn model." In Proceedings of the IEEE international conference on computer vision, pp. 1134-1142. 2015.[d] Zeng, Xingyu, Wanli Ouyang, Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu et al. "Crafting gbd-net for object detection." IEEE transactions on pattern analysis and machine intelligence 40, no. 9 (2017): 2109-2123.The ablation study in the experimental results did not compare with existing works, like TridentNet, and [c, d] to justify why another multi-scale approach is needed.Symbols are not illustrated well (Authors need not answer this in the rebuttal but need to revise in the revised version).`S() is the squeeze module: What is the meaning of squeeze module?There are no common definitions on `makes the input x thinner, `combination function, `unsqueeze module. This paper proposes a new general feature fusion operation, Multi-Scale Fusion Module (MSFM). By adding MSFM layers between feature extraction layers, it is observed that the detection result is improved with minor added parameters.Pros:Good to see new work to explore various of ways to perform feature fusion.Cons:Major comments:(1) The quality and clarify of the paper needs to be improved, for example, Table 3 has shifted horizontal line. (2) Ablation studies clarifies on the effects of the change of configurations, but does provide much evidence on why MSFM module helps with the detection task.Some minor comments:(1) Figure1a. could be further clarified by adding the notations mentioned in the equations to to the figure. (2) Good to report the variances/confidence-intervals of the metrics as well. The paper proposes a multi-scale feature fusion block and inserts the block into ResNet backbones for object detection. It is very similar to the inception block in IneceptionNets. The only difference is the proposed feature fusion contains feature map upsampling and downsampling (resize and resize^{-1}) for different branches. The paper has some merits as follows.1. The method has evaluated on different object detection frameworks, such as Faster R-CNN, Cascade R-CNN, Grid R-CNN, Dynamic R-CNN, RepPoints, etc.2. The method obvious performance gains on different frameworks with a few additional parameters.However, the flaws are obvious as follows.1. The novelty is very limited. Inception block is widely used in deep learning. The paper is only a small modification over Inception. The novelty is far below the bar of ICLR.2. The proposed feature fusion block is very computation expensive. The upsampling procedure makes the computation cost of the 3$\times$3 convolutions very high. The paper only reports additional parameters but does not report the additional computation cost (e.g., additional FLOPs and running time).3. Related methods are not compared. At least, inserting inception blocks into ResNets can be compared. The paper proposes creating diverse ensembles of neural networks using an evolutionary method for finding base learners with high performance as well as mutual diversity. The selected base learners are then aggregated for an ensemble using a known method for ensemble selection. The paper is generally well written and addresses a relevant problem of constructing ensembles while training neural networks instead of building models first, independently, and later constructing ensembles. Having said that, the paper lacks a significant contribution. The second phase (Ensemble Selection) of the proposed method is essentially the algorithm from Rich Caruana et al. 2004. The first phase of Pool Building suggests either a random generation or alternatively a vaguely described evolutionary method, lacking details or analysis.  It is not clear how exactly is the random initialization of architectures performed. Do you randomly select from a set of seed architectures or randomly create (i.e., random number of layers, random number of units, random activation functions, random initial weights, etc.)?Growing from a random neural architectures to multiple highly performing (besides being mutually diverse) through single permutations upon model training and evaluation seems like an expensive process. An evolutionary approach in such a manner seems in efficient. Can you report the time taken for some of the reported cases in the evaluation? The ensemble methodology of unweighted averaging is fairly naive. What was the reason to select this one particularly? Contribution #1 (page 2) isn't really a contribution. It is common knowledge amongst practitioners. The proposed method can lead to overfitting because the search seems to be based on a fixed set for evaluation. Regarding evaluation -- Can you explain how that is addressed? Have you evaluated your method on a broader variety of datasets? Can you confirm that the test data used for search/optimization is different than the one used for measuring reported performance? Did you consider comparing it other methods such as Tao 2019 (mentioned below) ?This paper can improve its literature survey by citing more directly relevant work in ensemble search using diversification. Here are few examples of more sophisticated ensemble evolution work, not necessarily for a DL base learner, but relevant nonetheless:-Bhowan, et al. 2013. Evolving diverse ensembles using genetic programming for classification with unbalanced data. Trans. Evol. Comp-Khurana et al. 2018. Ensembles with Automated Feature Engineering. AutoML at ICML.-Olson et al. 2019. TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning. Automated ML. -Tao, 2019. Deep Neural Network Ensembles. Machine Learning, Optimization, and Data Science. -Yao et al., 2008. Evolving artificial neural network ensembles, in IEEE Computational Intelligence MagazineOverall, it is a good problem, but this paper falls well short of the threshold. Summary:The paper focuses on the computation of Nash equilibrium in a Multi agent setting, with a very large number of agents. This leads to the consideration of reinforcement leaning algorithms for mean field games (i.e. with an infinite number of agents) and obtain the convergence of a single loop fictititious play algorithm to the Nash equilibrium of entropy regularized mean field games.  Strength of the paper:- The connection between the derived mathematical property and the very interesting problem of interest, i.e. learning in a multi agent setting with numerous agents is well motivated.  - The paper is well written and technically sound. The mathematical results provided are technical, difficult and are presented in a pedagogic mannerWeakness of the paper:- The convergence property relies on a very strong and a priori unrealistic assumption on the Lipschitz coefficients os the distribution <-> policy mappings: d_1d_2+d_3<1. Authors should justify when this assumption can indeed be satisfied and directly read on underlying Mean Field Game Structure. - No numerical experiments are presented to verify the convergence of the algorithm and compare it to alternative methods in the literature. Recommandation; Given the scope of the paper in comparison to the one of the ICLR conference, together with the weaknesses detailed above, I recommend to reject the paper. Questions during the rebuttal period:1. Can you provide reasonable sufficient conditions on the mean field game dynamics ensuring that the assumption d_1d_2+d_3<1 is satisfied? Even though it already appeared in previous papers, it is really very strong stated as such. 2. Can you provide numerical experiments confirming the theoretical findings of the paper?3. Focusing on penalized MFGs, the role of the penalization coefficient \lambda seems to be of particular interest, and a necessity in your approach. Is the dependence of the NE (policy and or state distribution) clear with respect to \lambda? What are the practical implications of the choice of \lambda? 4. How does your approach compare to the 2-time scale approach provided by Angiuli et al. (Unified Reinforcement Q-Learning for Mean Field Game and ControlProblems) in terms of single/multiple loops? # SummaryThe paper presents a study of using machine learning methods to calibrate a radio telescope using information from sensor data on, e.g., atmospheric conditions. The authors consider tree- and neighbourhood-based methods for predicting amplitudes and phases for seven antennas. The results show that the methods perform quite well in terms of RMSE and explained variance.# Evaluation## Strong pointsIt is a timely and interesting application, and it is clear that the field of astronomy holds lots of potential for machine learning applications.## Weak points* From a machine learning point of view, the methods are very simple.* The evaluation overall is confusing and somewhat weak. For instance, it is unclear to me whether the competing method, CASA, can be considered as the ground-truth or as a method to outperform.* The evaluation based on figures 4 and 5 seem to be done qualitatively.## RecommendationRejection.The main reason for my recommendation is that ICLR is not the right venue for this paper. It does not deal with representation learning, and the methods are too basic for a top-tier machine learning conference in general.In addition, the experimental evaluation, which is the main body of the paper, is quite weak and will need significant improvements.## Detailed feedbackThe methods considered in the paper (k-NN and tree-based methods) all base their predictions directly on the input features; thus, no representation learning is happening. Thus ICLR is not the right venue, and I doubt the paper would be accepted to a top-tier machine learning conference since it only considers (quite simple) off-the-shelf methods. KDD might be a better fit or an astronomical journal such as PASP.Furthermore, there seems to be some confusion with regards to what a multi-output regression model is. As described in Borchani et al. (2015), multi-output regression methods also make use of correlations between outputs. It seems the authors are aware of this, as they cite the models' ability to make use of the 'relationships between the targets', which I assume refers to modelling correlations between the output dimensions. However, this is not how the methods used in the paper work. For both k-NN and tree-based methods, there is a clear separation between inputs and outputs, so the paper does not deal with multi-output regression either. This is not a problem per se, however, as it is not clear that a multi-output model would be better suited for the task considered here.The paper completely lacks a section on prior work - both regarding related uses of machine learning and different ways to perform calibration.Regarding the methods, and given that the predictions seem to be part of a time-series, I am not convinced the chosen ones are the most suited for the tasks. It would make sense to test methods from signal processing as well, for instance Gaussian processes or a recurrent neural network like an LSTM.There are also no specific details on how the methods in the paper are constructed. Do they only use the instrumental and environmental sensor data as inputs, or do they use the temporal dimension in some way? Which hyperparameters were adjusted, and how were they selected?There is very little information regarding the experimental setup. How large were the training and test sets? What are the dimensionalities of the inputs and outputs? It seems like only one source was considered; why is that? Only testing one source seems brittle. Similarly, was cross-validation used? If not, why? If it was, we need details on how it was set up and what it was used for (hyperparameter tuning, experimental evaluation, etc.).For the experimental evaluation, I am confused about whether CASA is considered the ground-truth to compare with, or whether it is a method to outperform. Regardless, would it be possible to evaluate both CASA and your method on simulations to get an idea about how well each performs?It also confuses me that antenna 5 is referred to as 'the reference antenna'. In what way was it used as a reference?Further to the experimental evaluation, it would make sense to include the performance of a baseline or two, such as linear regression. I would also like to see error bars in figure 3, e.g., from cross-validation.Since the paper deals with quite simple methods, I would expect some analysis of why they work and when they might fail. In figure 3, especially for the amplitude, some antennas have dramatically larger errors. What happens here? Also, which of inputs were most informative for predicting the phase and amplitude? You could get a feeling for the importances by, e.g., training the models on only one feature, or all features expect one, and repeating this for all features in turn.# Questions1. Why did you choose these particular machine learning methods?2. How did you determine which method was the best? From the plots in figure 3, it looks like k-NN is slightly better overall.3. Did you perform any cross-validation? If so, where are the error bars in figure 3? If not, why not?  1. You mention 'large rms error bars in each model per antenna 3H and 4H'. Which error bars are you referring to?4. Are the RMSEs in Figure 3 wrt. the CASA estimations?  1. Also, to me (with no background in radio astronomy, though), it seems like a phase error of 0.3-0.4 is quite large (assuming one phase is 2\pi). How large errors can we expect from CASA?# Suggestions for improvement* Evaluate more than one calibration source. Without more sources, we can't be sure if your method performs consistently well.* Figures 4 and 5: it might be useful to see instead residuals between your method and what you consider ground-truth. This paper studied the memorization issue of generative modeling. It proposed a benchmark for a public generative modeling competition and observed how participants attempted to game the FID. It seems the paper only targeted FID. Actually in GAN evaluation, FID is not the only metric that are widely used. In addition, I doubt how the proposed competition can show the impact of memorization. Also, it is not clear how MiFID really evaulates the generilization ability. In addition, the title mentioning generative modeling is overclaimed (only GANs). The writting of the paper is not good. The introducation spent too many paragraphes on describing the background of GANs but failed to clarify the motivation/intuiation clearly. It also used several offencive words such as "cheating". Overall, I think this work has limit impact to the community and not provided deep insights to the audience. Based on these facts, I make my rating.  Summary:--------------The paper presents a method that attacks existing out-of-distribution (OOD) detection methods. Most of the existing OOD detection methods perform detection using a latent representation. Main motivation of the paper is that the size of the latent representation is much smaller than the input images which results mapping both OOD and in-distribution images to the same place in the latent space and diminishing OOD detection performance. With this motivation, the proposed method perturbs input images to obtain an image whose latent representation is similar to the latent representation of an in-distribution image. Since such perturbations can be obtained for any OOD image, existing OOD detection algorithms fails distinguishing such OOD samples. The paper contains experiments on multiple dataset to demonstrate that the proposed method obtains a latent representation similar to the representation of an in-distribution image.Comments:----------------1 - From the abstract, I infer that the paper has roughly 3 contributions: 1) the paper shows existing OOD detection methods are practically breakable, 2) Glow likelihood-based OOD detection is ineffective, and 3) present a simple theoretical solution with guaranteed performance for OOD detection. However, the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea. I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea. Also, since this contribution is mentioned in the abstract, I would expect seeing its description and the results in the main paper rather than the Appendix.2 - Until the end of the Introduction, it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods. I liked the motivating example in Figure 1, but the contribution can be given in a more clear way.3 - In the last paragraph of page 2, it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out. I found this statement quite vague. How this operation "ensures" that x_out to be OOD after the modification?4 - In the last paragraph of Section 2.1, it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization. Is this something that you observed empirically? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima. Also, it would be interesting to show how this loss evolves during the optimization.5 - In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values. So, for an image with size 224x224x3, there are 256^(224x224x3) possible images that can be generated; not 8^(224x224x3).6 - I didn't quite understand the message in the last paragraph of Sec. 2.2. Why \Omega_in is split into \Omega_{in_clean} and \Omega_{in_noisy}? How this is used in the proposed method?7 - I think that experimental evaluations are not comprehensive enough, 7.1. It is mentioned that implementing OOD detection methods are not necessary since Alg.1 already produces z_out ~= z_in. I don't agree with this since most of the SoTA methods do not rely on only a single representation but multiple representations at different layers [ref1, ref2, ref3]. Therefore, it is crucial to show the performance loss of these methods due to the proposed method.[ref1] Erdil et al., Unsupervised out-of-distribution detection using kernel density estimation[ref2] Lee et al., A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks[ref3] Sastry et al., Detecting Out-of-Distribution Examples with Gram Matrices7.2. Benchmark datasets that have been used in OOD detection paper are usually different than the ones used in the paper. Please see the references above. I wonder why the datasets choice are different than the standard benchmarks?7.3. As mention in paragraph 3 of Sec. 4., there are other generative models that show promising results. However, I didn't quite understand comparison with these methods are "out of the scope" while Glow is relevant.8. The parameters used in different experiments are different than each other. How are these parameters determined? How sensitive is the proposed method to parameter choice?Minor comments:-------------------------1 - In Introduction paragraph 2, there is a typo: laten -> latent2 - In Sec 2.1, paragraph 3: the algorithm is referred as "above algorithm" but it appears "below" in the paper.3 - In Sec 2.2, paragraph 3, there is a typo: state-of-art -> state-of-the-art4 - In Sec 2.2, paragraph 3, there is a typo: laten -> latentOverall:-----------I found the idea of attacking existing OOD detection methods interesting. However, I believe, this paper needs improvement in terms of clarity of the presentation, experimental evaluations and the presentation of the contributions. Therefore, my initial rating is reject for this paper. Summary:The paper defines an out-of-distribution attack, a process which drives an out-of-distribution (OOD) input to have the same latent representation to an inlier. The paper also analyzes that an encoder is inevitably vulnerable to out-of-distribution attack when its latent dimensionality is smaller than the dimensionality of the input.Decision:RejectStrength:The paper addresses an important vulnerability of classifier-based OOD detection. As classifier-based method is one of the currently dominating approaches for OOD detection, investigating its weakness is a significant contribution to the research community.Weakness:The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial.The main quantitative result, Table 1, is not very convincing. I suggest the authors  provide AUC scores computed before and after OOD attack, so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance.The paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods. Currently, only a relatively simple method of Hendrycks and Gimpel, 2016 is used. The method should include at least [1,2,3] to show the effectiveness of the proposed attack. At least some of these OOD detection methods may be able to resist the proposed attack. For example, multiple hidden layer representations from a classifier are used in [1], and therefore it can still detect OOD even if a specific latent representation is under attack.The organization of the paper needs to be improved. In the last sentence of the abstract, "a simple theoretical solution" is mentioned but is only addressed in Appendix. If it is a contribution that is important enough to be mentioned in the abstract, it should be covered in depth in the main manuscript instead of Appendix.Minor comments:- The visibility of figures are poor. The axis titles and MAPE values in Figure 2, 3, 4, 5 should be larger.- In Section 2.2, 8^{224*224*3} should 256^{224*224*3}. An 8-bit integer can represent 256 values. - The captions of Figure 3 and Figure 4 are the same.- The captions of Figure 5 and Figure 6 are too close.Typos:laten  latent (Section 1 paragraph 2 line 10)Dicussion  Discussion (Section 4 title)Difficulty  difficult (Section 4 paragraph 4 first line)[1] Lee, Kimin, et al. "A simple unified framework for detecting out-of-distribution samples and adversarial attacks." Advances in Neural Information Processing Systems. 2018.  [2] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. "Enhancing the reliability of out-of-distribution image detection in neural networks." arXiv preprint arXiv:1706.02690 (2017).  [3] Grathwohl, Will, et al. "Your classifier is secretly an energy based model and you should treat it like one." arXiv preprint arXiv:1912.03263 (2019). This paper aims to provide a scalable and competitive molecular design technique based on MCTS. Machine learning guided molecular design is a popular field and many solutions have been developed in the recent years. There is appeal, due to the high dimensionality of search space, to resort to MCTS based approaches that leverage stochastic pruning of the search space as an effective way of making the problem tractable. Positives: - The innovations herein are focused on engineering a parallelizable MTCS algorithm that can explore the space much faster (though it requires much higher compute at once).  This can be a useful tool for faster computation and proposal of drug-like moleculesNegatives: - In principle the methods introduced here have little specialized domain relevance to chemical design. I.e. it's unclear why the paper is focused on chemical design and benchmarks itself there. It seems more appropriate to show that the scalability can be general to any MCTS application. As such, it might be better to apply it in spaces where MCTS is already better developed. - The paper is light on results and their discussion. Instead of spending as much space talking about how MTCS works, which is well known, the authors are better off showing more metrics related to drug-likeness (e.g. QED) or validity checks.  It is unclear if the solutions proposed are actually sensible which could overstate the performance of the method. Adding long chains of carbon to improve the score is a notorious problem for these design methods, and it does seem to be an issue for the designed molecules presented in the appendix. I'm not a chemist so I cannot validate this molecules by eye, but I am quite concerned about the lack of validity checks. - Comparison and discussion of more performant methods like Zhou et al 2019 Scientific Reports (optimization of molecules via RL). - It is also unclear if the scoring method for logP used in this study is the same as the one used by the other methods mentioned. At least it doesn't seem to match the method used in Zhou et al 2019 or others reported there. Minor commentsTable 1 boldface is confusing, it makes more sense to boldface the best performing models. ------I think the paper makes some interesting engineering contributions, but the focus on chemical design is somewhat arbitrary, and the authors don't investigate the domain application with enough depth. Especially, I'm quite concerned about validity of the designed molecules, and lack of clarity that all methods reported herein are actually run with the same scoring method (e.g. the numbers aren't simply picked up from elsewhere when the oracle was different).  If the authors are able to provide more reassurance about the validity of the design, and the soundness of the oracle they have chosen to use, I'm happy to improve my score.  This work tries to identify  local patterns that can be used to provide explainability to GNN models. Different from previous work using simple pooling function to generate graph representation, this work clusters nodes into a predefined number of clusters using their embeddings from L-hop neighbors. Each cluster produces a pooling representation. The representations of all clusters are concatenated to form the representation of the whole graph. However, the manuscript was not well written with some confusing notations and logical problems. It should be beneficial to readers if the manuscript can first clearly demonstrate problems in existing graph classification problems. Some analyses are not solid. The proposed method performed obviously worse than several baseline methods.This work assumes that similar node embeddings should have similar structural roles and thus should be clustered together. Nodes with the same local structure may have the same embeddings, but not the other way around may not be true. The structural roles are defined is based on embedding similarity, which doesnt guarantee to enhance explainability. The author(s) claimed that most GNNs were not explainable. This is not quite true. Recently, the attention mechanism was used in GNNs to enhance explainability. There exist some other works, like GNNExplainer, aiming at explainable GNNs.The baseline models compared in this paper were proposed several years ago and not mainly focused on graph readout function. More typical methods for global pooling were missing in comparison. For instance, GlobalAttention (https://arxiv.org/abs/1511.05493) is also an explainable readout function. ASAPooling (https://arxiv.org/abs/1911.07979) is a recent work that also learns a sparse soft cluster assignment for nodes in the pooling phase. In addition, the proposed method performed worse than many baseline models in many cases. For experiments on OGB datasets, it only compares with original GCN model, but there has been a lot of state-of-art models proposed several month ago, achieving much better results than this paper. You can find the latest results on https://ogb.stanford.edu/docs/leader_graphprop/. Some notations need improvements.1.Equation (2): The trainable weight matrix is missing in GCN(A, X).2.Definition 1 denotes two nodes as u and v, but the notation becomes i and j when representing their l-hop neighbors. X is first used as the feature matrix but later used to indicate node vector.Some claims are not correct.1.In Section 3.2, Embedding nodes with MP creates embeddings that are close for nodes that are structurally equivalent. When many GCN layers are stacked together, final node embeddings tend to become similar.2. When computing the distance between two graphs, if those two graphs have the same distribution of structural roles, they will have embeddings that are close.  This may be true, but the overall graph topology information is omitted. Here structural roles represent local patterns.  Title: FTSO: EFFECTIVE NAS VIA FIRST TOPOLOGY SECOND OPERATORSummary of the paper:The goal is to fit the hyper-parameters of the neural network namely topology (i.e. network architecture) and operator (e.g. skip connection or convolution) rather than just fit the parameters (i.e. weights). The approach is similar to DARTS which relaxes the architecture choice to obtain a continuous optimisation.The main difference from DARTS is that rather than jointly selecting operator and topology, first the topology and then the operator is selected. It's roughly a form of coordinate descent. The paper is empirical in nature and claims good results. Pros:The idea of optimising topology then operator appears to work well in practice. Cons:The presentation is far from top tier conference standard. Examples include, this sentence from the introduction (yes, it's one sentence): "We first mathematically prove that, by greatly shrinking the graph of the search space, reducing the operators complexity in magnitude, lowering the required searching period from 50 epochs to one iteration and significantly easing the Matthew effect, namely that the complex operators may never get the chance to be well tuned, thus the found architecture only contains very simple operators, and performs poorly on the testing set, FTSO reduces the required parameters by a factor of 0.53×108, decreases the FLOPs per iteration by a factor of 2×105 and significantly promotes the accuracy compared to the baseline, PC-DARTS."(Anyway, the "mathematical proof" aluded to is an informal argument for an example, essentially saying that searching one coordinate then the other is cheaper than searching both jointly.)Also figures 2 and 3 are unreadable on printout. Also, there is no code to check the results. This is a significant factor considering the empirical nature.Recommendation:The paper is surely interesting to some, but needs a lot more work. Summary: This paper presents a method of incorporating prior knowledge into MCTS via language, using interactive fiction games as a test bed. Their method MC-LAVE used word embeddings on the language action space to help induce a non-uniform distribution over the action space.Pros:1. The motivation is very clear, MCTS is generally action agnostic and using language to provide additional semantic information to it can prove to be very effective. The search + RL paradigm has already been shown to work well in cases like Go. The idea of using cosine similarity in word embeddings is a simple but effective way of biasing the MCTS in the right directions.2. The paper in general is well-written and easy to follow, the qualitative analysis and the additional diagrams in the appendix illustrating the variations in policies are appreciated.Cons:Some of the claims are not quite accurate even when compared to the works already cited here -1. PUCT-RL is the only directly comparable baseline given action space and other handicap differences. (i) It appears that MC-LAVE is using the valid action handicap in Jericho as a *hard constraint* (Eq. 6 and Algorithm 1) - this means that the MC-LAVE only has a search space of on average < 100 actions per step. The other baselines all use the full template-based action space (except the DRRN) of size 10^8 - a auxiliary entropy loss is used there derived from the valid actions but it is not a hard constraint. As noted in contemporary works such assumptions dramatically reduce the difficulty and language understanding capabilities of text games (Yao et al. 2020 https://arxiv.org/abs/2010.02903).(ii) The second issue is that MC-LAVE assumes that the simulator is deterministic and can conduct rollouts and reset within the span of an episode - standard planning assumptions but incompatible with all other baselines (except for MC!Q\*BERT) which do not use this handicap. (iii) Some suggested baselines that make these assumptions would be a heuristic A\* search, or modifying any of the existing algorithms to use smaller action spaces and/or apply alternative exploration strategies seen in previous works such as modular policy chaining (that MC!Q*BERT uses) or Go-Explore (Madotto et al. https://arxiv.org/abs/2001.08868)2. Even with the results that already exist, it is claimed (for example in section 5.1) that MC-LAVE-RL is the only algorithm to pass bottlenecks such as getting the action "take lantern" right. But the diagrams in the appendix for the policy the MC!Q*BERT agent learns as well as the original paper for that agent show otherwise? 3. Other concerns along these lines: all of this paper's results are averaged over 3 runs while the other baselines are over 5 runs - an indication of variance would be useful to assess whether the differences are significant, especially since some of the margins quite small (23 on MC-LAVE-RL vs 22.8 for the next best on Ludicorp) added to the fact that hyperparameters are different for each game - does that imply that the authors tuned the hyperparameters for each game? The analysis shows that it outperforms only all the other baselines only on 5/9 games and matches on 3. The abstract and intro claim state of the art across all games. I think this means that some amount of claim rewriting is required in addition to the changed baselines.Overall, I think this paper has a clear motivation and some interesting ideas on how to incorporate semantic language information into planning algorithms. However, in its current state - the comparisons made are not meaningful which makes the claim of state of the art tenuous (state of the art does not matter so much as showing that you make progress in line with the motivation). Making these comparisons would require a heavy rewrite starting from the abstract to the analysis and so I would recommend reject right now but look forward to seeing an updated version of the paper in the future with some of these changes. This paper proposes a method for generating policies in cooperative games, using a neighbourhood-based factorisation of reward, and an iterative algorithm which independently updates policies based on neighbour policies and then propagates the policy to neighbours using function space embedding.The experimental results looked promising, so there seems to be an idea here worth communicating.The paper was very hard for me to follow. I'm not an expert in the area and wouldn't expect to follow all of the reasoning in constructing the method, but I would be expect to be able to follow some clear statements of the algorithm, or its theoretical properties (guarantees of some solution quality given certain assumptions, the parameters affecting this, etc.). Instead the main body of the paper felt like a collection of pieces that were used when developing the algorithm. I would suggest it might be easier to follow if written from the top down, instead: present a high-level overview of the idea, give a (detailed!) description of the algorithm, the experiments, and leave the derivation to the appendix.Despite being in the appendix, the algorithm is less than half a page, and doesn't explain the variables. eta and kappa might be described elsewhere, but it would be helpful to reference where. J is a loss: which one?One of the claimed contributions is this is *principled* method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statemenOne of the claimed contributions is this is *principled* method. However, the exact assumptions are not clear, and the chain of issues discussed throughout section 4 seems to include discussion of approximation. What makes this principled? This would seem to need a clear statement: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in?t: what are the exact assumptions, and what precisely is the quality of the output? Is it exact? What are the complete set of parameters? Where does approximation fit in?Another claimed contribution is computational efficiency. How does the computational cost compare to the baselines in the experiments?Proposition 1: "The optimal policy has the form ... 1/Z exp(...)"I found the use of optimal slightly hard to follow throughout this. The usual definition of optimal policy would be a value maximising policy, which would be an argmax rather than a softmax. Following that definition, this proposition wouldn't be true, so it seems like it needs more explanation, or more careful wording.The cited PRL article (Levine 2018) seems to retain this standard use of optimal: it uses a distribution over trajectories with an equation similar to here (a softmax over accumulated trajectory rewards), and makes use of the property that trajectories corresponding to an optimal policy have maximum probability in that distribution.Can the authors clarify this use of optimal?Proposition 1:For clarity, explain the intention of psi. Is this the future accumulated reward given the current state and selected action? Summary========The paper studies a certain notion of spectral sparsification of directed graphs. It claims the existence of nearly linear sized sparsifiers under this notion, and suggests empirical methods to produce such sparsifiers in nearly linear time.Comments=========Section 4: I am not sure what is the downstream justification for the notion of spectral approximation studied here (relative condition number) in the context of directed graphs, and it would help if the authors could elaborate on this. For example, the notion defined in Cohen et al. (2017) enables fast approximate solution of linear equations, which is a primitive in various algorithmic tasks on directed graphs. The notion in this paper does not rigorously lead to such consequences (or at least none are presented), which leaves the question why one should be interested in an existence result like Theorem 4.3. (The appendix contains some experimental results for linear system solving, but no rigorous claims.)About the proof Theorem 4.3: Could you please clarify how does the approximating matrix $L_{S_u} = B^T W_o^{1/2} T W_o^{1/2} B$ (defined in the end of section 8.2) give rise to a directed subgraph? How does one construct $S$ (or in other words, why can $L_{S_u}$ be written as $L_SL_S^T$ for $L_S$ of the form in eq (18))Section 5: I am generally not able to follow the derivations in section 5.2, and it may be needed to be written more clearly. Specific comments/questions:1. How come $\lambda_n>0$ if the matrix $L_{S_u}^+L_{G_u}$  has non-full rank?2. You define v_i once as the eigenvectors of $L_{S_u}^+L_{G_u}$ in the first sentence sentence as then as scaled eigenvalues of $L_{S_u}$ between eq (5) and (6), which one is it? Or are they supposed to be denoted differently?3. Could you elaborate on how you arrive at eq (5)?Section 6: The experiments in the main text concentrate on measuring the relative condition number of the sparsifier w.r.t the original graph as per the sparsification notion studied in this paper, but as written above it is not clear what do we actually get out of the sparsifier. In general I have doubts about fit to the venue; while ICLR scope is broad and inclusive and spectral sparsification has certain potential connections to ML, this paper does not highlight any of them, and it is not entirely clear what it is attempting to achieve.Conclusion:=========Pros: Spectral sparsification of directed graphs is a relatively new field, so far with initial results that invite further research and improvements. The paper implements an algorithm which seems to improve over baselines under certain metrics.Cons: It is not clear what ML task the paper is trying to solve or improve upon, and what improvement is achieved. Viewed as a theoretical submission, the analysis is unclear to me in many parts, and at present I am unable to confirm its soundness. The algorithm implemented in the experimental section seems rather detached from the preceding theoretical definitions, whose purpose remains somewhat unclear.  # SummaryThis paper claims to have 3 main contributions. C1: Understanding/Theory. It explains why the two tricks work in zero-shot learning (ZSL): (i) normalization + scaling in the compatibility function of the class features and the attributes, and (ii) attribute unit normalization.C2: Method. It proposes a class normalization scheme (Eq. 9 and 10) and Fig. 3. C2.1 From a theoretical explanation of C1 (ii), this fixes (ii) in a deep ZSL model.C2.2 It improves smoothness of a irregular loss landscape in ZSL.C3: Experiments. It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL. It also considers continual ZSL (Sect. 4), in which the proposed method is evaluated via mean accuracy (over timesteps) accuracy metrics and a forgetting metric.#### StrengthsS1. Simple method. This is a simple feature-attribute scoring function via scaled cosine similarity (with normalization).S2. Strong empirical results (on both accuracy and training speed). See Table 2.# WeaknessesW1. ClarityThe organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the theoretical results which are not rigorous and at times unsupported. Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing. W2. SoundnessI have a lot of concerns and questions here as I read through Sect. 3. At a high-level, I dont see a clear connection between improved variance control of prediction y^ or the smoothness of loss landscape and zero-shot learning effectiveness. Details below. This is in part due to poor clarity.W3. ExperimentsIMO, if the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2). - If baseline methods already adopt some of these tricks, it should be made clear and see if removing these tricks lead to inferior performance.- If baseline methods do not adopt some of these tricks, these tricks, especially class normalization, could be applied to show improved performance. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.) This is done to some degree in the continual setting.W4. Related workAs I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.#### Detailed comments (mainly to clarify my points about weaknesses)## Statement 1The main claim for this part is that this statement provides a theoretical understanding of the trick and allows to speed up the search [of the optimal value fo \gamma].However, I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, better stability and the training would not stale) and the zero-shot learning accuracy for this to be the why normalization + scaling works. My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.Moreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it? Finally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?## Statement 2 and 3Why wouldnt the following statement in Sect. 3.3 invalidate Statement 1? This may create an impression that it does not matter how we initialize the weights  normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit valueIt is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.Similar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? In particular, the authors again claim that the main bottleneck in improving zero-shot learning is variance control (the end of Sect. 3.3).I also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: And these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).H1: Would this statement still be true after we transform a_c with an MLP?H2: Why is it not a sensible thing to do if we just want zero mean and unit variance? H3: Why is such an approach far from being scalable? H4: What if these are things like word embeddings?H5: Fig. 12 and Fig. 13 are not explained.H6: Histograms in Fig. 13 look quite normal.How useful is Statement 2? Why is the connection with Xavier initialization important?Why is preserving the variance between z and y~ in Statement 3  important for zero-shot learning?## Improved smoothnessThe claim improved smoothness at the end of Sect. 3 and Appendix F is really hard to understand.F1: How do the authors define irregular loss surface?F2: Santurkar et al. (2018) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model, which suggests that our class-wise standardization will provide the same impact. This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear.Similarly to my comments to Statement 1-3, how is improved smoothness related to zero-shot learning effectiveness?  ## Other more minor comments1. Abstract: Are the authors the one to generalize ZSL to a broader problem? Please tone down the claim if not.2. After Eq. (2): Why does attribute normalization look inconsiderable (possibly this is not the right word?) or why is it surprising that this is preferred in practice? Dont most zero-shot learning methods use this (see for example Table 4 in [A])?3. Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).4. Under Table 1 These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a linear or multi-layer model.5. The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.6. The second paragraph of Sect. 3: What exactly limits the tools for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].7. What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected? [A] Synthesized Classifiers for Zero-Shot Learning[B] Zero-Shot Learning by Convex Combination of Semantic Embeddings[C] Class-Balanced Loss Based on Effective Number of Samples In its current form, I feel that the paper should be disqualified because it contains some results essential to its claims in the appendix (referenced in the second paragraph, page 5). However, this can be easily addressed - thus my full review below:Summary of the paper:The paper aims to analyze if and how neural networks learn modular representations. Modularity under the paper's definition means that the network learns representations that (1) specialize (using different modules for different functions) and that (2) compose in a re-usable fashion - i.e., that functions are used in diverse tasks.To do so, the authors train different probabistic masks (using a Gumbel-Sigmoid) on the weights of a neural network over a series of different tasks. They incentivize these masks to be sparse by regularizing the number of "active" elements in a mask. They then compare masks learned for different tasks (including simple arithmetic tasks, and permuted MNIST), and then compare the usage of the parameter masks over different tasks. They find that NNs learn to specialize only, without reuse.The paper continues by postulating that the reason is either that the network learned "bad" representations, or that the network did not learn the correct composition. They argue that results on the SCAN dataset show that the representations are of a sufficient quality, concluding that the network did not learn the correct composition.Commentary:The question the paper tries to answer is very relevant, on two levels. The first is that we do not understand how an NN learns sufficiently well. The second is that compositional modularity is a highly desireable property, and it is important to know if neural network exhibit it.Strengths:- The paper does some interesting analysis - In general, the paper is well-written, and easily understood.Unfortunately, the paper suffers from major drawbacks:- (this is a minor point I'm putting here to facilitate my discussion below) The paper is not positioning itself correctly in the literature, thereby using confusing terminologyWhile modularity is not a main focus in neural network research, there exists some meaningful research that has established some terminology. In essence, the paper talks about compositional modularity (and combinatorial generalization), but does not use this terminology. This makes the paper a little difficult to follow, if one is familiar with this literature. Within this terminology, specialize would be called "modularize" and Preuse would be called "compose", a terminology I will use in the following.  - The conclusions are not convincingThe core argument in the paper is that neural networks fail to modularize because they either learn insufficient representations, or because they fail to learn to compose (to learn the "algorithm" required to utilize the modules correctly). Because the network learns re-usable representations, they modularize, and must thus fail to compose. While this is probably true, I cannot help to feel a little underwhelmed. It is well known that neural networks are overparameterized (see the "lottery ticket" literature). For this analysis, this means that it appears to be easier for the model to re-learn using the available capacity than to re-use the existing modules.This is not particularly surpsising either, because this is, at its core, overfitting: the model does not generalize (which, in effect, is a "softer" way to re-use), but instead learns something akint to a separate function for different inputs.What the paper does not investigate is why a neural network not re-uses capabilities, even those should be a good fit for a problem. This would be a really interesting analysis, one which I would very strongly argue for acceptance in any venue.- The novelty of some parts is overstatedThis is particularly true for using binary masks for multi-task like learning. See "Bengio, E., Bacon, P. L., Pineau, J., & Precup, D. (2015). Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297."Additionally, the paper does not sufficiently relate their insights to the (cited) work around inducing modularity and compositionaly in networks, some of which does already come to similar results. ##########################################################################Summary: This paper presents an end-to-end deep retrieval method for recommendation. The model encodes all candidates into a discrete latent space, and learns the latent space parameters alongside the other neural network parameters. Recommendation is performed through beam search. The paper compares the method on two public dataset against several methods (DNN, CF, TDM) and concludes that it can achieve the same result as a brute-force solution in sub-linear time.##########################################################################Reasons for score: The paper presents an interesting end-to-end deep retrieval approach.  However, the paper suffers from several key limitations:First, it makes a very strong assumption (that vector-based approaches have fundamental limitations).  Because this assertion is very strong, it should be backed by a more thorough analysis than what is done on the paper (more details below).Second, it fails to take into account several key state-of-the-art methods (such as VAE).  The method proposed in the paper might perform significantly worse than this SOTA based on their reported results.Finally, it brings confusion between two problems: the one of choosing an algorithm (vector-based versus deep end-to-end) and the one of choosing a brute-force vs approximate nearest-neighbor.  Yet it is well-known that approximate nearest neighbor search is often almost as good as brute-force nearest neighbor, as can be seen here: http://ann-benchmarks.com/The method presented in the paper is interesting, though.  I believe this work could be published, but with significantly more research work.##########################################################################Pros:- Novelty: this paper present a method that, as far as I know, is novel.- Comparison to tree-based methods: the paper presents an interesting comparison with tree-based approaches ##########################################################################Cons: - Lack of validation: first and foremost, the work presented in this paper lacks validation experiments.  For a paper presenting a new algorithm and making a strong claim regarding vector-based methods, we would expect at least 4-5 datasets as is commonly done in the literature, e.g. with MSD, Netflix, Medium, Amazon, Yahoo datasets.  We would also expect more metrics, and in particular, the right recall values as is commonly done in the field.  In particular, other papers use recall@20 and recall@50 (as can be seen in paperswithcode.com) instead of recall@10.- Missing state-of-the-art: the paper misses on significant portions of state of the art regarding the evaluation.  Several key methods should be included in the evaluation, such as VAEs [1], EASE [2], RACT [3], SLIM [4] and CML [5].  While these methods are not end-to-end, the paper should compare its performance against these methods to conclude whether end-to-end deep retrieval yields better (or even similar) performance compared to them.  It turns out that some of these methods perform well on recall@20 and maybe better than the method presented in this paper.  Note that some of these methods are also sub-linear in the number of items, such as CML.- The paper is also missing a reference on solving the vector-based limitations, with Off-Policy Learning in Two-Stage Recommender Systems [6].- The paper claims to address "large-scale" recommender systems (at several places in the paper) but does not address this aspect.  There exist a significant body of literature on the topic of recommender systems operating at the scale of billions of users and items now, e.g. [7], [8].  Working at the scale of MovieLens and AmazonBooks is not large-scale.  In addition, a complexity analysis of the method would be very welcome.- Lack of clarity: The clarity of the paper could be greatly improved by putting the description of the algorithm in a single place.  At the moment, it is spread between Section 1 and Section 2.1.  In particular, Section 1 introduces D and K but does not explain what they are. Some aspects of the algorithms are described in a single line (a GRU is used to project the behavior sequence, but nothing is explained about it).  - Lack of code: the paper does not provide the code, which does not help for reproducibility and sharing with the community.  Providing code is paramount when proposing a new algorithm.[1] Daeryong Kim and Bongwon Suh. 2019. Enhancing VAEs for Collaborative Filtering: Flexible Priors Gating Mechanisms. In Proceedings ofthe 13th ACM Conference on Recommender Systems (RecSys 19). Association for Computing Machinery, New York, NY, USA, 403407. https://doi.org/10.1145/3298689.3347015[2] Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data. In The World Wide Web Conference (WWW 19). Association forComputing Machinery, New York, NY, USA, 32513257. https://doi.org/10.1145/3308558.3313710[3] Sam Lobel, Chunyuan Li, Jianfeng Gao, and Lawrence Carin. 2020. RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering. InEighth International Conference on Learning Representations (ICLR). https://www.microsoft.com/en-us/research/publication/ract-toward-amortizedranking-critical-training-for-collaborative-filtering/[4] Xia Ning and George Karypis. 2011. SLIM: Sparse Linear Methods for Top-N Recommender Systems. In Proceedings of the 2011 IEEE 11th InternationalConference on Data Mining (ICDM 11). IEEE Computer Society, USA, 497506. https://doi.org/10.1109/ICDM.2011.134[5] Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin. 2017. Collaborative Metric Learning. In Proceedings ofthe 26th International Conference on World Wide Web (WWW 17). International World Wide Web Conferences Steering Committee, Republic andCanton of Geneva, CHE, 193201. https://doi.org/10.1145/3038912.3052639[6] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong, and Ed H. Chi. 2020. Off-Policy Learning in Two-Stage Recommender Systems. In Proceedings of The Web Conference 2020 (WWW 20). Association for Computing Machinery, New York, NY, USA, 463473. https://doi.org/10.1145/3366423. 3380130[7] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+Million Users in Real-Time. In Proceedings of the 2018 World Wide Web Conference (WWW 18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 17751784. https://doi.org/10.1145/3178876.3186183[8] JizheWang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD 18). ACM, New York, NY, USA, 839848. https://doi.org/10.1145/3219819.3219869 #########################################################################Some typos: "grow" on page 1"from the all successors" on page 4upper-case are missing in the references (e.g. ALSH) ### SummaryThe paper presents a library for second-order analysis of the optimization of models implemented with PyTorch and potentially having millions of parameters. The library offers tools for easily computing and visualizing Hessian, curvature, loss landscape, and runnning simple statistics, for instance for studying the properties of local minima.Compared to existing tools, the proposed library is more complete and accurate, , as demonstrated by example analyses, and scales well with the number of model parameters and the dataset size.The library itself seems very useful, however the paper needs major revision in order to be publishable.### PresentationThe paper is generally poorly written. Many concepts are not defined or not well enough, for instance:1. input independence, page 2, by which it is not clear if independence between features or between samples is meant;2. number of moments, page 3, not defined;3. curvature, by which it should be specified earlier it is meant the loss curvature.To improve clarity, the paper should provide a comprehensive list of tools implemented in the library and give examples of why one would need to use each of them. The library's tools are described throughout the paper, but they are too scattered to have a complete view of the library.Moreover, the Lanczos algorithm, which is the central for the library, should be reported. Finally, the paper misses important parts, such as the reference page, a paragraph in page 5 and the cited and not reported Algorithm 1; it also has many typos; notations and domains are not provided for most equations and it is not clear why Equation (3) does not depend on the loss L.The current presentation doesn't meet the standards for a conference paper. I think this paper needs to be desk-rejected since the supplementary material reveals the authors' identities in multiple places, e.g. the Readme file and the example notebook. Various .py files have an author attribute set.This issue aside, both the package and the paper appear rushed to me and **not ready for publication**.Package:* Most importantly, I could not find any tests whatsoever. Of course tests do not guarantee anything, especially if implemented poorly, but I hope that nobody would trust a completely untested third-party codebase for their own research. A thorough(!) test suite would be a strict requirement from my view to even consider accepting a code library paper at a top-tier conference.* There is no setup.py file to install the package and dependencies. I tried importing some of the modules from the root directory but got errors because of missing dependencies (e.g. backpack is not listed in the requirements.txt).* The package is split into multiples modules with generic names such as 'core'. I would strongly suggest to put them in a joint namespace.* What's the reasoning for including implementations of standard architectures that are available e.g. through torchvision and high-level training functions? Are those required? In general, I think that for a utility library it is best to assume that users have an established workflow of constructing and training their networks (unless the point of the library is to simplify that workflow, but this does not seem to be the case here).Paper:* I don't think that Section 4 fits into the flow of the paper. I would rather see that space used to go more into depth with the examples. At the moment you assume that the reader already knows why they would want to e.g. estimate the spectral density, but I think it would be worth both motivating the experiments and discussing the results.* I think basing the code examples on actual network and data loader objects would make them more accessible. Presumably passing string arguments for model and dataset into your compute_eigenspectrum and train_network functions is limited to the architectures and datasets that you implement? How do users provide their own datasets or networks? Or is this not supported? Section 2 seems to claim otherwise.* The font sizes and families around the code listings are extremely inconsistent, especially pages 5 and 6 are visually fairly unappealing.* There is some discussion around the relationship to Backpack, but what about PyHessian? That package seems more closely related since it also focuses on computing spectral densities.* Comparing first and second order optimisers on a per-epoch basis as in Figure 1 does not seem practically relevant to me, a wall clock time comparison would be of much higher interest to most potential users.* I would recommend carefully checking the paper for spelling and language. I'm not a native speaker, but to me it seems like there are quite a few inappropriate capitalisations ("Training", "Testing"), inconsistent capitalisations ("eigenvalues" vs "Eigenvalues") and hyphenations ("eigen-decomposition").* The reference pages are missing (after page 8). The authors describe a QD-RL algorithm to solve continuous control problems with neural controllers. The authors state that  they maximize diversity within the population and  the return of each individual agent. Furthermore, the authors state that QD-RL selects agents from a Pareto front or from a Map-Elites grid. This paper is weak in estimating its performance in a clear way. The overall structure in Section 4 is not well defined and difficult to follow. Descriptions of the methods and technical details of the proposed study are incomplete. Furthermore, literature review simply lists studies without presenting a coherent and systematic introduction or critical evaluation. Overall, the contribution of the paper is not significant.  Summary:This paper studies attention mechanisms.The main results of the paper are upper bounds on sample complexities of attention models and the baseline non-attention model. The paper also further investigates several additional properties and provides empirical studies.  Concerns:(1) Without going into the details, the main problem of the paper is that it only gives upper bounds on the sample complexity. When comparing the upper bound of two algorithms, the comparison does not show which one is better. To show one algorithm is better than the other, you need to prove that ones upper bound is smaller than the others lower bound on sample complexity.(2) The paper proves the result only for a specific non-attention model. Without further justification, it is very unclear why this is enough to represent other non-attention models.Reasons for score:I vote for rejection. The theoretical results are not sufficient to support the claims made in the paper about attention models.  This paper reports a mathematical study of approximation properties of linear RNNs. The first part  reports a universal approximation theorem, and presents an analysis of how efficient the approximation is. In particular, it is shown that approximating a slowly decaying, power-law temporal filter requires a large number of neurons, a property the authors refer to as the "curse of memory". The second part of the paper examines the dynamics of learning under gradient descent, and gives arguments related to the existence of long plateaus seen in the loss as function of training epochs.The paper is written in a formal mathematical style (Definitions/Theorems). Not being a mathematician, I am not able to assess the formal correctness of the proofs, and I have found some parts not easily accessible. Most importantly, as currently presented the main results seem to be of limited interest to the ICLR community (see below for details). The paper as a whole is probably more appropriate for a more mathematical venue, where the novelty of the proofs may be better appreciated.Strengths:- attempts to put on a rigorous footing various experimental observations on RNN training- possibly novel mathematical derivations of approximation resultsConcerns:- the novelty of the results presented in the first part is limited. Previous works have reported various universal approximation theorems for RNNs. Several classical works on that topic are not mentioned, eg Doya 1993, or Maass 2007. The details of the mathematical derivation may well be novel, but I am not able to judge this aspect.- the fact that a diverging number of exponentials are needed to approximate a power-law function is also well known; from that perspective the "curse of memory" is not very surprising.- I found it difficult to extract key results from the second part on Optimization dynamics.  The paper shows the deep equilibrium model has non-degenerate neural tangent kernel in the infinite depth setting. The neural tangent kernel can be computed by a similar root-finding problem as that in the deep equilibrium problem itself. Some experiments have been performed to compare the performance of deep equilibrium neural tangent kernel with that of finite depth neural tangent kernel.Overall I vote for rejecting. My concerns are as follows:The paper lacks related literature. First, the motivation of considering deep equilibrium models is unclear to me. The authors should provide some further literature review. The advantage of using such a model in practice should be explained. Second, related proof techniques in the existing literature needs to be discussed. The result is expectable and the proof techniques are not novel. The main theorem (Theorem 1) is the simple extension of the existing results on neural tangent kernel. The following theorem (Theorem 2) is the consequence of the main theorem under some specified initialization.The theorems in the paper are lack of explanation. More discussion is needed to explain and extend the results in the paper.The experiment part is not well-organized. More description is needed to improve the results.The paper has some grammar mistakes and misuse of words. The paper needs to be revised carefully. To name a few:Abstract: DEQ model....DEQ models have...Section 3, 1st paragraph: we simplify fully-connected DEQs as DEQs.Section 3, 1st paragraph: In section 3.1, we show the NTK of the approximated DEQ using finite depth iteration... In the work, the authors applied graph convolutional neural networks to predict enzyme specificity. The problem is very important in biology, which even can be used for designing new drugs. Essentially, the authors generated energy-related features using Rosetta and then built the graph using the molecular structure. Then, they applied the existing GCN to the problem. In the evaluation section, they compared with Logistic Regression, Decision Tree, Random Forest, SVM, ANN, to show the superiority of the method. However, the manuscript has the following major flaws.1. There is no novelty in the methodology part. They only applied the existing GCN model. The manuscript is not for the audience of ICLR. 2. The baselines are too weak. They only compared with the classic ML methods. They did not compare with the state-of-the-art methods to predict the binding affinity. The authors can find a lot of related literature.3. Even compared with the classic algorithms, the proposed method did not outperform them by a large margin. 4. They provide the link to the Github repository, which makes the submission not anonymous anymore. That's the reason that I did not provide related works in comment #2. This paper uses a structure-based molecular interaction graph generated from the Rosetta interaction energy function to develop protein graph convolutional neural networks (PGCN) that predict enzyme substrate specificity. The authors clearly describe the goal of being able to accurately model the substrate specificity of enzymes such as proteases, including both a description of those substrates that the enzyme recognizes in addition to those substrates that it does not recognize. They propose that this model should capture the energetics of interactions between the enzyme and potential substrates, such that substrates recognized by the enzyme are assigned lower energies by the model than substrates that are not recognized by the enzyme. To address this challenge, they propose a protein graph convolutional neural network, in which the enzyme and substrate residues are modeled as nodes, while interaction energies obtained via a pairwise decomposition of the energy of the enzyme-substrate complex are considered as node and edge features.  The authors provide some survey of recent literature that covers protein-substrate interactions. They can improve by discussing the contributions of papers on this subject from a wider range of research labs - the majority of papers cited in this section feature a common author. Similarly in the related work on graph convolutional networks for protein-related problems the authors should cite work such as e.g. the graph convolutional models for protein ligand interaction prediction from Torng and Altman 2019 among others.  The authors present results for both 'hybrid' and 'energy-only' models. The energy-only feature encoding for the other machine learning models is not clearly described, and it is not clear why the energy-only feature encoding is of interest - the authors do not describe any context in which this encoding would be used in preference to the hybrid sequence and energy feature encoding, which performs better. However, for the energy-only feature encoding the model developed in this paper always performs the best - it is important for the authors to explain why this result is of interest to the reader, since the performance using this encoding is always worse than the performance of multiple other models using the hybrid encoding. For the hybrid encoding and overall, either the ANN or SVM models perform best. An important question is whether the considerably more detailed energy feature encoding of the PGCN model effectively contains information about the amino acid sequence, making the one-hot sequence encoding that is in present in the hybrid encoding but absent from the energy-only encoding redundant. This would explain why PGCN does comparatively well in the energy-only setting, compared to the much simpler coarse grained energy terms used by the other models. Overall it is very unclear what value the PGCN model and featurization adds.  This paper reports some observations about properties of linear regions in deep ReLU networks. Unfortunately, I have several major issues with the paper. (1) First of all, I am a bit confused about the motivation behind this work. The motivation is initially couched in terms of hash codes, so one gets the impression that the authors are going to propose a new hash coding scheme using neural networks. But this is clearly not the case. The proposed coding scheme is practically useless as a hash code because of the enormous dimensionality of the codes (I estimate this to be on the order of millions even for the toy MNIST case studied in the paper, but it would actually be very helpful for the reader if the authors explicitly mentioned the dimensionality of the proposed hash codes--at least the order of magnitude--). This is also why the authors are stuck with the toy MNIST dataset throughout the paper, because the proposed scheme is completely impractical for any reasonably large dataset and model.(2) This begs the question: what exactly is the significance of the observation that linear regions satisfy some properties of hash codes, if theyre not going to be used as hash codes? Its not meaningful to just point out that something satisfies some properties of hash codes. One can point to a million different things that satisfy some properties of hash codes. What exactly is the significance of linear regions having these properties?(3) This brings me to the related works section. This section is written very shallowly, the authors do not do a good job of situating their contributions in the context of prior works. You cannot just say: Other advances in linear region counting include & (p. 3) and then cite a bunch of references. You have to tell us what each of these papers did and how what youre doing in this paper differs from these earlier works and makes a meaningful and novel contribution to the prior literature.(4) The concept of redundancy ratio seems to play a central role in the paper, but it is not defined formally, just a verbal (potentially ambiguous) definition is given. Please define this concept formally to avoid any ambiguities. Im also not sure this concept is the right one for quantifying the goodness of a code: consider a case where each point in a dataset shares its linear region with exactly one other point in the dataset vs. a case where all points belong to the same linear region. It seems that the redundancy ratio will be 100% in both cases, however intuitively the code in the first case should be much better than in the second case (for example for retrieval).(5) Relatedly, Figure 3c suggests that the redundancy ratio is close to zero for an untrained random network. Then, by the authors own definition, the encoding is actually very good before training. Please consider what this means (also see point 7 below).(6) Unfortunately, I dont think classification results for MNIST only are very meaningful. Almost anything will get above 99% accuracy on MNIST. Moreover, no effort is made by the authors to understand what drives good test accuracy in these experiments. A very straightforward explanation is that accurate classification is primarily driven by higher layers, so one actually doesnt need most of the dimensions in the hash code for good classification performance (similar cache models using high layer features have been proposed before: e.g. Grave et al., ICLR 2017; Orhan, NeurIPS 2018; Khandelwal et al., ICLR 2020). (7) Most importantly, one of the main phenomena observed in this paper (the change of redundancy ratio over training) has already been reported in Hanin and Rolnick (ICML, 2019): they note that the number of linear regions in a deep ReLU net first decreases and then increases during training (please read their section 3 carefully). This would easily explain the trajectory of the redundancy ratio observed in Figure 3c (and in Figure 4a) in this paper. Moreover, the concept of diameter is also rigorously defined (and diameters of linear regions studied) in Hanin and Rolnick (ICML, 2019), but this is not acknowledged at all by the authors. This is a pretty serious omission. (8) Typos: should be: Mapping induced by a relu network (p. 1), it is worth noting (p. 2)Geometric properties of linear regions (p.3). Another diameters of linear regions? (p. 5) Summary:The paper presents results of two popular classes of methods for graph neural networks.  Namely, GAE and SEAL.  The paper show results on the Open Graph Benchmark of several existing methods, and concludes that GAE cannot learn structural link representations while SEAL can. The paper is descriptive in its presentation, and does a good work on detailing the ideas behind GAE and SEAL.  However, I felt the lack of a contribution on the paper.  As it stands, it reads more as a tutorial and presents key insights of existing methods.Pros:- Clear explanations of existing work.Cons:- No clear contribution over explaining existing approaches.- Experiments show existing results on OGB.- No results on the mentioned labeling trick are given.Comments:- In Fig. 1 you mentioned that $v_2$ and $v_3$ will get the same representation through a GAE.  Did you train one and observed it?  It will be more conclusive if you show the results and the embeddings to validate your observation.- Your labeling trick depends on the set of nodes $S$ used.  However, how do you select such sets to produce the labeling? Are you sampling all possible subsets $S$?  Since this seems to be a form of contribution, it should be clear how to use it in any scenario.- In your experiments, due to the way the paper presents the label trick as the novelty, I was expecting to see the boost of performance of such trick on the existing architectures.  However, Section 7 presents results of existing approaches on the OGB.Minor comments:- "Following (Srinivasan & Ribeiro, 2020; Li et al., 2020)" should be a textual citationOverall rating:Due to the main problems stated, I cannot recommend the paper for publication at ICLR.  The manuscript reads more like a tutorial, which may be of interest for readers finding a summary of the advances.  However, as a paper advancing the field of graph neural networks I feel it lacking. The authors tried to explore the key differences between two link prediction methods. However, some statements are not precise. The example the authors provided in the introduction is not correct. The GAE will also assign them different probabilities. When we use GAE to learn node embedding in the graphs, we usually have two different inputs: (A) nodes in the graph have input features (B) nodes in the graph do not have features and we assign each a one hot vector. For both two cases, each node in the graph in Figure 1 is unique node. Even node v2 and v3 play the same role in terms of graph structure, they still have different node embeddings considering the node features.It is worthy to discuss that why node embeddings learning from exisiting method cannot help SEAL. It can be seen from the experimental results in two SEAL papers[1][2] that the node embedding does not help improve performance.[1]Inductive Matrix Completion Based on Graph Neural Networks[2]Link Prediction Based on Graph Neural Networks The authors propose to change agents' incentives in order to lead to a better Nash equilibrium outcome. They propose a practical decentralized approach to computing the "optimal" change of incentives.I'm confused by many aspects of this paper:1. How would you implement the change of incentives in a any of the game theoretic domains? I.e. in what context would agents who lose money agree to participate without a centralized authority ordering them to?2. Why do you consider changes of incentives with bounded KL-divergence? What does that have to do with your motivating concern around communication (3rd paragraph of intro)?3. Why do you optimize the price of anarchy instead of social welfare at worst Nash equilibrium? These should be the same when you don't change the total loss, but then I don't understand the denominator of (3).4. What is $t$ in (4)? How does f^A_i depend on t?5. Shouldn't \beta_i appear in (4)? Or does (4) hold for all \beta_i (e.g. approaching infinity)?6. What should I conclude from your experiments? Of course if you completely mix the agents' utilities the Nash equilibrium would be locally optimal. The Nash equilibrium utilities at the modified games seem better, but it's not clear how much the games were modified.Misc:(A) Computing the Price of Anarchy in general is NP-complete rather than PPAD-complete because PPAD corresponds to finding *any* Nash equilibrium rather than the worst one.(B) "set of the equilibria of the game restricted to the line" - I don't understand what this sentence means.(C) Why don't you define utilitarian/egalitarian before Theorem 1?(D) Typos: "one-hot" "the the" The paper proposes a new regularization for the dictionary in the learned convolutional sparse coding model of Sreter & Giryes '18. The main contribution is that the dictionary is regularized to be composed of 1) a fixed low-pass filter and 2) a set of learned filters to occupy the complementary high-frequency space. A second contribution is that the thresholding in the network is adjustable according to the estimated noise level in the image. Comments:- Presentation-wise, I didn't get the motivation of the work after reading the introduction. Everything before Sec. 1.2 is a narrative of the existing methods, and all of a sudden Sec. 1.2 states the proposed method, but what is the problem that the paper wants to address?- The major contribution is the introduction of a fixed low-pass filter in the dictionary, but I don't see a clear justification as to why it is needed. If you want to model the DC component, why not simply use a bias term?- The paper tells a story that the dictionary need to be incoherent, that some prior work enforces this by using large stride convolution, and that the proposed method does not need to use large stride. But ultimately there is no incoherence regularization for the high-frequency part of the dictionary in the proposed method. So how do you enforce those high-frequency filters to be incoherent?- It is stated as a second main contribution of the paper that the thresholding in the network can be made to be dependent on the noise level in the image, so that the network is capable of performing blind denoising. However, the explanation for how it works (Sec. 2.3) is very sketchy. Why is the adaptive threshold only used in the last layer? Also, the threshold is set to be a multiple of noise variance, but how do you estimate the noise variance in practice? Where does the number 0.6745 come from?Overall, the presentation of the paper needs major improvement to make the motivation as well as technical details clear. In addition, the technical contributions appear quite minor and are not fully justified: it is unclear how fixed low-pass filter compares with a bias term, and it is unclear how effective the adaptive threshold is in practice.  The paper proposes an exploration scheme for RL in continuous action spaces using the principle of information maximization for globally optimal Q distribution.1. I felt that the paper isn't well written and discusses a lot of different concepts in a haphazard manner.  There are a lot of equations and symbols in the text without proper explanation and context which make it difficult to gather the main contribution. The language used gets vague in many statements made in the paper. For ex. "Proposition 1. Generally, the posterior probability is as follows".2. A lot of algorithm adaptations are proposed without actually carrying out ablations which make it difficult to discern if the proposed MI maximization is indeed responsible for performance. For ex. "Since the target for critic in the advanced algorithms, like SAC and TD3, is usually estimated pessimistically..". The authors should actually present ablations to support if a pessimistic estimate is indeed required for their adaptation for these methods. 3. Why haven't the authors included OAC as a baseline given that it outperforms SAC in several tasks? Further the results show little difference in performance in comparison with DSAC on the mujoco tasks, given that only 5 seeds were used in evaluation, it brings the significance of the results under question. The authors should provide appropriate measures like P-values to support the experiments. In this paper, the authors study the problem of optimization in two-player competitive zero-sum sequential games where the objective function is convex-concave in players' strategies. They relax the convex-concave assumption for the empirical study.This problem is an important and challenging problem in the field of reinforcement learning.The authors propose a policy gradient-based method and argue that it converges to the game's Nash equilibrium. They also conduct a series of empirical studies to show the significance of their method. While this is a challenging problem, my judgment is that the paper is not ready for publication yet.1) The presentation of theorem 1:The authors state that if "a sequence of (x^k,y^k)->(xhat,yhat) \wedge f(x^k,y^k) -f(u^k,y^k)-> 0 implies (xhat,yhat) is a saddle point". I am not sure I follow this sentence. 1-a) First, please clear up the notation when you use \wedge. For example, say ((x^k,y^k)->(xhat,yhat)) \wedge (f(x^k,y^k) -f(u^k,y^k)-> 0) since \wedge someitmes also means minimum. 1-b) In thm1, do the authors mean if "a" sequence (x^k,y^k) for k>0, satisfies the following conditions ((x^k,y^k)->(xhat,yhat)) and (f(x^k,y^k) -f(u^k,y^k)-> 0), then (xhat,yhat) is a saddle point?If that is the case, please consider rewording the thm1. Also, if that is the case, for what \nu^k and u^k?1-c) Algorithm 1 seems to produce a sequence of sets rather than (x^k,y^k). Please clarify it in your theorem.1-d) If \nu^k and u^k are some quantities that are produced by your algorithm, then the theorem seems not to be well-stated. Because I am not sure what sequence of \nu^k and u^k the algorithm produces in order for me to see what space of X and Y you are dealing with. 1-e) I see the same lack of clarity in the proof of thm1. 2) In assumption 1, you state that the sets C_x^k and C_y^k are compact subsets of X and Y. However, these are what your algorithm produces. So probably, you may want to design your algorithm such it guarantees such a requirement. 3) E_k seems to be not defined. As the authors stated, the main contribution of this paper is not the theory. And the theoretical study in this paper is to motivate their algorithm.4) Empirical study. I found the empirical study in this paper, somewhat not convincing. If the contribution is empirical, it would be useful to compare it with existing methods.I realized that the authors did not mention the work of "Learning with Opponent-Learning Awareness" which proposes LOLA. Also, the authors mention the work of "Competitive policy optimization". I strongly encourage the authors to make a clear empirical study and compare their method with prior works.Since the proposed algorithm seems like a well-known traditional per-step best response method, such comparisons are helpful for future submission. This paper presents an approach to low-dimensional embedding of data, with an emphasis on image datasets. The algorithm proceeds by first finding several local subspaces that fit the data and then tying these subspaces together using the center of mass calculation on the Stiefel/Grassmann manifold. The method appears to be computationally efficient, since it can be applied to fairly large image datasets, though no formal analysis is given. The main drawback of the paper is that the empirical results compare only to PCA. Several more modern methods for low-dimensional embeddings exist (e.g., T-SNE, UMAP), and it would be appropriate to compare to these. UMAP also allows for the incorporation of labeled data.Aside from this, the paper is quite difficult to follow due to the current organizational structure. A final minor comment is that the figure text is too small to be reasonably legible. The paper describes a  knowledge transfer technique based on  training a student network using annotation creating by  a teacher network. This is actually not a summary of the method but  the method itself. Most the the rest of the paper is devote do describe experiment details. The idea is  well known in machine learning community see e.g. Distilling the Knowledge in a Neural Network by Hinton. where is used to transfer knowledge from a huge network to a small network. Hence, there is not much novelty in the paper.Although the method is very simple it is difficult the follow the experimental results. It is written in a very unclear way.Do you use step 3 in the experiments? what is your conclusion regarding parameter fine tuning vs. your approach?Over all the paper is more suitable for a medical imaging conference than  fro a general deep learning conference.  Summary: The authors tackle the problem of trajectory prediction in autonomous vehicles.  They propose a data augmentation scheme where they enrich the raw trajectories with synthetically generated trajectories to reduce spurious modes in the predictions. This enriched dataset is hoped to improve  accuracy in trajectory prediction problems.Strengths:1. The paper belongs to an active area of research. Reliable and accurate Trajectory prediction is one of the central problems in autonomous driving.Weaknesses:1. The paper is very hard to read. For example, the contributions are confusing and not clearly defined. In the first paragraph of the Introduction, a data augmentation scheme is presented. But then on Pg.2, the authors present two notions of intents as their contributions. What is the relation? From my understanding, the authors use $\hat z \sim P(z)$ to generate new data  (I'm looking at  Eqn 4). And this eqn 4 is termed as the "classified intent". There are several things to unpack here: First, these contributions are not well-defined. What is a "classified intent" and "hallucinative intent". These are not standard terms in the trajectory prediction literature. So these terms need to be defined and explained. I would like to see a mathematical definition and references to relevant citations in the traffic psychology literature.Once it is formally defined what a "classified intent" and "hallucinative intent" is, the next thing would be to formally derive equations 4 and 5 from those definitions. Because eqns 4/5 are not derived with supporting derivations and justifications, the main concern here is that I don't find the motivation or relevance of these equations convincing.My suggestion here is that most of the current material on Pg.4 can be moved to an implementation section. Instead, use that space to include the motivations, definitions, justifications, and derivations for eqns 4/5 as explained above.2. The questionable nature of the proposed equations (4, 5) directly relates to my next point, that is, the lack of any useful improvement over SOTA. In Tab. 1, the max FDE improvement is 18cm. In Tab 2. it is 5cm. Furthermore, the authors report gains in terms of percentages, which is misleading and even dangerous, since in the real world, it is important to know absolute errors and not percentage errors relative to another benchmark.3. Additionally, the authors primarily compare using the FDE (a weaker and insufficient metric than the ADE) on most occasions, most critical of which are Tab.1 which contains the comparison with other methods and Tab.5 which contains the ablation experiments that highlight the benefits of the proposed equations 4 and 5. For fair evaluation, it is necessary to also present comparisons using ADE.In summary, the paper suffers from lack of a clear justification of the proposed contributions, unfair evaluations, and questionable significance of the results Summary: - In this paper, the authors propose a novel algorithm for pruning fully-trained ReLU neural networks. To motivate the algorithm, the authors first introduce a new network architecture with an affine skip-connection at each layer. Then the authors connect it to the theory developed in an `unpublished work`. They show that for such neural networks, the number of units in the original layer can be greatly reduced. The main idea of the proposed algorithm is basically to prune those neurons whose removal will not change the function a lot. To quantify this, the authors turn to the L2 norm of each neuron. Experiments on simple toy data and MNIST are conducted.Overall, I believe this paper is not ready for publication. So, I vote for rejection. - This paper massively refers to the unpublished work by the author, while the authors only provide only little details about the developed theory in the unpublished work. If this is a concurrent submission to ICLR, the author should still cite it anonymously.- To me, the proposed deep stack network is very similar to the formulation of the highway network in Srivastava et al., (2015).- The experiments are not convincing enough. The authors only conduct experiments on a simple toy dataset and MNIST. The numbers are fairly close to each other. To show the statistical significance, some measures, such as one standard error should be provided. I would encourage the authors to at least conduct some experiments on CIFAR datasets.Typos:- Exactly speaking, the learned function will not be the same same as before ->Exactly speaking, the learned function will not be the same  as before - Therefore the the function optimizing eq. (2) can be represented by finite number -> Therefore the function optimizing eq. (2) can be represented by a finite numberSrivastava, Rupesh Kumar, Klaus Greff, and Jürgen Schmidhuber. "Highway networks." arXiv preprint arXiv:1505.00387 (2015). In this submission, the authors propose a modification to the PBT (population-based training) method for HPO. It is interesting, however, there are several important issues to consider:1) The major part of the proposed method ROMUL is to replace some update rules based on Differential Evolution, which is a well-studied method in Evolutionary Algorithms. The novelty of the proposed method ROMUL is not high. But more important, it is unclear why such modifications are necessary. In other words, what new challenges in HPO can be addressed by conducting these modifications to PBT. Without clear and strong reasons to motivate these modifications, it is hard to evaluate the proposed method.2) The writing of this submission can be further improved. Many paragraphs and sentences are not logically organized, and it is difficult to understand the main points of the submission. For example, based on the Introduction section, it seems that the main part of this submission is to "empirically study the different training dynamics of ..." (second paragraph). And in introduction, the authors didn't well motivate the proposal of their method. Although several challenges are mentioned in the first paragraph, it is not clear which ones are solved by the proposed method, and how they are tackled.Personally I feel that "fixed step" issue in PBT is important, which should be mentioned early.Several interesting findings are provided in the experiments. The authors can make them clear and highlight them by improving the writing of current version. #######################################################################Summary:In this paper the authors propose a method for solving compositional tasks in the RL setting. The method (Self-Imitation via Reduction), is a 2-step method in which the agent first reduces the target task into two simpler tasks, and then solves the full task using as a demonstration the composite task uncovered in step-1.#######################################################################Reasons for score:I am currently voting for a rejection based on what seem to be two key omissions from the paper:1. The way in which the set of 'library' tasks is selected, and2. A measure of how expensive this offline component of the algorithm isThe experimental comparisons could also have been stronger.#######################################################################Pros:1. I think the paper is reasonable well written. I found just a few typos, and I think the key conceptual ideas are reasonably easy to follow. 2. The 2-step implementation of some sort of "policy reduction" followed by an imitation learning step would seem to be a good in principle idea that merits further exploration#######################################################################Cons:1. In Sec 3 the authors talk explicitly about a multi-task RL learning setting, and indeed a central part of the method is a search step over interim states $s_\beta$ , but I do not see explicitly how this set is constructed. This is of course a critical consideration for a number of reasons:(1)  If this set of tasks is large, then you are likely to find a good $s_\beta$, but the search space increases (as does the memory footprint)(2) If this set is small (and perhaps curated), then the subsequent result is weak (if the agent need only search over a small handful of interim tasks which already include moving the elongated box say, then of course the subsequent learning is rapid). 2. The experimental results would be more compelling if comparisons were made to methods that similarly had some access to interim policies. The comparisons to SAC for example are valid in that they provide a lower bound, but there is additional information available to SIR. Similar it is unclear what the comparison to SAC with a dense rewards shows exactly.3. The authors mention that the method is extensible even though "... tasks reduction only performs 1-step planning... SIR still retains the capability of learning an arbitrarily complex policy by alternating between imitation and reduction: as more tasks are solved, these learned tasks can recursive further serve as new reductions..." - this is a nice idea, but I saw no evidence of this implementation in the paper.#######################################################################Questions during rebuttal period:Please address the concerns above. Also, you have some comparison with hierarchical policy algorithms in 9.a/b - is it possible to extend these to the other domains?#######################################################################Some general comments:- I think the results for these multi-step methods are often easier to digest and understand if the phases are presented separately:    - Phase 1, reduction:        - for each experiment, which task reduction was uncovered by the agent?        - as you know, in many cases there are multiple valid reductions, which does your algorithm find and why?        - since your reduction phase is 1-step and greedy, in which situations might it not work so well        - etc.    - Phase 2, imitation:        - comparison to other methods        - effect of parameter choices        - etc.- A little more justification for you particular experimental comparisons can be helpful. It seems like there are a few potential avenues you might have liked to explore:    - comparison to an information impoverished baseline (like SAC)    - comparison to different task distributions    - comparison to other subtask methods    - etc. Summary:The paper presents an approach that for every object identifies the factors that have a high impact on the models' uncertainty. The approach consists of i) disentangled representations ii) a classifier on the top of the trained representations iii) technique that select dimensions of representation of an objects' (factors), that impact the uncertainty of a model the most. The i) and ii) are done in a known way; The disentanglement is done via Deep Convolutional Inverse Graphics Network (2015), and a classifier is trained with a standard maximum likelihood approach.The iii) suggests selecting a dimension that influences uncertainty the most, based on the gradient of uncertainty criteria (entropy of p(y|z), where z is embedding). In other words, the suggestion is to select the component of representation that "provides the highest decrease in entropy".The experiments are done in a controllable setting, where the generative process is known and can be controlled. The model demonstrated the ability to learn disentangled representations, in the case when control of generative prosses is available. Specifically, it is required to generate/have objects were "only one generative variable changes across the images". However, the connection to uncertainty is weak.Review:I have very mixed feelings about the work. On one hand, the problem is interesting (perhaps novel) and deserves the attention of the community. On the other hand, the paper is half-baked, there are conceptual flaws, the evaluation protocol is weak, there is an incorrect interpretation of experiment results;  My comment on novelty is the following: I would say that algorithmic novelty, is the weak point, however, bringing the attention of the community to important problems is not less important than new algorithms (and even is more important IMO). Taking into account that "novelty" is extremely subjective, I would say that there is nothing wrong with the novelty side. Writing: the paper is clearly written.  Concerns:1) The identified factors do not proofed to be meaningful and useful. Pure identifying factors of uncertainty or reduction uncertainty of a model do not give us the full picture. The factors may be flawed in many ways: factors may be too dependent on random errors of the model but not to be semantically meaningful. Reduction uncertainty of a model is not always useful, as most models nowadays are often wrongly overconfident, more confidence is not necessarily good! It has not been proven that identified factors are connected to uncertainty. 2) There is a noticeable connection between the proposed method and adversarial attacks. We can interpret a method as a way to "trick" a model to be more ceratin on adversarial modifications of embeddings. In this case, the results would not be useful.3) There are two flaws of experiments in section 4.2: i) the authors compare Expected Calibration Error between different datasets, these figures are not comparable, it is the same as compare accuracy between MNIST and ImageNet  ii) ECE is a biased estimate of true calibration with a different bias for each model, so it is not a valid metric to compare even models trained on the same data (see Vaicenavicius2019). Yes, ECE is the standard in the field, but it is the wrong standard that prevents us from meaningful scientific progress, so we should stop using it.Overall: The direction is interesting, however, the evaluation protocol needs to be more thoughtful. At the moment, it is impossible to verify the real performance of the method.Suggestions:1) Evaluate the identifying algorithm on downstream problems. For example, can we use these factors to collect additional data that will improve predictive performance better than uniformly sampled data (aka active learning)? There should be many more interesting settings.2) Evaluate the model on controllable uncertainty estimation setting: identify setting where we understand on which examples we expect the model to be uncertain (due to not enough data in a dataset in a certain region, etc.), validate selected factors.3) To use the squared kernel calibration error (SKCE) proposed in [Widmann2019] along with de facto standard, but biased ECE. The SKCE is an unbiased estimate of calibration. There might be some pitfalls of this metric that I'm not aware of, but the paper looks solid and convincing. Also, please put attention to Figure 83 in the ar%iv version. Editing:- Citations: It is better to use "authors (year)" style when a citation is a part of a sentence---"(Gabbay & Hoshen, 2020) proposes" to "Gabbay & Hoshen, (2020) propose", and otherwise "(authors, year)" when a citation is not a part of a sentence "on original VAE framework Higgins et al. (2016);" to on original VAE framework (Higgins et al., 2016; .....).- "only only" typo in 4.1[Vaicenavicius2019] Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas B Schon. Evaluating model calibration in classification. AISTATS, 2019.[Widmann2019] Widmann D, Lindsten F, Zachariah D. Calibration tests in multi-class classification: A unifying framework. In Advances in Neural Information Processing Systems 2019 (pp. 12257-12267). https://arxiv.org/pdf/1910.11385.pdf Major comments:-The methods suggested does not apply to real-world classifiers:oIt required training with strong labels only available in simulationoClassification is done based on a low-dimensional intermediate representation of the entangled space, and its accuracy is probably inferior to what can be obtained with general CNNs (no comparison was made)oI do not see how this can be applied to real-world competitive classifiers-The findings are rather limitedo The method suggested for finding which extrinsic variable is able to reduce the prediction entropy is a) not well founded, and b) was not tested experimentally  -There are presentation clarity problems, with non-sentences, broken references, etc..More detailed comments:-Separate the identity of cars from their pose (Yang et al., 2015).  this is not a sentence.-The probability estimates provided by neural networks are uncalibrated (not reflecting the real error probability)               oThis is not mentioned in the introduction, but is discussed in page 7-Page 4 maximizing the lower bound of the latent variable model distribution  what is a lower bound over a distribution? Lower bounds are defined for scalars, not distributions. This sentence is not clear.-Page 5: before equation7 the text reads it is possible to find the latent variable that would decrease the uncertainty&. However, the equation does not define a latent variable, not does the text in the 1-2 sentences following it. The equation defines a vector in R^M. It is not stated how it defines a latent variable-Page 5: The process described in equations 8,9 is essentially making a gradient step (of the entropy) in each of the M_E possible directions and then measures which step reduced the Entropy. This is a numerical re-estimation of the gradient. Why is it done? At least for small \alpha, the direction most minimizing the entropy can be determined from the gradient directly (the coordinate in which the (negative of ) gradient is highest)-Page 7: The results are shown in Table 4.2, = 4.2 is a broken referenceoAlso: it is not clear how mutual information is calculated. It requires discrete variables, and the quantization details may be important. -The classifier trained has a bottleneck at the  entangled representation layer, and is hence probably is far from being optimal in terms of its obtained error-The method suggested for estimating which extrinsic variables most affect the uncertainty was not applied experimentally (no results were reported) This paper proposes to identify sources of uncertainty by disentangling representations in latent spaces for object classification tasks. Experiments on a synthetic dataset demonstrate that the proposed method can disentangle different extrinsic variables' contribution to the prediction uncertainty.  In addition, the authors propose to modify the latent variables to decrease uncertainty in the predictions.### Clarity#### Pros- This paper provides a clear discussion of the main contributions, assumptions, experimental settings. #### Cons- The experiment section needs more analysis. For example, how to explain the results in Table 1, which variable contributes most to the uncertainty and which contributes least?- The paper contains typos and grammatical errors and need to be proofread carefully.- Several questions needed to be addressed.Questions:1. How to choose the latent representations? In real applications, there are usually more factors.2. It is not clear how the model in Figure 2 is trained. e.g. what is the loss function, do you need supervision on the latent variable.### Originality#### Pros- The paper proposes a  method to identify sources of uncertainty by disentangling representations in latent spaces in object classification tasks.#### Cons  The contribution is unclear as the disentangling method is based on the DC-IGN method.  Moreover, it is unclear how to relate the imaging factors to the latent variables as multiple imaging factors can impact the same latent variable. ### Significance#### Pros- It addresses an important problem; the problem of decomposition of the sources of uncertainty in model prediction is important and has not been sufficiently addressed in existing works.#### Cons- Entropy of the softmax is not a reliable criterion to capture uncertainty and, more importantly, other factors besides the imaging factors such as the input data density can also contribute to the uncertainty of the prediction. - The experiment is performed on synthetic data, which leads to limited evaluation conditions. As there is a distribution difference between synthetic data and real data, it would be more significant if the proposed method can be applied to real images.- The latent variables in the study are limited, only including light intensity, pose, color, etc. In real applications, there are usually many more factors such as background, occlusion, noise, resolution, etc. It would be better if the authors can study more factors or explain why specific factors are chosen. The problem definitions are not somewhat unclear. It seems that (1)-(7) is to find a pruning of the given tree, which are not explicitly mentioned. It is not nontrivial that the problem (1) leads to a tree. It would be necessary to have some proposition that ensures the desired properties (e.g., solution represents a tree) of the solution or cites such previously known ones.The obtained solutions in (3)-(7) are not discrete and how to round up continuous solutions to discrete ones are not shown. As a summary, the current writing lacks important information such as the correctness of the solutions, which significantly reduces the contribution of the paper, even if the experimental results look better than previous work. Summary-------Offline RL allows agents to be trained on static, offline datasets. Ifthe agent has already undergone offline training however, distributionshift makes further online training difficult. The authors propose toaddress this problem by keeping separate offline and online replaybuffers, and sampling different proportions of each. In addition, theypropose to ensemble a distillation policy. The proposed method isevaluated on logged MuJoCo datasets answering whether BRED improves overother fine-tuning methods and whether balancing or replay contributemore to BRED's success.Decision--------BRED is an interesting approach, but the experiments do not demonstratethat it is better than the baseline. As such, my preliminary rating ofthis paper is rejection. I appreciate that the sampling scheme proposedis simple and well motivated to address the problem of distributionshift. It is not clear whether the sampling scheme actually addressesthe problem of distribution shift however, as it gives the agent accessto additional experience. The distillation scheme also does not seem toaddress distribution shift directly and the experiments do notdemonstrate that it helps performance outside of the Walker2d-randomtask. Moreover, it is not clear whether the baselines are alsoensembles, which would make the experiment results less compelling.Originality-----------The contributions are: a sampling scheme combining offline and onlinesamples, as well as a distillation scheme. The sampling scheme isincremental and simple, but seems like a novel approach. Distillation onthe other hand, is well-explored in RL (Czarnecki et al. 2019) and doesnot seem to contribute much to the problem addressed in the paper. Takentogether, the paper has limited novelty.Quality and Clarity-------------------The paper is easy-to-follow and well-written. However, I have someissues with the title. The title suggests that your aim is to addressdistribution shift in online RL. However, distribution shift is aproblem due to offline training. Of course, the problem you areaddressing is further online training after offline training. Perhapsthe title should be reworded to make this more clear.Strengths----------   There is a clear exposition of the problem addressed, namely    distribution shift. This is well motivated by the t-SNE    visualization. Section 3 in particular highlights the correlation    between distribution shift and the performance of offline RL    algorithms.-   BRED, the proposed method, is a straightforward solution via    balancing offline and online replay buffers. In addition, the    schedule for updating the proportion sampled of each replay buffer    is well argued and also simple.-   The results are clearly motivated by questions in the beginning of    the section. Sensible baselines are used to compare the proposed    method (BRED), and these are discussed and reasoned in detail.    Furthermore, there is a good coverage of tasks (3 MuJoCo) and    difficulties (random, medium, medium-replay, medium-expert).Weaknesses-----------   While fine-tuning an offline RL agent can pose challenges, similar    challenges can arise in supervised learning. For example, Ash and    Adams (2019) note that in different regimes, warm-starting neural    network training can hamper generalization. Overall, I feel that the    correlation between distribution shift and online performance after    offline training is not shown to be causal, but correlative.-   It is unclear why distillation is included, as it does not directly    address the problem of distribution shift in offline RL, outside of    the intuition that it mitigates bad $Q$ estimates. This is not    enough motivation however, and the experiment results (Figure 5    and 6) does not show any significant benefit of ensemble    distillation over independent ensembles (besides Walker2d-random).    Furthermore, this may not be a fair comparison to other baseline    methods which do not seem to be ensembles.-   Despite a very well-motivated experiment section, the experimental    conclusions are far too strong. The results do not back this up,    when there is high overlap in the confidence intervals for many    Figures (in each section). Furthermore, there are unexplained    peculiarities in the results, such as very big drops in performance    or the fact that the agent trained with the offline dataset is    substantially better than the medium one.Detailed Comments------------------   "pointed out that offline RL algorithms& are not amenable to    fine-tuning, due to the difficulty of modeling the    dataset-generating policy in the online setup."    This is not clear to me, how does fine-tuning connect to modelling    the dataset-generating policy? Furthermore, how does the    dataset-generating policy (I assume this is the behavior policy)    connect to online RL?-   Figure 1b: the agent is trained on the offline dataset and then that    dataset is thrown away for the online agent, whereas the agent in    red gets to keep using it. This does show the difficulty of    fine-tuning but does not provide conclusive evidence that    distributional shift is the cause. Fine-tuning can be difficult even    in supervised learning (Ash and Adams 2019).-   "On the other hand, when using online samples exclusively, the agent    is exposed to unseen samples only, for which Q function does not    provide a reliable value estimate. This may lead to bootstrapping    error, and hence a dip in performance as seen in Figure 1b."    Can't this be shown experimentally in an environment to provide    conclusive evidence for distribution shift being the cause of the    dip in performance?-   Figure 3: It seems that halfcheetah-random results in a better    policy than halfcheetah-medium. This does not make sense to me. In    addition, there is a strange dip in performance for CQL-ft in Figure    3g. Many of the figures have overlapping confidence intervals    (although the shaded region is only the standard deviation, the    corresponding confidence interval would be approximately the    standard deviation. Anyway these should be confidence intervals    rather than standard deviation). While BRED does perform    statistically significantly better in some tasks, I don't think the    comparison is fair due to ensembling used only for BRED.-   Figure 4: Going from figure 4a to 4b, balanced replay is shown to be    more beneficial for a dataset generated by a random behavior policy.    This seems to suggest that the main benefit of BRED is artificial    exploration. Counter-intuitvely, this trend is reversed in Figure    4c. Moreover, the online-only curve is very different in Figure 4c    than 4a and 4b, and to my understanding there should not be any    difference. Due to the noisiness of the curves, its hard to make any    straightforward conclusion from these results.-   Figure 5 and 6: Again, there is no statistical difference in medium    and medium-replay between independent ensembles and a distillation    ensemble.Minor Comments---------------   Equation 3: missing brackets on the left side.Ash, Jordan T., and Ryan P. Adams. 2019. On Warm-Starting Neural Network Training. *arXiv:1910.08475*.<http://arxiv.org/abs/1910.08475v2>.Czarnecki, Wojciech Marian, Razvan Pascanu, Simon Osindero, Siddhant M. Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. 2019. Distilling Policy Distillation. *arXiv:1902.02186*.<http://arxiv.org/abs/1902.02186v1>. Summary In this paper, the authors introduce three approaches for automatically finding an augmentations for any RL task, and a novel regularization scheme to make such augmentations work effectively.Positive aspects:=============The papers language is clear and the authors provide a good overview of the problem of data augmentation for reinforcement learning. Furthermore, they nicely explain why data augmentation for RL isnt as straightforward as augmenting data for supervised learning learning. I believe that data augmentation could be a nice tool in the Reinforcement Learners toolbox, and Im glad to see a paper advancing the idea.Major Concerns:============This paper has not provided sufficient evidence that the authors proposed way of doing data augmentation is effective. In this paper, there are two main novel methods for doing data augmentation / insights in RL. I will discuss my concerns with both methods separately.Policy and Value function Regularization.-----------------------------------------------------The authors criticize the naive application of transformers in the PPOs buffer, as done in Laskin et al. (2020), saying that this changes the PPO objective. While I agree that such a naive transformation as in Eq. 2 is problematic, I fail to understand why application of transformation to states in the buffer would result in the Eq. 2, as the transformation would happen to the states being fed into both $\pi_\theta$ and $\pi_{\theta_\text{old}}$, resulting in an equation different from Eq. 2. One just needs to save the old policies (and old Advantage function), so that $\pi_{\theta_\text{old}}$ can be applied to transformed states, and not just use the actions from the buffer. This would seem like a straightforward fix.Yet the authors have proposed a different regularization fix which judging the experiments does seem to work, as shown in Figure 2. I suspect it works for another reason: since the regularization forces $V(s) = V(f(s))$ and $\pi(\cdot | s) = \pi(\cdot | f(s))$ I wonder if this isnt a method to allow prior knowledge to flow into the policy and value estimation. If the transformation(s) $f_i$ have been chosen such that one can be reasonably sure that true value and policy functions should be invariant to said transformations, then by enforcing $V(s) = V(f(s))$ and $\pi(\cdot | s) = \pi(\cdot | f(s))$  one is constraining V and \pi to fit the prior knowledge contained in $f_i$.So now were comparing apples and oranges, since your method (DrAC) gets to incorporate this prior knowledge, while PPO and RAD dont, and DrACs good performance isnt surprising.Automatic Data Augmentation----------------------------------------Here, given some candidates for data augmentation, the authors propose three methods to discover which candidate work well. Unfortunately, I dont fully understand the approach. The authors are examining a Meta-Reinforcement Learning setting, where one wishes to find a policy which performs well not just on one MDP, but on a whole distribution of MDPs. This leads to an inner-and-outer for-loop like setting, in the inner for loop, the agent does multiple episodes with a single environment, in the outer loop the agent gets new environments.I had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations, and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment, and how its transferred from past environments to future environments.The only support given for these methods is the experimental results. Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another. So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best.Minor Comments:=============$T_m(s|s,a)$ is the transition function, $R_m(s,a)$ is the reward function: Here, $T$ and $R$ are generally distributions and not functionsEq. 6 is confusing, since $f_*$ can refer to both $f_t$ (function at timestep $t$) and $f_i$ (the $i$-th transformation function). Do the update with something like $N_t(f) \leftarrow N_{t-1}(f) + 1$ is a bit clearer, since then its the number of times $f$ has been pulled before timestep $t$ The paper proposed two methods for reducing the correlation of neurons within a layer so that the effective dimension of the last layer can be reduced as well, and the two methods include enforcing orthogonality on weight matrices, and batchnorm after activation functions, There are many issues regarding the claims and the experiments presented in the paper. 1. Is the paper trying to propose a novel way of measuring the similarity between inputs or to improve the vector representations generated from the last layer to be more reflective and indicative in terms of the fine-grained structure of the data? Those two topics seemed intertwined with each other, and also it seemed that the authors are using one as the supporting argument for the other. For the first one, the authors claimed that the reasons for the information loss from bottom to the top layer in a neural networks include (1) correlation of neurons, and (2) insufficient width of the last hidden layer. Let's assume that both reasons are valid, then why bother using neural networks for comparing similarity between datasets? One can certainly consider kernel methods for comparison, and specifically kernel two-sample tests with characteristic or universal kernels. IMO, I don't agree with the reasons listed by the authors. In fact, let's take VGGNet and ResNet34 pretrained on ImageNet for comparison. The dimension of the top layer of VGGNet is 4096, while it is 512 for ResNet34, and as known, ImageNet classification task has 1000 different labels. It means that those 1000 512-D vectors learnt in the top layer in ResNet34 are correlated with each other to some degree, while those 1000 4096-D vectors from VGGNet could be orthogonal to each other. However, as shown, ResNet34 generalises better than VGGNet not only on ImageNet tasks, but also on other vision-related tasks. In this comparison, clearly the architecture matters more than the dimension of the last layer and the correlation of neurons.For the second one, let's consider a linear system, ridge regression in particular. By adding a certain level of l2 regularisation on the parameter vector or matrix, it effectively moves the covariance matrix of the input data close to an identity matrix, which improves the orthogonality of the covariance matrix. It seems that adding l2 regularisation is a simpler way than ones proposed in this paper to reduce the correlation of neurons. 2. The example presented in Fig 1. is not a valid piece of evidence for the claims made in this paper.From the perspective of information theory, the definition of information is the thing/observation that reduces the uncertainty. It heavily depends on the subject of the stufy. Therefore, from classification perspective, one can say that, in Fig. 1a, the model has observed enough amount of information so that the uncertainty of the two neurons' behaviours is drastically reduced, while Fig. 2b still presents a large amount of uncertainty. The paper referred to the term 'information' many times without consolidating specific occasions. 3. The activation function applied in the experiments is tanh, which squashes values to be in between -1 and 1, and the authors proposed to apply BatchNorm after tanh activation function. I don't see how the batchnorm after tanh is helpful in anyways as both mean and variance of the neurons are bounded already.4. The comparison conducted on the MNIST dataset seemed missing some fundimental vectorisation and quantisation methods, including both learning-based or non-learning-based. If the goal was to find a low-D vector representation of data, one can consider many approaches without using neural networks. This comment goes back to my first point in a way that I am not super clear about what the paper is trying to solve.  This paper explores 8-bit floating point formats for the inference of deep neural networks. The quantization is applied on the weight and activation tensors, but computation engine remains in FP32.  To cover the different ranges of weight and activation tensors, the authors propose to use exponent bias. The authors did ablation studies on the impact of numerical formats, e.g. bit-width and exponent biases, on model accuracies. The experiments are performed on vision models on ImageNet, including VGG and ResNet 50/34/18. The paper is relatively clear written, and experiments seem sound, however, I have some major concerns on the objective and novelty of this paper. 1). It is not clear to me the objective of this paper. This paper introduces an 8-bit quantized inference framework. However, it is well-known that, 8-bit precision can be applied to popular DNN models to accelerate inference while maintaining model accuracies and many such systems have already been put into products (e.g. TPU). The state-of-the-art inference quantization work are primarily in the precision of 4 bit or less. But this paper did not compare their work to any of the latest inference works. Instead, the three references this paper compared to, i.e. (Wang & Choi, 2018; Cambier et al., 2020; Sun et al., 2019), are all for the acceleration of DNN TRAINING, where the challenging can be very different, e.g. quantization of gradient tensors.2). On the same note, this paper claimed advantages of using FP32 computation engine, however, the three training papers, by definition, can also be used in FP32 computations through the same high precision converting described in this paper. In addition, it is not clear to me how much one can benefit from quantization while keeping computation in 32bit? For example, one may save some memories space, but the whole inference will be limited by the FP32 computations in terms of throughput, latency, power and cost. Can the authors provide an application case where only quantizing data is beneficial?3). The main contribution claimed in the paper are to use exponent bias to cover tensors with different range of distribution. However, this technique has be introduced and discussed in (Sun et at., 2019) paper and has also been investigated in detail in (Cambier et al., 2020) paper. The authors of this paper did not provide any new insight although using in a much simpler case, i.e. only forward paths.I think this paper lacks clear objective, novelty and insightful analysis, therefore not good enough for ICLR. Review: This paper proposes SplitSGD, a novel heuristic for adapting the learning rate. This novel learning rate schedule is based on the identification of stationary phases, where for stationary phases the authors refer to the training stage where the noisein the gradient estimate becomes dominant and therefore the iterates start jumping around a stationary point.The authors develop an heuristic to detect such phases. When a stationary phase is detected, the learning rate is decreased of a factor $\gamma$. Under some assumptions, the authors provide a theoretical analysis for thestationary phase detection mechanism which also underlines the relation with the learning rate value. SplitSGD is then benchmarked and compared against some of the standard learning rate decaying heuristics generally adopted in combination with SGD or other first order stochastic methods.___________________________________________________________________________________________________________________ + Overall the paper is well-written and clear.___________________________________________________________________________________________________________________ Concerns: 1. The learning rate, also called step-size, can be interpreted as a brutal approximation of the local curvature with a scaled identity matrix. All the work on second-order methods attempts to refine this brutal approximation with better estimates of the local curvature. Despite being the learning rate and how to adjust it the central topic of this paper, nothing is mentioned regarding thefundamental relation of the learning rate and the local curvature.2. There are some wrong statements here and there in the paper, i.e."""Specifically, in the case of a relatively small learning rate, we can imagine that, if the number of iterations is fixed, the SGD updates are not too far from the starting point, so the stationary phase has not been reached yet. """3. The authors claim to propose an optimization method while what they are proposing is an heuristic to decrease the learning rate in an adaptive fashion. 4. The costs of the stationary phase check are not discussed but the authors just briefly mention that SplitSGD comes with no significant extra costs. This does not seem to be the case though.5. The literature on increasing the batch size toward the final part of training is not discussed at all, as well as other state-of-the-art heuristics to deal with the noisy gradient estimate.6. The benchmarks are limited as the authors are not considering state-of-the-art learning rate  decaying heuristics such as the cosine decaying schedule. In addition, they could show more solid empirical results by letting an hyperparameter optimizer such as BOHB choose for the best learning schedule. Otherwise it is hard to say that the superior performance of SplitSGD is not a consequence of a wrong hyperparameter tuning of the competitors.7. I think the deep learning community at this stage needs less work on learning rate heuristics and more attentions on the theoretical analysis of the geometry of the landscape, novel optimization methods with theoretical guarantees and solid work on generalization properties, as there has been enough works on such heuristics which often ends up 'adding extra noise' and therefore constitute an obstacle in the process of bringing clarity and understanding in the field. This paper proposes an improvement of Binary Neural Networks from the work of Bi-RealNet (Liu et al. 2018) by utilizing the linearity of modules. The method removes the necessity of scaling factors and used a novel non-linear layer named Fully Parametric ReLU (FPReLU) to increase the capability of BNNs. In addition, similar to recent works in BNNs, the paper also utilize group convolution layers to reduce the number of parameters and save the computation's cost. The experiments are tested with image classification tasks on the large-scale dataset of ImageNet. The paper is well written and straightforward.  Although the results in ImageNet are promising and the computation is less, It raises concerns about the novelty of the work. The method used alike structure in Bi-RealNet with a bit of modification and group convolution which are already used in previous works of Liu et al. ECCV 2018 and Phan et al. CVPR 2020. The paper used novel FPReLU but missed some comparisons with a similar idea work using PReLU[1,2].For results in ImageNet, it would be nice if we can do more comparison and analysis with the state-of-the-art recent work [3].  In conclusion, with current manuscripts, the paper is not sufficient enough to present at the conference.[1].  Tang et al. How to Train a Compact Binary Neural Network with High Accuracy? AAAI 2017.[2]. Phan et al. MoBiNet: A Mobile Binary Network for Image Classification, WACV 2020.[3]. Liu et al. ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions, ECCV 2020. This paper aims to indicate a new direction to conduct an adversarial attack: to delay the inference time/raise the computational costs of a ''multi-exit'' model.However, both the motivation and novelty are lacking, I will argue for the rejection if other reviewers wish to accept it.From the perspective of the industry:1. We do not actually use the so-called ''multi-exit'' architectures, because they could be difficult to optimize, quantize, assemble, and so on. For mobile devices, we deploy different ''single-exit'' architectures to meet their computing capacity and balance the performance. And it is even trivial to maintenance these architectures because we have developed automation tools.2. On the cloud servers, the deployment is redundant, such that the service will still work stably, even all the queries require the maximal flops. 3. After all, it is easy to limit one's Queries Per Second (QPS), and nothing need to be worried about.From the perspective of academia:1. This paper only employs the PGD attack and the adversarial training on a specific task, the novelty is lacking.2. In section 5.2, if one knows the architecture and the training set and the task, how could this configuration being called ''black-box''. The paper presents a modification to conditional batch normalization, wherein an extra affine layer is introduced between the standardization and the conditional affine layer. The empirical results show the benefits of introducing this extra "sandwich" affine layer.The intuition behind the approach makes sense, but the formulation makes it difficult to see why SaBN is in fact beneficial compared to CCBN. It does not appear that the approach imposes any restrictions or regularization on the CCBN affine parameters; therefore the only difference between CCBN and SaBN seems to be a different parameterization. I would then expect both CCBN and SaBN to reach the same optimal training loss. It may be that the reparameterization provided by SaBN yields the optimization trajectories that lead to better-generalizing solutions, but it is not explained in the paper whether, or why, this happens. One way to probe this might be to reparameterize each gamma in BN or CCBN as $\gamma=\gamma_1 \gamma_2$ (or even $\gamma = \gamma_1^2$) and study the behavior of the resulting model.The paper proposes to measure the heterogeneity in the found representations (between different branches of CCBN) via the CAPV measure. In its definition, I could not find what the overbars signify but I assume it means the average over the channels. Also, the indexing of gammas from 0 to N should probably be from 1 to C, for consistency with Eq. (2). The definition of CAPV as the variance of gammas seems problematic, however, in ReLU models with batch normalization: models whose gammas differ by a constant factor represent the same function, so the variance of gammas can be arbitrarily changed without affecting the model. A more useful measure of heterogeneity would need to take this scale invariance into account.The paper shows several empirical studies, including one for architecture search -- with the main paper using DARTS, and the appendix using GDAS. This choice seems suboptimal, given that in the NAS-Bench-201 paper (https://openreview.net/pdf?id=HJxyZkBKDr), Table 5 seems to indicate that DARTS performs much worse than GDAS. In this work, the appendix devoted to GDAS seems to indicate that (1) there is no consistency on using or not using affine, between CIFAR-100 and Imagenet, and (2) GDAS-SaBN is not statistically significantly better than the better of GDAS and GDAS-affine.The results in this paper are encouraging, but I believe the paper needs to explain more clearly why SaBN is expected to work (given that it preserves the hypothesis class as well as the minimum of the training objective). Additionally, since it appears to amount to a per-dimension reparameterization, the reader might expect that some of the other reparameterizations could have a similar effect (including such simple interventions as changing the learning rates for some of the gamma or beta parameters), and compellingly demonstrating that the specific reparameterization given by SaBN outperforms such alternatives would make the paper stronger. This paper analyses the implicit regularization in matrix factorization. The author suggest that for infinitesimal initialization the implicit regularization can be equivalently understood as iteratively solving matrix factorization with increasing rank until the rank is high enough such that the model can be fitted to zero loss. They provide certain theoretical evidence for their hypothesis. Furthermore, they analyze certain models with higher depth, claiming that in those models the described phenomenon appears already with "less infinitesimal" initialization.As another contribution, the authors claim that their result refutes a conjecture by Gunasekar et al by giving a concrete counterexample. However, I think their counterexample is invalid:$M_{\text{rank}}$ does not have rank 1 as the authors claim; in particular, it is not even symmetric and, hence, it cannot be represented in the form $UU^T$. Also, I do not believe that there is a rank matrix, which can represent $M$. Hence, $M_{\text{nuc}}$ is the matrix of smaller norm, which fits to the observations.This mistake is the main reason for my low score. I am willing to raise my score, if these issues have been addressed.Further comments:-Do I understand it correctly that Theorem 6.3 and its interpretation below do not predict any different behavior as long as the number of layers is chosen larger than $3$? Moreover, I am not sure whether I find this argument very convincing: The authors claim that one advantage of deeper layers lies in the fact that it keeps the low-rank component more separated from the learned component. I agree that Theorem 6.3 gives some hint, but there is much more evidence required and at its current state this is rather speculative.Typos:-p.4: sixth line from below: Do you mean  $ \mathcal{L} (U_{r-1}) \approx \mathcal{L} (U_r)- \epsilon \mu_1$ instead of $ \mathcal{L} (U_r) \approx \mathcal{L} (U_r)- \epsilon \mu_1 $?-p.4 second line from below: $\phi$ is not really defined yet, if I make no mistake-Theorem 5.3: $\tilde{\gamma}$ is not yet defined-p.7: seventh line from below, $T_{(}' \alpha )$After having another look at the paper, I noticed that my complaint could be resolved easily in the following way: In $M_{rank}$ you replace 100 with $R^2$ and then the resulting matrix is symmetric and rank-one. Is this just a typo? I apologize that I missed this. Furthermore, I wonder whether the proof in Appendix H could be clarified. I feel that the part where the assumptions 5.5 and 5.7 are verified is not particularly clear and more details would be helpful.(Of course, I realize that I am late with this particular request, but I still believe that in this way the paper could be improved.) I find the problem setup a bit superficial and I was not convinced from the examples in the experiment section  that this is a meaningful real world situation.What is the parametric modeling of  p(y|z,c)?What is the parametric modeling of q(y|x)? Is it actually q(y|\mu_z) where mu_z = E(z|x)?In (6) do you have a tunable regularization coefficient for the regularization term?The notation in (5) is odd. z appears on the right side of the equation but not on the left side.In the last experiment (medical data) you show clustering results  but their is no reference in the results to the recurred binary indicator. We dont know whether  this information improved the clustering. Evaluation and improvement of fairness of machine learning algorithms is a very important issue. To this end, the authors of this paper propose a post-processing algorithm to enforce fairness in a narrowly defined notion of fairness. Unfortunately, I have serious concerns about the validity of the results and conclusions of the paper, and hence I cannot recommend the paper to be accepted.[**Definition**] This entire paper is only applicable to a narrow definition of fairness in Definition 1. Given that $a$ and $b$ are binary, Definition 1 (page 2) is essentially equivalent to assuming that $P[a= b =1 |c] = P[a= 1 |c]  P[b= 1 |c]$ which is weaker than the well-known demographic (statistical) parity, and is only applicable to a binary setting. Hence, the applicability of the proposed algorithm is extremely limited.[**Theory**] The theoretical exposure in this paper is confusing and not rigorous. The trouble begins from the unnumbered equation in page 4, where the authors define $\widetilde{h}$. It is unclear what $\widetilde{h}$ depends on. Also, the output is seemingly binary, but the authors claim this a _randomized prediction rule_ and $\widetilde{h}(\mathbf{x})$ represents the probability of predicting the positive class. **Proposition 1** is an obvious statement and not relevant to the claims of this paper, and hence should be removed. **Theorem 1:** In the proof of Theorem 1, the authors make an assumption about what the output of $f$ is. In different places the output is assumed to be in $[0,1]$ and $[-1, 1]$!! Even worse, they claim that the empirical average of some loss value is equal to its expectation (_what happened to generalization?_). That is where I stopped reading.[**Experiments**] The experiments are very weak and not convincing. The paper only compares with Hardt et al. but in an unconvincing way as detailed here. **(a)** the comparisons should be made in terms of a tradeoff curve between _fairness_ and _performance_. In the currently reported results, there are instances where Hard et al. and the proposed method are not comparable, e.g., kNN. **(b)** The comparisons are unfair because they are done with respect to Definition 1, which is a very narrow sense of fairness while Hard et al. impose fairness either with respect to _equalized odds_ or _equal opportiunity_  which require conditional independence of $\widehat{Y}$ and $S$ given $Y (=1)$. Hence, the comparison with Hard et al. is unjustified. At the very least the authors should compare with a plethora of baselines designed for demographic parity (e.g., Kamiran et al. 2012, Zemel et al. 2013, Feldman et al. 2015, Zafar et al. 2017, Jiang et al. 2019, Baharlouei at al. 2020). Even then, the comparison should be done with respect to the statistical parity violation as defined in Dwork et al. 2012, which is a more established notion of fairness. **Summary**:The paper presents a novel steps-size adaptation for the L-BFGS algorithm inspired by the learning-to-learn idea. The step-size policy is determined by two linear layers which compare a higher dimensional mapping of curvature information which is trained to adapt the step-size.**Reasons for score**:While the idea of learning to predict a suitable step-size is intriguing and is definitely worth pursuing I am not convinced that the proposed algorithm results in an active policy that usefully adapts the step-size. There are too many concerns that I think needs to be addressed and it is not clear if the speed up improves over the reliability of a line search. I therefore vote to reject the paper in its current form.**Pros**:- It is clear that a lot of thought has gone into the project to come up with the policy. I think it might have merit but requires additional tests.- The figures were first difficult to understand but once the content had been explained in the text the benefit of the chosen presentation became clear.**Concerns**:- My main concern is best visualized in Figure 3. Both the learned policy and the BTLS seem to mostly favour a step of 1 which raises several questions. 1) What would be the results of using no adaptation and rely on a step of 1 (or 0.9) constantly as a baseline? 2) The $\pi$-algorithm mostly uses a step-size of 1 which happens top be the upper boundary $\tau_M$, which means it is not clear if the policy network has learned that 1 is a good step or if it has not learned at all and the results are just due to the clipping. What would happen if $\tau_M>1$ for example? 3) Given that both the BTLS and $\pi$ mostly use $t_k=1$, is there any intuitive explanation as to why the results between the two algorithms differ by so much in figure 3? Are there additional figures where the BTLS similarly outperforms the competition (it did reach $10^{-5}$ first in ~60% of the tasks according to table 1, column 1)? This despite the fact that BTLS is at least 1 forward pass more expensive per iteration than the policy (for a fully connected network I think that is ~50% of the iteration cost).- The benefit of using the double network for the policy is not clear to me. What would be the result of using a single linear layer instead or a recurrent network that monitors temporal changes to the curvature information that is used as input?- Given that the input and output dimensionality of the policy network is of low dimension it would be interesting to see what the weights and biases look like for respective policy layers. By looking at the weights it would be possible to see what curvature information makes the network decide to adjust the step-length. Does the policy learn a cosine behaviour similar to the proposed possibility in the appendix? - Could the policy be used for another optimization algorithm for which $-g_t^\intercal d_k>0$, such as RMSprop or GD? It might be easier to understand the influence of the policy in such a setting.Comparably minor points:- Section 3 first paragraph ends with a statement regarding $\rho_i$ to keep the Hessian approximation s.p.d by ignoring iterations. Is this used in the implementation? - Table 1: According to what metric is "first" defined (iteration, time, function evaluations)? It would be good to mention the average value of this metric for each optimizer at the end of $K=800$ inner steps.- In Section 6 it says that the learning rate of Adam and RMSprop was chosen to yield fast convergence. I understand this as quickly reaching a threshold, not achieving best performance for allocated time. Is this correct? That could help explain why so many settings in table 1 and figure 4 fail to converge. Personally I think the first-order algorithms should be allowed to change the learning rate between problems to prevent them from not converging (ex. RMSprop).- Eq.7 s.t. -> with, or is the outer problem actually constrained and I missed something? This paper investigates the security of InstaHide, a recently proposed algorithm for scrambling a secret image data set so that it is still useful for learning but doesn't allow inferences about individual images in the data set. The paper provides an algorithm that takes the output of InstaHide, and reconstructs the secret images (in a particular parameter regime of very high-dimensional data). The attack works by reduction to a new variant of the phase retrieval problem with hidden inputs.The paper's results appear to be correct. I am not familiar enough with the phase retrieval literature to judge the technical novelty and interest of the new problem variant and algorithm. The authors claim that the new variant is of independent interest, but don't really spend time justifying that.  I remain neutral on that point. However, I disagree strongly with the authors' interpretation of their results for InstaHide's security, and do not think that they rise to the level of significance appropriate for ICLR.To explain the setting a bit: InstaHide starts with two data sets of images (vectors in $\mathbb{R}^d$)  called the public and private data sets, each of size $n$. Given parameters $M$ and $k$, it produces $M$ synthetic output images. For each output image, one does the following:1. select $k$ public images and $k$ private images (uniformly at random)2. let $y$ be the sum of the selected images3. Output a synthetic image $|y|$ consisting of the absolute values of the entries of $y$. (I've simplified the scheme a bitthe $n$'s and $k$'s can be different for the public and private databut the simplifications don't change this discussion.) The paper mentions $k=2$ as a reasonable value.The attack recovers the synthetic data set roughly when $d$ is reasonably large (at least $poly(k)\log(n)$) and when $M$, the number of synthetic data points, is very large (something like $n^k$). As mentioned above, the algorithm appears to be correct. **On InstaHide's security**I disagree with the authors' claims of their results' significance for security. I doubt that this paper would be acceptable at, say, a security or cryptography conference. The main issue is that InstaHide looks broken to begin with, even when only 1 image is output. The submission's main question is *"Under what settings can we rigorously prove that InstaHide preserves security of the private datasets used to generate the synthetic vectors?"*. Asking such a question requires first answering a seemingly much more basic question: *"What **clear, refutable conjecture** could one make about InstaHide's security that is both (a) sufficient to suggest it is reasonable to use on sensitive data and (b) not obviously false?*"Unfortunately, I'm not sure there is such a conjecture. For InstaHide to be a reasonable approach to image data privacy, it should be hold up to settings where an attacker knows quite a bit about the format of the input. Suppose, for example, that the images themselves are "sparse" in the following sense: each consists of small grayscale image, randomly placed on a much larger black background. Then the linear combination of a small number of them would also be sparse with high probability, and its absolute values would exactly reveal all of the component grayscale images. One would start getting attacks with $M=1$. For a more realistic but messy example, consider a setting in which the images are screenshots where most of each screen is text but  sensitive pictures may appear in different parts of the screen. Finally, Matthew Jagielski shared the following notebook with many people (including the authors of InstaHide) which works out a simple example with 2 images:https://colab.research.google.com/drive/1ONVjStz2m3BdKCE16axVHZ00hcwdivH2?usp=sharing(The images, one of a dog and the other of a bird, are easy to pick out from the absolute values of the combination.)**Implications for this submission**Because of the known problems with InstaHide,  it's not clear to me what light the submission really sheds on InstaHide's security. The example attacks above shows that it isn't hard to find natural distributions and settings of $k$ for which InstaHide allows recovery of the inputs. (Aside: doesn't the requirement of $M\approx n^k$ makes this attack unlikely to be practical, unless very small $k$ is necessary for utility?) The question in my mind is whether any reasonably robust security or privacy claim could be made. The last twenty years have seen major advances in how to formulate such claims, but those advances are not reflected in either the original paper or this follow-up.I would encourage the authors to rethink the framing for the question they address. Perhaps they can argue that the complexity of their setting is important for understanding InstaHide's security, or perhaps they can make a case that the new phase retrieval problem and algorithm are interesting in their own right (and rewrite the paper as a function of that)? Absent such a reframing of the paper, I don't think it should be accepted. **Minor Comments*** Page 2:  "to ensure we are not working in an uninteresting setting where InstaHide has zero utility, we empirically verify that in the setting of Theorem 1.1, one can train on the synthetic vectors and get good test accuracy on the original Gaussian dataset". It is fine to do experiments in the setting where InstaHide is useful, but ensuring high utility would seem to me to make attacks easier, not harder.* Page 3: "consider the extreme case where InstaHide only works with private feature vectors[...], so that the only information we have access to is the synthetic vector generated by InstaHide. In this case, it is clearly information-theoretically impossible to recover anything about [which combinations were chosen] or the private dataset." I strongly disagree. I don't see why that's true in the Gaussian settingisn't the mutual information between the synthetic data and the secret data infinite (since the coefficient vectors are discrete)? In any case, the "sparse images" setting above gives a simple illustration of how absolute values of linear combinations can leak tons of information. This paper proposes to better represent relationships between different variables within time series and use the result to perform anomaly detection. The experiments show reasonable results. However, the algorithm needs to be presented in a cohesive way to be able to understand what is actually being done. Various steps are presented, but they are not tied together in a way that a reader can implement the algorithm. A detailed algorithm explanation and pseudocode would give all the details needed, including some of what are referenced below.1. The introduction has a sentence "In practice, the overall status of an entity is more concerned about than each constituent metric..." I don't understand the meaning of this sentence.2. Section 2. Note that, with OCSVM, time series or sequence-oriented kernels can be used to enable it to be used with time series. E.g., Das, et. al., "Multiple Kernel Learning for Heterogeneous Anomaly Detection: Algorithm and Aviation Safety Case Study," KDD 2010.3. In section 3, it says that GenAD sets the duration of the training portion of the time series to be four times the duration of the testing portion. How was this chosen? What different splits were tested?4. Section 3.2.1: The proposition is not stated in a rigorous enough way to be able to tell what is being proven in the proof that is below it.5. In section 3.3, it says that GenAD selects a random 20% of the dimensions and masks them with fixed random series. Is a different 20% chosen with every time series or for a given problem? How do the results vary depending on the dimensions chosen? Paper proposes a way to adapt an autoregressive model (RNN in examples) to the incoming noisy signal to generate noise-free data output. The approach is interesting due to applying updates to the hidden state of the past observation. The proposed approached is named Active Tuning and evaluated on 3 toy tasks. The idea sounds interesting, however the lack of comparisons with other approaches and theoretical justification of why this approach is superior makes it hard to convince reader. Quality: Paper is well written and most of the concepts is clear. However, paper will benefit from a better explanation of the method, simpler diagram and equations to remove uncertainty on implementation. Originality: I believe the idea is novel and interesting for community. It has a potential to outperform meta-learning and sequence-to-sequence models on the task of model adaptation to noisy samples. Pros:- Idea is interesting and has potential. - Explanation is clear, but still can be improved. - Provided experiments show benefits of the proposed method with respect to direct regression task (same model trained with less or more noise amount)Cons:- Comparison with other techniques such as meta-learning, sequence-to-sequence models is required to understand the potential of the method.- Same comparisons might be interesting for tuning weights instead of hidden states. Or having only a small part of the model to be tuned (like the last layer). - Application to more practical problems could benefit the paper. For example image denosing task could be relevant (works like Noise2Noise, Noise2Self etc) 1. StrengthsThe authors target an important problem in Federated Learning: how to personalize the model to mitigate the nonIIDness.2. WeaknessThe proposed method is not novel. The third step which fine-tunes in the local and global models using a gate network is essentially fusing the global and local models. It is surprising to me that this method works better than fine-tuned after FedAvg. Most importantly, such an empirical method lacks analysis or convincing experimental results.Hyper-parameters are not well-discussed. The author mention that all experiments use the same learning rate 0.0001. This is definitely misleading. We have to adequately tune the hyper-parameters for each baseline and then make a fair comparison. Using the same learning rate for all baselines are wrong experimental settings. I believe fine-tuning after FedAvg can even get comparable performance if fully tune the hyper-parameters (learning rate, decay, batch size, epochs, rounds, etc).The authors claim that client and global models are not constrained to be the same model and could be implemented any two differentiable models. However, the authors do not provide the experimental result for this argument. I guess when the model architecture is different, the difficulty of hyper-parameter optimization will increase, which weaken the application of the proposed method.The proposed method has a severe efficiency problem. It requires holding three DNNs at the edge. This is impractical in federated learning where the edge devices are mainly resource-constrained (low memory, low computational ability)The training time is not mentioned.The proposed method does not use a client sampling strategy, a common practice in cross-device FL, to mitigate the scalability issue. What the performance if we want to learn 10 thousand sensors? Please check the original FedAvg for details.The dataset CIFAR10 and CIFAR100 are not difficult enough to demonstrate the concept of the proposed algorithms. Does it still work in a high-resolution setting like ImageNet (224*224). I believe training three DNNs will lead to serious efficiency issues.The opt-in and opt-out strategy is totally empirical without any intuition about why it works. That the author connects this strategy with a privacy guarantee is somewhat misleading to readers. Please provide an analysis in revision and properly describe the benefit.In the Introduction section, the following argument is a lack of evidence. Please cite related works to make the argument more convincing. Extended phases of local training between communication rounds can similarly break training, indicating that the individual client models will over time diverge towards different local minima in the loss landscape. Similarly, different distributions between client datasets will also lead to divergence of client models.The overall writing does not affect my understanding but can be improved. Related works are not fully discussed. In some knowledge distillation-based method, the personalization is also their benefit. For example, FedGKT [1] also has a personal client model and a global server model, which is just a single step training method without the need of multiple steps training.[1] Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge. https://arxiv.org/abs/2007.145133. Overall ScoreGiven the above concerns, I recommend reject this paper in the current stage. 4. QuestionsMay I have the comparison results between "the naive fine-tuning after FedAvg" and the proposed method? Please fully tune hyper-parameters for each baseline.5. SuggestionsI encourage the authors to do a deeper analysis and a better experimental design. If all the above concerns are addressed, I am happy to increase the score. The proposed method is a federated method allowing to have a certain amount of data shared between all the learners and some data specific to each learner. The targeted field of application is classification for problems where strong privacy is crucial. The method consists in learning a global classifier (with the shared data) as well as local classifiers (one per learner, using the local data). The inference, for each learner, is done with a local expert (another neural network) trained to combine inferences from the local and global models.The first objection I will make to this paper is the confusion it causes when talking about privacy. Federated learning methods, in a context of privacy protection, aim at building a global model, using private data, without revealing anything about the private data. Here, private data are not used to build the global model, and the local model obtained is not intended to be given, which makes talking about privacy seem irrelevant to me. For the reason explained above, I wonder about the motivation of the paper. Isn't the best solution, in this case, simply to learn a standard classifier for each local problem, using both local and global data and using a loss function that weights the examples according to the class distributions? It could be argued that if there are many local problems it is more expensive than learning a global classifier and combining it with a local classifier.  In this case efficiency is not privacy is the motivation and the gain should be evaluated in terms of efficiency. This is true, but in this case it becomes a domain adaptation problem: a model in which the distributions of p(y_i) are different from those of the local problem is given and should be used to improve the local problem. In this case, the domain adaptation methods apply and should have been investigated by the authors.My last comment is on equation (8). I do not understand the relationship between equation (7) and (8). In (7) only the weights of the expert are learned. In (8) all weights are learned. If equation (8) is applied, this means that local data is used to learn the global model through w_g, so there is a leak of local information which contradicts the strict respect of privacy.   The paper proposes a federated learning framework using a mixture of experts to trade-off the local model and the global model in a federated learning setting. A three-step pipeline is designed to train personalized FL with a mixture of global model and local model. Pros:1. The proposed setting is a new scenario that considers both opt-in and opt-out devices.2. The mixture of the global and local model is a reasonable solution to solve the personalization problem in FL. Cons:1. The paper lacks an overall loss function for the whole procedure. In particular, the three steps have three different loss functions, and the updating of the shared global model will cause inconsistent in minimizing multiple loss functions defined by equations 4, 5, and 6. 2. In Section 4, there is a big blank space before the section head and Table 2. The content layout and arrangement could be improved. 3. In Table 1 dataset CIFAR-10, the number of clients is 5 that is very small number for federated setting.  Moreover, the CIFAR-100 data only divided into 50 clients that are also a small number for FL.4. In Table 2, p = 1 indicates that each client has two classes only. However, the results of FedAvg are 17.13% that is much less than the reported results (85%) of the FedAvg paper by Google. 5. Authors claim the proposed method can protect user privacy since a client can select which data need to be excluded from the federation. However, no corresponding experiments support this claim.6. Convergence of the gating function is not discussed. The paper proposed a novel personalized federated learning method using a mixture of global and local models. In particular, a gating function is proposed to leverage the trade-off of two models on the device, and this solution is inspired by a classic work  Mixture of experts (Jacobs, 1991).Pros:1. The paper proposes a new federated setting by considering opt-in and opt-out. 2. In step 3, the training of the local mixture-of-experts method doesnt need to upload data and gradients to the server-side. 3. The proposed personalized federated learning framework is a practical solution. Cons:1. The proposed framework requires each client to choose which part of the data is sensitive. This is a very strong assumption in real-world applications.2. The proposed framework includes three steps. Step 1 & 2 are FedAvg and local supervised learning that are existing methods. Step 3 is to train a personalized local model by mixing local and global models. The mixed-use of global and local models (equation 6) is not a novel way of federated learning. The only novelty part of the method is to apply the mixture-of-experts (Jacobs et al., 1991) method to combine the local and global models. 3. The papers writing could be improved. The paper is an integration of the mixture-of-experts method with existing personalized federated learning. The paper's contribution is incremental.4. The authors should add a discussion to clarify how the old method (mixture-of-experts) can fit into the new environment: federated learning setting and deep learning. 5. To be a paper with self-contained contents, the paper should give a clear definition of the gating model/function h^k, and how to solve the optimization problem described in equation 8.6. An overall workflow or architecture figure is recommended. 7. An algorithm description is also required. 8. The experiment part is too weak to validate the effectiveness of the proposed method. For example, more datasets and baselines are required.9. A typo: We denote the number of clients by k& It should be denote the index of clients by k &  This paper examines n-gram level features encoded within the hidden states of recurrent neural models. The proposed method approximates the hidden states of LSTMs and GRUs with a first-order Taylor series, which the authors claim is an adequate approximation with small enough inputs. The authors apply their method to models trained on synthetic sentiment analysis and language modeling datasets. The paper is difficult to understand, and many assumptions are not properly justified. The experiments are also not convincing, as a large portion of the analysis focuses on small synthetic data, and some of them do not have clear takeaways or explanations as to why they were conducted in the first place. Overall, I cannot recommend the paper's acceptance in its current form.comments/questions:- is the scenario of extremely low input magnitudes realistic? how generalizable are these findings to standard initializations used in NLP architectures-  similarly, on page 3 the authors assume that the higher order terms of h_{t-1} are "insignificant"; in practice, it is unclear how often this is true. i wish the paper would contain more justification behind these assumptions, as they are critical for judging how faithful the approximation is and thus how useful the proposed method is for diagnosing RNNs.- can't the authors actually show quantitatively how good the approximations are? if the higher-order terms indeed do not affect the quality of the approximation, that could be justified by some experiments. i'm not really convinced by Table 2: the approximations could be quite different from the original model but still yield good downstream accuracy.- what is a "polarity score" (bottom of page 5)? I didn't quite understand what this is supposed to represent, is it how predictive of a label a particular span is? - why are synthetic datasets used at all here? experiments on a small set of sentences with a tiny vocabulary and artificial "double negatives" are not compelling. Appendix A.2 does not fully specify this dataset (nor motivate why it was created); what is e.g., its average sentence length?- the results on a real sentiment dataset (SST2) are confusing (sec 5.1.3): what does figure 4 show me that I couldn't already learn by simply passing those two phrases into the model as separate inputs and looking at the model's prediction? - what is the point of training models with the approximations instead of the original GRU/LSTM cell equations?  i don't understand the significance of Table 3. The paper aims to learn graph structured logical rules on knowledge graphs. It proposes three tasks, which I summarize as learning rule structure, instantiating a graph-structured rule (fill in a specific relation on the edge), and answering logical queries. However, many statements are entangled and mixed, the paper has many typos and inconsistency, making it hard to follow. I suggest the authors reformulate the tasks and the order or you can focus on one aspect/task and make it crystal clear.Questions:- What is the difference between logic rules and logical queries? The paper abuses terms like logical rules, logical queries, logical rules for query, making it extremely hard to follow. Are you learning logical rules or answering logical queries? - According to the definition 1, figure 1 is not showing 3 logical rules but rather 3 logical formulas/queries, because there is no $R_{cpx}$.- I don't quite follow and agree with the statement Note that, $R_{comp}$ can both be a relation that exists in the KG and the human-defined logic rule for a query, which tends to be more complex I think $\wedge_i^n R_i$ can be complex, no matter it is a chain, tree or graph, but $R_{cpx}$ should always be a concrete relation that exists on KG, otherwise its meaningless, because $R_{cpx}$ can simply represent anything. (I assume $R_{comp}$ is a typo and you mean $R_{cpx}$ here)- Here is another confusing statement Note that, in previous works (Hamilton et al., 2018; Ren et al., 2020), the logical rule $R_{cpx} = \wedge^n_i R_i$ is given, according to your definition 1, shouldnt the logical rule be $\wedge^n_i R_i \to R_{cpx}$? - What is the relationship between $e(v_i, v_i)$ and $R_i(v_i, v_i)$?- What is $G$ at the start of sec. 4.1, is it $G_e$ in section 2? There is also no $G$ anywhere in figure 1, you need to add some pointers.- We denote the adjacent matrix of relation $R_i$ as $A_i$ , which is a soft choice among adjacent matrices $A_k$ corresponding to all relations in R. Then which one is the adjacency matrix $A$ or $A$ bar? According to your statement, it is $A$, but according to Eq. 2, it is $A$ bar.- $\beta_{ik}$ comes out of nowhere. Why do you want to have this new parameter and what is the motivation?- Why do you want to by which we can use {$\beta_{ik}$} as learnable parameters to learn which relation should be assigned on each edge given the logical structure.? I think you already have instantiated the edges right? E.g., in the right case in Figure 1, you already know that some edges are read and some edges are friend.- The use of $S_r$ is not rigorous, for Eq. 3, it should be $s_r$({$X$},$Y)$ instead of $s_r(X,Y)$.- Why do you have $A_1$ to $A_5$ in Eq.3, but in the example, you only have 3 relations: read, read(inv), friend.- In Sec. 4.3, its not (1), (2), but rather Eq. (1), Eq. (2)- Add reference and citations to the statement in the Computation Complexity paragraph.- I did not find how the model answers logical queries (task 3) in the inference step, i.e., given a new $(${$X_j$}$,R_{cpx})$, how to find $Y$.- In the case study section, it only shows how the model deals with task 1, i.e., when the logical structure is given. Considering there are 3 tasks, it would be better if you can show examples for the other 2 tasks.- The datasets in the experiment section are small-scale and synthetic. What is the real-world application of the model? You claim that For more complex rules, we generate a synthetic dataset because the real-world datasets are usually noisy., but isnt the whole goal of the paper to model the real-world data and robust to noisy links?- What is the evaluation setup? Table 2 lists the performance of query answering, (task 3), where are the results for the other 2 tasks? Also, I assume DRUM/Neural-LP can only handle link prediction in the original paper, there lacks details how you improve DRUM to model complex queries. # SummaryThis submission proposes a multi-task convolutional neural network architecture for end-to-end driving (going from an RGB image to controls) evaluated using the CARLA open source simulator. The architecture consists of an encoder and three decoders on top: two for perception (depth prediction and semantic segmentation), and one for driving controls prediction. The network is trained in a two-step supervised fashion: first training the encoder and perception decoders (using depth and semantic segmentation ground truth), second freezing the encoder and training the driving module (imitation learning on demonstrations). The network is evaluated on the standard CARLA benchmark showing better generalization performance in new driving conditions (town and weather) compared to the CARLA baselines (modular pipeline, imitation learning, RL). Qualitative results also show that failure modes are easier to interpret by looking at predicted depth maps and semantic segmentation results.# StrengthsSimplicity of the approach: the overall architecture described above is simple (cf. Figure 1), combining the benefits of the modular and end-to-end approaches into a feed-forward CNN. The aforementioned two-stage learning algorithm is also explained clearly. Predicted depth maps and semantic segmentation results are indeed more interpretable than attention maps (as traditionally used in end-to-end driving).Evaluation of the driving policy: the evaluation is done with actual navigation tasks using the CARLA (CoRL'18) benchmark, instead of just off-line behavior cloning accuracy (often used in end-to-end driving papers, easier to overfit to, not guaranteed to transfer to actual driving).Simple ablative analysis: Table 2 quantifies the generalization performance benefits of pretraining and freezing the encoder on perception tasks (esp. going from 16% to 62% of completed episodes in the new town and weather dynamic navigation scenario).# Weaknesses## WritingI have to start with the most obvious one. The paper is littered with typos and grammatical errors (way too many to list). For instance, the usage of "the" and "a" is almost non-existent. Overall, the paper is really hard to read and needs a thorough pass of proof-reading and editing. Also, please remove the acknowledgments section: I think it is borderline breaking the double-blind submission policy (I don't know these persons, but if I did that would be a breach of ICLR submission policy). Furthermore, I think its contents are not very professional for a submission at a top international academic venue, but that is just my opinion. ## NoveltyThis is the main weakness for me. The architecture is very close to at least the following works:- Xu, H., Gao, Y., Yu, F. and Darrell, T., End-to-end learning of driving models from large-scale video datasets (CVPR'17): this reference is missing from the paper, whereas it is very closely related, as it also shows the benefit of a segmentation decoder on top of a shared encoder for end-to-end driving (calling it privileged training);- Codevilla et al's Conditional Imitation Learning (ICRA'18): the only novelty in the current submission w.r.t. CIL is the addition of the depth and segmentation decoders;- Müller, M., Dosovitskiy, A., Ghanem, B., &amp; Koltun, V., Driving Policy Transfer via Modularity and Abstraction (CoRL'18): the architecture also uses a shared perception module and segmentation (although in a mediated way instead of auxiliary task) to show better generalization performance (including from sim to real).Additional missing related works include:- Kim, J. and Canny, J.F., Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention (ICCV'17): uses post-hoc attention interpretation of "black box" end-to-end networks;- Sauer, A., Savinov, N. and Geiger, A., Conditional Affordance Learning for Driving in Urban Environments (CoRL'18): also uses a perception module in the middle of the CIL network showing better generalization performance in CARLA (although a bit lower than the results in the current submission).- Pomerleau, D.A., Alvinn: An autonomous land vehicle in a neural network (NIPS'89): the landmark paper for end-to-end driving with neural networks!## Insights / significanceIn light of the aforementioned prior art, I believe the claims are correct but already reported in other publications in the community (cf. references above). In particular, the proposed approach uses a lot more strongly labeled data (depth and semantic segmentation supervision in a dataset of 40,000 images) than the competing approaches mentioned above. For instance, the modular pipeline in the original CARLA paper uses only 2,500 labeled images, and I am sure its performance would be vastly improved with 40,000 images, but this is not evaluated, hence the comparison in Table 1 being unfair in my opinion. This matters because the encoder in the proposed method is frozen after training on the perception tasks, and the main point of the experiments is to convince that it results in a great (fixed) intermediate representation, which is in line with the aforementioned works doing mediated perception for driving.The fine-tuning experiments are also confirming what is know in the litterature, namely that simple fine-tuning can lead to catastrophic forgetting (Table 3).Finally, the qualitative evaluation of failure cases (5.3) leads to a trivial conclusion: a modular approach is indeed more interpretable than an end-to-end one. This is actually by design and the main advocated benefit of modular approaches: failure in the downstream perception module yields failure in the upstream driving module that builds on top of it. As the perception module is, by design, outputting a human interpretable representation (e.g., a semantic segmentation map), then this leads to better interpretation overall.## ReproducibilityThere are not enough details in section 3.1 about the deep net architecture to enable re-implementation ("structure similar to SegNet", no detailed description of the number of layers, non-linearities, number of channels, etc).Will the authors release the perception training dataset collected in CARLA described in Section 4.2?# RecommendationAlthough the results of the proposed multi-task network on the CARLA driving benchmark are good, it is probably due to using almost two orders of magnitude more labeled data for semantic segmentation and depth prediction than prior works (which is only practical because the experiments are done in simulation). Prior work has confirmed that combining perception tasks like semantic segmentation with end-to-end driving networks yield better performance, including using a strongly related approach (Xu et al). In addition to the lack of novelty or new insights, the writing needs serious attention.For these reasons, I believe this paper is not suitable for publication at ICLR. The paper proposes to enhance existing multi-level attention (self-attention) mechanism by obtaining query and key vectors (= value vectors) from all levels after weighted-averaging them. The paper claims that this is also theoretically beneficial because the loss function will converge to zero as the number of layers increase. It claims that the proposed architecture outperforms existing attention-based models in English MRC test (SQuAD), Chinese MRC test, and Chinese poem generation task.I find three major issues in the paper.1.  I think the proposed hypothesis lacks the novelty that ICLR audience seeks for. Through many existing architectures (ResNet, ELMo), we already know that skip connection between CNN layers or weighted average of multiple LSTM layers could improve model significantly. Perhaps this could be an application paper that brings existing methods to a slightly different (attention) domain, but not only such paper is less suitable for ICLR, but also it would require strong experimental results. But as I will detail in the second point, I also have some worries about the experiments. 2. The experimental results have problems. For English MRC experiment (SQuAD), the reproduced match-LSTM score is ~10% below the reported number in its original paper. Furthermore, it is not clear whether the improvement comes from having multiple attention layers (which is not novel) or weighted-averaging the attention layers (the proposed method). BiDAF and match-LSTM have single attention layers, so it is not fair to compare them with multi-layer attention. 3. Lastly, I am not sure I understood the theoretical section correctly, but it is not much interesting that having multiple layers allow one to approach closer to zero loss. In fact, any sufficiently large model can obtain close-to-zero loss on the training data. This is not a sufficient condition for a good model. We cannot guarantee if the model has generalized well; it might have just overfit to the training data.A few minor issues and typos  on the paper:- First para second sentence: In -&gt; in- First para second sentence: sequence to sequence -&gt; sequence-to-sequence- Second last para of intro: sentence fragment- Figure 3: would be good to have English translation. The authors propose an extension of cycle-consistent adversarial adaptation methods in order to tackle domain adaptation in settings where a limited amount of supervised target data is available (though they also validate their model in the standard unsupervised setting as well). The method appears to be a natural generalization/extension of CycleGAN/CyCADA. It uses the ideas of the semantic consistency loss and training on adapted data from CyCADA, but "fills out" the model by applying these techniques in both directions (whereas CyCADA only applied them in the source-to-target direction).The writing in this paper is a little awkward at times (many omitted articles such as "the" or "a'), but, with a few exceptions, it is generally easy to understand what the authors are saying. They provide experiments in a variety of settings in order to validate their model, including both visual domain adaptation and speech domain adaptation. The experiments show that their model is effective both in low-resource supervised adaptation settings as well as high-resource unsupervised adaptation settings. An ablation study, provided in Section 4.1, helps to understand how well the various instantiations of the authors' model perform, indicating that enforcing consistency in both methods is crucial to achieving performance beyond the simple baselines.It's a little hard to understand how this method stands in comparison to existing work. Table 3 helps to show that the model can scale up to the high-resource setting, but it would also be nice to see the reverse: comparisons against existing work run in the limited data setting, to better understand how much limited data negatively impacts the performance of models that weren't designed with this setting in mind.I would've also liked to see more comparisons against the simple baseline of a classifier trained exclusively on the available supervised target data, or with the source and target data togetherin my experience, these baselines can prove to be surprisingly strong, and would give a better sense of how effective this paper's contributions are. This corresponds to rows 2 and 3 of Table 1, and inspection of the numbers in that table shows that the baseline performance is quite strong even relative to the proposed method, so it would be nice to see these numbers in Table 2 as well, since that table is intended to demonstrate the model's effectiveness across a variety of different domain shifts.While it's nice that the model is experimentally validated on the speech domain, the experiment itself is not explained well. The speech experiments are hard to understandit's unclear what the various training sets are, such as "Adapted Male" or "All Data," making it hard to understand exactly what numbers should be compared. Why is there no CycleGAN result for "Female + Adapted Male," or "All Data + Adapted Male," for example? The paper would greatly benefit from a more careful explanation and analysis of this experimental setting.Ultimately, I think the idea is a nice generalization of previous work, and the experiments seem to indicate that the model is effective, but the limited scope of the experiments prevent me from being entirely convinced. The inclusion of additional baselines and a great deal of clarification on the speech experiments would improve the quality of this paper enormously. In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. The final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).The authors mainly combine a few previous approaches "Self-Imitation Learning," "Automatic Goal Generation for Reinforcement Learning Agents," and "Curiosity-driven exploration by self-supervised prediction" to design this algorithm which makes this approach less novel.General comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many othersThe main idea of the algorithm is clear, but the description of the pieces is missing.It is not clear in stochastic setting how well this approach will perform. The authors state that"Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments."I took a look at this paper and could not find neither proof or quantification of "effective"-ness. Please clarify what the meaning this statement is.Why s=s' is ambiguous to the inverse dynamics?What is the definition of acc in fig2?why (consin+1)^3/8 is chosen? Unsupervised image-to-image (I2I) translation is an important issue due to various applications and it is still challenging when applied to diverse image data and data where domain gap is large. This paper employs a neural mutual information estimator (MINE) to deal with I2I translation between two domains where there is a large gap. However, this paper contains several issues.1. Pros. and Cons.   (+) Mathematical definition of I2I translation   (+) Application of mutual information for conserving content.   (-) Lack of comparison with recent I2I models   (-) Lack of experimental results and ablation studies    (-) Unclear novelty2. Major comments   - The novelty of this paper is not clear. Excluding the mathematical definition, it seems that the proposed TI simply combines DCGAN and MINE-based statistical networks. For clarifying the novelty, the detailed architecture and final objective functions can be helpful.    - Recent works on unsupervised I2I translation are omitted including UNIT [1], MUNIT [2], and DRIP [3]. Also, the authors need to clarify the main difference of TI-GAN from comparing models.   - It is not clear to relate the mathematical definition of domain transfer to one-to-many translation within large domain gap.    - It is not clear how to use mutual information (MINE) for learning. There is no explicit definition of loss function considering MINE term.    - It is short of comparing other state-of-the art models such as UNIT, MUNIT, DRIP, and AugCycleGAN. They compared their results with CycleGAN only.   - Experiments are not enough to support the authors insist. There is not any quantitative metric or qualitative result on generating edge-to-shoes.    - It is difficult to read due to inconsistent usage of terms (e.g., Figure 3 and 4 (c)s)   - For better understanding, it requires to compare the patterns of MINE loss and adversarial loss.    - Experiments on more datasets such as animal, season, faces or USPS datasets.    - What is the main difference in the results between DCGAN-based and UNet-based models?Minor   - cicle_times symbol looks the product between distribution. But it should be defined before being used.   - A reference of CycleGAN is incorrectly cited.    - There are some typos in the paper.   - page 1: dependent  depend   - page 3: by separate  by separating   - page 6: S a I  S and I1. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks, CoRR, abs/1703.00848, 20172. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, Multimodal Unsupervised Image-to-Image Translation, CoRR. abs/1804.047323. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, Diverse Image-to-Image Translation via Disentangled Representations, ECCV 2018. Unsupervised image-to-image (I2I) translation is an important issue due to various applications and it is still challenging when applied to diverse image data and data where domain gap is large. This paper employs a neural mutual information estimator (MINE) to deal with I2I translation between two domains where there is a large gap. However, this paper contains several issues.1. Pros. and Cons.   (+) Mathematical definition of I2I translation   (+) Application of mutual information for conserving content.   (-) Lack of comparison with recent I2I models   (-) Lack of experimental results and ablation studies    (-) Unclear novelty2. Major comments   - The novelty of this paper is not clear. Excluding the mathematical definition, it seems that the proposed TI simply combines DCGAN and MINE-based statistical networks. For clarifying the novelty, the detailed architecture and final objective functions can be helpful.    - Recent works on unsupervised I2I translation are omitted including UNIT [1], MUNIT [2], and DRIP [3]. Also, the authors need to clarify the main difference of TI-GAN from comparing models.   - It is not clear to relate the mathematical definition of domain transfer to one-to-many translation within large domain gap.    - It is not clear how to use mutual information (MINE) for learning. There is no explicit definition of loss function considering MINE term.    - It is short of comparing other state-of-the art models such as UNIT, MUNIT, DRIP, and AugCycleGAN. They compared their results with CycleGAN only.   - Experiments are not enough to support the authors insist. There is not any quantitative metric or qualitative result on generating edge-to-shoes.    - It is difficult to read due to inconsistent usage of terms (e.g., Figure 3 and 4 (c)s)   - For better understanding, it requires to compare the patterns of MINE loss and adversarial loss.    - Experiments on more datasets such as animal, season, faces or USPS datasets.    - What is the main difference in the results between DCGAN-based and UNet-based models?Minor   - cicle_times symbol looks the product between distribution. But it should be defined before being used.   - A reference of CycleGAN is incorrectly cited.    - There are some typos in the paper.   - page 1: dependent  depend   - page 3: by separate  by separating   - page 6: S a I  S and I1. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks, CoRR, abs/1703.00848, 20172. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, Multimodal Unsupervised Image-to-Image Translation, CoRR. abs/1804.047323. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, Diverse Image-to-Image Translation via Disentangled Representations, ECCV 2018. Summary:This paper aims at improving the speed of the iterative inference procedure (during training and deployment) in energy-based models trained with Equilibrium Propagation (EP), with the requirement of avoiding backpropagation. To achieve this, the authors propose to train a feedforward network to predict a fixed point of the "equilibrating network". Gradients are approximated by local gradients only. The method is compared to standard EP on MNIST.The overall idea of the paper to speed up the slow iterative inference (during training and deployment) seems very reasonable. However, the paper seems to be still work in progress and could be improved on the theoretical side, the presentation, and especially the experimental evaluation. The paper is rather weak on the theoretical side. The main theoretical result is perhaps the analysis of the gradient alignment. However, I cannot follow their analysis and suspect that it is false. More detailed comments follow. Regarding the presentation, I found many typos which I don't consider in my evaluation. However, there are both minor and major issues with several equations. Details follow below. Another major concern is the lack of experimental evaluation. There is only a single plot that shows the learning curves of EP and the proposed Initialized EP with 2 different numbers of negative-phase steps and for 2 different architectures. The authors should put a lot more effort into the evaluation. For example, evaluate the influence of the hyperparameter in Eq. (10) (Is lambda &gt; 0 detrimental to the capacity of the equilibrating network?), etc.Lastly, as of my current understanding, the whole motivation for the EP framework is biological plausibility. In my opinion, this paper lacks a discussion of that motivation with respect to the proposed approach.To summarize, there are too many major problems that cannot be addressed only in the rebuttal phase. Details:- Sec. 1.1. Equilibrium Propagation --&gt; Sec. 2 (It is not part of the introduction) - In 1.1., "Equilibrium Propagation is a method for training a Continuous Hopfield Network for classification". EP is a method for training various energy-based models, not just hopfield networks. - Eq. (1): I find the notation very confusing. Specifically, I can't make sense of:    a) "$\alpha = \{\alpha_j: j \in  S\}$ denotes the network architecture". What does it mean for alpha to denote an architecture? Please be more specific.     b) In the definition of $\alpha_j$, you are constructing a set of neurons $i \in S  \cup I$, but then you are re-defining i in the same set, using the forall operator.     c) Even if the two above is corrected, I can't follow. Please simplify the notation (the energy function is not that complicated).- Eq. (1): Why is it $i \in S$ everywhere, rather than all neurons, including input neurons (as in [Scellier and Bengio 2017])? - The text between Eq. (2) and Eq. (3) introduces the classification targets by adding the gradients of another energy function $C(s_O, y)$ to the previously described energy function from Eq. (1). First $C(s_O, y)$ is nowhere defined. Second, The energy is a scalar, while the gradient is a vector, so there must be a mistake. I suppose it should be just $C(s_O, y)$ rather than its gradients?- Eq. (6): $f_{\phi_{j}}$ is defined as a function of multiple $f_{\phi_{i}}$ ? - Eq. (9): Again the index i is used twice. - Sec. 2.1: Can you elaborate on why the equilibrating network can create targets that are not achievable by the feedforward network? Is it a problem of your particular choice of model architecture? Isn't the "regularization" then detrimental to the (capacity of the) equilibrating network? - In Sec. 2.2 on page 5, you claim that given random parameter intitialization, the gradients should almost always be aligned. For random weight matrices, where the weights are drawn with zero mean, I cannot see how this is true. To compute gradients of layer $l$, backpropagation (in an MLP) computes the matrix-vector multiplication between transposed weight matrix and the gradients of layer l+1 (I am ignoring the activation function here). The resulting gradient should have zero mean.- Eq. (11): Is it the L1 Norm or L2?- Eq. (12): In the preceding text, you made claims about the gradient alignment for random parameter initialization. In Eq. (12) you analyze the gradients close to the optimum?- Eq. (12): What is f, it has never been defined. I suppose it should be the h from above? - Eq. (12): I don't understand how you arrived at these gradient equations, even the first one. Shouldn't it be the standard backpropagation in an MLP or am I missing something? Using the chain rule $\frac{\partial L_1}{\partial w_1} = \frac{\partial L_1}{\partial s_1} \frac{\partial s_1}{\partial w_1}$, I arrive at a different result. How can there be the derivative of f (or h) twice.- Sec. 3: Is beta really sampled from a zero-centred uniform distribution? On page 2, beta is introduced as a small positive number. Would a negative beta not cause the model to settle to a fixed point where maximally wrong targets are predicted?[Scellier and Bengio 2017] Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation This paper offers the argument that dropout works not due to preventing coadaptation, but because it gives more gradient, especially in the saturated region. However, previous works have already characterized how dropout modifies the activation function, and also the gradient in a more precise way than what is proposed in this paper. ## Co-adaptationco-adaptation does not seem to mean correlation among the unit activations. It is not too surprising units need more redundancy with dropout, since a highly useful feature might not always be present, but thus need to be replicated elsewhere.Section 8 of this paper gives a definition of co-adaptation,based on if the loss is reduced or increased based on a simultaneous change in units.https://arxiv.org/abs/1412.4736And this work, https://arxiv.org/abs/1602.04484, reached a conclusion similar to yoursthat for some notion of coadaptation, dropout might increase it.## Gradient accelerationIt does not seem reasonable to measure "gradient information flow" simply as the norm of the gradient, which is sensitive to scales, and it is not clear if the authors accounted for scaling factor of dropout in Table 2.The proposed resolution, to add this discontinuous step function in (7) with floor is a very interesting idea backed by good experimental results. However, I think the main effect is in adding noise, since the gradient with respect to this function is not meaningful. The main effect is optimizing with respect to the base function, but adding noise when computing the outputs. Previous work have also looked at how dropout noise modifies the effective activation function (and thus its gradient). This work, http://proceedings.mlr.press/v28/wang13a.html, give a more precise characterization instead of treating the effect as adding a function with constant gradient multiplied by an envelop. In fact, the actual gradient with dropout does involve the envelope by chain rule, but the rest is not actually constant as in GAAF. The paper studies the adversarial robustness of Bayesian classifiers. The authors state two conditions that they show are provably sufficient for "idealised models" on "idealised datasets" to not have adversarial examples. (In the context of this paper, adversarial examples can either be nearby points that are classified differently with high confidence or points that are "far" from training data and classified with high confidence.) They complement their results with experiments.I believe that studying Bayesian models and their adversarial robustness is an interesting and promising direction. However I find the current paper lacking both in terms of conceptual and technical contributions.They consider "idealized Bayesian Neural Networks (BNNs)" to be continuous models with a confidence of 1.0 on the training set. Since these models are continuous, there exists an L2 ball of radius delta_x around each point x, where the classifier has high confidence (say 0.9). This in turn defines a region D' (the training examples plus the L2 balls around them) where the classifier has high confidence. By assuming that an "idealized BNN" has low certainty on all points outside D' they argue that these idealized models do not have adversarial examples. In my opinion, this statement follows directly from definitions and assumptions, hence having little technical depth or value. From a conceptual point of view, I don't see how this argument "explains" anything. It is fairly clear that classifiers only predicting confidently on points _very_ close to training examples will not have high-confidence adversarial examples. How do these results guide our design of ML models? How do they help us understand the shortcomings of our current models?Moreover, this argument is not directly connected to the accuracy of the model. The idealized models described are essentially only confident in regions very close to the training examples and are thus unlikely to confidently generalize to new, unseen inputs. In order to escape this issue, the authors propose an additional assumption. Namely that idealized models are invariant to a set of transformations T that we expect the model to be also invariant to. Hence by assuming that the "idealized" training set contains at least one input from each "equivalence class", the model will have good "coverage". As far as I understand, this assumption is not connected to the main theorem at all and is mostly a hand-wavy argument. Additionally, I don't see how this assumption is justified. Formally describing the set of invariances we expect natural data to have or even building models that are perfectly encoding these invariances by design is a very challenging problem that is unlikely to have a definite solution. Also, is it natural to expect that for each test input we will have a training input that is close to L2 norm to some transformation of the test input?Another major issue is that the value of delta_x (the L2 distance around training point x  where the model assigns high confidence) is never discussed. This value is _very_ small for standard NN classifiers (this is what causes adversarial examples in the first place!). How do we expect models to deal with this issue? The experimental results of the paper are essentially for a toy setting. The dataset considered ("ManifoldMNIST") is essentially synthetic with access to the ground-truth probability of each sample. Moreover, the results on real datasets are unreliable. When evaluating the robustness of a model utilizing dropout, using a single gradient estimation query is not enough. Since the model is randomized, it is necessary to estimate the gradient using multiple queries. By using first-order attacks on these more reliable gradient estimates, an adversary can completely bypass a dropout "defense" (https://arxiv.org/abs/1802.00420).Overall, I find the contributions of the paper limited both technically and conceptually. I thus recommend rejection.Comments to the authors:-- You cite certain detention methods for adversarial examples (Grosse et al. (2017), Feinman et al. (2017)) that have been shown to be ineffective (that is they can be bypassed by an adaptive attacker) by Carlini and Wagner (https://arxiv.org/abs/1705.07263)-- The organization of the paper could be improved. I didn't understand what the main contribution was until reading Theorem 1 (this is 6 pages into the paper). The introduction is fairly vague about your contributions. You should consider moving related work later in the paper (most of the discussion there is not directly related to your approach) and potentially shortening your background section.-- How is the discussion about Gaussian Processes connected with your results?-- Consider making the two conditions more prominent in the text.-- In Definition 5, the wording is confusing "We define an idealized BNN to be a Bayesian idealized NN..." The paper once again looks at the problem of reducing the communication requirement for implementing the distributed optimization techniques, in particular, SGD. This problem has been looked at from multiple angles by many authors. And although there are many unanswered questions in this area, I do not see the authors providing any compelling contribution to answering those questions. A big chunk of the paper is devoted to expressing some shallow theorems, which in some cases I do not even see their importance or connection to the main point of the paper; see my comments below. In terms of the techniques for reducing the communication burden, the authors seem to just put all the other approaches together with minimal novelty.More detailed comments:- I do not really understand what the authors mean by noise damping. I would appreciate it if they could clarify that point. This seems to be a very important point as the model they propose for the noise in the process is basically based on this notion. It is a great failure on the authors' part that such a crucial notion in their paper is not clearly described. - The model that is proposed for noise is too strong and too simplistic. Do you guys have any evidence to back this up?- Theorem 2.1 is not a theorem. The result is super shallow and relatively trivial. - In corollary 2.1 it seems that no matter what the randomness in the system is, the algorithm is going to converge to the same solution. This is not true even for the non-strongly convex objectives, let alone the non-convex problems where there are so many stationary solutions and whatnot.- With regards to Fig 3 (and other related figures in the appendix) and the discussion on the multiplicative nature of compression: The figure does not seem to suggest multiplicative nature in all the regimes. It seems to hold in high compression/ low-frequency communication regime. But on the other side of the spectrum, it does not seem to hold very strongly.- The residual accumulation only applies when all the nodes update in all the iterations. I do not believe this would generalize to the federated learning, where nodes do not participate in all the updates. I do not know if the authors have noted this point in their federated learning experiments.- Theorem 3.1 is very poorly stated. Other that than it is shallow and in my opinion irrelevant. What is the argument in favor of the authors' thought that could be built based on the result of Theorem 3.1?- One major point that is missing in the experiments (and probably in the experiments in other papers on the same topic) is to see how much do all these compressions affect the speed of learning in different scenarios in realistic scenarios? Note that in realistic scenarios many things other than communication could affect the convergence time. * The construction of the training dataset is clearly flawed by the use of an automatic algorithm that would certainly introduce a strong bias and noisy labels. Even though the dataset is supposed to encode continuous traits, the validation with human subjects is performed in a binary fashion.* I miss more formality in the presentation of the methodology. Figure 3. does not seem very self-explanatory, nor does the caption. Which is the dimensionality of the input trait vector?. I assume the input would be the trait ratings predicted by the human subjects. However in the experiments training seems to be done with a maximum of two traits. This makes me wonder how the dense part of the network can handle the dimensionality blow-up to match the latent space dimensionality without suffering from overfitting. I would appreciate some disussion regarding this.* While I appreciate a section reasoning why the method is supposed to work, those claims should be backed with an ablation study in the experimental section.  * The qualitative results show a few examples which I find very hard to evaluate due to the low-resolution of the predictions. In both traits there seems to be the same facial features modified and I can't find much difference between trustworthy and aggresssive (the labels could be swapped and I would have the same opinion on the results). I miss additional trait examples that would make clearer if the network is learning something besides generating serious and happy faces.* The qualitative comparison with StarGAN seems unfair, as if one checks the original paper their results are certainly more impressive than what Figure 5 shows.* The authors show only two traits in the experiments which makes me a bit suspicious about the performance of the network with the rest of traits. The training datset considers up to 40 traits. Determinantal Point Processes provide an efficient and elegant way to sample a subset of diverse items from a ground set. This has found applications in summarization, matrix approximation, minibatch selection. However, the naive algorithm for DPP takes time O(N^3), where N is the size of the ground set. The authors provide an alternative model DPPNet for sampling diverse items that preserves the elegant mathematical properties (closure under conditioning, log-submodularity) of DPPs while having faster sampling algorithms.The authors need to compare the performance of DPPNet against faster alternatives to sample from DPPs, e.g., https://arxiv.org/pdf/1509.01618.pdf, as well as compare on applications where there is a significant gap between uniform sampling and DPPs (because there are the applications where DPPs are crucial). The examples in Table 2 and Table 3 do not address this. This paper proposes an algorithm for optimizing neural networks parametrized by Tensor Train (TT) decomposition based on the Riemannian optimization and rank adaptation, and designs a bidirectional TT LSTM architecture.I like the topic chosen by the authors, using TT to parametrize layers of neural networks proved to be beneficial and it would be very nice to exploit the Riemannian manifold structure to speed up the optimization.But, the paper needs to be improved in several aspects before being useful to the community. In particular, I found the several mathematical errors regarding basic definitions and algorithms (see below the list of problems) and Im not happy with lack of baselines in the experimental comparison (again, see below).The math problems1) In equations (1), (2), (7), and (8) there is an error: one should sum out the rank dimensions instead of fixing them to the numbers r_i. At the moment, the left-hand side of the equations doesnt depend on r and the right-hand side does.2) In two places the manifold of d-dimensional low-rank tensors is called d-dimensional manifold which is not correct. The tensors are d-dimensional, but the dimensionality of the manifold is on the order of magnitude of the number of elements in the cores (slightly smaller actually).3) The set of tensors with rank less than or equal to a fixed rank (or a vector of ranks) doesnt form a Riemannian (or smooth for that matter) manifold. The set of tensors of rank equal to a fixed rank something does.4) The function f() minimized in (5) is not defined (it should be!), but if it doesnt have any rank regularizer, then there is no reason for the solution of (5) to have rank smaller then r (and thus I dont get how the automatic rank reduction can be done).5) When presenting a new retraction algorithm, it would be nice to prove that it is indeed a retraction. In this case, Algorithm 2 is almost certainly not a retraction, I dont even see how can it reduce the ranks (it has step 6 that is supposed to do it, but what does it mean to reshape a tensor from one shape to a shape with fewer elements?).6) I dont get step 11 of Alg 1, but it seems that it also requires reshaping a tensor (core) to a shape with fewer elements.7) The rounding algorithm (Alg 3) is not correct, it has to include orthogonalization (see Oseledets 2011, Alg 2).8) Also, I dont get what is r_max in the final optimization algorithm (is it set by hand?) and how the presented rounding algorithm can reduce the rank to be lower than r_max (because if it cannot, one would get the usual behavior of setting a single value of rank_max and no rank adaptivity).9) Finally, I dont get the proposition 1 nor its proof: how can it be that rounding to a fixed r_max wont change the value of the objective function? What if I set r_max = 1? We should be explained in much greater detail.10) I didnt get this line: From the RSGD algorithm (Algorithm 1), it is not hard to find the sub-gradient gx = f(x) and Exp1 x (y) = ·xf(x), and thus Theorem 3 can be derived. What do you mean that it is not hard to find the subgradient (and what does it equal to?) and why is the inverse of the exponential map is negative gradient?11) In general, it would be beneficial to explain how do you compute the projected gradient, especially in the advanced case. And what is the complexity of this projection?12) How do you combine optimizing over several TT objects (like in the advanced RNN case) and plain tensors (biases)? Do you apply Riemannian updates independently to every TT objects and SGD updates to the non-TT objects? Something else?13) What is E in Theorem 3? Expected value w.r.t. something? Since I dont understand the statement, I was not able to check the proof.The experimental problems:1) There is no baselines, only the vanilla RNN optimized with SGD and TT RNN optimized with your methods. There should be optimization baseline, i.e. optimizing the same TT model with other techniques like Adam, and compression baselines, showing that the proposed bidirectional TT LSTM is better than some other compact architectures. Also, the non-tensor model should be optimized with something better than plain SGD (e.g. Adam).2) The convergence plots are shown only in iteration (not in wall clock time) and its not-obvious how much overhead the Riemannian machinery impose.3) In general, one can decompose your contributions into two things: an optimization algorithm and the bidirectional TT LSTM. The optimization algorithm in turn consist in two parts: Riemannian optimization and rank adaptation. There should be ablation studies showing how much of the benefits come from using Riemannian optimization, and how much from using the rank adaptation after each iteration.And finally some typos / minor concerns:1) The sentence describing the other tensor decomposition is a bit misleading, for example CANDECOMP can also be scaled to arbitrary high dimensions (but as a downside, it doesnt allow for Riemannian optimization and can be harder to work with numerically).2) Its very hard to read the Riemannian section of the paper without good knowledge of the subject, for example concepts of tangent space, retraction, and exponential mapping are not introduced.3) In Def 2 different function should probably be differentiable function.4) How is W_c represented in eq (25), as TT or not? It doesnt follow the notation of the rest of the paper. How is a_t used?5) What is score in eq (27)?6) Do you include bias parameters into the total number of parameters in figures?7) The notation for tensors and matrices are confusingly similar (bold capital letters of slightly different font).8) There is no Related Work section, and it would be nice to discuss the differences between this work and some relevant ones, e.g. how is the proposed advanced TT RNN different from the TT LSTMs proposed in Yang et al. 2017 (is it only the bidirectional part that is different?) and how is the Riemannian optimization part different from Novikov et al. 2017 (Exponential machines), and what are the pros and cons of your optimization method compared to the method proposed in Imaizumi et al. 2017 (On Tensor Train Rank Minimization: Statistical Efficiency and Scalable Algorithm).Please, do take this as a constructive criticism, I would be happy to see you resubmitting the paper after fixing the raised concerns! ## SummaryThis work presents a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling. By sampling from the weight distribution an ensemble of Binary Neural Networks could further improve the performance. In the experimental section, the authors compare proposed PBNet with Binarized NN (Hubara et al., 2016) in two image datasets (MNIST and CIFAR10).In general, the paper was written in poor quality and without enough details. The idea behind the paper is not novel. Stochastic binarization and the (local) reparametrization trick were used to training binary (quantized) neural networks in previous works. The empirical results are not significant. ## Detail commentsIssues with the training algorithm of stochastic neural networkThe authors did not give details of the training method and vaguely mentioned that the variational optimization framework (Staines &amp; Barber, 2012). I do not understand equation 1. Since B is binary, the left part of equation 2 is a combination optimization problem. If B is sampled during the training, the gradient would suffer from high variance.Issues with propagating distributions throughout the networkEquation 3 is based on the assumption of that the activations are random variables from Bernoulli distribution. In equation 4, the activations of the current layer become random variables from Gaussian distribution. How the activations to further propagate?  Issues with ternary Neural Networks in section 2.4For a ternary NN, the weight will be from a multinomial distribution, I think it will break the assumption used by equation 3.Issues with empirical evidencesSince the activations are sampled in PBNET-S, a more appropriate baseline should be BNN with stochastic binarization (Hubara et al., 2016) which achieved 89.85% accuracy on CIFAR-10. It means that the proposed methods did not show any significant improvements. By the way BNN with stochastic binarization (Hubara et al., 2016) can also allow for ensemble predictions to improve performance. ****Summary****This paper extends recent results on convergence of Bayesian fully connected networks (FCNs) to Gaussian processes (GPs), to the equivalent relationship between convolutional neural networks (CNNs) and GPs. This is currently an area of high interest, with Xiao et al. (2018) examining the same relationship from a mean-field perspective, and two other concurrent papers making contributions:https://arxiv.org/abs/1808.05587https://arxiv.org/abs/1810.10798Thus the scope of the paper fits well within the aims of the conference.I really appreciate that the authors did not shy away from studying the effect of pooling layers, and find the connection to locally connected networks they describe intriguing and insightful. On the experimental side, the investigation of the relative importance of compositionality, equivariance and invariance on performance of CNNs is very interesting.These experiments and investigations are however based on a theoretical foundation which suffers from several issues. The main problems are an incorrect proof of convergence of the joint distribution of filters, and an improper use of convergence in probability in cases where random variables do not share a common underlying probability space. Unfortunately, either of these by itself invalidates the main theoretical claims which is why I am recommending rejection of the paper.However, I believe that the argument in (A.4.3) can potentially be rectified, and, as I detail below, is of greater interest to the community relative to the ones in (A.4.1) and (A.4.2). If this is accomplished and the proofs in (A.4.1) and (A.4.2) are either also fixed or left out (A.4.3 is sufficient to justify the claims in the main body), I am willing to significantly improve my rating of this paper and potentially recommend acceptance. For this reason, a "detailed comments" section is appended at the end of the standard review where the technical issues are described in much greater detail.****General comments******Bayesian vs. infinite neural networks**The main theoretical claims concerning the relationship between Bayesian CNNs and GPs are within Section 2. Therein on top of page 4, the authors say "In Appendix A.4 we give several **alternative** derivations of the correspondence" (emphasis mine), and then progress to outline the skeleton of the argument (A.4.2) in Sections 2.2.1-2.2.3. Section 2.2.3 is concluded by statement of the main theoretical result of this paper, Eq. (10), which comes from (A.4.3) and can only be linked to the rest of Section 2 through the claim of equivalence between the "alternative derivations" (A.4.1), (A.4.2) and (A.4.3). The problem is that the equivalence claim does not hold, as explained below:The most important distinction here is between what I will call a "sequential" and a "simultaneous" limit. In the "sequential" case (A.4.1 &amp; A.4.2, Sections 2.2.1-2.2.3), layers are taken to infinity one by one, whereas in the "simultaneous" case (A.4.3, used to obtain the result concluding Section 2.2.3) all layers are **finite** for **all** members of the sequence, growing in width simultaneously.The "simultaneous" limit (A.4.3) is in my view more interesting as it tells us that **finite** BNNs do indeed converge to GPs in distribution, i.e. that for each expectation of a continuous bounded function of the outputs of the limiting GP, there exists a BNN with a **finite** number of neurons in **each** layer for which the expectation of the same function is arbitrarily close. From a practical perspective, "simultaneous" limit tells us that inference algorithms for BNNs (which can be inaccurate and/or computationally expensive) can sometimes be replaced by exact or approximate inference algorithms for the limiting GP (cf. Section 5 in (Matthews et al., 2018, extended version)).The "sequential" limit (A.4.1 &amp; A.4.2) on the other hand does not establish existence of finite BNNs arbitrarily close to a particular GP, or justify use of the GP limit as approximation for finite BNNs as above. This is because the width of individual layers goes to infinity in a sequence from first to last. This means that most of the networks that constitute the sequence converging to the GP will have **one or more infinitely wide layers** and thus do not correspond to the finite BNNs we usually work with. In other words, "sequential" limit can only ever establish that there exists a network with **all but the final hidden layer infinite** that is arbitrarily close to the limiting GP. The only case where "sequential" and "simultaneous" limits agree is thus in the single hidden layer case first studied by Neal (1996). I will call the networks with one or more infinite layers "infinite networks", inspired by the work of Williams (1997) and others. Notice that infinite networks cannot be described by Eqs. (1) and (2) as the weights would be zero with probability one and thus output of the network would only depend on biases. It is not immediately obvious how to formally replace Eqs. (1) and (2) in the case of infinite networks which is one of the technical issues with the approaches in (A.4.1) and (A.4.2) (see the detailed comments section for further discussion).Others may of course disagree and find "sequential" limits more interesting, but if the authors wish to keep the description of (A.4.2) in the main paper (Sections 2.2.1-2.2.3), it would be highly beneficial if readers were given the opportunity to understand the differences between the two types of limits so that they can form their own judgement. The authors should then also make clearer that the approach described in Sections 2.2.1-2.2.3 cannot be used to obtain the final result, Eq. (10). I would rather recommend reworking Sections 2.2.1-2.2.3 based on the "simultaneous" limit argument in (A.4.3) which unlike the current one can justify the result in Eq. (10) stated at the end.**Other comments**- (p.2, top) You say your results are "strengthening and extending the result of Matthews et al. (2018)" which is somewhat confusing. Matthews et al. prove a result for FCNs whereas this paper focuses on CNNs. Extension of (A.4.3) to FCNs may well be possible but is not included in this paper. Results in (A.4.1) and (A.4.2) are for the "sequential" whereas Matthews et al. study the "simultaneous" limit. Further differences:        - Matthews et al. prove convergence for any countable rather than only finite input sets.        - In Matthews et al.'s work, Gaussianity is obtained through use of a particular version of CLT, whereas this work exploits Gaussianity of the prior over weights and biases. Going forward, an extension to more general priors/initialisations (like uniform or any sub-Gaussian) is likely to be easier using the CLT approach.        - Matthews et al.'s assumption on the activation functions is independent of the input set (p.7, Definition 1), whereas this work uses an assumption that is explicitly dependent on input (Eq. (37)) which might be potentially difficult to check.- (p.15, A.2 end) Should also mention Titsias (2009), "Variational Learning of Inducing Variables in Sparse Gaussian Processes", as a classical reference for approximate GP inference.****Questions****- (Section 4) Can you please provide more details on the MC approximation? Specifically, is only the last kernel approximated, or rather all of them, sequentially resampling from the Gaussian with empirical covariance in each layer? In case you tried, is there any qualitative or quantitative difference between the two approaches?- (Section 4 and Appendix A) Daniely et al. (2016) assume that the inputs to the neural network are l^2 normalised. You mention that the inputs have been normalised in the experiments (A.6). Is this assumption used in any of your proofs? Have you observed that l^2 normalisation improves empirical performance?- (p.8, Figure 6) How was "the best CNN with the same parameters" selected? If training error is zero for all, was it selected by validation accuracy? I was assuming that what is plotted is an estimate of the **expected** generalisation error, whereas the above selection procedure would be estimating supremum of the support of the generalisation error estimator which does not seem like a fair comparison. Can you please clarify?- (p.8 and A.6) Why only neural networks with zero training loss were allowed as benchmarks? How did the ones with non-zero training error fared in comparison? Can you please expand on footnote 3?- (p.8, last sentence) "an observation specific to CNNs and FCNs or LCNs": Matthews et al. (2018, extended version) observed in Section 5.2 that BNNs and their corresponding GP limits do not always perform the same even in the FCN case (cf. their Figure 8). Their paper unfortunately does not compare to equivalent FCNs trained by SGD. Have you experimented with or have an intuition for whether the cases where SGD trained models prevail coincide with the cases where BNNs+MCMC posterior inference outperform their GP limit?- (p.15, Table 3) The description says you were using erf activation (instead of the more standard ReLU): why? Have you observed any significant differences? Further, how big a proportion of the values in the image is black due to the numerical issues mentioned in A.6.4?- (p.18, just after Eq. 39) Use of PSD_{|X|d} in (A.4.3) suggests this proof assumes "same" padding is used?! Does the proof generalise to any padding/changing dimensions of filters inside the network?- (A.6) Can you comment on the pros &amp; cons of "label regression" for classification and how does it compare with approximate inference when softmax is put on top of a GP (perhaps illustrating by a simple experiment on a toy dataset)?[end of standard review][detailed comments]****Technical concerns****Notation-wise, I would strongly encourage incorporating the dependence on network width into your notation, at the very least throughout the appendix. It would greatly reduce the amount of mental book-keeping the reader currently has to do, and significantly increase clarity at several places.One of my main concerns is that the random variables and their underlying probability space are never formally set-up. This is problematic because convergence in probability is only defined for random variables sharing the same underlying space. At the moment, networks with different widths are not set-up to share a probability space. The practical implication for the approaches relying on convergence in probability of the empirical covariance matrices K is that the convergence in probability is not well-defined exactly because the empirical covariance matrices are not set-up on the same underlying probability space. A possible way to address this issue is to use an approach akin to what Matthews et al. (2018, extended version) call "infinite width, finite fan-out, networks" on page 20. This puts the networks on the same underlying space and because the empirical covariance matrices are measurable functions of thus defined random variables, they will also share the same underlying probability space.Also regarding convergence in probability, please state explicitly with respect to which metric is the convergence considered when first mentioned (A.4.3 is explicitly using l^\infty; A.4.2 perhaps l^2 or l^\infty?), and make any necessary changes (e.g. show continuity of the mapping C in A.4.2).At several places within the paper, you state that the law of large numbers (LLN) or the central limit theorem (CLT) can be applied. Apart from other concerns detailed later, these come with conditions on finiteness of certain expectations (usually the first one or two moments of the relevant random variables). Please provide proofs that these expectations are indeed finite and make any assumptions that you need explicit in the main text.Another major concern is that none of (A.4.1), (A.4.2) and (A.4.3) successfully proves joint convergence of the filters at the top layer as claimed in the main text (e.g. Eq. (10)), and instead only focuses on marginal convergence of each filter which is not sufficient (cf. the comment on joint vs. pairwise Gaussianity below). This is perhaps sufficient if a single filter is the output of the network, but insufficient otherwise, especially when proving convergence with additional layers added on top of the last convolutional layer (as in Section 3) whenever the number filters is taken to infinity.It would be nice, but not necessary for acceptance of the paper, to extend the proofs to uncountable index sets. I think you could use the same argument as described towards the end of Section 2.2  in (Matthews et al., 2018, extended version) and references therein.**Other comments**- I would strongly encourage distinguishing more clearly between probability distributions and density functions. For example, I would infer that lower case p refers to the probability distribution from Eq. (6); however, in Eqs. (8) and (9) the same notation is used for density functions (whilst integrating against the Lebesgue measure). This is quite confusing in this context as the two objects are not the same (see next two comments). I would suggest using capital P when referring to distribution, and lower case p when referring to its density.- (p.4, Eq. 6) If p is a density, it cannot be equal to a delta distribution. If it is a probability distribution then I am similarly confused - convergence in probability is a statement about behaviour of random variables, not probability distributions; in that case possibly Eq. (6) is trying to say that the empirical distribution of K^l (which is a random variable) conditioned on K^{l-1} converges weakly to the delta distribution on the RHS in probability? Please clarify.- (p.5, Eq. 10) I would recommend stating explicitly the mode of convergence. If p is the density then even assuming A.4.3 can be fixed to prove weak convergence of the **joint** distribution of filters is not enough not justify Eq. (10) - convergence in distribution does not imply pointwise convergence of the density function. If p is the distribution, then I would possibly use the more standard notation '\otimes' instead of '\prod'.- (p.17, end of A.4.2) You say "Note that addition of various layers on top (as discussed in Section 3) does not change the proof in a qualitative way". Can you please provide the formal details? At the very least, joint convergence of filters will have to be established if fully connected layers are added on top. This is the main reason why joint convergence of filters in the top layer is important.****Specific comments &amp; issues for individual proofs******Approaches suited infinite networks ("sequential limit")**As mentioned in the beginning, it is not entirely clear how to formalise infinite networks in a way analogous to Eqs. (1) and (2) in your paper. This is important because you are ultimately proving statements about random variables, like convergence in probability, and this is not possible if those random variables are not formally defined. This section only comments on technical issues with the approaches described in (A.4.1) and (A.4.2). From now on, I assume that the authors' were able to formally define all the mentioned random variables in a way that fits with (A.4.1) and (A.4.2).(i) Hazan and Jaakola type approach (A.4.1)This approach essentially iteratively applies a version of the recursion first studied by Hazan and Jaakola (2015), "Steps Toward Deep Kernel Methods from Infinite Neural Networks".- (p.16, A.4.1) Please provide reference for the claim that "pairwise independent Gaussian implies joint independent Gaussian". This seems to assume that the variables are jointly Gaussian which is, as far as I can see, not established here.        - see second part of the linked answer for a nice example of three random variables with pairwise standard normal marginals, but joint not the multivariate standard normal:        https://stats.stackexchange.com/questions/180708/x-i-x-j-independent-when-i%E2%89%A0j-but-x-1-x-2-x-3-dependent/180727#180727 - (p.16, A.4.1) The application of the multivariate CLT is slightly more complicated than the text suggests. Except for the necessity of proving finiteness of the relevant moments, multivariate CLT does not out-of-the-box apply to infinite dimensional random variables like {z_j^{l+1}}_{1 \leq j \leq \infty} as claimed. Hence joint convergence is not proved which will be problematic for the reasons explained earlier.(ii) Lee et al. type approach (A.4.2)This type of approach follows the technique used by Lee et al. (2018), "Deep Neural Networks as Gaussian Processes".Application of the weak law of large numbers (wLLN): As mentioned before, convergence in probability is only possible between random variables on the same underlying space. This is usually not a problem when wLLN is applied as the random variables converge to a constant random variable. Because every constant random variable generates the trivial sigma-algebra, it is measurable for any underlying probability space and thus convergence in probability is well-defined. The situation here is more complicated because the target is constant only conditionally on the previous layer, i.e. is not constant. As a side note, even the conditioning is only well-defined if all random variables live on the same space (conditioning on a random variable is technically conditioning on the sub-sigma-algebra it generates on the shared space).Assuming the problem with all K^{l, t} (t denotes the dependence on network width), for all l \in {1, ... L} and t \in {1, 2, 3, ...}, being on the same underlying probability space is solved, the next point is application of the wLLN itself. You claim "we can apply the law of large numbers and conclude that [Eq. (6)]" (p.4) which is not entirely correct here. Focusing on the application when the sizes of all the previous layers are held fixed, the two conditions that have to be checked here are: (i) the conditional expectation of the iid summands in Eq. (3) is finite; (ii) the sequence of iid variables is fixed. Please provide an explicit proof of (i). Regarding (ii), I am specifically concerned with the fact that with changing t (and thus network widths), the sequence of random variables changes (because the previous K^{l-1,t} matrix changes) which means that completely different size of the current layer may be necessary to get sufficiently close to the target (which has itself changed with t). In other words, instead of having a fixed infinite sequence of iid random variables, you currently have a sequence of growing finite sets of random variables which are iid only within the finite sets, but not between members of the sequence (different t). The direct implication is that this type of proof is not applicable to the "simultaneous limit" case as claimed in the main text (Section 2.2 says all proofs are equivalent and lead to Eq. (10) which explicitly takes the simultaneous limit), since the application would require some form of uniform convergence in probability akin to (A.4.3). I think that the approach taken in (A.4.3) is a correct way to address this issue and would thus recommend focusing on (A.4.3) and leaving (A.4.2) out. The appendix seems to acknowledge that (A.4.2) does not work for the "simultaneous limit" - please adapt the main text accordingly.A note on convergence in probability: In Eq. (3), the focus is on convergence in probability of individual entries of the K matrices. This in general does not imply convergence of all entries jointly. However, the type of convergence studied here is convergence to a constant random variable which is fortunate because simultaneous convergence of all entries in probability can be obtained for free in this case (thanks to having a **finite** number of entries of K). I think it might be potentially beneficial for the reader if this was explicitly stated as a footnote with an appropriate reference included.A note on marginal vs joint probability: As you say above Eq. (23), you are only proving convergence of a single filter marginally, instead of the full sequence {z_j^L}_{1 \leq j \leq \infty} jointly. Convergence of the marginals does not imply convergence of the joint, which will be problematic for the reasons explained earlier.**Approaches for BNNs ("simultaneous limit")**(iii) The proof in (A.4.3)My biggest concern about this approach is that it only establishes convergence of a single filter marginally, instead of the full sequence {z_j^L}_{1 \leq j \leq \infty} jointly. Convergence of the marginals does not imply convergence of the joint, which will be problematic for the reasons explained earlier.Other comments:- (p.17) You say "Using Theorem A.1 and the arguments in the above section, it is not difficult to see that a sufficient condition is that the empirical covariance converges in probability to the analytic covariance".        - Can you please provide more detail as it is unclear what exactly do you have in mind?        - I will be assuming from now on that you show that a particular combination of the Portmanteau theorem and convergence of K^L in probability to get pointwise convergence of the characteristic function is sufficient.- (p.18) Condition on activation function: The class \Omega(R) is dependent on the considered input set X through the constant R. This seems slightly cumbersome as it would be desirable to know whether a particular activation function can be used without any reference to the data. It would be nice (but not necessary) if you can derive a condition on \phi which would not rely on the constant R but allows ReLU.- (p.19, Eq. 48) I see where Eq. (48) is coming from, i.e. from Eq. (44) and the assumption of \bar{\varepsilon} ball around A(K_\infty^l) being in PSD(R), but it would be nicer if you could be a bit more verbose here and also write out the bound explicitly (caveat: I did not check if the definition of \bar{\varepsilon} matches up but assume a potential modification would not affect the proof in a significant way).- (p.19) The second part of the proof is a little confusing, especially after Eq. (49) - please be more verbose here. For example, just after Eq. (49), it is said that because the two random variables have the same distribution, property (3) of \Omega(R)'s definition can be applied. However the two random variables are not identical and importantly are not constructed on the same underlying probability space. Property (3) is a statement about the the set of random variables {T_n (Sigma)}_{Sigma \in PSD_2(R)} and not about the different 2x2 submatrices of K^{l+1}, but it needs to be applied to the latter. When this is clarified, the next point that could be made clearer is in the following sentence where changing t will affect the 2x2 submatrices of K^{l+1,t} as well as the bound through U(t) and V(t); it is not immediately obvious that the proof goes through as claimed so please be a bit more verbose.****Typos and other minor remarks****- (p.2, top) "hidden layers go to infinity uniformly": The use of word uniformly is non-standard in this context. Please clarify.- (p.3, Eq. 2) Using x for both inputs and post-activations is slightly confusing.- (p.4, Eq. 5) Should v_\beta multiply \sigma_\beta^2 ?- (p. 4) The summands in Equation (3) are iid -&gt; "conditionally iid" (please also specify the conditioning variables/sigma-algebra).- (p.4, Eq. 4) Eq. (4) is slightly confusing given you mention that K is a 4D object on the previous page.        - I only understood K is "flattened" into |X|d x |X|d matrix when I reached (A.4.3) - this should be stated in main text as otherwise the above confusion arises.- (p.5, 3 and 3.1) The introduction of "curly" K is slighlty confusing. Please provide more detail when introducing the notation, e.g. state in what space the object lives.- (p.5, before Eq. (11)) Is R^{n^(l+1)} the right space for vec(z^L) ? It seems that the meaning of z changes here as compared to the definition in Eq. (2). If z is still defined as in Eq. (2), how exactly is the vec operator defined here? Please clarify.- (p.16, A.4.2) "law of large number" -&gt; "weak law of large numbers"- (p.17) T_n is technically not a function from PSD_2 only but also from some underlying probability space into a measurable space (i.e. can be viewed as a random variable from the product space of PSD_2 and some other measurable space).- (p.18, Eq. 38) Missing dot at the end. Also the K matrix either should or shouldn't have the superscript "l" (now mixed); it does have the superscript in Eq. (39) so probably "should".- (p.18, Eq. 39) Slightly confusing notation. Please clarify that both K and A(K) should have diagonal within the given range.- (p.18) "squared integrable" -&gt; "square integrable" or "square-integrable"- (p.18) Last display before Eq. (43): second inequality can be replaced by equality?!- (p.19, Eq. 47) The absolute value should be sup norm.- (p.19, Eq. 49) LHS is a scalar, RHS a 2x2 matrix (typo).- (p.19, last sentence of the proof) It does not seem the inequalities need to be strict. The authors implement a two-stage multi-objective optimization scheme to optimize neural network architectures with several conflicting goals.I can not accept the paper in its current form.In short, I have the following main criticisms:1. use of crowding distance(CD) instead of hypervolume-contribution.CD is not consistent with the HV estimator, especially CD might remove solutions that have a large HV-contribution and thus HV will not increase monotonically. The effect is even visible in Figure 8c) as in iteration 22, HV is decreasing as crowding distance removes a good offspring. In short: Crowding distance should not be used as long as the number of objectives does not prohibit computing the HV-contribution.2. No good justification of BN. It is unclear to me why BN should be used instead of more iterations at stage 1. In 4.4 BN is only compared to the uniform initialization, but this comparison has no meaning given that we already have an optimized front that improved on the uniformly sampled distribution. To be honest, the samples shown from BN do not look very convincing as a lot of very poor architectures are created.A proper comparison would be comparing the 2-step approach with only the first step and the same budget. Then we could compare samples from both distributions (either sampling from the front using mutation/crossover or sampling from BN). Also we would have a fair comparison of the obtained fronts and HV-values.3. Ablation study cross-overI am not convinced by the results presented. The paper says this is a "small scale" study but does not give the number of iterations/samples. It is clear that in the setup of the mutation operator cross-over might help, simply because it can change many more connections in a single iteration than mutation alone, which is limited to max 1 change. Allowing up to two mutations and no crossover could already proof to be better (orsmaller size of offspring population, see below)Smaller concerns:1. The results suggest that the uniform distribution might not be tuned well, as it only covers the "expensive" networks but not the "cheap" networks. A better initialization scheme that covers the x-axis better might already show vastly different results. As the Flop-objective is cheap to compute and does not require simulation, one could expect to tune this offline before initialization.2. No handling of Noise.During optimization, the chosen starting point and SGD algorithm will introduce noise into the process. Thus, the final test accuracy will be noisy. As an elitist dominance scheme is used, one might easily end up with an architecture that has a large variance when trained, i.e. when performing a final training pass on the full dataset, the performance might be very different. Moreover, the algorithm might stop convergence towards the true pareto front as it is held back by noisy "good" results. This should be discussed in the paper3. A single-offspring approach might be better than sampling a full population (or offspring size in the order of parallel instances one can expend to run). 40 sounds excessive given that the sampling distribution is only improved through selection and given that the pareto front approximation appears to include less than 40 elements. This might also affect the results in the ablation study for cross-over: more iterations with reduced offspring size allows for more mutations of successful offspring.4. Some unclear or wrong wordings:page 4: "As a consequence[...] the best solution encountered [...] will always be present in the final population. " What do you consider "best" in a 2-objective problem? Do you mean: the best in each objective?page 6, footnote1: this is not true. even without crossover the selection operator ties the solutions together, an offspring has to beat any point in the population, not necessarily its direct parent.5. Figure 8a) does not include the state of the art result for CIFAR10, see for example http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130 I didnt worked in the field of structure elucidation from NMR spectroscopy so I might missed something. This paper utilized Siamese neural networks (Bromley 1993) and contrastive loss (Hadsell et al 2006) to learn a latent representation from the NMR spectra, which can be used for similarity search and the most similar compounds will be able to shad some light for the structures of the unknown natural product. The experiments showed significant improvement (AUC under precision-recall curve) over competitors. My main concern is the lack of novelty in the methodology. The formulation is the same as Hadsell et al 2006 and the only change is an coefficient added to a loss term which kept to be fixed without explanation. As a result, I dont feel this is enough to make it publishable at ICLR.  Some detailed comments below:1. In eq(3), why P = 1.5? Some intuition or explanation?2. I spent some time understanding the evaluation method in Section 4.2 and still not very sure I understand it. What is the formulae to compute your Tanimoto scores? Is this the same as PubChem Tanimoto scores which depends on fingerprints?3. There is a formatting issue for the prediction and recall formulae.4. I know in the computational mass spectrometry community there was an recent paper, Critical Assessment of Small Molecule Identification 2016: automated methods where the winners were predicting the molecular fingerprints which directly shad the lights on the compound structures and can also be used for similarity searching, is this an option here for NMR data? Summary: The authors present an autoencoding strategy for disentangling structure and appearance in image data. They achieve this using a learned, spatial prior in the VAE framework.Writing: The paper contains grammatical errors and I found Section 2 (2.1 and 2.2, specifically) to be a bit confusing as many ideas were described with words when they could have been outlined more precisely mathematically. Major comments:The paper takes ideas from Zhang et al and Jakab et al and put them in a VAE context. The paper, however, constructs the ELBO in such a way that distance it from many key ideas of the VAE. Particularly, the paper decomposes the ELBO into three terms and proceed to define these terms as they wish. In this way, the novel contributions of this paper are left unclear and many decisions left unjustified.- Incorporating landmark/spatial information into autoencoders is not a new idea. Zhang et al and Jakab et al both train autoencoders with disentangled structure, and Finn, 2016 [1] uses a similar spatial landmark strategy when learning representations. Incorporating structural information as a prior distribution (as in this paper) is an interesting idea. However, it is not clear that the defined prior log p(y) is a proper density (integrates to 1). Given the importance of a prior distribution in VAEs, this choice should be precisely justified (maybe relate it to beta-VAE or just use a properly normalizable distribution)- The paper chooses variational distributions in such a way that removes the entropy term from the KL divergences. Specifically, both q(z | x, y) and p(z | y) have fixed, identity covariance, resulting in the KL divergences equating to L_2 distance. An important part of the VAE is the explicit incorporation of uncertainty by means of learned variances. Although this can sometimes be problematic (see beta-VAE), neglecting to include these variances at all removes an important aspect of VAE.- The paper includes a likelihood model which also throws away key ideas from VAE. Although some neural likelihood models as in the VAE have issues with blurriness, the reasons are still not completely understood. However, there are well explored alternatives to what the authors propose. For example, many image-based VAEs use Bernoulli likelihoods [2, 3] or autoregressive likelihoods [4]. Autoregressive models, especially, can produce sharp images. The authors introduce a unnormalizable likelihood which combines L1 loss with a function that incorporates L1 distance in VGG space. Using VGG in the likelihood model is unjustified and seems unnecessarily complicated, given the existence of powerful decoders that already exist in VAE literature.- The authors incorporate various connections and concatenations between neural networks and distributions that further complicate the variational lower bound. For example, p(z | y) is concatenated to q(z | x, y) in addition to acting as a prior on q(z | x, y). This introduces a dependency between the likelihood model and the variational posterior which normally does not exist. Furthermore skip connections are introduced between E_\theta and D_\theta, which complicate the ELBO further. The authors should explicitly write out the loss function they are optimizing at this point or describe how they are modifying ELBO to justify these chocise.Overall, I find it difficult to call this a variational autoencoder given the liberal modifications to the evidence lower-bound. However, even if I was to interpret this work as an autoencoder with a custom loss function, this model ends up very similar to that in Zhang et al with the main differences being the inclusion of an equivariance constraint in Zhang that is not present in this paper and that Zhang et al use a feature map thats multiplied by landmarks to incorporate appearance information whereas this paper uses the z representation as appearance information. The qualitative results of this paper, although good looking, are very similar to qualitative results in Zhang et al and Jakab et al. A quick comment: in Figure 6, you should use the same celebrity faces when comparing Jakab against your own work. The quantitative results only compare to the same model trained without the KL loss term; this, in my mind, is more of a sanity check than a fair baseline. The authors should be comparing against alternate strategies that incorporate spatial information, such as Zhang et al and Jakab et al.[1] Finn, Chelsea, et al. "Deep spatial autoencoders for visuomotor learning." 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.[2] Chen, Xi, et al. "Variational lossy autoencoder." arXiv preprint arXiv:1611.02731 (2016).[3] http://ruishu.io/2018/03/19/bernoulli-vae/[4] van den Oord, Aaron, et al. "Conditional image generation with pixelcnn decoders." Advances in Neural Information Processing Systems. 2016. I like the idea of trying to qualitatively illustrate the behavior of SGD when optimizing parameters of complex models, such as Deep and Conv Nets, but I think that the contribution is not very substantial. The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. It would helpful to illustrate the difference between "crossing" and "moving over" a barrier with a simple figure. The experimental validation is interesting, although I think it is limited and perhaps the conclusions that can be drawn from it are not so surprising. I believe it would have been interesting to study other important factors that affect the behavior of SGD, such as learning rate and type of momentum. For example, a larger learning rate might allow for more crossing of barriers. Also, different SGD algorithms (ADAGRAD, ADAM, etc...) would behave considerably differently I expect. At the moment these important factors are overlooked. It is not clear to me why we would want to avoid larger batch sizes. A larger batch size allows for a lower variance of stochastic gradients, and therefore faster convergence. I think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful SGD works, such as SAGA and the like. I agree that a smaller batch-size is preferable at the beginning of the optimization, but again this is a well known fact (again, see SAGA) and it is for computational reasons mostly (being far away from the (local) mode, a noisy gradient is enough to move in the right direction - no need to spend computations to use an accurate gradient). There is no guarantee that the local optimum close to initialization is a bad local optimum in general, so I don't think that using a large batch size at the beginning is a bad idea for this reason - again it is just computational. Another thing missing I think is the discussion around why it is potentially a good thing to cross the barrier, either at the beginning of the exploration or towards convergence to a local optimum. At the moment, the paper seems to report the behavior of SGD without key insights on the importance of crossing or avoiding crossing barriers.As a concluding remark - there has been a lot of work on the connections between diffusions and MCMC algorithms (see e.g., the Metropolis Adjusted Langevin Algorithm - MALA) and a lot of the considerations made in the paper are somewhat known. That is, random walk/diffusion type MCMC (and even gradient-based MCMC like Hybrid Monte Carlo) struggle a lot in non-convex problems and they hardly move across modes of a posterior distribution (equivalent to crossing barriers of potential). So I'm not at all surprised that SGD does not cross barriers during optimization and I would challenge the statement in the introduction saying "Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process." The authors propose to apply Dropout only in the backward pass, by applying a mask sampled from a Bernoulli distribution. They claim that this method can help in situations like optimizing non-decomposable losses where minibatch SGD is not viable. First and foremost, the paper has an acknowledgement paragraph that gives information violating, in my sense, the anonymity requirement. This being said, I have other concerns with the paper, and this possible violation didn't effect much my rating. First, the authors claim that the proposed method "is a flexible strategy for introducing data-dependent stochasticity into the gradient". However, it doesn't seem to me that the sampled dropped nodes are data-dependent. It is also not clear to me why the proposed method is better suited to non-decomposable losses and hierarchically structured data than the classical Dropout.Moreover, while the method is clearly related to Dropout, the paper lacks of comparison to this regularizer. This being said, the idea is sound, and can have a good impact in for example combining the good aspects of batch-normalization and dropout. However, the authors structured the paper on a completely different argument that doesn't convince me for the reasons cited above. Overview:The authors aim at finding and investigating criteria that allow to determine whether a deep (convolutional) model overfits the training data without using a hold-out data set.  Instead of using a hold-out set they propose to randomly flip the labels of certain amounts of training data and inspect the corresponding 'accuracy vs. randomization curves. They propose three potential criteria based on the curves for determining when a model overfits and use those to determine the smallest l1-regularization parameter value that does not overfit. I have several issues with this work. Foremost, the presented criteria are actually not real criteria (expect maybe C1) but rather general guidelines to visually inspect 'accuracy over randomization curves. The criteria remain very vague and seem be to applicable mainly to the evaluated data set (e.g. what defines a steep decrease?). Because of that, the experimental evaluation remains vague as well, as the criteria are tested on one data set by visual inspection. Additionally, only one type of regularization was assumed, namely l1-regularization, though other types are arguably more common in the deep (convolutional) learning literature.  Overall, I think this paper is not fit for publication, because the contributions of the paper seem very vague and are neither thoroughly defined nor tested.Detailed remarks:General:A proper definition or at least a somewhat better notion of overfitting would have benefitted the paper. In the current version, you seem to define overfitting on-the-fly while defining your criteria. You mention complexity of data and model several times in the paper but never define what you mean by that.Detailed:Page 3, last paragraph: Why did you not use bias terms in your model?Page 4, Assumption. - What do you mean by the data being independent? Independent and identically distributed?  - "As in that case correlation in the data can be destroyed by the introduction of randomness making the data easier to learn. What do you mean by "easier to learn"? Better generalization? Better training error? - I dont understand the assumptions. You state that the regularization parameter should decrease complexity of the model. Is that an assumption? And how do you use that later?- What does "similar scale mean? Page 4, Monotony. - You state two assumptions or claims, 'the accuracy curve is strictly monotonically decreasing for increasing randomness and 'we also expect that accuracy drops if the regularization of the model is increased, and then state that 'This shows that the accuracy is strictly monotonically decreasing as a function of randomness and regularization. Although you didnt show anything but only state assumptions or claims (which may be reasonable but are not backed up here). I actually dont understand the purpose of this paragraph.- Section 3.3 is confusing to me. What you actually do here is you present 3 different general criteria that could potentially detect overfitting on label-randomized  training sets. But you state it as if those measures are actually correct, which you didnt show yet.My main concern here, besides the motivations that I did not fully understand (s.b.), is the lack of measurable criteria. While for criterion 1 you define overfitting as 'above the diagonal line and underfitting as below the line, which is at least measurable depending on sample density of the randomization, such criteria are missing for C2 and C3.       Instead, you present vague of sharp drops and two modes but do not present rigorous definitions. You present a number for C2 in Section 5, but that is only applicable to the present data set (i.e. assuming that training accuracy is 1). Criterion 2 (b) is not clear.  - I neither understand "As the accuracy curve is also monotone decreasing with increasing regularization we will also detect the convexity by a steep drop in accuracy as depicted by the marked point in the Figure 1(b)" nor do I understand "accuracy over regularization curve (plotted in log-log space) is constant"?Does that mean that you assume that whenever the training accuracy drops lower than that of the model without regularization, it starts to underfit?Due to the lack of numerical measures, the experimental evaluation necessarily remains vague by showing some graphs that show that all criteria are roughly met by regularization parameter \lambda=0.00011 on the cifar data set.  In my view, this evaluation of the (vague) criteria is not fit for showing their possible merit. The authors propose a task of classifying objects from tactile signals. To do this, the image and haptic data are collected for each object. Then, image-to-haptic and haptic-to-label predictors are trained by supervised learning. In the experiment, prediction accuracy on unseen object class is studied. The paper is clearly written although it contains several typos. The proposed task of cross-modal inference is an interesting task. I however hardly find any significance of the proposed method. The proposed method is simple non-end-to-end predictors trained by supervised learning. So, the proposed model seems more like a naive baseline. It is not clear what scientific challenge the paper is solving and what is the contribution. Also, the performance seems not impressive. Im not sure why the authors average the haptic features. Lots of information will be lost during the averaging, why not RNNs. Overall, the paper seems to require a significant improvement. This paper provides a new framework for multitask learning in nlp by taking advantage of the similarities in 10 common NLP tasks. The modeling is building on pre-existing qa models but has some original aspects that were augmented to accommodate the various tasks.  The decaNLP framework could be a useful benchmark for other nlp researchers.  Experiments indicate that the multi-task set-up does worse on average than the single-task set-up.  I wish there was more analysis on why multi-task setups are helpful in some tasks and not others.  With a bit more fine-grained analysis, the experiments and framework in this paper could be very beneficial towards other researchers who want to experiment with multi-task learning or who want to use the decaNLP framework as a benchmark.I also found the adaptation to new tasks and zero-shot experiments very interesting but the set-up was not described very concretely:   -in the transfer learning section, I hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multi-task objective or if there would still be performance gain by pretraining a model on only one of those tasks.  For example, would a model pre-trained solely on IWSLT see the same performance gain when transferred to English-&gt;Czech as in Figure 4? Or is it actually the multi-task training that is causing the improvement in transfer learning?   -Can you please add more detail about the setup for replacing +/- with happy/angry or supportive/unsupportive? What were the (empirical) results of that experiment?I think the paper doesnt quite stand on its own without the appendix, which is a major weakness in terms of clarity.  The related work, for example, should really be included in the main body of the paper.  I also recommend that more of the original insights (such as the experimentation with curriculum learning) should be included in the body of the text to count towards original contributions.  As a suggestion, the authors may be able to condense the discussion of the 10 tasks in order to make more room in the main text for a related work section plus more of their motivations and experimental results.  If necessary, the main paper *can* exceed 8 pages and still fit ICLR guidelines.Very minor detail: I noticed some inconsistency in the bibliography regarding full names vs. first initials only. I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it.  I have three major complaints with this paper:                                                                                                                                                                              1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering.                                                                                                     Question answering is not a unified phenomenon.  There is no such thing as "general question answering", not even for humans.  Consider "What is 2 + 3?", "What's the terminal velocity of a rain drop?", and "What is the meaning of life?"  All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.                                                                                                     Question answering is a _format_ for studying particular phenomena.  Sometimes it is useful to pose a task as QA, and sometimes it is not.  QA is not a useful format for studying problems when you only have a single question (like "what is the sentiment?" or "what is the translation?"), and there is no hope of transfer from a related task.  Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems.We have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks).  I don't see any compelling justification for setting things up this way.                                                                                                     2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering.  There is nothing new in the transfer results that were presented here, however.  For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first).  For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly).  For the Czech task, fine tuning a pre-trained model has already been shown to help.  Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before.  The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label.  It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case.                                                                                                     3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice.  Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious.  Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior.                                                                                                     The three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results.  Any of these three could have been a single conference paper, had it been done well.  As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point.  Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?).  For MQAN, there's more than a page of the core new architecture that's pushed into the appendix.  And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere. The paper introduces a spectral regularization with the aim of obtaining representationsthat are easier to interpret.Some sentences are often confusing and, in general, clarity needs to be improved.The motivation of the work is not very strong in my opinion, in particular by adding sucha prior the space of possible solutions greatly shrinks and I am afraidthat interesting solutions will be lost. I think one should focus on propertiesrather than visual inspection.Also, isn't it that if we can clearly see the pattern, perhaps that pattern islinear and of easy discovery also by simpler models?More importantly, it seems that all experiments are performed on tasks where theunderlying structure is known, however this is almost never the case in practice.Assuming one uses the proposed spectral regularization, how would one interpretit in such cases?In section 2 please clarify the paragraph on bounded Lp norm.I am sorry but why isn't there a relation, for convolutional nets,between neurons in different channels? Each element in the feature map representsthe input surrounding that location in a k dimensional space.The authors state that the usual bottleneck for autoencoders is composed of 2/3neurons, this is simply not true. There has been extensive work onovercomplete representations that shows that is better to have many more dimensionsbut only few degrees of freedom.The spectral bottleneck should cite VQVAE as the approach is very similar and the authors should compare to it.For the topological inference experiment it is assumed that one knows the structure,but how to address the more general problem?More practically, the regularization enforces smoothing (if few eigenfunctionsare used, which is never explained in the paper) between connected nodes, didthe authors try to have a simple L2 penalty instead? E.g. minimize the differencebetween activations in the group.Regarding the capsule network example, when you write that without regularizationeach digit responds differently to perturbation of the same dimension, isn't itpossibly true only up to a, unknown, permutation of the neurons?To summarize, while the idea sounds interesting, I miss to find the easy interpretabilityof results and also the overall motivation sounds a bit weak. More importantly the selection of W, crucial for defining structure, is not discussed at all in the paper.Experiments are performed on toy examples only whereas here, given that we canpossibly interpret the results I would have liked something more involved tobetter show that this kind of interpretability is needed.Missing cites:[1] van den Oord et al, Neural Discrete Representation Learning.[2] Koutnik et al, Evolving neural networks in compressed weight space. The paper claims to propose a novel generative probabilistic neural network model such that its encoder (classifying an image) can be approximated by a convolutional neural network with ReLU activations and MaxPooling layers. Besides the standard parameters of the units (weights and biases), the model has two additional latent variables per unit, which decide whether and where to put the template (represented by the weights of the neuron) in the subsequent layer, when generating an image from the class. Furthermore, the authors claim to derive new learning criteria for semi-supervised learning of the model including a novel regulariser and claim to prove its consistency. Unfortunately, the paper is written in a way that is completely incomprehensible (for me). The accumulating ambiguities, together with its sheer length (44 pages with all supplementary appendices!), make it impossible for me to verify the model and the proofs of the claimed theorems. This begins already with definition of the model. The authors consider the latent variables as dependent and model them by a joint distribution. Its definition remains obscure, let alone the question how to marginalise over these variables when making inference. Without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.In my view, the presented material exceeds the limits of a single conference paper. A clear and concise definition of the proposed model accompanied by a concise derivation of the basic inference and learning algorithm would already make a highly interesting paper.Considering the present state of the paper, I can't, unfortunately, recommend to accept it for ICLR. Summary:--------------The paper considers the problem of constructing compositional robotic morphologies that can solve different continuous control tasks in a (multi-agent) reinforcement learning setting. The authors created an environment where the actor consists of a number of primitive components which interface with each other via "linking" and construct a morphology of a robot. To learn in such an environment, the authors proposed a graph neural network policy architecture and showed that it is better than the baselines on the proposed tasks.I find the idea of learning in environments with modular morphologies as well as the proposed tasks interesting. However, the major drawback of the paper is the lack of any reasonable details on the methods and experiments. It's hard to comment on the novelty of the architecture or the soundness of the method when such details are simply unavailable.More comments and questions are below. I would not recommend publishing the paper in the current form.Comments:----------------- If I understand it correctly, each component ("limb") represents an agent. Can you define precisely (ie mathematically) what the observations and actions of each agent are?- Page 4, paragraph 2: in the inline equation, you write that a sum over actions equals policy applied to a sum over states. What does it mean? My understanding of monolithic agents is that observations and actions must be stacked together. Otherwise, the information would be lost.- Page 4, paragraphs 3-(end of section): if I understand it correctly, the proposed method looks similar to the problem of "learning to communicate" in a cooperative multi-agent setting. This raises the question, how exactly the proposed architecture is trained? Is it joint learning and joint execution (ie there's a shared policy network, observation and action spaces are shared, etc), or not? All the details on how to apply RL to the proposed setup are completely omitted.- Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?- From the videos, it looks like certain behaviors are very unphysical or unrealistic (eg parts jumping around and linking to each other). I'm wondering which kind of simulator was used? How was linking defined (on the simulator level)? It would be nice if such environments with modular morphologies were built using the standard simulators, such as MuJoCo, Bullet, etc.All in all, despite potentially interesting ideas and setup, the paper is sloppily written, has mistakes, and lacks crucial details. This paper proposed a 3D shape generation model. The model is essentially an auto-encoder. The authors explored a new way of interpolation among encoded latent vectors, and drew connections to object functionality.The paper is, unfortunately, clearly below the bar of ICLR in many ways. Its technically incremental: the paper doesnt propose a new model; it instead suggests new way of interpolating the latent vectors for shape generation. The incremental technical innovation is not well-motivated or justified, either: the definitions of new concepts such as functional essence and importance vector are ad-hoc. The results are poor, much worse compared with the state-of-the-art shape synthesis methods. The writing and organization can also be improved. For example, the main idea should be emphasized first in the method section, and the detailed network architecture can be saved for a separate subsection or supplementary material. Its good that the authors are looking into the direction of modeling shape functionality. This is an importance area that is currently less explored. I suggest the authors look into the rich literature of geometry modeling in the computer graphics and vision community, and improve the paper by drawing inspiration from the latest progress there. This paper presents a method for generating 3D objects. They train a VAE to generate voxel occupancy grids. Then, they allow a user to generate novel shapes using the learned model by combining latent codes from existing examples. Pros:- The idea of linking affordances to 3D object generation is interesting, and relevant to the machine learning and computer vision communities.- They propose to evaluate the quality of the shape based on a physical simulation (Section 4.4.3), which is an interesting idea.Cons:- This paper is not well written. The method is described in too much detail, and the extra length (10 pages) is unnecessary. Cross entropy, VAEs, and many of the CNN details can usually just be cited, instead of being described to the reader.- The paper uses suggestive terminology, like "functional essence" and "functional arithmetic" for concepts that are fairly mundane (see Lipton and Steinhardt, 2018 for an extended discussion of this issue). For example, the "functional essence" of a class is essentially an average of the VAE latent vectors (Section 3.3.1). The paper claims, without sufficient explanation, that this is computation is motivated by the idea that "form follows function".- The results are not very impressive. There is no rigorous evaluation. They propose several nice metrics to use (eg. affordance simulation), but the results they present for each metric are quite limited. The qualitative results are also not particularly compelling.- The paper should more thoroughly evaluate the importance weighting that is described in Section 3.3.2. - The technical approach (combining VAE vectors to make new shapes) is not particularly novel[Overall:The paper should not be accepted in its current form, both due to the confusing writing, and the lack of careful evaluation. The paper describes a clipping method to improve the performance of one particular type of quantization method that is naive clipping to closest "bins". The contribution of the paper is the (possibly incorrect) derivation of the clipping value that causes the least quantization error IF assumptions can be made about the distribution of the parameters (in a non-bayesian sense). Thus, the significance is low due to both reasons.One conceptual issue is the assumed relationship between quantization error and classification accuracy. The literature has shown that high quantization error does not necessarily mean low classification accuracy when using non-uniform quantization. The proposed clipping does not account for classification accuracy (on training set), but I understand the motivation being that the training set is not available. 1. There seems to be an error in derivation of Eq (3), the first term should be $(x-sgn(x).\alpha) = x+\alpha$ for $x$ negative. Please comment on this.2. When solving the integrals, the authors simply pull the solution "out of the hat" and show that the derivative is the integrand. This is a very opaque presentation that we cannot see how you solved the integral. What is C in $\psi(x)$?3. The assumptions on the parameters are only valid for the particular model/dataset/precision. The assumption does not generalize arbitrarily. For example, models with quantized weights have bi-modal distributions. How would you clip the  activations after e.g. a ReLu? This is without going in to the weaknesses of the K-S test. 4. Experiments do not show any comparison to the large body of prior work in this area. 5. Page 4, para below (3), what is "common additive orthogonal noise"? You should explain or give intuition instead of simply referring to a different paper.6. In the uniform case, one would think f(x)=1/&lt;range of the interval&gt;=2\alpha. Why is it 1/\Delta?6. Section 4, range should be [-\alpha, \alpha] instead of [\alpha, -\alpha]? Since \alpha is positive. This paper proposes a method for learning sentences encoders using artificially generated (fake) sentences. While the idea is interesting, the paper has the following issues:- There are other methods that aim at generating artificial training data, e.g.:  Z. Zhao, D. Dua, S. Singh. Generating Natural Adversarial Examples. International Conference on Learning Representations (ICLR). 2018,  but no direct comparison is made. Also InferSent  (which is cited as related work) trains sentence encoders on SNLI: https://arxiv.org/pdf/1705.02364.pdf. Again a comparison is needed as the encoders learned perform very well on a variety of tasks. Finally, the proposed idea is very similar to ULMfit (https://arxiv.org/pdf/1801.06146.pdf) which trains a language model on a lot of unlabeled data and then finetunes it discriminatively. Finally, there should be a comparison against a langauge model without any extra training in order to assess the benefits of the fake sentence classification part of the model.- It is unclear why the fake sentence construction method proposed by either swapping words or just removing them produces sentences that are fake and/or useful to train on. Sure it is simple, but not necessarily fake. A language model would be able to discriminate between them anyway, by assigning high probability to the original ones, and low probability to the manipulated ones. Not sure we need to train a classifier on top of that.- I found the notation in section 2 confusing. What kind of distribution is P(enc(x,theta1)|theta2, theta3)? I understand that P(x|theta) is the probability of the sentence given a model, but what is the probability of the encoding? It would also be good to see the full derivation to arrive at the expression in the beginning of page 3. - An argument in favour of the proposed method is training speed; however, given that less data is used to train it, it should be faster indeed. In fact, if we consider the amount of time per million sentences, the previous method considered in comparison could be faster (20 hours of 1M sentences is 1280 hours for 64M sentences, more than 6 weeks). More importantly, it is unclear from the description if the same data is used in training both systems or not.- It is unclear how one can estimate the normalization factor in equation 2; it seems that one needs to enumerate over all fake sentences, which is a rather large number due to the number of possible word swaps in the sentence,- I am not sure the generator proposed generates realistic sentences only, "Chicago landed in John on Friday" is rather implausible. Also there is no generation method trained here, it is rule-based as far as I can tell. There is no way to tell the model trained to generate a fake sentence as far as I can tell.- It is a bit odd to criticise other methods ofr using LSTMs with "millions of parameters" while the proposed approach also uses them. A comparison should calculate the number of parameters used in either case.- what is the motivation for having multiple layers without non-linearity instead of a single layer? Summary:=======The paper proposes a discriminative training formulation for learning sentence representations, where a classifier is required to distinguish between real and fake sentences. The sentences are encoded with a Bi-LSTM and the resulting sentence representations are then used in a number of sentence-level tasks (classification, entailment, and retrieval). The experiments show benefits on most tasks compared to Skip-Thought and FastSent baselines, and the information captured by the representations is analyzed with probing tasks, showing that they are better at capturing certain kinds of information like the presence or order of words. The paper proposes a simple and fairly effective approach for learning sentence encoders. The basic idea is appealing and the experimental results are fairly good. However, at present it seems like more work is required for delivering a comprehensive evaluation and analysis. My main concerns with the paper are the insufficient comparison with prior work, its lack of clarity and organization in certain places, and the limited amount of work. Please see below detailed comments on these and other points, as well as suggestions for how to improve some of these issues.  Major comments:==============1. Better baselines and comparisons: - The results are compared only with SKip-Thought and (the weaker) FastSent. However, there are far better models by now. First, already in the Skip-Thought paper there is a version combining Naive Bayes bi-gram features which performs much better on some benchmarks, for example that version would be better than the paper's results on MR (80.4). - Moreover, there have been many newer papers with better results on many of the tasks [1, 2, 4, and references therein]. At the very least, mention should be made that there are better published results, and ideally there should be some comparison to the more relevant papers [1, and maybe others].  2. Paper organization and clarity:- I found Section 2 to be unnecessarily lengthy and disorganized. It mixes motivation with modeling, introduces excessive notation, sometimes without clearly defining it (what is L_{aux}? Why is U in eq. 2 not defined on first usage?), and digresses to weakly related discussions (the link to GANs seems vague and the relation to Introspective Neural Networks is not made clear). The last paragraph is largely redundant with the introduction. - There is also a statement that seems just wrong: "maximizing the data likelihood P(enc(x,\theta_1)|\theta_1,\thera_3)" -- the data likelihood is P(X | ...). Maximizing the encoding of x can be trivially achieved by simply having a constant encoding whose probability is 1. - The entire Section 2 can be condensed to one or two paragraph, essentially deriving the discriminative training task in equations (1) and (2). - On the paper organization level, this lengthy section is followed by the related work and then section 4 on "training tasks for encoders". There is again redundancy between section 4 and 2. Consider merging sections 2 and 4 into one Methodology section, where the general task is formulated, the sentence encoding (Bi-LSTM with max-pooling) and binary classifier (the MLP) are defined, and the fake sentence generation is described. This would make a better flow and remove excessive text. 3. Motivation and advantages of the approach:- The approach is motivated by shortcomings of sentence encodings based on language modeling, such as Skip-Thought, which are computationally intensive due to the large output space and the complicated decoding process. This is an appealing motivation, although there have also been simpler methods for sentence representations that work as well as or better than Skip-Thought [1, 2]. - The second motivation is not clear to me, and the claim that "the training text collection should include many instances of sentences that have only minor lexical differences but found in completely different contexts" needs more support, either theoretical or empirical. Why wouldn't a language model be able to distinguish such differences?- The advantages of the binary classification task make sense. The point about forcing the encoder to track both syntax and semantics is interesting. Have you tried to analyze whether this indeed happens? The probing tasks are a good way to evaluate this, but most of them are syntactic, except SOMO and perhaps CoordInv and BShift. Still, more analysis of this point would be good. - One concern with generating fake sentences by swapping words is that it would not apply to languages with free word order. Have you considered how well your approach would work on other languages? 4. Relevant related work: - The fake data generation resembles noise used in denoising auto-encoders. A recent application is in unsupervised neural machine translation [3], but there is relevant prior work (see references in [3]). - The binary classification task resembles that in [1], where they train a classifier to distinguish between the representation of a correct neighbor sentences and incorrect sentences. 5. Ideas for more experiments and analysis:- The results are fairly good by using only 1M sentences. How good would they be with the full corpus? What's the effect of training data size on the method? - Table 4 is providing nice examples showing how the fake sentence task generates better sentences representations. Can this be measured on a larger set of examples in aggregate? Why is t-SNE needed for calculating the neighbor rank? - Proving tasks are very interesting, but the discussion is limited. A more detailed discussion and analysis would be useful. - Consider other techniques for generating fake sentences. Minor comments:==============- Related work: the Skip-Thought decoder is a unidirectional LSTM and not a bidirectional one as mentioned, right? - Related work: more details on supervised approaches would be useful. - Section 4.1: how many fake exampels are generated from every real example? Have you experimented with this? - Section 4.2 mentions 2 hidden layers in the MLP but figure 3 indicates 3 layers. - Is there a reason to use multiple layers without a non-linearity in the MLP? This seems unusual. In terms of expressivity, this is equivalent to using one larger linear layer, although there might be some benefit in optimization. - Table 1 seems unnecessary as there is no discussion of how dataset statistics refer to the results. It's enough to refer to previous work. - What are some results missing in table 2, specifically SKipthought (1M) on COCO datasets? - The paragraph on sentence encoder implementation mentions a "validation set accuracy of 89 for word shuffle". Which validation set is that? How is convergence determined for word drop? - In analyzing sentence lengths, figure 2 shows the fake sentence to be similar to SKip-Thought on short sentences in SST. Do you have any idea why? Also, fake sentence is better than Skip-Thought on all lengths in MR, not just longer sentences, so I'm not sure there's any signal there. - Figure 3: what is the test set for WordShuffle? - The idea to create negative samples focused towards specific phenomena sounds like a good way to goWriting, grammar, etc.:======================- Introduction, paragraph 3, last sentence: start with "The". - Introduction, paragraph 4, first sentences: discriminative training task fake sentence detection -&gt; discriminative training task *of* fake sentence detection- Motivation: an useful -&gt; a useful; we assumes -&gt; we assume; then number -&gt; the number; this much -&gt; this is much- Motivation: do not differ -&gt; do not differ much? - Related work: skip-gram -&gt; skip-gram model; Training Skipthought model -&gt; Training a Skipthought model- Section 4: Prior work use -&gt; Prior work uses/used - Section 4.2: space between "Multi-layer Perceptron" and "(MLP)". This also happens with other acronyms. - Page 6: Our models, however, train -&gt; are trained - Table 3 caption: is bigram in -&gt; is bigram; is co-ordination is -&gt; is-coordination - Page 7: The analysis ... also indicate*s* ... but do*es* not ... - Figure 3 caption: classification/proving task -&gt; tasks - References: fix capitalization in paper titlesReferences==========[1] Logeswaran and Lee, An efficient framework for learning sentence representations[2] Khodak et al., A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors[3] Artetxe et al., Unsupervised Neural Machine Translation[4] Arora et al., A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs In this paper authors present a new Probabilistic Programming Language (PPL) MXFusion. Similarly to the languages for the deep learning (TensorFlow, PyTorch, etc.), this language introduce probabilistic modules that are used as building blocks for complex probabilistic models. Introducing modularity to the probabilistic programming, raises the problem of inference for probabilistic models. Since, we cannot obtain the exact solution on practice we have to resort to approximate inference methods. The approximate inference methods can be either generic, thus, being suitable for many probabilistic models but resulting in poor approximation, or specific, thus, having good approximation quality, but only for specific probabilistic models. Authors propose to address this problem by encapsulating specific inference methods in corresponding probabilistic modules. Doing so, one can perform approximate inference for every module with the best suitable inference technique. Authors demonstrate interface of MXFusion for three well known probabilistic models: Bayesian linear regression, deep kernel learning, Bayesian Gaussian process latent variable model.Approaching the problem of building complex probabilistic models by introducing modular PPL is an important direction of study. But, regarding this paper I have the following concerns.- In my opinion, the structure of the paper can be greatly improved. From general words about modularity and approximate inference authors dive to the very specific cases of probabilistic models. Following such structure, authors dont give a clear answer to the following questions. Why the paradigm of encapsulating inference methods in probabilistic modules is legitimate for constructing complex probabilistic models? What inference methods and probabilistic models can we use as building blocks? Do we need to be aware of specific inference methods that are encapsulated or we can use any blocks in any order as we do in deep learning frameworks?- Novelty of that paper is the new design of PPL. That is an interesting and important question for the community, but maybe ICLR paper is not the best format to present such kind of novelty. - From the specific examples in the paper, legitimacy of such modular structure is clear only for variational inference (that seems to be a common knowledge) and variational approximation of gaussian processes. But the application area of MXFusion remains unclear. Verbatim examples of code for the specific examples doesnt make the difference between MXFusion and other PPLs clear, because it can be treated as encapsulation of the code into some classes, that can be implemented in other languages as well.- Comparison with other frameworks can be improved. In experimental section authors provide comparison with GPy framework in terms of RMSE and log-likelihood for gaussian process with 50 inducing points. As I understood both frameworks use the same inference methods and achieve the same performance, so the experiment can be considered as sanity check for MXFusion. The paper could benefit from comparison between different inference methods and providing benchmarks for inference time.Overall, the paper proposes a new PPL that is an important direction of study, but have several drawbacks and conference paper format is not the best way to present such kind of novelty.Typos:- Page 1, despite the different of DNNs& -&gt; despite the difference of DNNs&?- Page 2, missing reference of the section- Page 2, section 3, ... sightly different form. -&gt; ... slightly different form? **Summary:**This paper proposes some interesting observations for training BWNs. 1: The scaling factors can be removed with batch normalization used. 2: The signs of the weights with large norms are determined and fixed at the early training stage. 3:  The binary weight networks can be further compressed. Moreover, the authors provide some empirical visualizations and results to demonstrate its analysis. However, the paper seems to be incomplete and needs to be further improved. **Pros:**  The observation that the signs of weights with large norms are determined and fixed at the early training stage is interesting. The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWNs, maybe BNNs as well.**Cons:**   1: Extensive descriptions are very confusing. I can only list some of them. 1): The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns. However, it is still not clear to me what the fixed structural patterns are. 2): In Sec. 3, the paper claims to quantize the less frequent kernels to those high frequent kernels to save space and we sort these binary kernels according to their appearance frequency& . However, the paper fails to explain the motivation for exploring the frequency of kernels. Some theoretical explanations are needed. 3):  In Figure 4s caption, what does the certain appears in one certain Conv layer mean?  4):  In Sec. 4.1, the authors propose to apply QBN on model compression.  However, it is not clear to me what the points the paper intends to express. Specifically, the authors claim that we can reduce the number of parameters to represent a binary-kernel by changing the small magnitude weights signs. Then the question comes. How can the number of parameters be reduced via changing signs? Also, the sentence we can compress their parameters to an extremely small number by replacing the whole 3×3 binary-kernel with fewer  is incomplete. 2: Extensive symbols are undefined, which makes me hard to understand the paper properly, especially in Algorithm 1, the main algorithm of the paper. For example, definition of $n$ in calculating $E$;  definition of  where();  definition of $K_m$, are not explained.3:  Extensive technical details are missing, which makes the algorithm difficult to understand. 1):   The authors should at least explain how to generate the power-of-two number of binary kernels in details in Sec. 3.1.  2):   During training, the paper generates $k$-bit (i.e., k is the bitwidth) number of binary kernels. However, during testing, the learnt binary weights should be fixed ($k=1$) and why the Quant Bit in Table 1 can be larger than 1? This makes me very confusing.4:  This paper only binarizes weights while I am wondering whether simultaneously binarizing both weights and activations can have the similar observations. 5:  There is no comparisons with current state-of-the-arts.  Also, in Sec 4.1, the paper claims that the compressed ratio can achieve to 1.5$\times$ with a bearable accuracy drop (less than 3% on Cifar-10) does not stand. The 3% loss on Cifar-10 is significant. 6:  There is no conclusions and future works discussions. 7:  It is widely known that the scaling factors can be absorbed into BN layers. In terms of this, it should not be a contribution of this paper.8: The paper introduces extensive heuristic hyper-parameters. Specifically, it does not give any strategy to automatically determine the optimal quantized bitwidth and threshold $\Delta$ for each layer. 9:  There are lots of grammar and typo issues. For example, the words in equations should use straight format (use \rm in latex). ## Summary of the WorkThe work propose a method which allows us to synthesize meta-RL and meta-IL, by pre-training and conditioning a context-based off-policy meta-RL algorithm on imitation data. Strongly inspired by PEARL (Rakelly, et al 2019) and meta-IL (Yu, et al 2020), this method outperforms previous methods by varying margins on a range of newly-introduced 2D and 3D robotics tasks. The work introduces several new design elements and losses to this family of methods, and the experiments do not make clear which ones are responsible for the increased performance. Additionally, it's not clear the included experiments can fully substantiate the long list of claims provided by the authors, not the least of which is that their method performs zero-shot adaptation to new tasks.## Pros and Cons### Pros* Addresses several important problems in adaptive RL - generalizing to complex tasks and outside-of-distribution tasks - using demonstration data to avoid costly random exploration - fine-tuning policies acquired with meta-RL/meta-IL* Hyperparameters and reward functions are well-documented* The visualizations are clear and helpful* Overall the work is well-organized### Cons* Makes many claims about the method which are difficult to fully substantiate in such a short paper* Treatment of prior work is limited to only a few methods from the past few years, and does not acknowledge many prior works in context-based meta-RL/MTRL. Relationship to the very-similar prior works PEARL and meta-IL is unclear.* Experiments section makes it very difficult to compare presented results to prior work* Lack of ablations make it unclear which (of many) new design decisions are responsible for the method's performance* Experiments don't appear to make a credible simulation of demonstration data which might be encountered in the real world* Claims of applicability to robotics necessitates comparisons with robotics benchmarks (See MetaWorld and/or RLBench).* Presented method is very complex* Experiments lack multiple seeds and any statistical significance tests* Terminology and notation often veers far from previous works and conventions, making it difficult to parse. Sometimes writing is ambiguous or unclear.## Evaluation### Quality3/5The presented work and experiments are high-quality in their motivation, implementation, and usually their presentation. I think the quality of this work suffers when we consider how well it positions itself with respect to prior work (not well), and the level of detail with which it explores and substantiates each of its claims (of which there are many, making it impossible to address any of them fully). The method section is extensive and recapitulates in detail many concepts from RL, variational inference, IL, etc. It can probably be more sparse to make room for a better treatment of prior work and more experiments/analysis of the work's claims.### Clarity2/5This work suffers from significant clarity issues, mostly around use of language (e.g. zero-shot learning vs few-shot learning), non-standard notation and terminology (e.g. "primal inference," "privileged information," etc.) and the use of equations which are not really necessary to demonstrate a point, ### Originality3/5While original in nature (for instance, introducing privileged information, considering fine-tuning and off-distribution tasks, using demonstrations for pre-training, etc.), the work does not make it easy for the reader to divine how its contributions build on significantly-similar previous works which are highly-cited. This does it and the reader a disservice. ### Significance2/5If all claims in the work are well-substantiated, it can be a very significant work, but I don't believe they are substantiated. For instance, the introduction mentions low-dimensional observation spaces as a limitation of current meta-RL/IL work, but the experiments don't seem to contain any use of high-dimensional (e.g. image) observation spaces. It's not clear how the claim of zero-shot learning is supported.### Misc Editorial Comments and Reviewer's Notes#### Claims* Addresses three limitations of current meta-RL/IL - shaped reward functions - constrained low-dimensional action/observation spaces - requires hand-defining low-dimensional observation/action spaces* A hybrid framework which combines the merits of RL and IL  - tasks defined only using demonstrations  - unlike other (meta)-IL algorithms, allows for improvement of the policy after adaptation* Uses only proprioceptive actions from the agent, and implicitly recovers the external environment state* Allows for learning new tasks without requiring any expert knowledge in the human teacher* Achieves "exceptional" adaptation rates and is capable of exploiting demonstrations for efficient exploration* Outperforms other meta-RL and meta-IL baselines* Is capable of zero-shot learning* Is capable of multi-family meta-learning and out-of-family meta-learning through clustering in the latent space* Shows how we can use privileged information (during training) to create an auxiliary loss for training the embedding function, allowing us to recover the "true underlying state"#### Mechanisms* Represents a task by a latent vector, which is the belief state of the task given a demonstration* Meta-training encourages high mutual information between the demonstration data and latent space* After adaptation, the agent can explore and update the latent space#### 1. Introduction* "hand-crafted, shaped reward functions..." nothing in the meta-RL formulation requires shaped reward functions, as opposed to a sparse ones which are easier to craft. Granted, on-policy meta-RL algorithms are challenges by sparse reward settings in a similar fashion that on-policy RL algorithms are challenged by sparse reward settings, but this is a property of RL in general and not just meta-RL. [2] is a meta-RL method which can cope with sparse rewards, and is extensively-cited in this work.* "defining a low-dimensional.." this is not a limitation per se of meta-RL or meta-IL -- there's nothing in their formulation which necessitates meta-RL operating on low-dimensional state as opposed to images, though it is certainly a design challenge. See [1]* "different task families" Perhaps I have misunderstood the authors, but this seem orthogonal to the purpose of the work and of meta-RL. It is not immediately clear what the authors mean by "task families." While there is certainly work on cross-domain transfer in IL and RL, adapting policies to different action and/or observation spaces is not a typical goal of meta-IL and meta-RL algorithms, so it seems strange to level this critique. [see "claims" above]#### 2. Related Work* Meta-learning and meta-RL far predates Wang and Duan. Please see [3,4, etc.]* Modern work on context-based meta-RL and adaptive RL predates PEARL. Please see [5, 6, 7] which all perform variational inference on trajectories to generate a latent context, which can then be used for adaptation#### 3. Method* The proposed approach seems hardly different than PEARL[2] with the following changes. The reviewer may have missed something, but given close relationship between these methods, please make crystal clear for readers the differences between this method and the substantially-similar PEARL method.  - Pre-training uses demonstration data rather than RL episodes  - This work studies what happens if you continue training after adaptation  - Introduces an auxiliary loss which allows conditioning on privileged information* 3.2: "Traditional meta-RL methods leverage RNN-based" - This is hardly true in any universal sense. Previous meta-RL method have used RNNs [8], variational inference [2], autoregressive models (attention)[9], hierarchy [10], exploration policies [11], etc. to implicitly model the latent task space.* 3.3: The included equations don't seem to add much to the paper's story and seem to recapitulate well-known results from RL, variational inference literature, or cited work. * 3.5: The use of a SAC expert trained on full-state versions of the environment is not a faithful simulation of expert data, which will be significantly noisier and lower-entropy than a SAC expert, and also is unlikely to be optimal according to any RL loss. I think that this reduces the IL aspects work to a form of offline RL where the offline data source is a SAC policy, and the results demonstrate that the method can reconstruct the privileged information which was available to the SAC agent. The authors note that they augment the demonstration data with "imperfect demonstrations", but are silent about how this is achieved (and it must be done with great care).#### 4. Experiments* 4.1: Though it is ambiguous from the text, these experiments seem to either present 1 experiment per method, or the average of 3 experiments per method. This is unfortunately not enough data to make a statistically meaningful comparison of the performance, especially considering the small performance differences involved. Please see [12] for a handy guide on how to compare performance. In short, you will likely need 10 seeds for each experiment and should conduct a statistical test to ensure your differences are real. Please include a 95% confidence interval in your plots for the benefit of readers.* 4.1: These experiments are meaningful and helpful, but it's also important to readers that they can verify you have reasonable implementations of the comparison methods. This necessitates providing some results for some of the environments used in Yu, et al and/or Rakelly, et al. How is the reader to know your implementation or hyperparameters are fairly representing the comparison methods?* 4.2: I think the reader would benefit from seeing t-SNE plots from the comparison methods as well. The presented plots look very similar to t-SNE plots generated by plotting samples from a PEARL posterior.* 4.2: It is very unclear what the authors mean by "zero-shot" learning. By my estimation, this method always requires some samples of the target environment to attain the presented performance, making it squarely a few-shot domain.* This work introduces many new design elements on top of PEARL and Yu et al, and it's unclear which of them are responsible for the observed performance. Please include ablations which compare your method's performance without each new design element, to demonstrate the impact of each.[1] https://arxiv.org/pdf/2006.07262.pdf[2] http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf[3] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1796[4] https://www.sciencedirect.com/science/article/abs/pii/S0893608002002289[5] https://openreview.net/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf[6] https://arxiv.org/pdf/1809.10253[7] https://arxiv.org/abs/1806.02813[8] https://arxiv.org/abs/1611.02779[9] https://arxiv.org/abs/1707.03141[10] https://arxiv.org/abs/1710.09767[11] https://arxiv.org/abs/1802.07245[12] https://arxiv.org/abs/1904.06979 In this paper, the authors present a generative-model-based Laplace mechanism. By training the VAE on some dataset, the trained encoder can be used to privatize raw data towards epsilon, delta-LDP. Though the method is novel, the privacy guarantee of the proposed method is not clearly stated and proved. Related experiments are not convincing, either.**Strength**The paper is well written with a clear motivation, explanation of methodology. To my knowledge, I believe the work is useful for the privacy research community. The proposed method is also novel.**Weakness**- The motivation to use the Laplace mechanism is not very clear. At the beginning of Sec. 2, the authors reason the usage by "as it provides strong theoretical privacy guarantees". This is not convincing for readers especially for those who are not familiar with LDP. Since the Laplace mechanism directly comes from the CDP, I would wonder how does the Gaussian mechanism works. How does the Laplace mechanism guarantee privacy better than the Gaussian mechanism? Reference or proof is essential here.- In page 3, the authors briefly mention that the local version of the Laplace mechanism can be epsilon-LDP if the sensitivity is accordingly defined. This really lacks rigorousness. In the following sections, the authors refer to (Dwork and Roth, 2014) for the post-processing theorem. Since the work (Dwork and Roth, 2014) is mainly about CDP, I am not sure how the post-processing theorem can be adopted for LDP. Either reference or clear proof is required.- Meanwhile, there lacks an end-to-end proof of the privacy guarantee of the VAE. I am not sure if the proposed VLM training guarantees privacy. Either, the privacy of encoding is not very clear. Especially, there involves a non-private training on stage 1.- The experiments are run with pretty week baselines. Through this paper, the authors actively use the same conclusion from CDP (Dwork and Roth, 2014). Thus, I suppose the state-of-the-art CDP algorithms should also be applicable to the experimented tasks, e.g, classification. For the specific task, how well is the proposed compared to the SOTA CDP private learning algorithms? For example, (Abadi, et al., 2016), or (Phan, et al. 2017). Especially, (Phan, et al. 2017) also proposed an adaptive Laplace mechanism without depending on pre-training of the mechanism.- In page 4, the DP-Adam mentioned in Stage 2 is not stated or proved in (Abadi et al., 2016). Only DP-SGD was discussed. A strict proof is required for the DP-Adam which intensively re-uses private results to help improve the gradients. Thus, the privacy guarantee is not straightforward.- Seems the VLM training is using a non-DP optimizer at stage 1. Then how the whole training could guarantee privacy on the VLM training set. In experiments, the VLM training set is directly extracted from the private dataset (MNIST). Even though the author experiments with diverse D_1 D_2 distribution for VLM train/test in Sec 4.2, the two datasets are still from the same dataset. In practice, when such a D_2 is private, it is hard to find a D_1 to be non-private. I am afraid this could cause serious privacy leakage. Therefore, I doubt if the experimental results are useful for proving the effectiveness of a private algorithm. More realistic dates should be used.- In Sec 4.1, the authors run the experiments in two steps. First, the VLM is trained with 'a DP encoder using D_1'. I am not clear how the DP encoder comes from. Does the VLM is also trained with DP? The setting has to be clarified.- The experiment comparison seems not fair for baselines. For VLM, there are two datasets for training VLM and encoding classification train data. However, the baseline only has classification training data. The VLM encoder has additional information about the data distribution or the noise (by back-propagation in VLM training). The unfairness in the information could be the core reason for the difference in performance. How does the baseline perform if it is pre-trained and tuned (hyper-parameters) on another dataset?(Phan, et al. 2017). Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning This paper claims to have shown some insights about the filters in a neural network. However, it has little contributions that are justifiable to be published and it missed way too many references.The visualization of filters is hardly any contribution over [1]. The claim that AM is the best visualization tool is a weird statement given that there are many recent references on visualization, such as [2-4], which the authors all missed.The proposed filter pruning is a simplistic approach that bears little technical novelty, and there has been zero comparison against any filter pruning approach/network compression approach, among the cited references and numerous references that the paper didn't cite, e.g. [5-6]. In this form I cannot accept this paper.[1] D Bau, B Zhou, A Khosla, A Oliva, and A Torralba. Network Dissection: Quantifying the Intepretability of Deep Visual Representations. In CVPR 2017.[2] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh Dhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. ICCV 2017[3] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff. Top-down Neural Attention by Excitation Backprop. ECCV 2016[4] Ruth Fong and Andrea Vedaldi. Interpretable Explanations of Black Box Algorithms by Meaningful Perturbation. ICCV 2017[5] Y. Guo, A. Yao and Y. Chen. Dynamic Network Surgery for Efficient DNNs. NIPS 2016[6] T.-J. Yang, Y.-H. Chen, V. Sze. Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning. CVPR 2017 This paper considers detecting anomalies in textures. For this task they use VGG-19 features and two human-inspired features from Portilla &amp; Simoncelli and Schutt &amp; Wichmann.With these features, they train one-class anomaly detectors. One such anomaly detector is a one-class SVM, and they introduce a loss for one-class neural networks.The novelty in this paper comes from the problem setup which I have not seen treated before. The loss function they propose also appears original.However, comparisons are limited. They compare against OC-SVMs, but these are known to be weaker than several types of anomaly detectors [1]. This paper would also do well to ground itself in more recent research on deep anomaly detection [2]. Likewise, the problem setting is limited. In all, experimentation could use more breadth and depth.[1] Andrew F. Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, Weng-Keen Wong. Systematic Construction of Anomaly Detection Benchmarks from Real Data. ODD, 2013.[2] Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. ICLR, 2017. The submission investigates the problem of detecting perceptual anomalies in visual textures.It proposes features from three different models, the Portilla &amp; Simoncelli texture model (PS), the Spatial Vision model by Schuett and Wichmann (SW) and CNN features from the VGG network. From these features it trains two anomaly detectors: one out of the box one-class SVM and a 3 layer neural network. The network is optimised with a loss function that encourages output values for the original texture to be larger than for a white-noise image obtained by shuffling its pixels. At the same time the range of output values for the original texture is encouraged to be small. The performance of the different approaches is evaluated using synthetic anomalies. However no distinction between perceptually striking and perceptually negligible anomalies is made and quantitative results are only reported for all synthetically generated anomalies.Two attempts are made to control if an approach specifically picks up on perceptually striking anomalies.a) detection rate on gaussian noise as a proxy for perceptually negligible anomaliesb) anecdotal evidence from visual inspection.I do not think that either of the two controls is sufficient to make a clear statement about which method is best in detecting perceptually striking anomalies. Therefore my main concern is that the performance evaluation is not suitable to achieve meaningful results.Furthermore the technical depth of the submission appears fairly limited. The main original contribution is the CNN loss that is introduced. However, the loss does not strike me as particularly compelling. It resembles a classifier between textures and white noise samples with the same pixel-wise statistics. I am not sure why this should be particularly suited to detect perceptual anomalies.Finally, showing quantitative results from only two textures does not feel like a very comprehensive analysis.In general  the submission tackles an interesting research topic. However, to show meaningful results I believe that one has to collect psychophysical data for the anomalies of interest to distinguish between anomalies that are perceptually detectable and those that are not.With such a test set one could then start testing hypothesis on which feature representation is most appropriate to model the perceptual results or optimise features directly to match human psychophysical results (similar to the study by Berardino et al. 2017 [1]). In its current form I am not sure what I can learn from the submission both in terms of anomaly detection and feature spaces particularly suited to detect perceptual anomalies in visual textures. [1] Eigen-distortions of hierarchical representationsA Berardino, V Laparra, J Ballé, E SimoncelliAdvances in neural information processing systems, 3530-3539 This paper focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task.Novelty:It is the first time I have seen this novelty detection task. This task could be part of the novelty of the paper. Another novelty comes from the new objective function they introduce.Weakness:1. The motivation of the new objective function is not clear to me. It seems that they first design the objective function and then build the interval-based decision function. There is not much intuition given.2. The experiment lacks of real data. Synthesized anomalies never exists in application. If it is a paper about application, real data is needed.3. The baseline is too simple. CNN could definitely beat SVM in image classification. Also using extracted features could be better than directly performing SVM on pixels.4. Do not see the results of OC-SVM in Table 1 even though they say they beat it.5. Also, I do not see any other reference work for this novelty detection problem. If it is a new problem, a clear definition of the problem is needed. If it is not, more references are needed.The writing of the paper is clear and easy to understand. But based on all the weakness above and lack of novelty, I think the paper should be rejected for now. The paper approaches the bit quantization problem from the perspective of neural architecture search, by treating each possible precision as a different type of neural layer. They estimate the proportion of each layer using a gumbel-softmax reparametrization. Training updates parameters and these proportions alternately. The authors claim that prior work has only dealt with uniform bit precision. This is clearly false e.g. https://arxiv.org/pdf/1807.00942.pdfhttps://arxiv.org/abs/1708.04788https://arxiv.org/pdf/1705.08665.pdfIn particular, https://arxiv.org/pdf/1807.00942.pdf uses the same approach, using gumbel-softmax to estimate the best number of bits. In the least, the authors needs to mention and contrast their approach, e.g. they can handle a budget constraint, but they use a fixed quantization function.There is an inherent strength in this approach that the authors have not fully explored. The most recent key discovery in low precision networks is that the optimal parameters take very different values depending on the precision, ie beyond simple clipping/snapping based on quantization error. The DNAS approach can capture this, because the parameters of different precisions need not be constrained via a fixed quantization/activation function (appendix B). Therefore the following questions become important to understand.1. How are the weights w updated for low precision. I understand that you first sample an architecture but there is no explanation of how the low bit (e.g. 1-bit) weights are updated. Do you update the 32-bit weights, then use the functions in Appendix B to derive the low bit parameters? This is much less interesting than the power of the DNAS idea. Do you directly update them using STE?2. Why is it important to train in an alternating fashion? How did you split the training set in to two for each ? Why not use a single training set?3. Are the "edge probabilities" over different precision in any way the function of the input (image)? It seems your approach is able to distinguish "easy" and "hard" images by increasing the precision of parameters. If so, this should be explained and demonstrated. 4. In Eq (10), it is unusual to take the product of network performance and penalty term for parsimony. This needs to be explained vs. taking a sum of the two terms which has the nice interpretation of being the lagrangian of a constrained optimization problem. Do you treat these as instance level weights? 5. Experiments only show ResNet architecture, whereas prior work showed a broaded set of results. Only TTQ and ADMM is compared, where the most relevant work is https://arxiv.org/pdf/1807.00942.pdf. It is not clear if the good performance comes due to the block connectivity structure with skip connections, combined with the fact that the first and last layers are not quantized. The paper proposes to use an estimate of the 'local' smoothness constructed by taking the difference of the gradients along the previous step. This is a simple idea and has been considered before in literature. The authors seem to take a very simplistic approach to the problem which seems to not work at all in high dimensions. I am reasonable certain that the analysis is incorrect as it is impossible to get linear convergence via SGD or even with GD in general settings. Looking at the proof which is written in a very unreadable way reveals that they make multiple assumptions which holds basically in the case of a quadratic and then further only in one dimension. In which case such a rate with GD is trivial. So the theory is blatantly wrong. Regarding the experiments they also look shaky at best and sometimes they diverge. I believe the paper is much below standard for ICLR. The authors use neural networks to parameterize conditional probability distributions. This is well-known and has been applied in the literature since extensions to generalized linear models beyond their canonical link function in the 70s. Their transformation from real-valued network output to, say, strictly positive concentration parameters in a Dirichlet are worth studying; but they don't analyze this in any detail.In addition, while lacking novelty may be fine in and of itself, the purpose of applying these ideas doesn't have a focused purpose. For example, the authors argue in the abstract this quantifies uncertainty. That's only true if you care about data noise, but the end-result is still point estimation for the parameters with uncalibrated probabilities. In the rest of the paper, they write primarily about simplex-valued outputs (i.e., soft one-hot labels). The paper shows how to model the outputs of neural networks via likelihoods other than commonly used ones. The likelihoods discussed include Beta, Dirichlet and Dirichlet-Multinomial. The paper introduces the gradient computation of these likelihoods and test them in several datasets. This paper lacks novelty and has conceptual mistakes. It is a common practice, in Bayesian learning, to model different types of data with different likelihoods. The examples discussed in this paper are very basis and the gradient computation is standard. I do not see anything new. And the authors misunderstand that if you involve some likelihood in training, you can quantify the uncertainty. It is wrong. Uncertainty should be estimated in the posterior inference framework --- you need to integrate the posterior distribution of the (latent) random variables into the test likelihood to obtain the predictive distribution, from which you can identify the confidence levels. Thats why auto-encoding variational Bayes framework is useful and popular.  What the paper is doing is still the point estimation. Besides, the paper exceeds the 8-page limit for the content. This work proposes a defence based on class-conditional feature distributions to turn deep neural networks into robust classifiers.At present this work lacks even the most rudimentary evidence to support the claims of robustness, and I hence refrain from providing a full review. In brief, model robustness is only tested against adversarials crafted from a standard convolutional neural network (i.e. in a transfer setting, which is vastly different from what the abstract suggests). Unsurprisingly, the vanilla CNN is less robust than the density-based architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla CNN are. No direct attacks - neither gradient-based, score-based or decision-based attacks - have been used to evaluate robustness. Please check [1] for how a thorough robustness evaluation should be performed.[1] Schott et al. Towards the first adversarially robust neural network model on MNIST. The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. The flow makes the paper readable.One main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do.It would be useful to provide references of equation 1 if used from previous text.In the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline.In 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). Missing reference: http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf The privacy definition employed in this work is problematic. The authors claim that "Privacy can be quantified by the difficulty of reconstructing raw data via a generative model". This is not justified sufficiently. Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy. The proposed method is not appropriately compared with the other methods in experiments.  In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct. At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better "privacy". However, the Pareto front of the proposed method is concentrated on a specific point. For example, the proposed method does not achieve high "privacy" as "noisy" does. In this sense, the proposed method is not comparable with "noisy". In my understanding, this concentration occurs because the range of \lambda is inappropriately set. This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range. --Minor:In Eq. 1, the utility is evaluated as the probability Yi=Yi'. What randomness is considered in this probability?In Eq 2, privacy is defined as maxmin of |Ii - Ii'|. Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T. In page 4. "The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder." I could not find any justification for this setting. Why "exactly reversed mode" can be the most powerful adversary? What is an exactly reversed mode?Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously. The resulting model would thus be highly affected by the setting of n and k.  How can you choose k and n? In this paper, the authors introduce a neural network architecture that has three components.First a VAE is used to encode images in to two latent states \hat{y} and \hat{z}, with \hat{z}intended to be class (e.g. face attribute) agnostic. The decoder reconstructs images from \hat{y}and \hat{z} concatenated together. A GAN style discriminator attempts to distinguish the decoded image from the original input image as real or fake, allowing the decoder to produce higher quality decoded images. An auxiliary network A attempts to classify the face attribute yfrom the class agnostic features \hat{z}, with the idea being that the encoder should try to produce \hat{z} vectors from which the class cannot be predicted. An additional classifier is trainedusing a classification loss \hat{L}_{class} on the encoded reconstructed image, the use of which I don't understand.I think additional work on section 2.5 through section 3 would be helpful to improve clarity.As one example, "y" is unnecessarily overloaded: y denotes a specific attribute, \hat{y}denotes a latent vector that is intended to not be class agnostic, \tilde{y} denotes theprediction of an auxiliary network on an intended class-agnostic latent vector \hat{z} ofthe presence of the original attribute y, and \hat{\hat{y}} denotes the non agnostic latentvector achieved by passing the decoded image back through the encoder.This notational complexity is compounded by the fact that a number of steps in the method arenot well motivated in the text, and left to the reader to understand their purpose. For example,the authors state that "we incorporate a classification model into the encoder so that our model mayeasily be used to perform classification tasks." What does this mean? In the diagram (Figure 1),where is this classification model? Why in the GAN loss is there a term that compares thefake loss with the result of classifying a decoded z vector? Is this z \hat{z}, or a latent vectordrawn from a distribution p(z)? If it is the former, how does this term differ from the secondterm in the GAN loss. If it is the latter, then shouldn't it be concatenated with some y in order tobe used as input to the decoder D_{\theta}?Why is it important to extract \hat{\hat{y}} from \hat{x}? In the paper you state that the loss"provides a gradient containing label information to the decoder," but why can't we use the known label yof the original input x to ensure that the encoder and decoder preserve this information if it is used as \hat{y}?Later in the paper, you explicitly state that \hat{\mathcal{L}_{class}} "does not provide any clear benefit."If that is the case, then you should ideally include it neither in the model nor in the paper. If it wasincluded primarily because previous models included it, then I would recommend you introduce its usein a background section on Bao et al., 2017 rather than including it in your model description with anexplanation like "so that our model may easily be used to perform classification tasks."Ultimately, this last point brings us to a good summary of my concerns with the model: the inclusionof too many moving parts, some of which the authors explicitly say later on provide no benefit.Moving on to experimental results, I think this is another area where I have a few concerns. First, inFigure 2, the authors argue that your model is "better for 6 out of 10 attributes" and comparable results for most others. The authors include a gap of 0.1 in the "Gray_hair" category as "better" but label a gap of 0.5in the Black hair category as "comparable." I think results in several of the categories are sufficiently closethat error bars would be necessary to draw actual conclusions. If "better" were to mean "better by 0.5" for example,then the authors method is better on 4 tasks (smiling, blonde hair, heavy makeup, mustache) and worse on 3 (black hair, brown hair, wavy hair).With respect to the actual attribute editing, my main concern here is a lack of comparison to models other than Bao et al., despite the fact that face attribute changing is an exhaustively studied task. A number of papers like Perarnau et al., 2016, Upchurch et al., 2017, Lample et al., 2017 and others study this task from machine learning perspectives, and in some cases can perform photorealistic image attribute editing without complicated machinery on megapixel faceimages. At least the images in Figure 3 and 4 are substantially downsampled from the typical resolution found in the Celeba dataset, suggesting that there was some failure mode on full resolution images. Summary: a method is presented for on-going adaptation to changes in data, task or domain distribution. The method is based on adding, at each timed step, an additional network module transforming the features from the previous to the new representation. Training for the new task/data at time t relies on re-training with all previous data, stored as intermediate features. The method is shown to provide better accuracy than naïve fine tuning, and slightly inferior to plain re-training with all the data.While the method is presented as a solution for life long learning, I think it severely violates at least two demands from a feasible solution: using finite memory and using finite computational capacity (i.e. a life-long learning cannot let memory or computation demands to rise linearly with time). Contrary to this, the method presented induces networks which grow linearly in time (in number of layers, hence computation requirements and inference time), and which use a training set growing indefinitely, keeping (representations of) all the examples ever seen so far. If no restrictions on memory and computation time are given, full retraining can be employed, and indeed it provides better results that the suggested method. In the bottom line, I hence do not see the justification for using this method, either as a life-long learner or in another setting.Pros:+ the method shows that for continuous adaptation certain representations can be kept instead of the original examples Cons:- The method claims to present a life long learning strategy, yet it is not scalable to long time horizon (memory and inference costs rise linearly with time)- Some experiments are not presented well enough to be understood.More detailed comments:Page 3:-Eq. 2 is not clear. It contains a term classification loss and feature_loss which are not defined or explained. While the former is fairly standard, leaving the latter without definition makes this equation incomprehensible. oI later see that eq. 7 includes the details. Therefore eq.2 is redundant. Page 4:-Eq. 5 seems to be flawed, though I think I can understand what it wants to say. Specifically, it states two sets: one of examples (represented by the previous feature extractor) and one of labels (of all the examples seen so far). The two sets are stated without correspondence between examples and labels  which is useless for learning (which requires example-label correspondence). I think the intention was for a set of (example, label) pairs, where the examples are represented using feature extractor of time t-1.-Algorithm 1 seems to be a brute force approach in which the features of all examples from all problems encountered so far are kept (with their corresponding labels). This means keeping an ever growing set of examples, and training repeatedly at each iteration on this set. These are not realistic assumptions for a life-long learner with finite capacity of memory and computation.oFor example, for the experiment reported at page 6, including 25 episodes on MNist, each feature transformer is adding 2 additional FC layers to the network. This leads to a network with &gt;50 FC layers at time step 25  not a reasonable and scalable network for life ling learningPage 6:-The results show that the feature transformer method achieve accuracy close to cumulative re-training, but this is not too surprising, since feature transformer indeed does cumulative re-training: at each time step, it re-trains the classifier (a 2 stage MLP) using all the data at all times steps (i.e. cumulative retraining). The difference from pure cumulative re-training, if I understand correctly, is that the cumulative re-training is done not with the original image representations, but with the intermediate features of time t-1. What do we earn and what do we loose from this? If I understand correctly, we earn that the re-training is faster since only a 2-layer MLP is re-trained instead of the full network. We loose in the respect that the model gorws larger with time, and hence inference becomes prohibitively costly (as the network grows deeper by two layers each time step). Again, I do not think this is a practical or conceptual solution for life long learning.-The experiment reported in figure 3 is not understandable without reading Lopez-Paz et al., 2017 (which I didnt). the experiment setting, the task, the performance measurements  all these are not explained, leaving this result meaningless for a stand-alone read of this paper.-Page 8: it is stated that we only store low dimensional features. However, it is not reported in all experiment exactly what is the dimension of the features stored and if they are of considerably lower dimension than the original images. Specifically for the MNIst experiments it seems that feature stored are of dimension 256, while the original image is of dimension 784  this is lower, but no by an order of magnitude (X10).-The paper is longer than 8 pages. The paper Generative model based on minimizing exact empirical Wasserstein distance' proposesa variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relyingon the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.Comparisons with other variants of Wasserstein GAN is proposed on MNIST.I see little novelty in the paper. The derivation of the primal version of the problem is already given in  Cuturi, M., &amp; Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).Using optimal transport computed on batches rather the on the whole dataset is already used in (amongothers) Genevay, A., Peyré, G., &amp; Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., &amp; Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  Also, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on batches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very well described in the paper (Section 3): https://openreview.net/pdf?id=S1m6h21CbComputing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to beproved and discussed.Finally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).Typos: Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup # Summary of model The paper proposes a mixture model formulation of NMT where the mixing coefficients are uniform and fixed. The authors then proceed to derive a lowerbound on the marginal likelihood p(y|x) = \sum_z p(z)p(y|x,z) > 1/K \max_z p(y|x,z)by picking the component z for which the joint likelihood is maximised. With a uniform p(z) this clearly selects the z for which the conditional p(y|x,z) is maximum. I use strictly greater here because p(z) > 0 and p(y|x,z) > 0 for every z.The loss L(\theta|x,y) for an observation (x,y) is \min_z - \log p(y|z,x; \theta)whose gradient with respect to NN parameters (theta) is \grad_theta \log p(y|z,x; \theta) for the component z that minimises the negative log-conditional and 0 for every other component, thus while this requires K forward passes (to solve \min_z), it only takes 1 backwards pass.# DiscussionI appreciate model-based (as opposed to search-based) attempts to improve diversity for generation tasks such as MT. Latent variable modelling aims at a more explicit account of the generative procedure, namely, the joint distribution, which can potentially disentangle and explain different modes of the marginal. Thus from that point of view, this paper points to an exciting direction. That said, in my view, the assumptions behind the proposed approach are not justifiable and some of the claims are simply not appropriate. Below I try to support this view.A stepping stone of this model is that p(y|x,z) must be "large for only one value of z" (as authors put it), and authors *assume* that will be the case. While the bound in equation (2) holds, whether or not p(y|z,x) turns out to be "large for only one value of z", it will be a very loose bound unless that happens. The key point is that one cannot *assume* it to be the case. One could perhaps *promote* it to be the case, but there's no aspect of the model formulation (or objective) that promotes such behaviour.Backpropagating through whichever component happens to assign the largest likelihood does not guarantee (nor encourages) the other conditionals to *independently* end up going to zero. Given the level of parameter sharing, I'd even consider the possibility that the exact opposite happens. As authors put it themselves "Instead, by sharing parameters, even unpopular experts receive some gradients throughout training."It's true they do, but they are being updated on the basis of the unilateral opinion of the selected component about the likelihood of the data.Note that the true posterior p(z|x,y) is exactly proportional to the likelihood, as the prior is *uniform and fixed*:  p(z|x,y) \propto 1/K p(y|x,z) \propto p(y|x,z)This means that the authors expect the likelihood to do component allocation on its own. That is, the conditionals p(y|x,z=1), ..., p(y|x,z=K) must somehow coordinate themselves in making good use of the latent components. Without any mechanism to promote "competition" (in the parlance of Jacobs et al 1991), I don't see how this can work.Also, the paper claims to model uncertainty, if I take the posterior to fulfil this claim, then I'm just left with a likelihood (again, due to uniform prior). In any case, a notion of uncertainty here would be conditioned on a point estimate of the network's parameters and should thus be worded carefully.# Clarifications1. "we aim to explicitly model uncertainty during training" can you make a case for where that happens in your model?2. "prevents the gating from training well and the latent variable embeddings from specializing" which gating?3. "While they showed improvements due to the regularization effect of the Monte Carlo gradient estimate. I find it strange to talk about the regularisation effect of a gradient estimate, perhaps you can be a bit more precise here? Or perhaps you are referring to some specific component of the objective function whose gradient we are estimating via MC and perhaps that component may have some regularisation effect.4. if you aim to have p(y|x,z) high for a single latent variable at a time, you are implicitly saying that every x has at most (or rather exactly) K translations with non-negligible probability. Is that sensible? # Pros/ConsPros* simple: the approach presented here requires no significant changes to otherwise standard architectures, it instead concentrates in a change of objective and training algorithm.* assessment of variability in translation: this paper proposes to use BLEU and a corpus of multiple references in an interesting (potentially novel) way. Cons* problematic assumptions: e.g. posterior will turn out sparse without any explicit way to promote such behaviour* unrealistic claims: e.g. modelling uncertainty* imprecise use of technical language: some technical terms are not used in their strictly technical sense (e.g. uncertainty, degeneracy), some explanations employ loosely defined jargons (e.g. regularisation effect of the gradient estimate)  This paper presents a feature interpolation strategy for fast semantic segmentation in videos. They first compute features of keyframes, then interpolate intermediate frames based on block-motion vectors (BMV), and finally fuse the interpolated features as input to the prediction network. The experiments show that the model outperforms one recent, closely related work wrt inference time while preserving accuracy.Positive:1. Efficient inference. The strategy cuts inference time on intermediate frames by 53%, while achieves better accuracy and IOU compared to the one recent closely related work.2. The ablation study seems sufficient and well-designed. The paper presents two feature propagation strategies and three feature fusion methods. The experiments compare these different settings, and show that interpolation-BMV is indeed a better feature propagation.Negative:1. Limited novelty. The algorithm is close to the optical-flow based models Shelhamer et al. (2016) and Zhu et al. (2017). The main difference is that the optical-flow is replaced with BMV, which is a byproduct of modern cameras.  2. Insufficient experimental comparison with other baselines. In experiments, the paper compares the proposed model with only one baseline Prop-flow, which is not a sufficient comparison to show that the paper really outperforms the state-of-art model. For example, the authors should also compare with Clockwork convnets for video semantic segmentation.     3. Some technical details are not clear. For example, in section 3.1, the paper mentions that the task network is built by concatenating three components but never clarifies them. Also, in algorithm 2, line 13 shows that F is a function with two entries, but line 8 indicates that F is a feature. This paper mainly focuses the imitation of expert policy as well as compression of expert skills via a latent variable model. Overall, I feel this paper is not quite readable, albeit that the prosed methods are simple and straightforward. As one major contribution of this paper, the authors introduce a first-order approximation to estimate the action of an expert, where perturbations are considered. However, this linear treatment could yield large errors when the residuals in (1) are still large, which is very common in high-dimensional and highly-nonlinear cases. Specifically, the estimation of J could be hard. In addition, just below (1), the authors mention (1) yields a stabilized policy, so what do you mean stabilized?Another crucial issue lies on the treatment of \Delta(s), which is often unknown and hard to modeled, Thus, various optimal controllers are introduced so as to obtain robust controllers. Similarly, in (9) it is also difficult to decide what is suitable perturbation distribution.Overall, the linear treatment in (2) and assumption on \Delta(s) in (5) actually oversimplify the imitation learning problem, which may not be applicable in real robot applications.Others small comments:-Section 2.1 could be moved to supplementary material or appendix, as this part is indeed not a contribution.- in (5), it should be -J_{i}^{*} This paper presents some experiments using random projections instead of embeddings from a 1-of-V encoding.  Experiments on the Penn TreeBank benchmark data set show that in a feed-forward language modeling architecture similar to that of (Bengio, 2003), the random projections substantially reduce the number of parameters of the model while not harming perplexity too much.The paper would need to be improved substantially in order to appear at a conference like ICLR.  First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.  First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.  Second, the paper needs to use more state-of-the-art architectures.  Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.  Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.  Changing the number/sizes of the network layers or using sparse weight matrices (perhaps with sparsity-inducing regularization) would be natural ways to reduce the parameter space.In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.MinorIn the start of Section 3, it is not clear why having the projection be sparse is desired.  Later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier.Equation 6 seems to have an error, the probability should be P(w_t | w_t-1...) instead of P(w_t , w_t-1...) if this is to represent the standard LM objective (the probability of the corpus).Sec 3.3: "all models sare" The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling. The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size. The idea is an interesting one, but this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.More specific comments--The initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).The main problem with the paper is the use of simple baseline setups as the only experimental configuration:o feedforward rather than recurrent network;o use of the Penn Treebank dataset only;o use of a small n for the n-grams.All or at least some of these decisions would need to be relaxed to make a convincing paper.The reasons for the use of the energy-based formulation are not clear to me. Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?Just before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries. But there are some minus ones in the random projection? The PPL expression at the bottom of p.5 doesn't look right. The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.It looks like all the results are given on the test set. Did you not do any tuning on the validation data?The plots in figure 4 are too small. It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission. For example, there are lots of typos such as "instead of trying to probability of a target word". The paper suggests a new way of sampling mini-batches for training deep neural nets. The idea is to first index the samples then select the batches during training in a sequential way. The proposed method is tested on the CIFAR dataset and some improvement on the classification accuracy is reported.I find the idea interesting but feel that much more is needed in order to have a better understanding of how the proposed method works, when it works and when it doesn't work. Some theoretical insight, or at least a more systematic experimental study, is needed to justify the proposed method. Summary:The paper presents a novel architecture for video prediction consisting of a feed-forward path with sparse convolutions and an LSTM generating predictions of chunks of video based on the sequence of input chunks. A feedback path links the LSTMs of the different sparse prediction modules. Experiments in video prediction are performed on moving-MNIST and the KTH action recognition dataset and the model achieves state-of-the-art performance on both. Interestingly, the model is exhibits prediction suppressions effects as have been observed during neurophysiological experiments in the inferotemporal cortex of macaque monkeys. The proposed method exhibits prediction suppression effects also in the lower layers, motivating a neurophysiological experiment in the earlier V1/V2 regions, which yielded an observation similar to the models prediction.Strengths:The performance improvements over competing methods on Moving-MNIST and KTH presented in the experimental section are significant. The analysis seems fairly thorough.Weaknesses and requests for clarification:- The description of the sparse predictive module is difficult to follow, and I am not sure I understood it completely. I find it a bit unintuitive to start the description with the errors, instead of explaining what is computed from beginning to end. The section reads more like a loose description of isolated parts instead of an integrated whole. Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this. Also, not every character in equations 1-5 and the algorithm has been defined. For example, what is L? - The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel. 3D convLSTMs have been previously used in 3d vision, see Choy, C. B., Xu, D., Gwak, J., Chen, K., &amp; Savarese, S. (2016, October). 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision (pp. 628-644). Springer, Cham.The application of 3d convLSTMs to video might be new, but the mentioned paper by Choy et al. (2016) should be cited.- You mention that padding is used for rows and columns. Are you using padding on the temporal axis as well?- The paper seems to be written in a rush, as it contains way too many typos and grammar mistakes, e.g. a hierarchical of (should be a hierarchy of or just hierarchical), feedforwad, Expriment (section 4 heading), achievedbetter, trained monkeys to image pairs, pervious, perserves, processure, sequnence viusal. Many typos could have been caught by a spellcheck! This would improve readability a lot!- The citations are not properly formatted: (1) If the author names are used as part of the sentence, use e.g. Lotter et al. (2016), else (2) If the author names are not part of the sentence, use (Lotter et al., 2016). These two styles are mixed randomly in the current draft. This makes the manuscript, which already contains a lot of language mistakes, difficult to read.- Abbreviations that are used but not introduced: CNN, IT, PSTH, DCNN, LSTM.- The related work section could benefit from referring to some of the related work in neuroscience.- Adding a sentence explaining the intuition behind using SatLU in equation (1) might be helpfulTo summarize my feedback: I think experimental results and analysis are strong, but the presentation is strongly lacking! The description of the approach definitely needs to be improved to make replication of the results easier. It might help to have someone who doesnt know the model already read the description and explain it back to you while revising the draft. I hope I could provide some helpful suggestions. I would recommend the manuscript for acceptance, if the presentation is significantly improved! This paper proposes a network architecture inspired by the primate visual cortex. The architecture includes feedforward, feedback, and local recurrent connections, which together implement a predictive coding scheme. Some versions of the network are shown to outperform the similar PredNet and PredRNN architectures on two video prediction tasks: moving MNIST and KTH human actions. Finally, the authors provide neural data from monkeys and argue that their network shows similarities to the biological data.The paper contains intriguing ideas about the benefits of sparse and predictive coding, and the direct comparison to biological data potentially broadens the impact of the work. However, major claims are unsubstantiated, and accuracy and clarity need to be improved to make the manuscript acceptable.Major concerns:1. The authors claim that their architecture is more efficient because it uses sparse coding of residuals. Implementation details and some quantitative arguments, ideally benchmarks, need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet.2. It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network. Does the PredRNN++ have access to as many current and future frames as the C-C net? Is this a fair comparison? Please provide a clearer description of the different versions of your network and how they relate to the baseline models. That section in particular has many confusing typos (frame-by-chunk, chunk-by-frame abbreviations mixed up).3. In Figure 6, the authors claim that more layers lead to better representations. What does better mean? It is implied that the networks with more layers actually make the different motions more discriminable. Please quantify this. For example, a linear classifier could be trained on the neural activations. Also, how is this related to the rest of the paper? Do the authors claim that this result is unique to the proposed architecture? In that case, please provide a quantitative comparison to the PredNet or PredRNN++.4. In Figure 9, the presentation is highly confusing. Plots (c) to (h) are clearly made to look like the monkey data in (b) (nonlinear x-axes?), but show totally different timescales (training epochs vs. milliseconds). Please explain why it makes sense to compare these timescales. Also, what does it mean for a training epoch to have a negative value? Minor comments:1. I dont understand the tension between hierarchical feature representations and residual representations brought up in Section 2. Do the PredNet and PredRNN++ not contain a hierarchy of representations?2. Figure 1 is not fully annotated and could be clearer. What does the asterisk mean? Why are there multiple arrows between the Ps? What do the small arrows next to the big arrows mean? Please expand the legend. Consider using colors to differentiate between components.3. I dont understand Figure 4c. According to the text, this plot shows effectiveness as a function of time, but the x-axis is labeled Layer Number. What does effectiveness over time mean? What does the y-label mean (SSIM per day?)? What is trunk prediction (not mentioned anywhere in the text)?4. For Figure 9, it is pointed out that activity is expected to be lower for E neurons, but is also lower for R and P. This is interesting and also applies to Figure 8, so it would be good to see Figure 8 split up by E/R/P, too. 5. The word Figure is missing before figure references.6. Please proof-read for typography, punctuation and grammar. Summary:This paper proposes three methods to improve the performance of the low-precision models. Firstly, to reduce the number of training iterations, the authors propose to do quantization on pre-trained models rather than training from scratch. Secondly, the authors propose to use large batches size and proper learning rate annealing with longer training time to reduce the gradient noise introduced in quantization. Experimental results demonstrate the effectiveness of the proposed methods.Contributions:1.The authors hypothesize that noise introduced by quantization is the limiting factor for training low-precision networks and present empirical evidence to support this hypothesis.2.The authors formulate the error as equation (1) and propose two techniques (large batches size and proper learning rate annealing) to minimize the final error.3.The authors conduct a series of experiments to demonstrate the effectiveness of the proposed methods.Cons:1.The novelty of this paper is limited. Firstly, fine-tune the pre-trained model is a well-known method in quantization. Secondly, using large batches size and proper learning rate annealing are more like tricks in hyper-parameter tuning rather than a method. 2.In table 2, the performance of the model in the second row (batch size=400) is worse than the baseline ones (batch size=256). In order to keep the same number of weight updates, the author increases the number of epochs during training, which results in performance improvement. Do large batches size really contribute to performance improvement? Whether the performance gain is due to the large batches size or more sampling data?3.The authors claim that large batches size can reduce the gradient noise introduced by quantization. It would be better to show the introduced noise with different batch sizes in figure 1. 4.This paper is not the first time for ResNet-50 with 4-bit quantization to outperform the full-precision network. EL-Net[1] has trained a 4-bit precision network, which leads to no performance degradation in comparison with its full precision counterpart.5.The title in experiments part is too long and confusing. It will be better to keep the short and meaningful title.References[1] Zhuang B, Shen C, Tan M, et al. Towards Effective Low-bitwidth Convolutional Neural Networks[J]. 2017. The paper W2GAN describes a method for training GAN and computing an optimal transport (OT) mapbetween distributions. As far as I can tell, it is difficult to identify the original contributionsof the paper. Most results are known from the OT community. The differences with the work of Seguy, 2018is also not obvious. I encourage the authors to establish more clearly the differences of their workwith this last reference. Most of the theoretical considerations of Section 3 is either based on unrealistic assumptions (case 1) or make vague assumptions 'if we ignore the possibly significant effect ...'that seem unjustified so far. Experimental results do not show evidences of superiority wrt. existing works.  All in all I would recommend the authors to better focus on the original contribution of their works wrt.  state-of-the-art and explain why the theoretical analysis on convergence following a geodesic path in a Wasserstein space is valuable from a practical view. Finally, I did not understand the final claim of the Abstract : 'Perhaps surprisingly, we also provide empirical evidence that other GANs also approximately followingthe Optimal Transport.'. What are those empirical evidences ? It seems that this claim is not supported somewhere else in the paper.Minor remarks: - regarding the penalization in eq. (5), the expectation is not for all x and y \in R^2, but for x drawn from \mu and y from \nu.   Same for L_2 regularization - Proposition 1 is mainly due to BrenierBrenier, Y. (1991). Polar factorization and monotone rearrangement of vectorvalued functions. Communications on pure and applied mathematics, 44(4), 375-417. - from Eq (7), you should give precisely over what the expectations are taken. - Eq (10) : how do you inverse sup and inf ?  - when comparing to Seguy 2018, are you using an entropic or a L_2 regularization ? How do you set the regularization strength ? - where is Figure 2.a described in section 4.2 ? Related works : - what is reference (Alexandre, 2018) ?   - regarding applications of OT to domain adaptation, there are several references on the subject.    See for instance Courty, N., Flamary, R., Tuia, D., &amp; Rakotomamonjy, A. (2017). Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9), 1853-1865.or Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., &amp; Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV for a deep variant. - Reference Seguy 2017 and 2018 are the same and should be fused. The corresponding paper   was published at ICLR 2018   Regarding this last reference, the claim 'As far as we know, it is the first demonstration of a GAN achieving reasonable generative modeling results and an approximation of the optimal transport map between two continuous distributions.' should maybe be lowered ? The paper proposes a way to use conditional variational autoencoder setups to design materials conditioned on properties (labels). The problem is of some significance in the materials design space using generative models, using machinery from neural network modeling - RNNs, VAEs, GANs and optimization. The writing is clear, and material is readable. However, I note the following issues with the content:1. Problem motivation not very clearI felt that the paper does not provide good enough context into the problem and had to piece it together from related work cited (e.g. [1], [2]). In particular, the work seems to be quite related to the work [1] by Kang and Cho, where they use a graphical model fashioned as a VAE, with the dependencies being the label or property y, the latent variable z and the molecule x - i.e. ELBO(log p(x, y)) in equation (2) of the paper. The paper's claim is that they address problems with the vanilla conditional VAE setup. However, it seemed difficult to figure out what exactly the paper was trying to fix in the original VAE setup. They mention "posterior collapse", although there does not seem to be much evidence given (please clarify) to show these deficiencies in the setup - I assume that we should look at section 5.2, Table 1, but no mention of posterior collapse is made in these likelihood numbers. I would also like to quote the following lines from the beginning of section 4. It is unclear again what is meant by saying that the decoder is sensitive to the encoder results and the accuracy might vary ..."Since the latent variable z generated from the encoder is inputted to the decoder to calculate Ltotal,the encoder and decoder are closely related. If the encoder generates non-informative or highlyvolatile z, then the decoder is difficult to reconstruct x from z. In this case, it may not be optimal toupdate the encoder and decoder simultaneously. The decoder is very sensitive to the encoder results,z, and the accuracy of the decoder training may vary according to the encoder results. In other words,the result of the encoder can be optimized in advance in direction in which the decoder trains well.That is, better encoder and better latent variable can make the decoder even better."2. The optimization process: Aside from the issues regarding the paper's motivation, the optimization procedure adopted does not seem to be very clear and could perhaps need clarification. The paper mentions a 'two step' optimization process, of first updating the encoder (keeping decoder fixed) and then updating the decoder. This is strongly suggestive of the wake-sleep algorithm by Hinton et al [2], but if that is so, the exposition would need considerable elaboration with equations, optimization objectives, and clarification on how the optimization machinery used solves the actual problems at hand (which I also had some difficulty following, as explained in the previous point). In other words, the connection between the technique adopted and the problem solved is not clear. I think the paper needs additional work in explaining the motivation of the problem with examples on why the original VAE setup is deficient and what the proposed fixes do to solve the problem at hand. [1] Kang and Cho: https://arxiv.org/pdf/1805.00108.pdf[2] Wake-sleep algorithm https://www.cs.toronto.edu/~hinton/csc2535/readings/ws.pdf **Summary of contributions:**This paper propose a new regularization for training the generator in GANs. They argue that when the discriminator is not optimal this encourages the generator to stay in region where the discriminator is close to optimal. They then propose an heuristic to adaptively control the coefficient of the regularization term. Finally they show empirically that the proposed regularization can indeed improve the generator performance in several setting.**Major Concern:**I have a big concern about the contribution of this work, the proposed regularization eq 8 is exactly the regularization also proposed in [[1] Gradient descent GAN optimization is locally stable (NeurIPS 2017)](http://papers.nips.cc/paper/7142-gradient-descent-gan-optimization-is-locally-stable) see eq 4.This related work is not even mentioned in this paper, also while the perspective is a bit different here, I find the theoretical motivation more rigorous in [1].**Clarity:**The paper is quite clear and well written.**Significance:**The experiments gives a good overview of the performance of the proposed method, however the results don't always show a major advantage of the proposed regularization for example on CIFAR there doesn't seem to be any benefit. Also this should be compared to other regularization technique for the generator for example Jacobian Clamping proposed in [Is Generator Conditioning Causally Related to GAN Performance? (ICML 2018)](http://proceedings.mlr.press/v80/odena18a.html).It would also be interesting to have a plot showing the evolution of the adaptative regularization coefficient along training.**Other comments:**There is actually a cheap and unbiased estimator for the AdvAs loss by using this property: $||\mathbb{E}[X]||^2 = \mathbb{E}[X_i^TX_j]$ where $X_i$ and $X_j$ are two independent random variables sampled from the same distribution as X. For an example on such an unbiased estimator in a context related to yours see Appendix C of [Stochastic Hamiltonian Gradient Methods for Smooth Games  (ICML 2020)](https://proceedings.icml.cc/static/paper_files/icml/2020/6356-Paper.pdf). # SummaryThe papers studies the problem of robust machine learning, where the labels of the a fraction of samples are arbitrarily corrupted. The paper proposes an algorithm to tackle this problem and evaluates it on a standard datasets.  # PositivesThe paper studies an important problem prevalent in modern machine learning, and proposes two algorithms to solve these problems. The experiments suggest that the proposed algorithm is better than the baselines.# Negatives The paper does not cite highly relevant papers, overclaims its results, and the theoretical results in this paper are immediate. Moreover, the paper is not well-written. More details are given below:+ Page 1: "Instead of developing an accurate criterion for detection corrupted samples, we adopt a novel perspective and focus on limiting the collective impact of corrupted samples during the learning process through robust mean estimation of gradients."+ This is not a novel perspective and has been known in robust machine learning community for some time [1,2]. These papers have the same underlying idea, but they are not discussed in this paper. [1] is only briefly mentioned in Remark 2, but the comparison is not fair. The results in [1] hold under fairly general conditions, where the results in this paper require the gradient to be uniformly bounded, which makes the problem significantly simple.+ Theorem 2 is a trivial result, well-known in field.  Moreover, the way it is presented is misleading and confusing. The error would depend on the quantile of  norms in G, which has been hidden under the O(.) notation. The proof is also missing from the paper. + Assumption 1, i.e., Lipschitz continuity of the loss function is very restrictive, which is not satisfied by popular choices of loss function. This assumption trivializes the problem and restricts its applicability.+ In the same vein, Theorem 3 assumes unrealistic assumptions. The assumption that $||W||_{op} \leq C$ is very restrictive and does not hold for usual learning tasks.This assumption in a sense is restricting that the covariates x in $R^d$ have bounded norms, whereas the norm of a typical vector in $R^d$ increases as $\sqrt{d}$.# ScoreI propose to reject this paper. Prior work ([1,2]) has studied this problem in a much greater generality, which are not discussed in this work. The assumptions in the present work are severely restrictive.## Other major comments:+ Robust linear regression, with arbitrary corruptions in responses, has been extensively studied in the literature but they have not been cited.  For example, see [3,4]. In particular, the least trimmed squares is an algorithm that removes outliers based on loss values, and comes with a theoretical guarantee via an alternating minimization algorithm [3,4].+ Theorem 1 is a folklore, and this should be reflected in main text. Currently, this information is only given in Appendix.+ The paper is not well written:  1.Proof of Theorem 2 is missing. 2. $O(.)$ notation hides the dependence on the important quantity in the papers. 3. Important notations have not been defined in the paper. 4. Abbreviations should not be used, for example, Thm., Algo., Asm., etc. 5. There are numerous typos and grammatical errors. For example, "has a remarkably impact". ## Relevant papers1. Diakonikolas, I., G. Kamath, D. M. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A Robust Meta-Algorithm for Stochastic Optimization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 97:15961606. Proceedings of Machine Learning Research. PMLR, 2019. http://proceedings.mlr.press/v97/diakonikolas19a.html.2. Prasad, A., A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust Estimation via Robust Gradient Estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82, no. 3 (July 2020): 60127. https://doi.org/10.1111/rssb.12364.3. Bhatia, K., P. Jain, P. Kamalaruban, and P. Kar. Consistent Robust Regression. In Advances in Neural Information Processing Systems 30, NeurIPS 2017, 21102119, 2017. http://papers.nips.cc/paper/6806-consistent-robust-regression.4. Bhatia, K., P. Jain, and P. Kar. Robust Regression via Hard Thresholding. In Advances in Neural Information Processing Systems 28, NeurIPS 2015, 721729, 2015. http://papers.nips.cc/paper/6010-robust-regression-via-hard-thresholding. The paper aims at a better understanding of the positive impacts of Batch Normalisation (BN) on network generalisation (mainly) and  convergence of learning. First, the authors propose a novel interpretation of the BN re-parametrisation. They show that an affine transform of the variables with their local variance (scale) and mean (shift) can be interpreted as a decomposition of the gradient of the objective function into a regressor assuming that the gradient is parallel to the variables (up to a shift) and the residual part which is the gradient w.r.t. to the new variables. In the second part of the paper, authors review various normalisation proposals (differing mainly in the subset of variables over which the normalisation statistics is computed) as well as the known empirical findings about the dependence of BN on the batch size. The paper presents an experiment that combines two normalisation variants. A further experiment strives at regularising BN for small batch sizes.Unfortunately, it remains unclear what questions precisely the authors answer in the second part of the paper and, what is more important, how they are related to the novel interpretation of BN presented in the first part. This interpretation holds for any function and can be possibly seen as a gradient pre-conditioning. However, the authors do not "extend" it towards the gradients w.r.t. the network parameters and do not consider the specifics of the learning objectives (a sum of functions, each one depending on one training example only). The main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network. The first one normalises using the statistics over channel and spatial dimensions, whereas the second one uses the statics over the batch and spatial dimensions. The improvements are rather marginal, but, what is more important, the authors do not explain how and why this proposal follows from their new interpretation of BN.Overall, in my view, this paper is premature and not appropriate for publishing at ICLR in its present form. I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\lambda^2 + n - 1) (\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. Conversely, its possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldnt necessarily result in good generalization, which seems to agree with the experimental observation. Now given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC?The experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper. The contributions of this paper are in the domain of policy search, where the authors combine evolutionary and gradient-based methods. Particularly, they propose a combination approach based on cross-entropy method (CEM) and TD3 as an alternative to existing combinations using either a standard evolutionary algorithm or a goal exploration process in tandem with the DDPG algorithm. Then, they show that CEM-RL has several advantages compared to its competitors and provides a satisfactory trade-off between performance and sample efficiency.The authors evaluate the resulting algorithm, CEM-RL, using a set of benchmarks well established in deep RL, and they show that CEM-RL benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.  It is a pity to see that the authors provide acronyms without explicitly explaining them such as DDPG and TD3, and this right from the abstract.The parer is  in general interesting, however the clarity of the paper is hindered  by the existence of several typos, and the writing in certain passages can be improved. Example of typos include  an surrogate gradient, "an hybrid algorithm,  most fit individuals are used  and so on& In the related work the authors present the connection between their work and contribution to the state of the art in a detailed manner.  Similarly, in section 3 the authors provide an extensive background allowing to understand their proposed method.In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.The proposed method is clearly explained and seems convincing. However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate&) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark. 2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark The paper presents an approach for an approach to addressing multi-goal reinforcement learning, based on what they call "one-step path rewards" as an alternative to the use of goal conditioned value function. The idea builds on an extension of a prior work on FWRL. The paper presents empirical comparison of the proposed method with two baselines, FWRL and HER. The experimental results are mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method. The idea of the proposed method is relatively simple, and is not theoretically justified. Based on these observations, the paper falls short of the conference standard. A method for activity recognition in videos is presented, which uses spatial soft attention combined with temporal soft attention. In a nutshell, a pixelwise mask is output and elementwise combined with feature maps for spatial attention, and temporal attention is a distribution over frames. The method is tested on several datasets.My biggest concern with the paper is novelty, which is rather low. Attention models are one of the most highly impactful discoveries in deep learning, which have been widely and extensively studied in computer vision, and also in activity recognition. Spatial and temporal attention mechanisms are now widely used by the community. I am not sure to see the exact novelty of the proposed, it seems to be very classic: soft attention over feature maps and frames is not new. Using attention distributions for localization has also been shown in the past.This also shows in the related works section, which contains only 3 references for spatial attention and only 2 references for temporal attention out of a vast body of known work.The unimodality prior (implemented as log concave prior) is interesting, but uni-modality is a very strong assumption. While it could be argued that spurior attention should be avoided, unimodality is much less clear. For this reason, the prior should be compared with even simpler priors, like total variation over time (similar to what has been done over space).The ablation study in the experimental section shows, that the different mechanisms only marginally contribute to the performance of the method: +0.7p on HMDB51, slightly more on UCF101. Similarly, the different loss functions only very marginally contribute to the performance.The method is only compared to Sharma 2015 on these datasets, which starts to be dated and is not state of the art anymore. Activity recognition has recently very much benefitted from optimization of convolutional backbones, like I3D and variants.The LSTM equations at the end of page are unnecessary because widely known. This paper introduces a purely synthetic dataset for crystallography diffraction patterns. For this very specific application domain, this might be a very welcome approach, however, I feel the paper is not strong enough for ICLR for two reasons:1. The scope is too narrow for ICLR. Only a limited readership will be interested in this specific problem. Since the contribution is mainly on the dataset level and not on the methodological level, I suggest submitting such an article in venues more focused on the application domain. I can see no contribution which is general enough to be interesting for the broader readership. A new dataset might be of interest if it is a really challenging one where current approaches cannot yield high performance levels while it would be easy for domain experts to recognize.2. The experiments are only on synthetic data. I agree that synthetic data in general can be very useful, if generated correctly (this has been shown in many works). For a substantial article contribution, one should, however, in general add much more exhaustive experiments. Besides analyzing the behavior on synthetic data, one should perform tests on real data and see the influence of, e.g., pre-training on this synthetic dataset. Furthermore, comparison to pre-training on other datasets should be performed. This paper proposed to improve the system resource efficiency for super resolution networks. First, I am afraid all the techniques considered in this paper have been investigated in previous works. Thus no new idea is proposed in this work. Also, it is also not clear why these improvement is particularly suitable for the task of super resolution. In my viewpoint, these techniques actually can be used to improve a variety of network architectures in both high-level and low-level vision tasks.Second, the experimental results are also weak. As this work is aiming to address the super resolution tasks, at least visual comparisons between the proposed methods and other state-of-the-art approaches should be included in the experimental part. But unfortunately, no such qualitative results are presented in the manuscript. Finally, the presentation of the paper should also be carefully proofread and revised. This submission proposes a method for learning to follow instructions by splitting the policy into two stages: human instructions to robot-interpretable goals and goals to actions. The authors claim to achieve better data efficiency, adaptability, and generalization as compared to the baselines.Here are some comments/questions:- One of the biggest limitations of the proposed method is that it can only work for one-to-one or many-to-one mapping of instructions to goals. As I understand (please correct me if I am wrong), the method can not work for contextual instructions where the goal depends on the environment and the same instruction can map to different goals, such as 'Go to the largest/farthest object'.- Another limitation of the method is that it requires a set of goals G, which is not trivial to obtain especially in partially observable environments such as embodied navigation in 3D space.- The experimental setup is unclear and several crucial details are missing:- "An instruction for approaching one of the five targets in the arena is generated and passed to the agent at first." -&gt; how is the instruction generated?- There's no example of the environment or the instruction in the submission- "Within the instruction become approaching more than one targets, one of two added targets is selected as internal targets pair with one of the remaining targets." I do not understand this sentence. How are the targets generated in the trajectory-oriented task? How are the instructions generated in this task?- Experimental results are not convincing:- The introduction motivates the need for understanding human instructions and the abstract says 'Given a human instruction', but I believe experiments do not have any human instructions.- All the environments seem to be fully-observable, it is not clear whether the method would work in partially-observable environments.- Only vanilla PPO and BC cloning are used as baselines. There are several competing methods for following instructions which the authors cite such as Hermann et al. 2017, Chaplot et al. 2017, Misra et al. 2017, etc. Why weren't any of these approaches used as a baseline?- The submission requires proof-reading, there are several typos in the manuscript (some are listed below), some of them make it very difficult to understand the setting.- Typos:- Sec 3.1 on Pg 4 mentions 'CEM' multiple times, it's not defined until 3.3.2 on Pg 6.- Pg 3 Theses sets -&gt; These sets- Pg 7 where the Reacher pointing at -&gt; where the Reacher is pointing at- Pg 7 What reacher observes the word is its fingertips position, coordinates in two dimension. -&gt; something is wrong in this sentence.- Pg 7 Then comes to the trajectory-oriented task, there are only a few differences from above -&gt; something is wrong in this sentence.- Pg 7 Within the instruction become approaching more than one targets -&gt; something is wrong here This paper proposed to tackle a large-scale fine-grained object classification problem by approximated CRF. The main motivation is to exploit the spatial conference of object labels to reduce noises in the instance-wise prediction. To this end, the task is formulated by sequential inference problem using CRF. To speed up training, several techniques are applied such as factorized pairwise-potential and approximation of CRF objective.  Although the paper presented a reasonable idea for their particular problem (i.e. classification of products in the store display), the significance of the work is quite limited as the same idea is not generally applicable to other settings (e.g. there is no strong spatial correlation of labels in general images). Also, the performance improvement over the instance object classification is not significant as shown in Figure 5 (Unary vs. Approximate factorized). Due to the limited significance and impact of the work, this reviewer suggests a rejection of this paper. This paper tackles the problem of estimating pairwise potentials when the number of labels is large. Two modifications are proposed: one is to factorize the matrix for pairwise potentials, and the other is to approximate the log likelihood objective with the MEMM objective.The problem and the proposed approach are well motivated. It is particularly useful to draw the connections between MEMM and piecewise-pseudolikelihood.The major weakness of the paper is whether the approximations are necessary. It is hard to see why approximating the log likelihood with MEMM is necessary, because inference and computing the gradients of the log likelihood have the same computational complexity. So the authors could have trained the model with the log likelihood.Regardless, it is still valuable to compare MEMM and log likelihood for training CRFs. However, the authors fail to show how well MEMM approximates the log likelihood. For example, the authors can compare the solutions when optimizing with the gradients of log likelihood and the with the gradients of MEMM. It is especially important to compute the training log likelihood for the two solutions, as it tells us how well MEMM approximates the log likelihood. This is also true for the low-rank approximation of the pairwise potentials. The authors fail to compare the case with low-rank approximation and the case without. It is important to evaluate the training error first with both methods as they share the same objective. This type of comparison should be apply to batch normalization as well.Approximating the pairwise potentials with matrix factorization is also not novel.  See the list below. (The list is by no means exhaustive. Please see the citations therein.)Dense and low-rank Gaussian CRFs using deep embeddingsChandra et al., ICCV 2017Efficient SDP inference for fully-connected CRFs based on low-rank decompositionWang et al., CVPR 2015Neural CRF parsingDurrett and Klein, ACL 2015Finally, some of the claims made in the paper (listed below) should be more careful.p.4the likelihood function, therefore, is log-linear and concave.--&gt; concave in what?the scoring function is still concave, ...--&gt; concave in what?the objective function is no longer linear or concave with respect to the network parameters, ...--&gt; what are the network parameters?but deep learning training techniques have been shown to yield good results ...--&gt; this argument is weak. the key is point out that SGD is used, plus SGD has been shown to work well on many matrix factorization problems. see the paper below.Online learning for matrix factorization and sparse codingMairal et al., JMLR 2010p.5the test time inference uses a global normalization ... avoids the label bias problem.--&gt; the partition function is not even computed when using Viterbi. I'm also not sure how this avoids the label bias problem.whitening the inputs to each layer may also prevent converging into poor local optima.--&gt; this is a hand-wavy claim. it would be best if the authors can provide citations to the claim. The authors propose a method to improve sample diversity of GANs. They introduce multiple discriminators, each aims to not only compare real and fake examples but also compare different "micro-batch" of examples. The diversity is controlled by a hyperparameter \alpha, and some strategies to set \alpha are studied empirically. I don't fully understand how the proposed objective can promote diversity. Under what situation will the discriminator fail to discriminate different fake samples? Many questions are unanswered in the paper. 1. What is the equilibrium of the proposed objective? Does the proposed objective has an equilibrium?2. How to choose the number of discriminators?3. How does the proposed approach compare to other papers trying to prevent mode collapse?The experiment Table 2 and 3 doesn't look convincing to me. The inception score and FID of the proposed approach clearly lack behind state-of-the-art approaches. I don't see any evidence showing that FID of microGAN can be better than, for example, WGAN. The paper proposes a multi-discriminator based extension to GAN training. Specifically, it proposes to split a minibatch of samples into further smaller minibatches (microbatches) and train different discriminators on each. The authors state that "since each D is trained with different fake and real samples, we encourage them to focus on different data properties". This seems incorrect. Random samples drawn from a distribution do not change various statistics of the distribution in expectation (such as means). It only introduces differences due to noise in the sampling process and this noise is not correlated across training iterations or consistent within a microbatch. Without a meaningful/consistent change in the distributions between microbatches there should be no different data properties for the various discriminators to focus in on. As a consequence, a discriminator which evaluates samples independently should not be able to perform this task. Using batchnorm in the discriminator introduces some batch level interactions via the mean and variance statistics but this appears to be serendipity that the authors do not highlight explicitly. As a sanity check - given a fixed generator - if you continue to train the discriminators on randomly drawn samples from this generator distribution does the microbatch discrimination objective continue to make progress and converge to a minimum? What happens if you remove batchnorm so that the samples are processed independently? Is there an additional detail to your paper/method that I missed or misunderstood that addresses the issue raised here? Can you better articulate your intuition on how randomly assigning data to different microbatches results in different data properties? Does the discriminator make use of batch level statistics in some more advanced way beyond just batchnorm such as the minibatch features in Improved GAN (Salimans 2016)? A baseline of always having alpha set to zero in order to tease out the potential improvements of the proposed approach from the potential benefits of having multiple discriminators would increase confidence in the approach.Pros: + The proposed IntraFID is interesting but is missing two baselines (IntraFID for two batches of real data and IntraFID for two batches of a baseline GAN without the proposed technique) which would help calibrate and contextualize the newly introduced metric.Cons:- The paper seems to have a flaw which calls into question whether it is well motivated (see main text).- The paper does not have any direct/controlled comparisons with other methods that utilize multiple discriminators or batch based discrimination.- The paper mis-states the Inception Score of Improved GAN. The best result from the Improved GAN paper achieves 6.86 in an unlabeled setting (see -L+HA in the ablation study in Table 3) but is listed as 4.36. - The paper misses relevant literature - CatGAN (Springenberg 2015) which trains a discriminator to minimize entropy over a categorical distribution assigned to the generator's samples while the generator is trained to maximize entropy of discriminator in this space.  It also misses PACGan (Lin 2017) which also augments a discriminator to look at multiple samples to improve diversity. This paper addresses adversarial detection through the absolute-value difference between the two logit vector values of a DNN binary classifier, with one class associated with normal data and the other with adversarial data. Assignment of examples to an "adversarial" class is problematic in that adversarial examples are typically generated in regions for which training data is very sparse. To cope with this, the authors propose use of the Background Check calibration techniques recently proposed by Perello-Nieto et al. (ICDM 2016).  Here, BC is used to estimate probabilities in a sparse "background" class (here, the adversarial class)  through a form of interpolation based on foreground and background densities. The underlying distributional assumption used for estimating foreground densities was that of a gamma function.  Rather than using BC's affine bias for estimating background density from the foreground density, the authors adapt it by raise the weighting for the "adversarial" decision to the fifth power. Unfortunately, no justification for this choice is given, other than to say that this was done with "domain knowledge informing the use of a power value". In their experimentation, the authors generate from CIFAR-10 data four kinds of adversarial attacks: noise alone, images with moderate noise, clear images with noticeable noise, and clear images with imperceptible noise. For a variety of attacks, they showed (in Table 2) differences between the average recall for normal examples vs the average recall for normal plus adversarial images. However, without knowing the proportion of adversarial examples used in testing, the significance of the reported differences cannot be judged. They also list the true positive rates of adversarial examples, which showed much variation from experiment to experiment (trending to rather poor performance for attacks with imperceptible noise). Again, the significance of the results cannot be judged without knowing the false negative rate, true negative rate, etc. Moreover, the results are reported without clearly identifying two of the attacks used ("Mom." is presumably Dong et al.'s attack using momentum in gradient descent, and Miyato et al.'s "VAT" is not properly introduced in the related work). Crucially, no evaluation of their method is made with respect to other adversarial detection strategies.Pros:* Overall, the calibration approach is well motivated, and likely to be of some benefit.* The paper is generally readable and understandable. The issues behind calibration and the use of BC are well explained.Cons:* The result is a simple and straightforward application of an existing technique - not greatly original.* Many design choices in the model (particularly the raising of one of the weights to a seemingly-arbitrary power) are mysterious. No indication is given as to other alternatives or how they might perform.* The experimental results are inadequate to judge the impact of the proposed calibration approach.* There is no comparison against other detection methods. I like the idea of finding the max-reward policy, but the entire framework seems to stand on a shaky ground.The main problem is with Equation (3), where the second equality is wrong. It is easy to come up with a counter-example. Consider an MDP with 4 states plus a zero-reward absorbing state. Assume a fixed deterministic policy and $\gamma$ close to 1. Let $s_1$ be the start state, with deterministic reward $r(s_1)=1$. With probability 1 $s_1$ goes to $s_2$. Let $r(s_2)=0$, and with equal probability it transitions to either $s_3$ or $s_4$. The immediate reward $r(s_3)=2$ and $r(s_4)=0$, after that it always goes to the absorbing state with 0 rewards thereafter. Since there can be only 2 trajectories, one with max-reward 2 and the other with max-reward 1, we have that $Q(s_1)=1.5$, but it is easy to see that $Q(s_2)=1$, and $\max(r_1,Q(s_2))=1\neq 1.5$. This paper proposes Expectigrad, which is a new optimizer for nonconvex optimization. The main idea is to consider arithmetic mean of squared gradients instead of exponential moving average and to use a normalization factor that takes into account the number of nonzeros observed during the run of the algorithm, for each component. The algorithm is analyzed for solving smooth nonconvex optimization and its practical performance is investigated.In terms of the strengths of the paper, I found the idea of using normalization $n_t$ depending on the sparsity interesting. However, the idea of using $s_t$, which is given in eq. (1) seems to be not novel. In particular, this step is already used in Adagrad based methods. For example, AdamNC method in Reddi et al., 2019 already shows that with such an update for second moment estimate, one can obtain convergence. This algorithm is also analyzed by Chen et al., 2018 in the nonconvex setting, which seems to be missed by the authors.In terms of the analysis, the authors make the simplifying assumptions of $\beta=0$ in page 5 and ignoring sparsity in page 16, which basically converts the algorithm to Adagrad. Can the authors clarify this connection and let me know if I am missing something? Therefore, I am not sure if the analysis brings new results that are not known in the literature. Such simplifications take away the novel parts of the algorithm and makes it impossible for the authors to show the effect of their idea in theory. Therefore the potential benefit of the method becomes purely experimental. For example, it is known in the literature that the extension of analyses for including nonzero (especially constant) $\beta$ is not always easy and one needs to be careful [1].Moreover, the author's usage of the term "regret" is not correct. The authors use the expected gradient norm as the optimality measure, which is the standard one for nonconvex optimization. Regret is used for online optimization, which is more general than stochastic optimization which is considered in this paper. Next, the authors analyze the algorithm in Thm 2 with increasing mini-batch sizes. However, Zaheer et al., 2018 shows that with increasing (or depending on the number of iterations $T$) mini-batch sizes, Adam (which is non-convergent in general) also works. Therefore,  I think this setting is not suitable since increasing mini-batch sizes shadow most of the theoretical challenges. It is better to analyze the standard setting without increasing mini-batch sizes.In practice, the comparisons with Adagrad and AdamNC are omitted. Moreover, it seems that the improvement of Expectigrad compared to SOTA is not significant.Therefore, in terms of algorithmic ideas and the novelties in theoretical analysis, this paper does not meet the bar for ICLR. In particular, the simplifying assumptions that the authors make for the algorithm ($\beta=0$ and no sparsity) essentially takes away the interesting parts of the proposed algorithm, and making the theoretical analysis not consistent with the algorithm used in practice. Second, the algorithm needs to use increasing mini-batch sizes (the rate in Thm. 2 has $1/b$ as an additive term), which is another oversimplified assumption for the analysis, many adaptive algorithms are proven to converge without this assumption. Empirical merit of the method is also marginal and some important comparisons are missing. As a result, I am voting for rejection.Chen et al., 2018: Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. ICLR, 2019.[1] A New Regret Analysis for Adam-type Algorithms, Alacaoglu, Malitsky, Mertikopoulos, Cevher, ICML 2020. The authors conjecture that convolutional downsampling is an underlying mechanism behind sample memorization in over-parameterized convolutional autoencoders. They claim that this effect leads the system to converge to a low-rank solution in contrast to the theoretically possible identity mapping. They support their claim with numerical experiments on linear and non-linear convolution autoencoders.Strengths:- The authors develop their idea in close connection to commonly used architectures.Weaknesses:- The main statements concern the architecture; however, the experiments do not account for the many confounding factors such as initialization or the chosen optimizer. The paper itself states on page 4 that the results depend on the initialization and cite Gunasekar et al. in the conclusion for an analogy, which, however, explores the implicit regularization effect of a gradient descent optimizer.- There is no clear and proved statement despite the suggestive mathematical nature of the writing (Conjecture, Proposition). The claimed 'proof' of the Proposition is conducted via experiment. In light of the above mentioned confounding factors, the current phrasing of the Proposition will not allow a formal proof as it is unclear what the system 'linear Network DS' even is.- The boundary between conjectured and inferred statements is very vague. For example, the meaning of  'prefers to learn a point map' is unclear.Overall, the exposition is insufficient in supporting the conjectured effect. The methodology could be strengthened in two directions:1) experimentally: designing numerical experiments that exclude confounding factors and surface the conjectured effect2) theoretically: abstracting the idea into a clear mathematical statement that can be provedI encourage the authors to extend their work for submission to a future venue. This paper introduces and details a new research framework for reinforcement learning called Dopamine. The authors give a brief description of the framework, built upon Tensorflow, and reproduce some recent results on the ALE framework. Pros:1. Nice execution and they managed to successfully reproduce recent deep RL results, which can be challenging at times.Cons:1. Given that this is a paper describing a new framework, I expected a lot more in terms of comparing it to existing frameworks like OpenAI Gym, RLLab, RLLib, etc. along different dimensions.  In short, why should I use this framework? Unfortunately, the current version of the paper does not provide me information to make this choice. Other than the framework, the paper does not present any new tasks/results/algorithms, so it is not clear what the contribution is.Other comments:1. The paragraphs in sections 2.1 and 2.2 (algorithmic research, architecture research, etc.) seem to say pretty much the same things. They could be combined, and the DQN can be used as a running example to make the points clear.2. The authors mention tests to ensure reliability and reproducibility. Can you provide more details? Do these tests account for semantic bugs common while implementing RL algorithms? Summary:The authors present an open-source framework TensorFlow-based named Dopamine to facilitate the task of researchers in deep reinforcement learning (deep RL). It allows to build deep RL using existing components such as reinforcement learning agents, as well as handling memory, logs and providing checkpoints for them.Emphasis is given on providing a unified interface to these agents as well as keeping the framework generic and simple (2000 lines of code).The framework was demonstrated on Atari games notably using Deep Q-network agents (DQN).The authors provide numerous examples of parameter files that can be used with their framework.Performance results are reported for some agents (DQN, C51, Rainbow, IQN).Given the actual trends in deep learning works, unified frameworks such as that proposed is welcome.The automatization of checkpointing for instance is particularly useful for long running experiments.Also, trying to reduce the volume of code is beneficial for long-term maintenance and usability.Major concerns:* This type of contribution may not match the scope of ICLR.* In the abstract and a large fraction of the text, the authors claim that their work is a generic reinforcement learning framework. However, the paper shows that the framework is very dependent on agents playing Atari games. Moreover, the word "Atari" comes out of nowhere on pages 2 and 5.The authors should mention in the beginning (e.g. in the abstract) that they are handling only agents operating on Atari games.* The positioning of the paper relative to existing approaches is unclear: state of the art is mentioned but neither discussed nor compared to the proposal.* The format of the paper should be revised:                - Section 5 (Related Works) should come before presenting the author's work. When reading the preceding sections, we do not know what to expect from the proposed framework.                - All the code, especially in the appendices, seems not useful in such a paper, but rather to the online documentation of the author's framework.* What is the motivation of the author's experiments?                - Reproduce existing results (claimed on page 1)? Then, use the same settings as published works and show that the author's framework reaches the same level of performances.                - Show new results (such as the effect of stickiness)? Then the authors should explicitly say that one of the contributions of the paper is to show new results.* The authors say that they want to compare results in Figure 3. They explain why the same scale is not used. To my opinion, the authors should find a way to bring all comparisons to the same scale.For all these reasons, I think the paper is not ready for publication at ICLR. Review: This paper proposed "Dopamine", a new framework for DeepRL.  While this framework seems to be useful and the paper seems like a useful guide for using the framework, I didn't think that the paper had enough scientific novelty to be an ICLR paper.  I think that papers on novel frameworks can be suitable, but they should demonstrate that they're able to do something or provide a novel capability which has not been demonstrated before.  Strengths: -Having a standardized tool for keeping replay buffers seems useful.  -The Dopamine framework is written in Python and has 12 files, which means that it should be reasonably easy for users to understand how it's functioning and change things or debug.  -The paper has a little bit of analysis of how different settings effect results (such as how to terminate episodes) but I'm not sure that it does much to help us in understanding the framework.  I suppose it's useful to understand that the settings which are configurable in the framework affect results?  -The result on how sticky actions affect results is nice but I'm not sure what it adds over the Machado (2018) discussion.  Weaknesses: -Given that the paper is about documenting a new framework, it would have been nice to see more comprehensive baselines documented for different methods and settings.  -I don't understand the point of 2.1, in that it seems somewhat trivial that research has been done on different architectures and algorithms.  -In section 4.2, I wonder if the impact of training mode vs. evaluation mode would be larger if the model used a stochastic regularizer.  I suspect that in general changing to evaluation mode could have a significant impact. The authors propose a new GAN procedure. It's maybe easier to reverse-engineer it from the simplest of all places, that is p.16 in the appendix which makes explicit the difference between this GAN and the original one: the update in the generator is carried out l times and takes into account points generated in the previous iteration. To get there, the authors take the following road: they exploit the celebrated Benamou-Brenier formulation of the W2 distance between probability measures, which involves integrating over a vector field parameterized in time. The W2 distance which is studied here is not exactly that corresponding to the measures associated with these two parameters, but instead an adaptation of BB to parameterized measures ("constrained"). This metric defines a Riemannian metric between two parameters, by considering the resulting vector field that solve this equation (I guess evaluated at time 0). The authors propose to use the natural gradient associated with that Riemannian metric (Theorem 2). Using exactly that natural gradient would involve solving an optimal transport problem (compute the optimal displacement field) and inverting the corresponding operator. The authors mention that, equivalently, a JKO type step could also be considered to obtain an update for \theta. The authors propose two distinct approximations, a "semi-backward Euler formulation", and, next, a simplification of the d_W, which, exploiting the fact that one of the parameterized measures is the push foward of a Gaussian, simplifies to a simpler problem (Prop. 4). That problem introduces a new type of constraint (Gradient constraint) which is yet again simplified.In the end, the metric considered on the parameter space is fairly trivial and boils down to the r.h.s. of equation 4. It's essentially an expected squared distance between the new and the old parameter under a Gaussian prior for the encoder.  This yields back the simplification laid out in p.16.I think the paper is head over heels. It can be caricatured as extreme obfuscation for a very simple modification of the basic GAN algorithm. Although I am *not* claiming this is the intention of the authors, and can very well believe that they found it interesting that so many successive simplifications would yield such a simple modification, I believe that a large pool of readers at ICLR will be extremely disappointed and frustrated to see all of this relatively arduous technical presentation produce such a simple result which, in essence, has absolutely nothing to do with the Wasserstein distance, nor with a "Wasserstein natural gradient".other comments::*** "Wasserstein-2 distance on the full density set": what do you mean exactly? that d_W(\theta_0,\theta_1) \ne W(p_{\theta_0},p_{\theta_1})? Could you elaborate where this analogy breaks down? *** It is not clear to me why the dependency of \Phi in t has disappeared in Theorem 2. It is not clear either in your statement whether \Phi is optimal at all for the problem in Theorem 1.*** the "semi-backward Euler method" is introduced without any context. The fact that it is presented as a proposition using qualitative qualifiers such as "sufficient regularity" is suspicious. This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong.The paper is quite well written and clear. However there is several unsupported claims (see below).A lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator.Pros:- The experimental section is quite thorough and the results seem overall good.- The paper is quite clear.Cons:- There is a major mistake in the derivation of the proposed method. In eq. (6) &amp; (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 &lt;=&gt; JSD(p||q) = 0 &lt;=&gt; p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either.- The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator.- I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator.- The authors claim that their method doesn't "just memorize the true data distribution". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. - The authors states that "adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training", can the author give some references to support that statement ?- According to the author: "Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean.Others:- It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ?Conclusion:The idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved.References:[1] Nowozin, S., Cseke, B., &amp; Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision  they dont encode shape-bias like human vision. Towards this goal, the authors modified the training data with shortcut features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task.Investigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. The paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting.Nonetheless, there are significant drawbacks in the presented work:1.        The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Lets make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.)The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldnt be used to predict the object label. The paper doesnt do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. 2.        Due to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so:a.        We tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on nopix case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed.Details: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: https://file.io/qiziAK. The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-à-vis the results in the paper. The same setting didnt achieve 100% either though it got close - achieving 98.4%. Summary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality.I encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including shape-features and submit to a future conference. In this paper, the authors propose POLO, a reinforcement learning algorithm which has access to the model of the environment and performs RL to mitigate the planning cost. For the planning, POLO uses the known model of the environment up to a fixed horizon H and then use an approximated value function in the leaf nodes. This way, instead of planning for an infinite horizon, the planning is factored to a shorter horizon, resulting in lower computation cost.The novelty and motivation behind this approach is limited. Similar or even more general approach for discrete action space is introduced in "Sample-Efficient Deep RL with Generative Adversarial Tree Search" where they also learn the model of the environment and additionally consider the error due to the model estimation. There is also a clear motivation in the mentioned paper while I could not find a convincing one for the current paper. Putting the novel limitation aside,  both of these paper, the current paper, and the paper I mentioned, suffer from very lose estimation bounds. Both of these works bound somewhat similar (not the same) things via L_inf error of value function which in practice does not necessarily result in useful or insightful upper bounds (distribution dependent bound is desired). Moreover, with the assumption of knowing the environment model, the implication of the current work is significantly limited.The authors do a good job of writing the paper and the paper is clear which is appreciatable.In equation 6 the authors use log-sum-exp and claim it corresponds to UCB, but they do not provide any evidence to support their claim. In addition, the Bayesian linear regression in the tabular setting is firstly proposed in Generalization and Exploration via Randomized Value Functions and beyond tabular setting (the setting in the current paper) was proposed in Efficient Exploration through Bayesian Deep Q-Networks. The claims in this paper are not strong enough and the empirical study does not strongly support or provide sufficient insight. For example experiments in section 3.2 does not provide much insight beyond common knowledge.While bridging the gap between model based and model free approaches in RL are significantly important research directions in RL, I do not find the current draft significant enough to shed sufficient light into this topic. This paper presents an approach for biasing an agent to avoid particular action sequences. These action sequence constraints are defined with a deterministic finite state automaton (DFA). The agent is given an additional shaping reward that penalizes it for violating these constraints. To make this an easier learning problem for the agent, its state is augmented with additional information: either an action history, the state of the DFA, or an embedding of the DFA state. The authors show that these approaches do reduce these action constraint violations over not doing anything about them.It's unclear to me what the use case is for constraints solely on the action space of the agent, and why it would be useful to treat them this way. The authors motivate and demonstrate these constraints on 3 Atari games, but it is clear that the constraints they come up with negatively affect performance on most of the games, so they are not improving performance or safety of the agent. Are there useful constraints that only need to view the sequence of actions of the agent and not any of the state?  If there are such constraints, why not simply restrict the agent to only take the valid actions? What is the benefit of only biasing it to avoid violating those constraints with a shaping reward? This restriction was applied during testing, but not during training. In all but the first task (no 1-d dithering in breakout), none of the proposed approaches were able to completely eliminate constraint violations. Why was this? If these are really constraints on the action sequence, isn't this showing that the algorithm does not work for the problem you are trying to solve? The shaping reward used for the four Atari games is -1000. In most work on DQN in Atari, the game rewards are clipped to be between -1 and 1 to improve stability of the learning algorithm. Were the Atari rewards clipped or unclipped in this case? Did having the shaping reward be such large magnitude have any adverse effects on learning performance?Adding a shaping reward for some desired behavior of an agent is straightforward. The more novel part of this work is in augmenting the state of the agent with the state of a DFA that is tracking the action sequence for constraint violations. Three approaches are compared and it does appear that DFA one-hot is better than the other approaches or no augmentation.Pros:- Augmenting agent state with state of DFA tracking action sequence constraints is novel and useful for this problemCons:- Unclear if constraints on action sequences alone useful- No clear benefit of addressing this problem through shaping rewards.- No comparison to simply training with only non-violating action sequences.- Algorithm still results in action constraint violations in 5/6 tasks. This paper presents a very specialized example to show that gradient descent on the cross-entropy loss WITHOUT REGULARIZATION leads to poor margin, which is very unrealistic. Moreover, I have the following concerns:1. In the two points classification example shown in Section 2, I want to see the plot of iteration versus cross-entropy loss during the gradient descent.2. Whether it makes sense to use cross-entropy loss to quantify loss for two-class classification problem with one point in each class? Statistically, it seems not reasonable at all.3. In Corollary 1, the authors made a further assumption, x^Ty=1, which is very unnatural.4. In the numerical results section, I want to see some results on some benchmark dataset. The presented numerical results are too weak to support the proposed differential training. The paper presents a way to compress the neural network architecture. In particular, it first extracts some characteristics for the neural network architecture and then learns two mapping functions, one from the encoded architecture characteristics to the expected accuracy and the other from the same encoded architecture characteristics to the number of parameters. In the meanwhile, the proposed approach learns the encoding and the decoding for the architecture characteristics. Pros:1. The idea of converting the architecture characteristics, which is discrete in nature, to continuous variables is interesting. The continuity of the architecture characteristics can help architecture search tasks.Cons:1. My main concern is the validity of the compression step, Procedure COMPRESS, in Algorithm 1. First, is only one step gradient descent applied? If it is, why not minimize the L_c until convergence?  Second, it seems that minimizing L_c cannot guarantee that both error and the number of parameters are reduced. It is possible that only one of them is reduced. 2. The writing of the paper needs to be improved. Some notations are not consistent with each other. For example, the loss notations in Line 19 in Algorithm 1 are different from those defined in Sec. 4.3. 3. There is no step size, \eta in Line 20 in Algorithm 1, but there is a step size in the last equation on Page 6. 4. It is unclear to me how the hyperparameters, such as the step size and \lambda's, are chosen. 5. More experimental results are needed to support the proposed approach. In summary, I think this paper is not ready to be published. This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. However, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as adversarial examples, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. While the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust & etc (for example, see [1]).  The attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. [1] Generalisation in humans and deep neural networks, arXiv:1808.08750  SummaryThis paper introduced a parameterized image processing technique to improve a robustness of visual recognition systems against noisy input data. The proposed method is composed of two components; a denoising network that suppresses the noise signals in an image, and gating network that predicts whether to use the original input image or the one produced by the denoising network. The proposed idea is evaluated on three tasks of object detection, tracking and action recognition. Originality and significance:The originality of the paper is very limited since the paper simply combines the existing image denoising technique with the idea of gating. The practical significance of the work is also limited since the model is trained and evaluated with only synthetically generated noise patterns; it is not surprising that the proposed method (both denoising and gating networks) works under this setting, as the noise is created synthetically under the same setting in both training and testing. To demonstrate the practical usefulness, it would be great if the model is evaluated with the actual source of noises (e.g. noises from input sensors, distortion by image compression, etc).  Clarity:I think the title of the paper is misleading; the proposed model is actually not a mixture of preprocessing units, as it combines *a* denoising unit together with identity mapping. The gating network is also not designed to incorporate a mixture of more than two preprocessing units, as it outputs only on/off switches instead of weights for K mixture components (K&gt;2).Minor comments:1) the paper argued the importance of lightweight preprocessing but have not provided analysis on computation costs. From the current results, I dont see the clear benefit of the proposed method (denoising network) over the average filtering considering the tradeoff between computation vs. performance. 2) In Figure 5, I suggest highlighting the differences among the examples for clarity. The paper proposes a conditional graph generation that directly optimizes the properties of the graph. The paper is very weak.1. I think almost all probabilistic graph generative models are differentiable. If the  objective is differentiable function of real       variables, it is usually differentiable.2.  The authors claim that existing works Simonovsky and Komodakis (2018) and Cao &amp; Kipf (2018) are restricted to use small graphs with predefined maximum size. This work does not overcome the limitation of small graphs issue too.3. The authors do not show any measure on validity, novelty or uniqueness which are now standard in literature.   Also I do not find any comparison with molGAN paper which tackles a similar objective.4. Could the authors show if the decoding process is permutation invariant? I am not really sure of that. I was trying to prove that thing formally, but I failed. This paper proposes a new meta-learner for few-shot learning that conditions the parameters of the model on the given query image. The authors argue that this allows the model to focus on features particular to the query, thereby facilitating classification. The paper introduces a kernel generator as a meta-learner and report performance on two standard benchmarks, Omniglot and miniImagenet.Several methods propose meta-learners that adapt the learners parameters to the task or each class in the task. This paper adapts to the query itself, which may provide other benefits, and provides a useful complement to prior work on parameter adaptation in few-shot classification.While the core idea itself is clearly articulated, the reading is dense and many of the finer points are vaguely presented. This makes the paper hard to read and its contribution unclear. In particular, the meta objective itself is not defined, the second loss function contains an undefined (learnable?) functions whose role is not entirely clear. In the experimental section, the authors mention that they use Prototypical Networks (Snell et al., 2017) on top of their kernel generator. This puts their contribution in a different light, now as an extension of Snell et al., (2017). Im also unclear about the novelty of kernel generator the authors supposedly introduce. The kernel generator appears identical to that of Han et at. (2018), in which case the contribution is its application to few-shot learning, not the kernel generator itself.Since the main contribution of this paper is to condition the learners parameters on the query, as opposed to the task or the classes in the task, the relevant comparison is with respect to such alternative methods. Several such benchmarks are missing (below), and when considered, the reported results are relatively weak. For an up-to-date collection of benchmarks on miniImagenet, see Rusu et. al., (2018, https://arxiv.org/abs/1807.05960).===[1] Gidari and Komodakis. Dynamic few-shot visual learning without forgetting. 2018.[2] Oreshkin et al.. TADAM: Task dependent adaptive metric for improvedfew-shot learning. 2018.[3] Qiao et al.. Few-shot image recognition by predictingparameters from activations. 2017.[4] Bauer et al.. Discriminative k-shot learning using probabilistic models. 2017. The paper discusses two ways of constructing adversarial examples (images) using PCA+knn in the input space. Compared to the litterature on adversarial examples, the modifications proposed by the authors are clearly visible to the human eye and the resulting images do not seem natural (see Figure 4 and 5). The authors acknowledge this difference between their work and the state-of-the-art (e.g., "Modified images are sometimes visible but still can keep original structures and information, this shows that adversarial images can be in more forms than small perturbations", Section 4.1), but it remains unclear why generating such images would be interesting in practice.The algorithm for generating adversarial examples from nearest neighbors and PCA is reasonable. It seems simple and fairly easy to implement. However, it does not seem to be competitive with the current litterature for generating adversarial examples. An important point of the authors is that their method constructs "adversarial" samples without taking into account the specific structure of neural networks (more generally, without any knowledge of the classifier). This claim would have more practical impact if the method was shown to fool more algorithms/types of models than usual approaches (e.g., fast gradient sign). But there is no comparison to the state-of-the-art, so it is unclear in what situation the method should be interesting.I found the motivation based on knowledge representation rather confusing, and I found no clear arguments for the PCA nor the k-nn approach. The write-up uses statements that are vague or not properly justified such as "For human beings, both abstraction and sparsity can be achieved with hierarchical storage of knowledge. This can be similar to object-oriented programming" (why is object-oriented programming relevant here?, is there any justification and formal statement for the first claim (e.g., a reference)?), "Neural networks store learned knowledge in a more hybrid way", "In summary, human beings can detect different objects as well as their transformations at the same time. CNNs do not separate these two." (there is no clear experiment proving that there is no "separation" because it is unclear what the DeepDream visualization of Figure 1 effectively proves). The equations do not really help (e.g., X_2 is supposed to be a vector if I understand correctly, what is X_2^{-1}?). Overall, I think the paper would gain a lot by making the motivation part more formal.In conclusion, the authors seem to depart from the idea that adversarial examples should a) fool the network but b) still feel natural to humans. There is no clear motivation for generating such unnatural adversarial examples, and there is no clear application scenario where the algorithm would generate "better" adversarial examples than usual methods.minor comments:* please add spaces before \cite commands **First of all, this paper uses 11 pages**Submission instruction is "There will be a strict upper limit of 10 pages."The readability of the manuscript should be improved. I'm not convinced why Chapter 2 motivates Chapter 3. I think Ch. 2 and Ch. 3 are different stories. Summary: The authors investigate the Breimans dilemma in the context of deep learning. They show generalization bounds in terms of the margin distribution. They also perform experiments showing the Breimans dilemma.Comments: I am afraid the authors miss an important related paper:Lev Reyzin, Robert E. Schapire:How boosting the margin can also boost classifier complexity. ICML 2006: 753-760Reyzin and Schapire explain the Breimans dilemma based on base classifiers complexity. In particular, their experiments show that arc-gv tends to use more complex decision trees than AdaBoost while it achieves better margin distribution over sample. That is, not only margin distribution, but also the complexity of base classifiers class matters. This is already explained by known Rademacher complexity based margin bounds.As for quiantile-based analyses on margin bounds the following result is known:Liwei Wang et al: A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin, Journal of Machine Learning Research 12 (2011) 1835-1863.They proved a shaper bound using the notion of equibrium margin. The authors should compare the presented results with this. The technical results of the paper look quite similar to known margin bounds and I am afraid the contribution is minor or redundant. The method tackles the problem of interpretability that is a very important issue for usually black-box deep networks. Unfortunately it is not very clear how is the achieved. I have read several times the part explaining the influence maps and the clustering based on them and it still doesn't make a lot of sense to me. I think that part has to be better justified and exposed. Moreover, results do not support the claim which makes me doubt even more about how effective the method proposed actually is. In conclusion, I think that better exposition and more solid experimental analysis is needed.  Also please check some writing problems: &gt; Introduction: "to acquire a generative function mapping a latent space (such as Rn)" &gt;  difficult to read, rephrase. "making it difficult to add human input" &gt; confusing. What do you mean by human input? I assume you refer to having control to make decisions about design. &gt; Section 3.1"the internal variable may leave the manifold it is implicitly embedded in as a result of the models training" : not clear, rephrase. Summary: Proposes a framework for performing adversarial attacks on an NMT system in which perturbations to a source sentence aim to preserve its meaning, on the theory that an existing reference translation will remain valid if this is done. Given source and target metrics for measuring similarity, an attack is deemed successful if the source difference is smaller than the relative decrease in target similarity to the reference. A first experiment measures correlation with human judgements of similarity between original and perturbed sentences, and concludes that chrF is better than BLEU and METEOR for this purpose. Next, standard gradient-based adversarial attacks are carried out, replacing the three tokens that result in the biggest drop in (approximate) reference probability, either 1) with no constraints, 2) constrained to character swaps of the original token, or 3) constrained be among the 10 closest embeddings to the original token. In comparisons on three language pairs from IWSLT,  the constrained attacks are found to preserve meaning and yield more successful attacks according to the current framework. The Transformer architecture was also found to deal less well with attacks under the 10-closest embedding constraint. Finally, adversarial training with the character-swap constraint confers some robustness to this attack, without degrading performance on normal text.I think it is a good idea to formalize a method for carrying out and assessing adversarial attacks, but the framework proposed here seems too narrow, as it excludes adversarial inputs that are sensible but not a close perturbation of an existing source/reference pair, or ones that contain varying amounts of noise. It is more difficult to measure output quality for such attacks, but that doesnt seem like a good reason for excluding them from what is intended to be a general framework. Note also that more difficult doesnt mean impossible, since good attacks can produce severely degraded output that is relatively easy to detect.I found some of the methodology questionable. Limiting source perturbations to character swaps and neighbors in embedding space, then using automatic metrics to measure semantic distance seems both unnecessary and unlikely to succeed. Unnecessary because knowing the class of perturbation already gives you a lot of information about semantic distance. Unlikely to succeed because automatic metrics are too coarse to reliably distinguish among different perturbations. This is particularly obvious in the case of using character ngram distance (chrF) to determine which character swaps preserve meaning best. The experiments that support the viability of automatic metrics in 4.2 do so by measuring correlation with human judgment when the number of perturbed tokens varies from 1 to 3. I think the good correlation is likely due to the metrics being able to detect that, eg, changing 3 tokens makes things worse than changing only one. To be convincing, the experiments would have to be repeated with number of perturbations fixed at 3, to match the setting in the remaining experiments. Apart from the interesting observation about the Transformers performance on embedding-neighbor attacks mentioned above, it is difficult to know what conclusions to draw from the experiments. In 4.3 it seems obvious a priori that perturbations intended to be relatively meaning preserving would indeed preserve meaning better than unconstrained ones. Similarly, it is not surprising that character swaps that by design produce an OOV token will cause more damage than choosing a near neighbor in embedding space. In 5.3, training with OOVs (resulting from character swaps) is of course not likely to hurt performance on test sets containing few OOVs, and, as is known from previous work, it will improve robustness to the same kind of noise. A final comment about the experiments is that word-based systems are not state of the art, and it isnt clear how much we could expect any conclusions to carry over to sub-word models.To conclude, although this is an interesting initiative, both the framework and the methodology need to be tightened up.Details:End of 2.1: this would be easier to interpret if you had previously specified the allowed range for s_src.3.2 For kNN, being semantically related doesnt imply that the relationship is synonymy, as would be required for meaning preservation. It also doesnt imply that the substitution will be grammatical, which could jeopardize meaning preservation even if the words are synonyms.CharSwap seems odd. If youre just going to replace a work with an OOV symbol in any case, why go to the trouble of swapping characters? No matter what actual semantic shift is caused by the swap, the model will always see exactly the same representation.4.1 Following previous work on adversarial examples for seq2seq models (Belinkov &amp; Bisk, 2018; Ebrahimi et al., 2018a) - this is misleading: Ebrahimi et al only work with classification, and dont use IWLST.4.1 Should mention the size of the training sets in this section.Table 1, first sentence, CharSwap example omits faire.4.3, Adding Constraints Helps Preserve& last sentence: but here you need to reason in the opposite direction.5.2 It would be good to also give absolute scores for table 6, so we can judge how much the systems actually benefited, and whether these gains were statistically significant. Summary:The paper proposes a spatio-temporal attention weighting mechanism in LSTM, applied to the task of human action recognition. VGG19 based frame features are fed to LSTM, soft attention is calculated based on previous works and temporal attention is predicted using another small neural network. The features are weighted by these attentions and eventually the network is trained with a regularized cross entropy loss. Empirical results are given on three datasets for action recognition, UCF11, UCF101 and HMDB51.Positives:- The problem addressed is a relevant and challenging CV problem- The idea of using of spatio-temporal attention is also interesting, as the actions are expected to have salient parts relatively sparsely located in space and time and focusing on them seems like an interesting direction to investigate.Negatives:- The paper is not well written in general- The novelty is low as similar attention mechanisms have been used before. Papers have been cited in related works but differentiation in terms of what the current method adds is largely missing. The spatial attention is borrowed from Xu et al. (2015) and Sharma et al. (2016) and the temporal attention is relatively simple (similar ideas have been explored with CNNs as well eg. [A,B]) so the exact contribution and it's novelty is not convincing- The results are not very convincing either, UCF11 is a very small dataset, on the bigger datasets the improvements over Video LSTM are small- Self implemented baseline (the current implementation with same base CNN and LSTM networks without any spatial or temporal attention, \alpha=\beta=1 fixed) as well as ablation studies (what happens when only spatial or temporal attentions are used) should be added for assessing the contribution of the different components- Some actual qualitative results should be added demonstrating the effectiveness of the proposed approach[A] Kar et al., AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos, CVPR 17[B] Bilen et al., Action Recognition with Dynamic Image Networks, accepted for TPAMI 2018, arxiv 1612.00738I feel that in the current form the paper is not ready for publication. The paper propose an end-to-end combination of spatial and temporal attention for videos. The method first extracts a vgg19 representation to any frame, reduced with spatial soft-attention.  The attended vectors are then fed to LSTM with a soft temporal attention.Strengths: The problem of applying both temporal and spatial attention is important and challenging in general.The reported numbers on HMDB51 and UCF101 are impressive, given the fact the authors only used rgb features. (Hope I haven't missed anything)Weaknesses:Recent datasets for action recognition, e.g., Moments in time, Charades, Youtube-8M etc, are missing. If the authors can show this model on any of these this will make their case stronger. I suspect the proposed model is limited to short-videos only,  keeping the spatial information means the features dimensions are increased by factor of 49. This is why Charades dataset can be very interesting to see, because the videos are longer. But also Moments in time, which is much larger.The writeup should improve significantly: Grammar mistakes, typos e.g.,  donates/denotes, operations in equations (eq. 5,6), punctuation after equations. etc..Even though the model is basic, It was really hard to follow. For instance, I couldnt follow the whole discussion about T classifications, and the voting. What exactly are we classify? Another example, eq5, \beta_t is not an actual attention score, but the energy potential that we later use to calculate the attention in eq7.Qualitative evaluation is barely there, one sample is not convincing enough, qualitative evaluation in vision-models should show many-many examples.  Fig 2,3. describe well-known techniques, I think it's better to add more examples instead.To conclude:This is important subject, but the paper is not matured enough. A better writeup, and evaluation on more recent dataset is necessary. Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.Comments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.Second, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?Finally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?Minor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?Overall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.[1] Venna et al. "Information retrieval perspective to nonlinear dimensionality reduction for data visualization." Journal of Machine Learning Research 11, no. Feb (2010): 451-490. The paper proposes a method of searching for a Nash equilibrium strategy in games where the strategy-to-payoff mapping is defined by a neural network. The idea is to perform gradient optimization of the payoff w.r.t. the strategy. Preliminary results on tic-tac-toe and variations of the prisoners dilemma task are presented. The paper has an interesting idea at the core. However, it is poorly written, does not properly discuss the related works and does not present a convincing method or experimental results. Pros:* The paper considers an interesting question of exploring the applications of neural networks to the game theory problems.* The idea of the paper is reasonable. It makes sense to me to perform gradient-based search over the strategies (assuming that the payoff is differentiable).Cons:* Writing- The paper is over the mandatory length limit of 10 pages.- The paper makes a grandiose claim: this paper provides a revolutionary way for reinforcement learning and a possible road toward general A.I. However, there are arguably no revolutionary ideas, and certainly no reinforcement learning experiments!- Despite the claim, the novelty of the paper is limited. There is no discussion of the related work: optimization of the neural networks w.r.t. the inputs [1]; related RL ideas such as model-based learning [2,3] and Monte-Carlo Tree Search [4].- The problem being solved is never formally stated. As far as I understand, Nash equilibrium is usually defined (1) in mixed strategies, while the paper seems to consider pure strategies; (2) in the scenario where every player attempts to maximize their payoff, while in the paper the players attempt to achieve some pre-fixed value of the payoff.- The flow of the paper is generally poor. Instead of presenting a general solution and then showcasing its applications, the paper iterates on similar ideas multiple times. For example, all four algorithms are just gradient-based optimization of either the weights or the inputs to a model.- The paper provides extremely misleading analogies and explanations. I am quite sure that a mosquito brain is not a one hidden layer fully-connected neural network! Also, the example of avoiding a moving hand is poor: since the outcome is life or death, the learning should happen via evolution, not during the lifetime of a single insect. The claim that the neural networks with sigmoid activation functions are less prone to local optima is questionable as well.* Method and experiments- The proposed method is essentially a greedy gradient-based planning procedure. For this to work, we need to have a very good environment model. This is a strong assumption that is not discussed.- The experiments are performed on very simple synthetic problems: matrix games and tic-tac-toe. They do not suggest that the method is general and can work on harder problems, say, Sokoban [2].- The experiments do not present any baselines, so it is unclear how well the method performs compared to the alternatives. One obvious candidate is gradient-free optimization, such as Nelder-Mead, and gradient descent with momentum, which can be less prone to local optima.[1] Brandon Amos, Lei Xu, J. Zico Kolter Input Convex Neural Networks, ICML 2017[2] Racanière et al. Imagination-Augmented Agents for Deep Reinforcement Learning, NIPS 2017[3] David Ha, Jürgen Schmidhuber Recurrent World Models Facilitate Policy Evolution, NIPS 2018[4] Thomas Anthony, Zheng Tian, David Barber Thinking Fast and Slow with Deep Learning and Tree Search, NIPS 2017 The paper proposes a neural component architecture, named Sensoriplexer (SP), with an aim to introduce resilience to agents performing machine learning tasks. Specifically, if different types of inputs can inform the decision of an agent, the SP aims to learn the dependencies among these inputs to make the agent's decision resilient to the absence of a subset of inputs. Pros: In theory, the concept of SP is appealing and can have many practical applications. The paper does a decent job motivating the problem and elucidating different aspects of the SP framework with the help of examples. Cons: 1. Readability and Presentation: Section 3 of the paper, which lays the context for the application of SP, is not clear. The mathematical notations (such as of the form $(E_i)_i, d_E, etc.$) are not adequately defined or discussed. Similarly, the notation $M$ is used to define two separate quantities, which adds to the confusion. 2. Significance of Results: The major issue that I find with the paper is the significance of empirical results. While the notion of adding resilience to ML architectures is the main motivation, the empirical results do not seem to provide convincing evidence for the utility of SP architecture in achieving it. The only result of note in the context seems to be on the visual shape classification task in the first set of experiments, where the 'A0' scenario achieves a better than chance performance (32.60% vs 25%) by the introduction of SP architecture. I am curious if this improvement in performance was enabled by the bias of the ML architecture to classifying one or two specific classes with better accuracy. The message given by the experiments for the emotion recognition task is unclear to me. The addition of SP architecture does not appear to impact the performance of the three architectures evaluated.  The goals of this work are ambitious: to clearly define a setting useful to both physicists and machine learning practitioners.The current paper is an excellent step in this direction.However, with several semesters of undergrad quantum mechanics courses, I found it difficult to follow the calculations needed to compute predictions. Moreover, given my graduate-level experience in machine learning, it was even more difficult to clearly understand the data and training algorithm. In the current state I do not think ICLR is an appropriate venue for this work, as it may confuse machine learning practitioners. (Though, I think the original goal of the paper is very worthy, and look forward to the final version of this work!)To try to give constructive critique:- it took me a long time to understand the setup of the problem. An illustration of FashionMNIST or MNIST would help, with an incoming photon, an arrow through the cut-out of the image (clearly delineating that the image is binarized, and that white pixels are cut out, allowing the photon to pass through), and a detector (An LCD screen is described  is this the detector as well? Or does the photon bounce back and is then detected?). Such an illustration would go a long way for a machine learning practitioner unfamiliar with the double slit experiment and whatnot.- an algorithm box. A machine learning audience is used to thinking in terms of training data and algorithms. In this case, an algorithm box would help, specifically as it shows where the additional information is coming from. It seems like the 'training' is in optimizing the parameters of the unitary transform. Clearly delineating input, output, and optimization steps will help clarify the method. - an equation for computing the predictions of the trained model. Given a photon that passed through a mask, a clear formula for computing the class probability with the trained unitary operator parameters.- clearly describing the baseline. It was hard to find the details of the classical performance reported. 'maximal classical performance' is confusing wording, and implies that 'maximal' is proven theoretically. Is there a citation for this? I may have missed it. If it is not proven theoretically, then the wording should be changed, and a clear description of the architecture, training data, and training algorithm should be used, and code should be included in the supplement. This will help machine learning folks understand exactly what the comparison is against. From a machine learning standpoint, is the single pixel input to the model randomly sampled every time? - developing an additional baseline. Finding a unitary transform to find bases corresponding to style and class is unfair to the classical method, which does not have access to this information. Not having a baseline that uses this additional information will further confuse machine learning folks, as it seems obvious that a model that uses additional information will outperform a 'classical' model that does not use this information. For example, a tensor decomposition/SVD that uses information about style and class might be possible.Hope this is helpful; I think with this additional work it could be quite a valuable contribution, as I think the ICLR community could be inspired to develop more methods that require complex-valued numbers such as this one. The paper studies that  a ML system using quantum interference gives better classification accuracy than a vanilla ML system, under the  constraint that a classification decision has to be made after detection of the very first photon that passed through an image-filter.The reader can gather that this work brings together the ML and QC worlds but it is not clear what the real motivation of this work is and primarily why is single-photon important. Does single photon equate to a single pixel? Or is this denoting the very first photon that passed the filter? Also is this constraint of detecting the very first photon valid? It might be good to know which audience reads the paper. If it is the ML audience, then a section on QC basics/terminology will help. Or at least a graphical abstract to drive home the point home, will be helpful. Was there a reason to use the Fashion-MNIST in conjunction with MNIST dataset? The authors can also consider to abbreviate Fashion-MNIST to F-MNIST throughout the paper.  This paper proposes to study the expressiveness of some graph kernels and graph neural networks. To this aim, the authors build a synthetic dataset of graphs and compute the pairwise similarities, which are compared to an oracle function supposed to quantify how much two graphs are isomorphic.The objective pursued in this paper seems to me to be interesting and useful for practitioners but the manuscript has important limitations.1- The oracle similarity introduced in the paper has good properties, that one can expect from a similarity function. However, taking this function as a reference oracle in the whole sequel is not properly justified. How is this similarity function canonical? It would be interesting to cite some references on this question.For example, does the reference oracle give better accuracy scores on some classification problems than the kernels and neural networks used in the paper? This would be an important indication to justify the choice of this reference.Section 3: Even if I agree that the number of permutation matrices is of order n!, it does not prove that the optimization problem is intractable. There exist other algorithms than exhaustive search.2- The synthetic dataset is both small (only 191 graphs) and with a small variety of graphs (between 2 and 9 vertices, between 1 and 36 edges). Under these conditions, isn't it difficult to draw solid conclusions?3- I am not sure that we can expect from a GNN to evaluate accurately the similarity between two graphs from the synthetic dataset at stake, if it was trained on a very different classification dataset?I have another concern with the fact that some kernels and neural networks capture how the attributes are connected to each other, and do not really catch the topology of an unannotated graph. In this context, giving the same attribute value to all nodes as proposed on page 6 just to be able to use kernels designed for annotated graphs seems strange.For all these reasons, I believe that the empirical study presented in this paper is not strong enough to justify its publication in this conference.Typospage 2: simiarlitypage 2: expessiveness __Summary__The paper proposes to combine imitation learning (GAIL) with model predictive control to improve on the policy, especially when the system dynamics are noisier at test time.Here, MPC uses the reward function and value function learned by GAIL and assumes access to a model of the test-dynamics.The proposed approach, IMPLANT, is compared to GAIL and behavioral cloning on MuJoCo experiments in a standard imitation learning setting and for three slightly modified test scenarios that add either action noise, transition noise, or causal confusing which adds the last actions to the state space during training and replaces this information by noise at test time.__Strong points / weak points__+ (+) The paper is well-written and very easy to follow+ (+) Using MPC on a reward function that was learned by IRL can be reasonable- (-) Lack of contribution / novelty- (-) Using the GAIL reward doesn't seem sound. An actual IRL method should be used- (-) additional assumptions compared to competitors (access to noisy model, computational tractability of online planning) reduce practicability and are not regarded during the comparisons__Recommendation__I recommend rejecting the submission because I can't find a noteworthy contribution.__Supporting Arguments__- *Lack of contribution:* I don't want to be gatekeeping, but I really don't see any noteworthy contribution. The standard approach for apprenticeship learning is to learn a reward function by IRL and then optimize this reward function to learn a policy. There is no requirement to use the same algorithm to optimize the reward that used for inferring it. It is not surprising that reoptimizing the reward performs better in face of dynamic perturbations compared to a direct policy transfer; after all, this is one of the main motivations for learning a reward function. It is also not surprising that a model-based MPC approach based on the policy can perform better than directly using the parametric policy, even when there are no changes in the dynamics. So, I actually don't even see the research question here. There can be a small contribution due to the empirical evaluation, however, the chosen baselines perform direct policy transfer and, thus, do not seem that interesting.- *Using GAIL for IRL:* The paper repeatably refers to GAIL as an IRL approach. However, GAIL does not infer a reward function in accordance with the problem formulation of IRL. The "reward function" used by GAIL is only valid for small policy updates of the current policy (generator). *Maximizing* this reward function will in general not result in any reasonable policy. For optimal policy and discriminator, the reward function would even be constant. So using this reward function for MPC doesn't seem sound. The evaluation did show that optimizing the reward function via MPC can be beneficial when using a small enough horizon and when sampling from the parametric policy, but this is not that surprising since the MPC control will in that case tend to remain close to the parametric policy. - *Additional assumptions:* The paper argues that IMPLANT is "zero-shot, unlike the proposed solutions of Fu et al. (2017) and de Haan et al. (2019) which require further interactions" with the test environment. However, I would argue that MPC should be treated like a reinforcement learning method here; it also requires interactions with the test environment (i.e., sampling from the dynamics model). I don't see why it would not be fair to use a policy-parametric RL algorithm (e.g. AIRL + reoptimizing) instead of MPC for optimizing the reward. Actually, I think that the test setting would then still favor MPC since it requires costly online optimization which is not always feasible in practice.__Questions__1. Are there problem settings where IMPLANT is applicable, but IRL+RL (e.g. AIRL + reoptimizing) is not?2. The paper mentions that a random rollout policy for MPC performs worse than using the learned policy. Is this also the case for a very large number of rollouts? Is the resulting control also worse in terms of the learned reward function or only w.r.t. the true reward function? __Additional Feedback__My main concern is that the current submission has too little contribution and, thus, I think that more research needs to be done before publishing a paper on this topic. Performing MPC after IRL seems way too little as a paper story. It's hard to suggest a direction for further research because I do not see the research question to start with. Regarding the combination of IRL and MPC, it might be interesting to investigate using MPC within the IRL loop. For example, it can be challenging to learn reward functions for multimodal demonstration based on a parametric (and usually unimodal) policy. The current paper could be improved by using an IRL reward and by comparing the approach with IRL+reoptimizing. However, at least for me, this would not affect my recommendation because these changes do not address my main concern.I hope this review is not too discouraging; the paper does have strong points in the presentation. Summary-------The paper makes the observation that various non-decomposable losses in machine learning can be rewritten as linear programs, whose constraints depends on the model output. This is the case for AUC, multi-class AUC, F-score, and to some extend NMF.The authors review these losses, and recall how they may be rewritten as LPs. The LP formulation as known for AUC and NMF, but as far as the reviewer understand, they are new for multi-class AUC and F-score.Then, the authors propose to directly backpropagate through the LP resolution to minimize non-decomposable losses, applied on top of deep architectures. For this, they propose to solve an approximate solution to the LP problem (a quadratic penalization of the constraint violations) using a modified Newton method. They propose either to backpropagate by unrolling the Newton steps, or by using the computed minimizer directly.Review------The endeavor of writing non-decomposable losses as LPs, to see these losses as pluggable LP-layer in deep architecture is interesting, albeit not original.Using a penalized approximation of the LP to be able to solve them efficiently using a Newton method is also interesting.The experiment section shows that it is indeed beneficial to directly optimize over a certain decomposable loss when we measure performance in term of this loss: in particular, it outperform using a simple logistic loss. This was completely expected, but it is good to verify it experimentally.On the other hand, the manuscript suffer from many unclear parts, and from a theoretical analysis that is not polished enough. In particular:    - Phi is not introduced beforehand p. 4, and the F-score part is very hard to understand.    - the NMF section is very unclear, in particular as the authors use vague terms in their construction, such as "zero padding ensures a sxs matrix". I do not understand the role of tilde p in (6).    - Lemma 1 is not stated properly, as there is no f in equation (7). The authors state that "each y has a neighborhood in which the Hessian is quadratic", which does not mean anything. The proof sketch of Theorem 2 is very vague, in particular when the authors state that "the possible choice of Hessian is finite".    - I do not understand whether rho is chosen at every iteration, and what is its importance.I have trouble understanding why the authors went to such lengths in theirtheoretical analysis. They modify a LP by making it a "smooth almost everywhere"problem, which can then be solved using any methods, and backpropagated throughusing either unrolling, or the computed minimizer (by virtue of Danskintheorem), or the implicit function theorem. There is therefore not need to backpropagate throught tilde A^{-1} b.The fact the the problem is only smooth almost everywhere may be a problem,which is not addressed by using a Newton method. It implies that the gradientbecomes a subgradient, and may hinder optimization performance. Remark 4dismisses this problem as unimportant, yet it is, as local convergence rates fornon-convex gradient descent requires smoothness.Relating to experiments:    - The reported performance does not show std errors across splits, which makes it impossible to compare in between similar methods (PPD-SG, PPD-AdaGrad and Ours). It appears that all three methods are within statistical variations.    - NMF is a long studied problem, with many powerful methods to handle large inputs. I do not understand the choice of using the input of a deep learning network for the experiment. As it it, the experiment proposed in this manuscript is not polished enough to be valuable. The paper analyzes ICLR submissions and trends in the reviewing and selection process.The paper is very interesting, and would be quite valuable for the senior members of the community to read and be familiar with. However, it does not seem to have anything to do with the ICLR topics. Hence, in this reviewer's view, it should either be privately disseminated to area chairs, or published as some form of appendix, but should not be part of the conference.Getting into the details, it is not clear why the metric for the regression is just the mean reviewer score, not including all grades (or at least a variance). There is information in a paper that a reviewer gives a particular high/low grade to. Constructing then simulated data that uses the variation data and being surprised they don't match sounds as a potential pitfall for this analysis. In addition, there are several biases identified in the paper. But since the AC are not identified as biased, and the papers are anonymous, it is not clear what is the mechanism suggested by the authors of how these biases manifest themselves. Is there a suggestion that CMU/Cornell/MIT have a specific way of writing papers? Do women? Or is there a suggestion that anonymity does not genuinely exist, and most reviewers have good knowledge on who are the authors of the papers they review? This work studied an interesting topic of training-free Neural Architecture Search (NAS). It utilizes two training-free indicators to measure the performance of a network without training it. The experiments show the proposed approach has improvement in searching time. However,  as the main contribution, the two measurements of the training-free indicators (trainability and expressivity) already be proposed by previous works. The rest contributions of this work are the proposed NAS networks, Instead of using the original loss function to guide the search process, this work simply combined those two measurements as a new ranking identifier of candidate architectures. The contributions not strong enough.Advantages:1. The writing quality of the paper is good enough2. The paper has good descriptions of the related work.Key weakness:1. The major contributions of this work, the two network measurement methods, were proposed by previous works.The first indicator:    Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018a.    Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems, pp. 85728583, 2019.    Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization. Journal of Machine Learning for Modeling and Computing, 1(1), 2020.    Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. 2019.The seconde indicator:    Huan Xiong, Lei Huang, Mengyang Yu, Li Liu, Fan Zhu, and Ling Shao. On the number of linear regions of convolutional neural networks. arXiv preprint arXiv:2006.00978, 2020.2. The differences between the proposed pruning-based NAS and previous work are not clear. What is the key novelty of the proposed pruning strategy? 3. According to Table 3, it seems that the TE-NAS couldnt find the optimal neural architecture like P-DARTS and PC-DARTS, which confirms the limitation of this training-free search framework. In this paper the authors study the adversarial robustness of BNNs on large scale datasets. BNNs have been shown to be a more robust learning paradigm due to their uncertainty/stochasticity. Given the empirical observation that adversarially trained BNN posterior variances converge to zero (which the authors need to do much more to show as this is not a well-established phenomena), the authors propose a hierarchical prior where they put a prior over the parameters of the Gaussian prior normally used  in mean-field variational inference. The authors show that performing approximate inference with a hierarchical prior leads to an increased variational posterior variance, which the authors claim is correlated to the observation of increased adversarial robustness. I think the empirical results are solid, and the direction of the paper is ultimately an important one. I truly encourage the authors to continue to pursue this topic. Unfortunately, this paper is severely handicapped by its lack of clarity in terms of accurate contextualization, and its pervasive use of misnomers.  These misnomers are so prevalent that they would certainly lead uninformed readers to incorrect conclusions about Bayesian deep learning. * On the true posterior:  There are many places throughout the paper where the authors discuss/reference the computation of the true posterior for a Bayesian deep neural network. This can only be done by exact Bayesian inference. That includes (1) proper marginalization over the space of parameters (2) normalization wrt p(X). Given a DNN with non-linear activations, computation of the true posterior is intractable. Despite this, the authors claim that one can infer the true posterior via variation inference methods (bottom of pg 1). Variational inference makes a closed form approximation of the posterior that one tries to learn the parameters of. Even learning the optimal parameters does not guarantee convergence, outside of the case of conjugation.  Further on this, computing the true adversarial posterior is even more intractable given that the intractability of computing the optimal adversarial example compounds the issue of performing exact Bayesian inference.* Conjugate priors and hierarchical priors are distinct under the Bayesian framework. Despite this, the authors name their hierarchical prior the conjugate prior. In this work, the authors suggest placing a prior distribution over the parameters of their prior distribution (i.e. a hierarchical prior), yet call it a conjugate prior. A conjugate prior in the standard Bayesian literature is a prior which is known to be in the same family as the true posterior. It is not known, and likely not true, that for general approximate Bayesian neural networks (e.g. mean-field approximations) the true or approximate posterior is Gaussian. Thus, it is likely false to call a mean-field prior approximation a conjugate prior. * Robust Optimization is the special case of adversarial training where only adversarial data is used. The authors conflate adversarial optimization (optimization with respect to an adversarial objective) with robust optimization which has a rich history in optimization prior to its application to deep learning. The end of section 2.1 should have its terminology corrected.  * On the notion of regularization in Bayesian deep learning. In several places the authors refer to the regularization term of the ELBO objective. This regularization term is the KL divergence with the prior distribution. While the prior distribution could be said to have a regularization effect on the posterior, saying that the prior distribution is a regularizer is reductive and probably misleading. This paper argues that regularization-based approaches to continual learning fail to distinguish between classes from different tasks in the class-incremental setting (without relying on task labels to simplify the problem). The paper uses a theoretical argument showing it is not possible to assess the discriminability between two classes from different tasks without access to the data (or a model of the data distribution), and also demonstrates a difference in performance when conditioning on task labels.I think the thesis of the paper is plausible, but unfortunately, I dont think this is demonstrated in the submission in its current form, and think there are a few flaws in the argument that need to be addressed.I do think this is an interesting and important line of inquiry, and could lead to valuable insights (eg. proving comprehensively that regularization-based approaches need to do a better job of capturing the data distribution, or showing they can be improved by doing so).I encourage the authors to address the comments below in lieu of this.Main comments:1) The argument relies heavily on the assumption that regularization-based approaches to supervised continual learning do not model the data distribution, eg. However by hypothesis, [the regularization-based model] does not model the data distribution &.First, this seems to conflate regularization-based methods and discriminative models, as regularization-based methods can be applied to generative modelling tasks (eg. VCL [1]), with a classifier used on learned representations - this scenario is not considered here. Second, there is work showing that discriminative models have some generative capabilities (eg. [2]), and it may be possible to extract this information to better model the data distribution and understand where previously learned decision boundaries are valid.2) The theoretical reasoning essentially demonstrates the simple intuition that we cannot assess whether the samples from two classes are linearly separable without having access to both sets of data or a model of the data from the class from the previous task. This insight makes sense, but its not clear that it is non-trivial or important: it says nothing about how well (and under what conditions) previously learned decision boundaries may generalise/transfer to a new task, and ignores my point (1) above that discriminative models do carry information about the data distribution.3) The empirical results primarily show that performance with some regularization methods is much worse when not using the task labels, in different situations (eg. with disjoint/joint classes in each task). This is known from previous work, since regularization-based methods get much poorer performance in class-incremental learning (ie. single head, no task label) compared to task-incremental learning (multi-head, task label given) - see for example [3][4].Unfortunately, these results alone cannot attribute the poor performance to the reasons argued in the paper (ie. the inability to capture previous data distributions, leading to the inability to discriminate classes across tasks), and it would be nice to see experiments that actually measure whether the poor performance is due to the arguments made. The paper also makes a broader claim about regularization-based methods being limited, but only evaluates EWC and K-FAC. Other regularization-based approaches like VCL (without coreset)[1] and BGD [5] could be explored too.4) The writing is quite difficult to follow at times: Id recommend another thorough proofread to address grammar /spelling and clarity issues. See a list of some examples below, but there are many others.5) There are a number of papers in continual learning that have been missed - both older regularization-based approaches, and some newer ones (see refs below)Some writing issues:- inferences should be inference in most places in the text. . eg. for inference, during inference, etc.- Most citations seem to not have parentheses, so they blend into the text. (Eg. for continual learning Kirkpatrick et al. (2017); & on page 1). Please use \citep instead of \citet when the citation doesnt flow from the text.- Section 3.2: The regularization term omega act& should be acts.- Also Section 3.2: a matrix pondering weights importance - not sure what pondering should mean here.- Definition 1, ...separating two dataset&, should be datasets- I think disentangled should be discriminable in the Third Part section of Section 4.2.References:[1] Nguyen, Cuong V., et al. "Variational continual learning." arXiv preprint arXiv:1710.10628 (2017).[2] Grathwohl, Will, et al. "Your classifier is secretly an energy based model and you should treat it like one." arXiv preprint arXiv:1912.03263 (2019).[3] Hsu, Yen-Chang, et al. "Re-evaluating continual learning scenarios: A categorization and case for strong baselines." arXiv preprint arXiv:1810.12488 (2018).[4] van de Ven, Gido M., and Andreas S. Tolias. "Three scenarios for continual learning." arXiv preprint arXiv:1904.07734 (2019).[5] Zeno, Chen, et al. "Task agnostic continual learning using online variational bayes." arXiv preprint arXiv:1803.10123 (2018).[6] Aljundi, Rahaf, et al. "Memory aware synapses: Learning what (not) to forget." Proceedings of the European Conference on Computer Vision (ECCV). 2018.[7] Aljundi, R., Lin, M., Goujaud, B., & Bengio, Y. (2019). Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems (pp. 11816-11825).[8] Rao, D., Visin, F., Rusu, A., Pascanu, R., Teh, Y. W., & Hadsell, R. (2019). Continual unsupervised representation learning. In Advances in Neural Information Processing Systems (pp. 7647-7657).[9] Chaudhry, Arslan, et al. "Using hindsight to anchor past knowledge in continual learning." arXiv preprint arXiv:2002.08165 (2020). The paper shows formally (under somewhat restricted conditions, like 2-layer NN and for "natural images") that adversarial perturbations mainly affect high-frequency content of the images. This is also shown empirically, and it is shown that low-pass filtering improves robustness against adversarial examples. The core idea of the paper (low-pass filtering improves robustness against AEs) is actually old. Several methods have been proposed that in one way or another perform a low-pass filtering of the image to reduce the perturbation noise and thus improve robustness to AEs. The core idea of the paper is, therefore, not novel. That perturbations must be mainly impinging on high-frequency content is an obvious conclusion. The effect of low-pass filtering on clean image classification is also, once again, neglected. What is the value of a defense method if it decreases accuracy for clean images?Besides, the experiments are rather limited in only using CIFAR-10 (which is a subset of CIFAR) and PGD attack. The experiments are also limited insofar as a comparison is made with adversarial training, without consideration for any of the dozens other defense methods available.Minor: On p. 1 there is a sentence "...that human can only perceive LFC." that sounds too categorical, it has to be rephrased. 3342 Identifying Treatment Effects Under Unobserved Confounding# SummaryThe authors propose a representation learning method for estimating causal effects in the presence of unobserved confounding when covariates that act as proxies for a latent confounder are available. The authors connect the problem to some recent results on the partial identifiability of VAE and non-linear ICA models. The authors lay out some conditions under which the causal effect may be identifiable and propose a VAE-based model, CFVAE, that enforces some of these conditions on the estimated model. They compare CFVAE to a previously proposed VAE algorithm, CEVAE, and other methods that are designed to work under unconfoundedness.# FeedbackIdentification of causal effects in the presence of proxy variables for unobserved confounders is an important but subtle problem. However, this means that the bar for making contributions in this area needs to be high. Because the conditions for identification cant be falsified in a particular application, making unclear statements about when the effects of interest are and are not identifiable is very important. Readers who misunderstand will only experience silent failures and make poor decisions as a result.Unfortunately, I dont believe this paper meets this bar of clarity. I think that the authors explore some interesting connections to recent work on identifiability in latent variable models, and understanding what these results imply for causal inference is important. In particular, the observation that one might estimate a decoder up to a _different_ affine transformation in the treated and control arms seems like a useful insight. But the results in the paper are incomplete, disorganized, and in some cases wrong. Ill list out a few general points here, then discuss some substantive issues with the papers specific argument.## General points about identifiability arguments### Identifiability is not a property of the methodIdentifiability is a property of the _data generating process_ and not a property of the model being used to do estimation. If the causal effect of interest is not identifiable in the process that generated the data, then using an identifiable model to estimate the causal effect will not solve the problem.The exposition in the paper seems to argue that using the right model will make the causal effect identifiable. This may just be a matter of unclear writing, but this is a broader point of confusion in the ML community, so its important that the authors be clear on this point.Here, I think it would make sense for the authors to state what their assumptions are about the data generating process, separately from the parameterization of their model. Reading the paper, the distinction between these two layers of assumptions was unclear.### Relaxing identifying assumptions is not an optionIn the same vein, if there are assumptions that the authors need to make to eliminate identification failure modes, then showing that the model works when those assumptions are relaxed does not inspire confidence. Unlike standard model misspecification which can be detected and debugged based on observables, making the wrong identifying assumptions in a model can result in silent failure, where the model can fit the observed data perfectly, but return a causal effect estimate that converges to the wrong place, or doesnt converge at all.If one is able to relax the identifying assumptions and still see success in experiments, this means either (a) the assumptions were unnecessary, or (b) the experiments did not probe the method well enough.### Assumptions needs to be stated clearly, with implications clearly highlightedWhen making identification arguments, assumptions play the role of eliminating equally plausible causal explanations of the observed data, until only the true one can remain (if the assumptions are true). These are not the kinds of assumptions that eliminate exotic corner cases; instead, they eliminate cases like the most obvious explanation for the observed data like the absence of unobserved confounding. Bounding arguments like the Manski and Kallus et al papers that are cited in the introduction construct the full set of causal explanations that identifying assumptions must narrow down to a point.All of this is to say that clearly stating assumptions, and the cases they eliminate, is essential for any identification argument. In the paper as it is written now, many assumptions are made implicitly or in passing, and it is unclear which assumptions are made for illustration (e.g., the noise on the outcome going to zero) and which assumptions are essential for the argument in general. The assumptions are always framed as mild and do not highlight situations that the assumptions eliminate (i.e., in which cases they would fail to hold).## Specific Concerns### Balancing covariate implies that the naive estimator just worksThe primary identifiability result of the paper involves the assumption that the observed covariates are balancing covariates, satisfying t \indep z | x. The authors argue that this is a weaker condition than requiring that x satisfy unconfoundedness. This may be true, but the gap between the two assumptions is, at most, a set of knife-edge violations of faithfulness. In terms of estimating causal effects, simply doing the standard covariate with x and ignoring z would give the right answer.This can be shown in two ways. First, graphically, t \indep z | x implies that there is no backdoor path through z from t to y when you condition on z. So z doesnt induce any non-causal association between y and t. Secondly, using the standard adjustment formula:\mu_t(x) = E[ E[Y | X = x, T = t, Z = z] ]= \int_z E[Y | X = x, T = t, Z = z] p(z | x) dz= \int_z E[Y | X = x, T = t, Z = z] p(z | x, t) dz  (using the balancing covariate property)= E[Y | X = x, T = t]In particular, the naive regression function E[Y |  X = x, T = t] only fails in cases where p(z | x, t) \neq p(z | x); i.e., when the distribution of the latent variable is different in the two observed treatment arms even after conditioning on x. The balancing covariate property eliminates this possibility.This also means that the VAE model specified can only generate data where the latent variable z does not introduce confounding.## Other Concerns * The adjustment formula in equation (2) is wrong. The last integral should be with respect to p(z | x), not p(z | x, t) (see the argument above). * There is a substantive difference between estimating individual level causal effects given the observed outcome (a counterfactual query) versus estimating the CATE. The authors do divide this into pre-treatment and post-treatment prediction, but the counterfactual query presents additional identification questions. In particular, whether the reported expectation is correct depends on Cov(y(1), y(0) | z, t), which is never observable. None of the identifying assumptions in the paper make any arguments about this quantity, so the models in the paper are making implicit strong assumptions here. * The f^{-1}(y) notation in the paper is very unclear in the case that there is actually outcome noise for y. The function f relates the latent z to the expectation of y, not y itself. When y includes independent noise, the distribution of f^{-1}(y) does not yield the marginal distribution of z; you need to deconvolve the independent noise in y, which is non-trivial. This paper presents a new attack against neural networks that combine Adversarial inputs and trojans. The key idea is to train a trojaned network that the victim might believe to be adversarially robust but the trojan will be activated when a trigger and adversarial noise are both present in the input image.Strengths -------------- The attack presented by the authors beat most existing defensesWeaknesses------------------ The contribution seems very incremental given that there exist a substantial number of works in both adversarial inputs and trojaning. The core training algorithm (Alg. 1) also seems very straightforward. - The threat model seems to be somewhat unrealistic. The adversarial inputs usually require norm-bounded global (all pixel) perturbations while inserting trojan triggers require local modifications to the input image. It is not clear to me what is achieved by putting these two different types of attacker models together that cannot be achieved by a regular trojan attack with larger triggers.     - the evaluation against existing trojan detection/prevention methods seem a bit unfair as they were never designed to consider adversarial inputs The authors propose two learning paradigms where the learning agent doesn't have access to a reward function but instead has to learn directly from the "assistance" from a trainer agent.While looking for ways to facilitate task specification and human integration in the learning process is a relevant and promising research goal, the authors don't explain the difference between their newly-proposed paradigms and the very rich literature on Inverse Reinforcement Learning and Learning from Demonstrations.Learning from human "assistance" instead of a reward function is not a new thing, and the surveys below (not cited by the authors) summarize a rich literature that does precisely that:Silva, Felipe Leno, and Anna Helena Reali Costa. "A survey on transfer learning for multiagent reinforcement learning systems." Journal of Artificial Intelligence Research 64 (2019): 645-703. -> Surveys many categories of works where one agent provides guidance to others, including humans providing "assistance" to learning agents.Argall, Brenna D., et al. "A survey of robot learning from demonstration." Robotics and autonomous systems 57.5 (2009): 469-483. -> Surveys learning from demonstration, where a human provides policy demonstrations to a learning agent, which usually doesn't have access to a reward functionGao, Yang, et al. "A survey of inverse reinforcement learning techniques." International Journal of Intelligent Computing and Cybernetics (2012). -> Inverse reinforcement learning, where the learning agent doesn't have access to a reward function and has to infer a policy from human assistanceWithout a comprehensive discussion about the differences between the authors' proposal and those paradigms, I can't judge the paper contribution. To my eyes, all the descriptions gave in the paper look just like the same as one of the problems surveyed in the papers above.For example, "Non-active reward learning" seems to me equivalent to inverse reinforcement learning.Also, "active reward learning" seems to me a special case of the action advising problem surveyed in the first paper above.In the case the authors deal with a different problem in this paper, I suggest that the manuscript is rewritten to thoughtfully explain the difference between all those scenarios. In case they indeed are correspondent scenarios, I suggest that the authors use the same notation as the previous works, and include comparisons with the state of the art methods in the experimental evaluation.-----------------------------------------------------Other suggestions- I don't get how the policy decision function will compute the expected reward if the reward function is unknown.- You assume access to a "human decision function", what exactly does that mean? do you need the probabilities for taking each action? If that's the case it is very unrealistic to expect that a human is able to provide probabilities for each action. Asking s/he to simply pick one action when requested is more realistic. -------------------Summary-------------------This paper proposes a simple approach to discover interpretable latent manipulations in trained text VAEs. The method essentially involves performing PCA on the latent representations to find directions that maximize variance. The authors argue that this results in more interpretable directions. The method is applied on top of a VAE model (OPTIMUS), and the authors argue that different directions discovered by PCA correspond to interpretable concepts.-------------------Strengths-------------------- The method is simple, and can be applied on top of existing text VAEs.- Learning interpretable and controllable generative models of text is an important research area, and this paper contributes to this important field.-------------------Weaknesses-------------------- There are only mostly qualitative results presented. While I agree that performing quantitative results is difficult with this style of work, the authors could have (for example) adopted methods from the style transfer literature to show quantitative results. These metrics include perplexity (to see how fluent the generations are), reverse perplexity, and style transfer accuracy (this may not be applicable since there is no ground truth "style" in this work, but the ground truth style could be heuristically defined for some transformations, e.g. for singular/plural transformations).- Human evaluation seems nonideal since it is only tested on 12 people.- The generations are actually not so good in my opinion? E.g. many of the generations in the appendix are ungrammatical and/or semantically nonsensical.  Again, metrics such as perplexity could quantify the fluency of generated text.- The method is only applied to one text VAE mode which specifically uses BERT/GPT-2 , so it is not clear if this will generalize to other models (e.g. models trained from scratch).-------------------Questions/Comments-------------------- In Figure 2, are these the top 4 principal directions? If not, how were these directions discovered?- "It is known that variational autoencoders trained with a schedule for the KL weight parameter (equation 1) obtain disentangled representations (Higgins et al., 2016; Sikka et al., 2019; John et al., 2019). Since OPTIMUS is also trained with KL annealing, canonical coordinates in its latent space are likely to be disentangled." I believe this is only valid for beta > 1 so it is not really applicable here. This paper presents a PCA-based latent variable language model for unsupervised latent variable interpretation.Pros:1. The authors propose to use PCA to extract the principal components of the results and claim them to be interpretable latent variables.Cons:1. The novelty is quite limited. Applying an existing well-known technique to obtain interpretable latent variables is not advancing this domain in the right direction.2. The explanation of latent variable in this paper is self-justified. The self-defined baselines cannot be convincingly conveyed that latent variable are interpreted. And the baselines are quite weak.3. In the quality evaluation, the authors do not show how clearly to modify the discovered latent variable to alter the sentences.Question:1. How do you encode a sentence in a two-dimensional space? Are both dimension probability?2. Other than the current quantitative and qualitative analysis, do you think any other quantitative evaluation will be helpful? This paper proposed to use both fine and coarse grained tokenizations of text to train large language models like BERT. The method is relatively straightforward. The input is tokenized at different two granularities (words and phrases for English; characters and words for Chinese). Each type of tokenized text is passed through BERT layers with shared parameters to generate contextual representations. At the finetuning stage, both fine grained and coarse grained representations of the CLS token are jointly used to predict the target.I was not sure about what exactly was Our Bert (word). Is it not using the BERT WordPiece tokenizer ? If not, which tokenizer is it using ? Also, if it is not using BERT tokenizer, I am very surprised to see it perform better than Google BERT in all cases. Since Google BERT uses a more fine grained tokenization, I would expect it to perform equally well or better. Please clarify this part.There are two baselines which I think are important for evaluation. First is a BPE tokenizer trained on the data you are using. This might resolve the problems with single granularity for both language considered here. Second is the "whole-word masking" approach of BERT. You can use fine grained tokenization but mask out whole words / phrases as the case may be, which might give you the best of both worlds. Without these baselines, I am not convinced that we should prefer Ambert to other approaches.      The paper proposes a method for improving the generalisation of model-based  RL algorithms. The paper proposes that one should learn a distribution of transition models, which they do by training with Dropout to improve generalization in model-based RL. While the paper is addressing an important problem, sadly it lacks in novelty, it is unclear if the results are significant and there is a lack of discussion and comparison to related work in this direction.Questions and comments:1. There is a series of work in model-based RL focused on ensemble methods which leverage a population of transition models throughout training to improve generalization. For example:   * [Kurutach et al., 2017](https://arxiv.org/abs/1802.10592)  which trains a model-free algorithm on purely imaginary data and uses an ensemble of transition models to avoid overfitting to errors made by the individual models.  * [Buckman et al. 2018](https://arxiv.org/abs/1807.01675) which trains an ensemble of transition models to capture the uncertainty over next step predictions. 2. Work by [Kahn et. al., 2017](https://arxiv.org/abs/1702.01182) seems to be also using dropout as an approximation to an ensemble of transition models to reduce overfitting in real-world navigation tasks. It would be helpful to discuss how your work differs from this work?3. It is quite unclear why the latest results of [Ha & Schmidhuber, 2018](https://arxiv.org/abs/1809.01999) with adjusted temperature werent used as baseline. They are better across the board, and the arguments provided in Section 4.1 do not seem highly convincing. Did you try to combine your method with a temperature-variant of World Model?4. While the effects of the dropout probability during inference has been studied in the paper, there is no discussion of why the dropout probability during training has been fixed to 0.05 in all the experiments. Was this as a result of a hyperparameter tuning?5. In Ablation 4.4, it appears that step-based dropout is performing better than fixed episode dropout. Why wasnt step-based dropout selected for the rest of the results? Also, doesnt this suggest that the original motivation for using dropout as a way to leverage an ensemble of transition models isnt strictly true and instead dropout is doing some kind of regularisation here instead?Overall, the problem being addressed by the paper is interesting, however, the contribution isnt particularly novel. At the same time, the experiments are slightly lacking and it is hard to assess the significance of the results, for example seed variability seems high when/if present (it is actually unclear what the reported variability corresponds to in Table 1 & 2) and most figures only show a single curve without any error bars. Hence, I believe this work is not ready for publication at this time. However, I encourage the authors to improve their work by a) discuss the relevant related work e.g. ensemble methods b) discuss differences to [Kahn et. al., 2017](https://arxiv.org/abs/1702.01182),  c) compare results to an explicit model ensemble (e.g. different initialization of the model) d) report results across multiple training runs,  e) explain the choice of hyperparameters and experimental setup & f) improve the clarity of the paper overall.  The submitted paper describes a very nice featurization library, AutoMunge, that converts NLP into features suitable for NNs.It's clear that the authors of the library have put a lot of thought into its construction, and it looks very useful.However, ICLR is about /learning/ representations, not about feature engineering. So this is off-topic for the conference. To make it on-topic, the authors could, e.g., compare using standard word representation techniques vs AutoMunge on a set of NLP tasks using some popular modern NLP architecture (perhaps BERT?). That would be a really interesting paper. The paper under review proposes to generalize MMD for discrete random variables whose labels take value in $\mathbb{R}^k$. They propose to estimate these generalized probability kernel distance using empirical estimators. Their properties are studied for two particular examples, namely a kernelized Stein discrenpancy and polynomials versions. Consistency and bias of both estimators are studied and bias corrected.The paper has numerous typos, with approximate English and lacks of rigorousness when introducing mathematical concepts; multiple notations are never defined. On the theoretical side, the setting on which the contribution relies on is quite strange: in general, labels of discrete variables does not relate to any notion of distance/ordering as in a classical RKHS setting making the relevance of the methodology quite questionable. These point with other technical issues are summarized in following remarks:## Major points* As mentioned above, it is quite rare in a discrete setting that the labels lies in $\mathbb{R}$ and satisfies a notion of distance. The authors should better motivate this setting by giving at least on relevant example, either theoretical or practical, where such structure is relevant.* What does a Stein operator in a discrete setting means? There is a diffenretial operator in Definition 5 that is difficult to generalize and apply in a discrete context.* The symmetric KDSD introduced in Definition 6 is claimed to be a probability kernel, but the proof that is satisfies Definition 2 is not given.* The so-called polynomial probability kernel seems to obviously require $l=k$ to satisfy conditions of Definition 3, i.e., that $|\phi(q,p)\| = 0$ implies that $p=q$. It can be called a probability kernel only in such condition.\end{enumerate}## Minor pointsThe paper has numerous typos and imprecisions; a subset of them are listed here.* p.1: 'underline' should be 'underlying'.* p.1 when using 'i.e.', always write ',e.i.,'.* p.1 and onward: there is always a space missing before each parenthesis.* p.1: Yi & Along (2020) should be a citep and not citet.* p.1: 'remain futher study' should be for instance 'is left for future work'.* p.1: KSD is not defined yet.* p.2: 'the introducting' is `the introduction'.* p.2 'in representing; is not right.* p.2 Is $[k]$ the sample space? If yes what is $\{x_1,\dots,x_n\}$? A sample? What is the probability measure $v_i$? Do you mean the probability that $X$ falls in $v_i$?* p.2 Definition 1: 'Given that distributionS p and q belong ... distributionS with...'. Also 'map' is singular. What is the 'function space' that you refer to? Also where does this definition comes from? Please give proper referencing.* p.2: Why is there a line break right at the start of 4.1?* p.2: what is an 'instance of integral probability metric'?* p.2: last equation $\mu_p$ is not defined, the product opertor $<.,.>_{\mathcal{H}}$ is not defined. $\mathcal{H}$ is not defined.* p.3: what these 'embedding functions'?* p.3 the RBH kernel is not defined.* p.3 second equation: what is $\phi$?* p.3 Definition 2: the index the sample space should be k, i.e., $y_1,\dots,y_k$ if it refers to the distributions and $n$ for a sample. Here it should be $k$ as it is written distribution.* p.3: 'examINE', 'members'.* p.4: the 'brief' proof provided here is only working for discrete variables, while the proof in Gretton et. al deals with continuous variables.* p.4: what is the 'term above'?* p.5 'illustrate'* p.5: there should not be such a thing as an 'art' in science. If you raise that question, then you should formally discuss this topic (choice of optimal $\phi$).* p.5 Second equation: what are $x_s$ and $x_t$? Notations between this equation and the next are not consistent ($n$ is paired with $x$ and then with $y$ in the next equation).* p.6: what is this so-called 'same property'?* p.6: pmfs is never defined.* p.6 Definition 5: notation $\mathcal{A}$ is never used. $s_p$ is not defined. $\Delta^*$ is not defined. If the latter is a differential operator, what does it means in the context of discrete random variables?* p.6: what is 'form 5'?* p.7: what forms 5 and 6?* p.7: Theorem 5: operator $L$ is not defined. A dot is missing. Are $p$ and$q$ density functions of pmfs?* p.7: 'preliminary results'.* p.7: what does 'justing' mean?* p.8: what is requirement $2$? The paper proposes to solve the forward and inverse problems of PDEs simultaneously (that is, learn both the governing differential, and the forward solution surrogate). This is a dramatic and bold idea, but the paper does not explain why this combination would be a good idea, or whats the benefit. Its unclear why is the solution surrogate useful. The experiments show that this provides good results on image classification, but the PDE motivation is lacking. The resulting model does not require any integration, which is a major advantage. However, the paper should be more transparent on discussing the disadvantages of lack of integrals. Without forward solutions, the model is at risk of cumulating errors over time.The paper seems to borrow its ideas almost completely from Raissi2019, and differences to it needs to be explicated. It seems that this paper takes Raissi2019 method and adds a loss function suitable for image classification. Given that none of the experiments are actually about learning PDEs (they are all image classification), this paper is very misleadingly titled. The method is also very incremental, and seems more like an application of Raissi2019 than an independent research work. Its also difficult to see why one would use a PDE for image classification at all. Labelling small images is already effectively a solved problem. I fail to see the PDE'ness of images.This work also does not actually learn a neural PDE, since the governing equations are assumed to be 16-parameter predefined function, and not a neural function. Only the solution surrogate seems to be a neural network. The title is then misleading also in this regard. I also have hard time seeing why not develop this bidirectional method for ODEs first? The paper should do this as an ablation study to first show that it grants some benefits in the simpler ODE case. The paper is written in a confusing manner, and lacks presentation polish (typos, language mistakes, strange figure order, lots of dubious statements, etc). The presented methods are also not introduced properly, and it seems that the reader needs intricate understanding of Raissi2019 first.The experiments show that the PDE-net, ODE-net and ResNet are all equally good at classifying MNIST and SVHN (with no significant differences). The results are missing log-likelihoods, standard deviations and training time analytics. Its difficult to see whats the benefit of the method here. In Tiny-experiments the comparison target of MobileNet seems arbitrary. Why compare to a mobile phone -optimized classifier, given that PDEs surely are far from ideal on such settings. There are no large-scale image classification tasks, nor standard image baseline methods (alexnet, vgg, wresnet). The ResNet comparison is also missing from Tiny. The out-of-distribution experiments are excellent, and show the PDEs improve clearly from ODEs and beat MobileNet. These results are very interesting, and potentially significant. Here more exhaustive experiments should be done, and comparisons to other augmentation methods performed. The authors should also try to give insight why the PDE is more robust to perturbations.Overall the paper presents an incremental improvement to PDE learning with limited novelty, with unclear presentation, unclear motivation and mixed but partially promising results. The paper needs more work, and should be reworked to be more independent of Raissi2019, and refocused (incl. title) towards the promising application of image robustness, and *why* the PDE are more robust than ODEs in this setting.Technical comments:o I do not understand what the dimension d means. It does not seem to a dimension at all, but instead a state vector of the state space? The notation is very misleadingo Neural ODEs do not have particularly small number of parameters (they are often applied in very simple cases or in small latent spaces, but here other NNs would be simple as well)o what is procrastinate?o the paper confuses layers and time to be equivalent, this is not true in neural ODEso where is eq 1 coming from, and why is the model only restricted to 3rd order (monomial) differentials? Surely a more general PDE definition could have been usedo general purpose PDE solvers do not exist: surely they exist, but are too slow to be practicalo eq 6: what is h?o eq 6: why is Raissi2019 performance studied here? I fail to see whats the relevance of repeating someone elses work. Is there some novelty here?o fig4: all methods seem to have very poor fits, given that this is a simple 1D problem with massive amount of data. o sec3: h(d,t) should be a function of h0 as well, and its unclear if this is a true or proxy solution. This paper introduces a way to annotate a glimpse sequence for an image. It uses BAYESIAN OPTIMAL EXPERIMENTAL DESIGN to achieve this. Using the obtained annotations, hard attention can be trained with a partially supervised way.My concerns mainly focus on the motivation and evaluation. Specifically, the authors claim using hard attention to do image classification may enable the use of modern approaches to computer vision in low-power settings such as mobile devices. But from the paper (besides the computation of glimpse sequence is computationally expensive.) at the end, the proposed method needs to run the glimpse network several times, and the structure of the glimpse network is similar to standard classification network, which needs only a single forward pass on the whole image. I'm doubt adopting the proposed approach can truly lead to a low power solution.  Image classification seems like an unsuitable task to show the benefits of hard attention.  Even if we would like to test on image classification, we need to include more baselines, such as standard convnets and compare them to the proposed method in terms of computational complexity and effectiveness. Other issues:  Stochastic image completion involves creating an empirical image distribution with 1.5 million images. Will this lead to an unfair comparison or data leakage?  It's better to include standard datasets for image classification such as ImageNet, CIFAR, stc.   It's interesting to see whether a hard attention method trained with REINFORCE will obtain a similar glimpse sequence and the proposed annotation method. The paper "Learning Active Learning in the Batch-Mode Setup with Ensembles of Active Learning Agents" proposes to deal with the problem of active learning via a weighted ensemble of agents. Each agent sequentially selects data to include in the batch to be labelled according to specific heuristics. Finally, the various agents are weighted according to parameters found by a gradient-less approach. While meta-learning of active learning is a very interesting and useful problem, I find the contribution of this paper too weak for a conference as ICLR. The approach is rather straightforward (only a linear combination of different heuristic agents), the experimental results are not fully convincing (there is no real gap between the ensemble and the best individual approach, so using the best agent at training time is maybe a strong alternative) and the paper lacks clarity and details. From my point of view the related work section should shortened to focus on mainly important aspects related to the presented work, a better detailed view (more formalized, less algorithmic) of the approach should be given, and some theoretical insights should be given before it could be considered for publication in a top machine learning conference.     ## SummaryThis paper proposes a replay-based continual learning method and applies it to physiological data.The proposed method consists of two heuristics to manage the replay memory:- Learn a weight parameter (task-instance parameter) for each data point by minimizing a special loss function, and store the data with large weight parameters.- Periodically compute epistemic uncertainty for all data in the replay memory, and replay the data with large uncertainty.---## ProsThis paper points out the importance of continual learning in the medical field.---## Cons### Weak contributionSince this paper deals with the continual learning of a specific type of data, I think there are two directions in which this study can be meaningful:- Test various CL algorithms in the new domain, determine which method performs better than others, and analyze the reason.- Use domain knowledge to design a new algorithm that performs particularly better on the domain.However, I think this paper belongs to neither of the two. There is no domain-specific component in the algorithm, and only two baselines are compared.If the method is general enough to be used in other domains, I think the authors should have also tested standard CL scenarios such as Split-CIFAR10 or Split-CIFAR100.### Lack of theoretical justificationThe proposed method is mostly heuristic and does not have a theoretical basis. Especially eq. (2) seems too arbitrary. Since this paper's empirical evidence is weak, I think a theoretical analysis is necessary to support the algorithm's efficacy.### Vague and improper definition of time incremental learningThe authors seem to propose time incremental learning for the first time, but the definition is too vague:> Time Incremental Learning (Task-IL) - the same dataset and prediction problem are used for each task, however the time of year at which the data were collected differs from one task to another. Such seasonality is most common in healthcare applications.The authors should specify what changes over time. However, I think there is even a bigger problem:- If the input distribution changes, it is not different from domain incremental learning.- If the output distribution changes for the same input, it is not continual learning. The model *should* forget old tasks and adapt to new tasks.- If both the input and output distributions change, it is class incremental learning.Therefore, I think time incremental learning is not a novel branch of continual learning. The corresponding experiments should better be reformulated as domain incremental learning or class incremental learning.---## Overall evaluationI do not think this paper proposes a novel idea with either a solid theoretical basis or strong empirical results. Therefore, I recommend rejection. Summary: The paper seek to improve KG representation (e.g. for link prediction and question answering) by combining logical reasoning (logical rule templates) with statistical methods (TransE). Rules are mined from the KG using AMIE and recursive backward steps are taken, using the mined rules, to determine if a fact is true.Quality: the paper is very difficult to read, to an extent that puts it below the quality I would expect for a top conference.Clarity: As above, the paper is very difficult to read, and even pushing through grammatical errors, the algorithm itself is very difficult to understand. In many cases, terms are used first and defined later, making it hard to comprehend and requiring a lot of going back-and-forth. As one specific example, shortly after definition 2.2, H_{s_cur} is compared to \phi, which hasn't been given a value (the clearest explanation is given in an appendix - in my view this should be in the main body since it essential). A footnote says that will be initialised to L_{s_o}, but that value (including the meaning of s_o) is not yet defined. As another example, \Delta_path appears in Eq 1, having not been previously defined (including any notion of "path"), it is then defined loosely in terms of "rules used in the extension ...", but this is fundamental to the algorithm and should be stated concretely (perhaps built iteratively with each extension step).Originality: The model uses a pre-existing rule mining algorithm (AMIE) and KG model (TransE) as a basic triplet score function within an exhaustive logical search framework. Using BFS to find supporting predicates does not seem particularly novel, but the means of combining score functions seems to be.Significance: Hard to say given the difficulty understanding the paper overall and its apparent unbounded complexity.Pros: The subject tackled, i.e. combining logical reasoning and statistical learning, is an important area of research.Cons: - The paper is insufficiently clear and the poor grammar makes it too difficult to comprehend to be acceptable at a top conference. - Many other works combine logical rules and KG embeddings but are not compared to. It is unclear why not. The KG embedding models (that do not use logical rules) compared to are not representative of state-of-art, e.g.   RotatE (Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.)  QuatE (Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. Quaternion knowledge graph embeddings. In Advances in Neural Information Processing Systems, pp. 27312741, 2019)  Tucker (Ivana BalaÇzevi´c, Carl Allen, and Timothy M Hospedales. Tucker: Tensor factorization for knowledge graph completion. arXiv preprint arXiv:1901.09590, 2019.)  MuRP (Ivana Balazevic, Carl Allen, and Timothy Hospedales. Multi-relational poincar´e graph embeddings. In Advances in Neural Information Processing Systems, pp. 44654475, 2019.)- The approach of breadth first search has the potential for exponential explosion and seems something that embedding approaches avoid by capturing these relationships implicitly. In that respect the proposed approach appears retrograde. The paper proposes a framework (EM-RBR) for doing Knowledge Base (KB) completion. Instead of the direct triple score from an embedding based method, EM-RBR allows the triple score to be calculated as a composition of the scores of the rules mined from the KB. EM-RBR uses a BFS type algorithm that recursively searches for reasoning paths connecting the triple while also updating the score.  The authors show that EM-RBR when used as an addendum to a translation-based embedding method (such as TrasnE, TransH) is able to outperform them. They show their results on FB15k and WN18. Issues:- The method section is not clearly explained and it was hard to understand the algorithm- The algorithm performs BFS on every example which is bound to increase the inference time, the authors claim that the approach is efficient and so should provide some analysis on that- The paper talks about other path-/rule-based methods, the authors should compare their approach with them (like Neural LP, MINERVA, DeepPath, etc). (Approaches that combine embedding based and rule-based methods are not novel, example: https://arxiv.org/pdf/1506.00379) - The authors evaluate their work on FB15k and WN18, which have been shown to have several issues (leaking of test triples as inverses, etc). Some of the current benchmarks for the task are FB15k-237 (https://www.aclweb.org/anthology/W15-4007.pdf) and WN18RR (https://arxiv.org/abs/1707.01476). - The authors show results on a version of FB15k they create called FB15k-R. The only thing they talk about it is   >  We create FB15k-R, a subset of FB15k, which contains 1000 testedtriplets that have rich rules to take reasoning"- **This is a major problem**. There is no explanation regarding how this subset was chosen. It seems that it was chosen specifically because their model performed well on it. This is blatant cherry-picking.  On the basis of the above points, I recommend rejection.  This paper considers the stochastic proximal point algorithm for solving nonconvex nonlinear least squares optimization problems. A linearization strategy is used to accelerate the procedure and in each iteration the algorithm works by solving a linear system. Some convergence analysis for the proposed is present. Some experiments have been conducted.I have the following comments.1. In the proposed algorithm, the authors only take one example instead of a batch of training examples to construct the gradient. This strategy often results in much large variance and slow convergence in practice.2. The results in Proposition 1 does not imply the convergence of the algorithm. The theoretical analysis is incremental. 3. The numerical comparisons are not sufficient.  The authors should include the comparisons with state-of-the-art second-order optimization solver such as K-FAC.  This paper studies the stochastic proximal point algorithm (SPPA) for large-scale nonconvex optimization problems. The authors propose to use Gauss-Newton to perform the proximal update in nonlinear least squares and L-BFGS or accelerated gradient for generic problems. The authors derive the convergence of SPPA to a stationary point in expectation for nonconvex problems, and perform numerical experiments to showcase the effectiveness of the proposed method compared to SGD and its variants. The paper is generally clear, yet the convergence analysis is mainly based on adapting Bottou et al. (2018). While the proposed methods could be significant additions to stochastic optimizers in deep learning, I found the study of the current paper is insufficient; see the cons below. Pros: - It is well-known that the proximal point algorithm (PPA) converges faster than gradient descent (GD), and the same holds for their stochastic counterparts. One advantage is that the step sizes in PPA and SPPA can be larger than those in GD and SGD, which can speed up convergence. The proximal steps in PPA and SPPA are however hard to perform for generic (nonconvex) problems since the proximity operator of the objective functions usually do not have closed forms. The proposal of the authors to perform efficient inner-loop optimization schemes like Gauss-Newton and L-BFGS allows approximation of such proximal steps, without much computational burden added. Cons: - Theory:    * I found that Assumption 3 is too strong and do not think it is a standard assumption. Otherwise the constant $ c $ can be very large. This also leads to a question of why using the upper bound $ c $ is the second part of the RHS of (20) but not the first part?    * Also why $ \sqrt{\lambda_t} $ instead of $ \lambda_t $ in (20)? If I did not misunderstand, it is derived from (14).    * As $ c $ and hence $ C $ can be very large, the bounds (9) and (10) in Theorems 2 and 3 can well be vacuous.    * Also the quantifier in Assumption is missing (for all $ i\in \lbrace 1, \ldots, n \rbrace $?)     * Another pitfall of the theoretical results of this work is that the convergence analyses of the proposed SPPA-LBFGS, SPPA-AGD and SPPA-GN are all missing, especially since this work considers nonconvex problems. - Experiments:    * To showcase the proposed methods are really comparable to or outperform methods like SGD or Adam, numerical experiments should be performed on data sets of larger scales and much deeper networks. In particular, the regression data sets in the paper are so small that the gain of the proposed method over other baselines are so marginal. Typos: - Theorem 1: do you mean the limit is equal to 0 or is finite? I guess something is missing. - Proof of Theorem 2: $ L(\theta_1) - \mathbb{E}[L(\theta_{T+1})] $ instead of $ L(\theta_0) - \mathbb{E}[L(\theta_{T})] $- (10): the LHS of the inequality should be $ \alpha_t $ instead of $ \alpha $ **Summary**This paper introduces a library that implements 1 infectious disease model and 3 agents in a gym environment. The authors provide analysis for a covid-19 lock down scenario using their library.**Strengths**The paper addresses an important and timely topic. The paper is easy to read and follow. The authors have open sourced their library and it seems to be well documented.**Weaknesses**The main weakness of the paper is the strength of the contributions. The authors main contributions are a gym environment for a specific epidemiological model.Unfortunately, the novelty is somewhat lacking. As the authors mention in the conclusion section this line of work has been heavily researched. There are many infectious disease models and simulation environments already out there. E.g. a gym like interface has even been already open sourced earlier this year https://github.com/google/ml-fairness-gym/blob/master/environments/infectious_disease.py .In terms of impact, the number of environments and agents is also quite limited at this stage. Its also unclear if there is likely to be adoption by serious policy makers. Absent such impact statements it remains as one of many simulation frameworks out there.**What could make the paper better**a) Many more environments and agents need to be implemented such that this library has the potential to become the standard for infectious disease simulation.b) A lot more analysis comparing agents and disease scenarios that truly unearth interesting scientific observations. c) Real world impact statements of adoption by policy makers and governments. The authors investigate different tokenization methods for the translation between French and Fon (an African low-resource language). This means that they compare different ways to construct the input and output vocabularies of a neural machine translation (NMT) system. They further propose their own way to create those units, based on phrases, which is called WEB.The NMT system the authors use follows Bahdanau et al. (2015): it is a GRU sequence-to-sequence model with attention. The dataset they use has been created and cleaned by bilingual speakers and consists of roughly 25k examples (this is a really small dataset for NMT, so the authors are taking on a really hard task!). WEB works in the following way: after phrases have been found automatically, bilingual speakers analyze what the longest phrases which correspond to translated phrases in the other language are. Only the longest phrases for each example are kept for the final vocabulary. The authors show that WEB improves the performance in both translation directions by a lot on all metrics, clearly showing that the work they invest into creating the vocabulary pays out. Thus, I think this work is important to be able to provide speakers of Fon with a functioning translation system.However, I am unsure if this work is suitable for a machine learning conference. While the overall goal of this work is to create an NMT system, the main contribution is the manual cleaning of the dataset and semi-manual creation of the vocabularies. I would recommend to the authors to submit this paper to a conference with a stronger focus on NLP and NLP resources (maybe LREC)? I further want to emphasize that I think work like this paper is incredibly important and the authors shouldn't feel discouraged. Importantly, the manual labor needed for WEB has been a lot and it's obvious that it helps for NMT. I just don't think that this paper is a good fit for ICLR.Minor point: has the creation of WEB access to the test data? If so, the authors should change that (or collect new test data?) to ensure a fair evaluation.  In this paper, the authors develop a new policy gradient method to reduce the variance in the gradient estimations.In the commonly used policy method, the bias is a function of the state. e.g., V(x_t). In this paper, the authors propose to use bias V(x_t,\phi_t) where \phi_t is a statistics of future events such that \phi_t is conditionally independent of the action at time t.The authors show that using such statistics in V(x_t,\phi_t) results in a reduction in the gradient estimate used in policy gradient methods. Later, the authors also show that their method performs well in practice.There is a set of problems with the paper's presentation, which resulted in the negative evaluation.The analysis in the paper is straightforward and also easy to follow. However, I could not find how the proposed algorithm learns the \phi.I encourage the authors to improve the clarity, presentation, and language in this paper. 1) I did not get what the authors mean by luck or skill. These terms do not seem to be coherent terms in this paper. I highly encourage the authors to rethink such usage. Unless the authors mathematically define it in the paper. 2) "Another issue of model-free methods is that counterfactual reasoning, i.e. reasoning about what would have happened had different actions been taken with everything else remaining the same, is not possible." Can the authors clarify it? Why is it not? When I learn a Q function, that tells me what would be the expected return if I choose other actions following the same policy, right? If you mean evaluating other policies is not possible, I still doubt the statement is true. 3) "Given a trajectory, model-free methods can in fact only learn about the actions that were actually taken to produce the data, and this limits the ability of the agent to learn quickly."Can you clarify this? I can use function approximation based methods, and then, the first part of the authors' statement is no longer true. The second statement is inaccurate since the author did not quantify with respect to what method the quickness in learning is compared to.4) "actions taken by the agent will only affect a vanishing part of the outcome". What do the authors mean here? What the vanishing part of the outcome refers to?5) "mak- ing it increasingly difficult to learn from classical reinforcement learning algorithms", what the authors mean by learning from classical RL algorithm? and why the authors think a better credit assessment is needed and is the way to go. What motivates the authors to state the issue is the credit assignment?6) "Second, removing the value function V (Xt) from the return Gt does not bias the estimator and typically reduces variance". Would the author refer to a paper stating that removing the value function V (Xt) from the return Gt typically reduces variance?7)"This estimator updates the policy through the score term; note however the learning signal only updates the policy À¸(a|Xt) at the value taken by action At = a "I am not sure I understand this sentence. Is À¸(a|Xt) the policy, or it is À¸. Do authors have a different model for each state and action pair? Even in that case, since the need to normalize action probability, changing À¸(a|Xt) will affect other À¸(a|X) as well. Therefore, I am not sure what the authors mean here.8) Distinction between single action and all actions.In both propositions 1 and 2, it seems that the learning signal is provided for both actions. It is not clear to me how the authors make the distinction. Especially here "The policy gradient theorem from (Sutton et al., 2000), which we will also call all-action policy gradient, shows it is possible to provide learning signal to all actions,".I am not sure what the authors mean. The authors state that"A particularity of the all-actions policy gradient estimator is that the term at time t for updating the policy À(a|Xt)(Q(Xt, a) depends only on past information;" but it seems to me that Q is a function of the measure on the future. Isnt it the case?9) To motivate the usage of phi, the authors talk about a scenario in a soccer game, which again I could not find useful, especially when they bring luck and skill. The authors state that "When using the single-action policy gradient estimate, the outcome of the game being a victory, and ,assuming a ±1 reward scheme, all her actions are made more likely". How is it possible that all actions become more likely? when their probabilities should be sum to one?I am not sure again. Are the authors talking about using one trajectory for all the estimates? The update in proposition 1 shows that in the case the agent action does not change the outcome, then the gradient is zero.10) The authors state that"In contrast, if the agent could measure a quantity ¦t which has a high impact on the return but is not correlated to the agent action At , it could be far easier to learn Q(Xt , ¦t , a)."It is not clear why learning Q(Xt, a) is harder than Q(Xt , ¦t , a). So far, Q(Xt, a) seems an easier function to approximate and most likely needs a fewer sample to learn Q(x, a) than something presumably complicated like Q(x, \phi , a).11) In section 3.1, I strongly encourage the authors to elaborate more clearly on what they do. Is W a scaler? if yes, then how F can be constructed?Do you draw U,V,W each time step??12) Aside from many unclear statements in this paper that the authors can easily address, I could not find how the authors find \phi. Since this is the main key component of the paper, it would be great if the authors could explain it in depth. I also could not find it clear in the appendix. 13) I strongly encourage the authors to expand their study on plain MDP before getting to the POMDP complication. It is not clear where the performance gain comes from. The paper considers the problem of learning a value function in a deterministic MDP and proposes a heuristic to reduce the bias of value estimates under the greedy policy. For this, they consider the class of Double Q-learning algorithms and describe a setting where estimation under this algorithm can lead to multiple fixed point solutions. They propose to control underestimation bias by estimating the next-state value as the maximum of either the current next-state value or the value of some trajectory in the replay buffer. They also propose another version that uses clipped target values. The two algorithms are evaluated on several Atari games and show, in some cases, improved transient behavior over double DQN. Controlling for bias due to function approximation is an important problem, and one where general solutions could have a potentially large impact for RL and AI. Giving this subject the attention it deserves will require an understanding of the bias problem in stochastic settings. Many will be curious about what can be said about this problem and its solution(s) using formal mathematical language. The multiple-solutions hypothesis is a plausible approach to gaining a better understanding of bias, but there could also be other angles to explore. I think the current research this paper presents should be broadened to understand not just deterministic environments, but stochastic ones as well. The experimental results should also be questioned: why is there little to no asymptotic performance gain using the proposed method if it indeed produces an estimate with less bias? There are more questions the community would wish to answer with a paper like this; Where does underestimation bias come from, and how can it be controlled in general? Currently, however, the presentation and technical material are not sharp enough to answer such questions. Therefore, it is important that the authors continue to mature this work so that, when it is ready for publication, it will make a significant contribution. Summary: The authors propose a new framing of the matrix local low rank representation technique, identifying 3 different subproblems and claiming most existing approaches sonly solve some, then proceed to propose a novel method that solves all three, evaluating on synthetic data.Review:* The paper is confused and hard to follow. The mathematical notation is inconsistent (e.g. what is $dist1$, $dist2$ in related works? what is $K_\Omega^h$?). There are many grammatical errors. The proposed algorithm is complex and made of several different components that are not appropriately justified. No ablation study is performed. The experimental setup is not described appropriately(e.g. you mention a feed forward neural network in section 3.4 but don't report any parameter such as number of layers or size). * The evaluation is limited and not very supportive of the author claims, especially for real data more experiments would be necessary. The section on scalability lacks quantitative measures of the claimed good performances. Summary: While the idea could potentially be promising, a more clear explanation and more attentive evaluation are fundamental to assess the methods applicability and value. The paper presents a method to verify if a NN is performing as expected in sensitive applications. Although the general area is very important in machine learning, the paper is not very well presented: The problem is not well stated, the approach is not very clear, and the results are not well justified.- The presentation and the writing of the paper should be improved. Unfortunately, with the current format it is hard to glean the idea of the paper. There are some typos (e.g., 'bu' in page 3, 'plot' in page 4, etc.).- There are some concepts that are not defined early on and maybe never in the paper. For example, what is the problem that the paper tries to solve mathematically? It is not very clear. What is the mathematical definition of a preimage?- The authors say: "Preimages are most insightful and useful when the inputs and outputs have definite interpretation application areas where the need for massive networks is less". Its hard to fully understand but it seems that the method suffers scalability issues. Can this be formally analyzed? What is the complexity of the algorithm in time and space? Why is there a scalability issue? Is it a fundamental problem? How does this limit the scope and applicability of the method? Also, what does "definite interpretation" mean?- The NN used in the experiments are very tiny. I would consider experiments that reflect more realistic situations in the real-world. Current setup significantly limits the scope of the method.- It is not clear how to verify the performance of the method. Results in Figure 1 and 2 does not show us the quality of the method, is it doing good or bad? I found the results in Figure 1 surprising as the moon data is fairly symmetric while the preimage is biased towards one class. Is there a reason for that? The paper details a way of investigating the space of pre-images that lead to a particular output from a ReLU layer, with the goal of using the inverted representations as a way to understand the deep neural networks. Three experiments are proposed where the authors claim that the computed pre-images help interpret the network decision making. Overall the paper is interesting, however I am not certain of the novelty as some related work is not discussed. Additionally, although the practical application of the method is interesting, the clarity could be improved for the last experiment. Positives:* Understanding the invariances of neural networks can potentially lead to more interpretable models, and one way to investigate this is by looking at the preimages for a network. * The paper is a nice mix of theoretical results which lead to practical applicationsQuestions and Concerns:* The authors state that maxpool can be rewritten in terms of a linear component and a ReLU, but this is non obvious. If this is true, a mathematical formulation should be explicitly included in the paper. * The paper is missing some potentially related references. Previous work has investigated how multiple stimuli can get mapped onto (approximately) the same point in recognition networks by inverting the representations via iterative gradient descent (Mahendran & Vedaldi 2015, and recent work including invarianced based adversarial examples in Jacobsen et al. 2019 or model metamers in Feather et al. 2019). How does the proposed preimage computation help improve model interpretability beyond this previous work, especially given the authors statement that the method is intractable for large networks? * The paper does not discuss invertible networks which have a bijective mapping between the input and output (ie Invertible Residual Networks in Behrmann et al. 2019). Discussing this work seems relevant if the goal is to make models such that one can start with hypothetical outputs and understand the inputs that lead to them. * The final example of using this method in practice for ACAS systems is interesting, but it is difficult to follow what success would mean for this experimentMinor points: * The following sentence on page 3 seems to be missing something Preimages are most insightful and useful when the inputs and outputs have definite interpretation  application areas where the need for massive networks is less..* There it a typo in the last sentence of page 3 (bu->but) Summary: In this paper, the authors investigate the inner-loop optimization mechanism of meta-learning algorithms. The analysis shows the effectiveness of the multi-step adaptation and (1) the key of meta-learning is how to design a well-differentiated classifier. They then propose Random Decision Planes (RDP) and Meta Contrastive Learning (MCL) and achieve comparable performance with existing methods.Pros:1. Empirically investigating the performance w.r.t. the change of optimization mechanism on both head and body is very important in the meta-learning field.Cons:1. My major concern is about the contribution of this paper.  - The discussion is interesting. But most of the analysis results is widely accepted. For example, adapting the head layer improves performance. One claim  "as the model converges, the body accuracy even decreases in the first few adaptation steps" is not well-explained. - The final goal of this paper is to design a better metric for meta-learning (feature mapping function + differentiated metric design). I think the goal is the same as metric-based meta-learning, which is not new for me.  - In addition, the baselines and experiments are not sufficient to support the goal. Especially, the proposed methods (both MCL and RDP) only show comparable performance compared with metric-based methods (e.g., MetaOptNet). It would also be better if the authors can add more metric-based baselines for comparison (e.g., [1],[2]). Moreover, they should also involve metric-based methods in the efficiency comparison section (Appendix C.3).Thus, I feel the overall contributions are not enough to be accepted. 2. Besides the Cons 1, if the authors focus on gradient-based meta-learning. Similar to ANIL, more experiments on different types of applications are supposed to conduct. For example, the experiments on regression and reinforcement learning tasks.3. The paper is not well-written. Here are some comments: - It would be more clear to formally formulate the MCL and RDL with more notations and equations. - Most figures are not clear. It is better to replot them with larger font size in legend, thicker lines, etc.[1] Ye et al., Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions. CVPR 2020.[2] Wang, Yan, et al. "Simpleshot: Revisiting nearest-neighbor classification for few-shot learning." arXiv preprint arXiv:1911.04623 (2019). This paper studies gederated Generative Adversarial Networks (federated GANs).In particular, the authors propose a new method, UA-GAN, which is claimed to be better than earlier approaches.I have several concerns.  First, the writing can be improved significantly.  Second, the statements in the text are often rather vague.  This makes it hard to understand what are the results and to verify their correctness.   E.g., theorems are formulated either vaguely or unprecisely.Moreover, the theorems refer to "optimal discriminators", however there is no guarantee that one can find such an optimal discriminator or that one can even verify whether a discriminator is optimal.  (In fact, the paper doesn't define the term "optimal" precisely)   Even if it would be possible to learn an optimal discriminator, then there is no bound on the amount of time this would take (as that depends on various parameters of the learning problem and the learning algorithm).  The paper contains more vague statements, e.g., Remark 1 claims the method preserves privacy, but doesn't define what information is kept private.   For example, it seems the size of the private datasets is needed for the central weighting and hence made public.  Moreover, there is no proof that from the information leaving the individual centers nothing about the sensitive data can be inferred.  For example, from the point of view of differential privacy, even if only aggregates are revealed, if their revealed value is exact then probably (epsilon,delta) - differential privacy is not guaranteed.In conclusion, it is hard for the average reader to understand the paper due to a lack of precision, and the paper insufficiently specifies definitions, assumptions made, and precise formulations of results.Some details:* The first line of the abstract suggests that GANs are federated networks, while the second line of the abstract correctly states that this is called "federated GAN" and the first line of the introduction correctly describes GAN as generating realistic data.* Algorithm 1: "Eq. equation 2." -> repetition* last line page 3: "algorithm equation 1" -> "Algorithm 1"* top of page 4: the generator G(z) seems to depend on an argument z of which the nature is not revealed immediately.  Later, Equation (2) suggests that z can be drawn from \mathcal{N}(0,1) and hence is a real number.  It is unclear why the argument of G would be just one-dimensional.* top of page 4: while the word "discriminator" is used very frequently, no precise definition of this concept is provided, nor an explanation of how the discriminators are obtained.  With the help of Eq (2) some readers may be able to guess that D_j must have values in the open interval (0,1).* Definition 1: as the symbol q is already used for distributions, it is preferably to not use it for "a probability in (0,1)" too.* After definition 1: "The central idea of UA framework" -> "framework" needs an article* just before Section 3.1:  We will provide error bound -> ... an error bound* Theorem 1 uses "Jenson Shannon divergence loss", which isn't defined in the paper (and no definition is cited).  "Jenson Shannon divergence" is a somewhat well-known concept in probability theory, but even for those knowing this it is unclear how to get from it to "Jenson Shannon divergence loss".* Equation (6) in Theorem 1 seems to give an expression similar to the Jenson-Shannon divergence definition, but doesn't appear a statement the theorem is claiming to be true.  The rest of the sentence refers to q^* and q, but q is a bound variable in Eq (6), i.e., it has no meaning outside the scope of "argmin_q", and q^* doesn't occur in the formula (so why do you say "where q^* = ..." ?).  It is hence hard to parse the theorem statement and discover what is the claim exactly.* A proof is provided in appendix.  However, the proof first says "To prove the theorem, we first introduce the following Lemma", the text next states lemma 1, but never returns to the proof of Theorem 1 (as the next title says "Proof of Theorem 4"). ## SummaryThe authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model.This relationship suggests a mapping between the two models which is imperfect, a loss seems to be derived to reduce this mismatch along the network training. The method is tested on CIFAR-10 and CIFAR-100, and compared with some other methods for converting ANNs to SNNs.## Critical reviewThis topic is potentially important since spiking neural network are gaining popularity. But this paper is clearly badly written and it is extremely hard to understand, both in the math and in the text. I don't think it would help the progress of the field to publish the article in the current form.I tried to read that carefully and got lost after equation (4), the transition to equation (5) and (6) are not clear at all. I do not understand what is an approximation, what is a definition and what is a derivation.Also (5) seems wrong in itself, the authors are trying to approximate a rectified linear network but it suggest that the activity will be equivalent to a linear network (at least when v(T) is small) ? And magically this changes in (6), and a clip non-linearity is introduced ?The Figure 1 seems very encouraging at first, because it suggests that there is a clear and easy mapping between accumulating the spikes and computing a relu. I did not understand where this is appearing in the math and I cannot check whether the intuition conveyed by the figure is correct or not.I was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? This is not shown.  I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa.Since I had not understood the basics of the paper, it was impossible for me to understand the later section about the conversion error. My only take is that it seems wrong at first sight: how minimizing the error in the loss would minimize the mismatch between the network activity? The paper proposed a neural-bandit approach using Thomson sampling for leveraging the DNNs non-linear representation. There is a growing interest in using Multi-armed Bandits with DNN in an end-to-end learning fashion but the difficulty of the understanding how the representation is learned in DNN, makes this problem a very challenging and interesting one. The paper proposes one solution to consider explore-exploit tradeoff in the non-linear DNN space. There is still several unanswered questions in this paper and leave the readers with more confusion than clarify. This is not because the paper is not written properly but the topic is fairly new and a very few works have considered this problem setting. The likelihood matching seems interesting idea but I couldnt find a reason whether it contributed to reward boost compared to using all the history. Some of the claims in the paper is demonstrated experimentally, not theoretically. Please clarify the following questions to understand the paper better.   ### Major Questions:* The realizability assumption A2 doesnt seems to apply here as the history of the decisions (stored in the replay buffer) is used to update the representation. * The paper says $\mu$ is assumed to be fixed in this problem, even before the representation is known. As we update the representation,$\mu$ for the new representation varies and cannot be fixed.* Memory is poorly used without any significance to past decisions that are somehow key to changing the reward distribution or representation or both.* Unlike in the earlier work (Zhou et. Al), no theoretical guarantees have been provided. As in the earlier work, Network width/depth based guarantees can be used to show how the representation affects the realization assumption.* It is unclear why Alg 3 with limited memory outperforms Alg 2 with Full memory (stores all the past decisions) on both linear and non-linear datasets (Mushroom, Financial, Statlog, Epileptic). Please clarify.### Minor Questions:* $\Phi_0$ was introduced before explaining about the likelihood matching. I suggest moving the updates at the end.* It would be beneficial for the paper if an additional experiment on how varying the size of the memory buffer affect the cumulative rewards* The paper claims that the linear models work only for medium-sized inputs (with around 1000 features) due to numerical issues. Can you provide any further analysis on this? Perhaps varying the dim d vs cumulative rewards for linear vs Neural linear would demonstrate this claim. If there is an earlier work that did this, the citation would suffice. This paper shows a new method to hurt the disentanglement of Variational AutoEncoders (VAEs) by only slightly modifying the images. It conducts experiments and shows that the disentanglement metrics drop significantly by this carefully designed perturbation compared to uniformly noise, and the results are consistent across different variants of VAEs and various datasets. These results show that the success of VAEs are based on the structured nature of the datasets, and the community needs to find new datasets to learn the semantic representations. The idea proposed by this paper is interesting. It believes that the disentanglement of beta-VAEs comes from the local PCAs, which coincides with global coordinate alignments, and this strange behavior stems from the assumption of diagonal posterior. Thus the global representation of beta-VAEs is sensitive to local changes. Also, this paper comes with a detailed background description, which is very much self-contained. The analysis of the results is also exhaustive, and it well supports the claim of this paper. However, the methodology contribution of this paper is marginal and the method it adopts seems over-simplified. The method is not technical and is nothing but noise to the generation process, and the design of the noise is not elaborate, either. E.g., instead of a detailed noise on all coordinates, the paper only applies this on the "most important latent coordinate", which is less convincing and lacks an explanation. Also, the conclusions of this paper are already well-discussed by previous works, and the experiments of this paper seem like a verification. The section 3 of this paper revisits many previous results but no new idea is proposed across this paper. More metrics should be used. As described in the paper, various metrics have been proposed: SAPScore, FactorVAEScore, DCI Score, but this paper only adopts the Mutual Information Gap, which is neither typical nor representative. Also, the correlation between these metrics is not very much high according to fig2 of Locatello et al. Given all these metrics are already implemented by disentanglement_lib and the codebase of this paper is based on that paper, it's not too hard to show all of them. Some connections between section 3 and the remainder of the paper are not clear. E.g. this paper argues the importance of local and global alignments in section 3, but it's unclear which part of section 4 corresponds to this point. The term "local expansiveness" is used in section 4 but was not discussed in section 3. Some complex ideas were discussed in section 3 but never touched again. It is said that the modification to the images is barely visible to human eyes, but clearly, the images of fig 3 are heavily blurred and it's easy to tell the difference. This perturbation is not "a small modification" and is human sensible in contrast to other attack/defense papers. The authors should show out a measure of perturbations, e.g. the relative magnitude of epsilon. No baseline works are introduced. The authors only compare their results with a model that works with a random baseline, and it's not convincing.Questions: At the end of section 4, you propose a simplification of the method by a shifting window. Why don't you just apply the modification for the whole image? Is there a specific reason that prevents you from doing so? Presentation suggestions: The paper spends almost 3 pages on background research, but only 1 page on its method. It'd better put more stuff in section 4 and move some content from section 3 to section 2 or do a better summarization. Typo: The third line of section 4.1: it's -> its **Summary**The goal of this paper is to improve our understanding of reward-agnostic metrics drawn from the literature through comparison with human behaviour and task reward. This paper compares two intrinsic reward methods against three baselines on three Atari environments on five metrics, including task reward, a simple metric for human similarity, and three information-theoretic assessments of aggregated observation counts drawn from the literature, which they call task-agnostic metrics. The authors report the correlation between the different metrics.**Strengths and Weaknesses**Constructing a comparative understanding of the many methods for exploration, intrinsic motivation, and curiosity is a vastly underdeveloped area. I think that this paper's goal is to do some of that work, which I see as a strength. However, the experiments are not appropriately designed to provide reliable results and the paper includes substantial errors in understanding the existing literature, and as the paper is essentially an empirical survey, appropriately representing the other literature is critical. Visually inspecting Figure 4, it appears that the results would be completely different if the no-op agent was excluded (and to a lesser extent, the random agent). My concern is that these baselines are categorically different from the agents we are actually interested in and appear to strongly affect the results. For example, without the no-op agent, it appears that the correlation between Human Similarity and Empowerment would be much weaker, and might actually be negative.The Human Similarity metric does not seem to be a meaningful metric for what it is designed to measure. This is of particular concern to me because much of the interpretation of the data relies on comparison with the Human Similarity metric, so using such a simplified metric doesn't seem sufficient. The Human similarity data only considers which observations an agent shares with the human data, without regard for how many times each one visits a particular state. A human might make exactly one observation in a given bucket, and an agent making only one observation in that bucket would receive the same score for it as an agent that returns to that state millions of times. The generalization between state observations created by the preprocessing seems like it can only exacerbate the issue.A similar concern arises when looking at the curiosity metric. Using entropy of sensory input visitation as a metric measures uniformity of visits to states, rather than measuring the ability of the agent to visit as many states as possible. In particular, you can construct examples in which visiting a small subset of states with uniform frequencies results in higher performance on this metric than covering more states, but with less uniform distributions. In principle, most researchers designing algorithms to improve exploration algorithms would care about this distinction. Intuitively, actually visiting a state and ensuring that the agent has observed what is there is important for ensuring the agent can find the optimal parts of the world.The use of the word curiosity in this paper is problematic overall. Using the word curiosity to refer to both a metric and a set of methods leaves quite a bit of room for confusion for the reader. In particular, the methods and their metric are not as closely related as the authors suggest in the paper. While the authors appear to have the misconception that methods like ICM and RND are designed to increase the entropy over observations (stated on page 6), this is not the case. Importantly, these rewards are designed to be consumable, so they eventually no longer shape the behaviour of the agent and the agent is left to pursue (typically external) goals. That could result in visit frequencies being highly non-uniform.The word curiosity has been used in the realm of reinforcement learning to refer to many very different methods, not necessarily methods that measure probability under a trained density model, and it isn't appropriate to provide this blanket definition of the word curiosity without some language to tell the reader that the word curiosity is simply a shorthand in this paper, in particular, to refer to methods that fall under the given definition.While this paper makes clear calls to the foundation of ideas from the literature that are employed in this paper (e.g., work on curiosity, information gain, empowerment, human performance on Atari, etc.), there is no discussion of related kinds of comparative work that already exists in the literature. Neither the literature comparing multiple intrinsic reward agents nor the literature comparing the exploratory behaviour of RL agents with that of humans is discussed. **Recommendation**I am recommending that this paper be rejected on the basis of lack of appropriate evidence for their claims and inappropriate use of language to describe curiosity, a word with a diverse history in the literature. **Specific Examples of Issues**The characterization "Curiosity encourages encountering rare sensory inputs, measured by a learned density model" (p. 1) does not capture the definition of curiosity used as a metric: "the cross entropy of future inputs under a density model trained alongside the agent" (p. 4)The characterization is inherently contradictory, as if curiosity is "successful" what does it mean for a sensory input to be rare? The characterization might be better captured by a definition that requires visiting many states.The Go-Explore algorithm by Ecoffet et al. (2019) is explicitly not an intrinsic motivation algorithm (for example, see the paragraphs devoted to contrasting Go-Explore with IM methods on page 2 of Ecoffet et al., 2019) and the paper provides little evidence of the empirical success of IM methods, so citing the paper for such evidence does not appear appropriate. "Despite the empirical success of intrinsic motivation for facilitating exploration ..." (p. 1) **Additional Feedback (Here to help, not necessarily part of decision assessment)**I found myself trying to come up with a more appropriate name for the metric you call curiosity, and I think that "Observation entropy" might capture the mathematical definition appropriately.More data might improve the quality of the results of your experiments; if you are interested in including other intrinsic-reward methods into future experiments, a list of fifteen different intrinsic rewards is included in https://arxiv.org/abs/1906.07865Can you clarify what preprocessing is done for the images fed to the agents? This information belongs somewhere prior to "We first convert the RGB images to grayscale as they were seen by the agents." (p. 3)I can't find the definitions of A (likely the action set?) and X (likely the set of possible 8x8 discretized images?) (used on p. 3) and it would be helpful to have these notations defined explicitly."has enable agents" (p. 1) Typo."Atari Learning Environment" (p. 2) I this was meant to be "Arcade Learning Environment""task-agnostic metric" (p. 5) Typo."human similarity it correlates" (p 8) Typo."For this reason, intrinsic rewards (Burda et al., 2018b) or human demonstrations (Aytar et al., 2018) are important to succeed at the game." (p. 12) Rather than "are important" I would suggest "have been important" since there is no evidence that there doesn't exist some method of another category that succeeds in Montezuma's Revenge that hasn't been published yet."chooses one of a set" (p. 12) reads a little strangely, since the agent is choosing an action, not a set.ICM is not designed to be a complete agent (as it "can potentially be used with a range of policy learning methods," Pathak et al., 2017, p. 16) and so the phrase "is an exploration agent" (p. 12) is not accurate. I understand that you are using a PPO agent augmented with ICM, following Burda et al. (2018a), but that would be helpful information to include in your description of the agents in the appendix (perhaps along with a reminder to the reader about where to find the OpenAI implementations that you are using).In Appendix D, the explanation of ICM (p. 12) would benefit from explaining what learning algorithm/agent architecture is used to optimize the intrinsic (or intrinsic + extrinsic) reward, to parallel the description given for PPO. The authors propose a 3-phase heuristic algorithm to learn a causal graph from interventional data using continuous optimization. Unfortunately, the paper is hard to follow. Specifically, the exact procedure should be clarified by the authors. If I understand correctly, first they fit to observational data by searching over the space of graphs using a smooth representation for the adjacency matrices. To fit to the interventional data, first, the interventional target is estimated by a heuristic approach and the contribution of these variables to the likelihood is ignored since they are set by the experiment. (there are random graph sampling stages in between that are not clear to me, please elaborate on this). This interventional scoring is done for all interventional data and is turned into a single gradient update.The paper is hard to parse. My main concern is that, unlike the existing work which the authors compare with in the experiments, the proposed method is not a systematic approach and accordingly it is hard to reason about its use even though it performs well in the experiments. Especially given that some choices made in the algorithm design are not properly justified. Indeed, even with interventions, we do not expect to recover the full structure but only a subset of the edges correct. Comparisons with the other methods should be expanded into a section where these methods are detailed to showcase the methodological differences. The following are my detailed feedback."A natural application of Bayesian networks is to describe cause-effect relationships between variables."Please distinguish Bayesian networks from the causal networks. Former do not carry causal meaning. A good reference to cite in addition to Peters et al. for SCMs is Pearl's 2009 Causality book. "Although there is no theoretical guarantee that the true causal graph can be identified in that setting, evidence so far points to that still being the case."Please modify this statement as it sounds too vague. The list of contributions require knowledge of the latter sections. Please make it self contained if possible. "can't"->"can not"SCM definition is not only structural equations but also talks about interventional distributions. Please see Pearl 2009.The last line in page 2 overlaps with the page number. Some recent related work is missing: Mooij et al. "Joint Causal Inference from Multiple Contexts" JMLR'20.Kocaoglu et al. "Characterization and Learning of Causal Graphs with Latent Variables from Soft Interventions" NeurIPS'19.Brouillard et al. "Differentiable Causal Discovery from Interventional Data" arXiv'20.Mooij et al. is cited but please add it in Section 3 among constraint-based inverventional learning frameworks as well.Brouillard et al. is too recent, hence its omittance is understandable. However, it attacks the same problem considered here. I believe including it as independent discovery would help connect literature together nicely. I am not going to take this work into consideration in my evaluation since it is uploaded on arXiv only very recently."the methods only uses"->"the methods only use"citing Murphy "This is different from our setting where the intervention is unknown to start with and is assumed to arise from other agents and the environment."Murphy can handle unknown interventions as well. Moreover Mooij et al. handles unknown interventions too. "The set of functional parameters ¸i parametrizes the conditional probability distribution of Xi given its parent set Xpa(i,C), with C < Ber(Ã(³)) a hypothesized configuration of the SCMs DAG."Can you clarify this sentence?"During Phase 1, the functional parameters ¸ are trained to maximize the likelihood of randomly drawn observational data under graphs randomly drawn from our current beliefs about the edge structure."Why do you draw synthetic data? Likelihood is typically maximized using real data at hand. It's hard to follow the exact procedure here. Intervention targets are predicted using a heuristic. Why not use the existing methods? I believe the computational aspect is seen as a problem but JCI by Mooij et al. should be fast enough. Can you convert Section 3 into a pseudo-code for the algorithm description? I believe many details are skipped and some key points of the approach is not clear by the brief text in each subsection. "should be taken as givens"->"should be taken as given"In the experiments, please compare with Mooij et al. Their method should be as fast as FCI and it would be interesting to see how the results compare.  *Summary*This paper proposes an architecture that addresses transferability of compositionality. The proposed architecture consists of three components: a network that transforms the input X into a series of hidden representations {H_1, H_2, ... H_K}, a network that reconstructs the input X from this series of hidden representations, and a prediction network that generates a prediction from the hidden representations. The authors propose several datasets meant to address transferability of compositional generalisation, and show that their architecture significantly improves standard DNN architectures as well as humans on these datasets.*Motivation for score*Compositional generalization in neural networks is a relevant and hot topic, with still many open questions. This paper aims to contribute on this topic and proposes some interesting datasets. However, However, despite citing several papers addressing compositionality in neural networks in the related work section, I am not convinced that the authors have properly understood the questions that are asked in this domain and were able to address them properly. Below, I outline my concerns.1. Definition of compositionalityI do not find the definition of compositionality that the authors propose well motivated.- None of the three papers cited in the introduction to motivate the work actually has the word "compositionality" in the paper- The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits- The author's definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself. In addition to that, whether the mapping between input and output is compositional does not depend on what is in the train and what is in the test distribution. A model can understand the compositional structure of a dataset also if it has been trained on *all* examples of the dataset, only it will be impossible to behaviourally evaluate if it has. For this reason, much previous work on compositionality in neural networks has created datasets where the training and testing data were distributionally different (as also the authors of this paper do).I would recommend the authors to have a look at the paper _Compositionality decomposed: how do neural networks generalise?_ (Hupkes et al.; 2020), for a detailed account of compositionality in the context of neural networks. In particular, their section on _localism_ is particularly important for the author's definition of compositionality.2. ArchitectureI find the proposed architecture interesting, but it is not completely clear to me how it differs from an auto-encoder setup where the encoding is larger than the input instead of smaller (it is very well possible I misunderstood). Nevertheless, it can be interesting to see if auto-encoding based architectures behave better on datasets proposed to evaluate compositionality. One thing that is not clear to me is how the number of components _K_ is determined.3. DataI appreciate the effort of the authors to design new datasets that test out-of-distribution generalisation. I do have a few comments/questions:- If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn't it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?- What is the motivation for using a new dataset, rather than one of the previously proposed datasets for out-of-distribution generalisation?- It is nice that the authors try to include tests from different domains, but I think that calling a dataset mapping inputs like "januarymarch" to (0, 2) cannot really be called "natural language processing"Overall, I do not believe that this paper should be accepted for the conference. ## SummaryThis paper studies "compositionality" and in particular the way in which it "transfers" on test data. They run simple baselines on three experiments (overlapped MNIST, colored MNIST and concatenated month names) and find that the baselines do not learn compositional representation. They proposed the use of an *auxiliary reconstruction network and a regularized optimization* which improves on these baselines. ## AnalysisThe authors frequently say that *compositionality may not transfer to test distribution* but I have a hard time understanding exactly what they mean by this. As I understand it, "compositionality" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. They claim theirs is the *first work for the transferability problem of compositionality* which I find really hard to believe. I would have appreciated a thorough study of the "compositionaly" limitations of previous techniques.I found section 4 particularly hard to understand. A lot of symbols, equations and nomenclatures seem to be used with too little introduction. As a result, I cannot vouch for the correctness of this section.Given their claim that this is the *first work for the transferability problem of compositionality* the experiments presented on section 5 are on a new dataset and are not compared to previous work. Moreover, the proposed experiments seem relatively simple (two overlapped MNIST digits, colored MNIST digits, concatenated month names) and their baseline seem trivial (*we use a standard neural network with two sub networks, each for an output*) ## ConclusionOverall, I find the claim of this paper substantial, while the experiments are relatively simple with trivial baselines and an absence of comparison to related work.## TyposI find the text difficult to read, it would benefit from a thorough revision. Ex: *This work is orthogonal to many efforts of learning compositionality in training distribution.* The authors contribute an approach to automatically distinguish between good and bad student assignment submissions by modeling the assignment submissions as MDPs. The authors hypothesize that satisfactory assignments modeled as MDPs will be more alike than they are to unsatisfactory assignments. Therefore this can potentially be used as part of some kind of future automated feedback system. The authors demonstrate this approach on an assignment for students to recreate a simple pong-like environment. They are able to achieve high accuracy over the most common submissions. The major strength of the paper is in the novelty of the application domain. Reinforcement learning is not typically applied to the education domain in this way. I think theres certainly potential. In addition, the results do give a positive signal about this research direction. I have a number of concerns in terms of weaknesses of the paper. First, many of the details of the work are unclear in the current paper draft. I am not sure what the architecture for the agent and the classifier was exactly. Further Im not sure what exactly the training data was for the agent and the classifier. Separately the authors include the numbers 711,274 (the number of submissions), 18 programs, and a single standard program. Finally it is unclear exactly how long the agents trained for, the authors only state that they trained the agents until they can reach maximal reward, without a sense of the number of training episodes. I think this approach is very interesting, but Im concerned about whether its an appropriate one for this problem domain. If the main issue is feedback then a RL approach is not going to be particularly helpful given RLs blackbox nature. Further, since the authors could establish a gold standard baseline automatically its unclear why they would need an automated approach that only looked at pixels anyway (though its also unclear to me the extent to which the gold standard method used in this paper reflects how an instructor would evaluate this assignment). This is also a very simple environment which is well-suited to being modeled by an MDP (since it is a game), and Im not convinced that this would scale to more complex environments. Finally, its unclear to me why student submissions would need to be evaluated from raw pixels and reward values. This seems like an arbitrary constraint since a CS instructor would have access to the model/MDP/submission directly.Im not convinced by the evaluation. The authors only compare to a relatively straightforward code-as-text baseline. While the authors mention that they attempted an image classifier and found that static images were not suitably discriminative, I dont see why the authors couldnt have tried sequences of images. Id also like to see a more natural baseline, such as a hand-written classifier by a CS instructor or some approximation of this. Something as simple as a KNN classifier could also be worth considering for a baseline. Further, its unclear to me what the evaluation domains were, were they the 250 instances taken from the head and tail parts of the dataset? If so, were these all unique instances?I think this work is interesting, but too immature for publication at this time. Im recommending rejection.Some questions for the authors: -What was the training curriculum used for the agent(s)? Some clarity across the different numbers of training instances would be appreciated.-Did the authors attempt to use sequences of frames in a more straightforward classifier? Or any other way to represent movement/video? Why not, if not?-Is there something Im missing about why this is an appropriate solution to the problem identified by the authors?The language across the paper is also uneven and some claims are not substantiated. Id recommend another pass. Some particular points: - teacher often has -> a teacher often has- Is it fair to assume teachers can prepare a few incorrect implementations? Id like to see a citation to back up this claim, since my understanding is the opposite for complex assignments. - a simulator faithfully executes command-> a simulator that faithfully executes commands- and will return positive reward when Score point  ->  and returns positive rewards when the Score point - Socre opponent point-> the Score opponent point- such simulator-> such a simulator- Code.org didnt develop this interface they adapted it from Scratch (Maloney et al.)- Can you present some evidence that your gold annotation is correct?- Why is sample efficiency a concern? Summary:This paper provides a statistical test that checks whether a subgroup would benefit from a certain intervention, even if on average the entire population does not benefit from that intervention. This test is developed for the online setting where we have a stream of data coming in and we want to terminate the trial (here, A/B testing) as soon as possible in order to limit the possible adverse effects that the trial might have.Pros:- The paper is well-written and easy to read / understand.- According to the papers literature review, this work seems to be original.Cons:- There are many design choices that the authors do not discuss the reasons behind making those choices; an example is: employing the augmented inverse probability weighted (AIPW) estimators. - It is unclear why $\Delta$ should be defined as that in Eq. (11).- There is no step-by-step discussion on how the test statistic is derived as that in Eq. (12) or (15). By the way, are these two equations equivalent? If so, how?- Most importantly, I think the scope of this paper does not match with that of the ICLR conference, as there is no representation learning aspect in this work. Perhaps AISTATS, or even IJCAI or AAAI would be a better fit to publish this paper.Minor comments:- Sec. 3.1: $g(\mu)=\log(\frac{\mu}{1-\mu})$ is undefined for a binary $\mu$.- Item (4) in Algorithm 1: $\overline{D}_k$ is a scalar and has no standard deviation. The paper uses the bandit learning framework to study the online learning problem for routing in a city network . After each routing decision, the learning agent observes the actual delay on each edge, which is given by the congestion function on the given flow plus a random noise, and the reward is the total delay on all edges. The paper proposes a learning algorithm similar to the UCB approach, provide the regret bound result, and conduct simulations on the New York City network to verify performance of the algorithm. The contribution of the paper in my view is mainly on the setting where the nonlinear congestion function (satisfying a Lipschitz condition) with arbitrarily given flow is considered, and on using dynamic splitting to refine the budgets based on the number of observations.However, there are a number of issues in the paper that makes the overall contribution questionable, as I list below.- The first issue is that the authors is missing an entire line of very relevant result work and results. In particular, the work is closely related to the combinatorial semi-bandit research. The following are several most relevant studies, while many others are available in the literature.[1] Chen, Wang, Yuan, and Wang. Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms. JMLR'2016, Conference version appeared in ICML'2013[2] Qin, Chen, and Zhu. Contextual Combinatorial Bandit and its Application on Diversified Online Recommendation. SDM'2014The routing problem in the current paper has a number of similarities with the CMAB framework in [1]: each road segment (edge) is a base arm, and it can be individually observed when the selected path contains the edge --- semi-bandit feedback, and each action is a set of base arms (a path) that follows certain constraints. The adversarial chosen path source and destination and the current flow can be viewed as the context in CMAB. The contextual CMAB problem is studied in [2]. The current paper has a bit more complication due to the congestion function. But the authors treat it by discretizing the flow into buckets. Essentially this is treating the pair of (edge, flow budget) as a base arm in the CMAB framework. Therefore, I believe that if we do not do dynamic bucketing and use a fixed discretization budget, the current problem fits as a special instance in the CMAB framework, and thus can be solved by the CUCB algorithm for CMAB. Noticeably the CUCB algorithm has O(\sqrt{t}) regret (ignoring the additional log t term). However, the current paper only has a result for O(t^{2/3}), and the authors mention both in the introduction and conclusion that achieving O(\sqrt{t}) regret as a future research work. But with proper setup as the above, I believe the O(\sqrt{t}) regret has been achieved. Therefore the authors are seriously missing some existing work in this regard. Of course, the above discussion only uses static buckets. But applying dynamic buckets should only improve the results. The static bucketing may give some extra factor in the regret bound in terms of the number of budgets, but that is independent of time t. Therefore, I believe the dependency on time t should be O(\sqrt{t}), and it should be easily achieved under the CMAB framework, such as using the CUCB algorithm in [1].- The second issue is that in terms of the theoretical analysis, the authors only provide the regret bound result as a theorem in the introduction. There is no more discussion on the factors related to the regret bound, such as |E|, and \beta. Are their dependency tight or not? There is also no explanation on the outline of the analysis. In particular, how to incorporate the dynamic splitting of the buckets into the analysis. Dynamic splitting is the only thing that is different from existing work in my view, and its advantage should be further discussed.- The third issue is on the experimental evaluation. The authors do not compare the proposed algorithm with any baseline algorithms. Baseline algorithms that could be considered include epsilon-greedy algorithms and Thompson Sampling based algorithms. Without such comparison, it is hard to understand the benefit of the proposed algorithm. - Another issue is that the proposed algorithm is essentially very close to using the lower confidence bound (LCB) for the edge delays in the CMAB setting. Since the optimization problem is minimizing the delay, so it is understandable that the standard UCB is replaced by the LCB. The authors need to discuss whether there is any more difference between their algorithm and the LCB/UCB based algorithms. Summary of the paper:The paper proposes a novel architecture called Heterogeneous Graph Transformers (HGT) to solve Constraint Satisfaction Problems (CSP) using unsupervised learning. Their model seems to achieve good results on classical problems encoded as CSP (3-SAT, k-coloring, k-cover, k-clique) with a few hundreds of variables and clauses, along with faster running time as compared to other contemporary methods because of a clause-parallel model architecture. The main contribution of the paper is to modify typical message-passing steps in graph neural networks to take more of the clause-variable structure into account, and accelerate message-passing in-between clauses and variables, respectively.Strengths:- A new approach to solving CSP using Graph Neural Networks/Transformers with some modeling improvements leading to reduced solution times and better success rate in finding satisfiable assignments.Weaknesses:- The experimental setup and results need significant improvements before this work can be published; see details in Questions below.- Writing: The motivating story in the introduction is not so clear-cut. There is a mention of previous work being limited to sequential algorithms, but no strong intuition is offered for why clause-parallelism is useful. Additionally, some key technical terms (e.g., "cross-attention", "meta-paths") are mentioned with little context, making it difficult for the reader to start grasping the paper's contributions early on.- Novelty: The attention mechanisms presented are very much similar to other attention mechanisms like Selsam et al. (2018). The use of transformer-type models is not uncommon, and was used prominently for TSP-type problems in this paper (which was not cited): Kool, Wouter, Herke van Hoof, and Max Welling. "Attention, Learn to Solve Routing Problems!." International Conference on Learning Representations. 2018.Recommendation: There are some positives to this work, particularly in the careful message-passing design. However, I have to recommend a Reject because the experimental results are very, very limited at this point. It is unlikely that the authors will be able to address all the issue I raise here, but I encourage them to do so in the near future and submit to future conferences.Questions to the authors (in no particular order):1- Please show comparisons with a state-of-the-art optimization CSP solver such as IBM CPLEX's CP Optimizer or others.2- Test accuracy: Figure 4 only showcases the speedup achieved by the method compared to other approaches. Please report the test accuracy numbers (mean and standard deviation). Otherwise, the tables show only training results which does not tell the reader much.3- Homogeneous and heterogeneous graph attention: The distinctions between homogeneous and heterogenous graph attention is not clearly stated. The only distinction seems to be different sizes of the initial node (v in V) and edge (u in U) embedding sizes; v is of F_v dimension and u is of F_u dimension as opposed starting with same dimensions as in Velickovic et al 2017. Ideally, in heterogeneous attention, e_vi_uj and e_uj_vi should lead to different values. However, based on the definition in Equation (3), both of them will attain the same value. It would be nice if you can clearly state the differences.4- "four encoder-layers in the encoder, and three decoder-layers in the decoder.": how did you choose the numbers of layers here? Same question for all the hyperparameters described under "General Setup" in Section 5.3.5- Section 4.3.1 The definition of $l(x_i, e_{ia})$ after equation (5) is different from how $l(x_i)$ is used (with a single argument) in (5). What is the definition of $e_{ia}$? Also, can you clarify what you mean by "is applied to specify the polarity of each variable"? 6- Table 3: the time difference with PDP is clearly implementation-dependent. Can you comment on the significance of these results? Also, it is impossible to guess how many learnable parameters PDP and your model have; can you add that information to the table? Minor:- "The modern approach is trending to solve CSP through neural symbolic methods." --> "One modern, trendy approach to solving CSP is through neural symbolic methods."- Unclear what this sentence means: "Hence, the resulted model is bounded by the greedy strategy, which is generally sub-optimal."- "The adjacent matrix" --> "The adjacency matrix"- "literals forming a partition different from" --> "literals forming a partition are different from"- "a highly paralleled message-passing" --> "a highly parallelized message-passing"- Page 2 para 1, missing full stop in the last line.- Figure 2(b), the annotations "Encoder" and "Decoder" in the light blue boxes seem to be interchanged This paper approaches the problem of representation learning for robotic learning, with a focus on being robust to potentially misleading distractor objects. Concretely, they propose a method which learns a representation of sequences of actions by reconstructing a future state, and uses this representation of actions to learn skills. They show that this leads to some improvement in identifying more diverse "skills" in settings where there are external distractors.Pros:+ The problem of learning representations for robotic learning which are robust to distractors is very important and relevant. + The proposed approach is simple to implement.Cons:I have several concerns about the paper, concretely regarding (1) the clarity of presentation, (2) the correctness/generality of the proposed method, and (3) the conclusions drawn from the experimental results.(1): There are serious issues with the clarity of the paper. First, much of the abstract/introduction motivates the challenge of learning representations of visual inputs that are robust to distractors. The related work also covers some prior works on this topic in general the paper seems to be proposing a technique to address this challenge. But upon reaching the method, it appears that the paper is not concerned with representation of states at all, but rather representations of sequences of actions with no state information. This a completely disconnected from the motivation/problems presented in the introduction/related work. It is not clear what the objective of the paper is - is it to learn efficient state representations for reinforcement learning? is it to learn a generative model over actions? What is the goal of such a representation, better reinforcement learning? These questions should be clearly answered early in the paper.Second, this representation of action sequences is not actually used for any specific RL or task learning. Instead it is used with a method for unsupervised skill discovery AURORA, which tries to explore + build a set of controllers. If the motivating problem is representations robust to distractors, why use a unsupervised skill discovery framework like this? It seems like the problem of robust representations can be studied in a much simpler way (evaluating some task performance in the face of distractors). On the other hand, if the goal is a better exploration + skill discovery framework using a compact action representation, that should be laid out from the beginning of the paper.(2): I have some doubts about the correctness of the method (or the implicit assumptions made). First, The idea of the paper is to encode a sequence of actions into a latent distribution from which the future observation can be recovered. Is this not simply learning a forward dynamics model, but under the very restrictive assumption that the future state does not depend on the current state? Is this work considering an MDP? The necessary preliminaries are not stated, so its not clear what problem setting the paper considering.Second, the way the KL penalty is used on the latent of action sequences does not seem correct. The encoder is outputting a latent distribution over action sequences. The reason to have this be a distribution is to capture stochasticity in the learned representation. But the authors then argue they don't want the latent distribution to have high variance, so they only use a penalty on the mean. In that case, why have the encoder output a distribution?(3): Experimental conclusions.The only experimental comparison is the an auto encoder on state. Since the proposed method is not encoding state, it seems obvious that the representation would be robust to state perturbations, while the auto-encoder on the state would not be. In fact its not clear why the auto-encoder on state is an effective baseline, if the goal is to learn sets of controllers composed of action sequences. Wouldn't simply learning an auto-encoder on action sequences be a more effective baseline?Also more of a minor point, but the points to not considering realistic/stochastic settings with distractors as a limitation of prior work, but the domains considered in this paper are far from realistic.  Summary:This paper seeks to learn a representation space for trajectories that discards state elements that are not affected by the agent. To do so, it proposes an objective to predict the last observation in a trajectory from the sequence of actions taken in the trajectory.Pros:Tackles the problem of generating diverse task-relevant behaviorsCons:The motivation and the presentation of the method are very confusingI am not convinced that the objective is a reasonable one (see comments below)Experiments are very toy, and the analysis is hard to parseDetailed CommentsIntroduction: When the dimensionality of the representation is smaller than that of the input data, some information is lost when creating the representations. Dimensionality reduction does not necessarily imply information reduction. Also, the goal of representation learning need not be dimensionality reduction (sparse coding, for example).I do not think that terms such as representation learning and reinforcement learning should be capitalized. They are not proper nouns.Its strange that a refers to a sequence of actions, but x refers to a single observation.It seems like in Equation (2) you would want to predict a distribution over final states, to account for stochastic dynamics. Also, it seems like there is an implicit assumption that all objects need to start in the same place. For example, say you want a robot to pick up a cup. Depending on the starting position of the cup, the same sequence of actions could lead to very different final states.The discussion in Section 3 about choosing the form of the penalty P is quite confusing and not rigorous.Its not clear until the experiments section of the paper that the goal is to produce diverse trajectories. This makes the beginning of the paper very hard to understand.How is the representation learned by the auto-encoder of each observation transformed into a behavioural descriptor for the whole trajectory? Is the behavioural descriptor simply taken to be the embedding of the final observation? If this is the case, then how do the DIs affect the auto-encoder baseline at all?The air-hockey plot seems to be missing a line?Recommendation:RejectThe method as presented seems flawed, and the experiments are toy and hard to make sense of. The paper overall is very difficult to understand.  The paper proposed the Legendre Deep Neural Network (LDNN) to solve VolterraFredholmHammerstein integral equations. Specifically, the network uses Legendre polynomials as the activation in the first layer and uses Gaussian quadrature to discretize the integral operator as a summation. The numerical examples are performed to verify the performance of LDNN. However, the method is not novel and the numerical examples are too simple.Major comments:- The proposed method is the same as the physics-informed neural network (PINN) for solving integral PDEs proposed in https://arxiv.org/abs/1907.04502, but this is not mentioned in the paper. In fact, the method proposed in https://arxiv.org/abs/1907.04502 is more general than the method in this paper and can solve more types of PDEs.- The only difference is that here the first layer of the network uses Legendre polynomials as the activation, but there is not any evidence in the paper that if using Legendre polynomials would make a significant difference.- The integral equations solved in this paper are very simple. The equations only have one or two integrals, the problem is one dimension, and the solutions are quite smooth. In https://arxiv.org/abs/1907.04502, an integro-differential equation is solved. The same method has also been used to solve the 1D/2D/3D time-fractional/space-fractional/time-space-fractional PDE in a complicated geometry for both forward and inverse problems (https://doi.org/10.1137/18M1229845), which is much harder than the problems solved in this paper. Summary:This paper proposes replacing/combining Transformer self-attention with synthetic attention weights that do not rely on pairwise dependencies between token positions. Synthetic attention relies on either the input at the given position (dense synthesizer) or is altogether randomly initialized (random synthesizer).The goal of the paper is to show that synthetic attention is a competitive alternative to self-attention. Some thoughts:1. The claim that combining synthetic attention with self-attention improves performance seems pretty unfair since the synthetic attention adds extra parameters that the baseline using only self-attention doesnt have. This claim seems pervasive throughout the experiments but doesnt appear to be well supported. Also, it would be interesting to see what the learned weights for the different types of attention look like - does it mostly just use self-attention?2. The fact that synthetic attention is competitive is interesting, but how does it compare to other methods for replacing self-attention (eg: Wu et al, 2019s work on convolutions)? This question hasnt really been properly addressed. They report a wide range of experiments in their paper, but the only setting compared here is T5 pre-training, and even there the metric is dev set loss for masked language modeling, which doesnt objectively mean much. 3. The random synthesizer results are interesting and surprising. It would be interesting to delve deeper into figuring out why the randomly initialized parameters serve as a reasonable proxy for token dependent weights. Are there any results from the Fixed random synthesizer in the paper? Dont see any.4. Speed is recorded in terms of FLOPS for T5 pre-training, what about model convergence? Does it take longer for the model to converge? Doesnt have to be on the T5 pre-training, MT or LM or the other tasks are fine too.5. Overall, the paper isnt very clearly written. The motivations are not highlighted - why should one want to use synthetic attention? The notation, language and naming conventions are also a bit sloppy and inconsistent.It might be worth considering an analytic framing for the paper where the goal is to study what makes the pairwise interactions replaceable and what going from all pairs to convolutions to dense and then finally random looks like in terms of model behavior and outputs. Also, is self-attention more replaceable in some layers than others? Eg see the ideas in sandwich transformers: https://arxiv.org/pdf/1911.03864.pdf. The paper considers a particular setting of so-called meta-reinforcement learning (meta-RL) where there is a distribution over reward functions (the transition function is fixed) and with some access to this distribution, the goal is to produce a learning algorithm that  "learns well" on the distribution. At training time, a sample of reward functions are drawn and an algorithm returns a learning program that at test time, can reinforcement learn the test environment as specified by a test reward function. The paper proposes to generate a set of training reward functions instead of relying on some "manual" specification, thus the term "unsupervised" in the title. It also proposes an algorithm, basing on a recent work in skill discovery (DIAYN, not yet peer-reviewed), to find such reward functions. Firstly, the exposition is hard to follow. For example, the "Task acquisition via random discriminators" subsection, without first mentioning DIAYN, seems out-of-context: what is D_phi_rand(z|s)? a joint distribution of (reward function, state) makes no sense. It only makes sense when there is a stochastic process, e.g. MDP coupled with a policy. Secondly, the reasoning is very informal based on a vague vocabulary (not trying to pick on the authors, these are not uncommon in deep learning literature) are used without rigorous arguments. Section 3.1 brought up a natural objection -- I applaud the authors' self-critique -- based on "no free lunch theorem," but it dismisses it via "the specific choice for the unsupervised learning procedure and meta-learning algorithm can easily impose an inductive bias" without specifying what (and how) choice leads to what "inductive biases." This is crucial as the authors seem to suggest that although the "inductive bias" is important -- task design expresses such -- an unsupervised method, which requires no supervision, can do as well. Thirdly, the baseline comparison seems inappropriate to me. The "fair" baseline the authors proposed was to RL a test task from scratch. But this is false as the meta-RL agent enjoys access to the transition dynamics (controlled Markov process, CMP in the paper) during the so-called meta-training (before meta-testing on the test task). In fact, a more appropriate baseline would be initialize an RL agent with with a correct model (if sample complexity in training is not a concern, which seems to be the case as it was never addressed in the paper) or a model estimated from random sample transitions (if we are mindful of sample complexity which seems more reasonable to me). One may object that a (vanilla) policy-gradient method cannot incorporate an environment model but why should we restrict ourselves to these model-free methods in this setting where the dynamics can be accessed during (meta-)training?Pros:1. It connects skill discovery and meta-RL. Even though such connection was not made explicitly clear in the writing, its heavy reliance on a recent (not yet peer-reviewed) paper suggests such. It seems to want to suggest it through a kind of "duality" between skills/policies and rewards/tasks (z in the paper denotes the parameter of a reward function and also the parameter of policy). But are there any difference between the two settings? Cons:1. The writing is imprecise and often hard to follow.2. The setting considered is not well motivated. How does an unsupervised method provide the task distribution before seeing any tasks? 3. The restriction of tasks to different reward functions made the proposed baseline seem unfairly weak.In summary, I could not recommend accepting this work as it stands. I sincerely hope that the authors will be more precise in their future writing and focus on articulating and testing their key hypotheses. The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments.  First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 &amp; 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. In section 3, the authors then introduce the "max-marginal regularization" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word "marginal" instead of "margin" seems quite wrong to say the least.  Secondly, the stated definition seems to be wrong. In the definition the range of values for \gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \gamma not equals 0) below and show that both have issues:(a) \gamma &gt; 0: This seems to imply that when the log-likelihood of ground-truth is already \gamma better than the log-likelihood of the random negative, the loss comes to life. Strange!(b) \gamma &lt; 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. I'm assuming the authors swapped y with y^{-} in the "regularization" part.Anyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well.  Coming to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two "Where is your location?" outputs.  The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. A lot of typos/wrong phrasing/wrong claims and here are some of them:(a) Page 1, "lead to the misrecognition of those common replies as grammatically corrected patterns"? - No idea what the authors meant.(b) Page 1, "unconsciously preferred" - I would avoid attaching consciousness before AGI strikes us.(c) Page 1, "Above characters" -&gt; "Above characteristics"(d) Page 1, "most historical" -&gt; "most previous"(e) Page 2, "rest replies" -&gt; "rest of the replies"(f) Page 3, "variational upper bound" -&gt; Not sure what's variational about the bound(g) "Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance"? - No idea what the authors meant. ÿThis paper applies a rotation-equivariant convolutional neural network model to a dataset of neural responses from mouse primary visual cortex. This submission follows a series of recent papers using deep convolutional neural networks to model visual responses, either in the retina (Batty et al., 2016; McIntosh et al., 2016) or V1 (Cadena et al., 2017; Kindel et al., 2017; Klindt et al., 2017). The authors show that adding rotation equivariance improves the explanatory power of the model compared to non-rotation-equivariant models with similar numbers of parameters, but the performance is not better than other CNN-based models (e.g. Klindt et al., 2017). The main potential contributions of the paper are therefore the neuroscientific insights obtained from the model. However, I have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below). Together with the fact that the model does not provide better explanatory power than other models, I cannot recommend acceptance. I am open to discussions with the authors, but do not anticipate a major change in the rating.1. As noted by the authors, the finding that Feature weights are sparse (page 6) could be due to the sparsity-inducing L1 penalty. The fact that a model without L1 penalty performs worse does not mean that there is sparsity in the underlying data. For example, the unregularized model could be overfitting. A more careful model selection analysis is necessary to show that the data is better fit by a sparse than a dense model. 2. The finding that there are center-surround or asymmetric (non-gabor) RFs in mouse V1 is not novel and not specific to this model (e.g. Antolik et al., 2016). 3. Many of the receptive fields in Figure 6 look pathological (overfitted?) compared to typical V1 receptive fields in the literature. I understand that sensitivity to previously undetected RF features is a goal of the present work. However, given how unusual the RFs look, more controls are necessary to ensure they are not an artefact of the method, e.g. the activation maximization approach with gradient preconditioning, the sparsity constraints, or overfitting. Perhaps a comparison of RFs learned on two disjoint subsets of the training set would help to determine which features are reproducible.4. Should orientation be treated as a nuisance variable? Natural image statistics are not rotation-invariant. In the visual system, especially in mice, it is not clear whether orientation is completely disentangled from other RF properties. The orientation space is not uniformly covered, and some directions have special meaning (e.g. cardinal directions), such that it might be invalid to assume that the visual system is equivariant to rotation. (The same concern applies to the translation equivariance assumed when modeling visual RFs with standard CNNs.) Of course, there is a tradeoff between model expressiveness and the need to make assumptions to fit the model with realistic amounts of data. However, this concern should at least be discussed.5. Some more details about the neural recordings would be good. What calcium indicator? How was the recording targeted to V1? Perhaps some example traces. This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.Although the idea behind this paper is fairly simple, the paper is very difficult to understand.  I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method. Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2. However, in Figure 2, it is used for evaluating defense schemes. Again, this confuses me on what is the main topic of this paper. Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case? Intuitively, it should provide similar results to the success-failure curve.The paper also lacks experimental results, and the main conclusion from these results seems to be "MNIST is not suitable for benchmarking of adversarial attacks". If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.Meanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations. This paper proposes a new detection method for adversarial examples, based on a prior network, which gives an uncertainty estimate for the network's predictions.The idea is interesting and the writing is clear. However, I have several major concerns. A major one of these is that the paper considers "detection of adversarial attacks" to mean detecting adaptive and non-adaptive attacks, while the latter are (a) unrealistic and (b) a heavily explored problem, with solutions ranging from clustering activations, to denoising the image via projection (in particular, once can use any of the circumvented ICLR 2018 defenses which all work in the non-adaptive sense, and check the prediction of the denoised image vs the original). Thus, the paper should focus on the regime of adaptive attacks. Within this regime:Motivation:- This work seems to suggest that dropout-based detection mechanisms are particularly successful. While Carlini &amp; Wagner finds that the required distortion (using a specific attack) increases with randomization, the detection methods which used dropout were still completely circumvented in this paper.- The claim that adversarial examples are "points off of the data manifold" is relatively unmotivated, and is not really justified. Justification for this point is needed, as it forms the entire justification for using Prior Networks.- Detecting adversarial examples is not the same problem to detecting out-of-distribution samples, and the writing of the paper should be changed to reflect this more.Evaluation:- 100 iterations is not nearly enough for a randomization-based or gradient masking defense, so the attacks should be run for much longer. In particular, some of the success rate lines appear to be growing at iteration 100.- There is no comparison to any other method (in particular, just doing robust prediction via Madry et al or something similar); this should be added to contextualize the work.- The term "black-box" attacks can take on many meanings/threat models. The threat models in the paper need to be more well-defined, and in particular "black-box attacks" should be more accurately defined. If black-box attack refers to query-based attacks, the success rate should be equal to those of white-box attacks (or very close to it), as then the attack can just estimate the gradient through the classifier via queries.- The fact that the attacks do not reach 100% on the unprotected classifier is concerning, and illustrates the need for stronger attacks.Smaller comments:Page 1: Abstract: Line 4: missing , at the end of the linePage 1: Abstract: Line 5: However, system can missing a before systemPage 1: Abstract: Line 10: have been shown should be has instead of havePage 1: Abstract: Line 13: In this work missing a , afterPage 1: Last paragraph: Line 2: investigate missing an s and should be investigatesPage 2: Section 2: Line 5: in other words missing a , afterPage 2: Section 2: Line 8: In order to capture distributional uncertainty  missing a , afterPage 2: Last paragraph: Line 2: Typically missing a , afterPage 3: Paragraph 2: Line 1: In practice, however, for deep, no need for the last ,Page 3: Section 2.2: paragraph 1: Line 2: a Prior Network p(À|x; ¸Æ),  no need for the last ,Page 3: Section 2.2: paragraph 1: Line 3: In this work missing a  , afterPage 3: second last paragraph: Line 1: refers to figure as fig and figure (not consistent)Page 3: second last paragraph: Line 3: uncertainty due severe class missing to before severePage 4: Paragraph 1: Line 2: to chose should be to choosePage 4: Paragraph 2: Line 1: Given a trained Prior Network missing a , afterPage 4: Paragraph 2: two extra ,Page 6: paragraph 1: Last line: 5 needs to be written in words and same for 10 in the next paragraphPage 7: section 4.2: paragraph 3: For prior networks need a , afterPage 8: Paragraph 1: Line 6: and andPage 8: Conclusion: Line 4: In section 4.2 needs a , afterPage 8: Conclusion: Line 7: it difficult missing isPage 8: Conclusion: Line 9: is appropriate should be if instead of is This paper introduces an ensemble version of Deep RL by bagging Q-function approximation estimates. In the experiments, the performance of the proposed work is compared to the baseline, single DQN. In spite of the contribution, this paper has a critical issue. It has been extensively studied in the literature that ensemble DQN could lead to better performance than a single DQN. See the seminal work by Osband et al. (2016). The authors did not cite this paper, not to say a long list of recent works who have cited this seminal work. This indicates that the authors fail to conduct a serious literature review. In addition, more comprehensive experiments are required to compare the proposed work with the state-of-the-art ensemble DQN methods.Osband et al. (2016), Deep Exploration via Bootstrapped DQN. NIPS. The authors argue that they propose a method to find adversarial examples, when the data lie on a Riemannian manifold. In particular, they derive a perturbation, which is argued to be the worst perturbation for generating an adversarial example, compared to the classical Euclidean derivation.I strongly disagree with the proposed (Riemannian) geometric analysis, because there are several technical mistakes, a lot of arbitrary considerations, and flawed assumptions. In particular, my understanding is that the proposed method is not related at all with Riemannian geometry. For justification I will comment some parts:#1) In Section 1 paragraph 4, and in Section 2.3 after Eq. 14, the sentences about the gradient of a function that is defined on a manifold are strange and unclear. In general, the gradient of a function defined on a manifold, points to the ascent direction. Thus, if I understood correctly the sentences in the paper support that the gradient of such a function is meaningless, so I think that they are wrong.#2) How the $\ell_2$-ball on a manifold is defined? Usually, we consider a ball on the tangent space, since this is the only Euclidean space related to the manifold. Here, my understanding is that the authors consider the ball directly on the manifold. This is clearly wrong and undefined.#3) To find the geodesic you have to solve a system of 2nd order non-linear ODEs, and there are additional details which I will not include  here, but can be easily found in the Riemannian geometry literature. Also, I think that the Lemma 2.2. is wrong, since the correct quantity of Eq. 3 is $ds^2 = g_ij(t)d\theta^i d\theta^j dt^2$, where $dt\rightarrow 0$ based on the included proof. This is clearly not a sensible geodesic, it is just the infinitesimal length of a line segment when $t\rightarrow 0$, which means that the two points are infinitesimally close.#4) If $x, y$ is on a Riemannian manifold then the $x+y$ operator does not make sense, so Eq. 7 is wrong. In particular, for operations on Riemannian manifolds you need to use the exponential and the logarithmic map.#5) Continuing from #4). Even if we consider the perturbation to be sufficiently small, still the $x+\epsilon$ is not defined. In addition, the constraint in Eq. 8 is wrong, because the inner product related to the Riemannian metric has to be between tangent vectors. Here the $\epsilon$ is an arbitrary quantity, since it is not defined where it actually lies. In general, the derivation here is particularly confusing and not clear at all. In my understanding the constraint of Eq. 8 is a purely linear term, and $d$ is not the geodesic distance on a manifold. It just represents the Mahanalobis distance between the points $x$ and $x+\epsilon$, for a matrix $G$ defined for each $x$, so it is a linear quantity. So Eq. 9 just utilizes a precondiner matrix for the classical linear gradient.#6) The Eq. 12 is very flawed, since it equalizes a distance with the Taylor approximation error. I think that this is an unrealistic assumption, since these terms measure totally different quantities. Especially, if $d$ is the geodesic distance.#7) The upper bound in inequality Eq. 13 comes from Eq. 12, and it is basically an assumption for the largest absolute value of the Hessian's eigenvalues. However, this is not discussed in the text, which are the implications?#8) I find the paper poorly written, and in general, it lacks of clarity. In addition, the technical inconsistencies makes the paper really hard to follow and to be understood. I mentioned above only some of them. Also, there are several places where the sentences do not make sense (see #1), and the assumptions made are really arbitrary (see #6). The algorithms are not easy to follow. Minor comments, you can reduce the white spaces by putting inline Eq. 3, 4, 5, 6, 9, 10, 14, and Figure 2. The notation is very inconsistent, since it is very unclear in which domain/space each quantity/variable lies. Also, in Section 2.5. the authors even change their notation.In my opinion, the geometrical analysis and the interpretation as a Riemannian manifold is obviously misleading. Apart from the previously mentioned mistakes/comments, I think that the proposed approach is purely linear. Since actually the Eq. 14 implies that the linear gradient of the loss function, is just preconditioned with the Hessian matrix of the loss function with respect to the input $x$. Of course, if this function is convex around $x$, then this quantity is the steepest ascent direction of the loss function, simply on the Euclidean space where $x$ lies. However, when this function is not convex, I am not sure what is the behavior when all the eigenvalues of the Hessian are set to their absolute value. Also, the (arbitrary) constraint in Eq. 13 implicitly sets a bound to the eigenvalues of the Hessian, which in some sense regularizes the curvature of the loss function. To put it simple, I think that the proposed method, is just a way to find a preconditioner matrix for the linear gradient of the loss function, which points to the steepest direction. This preconditioner is based on the Hessian of the loss function, where the absolute values of the eigenvalues are used, and also, are constrained to be bounded from above based on a given value.Generally, in my opinion the authors should definitely avoid the Riemannian manifold consideration. I believe that they should change their perspective, and consider the method simply as a local preconditioner, which is based on the Hessian and a bound to its (absolute) eigenvalues. They should also discuss what is the implication by setting the eigenvalues to their absolute values. However, since I am not an expert in the field of adversarial examples, I am not sure how novel is this approach. Summary:This paper presents a neural network based tree model for the regression via classification problem. The paper is easy to follow but it failed to give motivations for the significance of this work.  I do not understand why regression via classification is any useful and what value it brings to the well studied regression problem with many different function approximators. The paper neither explain why regression via classification is any useful nor does it motivates the need for the presented model. The presented experiments are also not thorough, there are stronger and simpler baselines for regression like random forests, gradient boosted trees  or kernel ridge regression which are not evaluated and compared. I think this work do not pass the acceptance bar at ICLR conference. Comments:1. I was not aware of this age and height estimation tasks. i-vectors are the standard features for speaker recognition.  Can the authors please elaborate in a  line or two why i-vectors would be suitable for age and height estimation?.2. The regressor function r() simply gives out the mean value of the bin. The authors could have provided on details on why this choice ? and how it affects MAE ?3. Each node in the NRT is successively being trained on a lesser amount of data. why do all the node-specific neural networks need the same parameter size then ?4. In Conclusion the authors say,  "In addition, we proposed a scan method and a gradient method to optimize the tree." The authors do not very clearly mention these two methods in the text, neither are the results demonstrated in that way.Miscellaneous comments:1. This line seems incomplete in Section 1: "Traditional methods for defining the partition T by prior knowledge, such as equally probable intervals, equal width intervals, k-means clustering, etc. [4, 5, 3]." 2. The notations used inside the nodes in Figure 1 has not been defined in the paper. 3. Figure 2 and 3 axes don't have labels. Figure 3 caption says age, but it is for heights.4.  In Section 4.4: Figure 4.4 should be Figure 4 and at one point "This is visible in 4.4" should be "This is visible in Figure 4" This paper attempts to mitigate catastrophic problem in continual learning. Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks. Here are my detailed comments:Catastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model. Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model. Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain. However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks. Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information. As far as I am concerned, this is the main contribution of this work. Nevertheless, I think there are some deficiencies in this work. First, this paper is not easy to follow. The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper. For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear. Second, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks. However, in this way, when more and more tasks come, the generator will become larger and larger. The storing problem still exists. Generative replay also brings the time complexity problem since it is time consuming to generate previous data. Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method. Third, the datasets used in this paper are rather limited. Three datasets cannot make the experiments convincing. In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10. I hope the author could explain this phenomenon. Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent. Fourth, there are some grammar mistakes and typos. For example, there are two "the" in the end of the third paragraph in Related Work. In the last paragraph in Related Work, "provide" should be "provides". In page 8, the double quotation marks of "short-term" are not correct. Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios. The proposed method is also heuristic and lacks promising guarantee. The authors prove some theoretical results under the mean field regime and support their conclusions with a small number of experiments. Their central argument is that a correlation curve that leads to sub-exponential correlation convergence (edge of chaos) can still lead to rapid convergence if the rate is e.g. quadratic. They show that this is the case for ReLU and argue that we must ensure not only sub-exponential convergence, but also have a correlation curve that is close to the identity everywhere. They suggest activation functions that attain conditions as laid out in propositions 4/5 as an alternative.The paper has many flaws:- the value of the theoretical results is unclear- the paper contains many statements that are either incorrect or overly sweeping- the experimental setup and results are questionnableTheoretical results:**Proposition 1: pretty trivial, not much value in itself**Proposition 2: Pretty obvious to the experienced reader, but nonetheless a valuable if narrow result.**Proposition 3: Interesting if narrow result. Unfortunately, it is not clear what the ultimate takeaway is. Is quadratic correlation convergence "fast"? Is it "slow"? Are you implying that we should find activation function where at EOC convergence is slower than quadratic? Do those activation functions exist? It would be good to compare this result against similar results for other activation functions. For example, do swish / SeLU etc. have a convergence rate that is less than quadratic?**Proposition 4: The conditions of proposition 4 are highly technical. It is not clear how one should go about verifying these conditions for an arbitrary activation function, let alone how one could generate new activation functions that satisfy these conditions. In fact, for an arbitrary nonlinearity, verifying the conditions of proposition 4 seems harder than verifying f(x) - x \approx 0 directly. Hence, proposition 4 has little to no value. Further, it is not even clear whether f(x) - x \approx 0 is actually desirable. For example, the activation function phi(x)=x achieves f(x) = x. But does that mean the identity is a good activation function for deep networks? Clearly not.**Proposition 5: The conditions of prop 5 are somewhat simpler than those of prop 4, but since we cannot eliminate the complicated condition (ii) from prop 4, it doesn't help much.**Proposition 6: True, but the fact that we have f(x) - x \approx 0 for swish when q is small is kind of obvious. When q is small, \phi_swish(x) \approx 0.5x, and so swish is approximately linear and so its correlation curve is approximately the identity. We don't need to take a detour via propposition 4 to realize this.Presentation issues:- While I understand the point figures 1, 2 and 4b are trying to make, I don't understand what those figures actually depict. They are insufficiently labeled. For example, what does each axis represent?- You claim that for ReLU, EOC = {(0,\sqrt{2})}. But this is not true. By definition 2, EOC is a subset of D_\phi,var. But {(0,\sqrt{2})} is not in D_\phi,var, because it simply leaves all variances unchanged and does not cause them to converge to a single value. You acknowledge this by saying "For this class of activation functions, we see (Proposition 2) that the variance is unchanged (qal = qa1) on the EOC, so that q does not formally exist in the sense that the limit of qal depends on a. However,this does not impact the analysis of the correlations." Section 2 is full of complicated definitions and technical results. If you expect the reader to plow through them all, then you should really stick to those definitions from then on. Declaring that it's fine to ignore your own definitions at the beginning of the very next section is bad presentation. This problem becomes even worse in section 3.2, where it is not clear which definition is actually used for EOC in your main result (prop 4), making prop 4 even harder to parse than it already is.Correctness issues:- "In this chaotic regime, it has been observed in Schoenholz et al. (2017) that the correlations converge to some random value c &lt; 1" Actually, the correlation converges deterministically, so c is not random.- "This means that very close inputs (in terms of correlation) lead to very different outputs. Therefore, in the chaotic phase, the output function of the neural network is non-continuous everywhere." Actually, the function computed by a plain tanh network is continuous everywhere. I think you mean something like "the output can change drastically under small changes to the input". But this concept is not the same as discontinuity, which has an established formal definition.- "In unreported experiments, we observed that numerical convergence towards 1 for l e 50 on the EOC." Covergence of a sequence is a property of the limit of the sequence, and not of the 50th element. This statement makes no sense. Also if you give a subjective interpretation of those experimental results, you should present the actual results first.- "Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions." This statement is very vague and sweeping. Also, one could argue that the fact that ReLU is much more popular and tends to give better results than tanh in practice disproves the statement outright.- "Tanh-like activation functions provide better information flow in deep networks compared to ReLU-like functions. However, these functions suffer from the vanishing gradient problem during back-propagation" At the edge of chaos, vanishing gradients are impossible! As Schoenholz showed, at the edge of chaos, \chi_1=1, but \chi_1 is also the rate of growth of the gradient. Pascanu et al (2013) discussed vanishing gradients in RNNs, which is a different story.- "Other activation functions that have been shown to outperform empirically ReLU such as ELU (Clevert et al. (2016)), SELU (Klambauer et al. (2017)) and Softplus also satisfy the conditions of Proposition 4 (see Supplementary Material for ELU)." Firstly, SeLU does not satisfy proposition 4. f(x) \approx x requires \phi to be close to a linear function in the range where the pre-activations occur. Since SeLU has a kink at 0, it cannot be close to a linear function no matter how small the pre-activations are. Secondly, softplus also doesn't satisfy proposition 4, as \phi(0) = 0 does not hold. Thirdly, this statement is too sweeping. If ELU / SELU / Softplus "outperform" ReLU, why is ReLU still used in practice? At best, those nonlinearities have been shown to outperform in a few scenarios.- "We proved in Section 3.2 that the Tanh activation guarantees better information propagation through the network when initialized on the EOC." Prop 4 only applies in the limit as \sigma_b converges to 0. So you can't claim that you showed tanh as "better information propagation" in general.- "However, for deeper networks (L e 40), Tanh is stuck at a very low test accuracy, this is due to the fact that a lot of parameters remain essentially unchanged because the gradient is very small." But in figure 6b the accuracy for tanh is decreasing rapidly, so therefore the parameters are not remaining "essentially unchanged", as this would also cause the accuracy to remain essentially unchanged. Also, if the parameter changes are too small ... why not increase the learning rate?- "To obtain much richer priors, our results indicate that we need to select not only parameters (Ãb , Ãw ) on the EOC but also an activation function satisfying Proposition 4." Prop 4 only applies when \sigma_b is small, so you additionally need to make sure \sigma_b small.- "In the ordered phase, we know that the output converges exponentially to a fixed value (same value for all Xi), thus a small change in w and b will not change significantly the value of the loss function, therefore the gradient is approximately zero and the gradient descent algorithm will be stuck around the initial value." But you are using Adam, not gradient descent! Adam explicitly corrects for this kind of gradient vanishing, so a small gradient can't be the reason for the lack of training success.Experimental issues:- "We use the Adam optimizer with learning rate lr = 0.001." You must tune the learning rate independently for each architecture for an ubiased comparison.- In figure 6b, why does tanh start with a high accuracy and end up with a low accuracy? I've never seen a training curve like this ... This suggests something is wrong with your setup.- You should run more experiments with a larger variety of activation functions.Minor comments: - "Therefore, it is easy to see that for any (Ãb , Ãw ) such that F is increasing and admits at least one fixed point,wehaveKÆ,corr(Ãb,Ãw) e qwhereqistheminimalfixedpoint;i.e. q := min{x : F(x) = x}." I believe this statement is true, but I also think it requires more justification.- At the end of page 3, I think \epsilon_r should be \epsilon_qThere are some good ideas here, but they need to be developed/refined/polished much further before publication. The above (non-exhaustive) list of issues will hopefully be helpful for this. This paper formulates feature attribution from a feature selection perspective, and compares EFS (Exclusive Feature Selection) and IFS (Exclusive Feature Selection), which shows IFS is a better fit for feature attribution.[+] The paper is well-structured and the proposed approach is clearly presented.[-] It would helpful if the author could discuss the time complexity of proposed methods and compare the running time with baseline methods in evaluation.[-] My major concern on this paper is the significance, as the contribution of the paper seems to be very limited.    1) Formalizing the feature attribution problem as a feature selection problem is straightforward. IFS and EFS are just Forward and Backward stepwise feature selection, which are classic feature selection schemes. Applying them to feature attribution/saliency map does not seem to have much technical contribution.    2) One claimed contribution of this paper is that existing feature attribution methods can be viewed as approximation of IFS and EFS. However, this contribution also seems to be minor. As many feature selection methods are known to be approximation of backward or forward stepwise feature selection, it is straightforward to show the connection between other feature attribution methods and IFS/EFS.In conclusion, I would recommend to reject this paper due to the limited novelty and technical contribution. This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input.My biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage).It would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet's depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here.That said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio).The fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process.I think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model.Overall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used.Miscellany:- In the abstract: "is substantially better in quality", compared to what?- In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don't tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective.- Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3.- "Conditioning Deep Generative Raw Audio Models for Structured Automatic Music" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference.- In the contributions of the paper, it is stated that "the generated audio is practically identical to ground truth as can be seen in Figure 4" but the CQTs in this figure are visibly different.- I don't think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio).- At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued.- Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning.- Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities.- The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without).- In Section 4.3, specify the unit, i.e. "Delta &lt; 1 second".- For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks.- In Section 4.3 under "global conditioning", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used? This paper gives a theoretical analysis of adversarial examples, showing that (i) there exists a tradeoff between robustness in different norms, (ii) adversarial training is sample inefficient, and (iii) the nearest neighbor classifier can be robust under certain conditions. The biggest weakness of the paper is that theoretical analysis is done on a very synthetic dataset, whereas real datasets can hardly be conceived to exhibit similar properties. Furthermore, the authors do not give a bound on the probability that the sampling conditions for the robust nearest neighbor classifier (Theorem 1) will be satisfied, leading to potentially vacuous results.While I certainly agree that theoretical analysis of the adversarial example phenomenon is challenging, there have been prior work on both analyzing the robustness of k-NN classifiers (Wang et al., 2018 - http://proceedings.mlr.press/v80/wang18c/wang18c.pdf) and on demonstrating the curse of dimensionality as a major contributing factor to adversarial examples (Shafahi et al., 2018 - https://arxiv.org/abs/1809.02104, concurrent submission to ICLR). I am very much in favor of the field moving in these directions, but I do not think this submission is demonstrating any meaningful progress.Pros:- Rigorous theoretical analysis.Cons:- Results are proven for particular settings rather than relying on realistic data distribution assumptions.- Paper is poorly written. The authors use unnecessarily complicated jargon to explain simple concepts and the proofs are written to confuse the reader. This is especially a problem since the paper exceeds the suggested page limit of 8 pages.- While it is certain that nearest neighbor classifiers are robust to adversarial examples, their application is limited to only very simple datasets. This makes the robustness result lacking in applicability.- Weak experimental validation. The authors make repeat use of synthetic datasets and only validate their claim on MNIST as a real dataset. ML models are trained on a predefined dataset formed by a set of classes. Those classes use to be the same ones for training and testing. However, what happen when during testing time images with classes unseen during training are shown to the model? This article focus in this problem which is not currently taking much attention by the mainstream research community and is of great importance for the real world applications.This article tries to detect areas of the image where those out-of-distribution situations appear in semantic segmentation applications. The approach used is by training a classifier that detects which pixels are out of distribution. For training two datasets are used: the dataset of interest and another different one. The classifier learns to detect if a pixel is from the dataset of interest or from another distribution.The main problem I found with this article is that I couldn't fully understand it. Maybe because the text needs a bit more of review and improvement or maybe because Im not very familiar with the topic. Moreover the article is 10 pages while it is encouraged to be 8. I find that the method of the paper is quite simple and can be explained more straight forward and in less pages. The related work section overlaps a lot with the intro, I suggest to combine both. First two paragraphs of the method seam that should be in the intro. Model details from the experiments I consider that should be explained in the method. I miss a figure explaining the architecture of the model. Why using the semantic segmentation model proposed and no something standard? For instance Tiramisu (That is also based on dense layers). Note that the method used for semantic segmentation is 10 points lower than the SOTA in Cityscapes. Figure 1 is impossible to read as the captions are too small. The representations of figures 2-5 are difficult to interpret. There is no comparison to SOTA Overview: This nicely written paper contributes a useful variance reduction baseline to make the recent formalism of the DiCE estimator more practical in application. I assess the novelty and scale of the current contribution as too low for publication at ICLR. Also, the paper includes a few incorrect assertions regarding the control variate framework as well as action-dependent baselines in reinforcement learning. Such issues reduce the value of the contribution in its current form and may contribute to ongoing misunderstandings of the control variate framework and action-dependent baselines in RL, to the detriment of variance reduction techniques in machine learning. I do not recommend publication at this time.Pros:The paper is well written modulo the issues discussed below. It strikes me as a valuable workshop contribution once the errors are addressed, but it lacks enough novelty for the main conference track.Issues:* (p.5) "R_w and b_w are positively correlated by design, as they should be for variance reduction of the first order gradients."This statement is not true in general. Intuitively, a control variate reduces variance because when a single estimate of an expectation of a function diverges from its true value according to some delta, then, with high probability, some function strongly correlated with that function will also diverge with a similar delta. Such a delta might be positive or negative, so long as the error may be appropriately modeled as drawn from some symmetric distribution (i.e. is Gaussian).Control variates are often estimated with an optimal scaling constant that depends on the covariance of the original function and its control variate. Due to the dependence on the covariance, the scaling constant flips sign as appropriate in order reduce variance for any delta. For more information, see the chapter on variance reduction and subsection on control variates in Sheldon Ross's textbook "Simulation."The fact that a control variate appears to work despite this is not surprising. Biased and suboptimal unbiased gradient estimators have been shown to work well for reasons not fully explored in the literature yet. See, for example, Tucker et al.'s "Mirage of Action-Dependent Baselines", https://arxiv.org/abs/1802.10031.Since the authors claim on page 6 that the baseline is positively correlated by design, this misunderstanding of the control variate framework appears to be baked into the baseline itself. I recommend the authors look into adaptively estimating an optimal scale for the baseline using a rolling estimator of the covariance and variance to fix this issue. See the Ross book cited above for full derivation of this optimal scale.* The second error is a mischaracterization of the use and utility of action-dependent baselines for RL problems, on page 6: "We choose the baseline ... to be a function of state ... it must be independent of the action ...." and "it is essential to exclude the current action ... because the baselines ... must be independent of the action ... to remain unbiased." In the past year, a slew of papers have presented techniques for the use of action-dependent baselines, with mixed results (see the Mirage paper just cited), including two of the papers the authors cited.Cons* Much of paper revises the DiCE estimator results, arguing for and explaining again those results rather than referring to them as a citation. * I assess the novelty of proposed contribution as too low for publication. The baseline is an extension of the same method used in the original paper, and does not generalize past the second order gradient, making the promising formalism of the DiCE estimator as infinitely differentiable still unrealizable in practice.* The experiments are practically identical to the DiCE estimator paper, also reducing the novelty and contribution of the paper. This paper uses several different techniques in IL and RL to improve performance on 6D robot grasping. It uses an expert planner OMG to collect initial data for BC as well as for online IL via Dagger. The uses DDPG to further train as well as fine tune on new unlabeled objects.The topic is very relevant and of current interest as more real world applications will need more than just 2D grasping that bin-picking has addressed and related work is sufficiently discussed.The technical contribution seems weak as the paper mostly explores known methods and well-known 'trade-tricks' (goal conditioning or loss on goal) towards a grasping centric problem which is also heavily explored as part of various RL tasks in literature. The main weakness of the work however is the lack of clear motivation for why such a complicated procedure is necessary compared to the expert planner already being used - the experiments aren't designed to address this question.- Using a planner as an expert for IL is common practice and I don't think counts as a major contribution as presented at the end of the introduction.- The bulk of IL experiments focus on what input representation is helpful. While it is not surprising that 3D inputs like point clouds would be better for 6D grasping these are more suitable as ablations than main experiments investigating the proposed method itself, compared to other sota approaches learning based or otherwise.- In several places 'contact-rich' and 'different dynamics' is motivated without clear explanation early on until the experiments identified what the setup was. The former does not seem to be well explored in the experiments, '...especially in those contact-rich scenarios...'. Aren't all grasping problems contact rich (unless only reaching to a pre-grasp is being considered) or were there some new scenarios constructed to specifically study the relative effects of contact?- Results in table 1 and figure 4 present mean statistic from 3-5 runs. This seems small, variance bands should be shown in figure 4 to see if the small number of sample are sufficient to capture the full picture.- The problems studied could be addressed by planar grasping as well. Complex setups with clutter, etc would better motivate if the presented approach is able to scale to scenarios where 6D grasping is necessary. Other comments:- Does not including the BC loss for some samples in the batch (or between training iteration) cause any discontinuities or make learning unstable, as the loss landscape discontinuously changes? Summary:The authors propose an approach to calibrate conditional distribution estimation models. The approach uses normalizing flows to transform an existing model's predictions into a prediction that better matches the empirical quantiles to the theoretical quantiles. After this remapping procedure, the authors introduce a new plot to visualize calibration. Empirical benchmarks are run on a suite of UCI datasets.Review:I'm not sure what is really that interesting here. My high-level problems:- The remapping that the authors propose is just using a normalizing flow with a simple quantile calibration. Why do we need normalizing flows for this at all? Any model can be recalibrated using any other model here. Is there some special reason for normalizing flows here?- The new plot introduced is more confusing than illuminating. I really didn't follow it at all. It is very crowded and takes a lot of inspection to understand what is going on. I suspect all of this could have been visualized much clearer by using a handful of simpler plots that are straightforward.- The benchmarks do not really compare against very competitive models. The authors choose to use a model that was only pushed to the arxiv a month ago as the baseline. Why? Then the alternative choices are strawmen: a Bayesian linear regression model, a variational dropout model, etc. There is a wealth of conditional distribution estimation literature with companion code publicly available on github (NADE, MAF, MDNs, etc). Why not use those?- Does this really help us do anything new? Is this just "my model is 0.1% better on 8 UCI datasets than 4 other models"? Seems like a pretty uninspiring result if that's the idea.Overall, I just don't know what to see as the big contribution here. The paper feels a little rushed and could use a slower, more methodical pace where the authors carefully think through the contribution(s) and why they're necessary. A more thorough comparison to related work is also called for. Overall, I found the main idea of this paper very interesting and the experimental results promising; however, there were several major and minor technical issues with the work that need to be resolved.--- Major comments ---1. There appear to be at least two major technical issues:    a. A substantial portion of the work is based on the author's assertion that $Y_i(0) = Y_i - \psi(X_i)T_i$ which is not, in general, true. We can see this with a simple counter-example. Let $Y(0)$ be a binary variable. Then $\psi(X) = E[Y(1)|X] - E[Y(0)|X]$ will be some value in $[-1,1]$, but if $\psi$ is any value other than $1$, $0$, or $-1$, then $Y_i - \psi(X_i)T_i$ will be non-binary and thus not equal to $Y_i(0)$. This assertion only holds if $X$ and $T$ uniquely determine $Y$, which is not generally the case. Thus, it remains to be shown that enforcing equation (8) is equivalent to enforcing equation (5).    b. In the first condition of Theorem 1, the authors assume that either $\hat{f}$ or $\hat{\pi}$ is consistent. If $\hat{f}$ and $\hat{\pi}$ were estimated separately and used as plug-in estimators, as in Chernozhukov et al. (2018), this would be a reasonable assumption, as it would be up to the user to show that their nuisance parameter estimates are consistent. However, $\hat{f}$ and $\hat{\pi}$ are estimated using Equation (6) and thus it is up to the authors to show that solving (6) gives consistent estimates of $\hat{f}$ or $\hat{\pi}$ under correct model specification. This very well may be the case, but it cannot simply be assumed.2. I found the introduction very hard to follow. In particular, it is never really made clear what the motivation for the work is. Extrapolating from the experiments, it appears that the goal is to derive an estimator with lower variance than existing estimators, but that is not stated in the intro. I would recommend restructuring to something like: CI from observational data is important because XYZ. It is desirable that estimators for the ATE have the lowest possible variance. We propose a new estimator that, empirically, has lower variance than a variety of state-of-the-art causal estimation methods. We do this by translating the exchangeability assumption into an explicit constraint on the estimator objective. This constraint reduces variance because ABC. Additionally, lit review portion of the intro reads as a random list of methods with no clear connection between them or to the proposed method. Some specific issues are:    a. It is incorrect that "the outcome of an alternative treatment has to be estimated". In particular, IPW methods do not do this. In fact, methods based on estimating the conditional expected outcome do not do this either since, as stated above, the expected outcome is not equal to the outcome.     b. "to find similar subjects" --> "to find similar subjects who received different treatments"    c. "seek weights such that the treatment assignment is unassociated with the covariates" --> "reweights the data so that treatment assignment is unassociated with the covariates in the reweighted distribution"    e. "However, they do not require the...": None of the methods described here require this, but they do require that they be conditionally independent. Also, potential outcomes have not yet been defined.3. Page 2, Contribution section, "Compared to other estimators, its asymptotic variance is strictly smaller.": It is my understanding that TMLE is also semi-parametrically efficient, so I believe this statement is incorrect as are similar statements in the "Comparison to other estimators" section. Further, both Chernozhukov et al. (2018) and the work on CV-TMLE show $\sqrt{N}$-consistency without relying the Donsker class assumptions.--- Minor comments ---1. In equations (4) and (5), the authors jump from a constraint on the true distribution to a constraint on the empirical distribution. I recommend adding a statement indicating this.2. What score was used to select the best $\lambda$ on the validation set? 3. RHS of Equation (11): $\epsilon$ --> $\hat{\epsilon}$ or drop the "evaluated at" bar. This paper adopted Stein's method to connect an explicit density estimator and an implicit sample generator to propose an objective function for deep generative learning.This paper is written well and the organization is very clear.However, this paper is very similar to a NeurIPS-19 paper (Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test).Besides, the authors didn't cite this paper in the current submission.First, the top-level idea is the same.1. They all adopted two generative models. One is explicit and the other is implicit.2. They all adopted Stein's method to connect these two generative models.Second, important technical details are similar.1. They all used energy-based model in the explicit part.The energy-based model is used to mimic the underlying distribution of the real data.2. They all used Stein's method to avoid solving the normalization constant.Stein's method is a likelihood-free method that depends on the distribution only through logarithmic derivatives.When taking derivatives, the normalization constant will be eliminated.Third, these two papers have the same target.They hope the explicit generative model characterize the formulation of the distribution andthe implicit one produces vivid or genuine-looking images.The novel part of this submission is the introduction of kernel Sobolev dual norm and Moreau-Yosida regularization.There is a big gap between the optimization formulations in Equation (3), Theorem 1 and Theorem 2 and the experimental results shown in Section 5.Besides, there are no open source codes provided.It is very hard for me to figure out the details of the experiments and meantime to check the reproducibility of this paper.In summary, I hope that the authors will correctly cite the closely related NeurIPS-19 paper,and clearly demonstrate their completely new contributions as compared to the NeurIPS-19 paper.Since ICLR is a highly selective conference,the originality and significance of one submission will always be in the first priority.Although the writing quality of this paper is good, I cannot accept this paper in current state. This paper proposes effective gradient flow (EGF), which is a layer-wise normalized gradient flow. Compared to (unnormalized) gradient flow, the paper shows that the proposed EGF is (slightly) better correlated with metrics like test loss and test accuracy (see Table 1). Given that this claim is supported with experimental results, the paper would become much stronger if a larger number and a more diverse set of data-sets were used (in addition to CIFAR-10 and CIFAR-100, which are two very similar image-data-sets) as to show that the claim holds more generally. Apart from that, given that the correlations are (only) about 0.4 in Table 1, it seems that only some aspects are explained by EGF. This said, if we continue with the insight that EGF is helpful, how would EGF help to design better sparse models and optimization methods? Why is it easier to compute EGF instead of directly the relevant metrics (like test loss etc.) to determine what training methods and model architectures work better?  In fact, Figure 2, top row, exactly does this: test-accuracy is directly used to determine which training methods work best for sparse models. What is the additional insight/benefit of using EGF (like in the bottom row of Figure 2) instead of test-accuracy ?The paper also proposes the SC-SDC framework, see also results in Table 3. The key idea is to compare  sparse and dense networks that have the same number of (non-zero) weights. While this is a good start, I am not sure that this is really a fair comparison, though. I would expect the sparse network to have a larger capacity than a dense network with the same number of parameters. While there are many ways to see this, the simplest  might be that  a weight in a sparse model requires two parameters, its value and its index if we wanted to encode the model. Another way to see it might be that a dense model can be pruned to become sparse(with fewer weights), but without losing much prediction accuracy. As a consequence of potentially comparing sparse and dense networks of different capacities, the results in Table 3 might be biased in favor of sparse models. From this perspective, it is remarkable that sparse models do not clearly outperform dense models in Table 3, which might indicate that the training of sparse models with the analyzed approaches still suffers from the problems outlined earlier in the paper.Equipped with the proposed methods EGF and SC-SDF, the paper then analyzes several (standard) approaches like batch normalization etc as to determine which of these approaches are useful for training sparse models.  Again, given that only two very similar data-sets were used (CIFAR-10 and 100), it is unclear if the found results would generalize to other data sets. Moreover, it is not clear why EGF and SC-SDF are needed to determine which training-methods work well for sparse models. In fact, Figure 2 (top row) directly shows the test-accuracy for the various approaches--i.e., without using the proposed EGF and SC-SDF. EGF is shown in the bottom row in Figure 2, as to illustrate that EGF has a similar behavior as test accuracy in the first row in Figure 2. What is the additional insight obtained from EGF compared to using only test-accuracy as to determine that  batch-normalization works well ? Some minor points:How are test loss and test accuracy defined in this paper, and what is the difference? I did not immediately find the answer in the referenced paper (Jiang 2019).Are really all 6 digits statistically significant in Table 1 ?Figures 7 and 9 in the appendix are missing the actual image.The paper has several typos, like and., or the first sentence in Appendix A The paper presents a meta-RL method extends previous work on meta-RL by including an imitation learning step. It is mentioned that the behaviour closing part of this extended algorithm can come from a teacher or some other source. Since this extension is the major contribution, it must be discussed in more detail. I also don't understand why My second problem with the paper is reproducibility. The purpose of OpenAI is comparability and reproducibility of algorithms. It is not sufficient to simply state that you have used TensorFlow. We need information about the architecture, etc so that the results can be reproduced. Also, since you use OpenAI, the score for each experiment should be compared to the best scores known from the website so that the performance of the new algorithm can be compared to others. [Summary]- This work proposes a new complex latent space described by convolutional manifold, and this manifold can map the image in a more robust manner (when some part of the image are to be restored).[Pros]- The results show that the latent variable mapped to the image well represents the image, and it will be helpful for the image restoration problem.- it seems novel to adapt the idea of DIP for defining complex latent space.[Cons]- The main concern is that there is no guarantee that the defined latent space is continuous. It means that it is difficult to judge whether the interpolated point (phi_in, s_in) between two points: (phi_1, s_1) and (\phi_2, s_2), will be matched to the image distribution. Equation 2 in the paper seems that it just fit the generator parameter theta to map the phi_i and x_i and memorize the mapping between the training images and the given latent convolutional variables. If the proposed algorithm just memorizes the training image and map them into given the latent convolution, the result cannot justify the proposal that the author proposes a new latent space.[Summary]- This work proposes an interesting idea of defining complex latent space, but It is doubtful that this work just memorized the mapping between the training images and the latent convolutional parameters.- I want to see the (latent space) interpolation test for the proposed latent convolutional space. If the author provides a profound explanation of the problem, I would consider changing the rating. The authors present a heuristic method to detect periodicity in time series. It extends a previous approach in dealing with noise and the setting of multiple periodicities.The topic does not match the scope of ICLR and would be better suited for a different venue.The method is demonstrated in a purely experimental fashion. However, without detailed inspection of the datasets it remains unclear in what cases the heuristics apply and where they fail. A more thorough analysis of the robustness of the algorithm is necessary, in particular a detailed presentation of failure cases. This paper introduces a method to do period detection. It builds off of the autoperiod method by adding density clustering, a lowpass filter, and a linear detrending after auto correlation.The results section was very vague. The process of  generating the synthetic signals was not specific. There were no visualizations, which would help the reader understand how this method performs better. Visualizations would have been especially useful for the real datasets.Having said this, I don't think this paper is fit for this conference. Summary:This paper proposes a novel method for generating novel molecules with some targeted properties. Many studies on how to generate chemically valid molecular graphs have been done, but it is still an open problem due to the essential difficulty of generating discrete structures from any continuous latent space. From this motivation, the 'constrained' Bayesian optimization (BO) is applied and analyzed. Posing 'constraints' on the validity is realized by probability-weighting onto the expected improvement scores in BO. The 'validity' probability is learned beforehand by Bayesian neural nets in a supervised way. As empirical evaluations, two case studies are presented, and quality improvements of generated molecules are observed.Comment:- The presentation would be too plain to find what parts are novel contributions. Every part of presentations seems originated from some past studies at the first glance. - In this paper, how to pose the validity 'constraint' onto Bayesian optimization would be the main concern. Thus if it is acquired through supervised learning of Bayesian neural nets in advance, that part should be explained more in details. How do we collect or setup the training data for that part? Is it valid to apply such trained models to the probability weighting P(C(m)) on EI criteria in the test phase? Any information leakage does not happen?- The implementations of constrained BO is just directly borrowed from Gelbart, 2015 including parallel BO with kriging believer heuristics? The description on the method is totally omitted and would need to be included.- How training of Bayesian neural nets for 'Experiment II' are performed? What training datasets are used? Is it the same as those for 'Experiment I' even though the target and problem are very different?Pros:- a constrained Bayesian optimization with weighing EI by the probabilities from pre-trained Bayesian neural nets applied to the hot topic of valid molecule generations.- Experiments observe the quality improvementsCons:- unclear and insufficient descriptions of the method and the problem- novel contributions are unclear iRDA Method for sparse convolutional neural networks This paper considers the problem of training a sparse neural network. The main motivation is that usually all state of the art neural networks size or the number of weights is enormous and saving them in memory is costly. So it would be of great interest to train a sparse neural network. To do so, this paper proposed adding l1 regularizer to RDA method in order to encourage sparsity throughout training. Furthermore, they add an extra phase to  RAD algorithm where they set the stochastic gradient of zero weights to be zero. They show experimentally that the method could give up to 95% sparsity while keeping the accuracy at an acceptable level. More detail comments: 1- In your analysis for the convergence, you totally ignored the second step. How do you show that with the second step still the method converge? 2- \bar{w} which is used in the thm 1, is not introduced. 3- In eq 5, you say g_t is subfunction. What is it? 4- When does the algorithm switch from step 1 to step 2? 5- In eq 35 what is \sigma? 6- What is the relation between eq 23 and 24? The paper says 23 is an approximation for 24 but the result of 23 is a point and 24 is a function. 7- What is MRDA in the Fig 1? The submission made a few modifications to the RDA (regularized dual averaging) optimization solver to form the proposed "iterative RDA (iRDA)" algorithm, and shows that empirically the proposed algorithm could  reduce the number of non-zero parameters by an order of magnitude on CIFAR10 for a number of benchmark network architectures (Resnet18, VGG16, VGG19).The experimental result of the paper is strong but the algorithm and also a couple of statements seem flawed. In particular:* For Algorithm 1, consider the case when lamda=0 and t -&gt; infinity,  the minimization eq (28) goes to negative infinity for any non-zero gradient, which corresponds to an update of infinitely large step size. It seems something is wrong.* Why in Step 2 the algorithm sets both g_t and w_t to 0 during each iterate? It looks so wrong.*The whole paper did not mention batch size even once. Does the algorithm apply only with batch size=1? *What is the "MRDA" method in the figure? Is it mentioned anywhere in the paper?*What are "k", "c"  in eq (25)? Are they defined anywhere in the paper?*Theorem states 1/sqrt(t) convergence but eq (28), (31) have updates of unbounded step size. How is this possible? This paper claims to propose a new iRDA method. Essentially, it is just dual averaging with \ell_1 penalty and an \ell_2 proximal term. The O(1/\sqrt{t}) rate is standard in literature. This is a clear rejection. The authors present an algorithm that incorporates deep learning and physics simulation, and apply this algorithm to the game Flappy Bird.  The algorithm uses a convolutional network trained on agent play to predict the agents own actions given a sequence of frames.  Using this action estimator output as a prior over an action distribution (parameterized by a Dirichlet process), the algorithm iteratively updates the action by rolling out a ground-truth physics simulator of the environment, observing whether this ground-truth simulation yields negative reward, and updating the action accordingly.While I find the authors' introductory philosophy largely compelling (it draws inspiration from developmental psychology, learning to model the physical world, and the synthesis of model-based and model-free learning), I have concerns with most other aspects of the paper.  Specifically, here are a few points:1)  The authors only apply their algorithm to a single game (Flappy Bird), a game that has no previously established benchmarks.  In fact, while there is no prior work in the literature on this game (perhaps because it is considered very easy), some unofficial results suggest that it is solvable by a straightforward application of existing methods (see this report:  http://cs229.stanford.edu/proj2015/362_report.pdf).  The authors do apply one baseline (out-of-the-box DQN) to this game, but the reported scores are suspiciously low, particularly in light of the report linked above.  No training curves or additional baselines are shown, and no prior work on this game in the literature exists to compare against.2)  The authors algorithm uses privileged information which eliminates the possibility for a fair comparison to baselines.  Specifically, their algorithm uses ground-truth state (not just image input), and a ground-truth physics simulator (which should be an enormous advantage).  Their one baseline (DQN) does not have either of these sources of privileged information, hence cannot be a fair comparison.3)  The authors algorithm is not general-purpose.  Because the algorithm itself uses a ground-truth environment-specific state, a ground-truth environment-specific simulator, and relies on a crash boolean (whether the bird hit a tree) specific to this game, it cannot be applied out-of-the-box on a different environment.4)  The authors make some claims that are too strong in light of the reported results.  For example, they claim that the performance of the model outperforms all model-free and model-based approaches (section 1), while they do not even compare against any model-based baselines (and only a single model-free baseline, DQN, which is not state-of-the-art anymore).Overall, I would recommend the authors choose a game or set of games that has/have established baselines in the literature, come up with a general-purpose algorithm which doesnt rely on a ground-truth physics simulator, and more rigorously compare to existing methods. This paper proposes a new metric called the image score that compares the similarity of activation between a given image with a pool of groundtruth images. The paper finds it useful for semi-supervised learning with self-teaching, where the network picks the most confident sample and use the network prediction as the label. It finds that the proposed method is better than 1) not using the unlabeled data and 2) using softmax as an indicator for model prediction certainty.Motivation: The introduction begins by motivating the interpretability story of deep learning, but I dont see gaining any more interpretability by reading the rest of the paper. The paper proposes to improve interpretability by assigning a score to each individual example, but then the obtained scores are not properly analyzed in the paper, and only final classification accuracy is evaluated. What are the training samples that makes the model make certain decision at test time? How to measure the correlation between the usefulness of training samples and the proposed image score? These questions left unanswered in the paper. Figure 1 helps a little bit, but then the top row is not necessarily the bad images, but maybe hard examples that needs extra attention to learn. Therefore, I think the end results presented in the experiments do not align with the motivation. Rather than shooting for interpretability, this is just another semi-supervised learning paper.Models: The major issue of this paper is the model formulation that is not well motivated. The intuition of how the authors come up with the equation for computing the image score is not well explained. Hence the formulation seems very ad-hoc, and it is unclear why this is the selected method.Experiments: As a semi-supervised learning paper, a common setting for CIFAR-10 is to use 4k labeled images. Here, the method uses 30k, which is 7.5x the size of the usual setting. It also does not compare to prior semi-supervised learning work (e.g. one of the recent one is: https://arxiv.org/abs/1711.00258). The only two baselines discussed here are weak. Also the improvement from the baselines by using the proposed method is not very significant.Comparison: Figure 2-4 shows some positive correlation between the accuracy and score, which is fair, but it doesnt compare to any baselines--the only one we have is softmax baseline and it is not shown in the figure.In conclusion, I couldnt see how the paper improves interpretability as claimed in the introduction. The proposed method seems ad-hoc, without any justification. Being considered as a semi-supervised learning paper, it lack significant amount of comparison to prior work and adopting a common semi-supervised benchmark. Due to the above reasons, I recommend reject.---Minor points:...almost all of the existed works investigate only the models and ignore the relationship between models and samples. This is over-exaggerated. I believe most of the visualization techniques are dependent on the actual input samples. It is true to say about training samples not samples in general.all correctly classified images should have similar chain of activation, while incorrectly classified images should have very different activations both within themselves and with correctly classified images. This claim seems not backed up. How do you know it is the case for all correctly classified images? What defines similar/different? This paper attempts to establish a notion of thermodynamics for machine learning. Let me give an attempt at summary. First, an objective function is established based on demanding that the multi-information of two graphical models be small. The first graphical model is supposed to represent the actual dependence of variables and parameters used to learn a latent description of the training data, and the model demands that the latents entirely explain the correlation of the data, with the parameters marginalized out. Then, a variational approximation is made to four subsets of terms in this objective function, defining four "thermodynamic"  functionals. Minimizing the sum of these functionals puts a variational upper bound on the objective. Next, the sum is related to an unconstrained Lagrange multiplier problem making use of the facts (1) that such an objective will likely have many different realizations of the thermodynamic functionals for specific value of the bound and (2) that on the optimal surface the value of one of the functional can be parametrized in terms of the three others. If we pick the entropy functional to be parameterized in terms of the others, we find ourself precisely in the where the solution to the optimization is a Boltzmann distribution; the coefficients of the Lagrange multipliers will then take on thermodynamic interpretations in of temperature, generalized chemical potentials, etc. At this point, the machinery of thermodynamics can be brought to bear, including a first law, Maxwell relations (equality of mixed partial derivatives), etc.I think the line of thinking in this paper is very much worth pursuing, but I think this paper requires significant improvement and modifications before it can be published. Part of the problem is that the paper is both very formal and not very clear. It's hard to understand why the authors are establishing this analogy, where they are going with it, what's its use will be, etc. Thermodynamics was developed to explain the results of experiments and is often explained by working out examples analytically on model systems. This paper doesn't really have either such a motivation or such examples, and I think as a result I think it suffers.I also think the "Tale of Two Worlds" laid out in Section 2 requires more explanation. In particular, I think more can be said about why Q is the the "world we want" and why minimizing the difference between these worlds is the right way to create an objective. (I have no real problem with the objective once it is derived.) Since this paper is really about establishing this formal relationship, and the starting point is supposed to be the motivating factor, I think this needs to be made much clearer.The I(Z_i, X_i, Theta) - I(X_i, Z_i) terms could have been combined into a conditional mutual information. (I see this is discussed in Appendix A.) This leads to a different set of variational bounds and a different thermodynamics. Why do we prefer one way over the other? At the level of the thermodynamics, what would be the relationship between these different ways of thinking? Since it's hard to see why I want to bother with doing this thermodynamics (a problem which could be assuaged with worked examples or more direct and clear experiments), it's hard to know how to think about this sort of freedom in the analogy. (I also don't understand why the world Q graphical model is different in Appendix A when we combined terms this way, since the world Q lead to the objective, which is independent of how we variationally bound it.) I think ultimately the problem can be traced to the individual terms in the objective (7) not being positive definitive, giving us the freedom to make different bounds by arranging the pieces to get different combinations of positive definite terms. How am I supposed to think about this freedom?In conclusion, I would really like to see analogies like this worked out and be used to better understand machine learning methods. But for this program to be successful, I think a very compelling case needs to be made for it. Therefore, I think that this paper needs to be significantly rewritten before it can be published. This paper proposes a justification to one observation on VAE: "restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization". The explanation given in this work is based on Gaussian mean-field approximation.I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example- the sentence under eq. (2)- the sentence "Bacause the identity of the datapoint can never be learned by ..." What is the identity of a dat point?It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods. Somehow, those connections are not clear to me.Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -&gt; Y -&gt; Z and we have I(X,Y) &gt;= I(X,Z). However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D should be: D &lt;- \theta -&gt; \tilde{\theta} (if it is a generative model) or D -&gt; \theta -&gt; \tilde{\theta} (if a discriminative model). Either case, I don't think we can have the inequality in eq. (5). The authors propose a new method of measuring a knowledge within the learned CNN: the representations of CNN layers and word2vec embeddings and compared, and the similarity between them are calculate. The authors claim that the similarity score increases with learning time, and the higher layers of CNN have more similarity to word2vec embeddings than the lower layers..CNN and word2vec use different datasets. CNN uses the vision pixels and word2vec uses the words in the sentences. A certain amount of representation patterns can be expected to be shared, but surely the extent is limited (correlation 0.9 in Fig. 1). Because of this limitation, the proposed similarity measure must not be claimed as the measure of knowledge accumulation in CNN. In addition, the authors have to be precise in defining the measure and provide the information captured by the measure. In the literature, I can see something is shared by the two algorithms but do not know what is this something. The authors claim that semantics are shared, but replacing semantics to something does not make any difference in this manuscript. Further investigations and confirmations are needed to report which information is actually similar to each other.Minor: the 1 vs. 2 accuracy measure is not defined.In summary, the proposed measure may capture some information but the explanation about this information is unclear. The information seems to be a rough similar pattern of concept representations. Further rigorous investigation of the proposed measure is necessary to confirm which information is captured. The current version is not sufficient for acceptance. Summary:This paper proposed a few-shot learning approach for interactive segmentation. Given a set of user-annotated points, the proposed model learns to generate dense segmentation masks of objects. To incorporate the point-wise annotation, the guidance network is introduced. The proposed idea is applied to guided image segmentation, semantic segmentation, and video segmentation.Clarity:Overall, the presentation of the paper can be significantly improved. First of all, it is not clear what the problem setting of this paper is, as it seems to have two sets of training data of fully-annotated images (for training) and the combined set of point-wise annotated images and unannotated images (guidance images T in the first equation); It is not clear whether authors generate the second dataset out of the first one, or they have separate datasets for these two. Also, it is not clear how the authors incorporate the unannotated images for training. The descriptions on model architecture are also not quite clear, as it involves two components (g and f) but start discussing with g without providing a clear overview of the combined model (I would suggest changing the order of Section 4.1 and Section 4.2 to make it clearer). The loss functions are introduced in the last part of the method, which makes it also very difficult to understand. Originality and significance:The technical contribution of the paper is very limited. I do not see many novel contributions in terms of both network architecture and learning perspective.Experiment:Overall, I am not quite convinced with the experiment results. The method is compared against only a few (not popular) interactive segmentation methods, although there exist many recent works addressing the same task (e.g. Xu et al. 2016). The experiment settings are also not clearly presented. For instance, what is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets? How do you sample point-wise annotation from dense mask labels? How does the sampling procedure affect the performance? The performance of the guided semantic segmentation is also quite low, limiting the practical usefulness of the method. Finally, the paper does not present qualitative results, which are essential to understanding the performance of the segmentation system. Minor comments:1. There are a lot of grammar issues. Please revise your draft.2. Please revise the notations in equations. For instance,     T = {{(x_1, L_1),...} \cup {\bar{x}_1,...}    L_s = {(p_j,l_j):j\in{1,...,P}, l\in{1,...,K}\cup{\emptyset}}    Also, in the next equation, j\in\bar{x}_q} -&gt; p_ j\in\bar{x}_q} (j is an index of pixel) SummaryThis paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.The main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.Specifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.By performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.StrengthLearning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.This paper tackles this problem and showed results on various segmentation problems.WeaknessThe proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.This paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.For example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. In addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.For example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.There are some strong arguments that require further justification. - In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).However, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.Overall commentI believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. Especially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches. The paper formulates a definition of easy and hard examples and studies the properties and the training implications of such examples. The paper does not attempt to present insights that change training for the better (although suggests this could be future work), so the primary value it claims to add is our understanding of neural networks. I think the paper presents findings that most deep learning practitioners already find intuitive, which is why I think the paper falls short in its primary mission. An exposé like this could be valuable for an introductory text in deep learning, but I do not think the analysis meets the bar for a cutting edge insight and do not think it should be accepted.Strengths:- Quantifying easy and hard and using that as a starting point for further analysis is not a bad starting point at all.- The experiments form a good starting point for interesting analysis.- The paper is easy to follow and understand.Weaknesses:- The biggest weakness is that I just didn't find any of conclusions from this study to be that surprising or interesting. I think it's pretty obvious that neural networks start by learning the most immediately discriminative features ("frequent patterns"). The visual examples of easy and hard examples are not surprising at all (I think you get similar clusters if you just show bottom and top of model confidences, which I think many of us have). In section 7.1, the result that most misclassified examples are hard examples is presented as a surprising result. This is confusing, because this exactly what I would have expected given how you define easy/hard. It would be far more surprising if misclassified examples were all considered easy under your definition.- The paper only scratches the surface. In Figure 2, the results for the two different datasets are quite different. This means only two datasets is probably not enough for us to understand what is going on here in general. Just the conclusion that datasets may be different in terms of easy/hard samples does not take the analysis far enough. It's also unclear what the reader should make of these conclusions.- In Table 1, let's say the bottom 30% of samples are actually equally easy. This would mean that the "easy" examples are just a random 1/3 of those samples. Basically, I'm worried about the implications of having a hard cut-off at 10% and if there are situations where the bottom 10% actually changed quite a bit, but the broader picture of easy really didn't change that much. I guess I'm saying that I didn't quite gain confidence that definitions of easy/hard and matching rate are the correct way to go here and there might be a better metric that can look at the continuum of easy/hard from e = 0 to 1. You could have some kind of distance function where if an example moved from 5th percentile to 12th percentile, it would constitute a distance of 7. This is perhaps not the right thing either, but presenting an alternative metric and showing that the numbers (up to scale) and conclusions are unchanged would be nice.Other comments:- The new terminology of "contradicted pattern" and "non-contradicted pattern" is a bit confusing. Why aren't you just calling these "non-discriminative" and "discriminative"? If a mantis and a ladybird are both typically on a leaf, the leaf is not discriminative for this task. However, if a mantis and a boat are typically on differently colored backgrounds, the background is discriminative.Minor comments:- page 1, "easy and hard examples differ on various CNNs architectures" -&gt; "CNN"- page 2, "as a criteria" -&gt; "criterion"- page 2, "We then redefine easy and hard" -&gt; don't you mean just "define"? Or do you mean that the words already have casual meanings, so this is a redefinition? I still think "define" is less confusing here.- page 2, I think it's confusing that both easy and hard use the threshold \tau, suggesting it is the same. Maybe put a subscript to make it clear that the two \taus are different.- page 6, "accuracy does not drop" -&gt; could use a "does not *even* drop" for clarity- page 7, "7.1 Do misclassified examples in validation dataset are hard examples": "Do"-&gt;"Are", remove "are" This paper proposes a specific measure of difficulty for training examples called easiness. Easiness is based on training the model N times and counting the number of times an example is classified correctly. Based on this measure, they introduce matching rate as a measure of similarity of two architectures. Two architectures are suggested to be similar if the set of easy and hard examples is similar. The rest of the paper presents comparisons of architectures. Considering the problems below, I dont see any reliable contribution in this paper.- Why this specific definition of easiness? Can you compare to simply using loss as a measure for the difficulty of an example?- e_t seems to be measuring the variance of training on a single example. If there is only one example that is always classified correctly, the denominator can be simplified to K. It doesnt tell us how many training iterations it takes to fit that example.- Why this specific formulation for matching rate? Why not a more common measure of similarity between sets such as intersection over union (IoU)? Can you suggest any references using a similar similarity score?- Numbers in Table 1 do not seem particularly big to support the claim in section 4 that ...CNNs start learning from the *same* examples even if CNN architectures are different. 0.20 is definitely bigger than random 0.1 for the matching rate but it still means only a 20% match.- Random 0.1 is redundant in table 1.- In section 4, define contradicted patterns.- Are all images in Figure 1 for one model? How does it compare to visualizing examples according to their loss?- The conclusion in section 5 says ... different CNNs start learning from similar patterns. As mentioned above, easiness and consequently matching rate do not provide information about the progress of training and only final trained models. Regardless, this conclusion does not seem particularly unexpected or informative.- Section 6 proposes to test a model on data with a different structure from data provided in training. This is a distribution mismatch and the model is not trained to handle. This paper considers an optimization problem defined on the intersection of multiple manifolds and the intersection is not a manifold. An optimization algorithm is proposed and its convergence analysis is given. An experiment of neural network with batch normalizatiion is used to demonstrate the performance of the algorithm.The problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. In addition, the convergence analysis is not complete. See more details below. Therefore, I donot think this paper can be published at this stage.*) P2, Section 2.1, line 2: The statement "A manifold is a subspace of R^n$ is not true in general.*) P2, Section 2.1, line 7: The statement "manifold is not a linear space" is not true in general. A manifold can be a linear space, such as the vector space R^n.*) P2, Section 2.1, below (2): The statement "Riemannian gradient is the orthogonal projection of gradient \nabla f(x) ..." is not true in general.*) P3, (3) and (4): what is the definition of $h_k^{(1)}$ and $h_k^{(2)}$. Are they arbitrary or the ones given on Page 4?*) P4, Theorem 2.3: the iterates {x_k} converges in the sense that \|gradf(x_k)\| goes to 0. Does {x_k} go to the intersection of the two manifolds \mathcal{M}_1 and \mathcal{M}_2? To complete the proofs, the author may need to show that \|gradf(y_k)\| goes to 0 and {x_k} and {y_k} have the same limit.*) P5, the grassmann manifold with p = 1: G(1, n), is called projective space, and the Stiefel manifold with p = 1: St(n, 1) is called the unit sphere.*) P6, the discussion of the intersection of G(1, n) and St(n, 1) does not make sense to me. G(1, n) is a quotient manifold, which is not a submanifold of R^n. Given a quotient manifold, the typical way in optimization framework is to choose representation of the quotient manifold. Fortunately, the projective space has a global orthogonal section, which is the unit sphere. In other words, G(1, n) is diffemorphisic to the unit sphere St(n, 1), and even can be isometric if appropriate Riemannian metrics are used on G(1, n) and St(n, 1). Therefore, I don't understand the notion of the intersection of G(1, n) and St(n, 1). (Since the reviewer was unclear about the OpenReview process, this review was earlier posted as public comment)Most claims of novelty can be clearly refuted such as the first sentence of the abstract "...This work presents a new approach to active anomaly detection..." and the paper does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.            1. Related Works: "...active anomaly detection remains an under-explored approach to this problem..."          - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:            - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, "Ai2: Training a big data machine to defend," International Conference on Big Data Security, 2016.              2. "To deal with the cold start problem, for the first 10 calls of select_top...":          - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)              3. Many arbitrary hyper parameters as compared to simpler techniques:          - The number of layers, nodes in hidden layers.            - The number of instances (k) per iteration            - The number of pretraining iterations            - The number of times the network is retrained (100) after each labeling call            - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).              4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.            5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:          - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (pradeepr/arxiv npmix v.pdf)          - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)          - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)          - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)                6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.        - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.              7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).              8. Does not compare against current state-of-the-art Tree-based AAD          - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.              9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.            10. Does not propose any other query strategies than greedily selecting top.            11. Question: Does this support streaming? This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments. Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts. The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified. The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.I have several further concerns about this work:* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present. I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE? This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.Other comments:* In the introduction, an adversarial criterion is referred to as a "discriminative objective", but "adversarial" (i.e. featuring a discriminator) and "discriminative" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.* Some turns of phrase like "recently gained a flourishing interest", "there is still a wide gap in quality of results", "which implies a variety of underlying factors", ... are vague / do not make much sense and should probably be reformulated to enhance readability.* Introduction, top of page 2: should read "does not learn" instead of "do not learns".* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a "domain confusion loss"), contrary to what is claimed in the introduction.* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model. I think all claims about running time should be corroborated by controlled experiments.* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. "matching samples").* Section 3.1, "amounts to optimizing" instead of "amounts to optimize"* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one. As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to "Generating Sentences from a Continuous Space" by Bowman et al. (2016). This should probably be cited instead.* "circle-consistency" should read "cycle-consistency" everywhere.* MMD losses in the context of GANs have also been studied in the following papers:- "Training generative neural networks via Maximum Mean Discrepancy optimization", Dziugaite et al. (2015)- "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy", Sutherland et al. (2016)- "MMD GAN: Towards Deeper Understanding of Moment Matching Network", Li et al. (2017)* The model name "FILM-poi" is only used in the "implementation details" section, it doesn't seem to be referred to anywhere else. Is this a typo?* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?* The descriptor distributions in Figure 3 don't look like an "almost exact match" to me (as claimed in the text). There are some clearly visible differences. I think the wording is a bit too strong here. The authors propose an iterative method for discarding outlying training data: first, learn a model on the entire training dataset; second, identify the training examples that have high loss under the learned model; and then alternate between re-learning the model on the training examples that do not have high loss, and re-identifying the training examples with high loss under the new model. This method works for both supervised and unsupervised learning, and the authors show that in theory, their method has some convergence properties in the mixed linear regression and Gaussian mixture model settings. The authors also run some experiments on neural networks and datasets with synthetic noise to show the benefits of their proposed method.The problem of noisy datasets is relevant to almost all machine learning problems in the real world, and the authors' method shows promise as a straightforward way to increase performance on such noisy data. However, my opinion is that the authors need to make a stronger case, theoretically and/or experimentally, for why their method should be preferred to other methods. Detailed comments follow.== Experiments ==1) No comparisons are provided to other outlier detector methods (e.g., based on nearest neighbors, distance to centroid, influence functions, etc.) or techniques that also purport to deal with noisy labels (e.g., by modifying the learning algorithm or loss function). While there are too many existing methods to expect the authors to benchmark against all of them, it's important to at least have a couple of representative comparisons. 2) It'd be nice to have an ablative analysis to tease out the factors behind the gain in accuracy. For example, is the iteration important, or would a single pass suffice? How robust is the algorithm to tau, the fraction of data to discard? (The authors do test initializing randomly vs. initializing on the full dataset.) 3) The systematic label noise scenario seems to favor the authors' method (though the authors claim that it is a harder scenario than random label noise). It'd be helpful to see if the method works against random noise.== Theory ==4) The assumptions seem very restrictive. For example, for mixed linear regression (section 4), the features of all examples are assumed to be drawn i.i.d. from an isotropic Gaussian (so even the bad samples are drawn from the same distribution as the good samples; and all features are independent). To my knowledge, this is not a "standard and widely studied" assumption. For the Gaussian mixture model (section 5), a similar isotropic Gaussian assumption is made for each mixture. 5) Beyond the independence assumptions mentioned above, the initialization results make additional assumptions on the "bad" data (e.g., average distance of the good vs. bad parameters) that I found hard to parse. How strong are these assumptions? Do they hold on real datasets?6) The convergence results (Theorems 1 and 3) have a constant term sigma in them. This is surprising and seems to me to considerably weaken the result -- one would expect that the dependence on sigma will decrease with n.I think either a strong experimental or strong theoretical section would be sufficient for me to recommend acceptance. However, the paper currently shows potentially interesting experimental/theoretical results but does not do a comprehensive job of either side. This paper introduces a framework for situation when the training samples are not pure. The idea is a simple approach by training a model and removing a portion of examples from the training set based on the loss of the model. The authors provide some theoretical study on two models: linear regression and Gaussian mixture model and utilize deep neural network to show their framework performs well experimentally. My main problem with this work is the difficulty in understanding whether the reason our training model produces a large loss on some examples is due to them being bad examples or is because the model is not good enough and needs improvement. For example, one can always overtrain a classifier such that it classifies the training examples perfectly. Now the question become how much should I train my classifier. In case of Deep Neural Networks for example, the number of epochs can change the loss occurred by classifiers on the examples and it is not easy to know when to stop training in order to utilize the procedure introduced in this work.The theoretical work is related to linear regression and Gaussian Mixture model but the experiments are relayed to Deep Neural Nets! So I am not sure if this setup makes sense. Either both should be for DNN or neither should be.I am not sure if I understand Section 5 and the discussion related to the Gaussian Mixture Model. In Gaussian Mixture model, there are multiple components and each commonest has its own parameteres. So not sure (1) why the authors assume only mean parameter. (2) Given that Gaussian mixture model assumes multiple components, doesn't it automatically address the problem by putting the samples from different distribution in a different component?Page 5 typo: closest point closest toThe parameter \tau is set to 5 percent less that the true ratio of good samples (correct labels). This seems a pretty bias choice and implicitly applied that one needs to know the true value of this ratio which is a huge expectation. The authors need to investigate the effect of the changes of this value on the performance of their proposed framework! To me, it seems that the results can be hugely affected by the value of this parameter. The experiment with GAN is very wired. How can you expect to have a data set with 20 percent of its examples be bad cases. The authors need to justify that such cased can happen in real applications. The paper presents an anomaly detection method called MMOCGAN which is claimed to work well on high-dimensional datasets with limited, multimodal data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance:- Results are presented on a single private dataset, and I dont see any indication that the dataset will be shared with the community. This is problematic because there is no way for the community to reproduce and validate these results. I dont think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.- Given the small number of non-pass products in the dataset (22), its unclear to me whether a held-out test set was used, or if hyperparameter selection was performed on the full set of non-pass products.- The use of a pre-trained, domain-specific feature extractor is briefly mentioned but no details (what is the architecture, on which data its been trained, etc.) are provided.- The central idea in the paper is to have the generator capture the "complementary distribution" of the data-generating distribution. The way in which this distribution is defined is not specific enough (it depends on hyperparameters C and epsilon for which there is no clear prescribed value). On a conceptual level it seems to me that for a data-generating distribution corresponding to a low-dimensional manifold embedded in a high-dimensional space the complementary distribution will essentially be uniform random noise, and in that case its unclear to me how its supposed to "simulate anomalies".- The way the proposed method is presented makes it look ad-hoc: several moving parts (InfoGAN, generator loss term encouraging it to learn the "complementary distribution", feature matching regularization term, pull-away loss term, discriminator entropy term) are connected together and their individual inclusion in the final loss is loosely justified. In practice, looking at the results its impossible for me to tell which term is necessary and which is not.- The word "modal" is used throughout as a noun. Im not sure if the authors mean "model", "mode", or "modality", but based on the context I assume they mean "mode" as in "mode of the distribution".- The use of an InfoGAN architecture and loss is not credited clearly enough to Chen et al. and may give the impression to a casual reader that the idea is novel to this paper. The paper also does not make it clear how the number of categories or modes for the latent variable should be chosen, and what was the value used for the experiments.- The paper is legible, but there are several grammatical errors and typos throughout that make it harder to read than necessary. The paper presents an anomaly detection method called MMOCGAN which is claimed to work well on high-dimensional datasets with limited, multimodal data. The proposed idea is to train a GAN generator to simulate anomalies in the data in order to provide the one-class classifier with more negative examples. Overall I find that the paper is not clear and reproducible enough for me to recommend its acceptance:- Results are presented on a single private dataset, and I dont see any indication that the dataset will be shared with the community. This is problematic because there is no way for the community to reproduce and validate these results. I dont think results on private datasets should systematically be rejected, but they should at least be presented alongside results on public benchmarks to enable some form of reproducibility.- Given the small number of non-pass products in the dataset (22), its unclear to me whether a held-out test set was used, or if hyperparameter selection was performed on the full set of non-pass products.- The use of a pre-trained, domain-specific feature extractor is briefly mentioned but no details (what is the architecture, on which data its been trained, etc.) are provided.- The central idea in the paper is to have the generator capture the "complementary distribution" of the data-generating distribution. The way in which this distribution is defined is not specific enough (it depends on hyperparameters C and epsilon for which there is no clear prescribed value). On a conceptual level it seems to me that for a data-generating distribution corresponding to a low-dimensional manifold embedded in a high-dimensional space the complementary distribution will essentially be uniform random noise, and in that case its unclear to me how its supposed to "simulate anomalies".- The way the proposed method is presented makes it look ad-hoc: several moving parts (InfoGAN, generator loss term encouraging it to learn the "complementary distribution", feature matching regularization term, pull-away loss term, discriminator entropy term) are connected together and their individual inclusion in the final loss is loosely justified. In practice, looking at the results its impossible for me to tell which term is necessary and which is not.- The word "modal" is used throughout as a noun. Im not sure if the authors mean "model", "mode", or "modality", but based on the context I assume they mean "mode" as in "mode of the distribution".- The use of an InfoGAN architecture and loss is not credited clearly enough to Chen et al. and may give the impression to a casual reader that the idea is novel to this paper. The paper also does not make it clear how the number of categories or modes for the latent variable should be chosen, and what was the value used for the experiments.- The paper is legible, but there are several grammatical errors and typos throughout that make it harder to read than necessary. This paper tackles a very interesting subject but lacks sufficient clarity of presentation to allow me to do a proper review.First, there are many sentences which are not well-formed or are ambiguous (in pretty much all the sections). Then there are terms which are introduced without being first clearly explained or defined. Finally, there are issues with the mathematical clarity as well, with many notations which are used without being explained or defined. Sometimes one can figure out the missing information later (e.g., fig 1 talks about mutual information objectives without stating if we want to maximize or minimize it, but later in the text we figure that out) but it makes reading very difficult.What is a 'transformed one' (on page 2)What is a 'geometric intrinsic reward'?Where are the intrinsic rewards defined?What is a 'non-parametric classifier'? A neural net? an kernel SVM?There are also some mathematical problems:- if f (page 3) has a discrete output, then it will probably lose information, so it cannot be inverted (contrary to the stated assumption that f(x)!=f(y) for x!=y)- what are the differences between the different Q functions being defined? do the correspond to different action spaces? What is Q_task? What is pi_meta?- in eqn 2, I do not think that the log q_c term maximizes the mutual information between actions and (G(t),G(t+1)), i.e. it would be missing an entropy term- what is Z_c in eqn 2? The authors study the task of sample-based quantitative evaluation applied to GANs. The authors suggest multiple modifications to existing evaluation pipelines: (1) Instead of embedding the samples in the InceptionNet feature space, train a domain-specific encoder. If labeled data is available, add a cross-entropy loss to the encoder training objective so that the class can be predicted. (2) Instead of fitting a single Gaussian in the feature space, fit a GMM instead. This should allow for a more fine-grained class-aware distance between the (empirical) distributions. Pro: Attempt to attack a critical issue in generative modeling. Good overview of competing approaches.Several ablation studies of evaluation measures and the behavior of FID with respect to the representation space.The ideas make sense on a conceptual level, albeit suffering from major practical concerns.Con:- Clarity can be improved (e.g. use of double negatives as in the top of page 3), the same arguments repeated multiple (&gt;3) times (i.e. deficiencies of FID and IS, etc.), Many statements which should be empirically tested are stated as folklore (last paragraph on page 3). In general the paper merits another polishing pass (mode != model, last paragraph in  section 3, unmatch, etc.).- Why would a VAE capture a good feature space? It is known that the tradeoff between what is stored in the latent space versus the discriminator *completely* depends on the power of the discriminator -- if the discriminator is flexible enough it can just learn the marginal distribution and ignore the latent code. Hence, this subtle issue will likely undermine the entire model comparison.- Using the predictive distribution as a soft label for CAFD. Interesting idea, but why would one have access to labels in the first place? Why wouldn't one use a conditional GAN if we already have labels? Secondly, why would the modes necessarily correspond to classes?- Stated issues with FID: Why would you expect FID to be resistant to such drastic transformations as blocking out a significant proportion of pixels with blocks? This is a *major* change in the underlying distribution. The fact that humans can fill in this gap should have nothing to do with the quality of the underlying model. Arguably, you can also hide one eye, the nose and the mouth and still judge the sample as good.The ideas presented in this paper are conceptually interesting. However, given the drawbacks discussed above I cannot recommend the acceptance of this work. Summary:This paper presents a simple auxiliary loss term for model-based RL that attempts to enforce consistency between observed experience trajectories and hallucinated rollouts.  Simple experiments demonstrate that the constraint slightly improves performance.Quality:While I think the idea of a consistency constraint is probably reasonable, I consider this a poorly executed exploration of the idea.  The paper makes no serious effort to compare and contrast this idea with other efforts at model-based RL.  The most glaring omission is comparison to very old ideas (such as dyna) and new ideas (such as imagination agents), both of which they cite.Clarity:The paper is reasonably clear, although there are some holes.  For example, in the experimental section, it is unclear what model-based RL algorithm is being used, and how it was modified to support the consistency constraint.  (I did not read the appendix).Originality:It is not clear how novel the central idea is.Significance:This idea is not significant.Pros:+ A simple, straightforward idea+ A good topic - progress in model-based RL is always welcomeCons:- Unclear how this is significantly different from other related work (such as imagination agents)- Experimental setup is poorly executed.  - Statistical significance of improvements is unclear  - No attempt to relate to any other method in the field  - No explanation of what algorithms are being used The paper presents theoretical analysis for recovering one-hidden-layer neural networks using logistic loss function. I have the following major concerns:(1.a) The paper does not mention identifiability at all. As has been known, neural networks with even only one hidden layer are not identifiable. The authors need to either prove the identifiability or cite existing references on the identifiability. Otherwise,  the parameter recovery does not make sense.Example: The linear network takes f(x) = 1'Wx/k, where 1 is a vector with every entry equal to one. Then two models with parameters W and V are identical as long 1'W = 1'V.(1.b) If the equivalent parameters are not isolated, the local strong convexity is impossible to hold. The authors need to carefully justify their claim.(2) When using Sigmoid or Tanh activation functions, the output is bounded between [0,1] or [-1,+1]. This is unrealistic for logistic regression: The output of [0,1] means that the posterior probability has to be bounded between 1/2 and e/(1+e); The output of [-1,1] means that the posterior probability has to be bounded between 1/(1+e) and e/(1+e).(3) The most challenging part of the logistic loss is the lack of curvature, when neural networks have large magnitude outputs. Since this paper assumes that the neural networks takes very small magnitude outputs, the extension from Zhong et al. 2017b to the logistic loss is very straightforward. (4) Spectral initialization is very impractical. Nobody is using it in practice. The spectral initialization avoids the challenging global convergence analysis.(5) Theorem 3 needs clarification. Please explicitly write the RHS of (7). The result would become meaningless, if under the scaling of Theorem 2, is the RHS of (7) smaller than RHS of (5).I also have the following minor concerns on some unrealistic assumptions, but these concerns do not affect my rating. These assumptions have been widely used in many other papers, due to the lack of theoretical understanding of neural networks in the machine learning community.(6)The neural networks take independent Gaussian input.(7)The model is assumed to be correct.(8)Only gradient descent is considered. Summary------The authors propose an adaptation of the Adam method, with the AMSGrad correction and an additional parameter to p to exponentiate the diagonal conditioning matrix V (Padam).The proposed method changes two aspects: first, there is no need to retain two version of the rescaling matrix v, where amsgrad and Padam keeps the last monotone \hat v)t and non-monotone version v_t. Secondly, a new parameter q is introduced, that replaces the q=2 in the moment estimation phase of (P)Adam.A regret analysis is proposed in the convex case, while a vanishing bound on the gradient is derived in the non-convex smooth case.Review------Although improving optimization methods is certainly important for the machine learning community, the reviewer have strong concerns about this paper.First of all, the paper is hard to read as it contains too many approximations. What does 'SGD is known to work reasonably well regardless of their problem structure' means ? Same thing for 'Its performance deteriorates when the gradients are dense due to a rapid decay of the learning rates.' The authors uses many times elliptical discourse to detail the course of their analysis, which is non informative: for instance, 'one can easily derive the upper bound expression', and 'It is not difficult to conclude that when G_t [...]'. This level of writing is not professional. Some completely irrelevant argument are proposed to justify the method: 'For instance the extension from l_2 norm to l_p norm and generalization from Cauchy-Schwart to Holder inequality'.The reviewer has interrogations about the relevance of the proposed algorithm. The additional parameter q needs to be tuned, which carries only the promise of further overfitting. I would have been convinced by an sequence of experiment where q is set automatically by considering a validation set, and then tested on a left out test set. However, the authors report only the results for the best q, with non significant differences (and not quantified, there is no result tables). Using q=2 at least made sense from the point of view of empirical Fisher matrix approximation.The review also have several concerns aout the correctness of the proposed arguments. First, the major argument of memory usage stems from 1) a miscalculation and 2) a misunderstanding of memory bottlenecks in deep learning. 1) adam models keeps in memory x_t (the model parameter), g_t (the model gradient), m_t (momentum) \hat v_t and v_{t-1} (the monotone and non monotone version of the second order moment estimation. In contrast, the proposed model do not track v_{t-1}: this amounts to a memory saving of 20% considering all model related parameters. 2) more importantly, the most important memory usage in deep leaning comes from the activations that need to be kept in memory during the forward pass to perform the backward pass. Even the biggest model are less than 1GB, and most of the memory used during training is dedicated to intermediary activations. This makes the major argument of the paper less convincing, and misleads the reader.Second, even when disregarding the slightly abusive assumptions over the iterate sequences, that are common in the adaptive stochastic optimizers community, I think that the bound proposed in theorem 1 is non informative, as the second term behaves like T sqrt(T) assymptotically, due to the presence of 1 / \alpha_t. This does not show the convergence of averaged regret R_T / T.Regarding the experiment section, I am afraid that testing a new optimizer over MNIST and CIFAR is not enough to show the relevance of the method for the whole deep learning community. An eperiment over a non-toy dataset (eg ImageNet), and on non computer-vision dataset (eg from NLP) would be a minimum, besides the overfitting concern described above.In conclusion, it is the reviewer's opinion that significant rework in term of presentation and strong improvement of the experiment section to make the case for the Game optimizer.Minor comments------------Table 1: what do you bound when you compare results ? I think there is a typo in Zhou et al. result: 1/2 should read p.Eq (1): it is rather surprising to use x_t as the model parameters in the ICLR community. p 7: the dimension d could be larger than T when training large-scale neural networks: how does it relate to comparing sqrt(dT) to (dT)^s ? The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR.The authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions:- the authors argue for "phonemes and ctc", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary.- why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own "homonym" argument applies, too, and "mop" (or "mom") and "pop" should be mapped to the same "viseme" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier.- how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system?- LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference?- is the data going to be available?- what is a "production-level speech decoder"? how come your model "is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques" if Google does essentially the same ("in production")?- in Section 1, you say that "by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts". in Section 5, you demonstrate the "generalization power of our V2P approach"and find that it "is able to generalize well" - please clarify- "speech impaired patients" often have non-canonical articulation, the proposed system may not work well for them- it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. - finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good? This paper presents an end-to-end system that can recognize single-channel multiple-speaker speech with multiple languages.Pros:- The paper is well written.- It shows the existing end-to-end multi-lingual ASR (Seki et al., 2018b) and end-to-end multi-speaker ASR (Seki et al., 2018a) techniques can be combined without any change to achieve reasonable performance.- It demonstrates the challenge of single-channel multi-lingual multiple-speaker speech recognition, and compares the performance of the multiple-speaker system on the mixed speech and the single-speaker system on the isolated speech.Cons:- It lacks novelty: the proposed framework just simply combines the two existing techniques as mentioned above.- The training and evaluation data are both artificially created by randomly concatenating utterances with different languages from different speakers with different context. I am not sure of how useful the evaluation is, since this situation is not realistic. Also, currently it cannot test the real code-switching since the utterances are not related and not from the same speaker.- There are not enough analyses. E.g. it would be good to analyze what contributes to the gap between the single-speaker ASR system performance on the isolated speech and the multi-lingual multi-speaker ASR system on the mixed speech. How well does the proposed end-to-end framework perform compared to a two-step framework with speaker separation followed by multi-lingual single-speaker ASR? The authors propose to build a speech recognition system that has been trained to recognize a recording that has been produced by mixing multiple recordings from different languages together, and allowing for some code switching (also done artificially by concatenating different recordings).While this sounds fancy and like a hard problem, it is in fact easier than recognizing two speakers that have been mixed together speaking the same language, which has already been solved in (Seki, 2018a), from what I can tell. I don't see any contribution in this paper, other than explaining how to create an artificial (un-realistic) database of mixed speech in multiple languages, and then training a multi-speaker end-to-end speech recognition system on that database. This paper presents a framework to train an end-to-end multi-lingual multi-speaker speech recognition system. Overall, the paper is quite clear written.- Strengthens:+ Experimental results show consistent improvements in speech recognition performance and language identification performance.- Weakness:+ I'm not sure whether the framework is novel. The authors have just mixed training data from several languages to train an end-to-end multi-speaker speech recognition system.+ I don't see the real motivation why the authors want to make the task harder than needed. The example provided in figure 1 is very rare in reality.+ The authors claimed that their system can recognise code-switching but actually randomly mixing data from different languages are not code-switching.+ In general, it would be better to have some more analyses showing what the system can do and why. This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.\n\nSince SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.         # SummaryThis paper focuses on how to transfer knowledge between classes. The authors proposed to transfer classifiers instead of features. The proposed to linearly combine the classifiers from rich classes to construct more robust classifiers or rare classes. The combination weights are predicted from a learned neural network for each rare class. The experimental results on two benchmark datasets outperform some existing methods. # Strengths- While the idea of constructing classifiers by a linear combination of other classifiers have been proposed for several different problems (e.g., zero-shot learning), its application to long-tailed classification seems to be novel.- The idea is clean and clear. The approach part is well-written.- The proposed method improves the performance of tail classes.# Weaknesses1. There is no comparison to existing methods (proposed in other problems) that use linearly combine classifiers. Note that, those methods can be applied to long-tailed classification with minimal modifications. For example, for ZSL, due to the lack of visual information of unseen classes, the combination weights can only be estimated from the semantic descriptions. Here, with the W_j for each class, one baseline is thus to replace the semantic descriptions by W_j or even visual features for estimating the combination weights. The authors should compare to those methods.2. While the approach is clearly written mostly. The need for independent alpha-net modules for each tail class is unclear. Note that, an alpha-net is a two-layer network with lots of parameters, and for tail classes, there are only a few labeled data instances. Learning for each class an alpha-net may be vulnerable to over-fitting. - There is no equation of the training loss for the alpha-nets. It will be great to provide it. If I understand correctly, the loss is still a softmax loss across all the classes (head and tail), but only the alpha-net parameters (for the tail classes) are being learned.- The alpha-nets seem to be learned from the data that have been used to train the classifiers in the first stage. As neural networks can usually achieve very low training error, alpha-nets with a one-hot output (i.e., only use V^j_0) may already lead to very high training accuracy. Im not sure if alpha-nets learn anything meaningful. - An ablation study on the algorithm design, for example, using a shared alpha-net for all the classes, should be included.3. The related work and experimental comparison are insufficient. Only one paper has been compared in Table 1 and Table 2. There have been many papers published in CVPR 2020 and ECCV 2020. The authors, however, cited NO papers published in 2020.4. Can the authors provide more discussions on why the performance at medium, many, and all classes drop in comparison to the baselines? For now, it seems that the performance improvement comes simply by trading the prediction/adjusting the classifier strengths among classes: for example, increasing the classifier norms of tail classes. # Minor- The figures and captions can be improved. Specifically, Figures 1 and 2 are not self-contended: it is hard to understand the figures without looking at the main text.# JustificationsWhile the proposed idea seems novel for long-tailed classification, the paper lacks comparisons to existing methods and comparisons to similar algorithms proposed in other problems (with minimal changes). There is no ablation study on why we need an alpha-net module for each class. There is no overall performance gain, making it hard to tell if alpha-nets really improve classifiers or simply trading predictions/adjusting the classifier strengths among classes. I thus give a score of 3. This paper addresses the well-known long-tail classification problem. The argument made here is that most of the existing methods attempt to transfer knowledge in the feature space, which is true. Based on this motivation, the paper proposes a method to do the knowledge transfer in the model space instead. The idea is to apply K-NN method in the model space to pick up a group of strong classifiers trained on the head classes with sufficient training samples available that are closest to a weak classifier in the model space and then a linear combination of the group of strong classifiers and the weak classifier to form a stronger classifier to the tail classes where only few samples are available for training; the linear combination weights are learned from a simple neural network, called Alpha Net. Two datasets artificially truncated from ImageNet and Places, respectively, that were also used in the peer work in the literature, were used to report the evaluations.The paper reads very well, except for a few grammatical errors. The presentation is clear and easy to follow.My major comments follow. The long-tail learning problem is not new, and the idea of knowledge transfer in the model space is not new either (e.g., King et al 2019 referenced in the paper and in fact that reference was updated in 2020 with better results beating what this paper reported). Consequently, the novelty of this work is rather limited.Further, I have a strong reservation in considering the proposed method as a technically sound approach. Conceptually, the idea of combining a group of closest strong classifiers with a weak classifier to form a stronger classifier in the model space is based on the proximity presumption. Regretfully, unlike in the feature space where the proximity presumption is valid in general (unless the feature points are located close to the class boundaries), I am not convinced that the same proximity presumption is valid in the model space, as it is easy to give many counter examples.Regarding to the experiments, I would like to mention that the authors of the closest competitor, Kang et al 2019 referenced in the paper, have updated their work in arXiv this year with results beating what was reported in the paper. Also they used more datasets to evaluate their method than the two datasets used in the paper. So it is difficult to argue that the proposed method represents the state-of-the-art.Overall, I am not convinced that the proposed method is technically sound and advances the state-of-the-art literature. The proposed framework is demonstrated with two datasets from the transportation sensornetwork. The experimental results on model performance and training property demonstrate theefficacy of the proposed method empirically. The model used is from existing works, but thetraining framework is relatively novel in the FL community.Strengths1. It is very encouraging to support advanced models for federated learning. The authorgoes in the right direction. As far as I know, this is the first time that GCN+GRU isdiscussed under the context of federated learning.2. The techniques discussed in this work is comprehensive since the authors introduceboth the model and the training method.Weakness***1. Motivation is not well-discussed***(1) From the introduction section, the author claim that the node-level datasets cannot becentralized. But in practice, at least sensors in IoT networks CAN be centralized to edgeservers.(2) Especially, the datasets used in experiments does not have any privacy issue. It is verystrange to me why we should concern the privacy of the *traffic speed* from road networksensors. We can check the road network speed in real-time using Google Map. In other words,the speed of a specific sensor is not sensitive and can be acquired by public service. Currently,the Google Map example demonstrates that the road network sensors can be uploaded toservers from transportation agencies without any privacy concerns.(3) Another serious problem is that the DNN-based model (encoder-decoder used in theproposed method) are not deployable in the sensor device due to computational resourceconstraint. In practice, the number of road network sensor in transportation is huge, so it isimpractical to ask for an expensive upgrading which requires to install a powerful DNN enabledchips.***2. No technical contribution***The focus of this work is diverse (both model and training), but lack of contribution in eachaspect.(1) The authors argue the contribution of modeling (e.g., 4.1 modeling spatial dependencies, 4.2inductive v.s. transductive). However, for the modeling part, I believe the contribution is fromGCN+GRU-based modeling, which is already there in many data mining publications (Equation(1)(2)(3) are not newly proposed models). Please refer to [1][2]. The model used in this paper isa simplified version of these two GCN+RNN-based models in transportation. More advancedmodels following these two can obtain better performance than the model used in this paper.[1] Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for TrafficForecasting[2] Diffusion convolutional recurrent neural network: Data-driven traffic forecasting(2) The authors also claim the benefit of alternating optimization (4.3, 4.4). It is good to knowsuch an alternating method works well in practice. However, the author does not provide anyin-depth analysis or comprehensive experimental evidence (see my comments 3 below fordetails).In short, I suggest the author focuses on one aspect of federated learning to consolidate thecontribution. At least, the current version raises my concern that this work is a simplecombination of many techniques without meta-contribution in each one. If the overallcharacteristics are good, it should be encouraged. However, the effectiveness is not obvious:higher accuracy, security, fairness, privacy, communication, and computation efficiency, Icannot see any of these advantages of such a combination:[Model Performance] In terms of higher accuracy, as mentioned in (2), the model used is notnovel. More advanced models can obtain higher accuracy. See following works of 2-(2) [1][2] fordetails.[Optimization] The experimental results of the alternating optimization has somecounter-intuition results. See 3 for details.[Security/Privacy] As for security and privacy, the hidden vector exchange may leak privacy butthis work does not discuss it. See 8 for details.[Computational Efficiency] The model used in edge devices are too computational expensive forresource constrained sensors. See 4 for details.***3. The training Algorithm is novel but the experimental result is not convincing***The training method is relatively novel in federated learning. However, the authors should eitherprovide enough intuition to explain why this work (any related works in optimization theory?) ordemonstrate it with experimental design. The author attempts to prove that alternating trainingworks better but its not convincing. In Section 4.3, all the experimental results are notconvincing or counterintuitive:First, I am surprised that the authors have a result to say that split learning performs worse thancentralized training. In the essence of optimization, split learning is the same as centralizedtraining. The only difference is that split learning offloads part of the model architecture to theserver-side to reduce the communication/computation cost.Second, due to the first reason, that AT w/o FedAvg and AT + FedAvg works better than SLcan NOT demonstrate the effectiveness of alternating optimization. As I mentioned previously,to prove the effectiveness of alternating optimization, we need more analysis or comprehensiveexperimental design.Third, another counterintuitive result is that AT + FedAvg performs better than centralizedtraining (the red curve is lower than the blue curve). Normally, in federated learning, one shouldbelieve that centralized training is the lower bound of training/validation errors. An alternatinglocal SGD method normally leads to variance bias in the training process, thus I believe itsimpossible to obtain a better performance than centralized training.The last concern is that the experiments only perform at one dataset (METR-LA). Please alsocheck whether the same conclusion holds in another dataset.***4. Communication and computational cost***The computational cost is evaluated, but whats the implication of the number showing in termsof FLOPS? I dont think the resource constrained road network sensor can handle so manyFLOPS. Does any running time result in a real-world sensor device? If testing the sensor isprohibited, please provide the running time result at a low-performance smartphone or other IoTdevices (NVIDIA edge GPU devices).***5. Overall training time***The overall training time and the bandwidth of exchanging hidden vectors should be mentioned.If the network size of the exchange information is too high and the bandwidth is limited inwireless communication (e.g., road sensor networks), a single iteration will cost too much timeor failure due to communication protocol constraint (e.g., in IoT setting, maximum payloadlength is only 65535 bytes), which will lead to a long training time or impractical assumption.Please discuss the size of exchange information (hidden vector/gradient) and the total trainingtime in revision.***6. Scalability issue is ignored***The proposed method does not use a client sampling strategy, a common practice incross-device FL (see the original FedAvg paper [1]), to mitigate the scalability issue. What theperformance if we want to learn 10 thousand sensors? Especially in the transportation setting,the scalability is very important. However, the authors do not discuss this.[1] Communication-Efficient Learning of Deep Networks from Decentralized Data.https://arxiv.org/abs/1602.05629***7. Dataset: Non-I.I.D. is not properly discussed***A key assumption of Federated Learning is that datasets across devices are non-I.I.D. This islargely ignored by the proposed methods. Slightly mentioning the heterogeneous property is notenough. Please discuss the details of this in revision (e.g., show the distribution diagram). Moresignificantly, DNN models like encoder-decoder architectures normally eat a lot of samples, butthe number of samples in each node is small in practice. The assumption that each node hasenough dataset to train a good model is too strong (we cannot assume the edge device has thestorage capability to store months of time series data since the storage ability is limited at theedge, ***4-6 months*** as the author mentioned). Maybe the alternating optimization methodhelps, but it lacks analysis. In my opinion, its more interesting and practical to construct agraph-level federated learning method since multiple nodes can be centralized to an edgeserver belongs to a company or an agency like the transportation scenario.***8. Privacy***The authors discuss some privacy-preserving methods in related works, but the proposedmethod does not include any privacy design. For example, [1] analyze potential leakage fromthe hidden vectors, which proves that the proposed method does not have privacy advantagesthan exchanging gradient/models.[1] Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box KnowledgeTransfer. https://arxiv.org/abs/1912.11279***9. Lack of Related works***Since alternating optimization (training algorithm) is the novel part of the proposed method,there is a lack of literature review of this method, either in the general ML optimization literature,or the specific federated learning publications. Without such a comparison, the novelty is notconvincing.***10. Some misunderstandings of federated learning***(1) In the first paragraph of the introduction, the authors claim that decentralized training canimprove latency, but this argument is vague. Improving training latency or inference latency?Federated learning will cost time to train on devices, which requires a long time due to manyrounds of communication synchronization. For the inference, to improve the latency, we needmodel compression techniques, which is also not the main goal of the proposed method.(2) The term decentralized training is misused. In federated learning, decentralized trainingalso refers to training using a decentralized topology (no central server). Note that FedAvg usedin the proposed method is a centralized training with decentralized datasets. Please clarify thedifference in revision, making sure the readers understand which aspect is decentralized, thedatasets, the distributed optimization algorithm, or just computation.***11. Reproducibility***The proposed algorithm is complex. Without the code release, its hard to check the correctnessof the implementation. Deep learning normally costs time, I am curious to know how the authorsimplement such a complex system/algorithm which can train so many nodes in parallel. If it is asimulator, the training time will be too long since so there so many nodes (300+) with DNNmodels. If the authors can provide code, maybe all my confusion can be addressed, and I amwilling to increase my rating.***Suggestions***(1) I encourage the authors demonstrate the idea with more realistic datasets. For example,in social network, it is possible to run models on the smartphones of Internet users toprotect their privacy, which is much better than the transportation example.(2) In the current version, there is no contribution in modeling. Thus, I suggest authors tothink about customizing the model to obtain benefit of efficiency and privacy.(3) More intuition and discussion are requires for the training method. Why alternatingminimization works well in the GCN setting.(4) Since the training results are not convincing, I suggest authors run more experiments tounderstand the proposed method and explain why there are some counter-intuitiveresults.(5) Privacy is not the main focus, but some discussions and related works should bementioned.Overall CommentsDue to so many concerns mentioned above, I encourage the authors to have a deeperunderstanding of federated learning and address these issues in the revision, especially themotivation, and demonstrating the effectiveness of the proposed alternating training method.After thinking a while, I cannot find a practical scenario that requires GCN-based node-levelprivacy. At least, the sensor dataset is not a good example. Maybe the authors can search for apractical dataset to demonstrate motivation.In addition, please focus on just one gist rather than intertwine too many aspects and claim allcontributions without any trade-off discussion. Besides the training framework, its alsointeresting research if we can see newly developed model architectures can trade-off manyimportant aspects in FL. Summary: This paper presents a new training algorithm for vector-quantized autoencoders (VQVAE), a discrete latent variable model akin to continuous variational autoencoders.The authors propose a soft-EM training algorithm for this model, that replaces hard assignment of latent codes to datapoints with a weighted soft-assignment.Overall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.The technical presentation of the work by the authors starts only at page 5 (taking less than a full page), after several pages of imprecise presentation of previous and related work. The paper could be significantly improved by making this preceding material more concise and rigorous. Quantitative experimental evaluation is limited to a machine translation task, which is rather uncommon in the literature on generative latent variable models. I would expect evaluation in terms of held-out data log-likelihood (ie bits-per-dimension) used in probabilistic generative models, and possibly also using measures from the GAN literature such as inception scores. Datasets that are common include CIFAR-10 and resized variants of the imagenet dataset.  Specific comments:- Please adhere to the ICLR template bibliography style, which is far more readable than the style that you used. - Figure 1 does not seem to be referenced in the text. - The last paragraph of section 2.1 is unclear. It mentions a sampling a sequence of latent codes. The notion of sequentiality has not been mentioned before, and it is not clear what it refers to in the context of the model defined so far up to that point. - The technical notation is very sloppy. * In numerous places the paper refers to the joint distribution P(x1,&,x_n, z1, &, zn) without defining that the distribution factorizes across the samples (xi,zi), and without specifying the forms of p(zi) and p(xi|zi). * This makes that claims such as computing the expectation in the M step (Equation 11) is computationally infeasible are not verifiable. - Please be clear about how much is gained by replacing the exact M-step with a the one based on the samples from the posterior computed in the E-step. - What is the reason to decode the weighted average of the embedding vectors, rather than decoding all of them, and updating the decoder in a weighted manner?- reference 14 for Variational autoencoders is incorrect, please use the following citation instead: @InProceedings{kingma14iclr,  Title                    = {Auto-Encoding Variational {B}ayes},  Author                   = {D. Kingma and M. Welling},  Booktitle                = {{ICLR}},  Year                     = {2014}}- The related work section (4) provides a rather limited overview of relevant related work. Half of it is dedicated to recent advances in machine translation, which does not bear a direct connection to the technical material presented in section 3.- There is no justification of using *causal* self-attention on the source embedding, is this a typo?- As for the experimental evaluation results: it seems that distillation is a much more critical factor to achieve good performance than the proposed EM training of the VQ-VAE model. Unfortunately, this fact goes unmentioned when discussing the experimental results. - What is the significance of the observed differences in BLEU scores? Please report average performance and standard deviations over several runs with randomized parameter initialization and batch scheduling. - It seems that the tuning of the number of discrete latent codes (table 2 in appendix) and other hyper-parameters (table 3 in appendix) was done on the test set, which is also used to compare to related work. A separate validation set should be used for hyper parameter tuning in machine learning experiments.- It seems that all curves in figure 3 collapse from about 45 BLEU to values around 17 BLEU, why is this? The figure is hard to read since poor quality, and curves that are superposed. The paper considers the problem of building a composite network from several pre-trained networks and whether it is possible to ensure that the final output has better accuracy than any of its components. The analysis done in the paper is that of a simple linear mixture of the outputs produced by each component and then by showing that if the output of the components are linearly independent then you can find essentially a better ensemble. This is a natural and straightforward statement with a straightforward proof. It is unclear to me what theoretical value does the analysis of the paper add. Further the linear independence assumption in the paper seems very strong to make the results of value. Further the paper seems very hastily written with inconsistent notation throughout making the paper very hard to read. Especially the superscript and the subscript on x have been jumbled up throughout the paper. I recommend rejection and encourage the authors to first clean up notation to make it readable. This paper studies composite neural network performance from function composition perspective. In theorems 1, 2 and 3, the authors essentially prove that as the basis functions (pre trained components) increases (satisfying LIC condition), there are more vectors/objects can be represented by the basis. To me, this is a very straight forward result. As the basis increases while the LIC condition is satisfied, we can of course represent more objects (the new component is one of them). I don't see any novelties here. The result is straightforward, and this should be a clear rejection. The paper aims at justifying the performance gain that is acquired by the use of "composite" neural networks (e.g., composed of a pre-trained neural network and additional layers that will be trained for the new task).I found the paper lacking in terms of writing and in terms of clarity in expressing scientific/mathematical ideas especially for a theory paper.Example from the Abstract:"The advantages of adopting such a pre-trained model in a composite neural network are two folds. One is to benefit from others intelligence and diligence, and the other is saving the efforts in data preparation and resourcesand time in training"The main results of the paper (Theorem 1,2,3) are of the following nature: if you use more features (i.e., "components") in the input of a network then you have "more information", and this cannot be bad. Here are the corresponding claims in the Abstract:"we prove that a composite neural network, with high probability, performs better than any of its pre-trained components under certain assumptions.""if an extra pre-trained component is added to a composite network, with high probability the overall performance will be improved."However, this argument seems to be just about expressiveness; adding more features can be statistically problematic. Furthermore, why is it specific to pre-trained components? Essentially the theorems are about adding any features.Finally, the assumption that the pre-trained components are linearly independent is invalid and the makes the whole analysis somewhat simplistic.The motivating Example 1 just shows that the convex hull of a class of hypotheses can include more hypotheses than the class itself. I don't see any connection between this and the use of pre-training.Other examples unclear statements from the intro:"One of distinctive features of the complicated applications is their applicable data sources are boundless. Consequently, their solutions need frequent revisions.""Although neural networks can approximate arbitrary functions as close as possible (Hornik, 1991), the major reason for not existing such competent neural networks for those complicated applications is their problems are hardly fully understood and their applicable data sources cannot be identified all at once."There are many typos in the paper including this one about X for the XOR function:"Assume there is a set of locations indexed as X = {(0; 0); (0; 1); (1; 0); (1; 0)} with the corresponding values Y = (0; 1; 1; 0). Obviously, the observed function is the XOR" In this paper a method for pose estimation is proposed, which is based on the well known neural model stacked hourglass networks. The novelty in the proposed paper is a multi-scale formulation, which creates multiple scales from the input image and feeds them into different hourglass modules. The different scales are weighted differently, where the weights for a given scale depend on the error obtained on previous scales.Important issues:The novelty seems to be not sufficient to me, as multi-scale solutions are not new in computer vision and have been applied a lot in pose estimation as well, be it deep neural models or older techniques. In particular in deep models, multi-scale techniques have been proposed extensively for resolution preserving image to image mappings (which is done here), beginning with quite old techniques (in deep learning time scales) going back to 2012 (Farabet, C., Couprie, C., Najman, L., LeCun, Y., 2012. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In: ICML.), or formulations which integrate scales and layers, starting with hyper-columns in 2015 (Hypercolumns for object segmentation and fine-grained localization, CVPR 2015) and many more recent variants.In 2018, multi-scale formulations are now standard techniques in computer vision with deep networks. I am not sure how the proposed method makes a difference. Also, I am wondering whether there shouldnt be some parameter sharing between the models of the different scales, as is often done in the literature now to reduce model capacity.The way the multi-scale Since different resolutions are created is particular. Why not just simply subsample the input images to different resolutions? Why are these trained layers needed as a preprocessing?Weighting different scales is not fundamentally new. We also dont know whether it improves performance, it is not part of the ablation study.The method has been compared to several methods, but which are not state of the art anymore. Most papers are from 2016, the field advanced quickly. The performance gains of the multi-scale formulation are pretty low, and overally, the method is not state of the art on the targeted benchmark. Obviously I do not want to say that a paper needs to be state of the art on a benchmark to get published, even at a top-level conference, far from it. However, if the methodological and theoretical contribution of the paper is rather minor, than the performance and evaluation should be flawless.Minor remarks:The writing of this paper is somewhat fuzzy, using non-standard technical language, which I could not decipher and which seems to me somewhat misleading. Examples are:- the stacked hourglass network theoretically increases the stacked depth: how does theory tell us anything about depth in this context?- difficult to form differentiated and determined collaboration mechanisms for each stacked hourglass: I dont understand what this means, simply. This phrase is also repeated several times in the paper in different places- information loss caused by the functional consistency of hourglass networks: I dont understand what functional consistency means here, and why it leads to information loss.The description of Algorithm 1 does not seem to be necessary to me. It basically follows from the equations 4-6, which are executed in order and per layer. Authors extends stacked hourglass network with inception-resnet-A mudules and a multi-scale approach for human pose estimation in still RGB images. Given a RGB image, a pre-processing module generates feature maps in different scales which are fed into a set of serial stack hourglass modules each responsible for a different scale. Authors propose an incremental adaptive weighting formulation for each stack-scale-joint. They evaluate proposed architecture on LSP and MPII datasets.positive:- Having an adaptive weight strategy is a necessary procedure in multi-loss functions where cross-validation or manual tuning of fixed weights are expensive. While the weights are functions of the loss, it is not analyzed and evaluated thoroughly, e.g. evolution of weights for each joint-stack-scale. Even it is not given in the section 5.2.1. So it is hard to judge effectiveness of the proposed loss. negative:- In general experiments section is the most weakness of the paper. I comment some points in the following:a) Multi-scale image processing is not a novel idea in computer vision and specifically in human pose estimation. The authors have not compared their methods with most recent papers in the literature and I believe the results are not state-of-the-art (see [1] for instance which is a multi-scale approach).b) What is the effect of each scale in the results and for each joint? This must be analyzed and shown visually or numerically.- The number citations is not enough.- The writing must be improved.overall:Regarding mentioned comments, I believe the paper needs a huge extra effort (both analytically and numerically) to be adequate for publication. Therefore, I recommend rejection.[1] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning feature pyramids for human pose estimation. In: ICCV. (2017) The paper deals with the problem of community detection on graphs, examining the impact of graph measures. To do so, the paper proposes an experimental framework where clustering is achieved using the kernel k-means algorithm, and the performance of graph measures is examined on various instances of artificially generated graphs using the LFR benchmark. The overall approach is empirical, supported mainly by the experimental results. The main observations concern the consistent behavior of particular graph measures across multiple settings of the dataset.Strong points:-- The paper addresses an important problem in network analysis, which also concerns practitioners in a wide range of disciplines.-- Various graph measures are considered in the evaluation. This is very interesting, since, in my view, some of them are not very well-known among the graph clustering / community detection communities.-- The paper is well-structured and well-written. Most of the concepts, including the experimental framework, are clearly presented.Weak points:--- My main concern about the paper has to do with the consistency of the proposed evaluation framework under different evaluation criteria and graph data beyond the LFR benchmark. Firstly, as the paper also mentions, focusing only on LFR graphs can definitely reveal important properties of algorithms, but limits the generalization of the observations in the case where other generators might be used (e.g., SBM) or even real-world graphs. Besides, the argument made in the paper that the LFR benchmark generates graphs similar to real-world ones is not very accurate. LFR focuses on the clustering structure as well as on the degree distribution but might miss other key properties including graph diameter or the number of triangles (or the clustering coefficient). Is there any evidence that could support the observations of the paper in the case of real graphs? --- A closely related point has to do with the observation that real-world graphs do not have a clear clustering structure, i.e., well-defined cuts. Focusing only LFR graphs might not be enough to capture such instances. --- The modularity criterion is used to evaluate the quality of communities, which overall is a widely used criterion. Nevertheless, modularity has been shown to be prawn to the particular structure of the communities (e.g., resolution limit). How is this taken into account in the evaluation of the communities?--- Another point is that the paper is purely empirical. Definitely, this is a good starting point, especially when the focus is on experimental settings that have not been used before. Nevertheless, despite the interesting observations, I would expect to have a (theoretical) justification or some reasoning of why SCCT performs well on LFR graphs, or for instance why PPR which is a widely used measure, shows poor behavior.--- In the paper, all graph measures are considered as kernels. How valid is this argument? Why not just using the pure k-means for graph measures which are not kernels?  Summary: The paper studies a new attack model based on spatial transformations. The authors first formalize an attack model based on spatial transformation and then study attacks and defenses for this model. Clarity: While the paper studies an important problem -- it's important to move out of the norm ball based attack models and consider different attacks like spatial transformations, in the current version, the presentation lacks clarity in both the formulation of the attack model, attacks, defenses and explanation of the results. For example, the impossibility result isn't clear: the claim is that any classifier has adversarial spatial transformations that are successful in causing misclassifcation for some threshold on the size of transformation. There is no explanation of how large this threshold is in practice. Is it small enough to be called an "impossibility result"? What does this threshold intuitively depend on?Originality: The key contribution seems to be the formalization of some notion of spatial transformation. However, the final expression (Proposition 1) basically looks just like an l_p norm but after transforming it by some "fixed" matrix M. The expressions for this new attack model where || M r|| &lt; \eps for some perturbation \eps look pretty similar to the case previously considered (where M was essentially identity). For example, Raghunathan et al. 2018 and Hein &amp; Andriushchenko 2017. The paper is also missing discussion on the structure of this matrix M, and how it changes the attacks and defenses in practice&nbsp;Significance: I think the problem of spatial transformation based adversarial examples is important and the authors have the right goals. However, the current presentation makes it hard to understand the main results provided and hence I would rate that the contribution is not very significant. Overall: I highly recommend the authors to revise the presentation and clarify a) the main conceptual differences of the new attack model (matrix M of proposition 1) b) Formalize the impossibility and possibility results carefully with concrete theoretical/empirical results to back the claims The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function (both are "soft" values rather than the hard binary values used in the Bloom filter). Experiments show that, when there is structure in the data set, the Neural Bloom Filter can achieve the same false positive rate with less space.I had a hard time understanding how the model is trained. There is an encoding function, a write function, and a query function. The paper talks about one-shot meta-learning over a stream of data, but doesn't make it clear how those functions are learned. A lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments. But even that does not contain much detail, and it's not obvious how this is related to one-shot learning. Overall, the paper is written from the perspective of someone fully immersed in the details of the area, but who is unable to pop out of the details to explain to people who are not already familiar with the approach how it works. I would suggest rewriting to give an end-to-end picture of how it works, including details, without appendices. The approach sounds promising, but the exposition is not clear at all. The authors propose a method for learning representations for graphs. The main purpose is the classification of graphs.The topic is timely and should be of interest to the ICLR community. The proposed approach consists of four parts: Initial feature transformationLocal features aggregationGraph poolingFinal aggregatorUnfortunately, each of the part is poorly explained and/or a method that has already been used before. For instance, the local feature aggregation is more or less identical to a GCN as introduced by Kipf and Welling. There are now numerous flavors of GCNs and the proposed aggregation function in (2) is not novel. Graph pooling is also a relatively well-established idea and has been investigated in several papers before. The authors should provide more details on their approach and compare it to existing graph pooling approaches. Neither (1) nor (4) are novel contributions. The experiments look OK but are not ground-breaking and are not enough to make this paper more than a mere combination of existing methods. The experiments do not provide standard deviation. Graph classification problems usually exhibit a large variance of the means. Hence, it is well possible that the difference in mean is not statistically significant. The paper could also benefit from a clearer explanation of the method. The explanation of the core parts (e.g., the graph pooling) are difficult to understand and could be made much clearer. The paper proposes a method for improving the stability of reinforcement learning with value function approximation, e.g., deep Q-learning. The key idea is fitting a Q function to rewards, fitting another Q function to negative rewards, then estimating Q values using a linear combination of the two Q functions. The method is applied to DQN, double DQN, and on-policy actor-critic on the CartPole, Mountain Car, and Pendulum tasks in OpenAI Gym.The writing isn't clear, especially in the introduction. Phrases like "risky", "badness of a state", and "inverse policy" are used without definition.The experiments only test one value of \lambda. Since \lambda is the one hyperparameter that controls the degree to which the inverse rewards influence the Q value estimates, I think it is critical to test the performance of the proposed method under various values of \lambda (e.g., by sweeping the unit interval in increments of 0.1).One of the central claims is that the proposed Q value estimator gives more accurate estimates of returns than the estimators used in previous deep Q-learning methods. However, the experiments never compare the predicted Q values to the true values, as is done in [3].The experiments only evaluate the proposed method on the CartPole, Mountain Car, and Pendulum tasks, which have very small action spaces. I suspect the benefit of the proposed method will be smaller in environments with a larger number of possible actions, since the inverse policy may fail to accurately estimate the values of actions that are neither the best nor the worst at any given time. One of the central claims is that the proposed method improves the stability of Q-learning, but it is unclear how many random seeds were tested in Figure 2 and Table 2. It appears that only the data from one training run was used, and the reported standard deviations are computed using the last 10% of episodes in that single training run. Furthermore, the curves are smoothed using a moving average with a window size of 100 episodes. Together, these two details make it extremely difficult to evaluate the claim that the proposed algorithm is more stable, and also makes it difficult to evaluate the significance of the differences between the method's performance and the baselines. [1] shows how results on a small number of random seeds tend to not be reproducible.[1] https://arxiv.org/pdf/1709.06560.pdf[2] http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf[3] https://arxiv.org/pdf/1509.06461.pdf This paper considers the trade-off between the prediction accuracy of deep neural networks (DNNs) and sensitivity to adversarial examples.Reviewing the (Gaussian) channel capacity and rate-distortion theory, i.e., the information bottleneck, the authors discuss their implications on the generalization performance of DNNs. The experiments demonstrate the SNR of gradients, information plane, the generalization gap, and fault tolerance against adversarial examples.While the interpretations of DNN learning by the information theoretic concepts are interesting, most of them are already known results, and hence provide little novel theoretical knowledge.The discussions in Section 3 are superficial. It is not clear how they are related to the main arguments of this paper.While the experiment of fault tolerance is interesting, the implications obtained from experiments are somewhat trivial.minor comments:p.2, l.15: h, w, and c are undefined.   Section 2: Rate-distortion theory is usually explained by the sphere covering argument instead of sphere packing. Section 4.3.1: It is not explained what zero and one-shot transfer learning is.Pros:The experiment of fault tolerance is interesting. Cons:Theoretical parts are basic results of information theory.The implications of experiments are somewhat trivial. The paper proposes two additional steps to improve the compression of weights in deep neural networks. The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.There are several weaknesses in this paper. The first one is clarity. The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.The paper can be made more mathematically precise. The input and output types of each block in Figure 1. should be clearly stated. For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits. Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.The figures are almost useless, because the captions contain very little information. For example, the authors should at least say that the "D" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned. Many more can be said in all the figures.The second weakness is experimental design. There are two conflicting qualities that need to be optimized--performance and compression rate. When optimizing the compression rate, it is important not to look at the test set error. If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set. The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.Optimizing compression rates should be done on the training set with a separate development set. The test set should not used before the best compression scheme is selected. Both the results on the development set and on the test set should be reported for the validity of the experiments. I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning. Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed. This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.1. Stochastic Line SearchDetermining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.First, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so Im on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. How are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?The theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?The algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?In deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\min(1, \xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?The theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\hat{p}_k^T \hat{g}_k] = E[\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.All the proofs also depend on a linear Taylor approximation that is not well-explained, and Im wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\hat{f}_{z} (x + \alpha \hat{p}_z) = \hat{f}_{z} + \alpha \hat{p}_z \hat{g}_z(x + \bar{\alpha} \hat{p}_z)$, where $\bar{\alpha} \in [0, \alpha]$. How does this affect the proof?Lastly, I would propose for the authors to change the name of their condition to the Armijo condition rather than using the term 1st Wolfe condition since the Wolfe condition is typically associated with the curvature condition (p_k g_new &gt;= c_2 p_k g_k), hence referring to a very different line search. 2. Design of the Quasi-Newton MatrixThe authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because it is not obvious that enforced symmetry necessarily produces a better search direction and treating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the true gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.Additionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.The matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.Lastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?All of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.3. Numerical ExperimentsAs written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)In addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and its not clear if either of these are done in the paper.Typos/Grammatical Errors:- Pg. 1: Commas are needed in some sentences, i.e. Firstly, for large scale problems, it is&; &compute the cost function and its gradients, the result is&- Pg. 2: Interestingly, most SG algorithms&- Pg 3: Remove at least a in second line- Pg. 3: suboptimal, not sup-optimal- Pg. 3: Such a solution, not Such at solution- Pg. 3: Capitalize Lemma- Pg. 4: fulfillment, not fulfilment- Pg. 7: Capitalize Lemma- Pg. 11: Before (42), Cov \hat{g} = \sigma_g^2 I- Pg. 11: Capitalize LemmaSummary:In summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.References:[1] Berahas, Albert S., Jorge Nocedal, and Martin Takác. "A multi-batch l-bfgs method for machine learning."&nbsp;Advances in Neural Information Processing Systems. 2016.[2] Byrd, Richard H., et al. "A stochastic quasi-Newton method for large-scale optimization."&nbsp;SIAM Journal on Optimization&nbsp;26.2 (2016): 1008-1031.[3] Schraudolph, Nicol N., Jin Yu, and Simon Günter. "A stochastic quasi-Newton method for online convex optimization."&nbsp;Artificial Intelligence and Statistics. 2007. Paper Summary: This paper proposes to reconstruct the generated images to the their corresponding latent code. As claimed, the goal is to improve the accuracy and efficiency of inference mapping better than other inference mapping techniques, while maintaining their generation quality. Instead of using an independent encoder, the authors propose to share the encoder parameters with the discriminator: a Connection Network (CN) is built on top of the features extracted by the discriminator. The weight-sharing machisme shows better performance in Figure 1.The proposed method has two benefits: : a) manipulating the image by disentangling the latent space and b) suggesting a new metric for assessing the GAN model by measuring reconstruction errors of real data.General Comments:In term of algorithm, the paper essentially adds the conscontruction term (CN) to the standard GAN loss, and partially shares the weights of the encoder and discriminator. However, it is almost identical to the existing works, which are NOT cited, and the connections are not discussed.Connection to InfoGAN: To relate the generated images to the latent code,  the proposed method employs the reconstruction loss, InfoGAN employs the mutual information. Note that reconstruction loss = negative log likelihood, and effectively is equivalent to Mutual Information and Conditional Entropy in the case. Please see the discussion in Lemma 3 and Appendix A of [3] for detailed discussion.  Further, InfoGAN has proposed to to sharing weights of the encoder and discriminator, exactly the same with this submission. The claimed advantage is to disentangle the latent space. It is not surprise at all, once the authors see the connection to InfoGAN, which was originally proposed to disentangle the latent codes.Connection to CycleGAN: CycleGAN consists of four losses: two reconstruction losses and two standard GAN losses. As shown in Section 4 of [3] Connecting ALI and CycleGAN, one reconstruction loss and  one standard GAN loss is sufficient to achieve CycleGANs objective, the other two losses would only help to accelerate. In another word, the proposed method is exactly half of the CycleGAN losses.The author mention in Abstract that the bidirectional generative models introduce an encoder to establish the inverse path of the generation process. Unfortunately, their inference mapping does not accurately predict the latent vector from the data because the imperfect generator rather interferes the encoder training. This is the non-identifiable issue of ALI/BiGAN discovered in [3]. Please clarify. The proposed method should compare with [1] and [2] in great detail, to demonstrate its own advantages. Given the missing literature, the current experimental comparisons seem not that meaningful, because the baseline methods are not really the competitors. One interesting contribution of the submission is to consider the reconstruction errors to measure the quality of GANs. To my best knowledge, it is original. References:[1] InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, NIPS 2016[2] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV 2017[3] ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching, NIPS 2017 The paper proposes using the GAN discriminator for inference mapping, mapping an image to a latent code that would be used to generate the image by the encoder, based on the argument that the discriminator can be used as a powerful feature extractor because it has seen both real and fake data during training. The paper compares the proposed approach to several approaches that train an inference model together with a generator. While the paper compares its approach to several baselines, they are not the most relevant ones. In fact, the most relevant baseline is not cited and compared. As a result, the novelty of the paper is not justified. Specifically, the baselines the paper compare to are mostly methods that jointly learn an inference model and a generation model, while the proposed approach first learns a generation model and then fits an inference model (it is referred to as the connection network in the paper). In this regard, the paper should compare its approach to methods that first learns a generation model and then learns an inference model. The iGAN work by Zhu et. al. ECCV 2016 is arguably most relevant approach. Especially, they also use the discriminator architecture for the inverse mapping. Unfortunately, the work is neither cited nor compared.In addition, pretrained networks such as VGG and ResNet have been known to be powerful feature extractor. It would be ideal the paper can compare the proposed approach to that using VGG and ResNet for finding the z for a given image.Finally, the paper seems to lack of comprehensive knowledge on how the inference mapping has been investigated in the GAN literature. For example,  the statement that "BEGAN (Berthelot et al., 2017) made the first attempt to solve the inverse mapping from x to z using the non-convex optimization" in the introduction section is incorrect. The scheme is used in at least two 2016 papers (Liu and Tuzel NIPS 2016 and Zhu et. al. ECCV 2016). This paper aims at comparing end-to-end learning vs separately learning a state representation and subsequently a controller.While this would be a relevant and important topic, the paper does not currently present consistent evidence to support this hypothesis.In particular:- The approach proposed in the approach is not explained in sufficient details. After reading Sec.4 I have only a very vague and high-level idea of how the proposed approach might work. In Figure.2, what is I_t? What is the model that you are training? How are you learning this model? how do you define L_inverse?- The cited literature about state representation learning is absolutely incomplete. Papers like Lange et al. , Wahlström et al. and Finn et al. and citations herewithin.- From the experimental results, it is difficult to say anything definitive about the proposed hypothesis. 1) There are multiple end-to-end approaches in the literature, with significant differences in performance. which one are you using? (it seem A2C and PPO, but to which label do they correspond in the tables?) 2) How do you tune the weights of the reward function proposed? This seems an important design choice, but it is not much discussed. 3) In the table reported (e.g., Table 1) it does not seem to me that SRL consistently outperforms other approaches. Even for the arm tasks, Random features seem to outperform the proposed approach (and indeed all the methods except the ground truth). What is going on there?Overall¸ the paper would benefit from a clearer and more detailed text, and from improved experiments and comparisons.Minor comments:- It is unclear to me what "goal-based robotic tasks" means. How do you define a task without a goal?- An important and missing characteristic of a suitable state representation should be the generalization. In fact, a good representation would ideally allow the agent to generalize to some degree. - It seems very odd to me that the "action should be implicitly encoded into the state representation" could you elaborate of the motivation for this and the effects?References:- Autonomous reinforcement learning on raw visual input data in a real-world applicationS Lange, M Riedmiller, A VoigtlanderNeural Networks (IJCNN), The 2012 International Joint Conference on, 1-8- From pixels to torques: Policy learning with deep dynamical modelsN Wahlström, TB Schön, MP DeisenrotharXiv preprint arXiv:1502.02251-  Deep Visual Foresight for Planning Robot MotionChelsea Finn, Sergey LevineInternational Conference on Robotics and Automation (ICRA), 2017 This paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework. The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.Major points:Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty. I dont recommend to accept this paper, at least in the current format.The paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is possible to generate a large high quality (SPARQL query, local form) dataset. For the second contribution, there isnt any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications. I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesnt explain well the intuition of this write operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?In general I find Section 3 pretty difficult to follow. What does keeping notes mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage. I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode. \alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.Minor points:- tau Yih et al, 2016 --&gt; Yih et al, 2016- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange. This paper presents an overlapping community detection method. The idea is to use a graph neural network (namely, the graph convolutional network) with node embeddings constrained to be non-negative. The non-negative embeddings helps to learn the community membership of each node (and each node can belong to multiple communities). The idea is natural, though not novel. The only main novelty, as compared to various other recently proposed graph embedding approaches, lies in making the node embeddings non-negative. Rest of the pieces are fairly standard, including the link functions, such as Bernoulli-Poisson.  Therefore the paper is quite thin in technical novelty.In addition to the limited technical novelty, I have a few other concerns as well, including some on the experimental evaluation:- Real-valued node embeddings obtained from shallow/deep graph embedding methods can be used with *overlapping* versions of k-means. This can be a solid baseline.- The paper relies on subsampling the edges and non-edges to speed-up optimization. However, the encode still seems to use the entire adjacency matrix. If that is not the case, please clarify.- The reported results are only on overlapping community detection. Most of the shallow/deep graph embedding methods can also be used for link prediction task (many of the recent paper report such results). It will be nice to provide results on this task.  - There has been some recent work on using deep generative models for overlapping community detection with node side information. For example, see "Deep Generative Models for Relational Data with Side Information" (Hu et al, 2017). Interestingly, they too use Bernoulli-Poisson link (but not GCN).- None of the baselines are deep learning methods. As I pointed out, one can use real-valued embeddings from such methods with overlapping k-means (or other overlapping clustering methods). Link-prediction results can also be compared.In summary, I think the paper lacks both in terms of technical novelty as well as experimental evaluation and therefore doesn't seem to be ready. I would encourage the authors to consider the suggestions above. The paper proposes a subgradient descent method to learn orthogonal, squared /complete n x n  dictionaries under l1 norm regularization. The problem is interesting and relevant, and the paper, or at least the first part, is clear.The most interesting property is that the solution does not depend on the dictionary initialization, unlike many other competing methods. The experiments sections in disappointingly short. Could the authors play with real data? How does sparsity affect the results? How does it change with different sample complexities? Also, it would be nice to have a final conclusion section. I think the paper contains interesting material but, overall, it gives the impression that the authors rushed to submit the paper before the deadline! The proposed method is too simplistic, the model being succinctly described in less than one page with many errors in the given math expressions. Only the model is given. The optimization problem, as given in (1) is not explained. the authors need to stud the optimization problem, to derive its resolution, and to describe the obtained algorithm.The authors main motivation is to maps the input convolutional features from the same class into tight clusters. In such a space, the clusters become compact and well-separated, &. However, in the proposed method is operating in this way. The model is a simple transformation, and nothing ensures the compactness of the feature space, neither the separability of the classes.It is difficult to understand the arm CNNs with radial basis feature transformation. There are two figure in the paper that seek to show this modification of CNN, but this is not enough because nothing is said in the text, which makes these images difficult to understand. Moreover, the figures have notations different than those in the  main body, such as F_{l-1} as opposed to F_{i,j,K}.What is the transformation to be learned ? Is it T as given in the text before (1), or P as given in (3). In (1), it seems that it is a mix of both, namely T* = argmin_P ! Moreover, it is written To enforce Ts positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T 2T.  Decomposing T as TT, means that T is very very special.Equation (4) is not correct. The summation is on i, which is not in the expressions, but in the result with F_{i,j,K}.With the exception of Tables 3 and 4, most experiments are on comparing the conventional versus the proposed method. The authors need to compare to other methods available in the literature on defense against adversarial attacks. Moreover, it is not clear why the author compare the proposed method to ADVT (adversarial training) in Table 4, and not in Table 3.Some references are incomplete. For example, the second reference is missing the publication type, volume, & In this paper the authors focus on the problem of weakly-supervised action localization. The authors state that a problem with weakly-supervised attention based methods is that they tend to focus on only the most salient regions and propose a solution to this which reduces the difference between the responses for the most salient regions and other regions. They do this by employing marginalized average aggregation to averaging a sample a subset of features in relation to their latent discriminative probability then calculating the expectation over all possible subsets to produce a final aggregation.The problem is interesting, especially noting that current attention methods suffer from paying attention to the most salient regions therefore missing many action segments in action localization. The authors build upon an existing weakly-supervised action localization framework, having identified a weakness of it and propose a solution. The work also pays attention to the algorithm's speed which is practically useful. The experiments also compare to several other potential feature aggregators.However, there are several weakness of the current version of the paper:- In parts the paper feels overly complicated, particularly in the method (section 2). It would be good to see more intuitive explanations of the concepts introduce here. For instance, the author's state that c_i captures the contextual information from other video snippets, it would be good to see a figure with an example video and the behaviour of p_i and c_i as opposed to lamba_i. I found it difficult to map p_i, c_i to z and lambda used elsewhere.- The experimental evidence does not show where the improvement comes from. The authors manage to acheieve a 4-5% improvement over STPN through their re-implemenation of the algorithm, however only have a ~2% improve with their marginalized average attention on THUMOS. I would like to know the cause in the increase over the original STPN results: is it a case of not being able to replicate the results of STPN or do the different parameter choices, such as use of leakly RELU, 20 snippets instead of 400 and only rejecting classes whose video-level probabilities are below 0.01 instead of 0.1, cause this big of an increase in results? There is also little evidence that the actual proposal (contextual information) is the reason for the reported improvement.- There seems to be several gaps in the review of current literature. Firstly, the authors refer to Wei et al. 2017 and Zhang et al. 2018b as works which erase the most salient regions to be able to explore regions other than the most salient. The authors state that the problem with these methods is that they are not end-to-end trainable, however Li et al. 2018 'Tell Me Where to Look': Guided Attention Inference Network' proposes a method which erases regions which is trainable end-to-end. Secondly, the authors do not mention the recent work W-TALC which performs weakly-supervised action localization and outperforms STPN. It would be good to have a baseline against this method.- The qualitative results in this paper are confusing and not convincing. It is true that the MAAN's activation sequence shows peaks which correspond to groundtruth and are not present in other methods. However, the MAAN activation sequence also shows several extra peaks not present in other methods and also not present in the groundtruth, therefore it looks like it is keener to predict the presence of the action causing more true positives, but also more false positives. It would be good to see some discussion of these failure cases and/or more qualitative results. The current figure could be easily compressed by only showing one instance of the ground-truth instead of one next to each method.I like the idea of the paper however I am currently unconvinced by the results that this is the correct method to solve the problem. This paper explores augmenting the training loss with an additional gradient regularization term to improve the robustness of models against adversarial examples. The authors show that this training loss can be interpreted as a form of adversarial training against optimal L2 and L_infinity adversarial perturbations. This augmented training effectively reduces the Lipschitz constant of the network, leading to improved robustness against a wide variety of attack algorithms.While I believe the results are correct and possibly significant, the paper is poorly written (especially for a 10 page submission) and comparison with prior work on reducing the Lipschitz constant of the network is lacking. The authors also made little to no effort in writing to ensure the clarity of their paper. I would like to see a completely reworked draft before opening to the idea of recommending acceptance.Pros:- Theoretically intuitive method for improving the model's robustness.- Evaluation against a wide variety of attacks.- Empirically demonstrated improvement over traditional adversarial training.Cons:- Lack of comparison to prior work. The authors are aware of numerous techniques for controlling the Lipschitz constant of the network for improved robustness, but did not compare to them at all.- Poorly written. The paper contains multiple missing figure references, has a duplicated table (Tables 1 and 3), and the method is not explained well. I am confused as to how the 2-Lip loss is minimized. Also, the paper organization seems very chaotic and incoherent, e.g., the introduction section contains many technical details that would better belong in related works or methods sections. (best read in typora)The authors claim to propose a family of methods and generative models that are suited better for downstream tasks than previously proposed approaches.## Major pointsIt feels as if the proposed method tries to be many things. First, it is used for finding unsupervised representations down stream. Then, it still tries to be a generative model "of sorts", which is the reason for the use of variational inference in the first place. Additionally, the approximate posterior necessary to evaluate the ELBO is simultaneously used as a feature extractor.The resulting issues are:  - A "bad" variational posterior is used because it is unclear how to get vectorial features otherwise.   - An adhoc likelihood function is used, which is not sufficiently well explored theoretically in the paper.  Specifically,      - Stochastic generation is claimed to be "more complex than simple Gaussian"; the burden of proof is on the authors, as Gaussian density is closed under multiplication.       - It appears to be a Monte Carlo approximation to sth that is computable in closed form.      - It is not clear if that MC approximation is normalised and if the normalisation is the same at each optimisation step. Does this bias optimisation? What happens to the KL penalty weight?  - The ELBO change (prior updating) seems to make the claim that we still have a generative model (as written in the intro) invalid. My intuition is that the KL penalty vanishes for small step rates of the optimiser, reducing the model to that of a noisy auto encoder.## SummaryThe authors want to evaluate variational sequence models for feature extraction for downstream tasks. But why? What is the use of a generative inspired algorithm, when necessary ingredients are discarded? Both goals appear to be at conflict and I am not convinced that the variational ingredient is necessary.I do not cover the experimental section since the method itself has issues so severe that I don't consider it relevant. ## Minor points- Notation $\mu_{\phi_t}$ gives the impression that $\phi$ is time dependent.- Equations (9) and (11) are formatted badly.- The approximate posterior used was used first in (Bayer &amp; Osendorfer, "Learning stochastic recurrent networks", 2014) not (Chen 2018).- Diagrams follow GM notation only half-heartedly. The paper aims to come up with a criterion for evaluating the quality of samples produced by a Generative Adversarial Network. The main goal is that the criterion should not reward trivial sample generation algorithms such as the one which generates samples uniformly at random from the samples in the training set. I personally feel that if sample generation is the only goal, then this trivial algorithm is perfectly fine because, statistically, the empirical distribution is in many, though not all, ways, a good estimator of the underlying true probability measure (this is the idea that is used in the statistical technique of Bootstrap for example). However the underlying goal in unsupervised learning problems where GANs are used is hardly sample generation. The GANs also output a whole function in the form of a generative network which converts random samples into samples from the underlying generating distribution. This generative network is arguably more important and more useful than just the samples that it generates. An evaluation scheme for GANs should focus on the generative network directly rather than on a set of its generating samples. Even if one were to regard the premise of the paper as valuable, the paper still does a poor job meeting its objective. A measure D_CNN is proposed as a benchmark. It must be remarked that D_CNN is not even properly defined (for example, there is a function \Delta in its definition but it is never explained what this function is). D_CNN is a variant of the existing notion of Neural Network Divergences. Only a numerical study (with no theory) is done to illustrate the utility of D_CNN for evaluating samples generated by GANs. The entire paper is very anecdotal with very little rigorous theory. Strengths:+ The paper identifies a valid limitation of the MAML algorithm: With a limited number of gradient descent steps from a single initialization, there is a limit to the ability of a fixed-size neural network to adapt to tasks sampled from a diverse dataset.+ The tSNE plots show some preliminary interesting structure for the simple regression and RL tasks, but not for the classification task.Weaknesses:- The motivation of uncovering latent modes of a task distribution does not align with the proposed method. The algorithm computes a continuous representation of the data from a task (which is fixed during gradient-based fast adaptation). The mode identity, on the other hand, should be a discrete variable.  Such a discrete variable is never explicitly computed in the proposed method.- The technical writing is unclear and jargon is often used without definition. Importantly, one of the central motivators of the paper, "task modulation", is never given a precise definition.- The standard few-shot classification task (Omniglot) does not clearly consist of a task distribution that is multimodal, so the method is not well-motivated in this setting.- Experimental conclusions are weak.Major comments:- The paper neglects to discuss how the proposed method could be used in the context of other methods for "gradient-based meta-learning" such as Ravi &amp; Larochelle (2016). I believe the attention-based modulation and the FiLM modulation could be easily adapted to that setting. Why was this not discussed or evaluated?- Conditioning has been used in the context of few-shot learning before, but this is not discussed (https://arxiv.org/abs/1805.10123, https://arxiv.org/abs/1806.07528).- The paper often confounds task representation with neural network parameter values. For example, Figure 1 depicts the adaptation of parameter values with gradients (\nabla L), yet the caption describes "task modes." More careful writing would disentangle these two components.- The motivation for the particular form of the task embedding computation is not given. What were the other options? Why not, for example, an order-invariant function instead of a bidirectional GRU?- In all of the experiments, there is no appropriate baseline that keeps the parameter dimensionality constant, so it is unclear whether the (marginal) improvement in performance is due to added expressivity by adding more parameters rather than an algorithmic improvement. I suggest an ensembling baseline with an appropriate number of ensemble members.- There is no evaluation on a standard benchmark for few-shot classification (miniImageNet), and the Omniglot improvement is small.- The reinforcement learning comparison at some point compares MUMOMAML with modulation applied (therefore with access to task-specific data) to MAML with no adaptation (and therefore no access to task-specific data). This is not entirely fair.- tSNE results can be misleading (e.g., see https://distill.pub/2016/misread-tsne/), and the task delineation is not extremely clean. I would be more convinced if a clustering algorithm were applied.Minor comments:- The paper needs to be checked over for English grammar and style.- everywhere: The "prior" referred to in this paper is not a prior in the Bayesian sense. I suggest a more careful use of terminology.- abstract: "augment existing gradient-based meta-learners" You augment a specific variant of gradient-based meta-learning, MAML.- pg. 1: "carve on a snowboard" don't know what this means- The terminology of "task distribution" and "modes" thereof is used without introduction in the introduction section. The terminology "model-based meta-learning/adaptation" and "gradient-based meta-learning/adaptation" is also used without introduction here. This makes the introduction unnecessarily opaque. Consider the reader who is not familiar with meta-learning papers; they would have a very hard time parsing, for example, the phrase "...this not only requires additional identity information about the modes, which is not always available or is ambiguous when the modes are not clearly disjoint..." (pg. 1).- Further, the terminology "model-based" seems non-standard, and is aliased with the term model-based reinforcement learning (which specifically refers to the set of RL algorithms that make use of a "model" of transition dynamics). Since the paper tackles a reinforcement learning benchmark, this may lead to some confusion.- pg. 3; "our model does not maintain an internal state" Is the task representation/embedding not an internal state?- pg. 3: "relevant but vaguely related skills" this is imprecise- pg. 3: The episodic training setup, which is standard to meta-learning setups, could be much better described. The MAML algorithm could be given better intuition.- everywhere: "task specific" -&gt; task-specific- Algorithm 1: "infer" is a misuse of terminology that usually refers to an operation in latent variable probabilistic modelling. Since the computation of \tau is purely feedforward, I recommend writing "compute."- \tau should be used in some places where v is used instead The paper proposes to add "recurrent" connections inside a convolution network with gating mechanism. The basic idea is to have higher layers to modulate the information in the lower layers in a convolution network. The way it is done is through upsampling the features from higher layers, concatenating them with lower layers and imposing a gate to control the information flow. Experiments show that the model is achieving better accuracy, especially in the case of noisy inputs or adversarial attacks. - I think there are lot of related literature that shares a similar motivation to the current work. Just list a few that I know of:Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. "U-net: Convolutional networks for biomedical image segmentation." International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.Lin, Tsung-Yi, et al. "Feature Pyramid Networks for Object Detection." CVPR. Vol. 1. No. 2. 2017.Newell, Alejandro, Kaiyu Yang, and Jia Deng. "Stacked hourglass networks for human pose estimation." European Conference on Computer Vision. Springer, Cham, 2016.Yu, Fisher, et al. "Deep layer aggregation." arXiv preprint arXiv:1707.06484 (2017).The current work is very similar to such works, in the sense that it tries to combine the higher-level features with the lower-level features. Compared to such works, it lacks both novelty and insights about what works and why it works.- The performance gain is pretty marginal, especially given that the proposed network has an iterative nature and can incur a lot of FLOPs. It would be great to show the FLOPs when comparing models to previous works.- It is interesting observation that the recurrent network has a better tolerance to noise and adversarial attacks, but I am not convinced giving the sparse set of experiments done in the paper.Overall I think the current work lacks novelty, significance and solid experiments to be accepted to ICLR. This paper applies a weakly-supervised learning approach to identify factors of object postures in an image dataset. The core idea is to introduce two sets of images. The first set is the reference data set with grouped objects of different active/inactive posture constraints. This set is used to provide weak supervision information in posture identification. The second set is the probe set. It does not necessarily require posture grouping of objects. Affinity Cycle Consistency loss is set up to automatically map objects of similar active postures between the two image sets (objects of similar postures are supposed to be the nearest neighbors in the learned embedding space). The experimental study verifies the validity of the proposed factor isolation algorithm. Generally this paper is well written and clearly explains the motivation/problem definition. However, we have the following concerns on the innovative contribution of this work.The innovation of this paper is very limited. The core technology applied in this work was originally proposed by Dwibedi et al in the paper "Temporal Cycle-Consistency Learning".  As cited in Section.1 of this paper, this work employs directly the Cycle-Consistency learning mechanism (while in a different application scenario). The only difference is: this work is built based on affinity relation between pairs of images with similar postures, while the original idea was applied for temporal sequence alignment in video processing. Not significant algorithmic innovation is introduced, compared to the previous work.  It is clearly below the threshold for a high-quality venue like ICLR.  In this work, the authors propose new ways of averaging updates received at the server from a subset of clients in a federated scenario. Specifically the authors aim to address issues arising from the non-iid nature of the data that arises in FL and propose to treat BatchNorm parameters differently from other NN parameters. IntroductionReading this paper, I am confused about terminology used by the authors. Specifically, the authors discuss 'data bias' and 'parameter bias'. In in the introduction, the authors claim that 'Conventional approaches average gradients uniformly from the clients, which could cause great bias to the real data distribution'. Assuming the authors understand FedAvg to be a conventional approach, the averaging with $|x_k|/|x|$ (Eq. 1) is not uniform but weighted by the local dataset size. Further, the meaning of 'causing bias to the real data distribution' is not clear to me. Unfortunately, the further explanation and Figure 1 don't help me in understanding what is meant. The 'GroundTruth' distribution of labels for cifar10 is approximately uniform. From the context, I understand that the authors try to describe some consequence of a non-i.i.d sampled distribution of data according to labels, but I cannot understand the point they try to make. Next, the authors discuss 'parameter bias'. The authors distinguish between BN parameters and other NN parameters. They term the BN parameters 'statistical parameters such as mean and variance'. Generally speaking, BN contains the 'scale and shift' parameters $\gamma$ and $\beta$ (https://arxiv.org/pdf/1502.03167.pdf), which I am assuming the authors reference to. Again, the authors make use of the term 'bias' to say: '[...] bias on the BN layer parameters'. I am not familiar with the notion of 'bias on a parameter' and would like the authors to clarify. Based on Figure 2 I assume they aim to convey that BN parameters in a FL setting converge to different values compared to a centrally trained model. In Section 3.1 the authors further make the distinction explicitly between 'gradient parameters', by which they mean weights and biases as opposed to 'statistical parameters' of BN. Since the scale-and-shift parameters of BN are also updated by gradient descent, I am wondering if the authors mean the mean and variance estimate across data-points for feature-maps, which also plays a central part in (federated) BatchNormalization. The authors in their experimental section make no mention of how they form these global estimates for mean and variance in BN, so they omit that crucial detail there.The authors specifically focus on label-skew as source of non-i.i.d-ness in this work, but never make this limitation explicit. Since the non-i.i.d challenges in FL are not limited to label skew, I believe the authors should make this explicit. Related work:The issues with BN in the federated setting has been described for example in Section 5 of https://arxiv.org/pdf/1910.00189.pdf. There, the authors propose to replace BN with GroupNorm, an approach that has been adopted in several follow-up and recent works in FL with models that originally contain BN. I would encourage the authors to compare their work against this approach, both in the RW section and also in the experimental section.Method Section.Notation-wise, I encourage the authors to not use $x$ or $x_k$ to denote a (labeled) dataset, since $x$ is usually reserved for a single data-point with associated label $y$. In Eq. (1) the loss formulation of FL is a bit sloppy since the parameters to optimise for, $W$ do not appear in the RHS of the equation. In FedAvg, we explicitly optimise $min \sum_k |x_k|/|x| L_k(W,x_k)$, where the local parameter estimates $W_k$ appear as intermediate parameters as a consequence of multiple local optimisation steps. The authors claim that 'data points available locally could be biased from the overall distribution'. Again, I believe to understand the intended meaning to be the non-iid issue, but I encourage the authors to make their understanding of 'bias' more concrete. In FedAvg, the individual clients do not transmit gradients $\nabla L_k ()$ to the server (Section 3.2.1). This is the approach in conventional distributed SGD as employed in a high-speed-connected data-centre for speeding up centralised training. In this centralised setting, the non-iid problem does not exist. Instead, in FL, clients transmit parameters that have been updated through a series of gradient-descent steps. More recent work (https://arxiv.org/abs/2003.00295) makes the role that these transmitted parameters have in an interpretation as a gradient. This distinction is important.The derivation in Appendix A.2 for show-casing the unbiased-ness of the variance parameter averaging seems wrong. Going from the third to fourth equation makes a mistake and also if the original expectation was equal to the sum of weighted expectations, then simply averaging would actually be the unbiased estimator. The authors here are falsifying their own argument through a derivation mistake. The last equation should only pull the expectation into the sum in the right-most term and you are done.Section 3.2.3I like the notion of modelling the datasets as GMM and to infer responsibilities at averaging time. The explanation of the approach is confusing to me, however. If I understand it right, then EQ 9 describes a VAE setup per client k. There is no sharing of parameters or latent space between clients. $s_k = [\mu_k,\sigma_k]$ are the mean and standard-deviation across the whole local dataset at a client $k$. The authors propose to encode this single vector $s_k$ into a latent space z_k of dimension $C$. The authors do not explicitly specify the prior p(z_k), but given the constraints on $z_k$, I assume it is meant to be a Dirichlet distribution. From context I could imagine that it has something to do with the per-client label distribution. Since each ELBO is per-client and each client has just one data-point $s_k$, I do not understand the need for auto-encoding. Simply infer z given a decoder-model for the single data-point.  The authors mention the use of neural networks, but they do not detail their architecture choices anywhere. It is also unclear to me how $\pi_k$ falls out in equation 10. I imagine it I corresponds to some sort of posterior across all local models' encoding z. Since the latent-spaces across clients k do not share any meaning, I don't see how that can be sensible.  All together, this section is not readable to me. I can see the appeal of reweighing updates through a specific formulation pi_k, but since the updates are label-independent (eq. 3,4,5), how does that play into this.Section 4My understanding of proofs of this form is somewhat limited. From what I can gather, this proof shows that FedAvg converges if the per-client loss-function is pre-multiplied by a constant factor $\pi_k$. As such, the convergence proof should be analogous to what is presented in e.g. https://arxiv.org/pdf/1907.02189.pdf with the exception of the update in Eq 5. Maybe the other reviewers can comment further on this. Section 5.In the Federated Setting, 20 clients should be considered not enough for experimental validation generally speaking. I appreciate the breath of experiments in terms of models, algorithms and related algorithms. Unfortunately, the authors chose to define their own non-iid-split of the datasets. In general, I would appreciate to see comparisons with existing data-set splits in related work on non-iid data, such as for example https://arxiv.org/abs/2003.00295. This would help avoid the bifurcation of the literature. The reference to q-FedSGD seems to be wrong.After reading the experiment section, some serious questions arise:What is the fraction of selected clients per communication round? What is the number of local epochs per client? Some of the related works seem to suggest performing FedSGD (also the first paragraph in Section 3.2.1 suggests this). If you have single-gradients per device and equal-size data-sets per client, then I don't see how non-iid data-distribution across clients are an issue as this approaches a global mini batch step, where each mini-batch consists of smaller mini-batches, one from each client. The non-i.i.d issue in FL stems from the fact that each client optimises on its own for a sufficiently long time that the resulting progress is destroyed by averaging in parameter space. The authors need to specify their setup here. Concretely, I would want to at least see Cifar10 split into 100 clients, 10 of which are selected at every round. Each client needs to optimise locally for a full epoch on its own dataset of 45000/100 = 450 data-points. The authors propose two things: A new averaging approach by computation of $\pi_k$, as well as a new approach to estimating the gradient for the scale-parameter of BN using pooled averaging. These two things need to be studied separately by setting $\pi_k = |x_k|/|x|$ in one scenario. At the moment, my trust into the computation of $\pi_k$ is very low, since the corresponding section is not understandable to me. The pooled averaging approach seems sensible and I am curious to see if it solves the issue of BN in FL. Additionally, the authors need to clarify how the BN-statistics are computed at test time. I would also like to see a comparison of the proposed models with BN and with the state-of-the-art method which is replacing them with GroupNormalization. Finally, I would like to thank the authors for the interesting approach to training BN-equiped Neural Networks in a federated setting. This idea seems promising to me. The approach for computing pi_k is not clear to me and I would like the authors to revise it. Please specify the role that knowledge of the label-distribution plays and if the method is applicable when non-iid-ness stems from other sources than label skew. (I propose looking at the FEMNIST dataset for example). Many issues with this paper remain and I encourage the authors to overhaul their work. I see no issues with the Code of Ethics PAPER SUMMARYThis paper presents a new federated learning scheme for neural network that accounts simultaneously for different layer types and different (heterogeneous) local data distributions. In particular, the proposed method differentiates between parameter updates for conv, fc versus bn (batch-norm) layers. The main argument for this differentiation is that aggregating parameter gradient for bn layers is vulnerable to high bias when local data are heterogeneously distributed. This leads to a new proposal on aggregating bn parameters directly (instead of using their gradient).Both parameter gradient and parameter updates are weighted by the same set of probabilistic weights that seem to be associated with mixture weights of a GMM model that generate local training data. These weights are learned via a deep generative model that encode local statistics of local data into a latent space, which seems to be associated with random drawn from a (latent) categorical distribution associated with those mixture weights. A theoretical convergence guarantee for the proposed algorithm is also provided.NOVELTY & SIGNIFICANCEThis paper poses an interesting view on the parameter aggregation of federated learning. I appreciate the authors' perspective that perhaps the aggregation of parameters defining different layer types need to be diversified to fit the nature of the data. However, I find the proposed solution and its fundamental idea of modeling such heterogenity somewhat arbitrary and perhaps flawed. To elaborate:First, it is not at all clear to me why the authors associate the weights that combine parameter gradients (for fc and conv) and parameters (for bn) with the mixture weight that generate local data. This seems like an arbitrary choice to me and there seems to be a flaw here: if a local device receives very little data but its data come from a mixture component with large weight, its gradient will likely be biased (due to the lack of data) but will still dominate others (due to its large mixture weight) -- I would like to hear the authors' thoughts on this.Second, if I understand correctly, the auto-encoder model devised to learn those mixture weights is supposed to encode the local statistics (i.e., mean & variance estimates of a Gaussian that fit the local training input) to a categorical distribution over label classes. I am not sure if I missed something here but this is a bit strange: by right, the mean and variance of the *input* distribution do not contain any information about its *label* -- how does the embedding manage to generate information that does not exist in the first place? Likewise, how would such *label* information be related to the mixture weights of the *input* distribution via Eq. (10) -- could the authors elaborate further on this?Third, on a high-level of idea, I am also not fully convinced that the aggregating the bn parameter gradient is vulnerable to high bias. This statement somehow came across as a politically correct statement with no concrete substantiation. It will be better if there is some technical demonstration that explicitly show that for bn layer, the bias of the aggregated gradient is higher than the bias of its aggregated parameters.Last, I find the theoretical analysis is not at all particular about the inner working of the proposed algorithm. The analysis is on one hand generally applicable to any function that satisfies the stated assumption and, on the other hand, not specific enough to account for the facts that the combination weights are also being updated and that the bn parameters are not updated via gradient aggregation. Given this, I do not think the analysis is applicable to this setting, and it should be evaluated separately in its own setting.   TECHNICAL SOUNDNESSI have made high-level check over the main bulk of technical derivation and have not spot any glaring issues. But, as I said above, on the idea level, the three technical contributions here (separating bn aggregation from those of fc and conv, learning the mixture weights from local statistics (barring access to data) and the theoretical analysis) are orthogonal with little connection to one another -- for the first two contributions, there seems to be flaws on the idea level that need further clarification.EXPERIMENTThe reported results appear positive but it seems the authors only generate those for one single run. As the improvement margin is relatively small, it is important to average results over multiple runs to make sure the improvement is significantly above the deviation margin.Furthermore, reporting only the final performance gives very little insight regarding how the invented components help avoid accumulating high bias, and also whether the auto-encoding scheme correctly recover the mixture weight -- perhaps this can be shown by evaluating the proposed mechanism on a synthetic dataset where we know the ground-truth.  The paper proposes a meta algorithm to train a network with noisy labels.It is not a general algorithm but a simple modification of two proposed methods.  It is presented as a heuristics and it  would be helpful to derive a theoretical framework or motivation for the proposed algorithm. My main concern is related to the experiment results. The results of the baseline method look strange. Why there is a strong decrease in the MNIST test accuracy after 20 epochs? Standard training of neural network is very robust to label noise.  In case of  20% symmetric error  (figure 2c)  the performance degradation using standard training should be very small. Hence it is difficult to evaluate to performance of the proposed method.At the beginning of the experiment section you mentioned several algorithms   for training with noisy labels.  I expect to compare your results to at least one of them. This paper looks at a specific implementation of a "genetic algorithm" (GA) when applied to learning Atari games.Using a black-box "random search" technique, with a few heuristic adaptations, they find performance that is roughly competitive with several other baseline methods for "Deep RL".More generally, the authors suggests that we should revisit "old" algorithms in Machine Learning and that, when we couple them with larger amounts of computation, their performance may be good.There are several things to like about this paper:- The authors place a high importance on implementation details + promise to share code. This seems to be a paper that is heavily grounded in the engineering, and I have high confidence the results can be reproduced.- The algorithm appears broadly competitive on several Atari games (although Table 1 is admittedly hard to parse).- The algorithm is generally simple, and it's good to raise questions of baseline / what are we really accomplishing.However, there are several places where this paper falls down:- The writing/introduction is extremely loose... terms are used and introduced without proper definition for many pages.     + How would be think about the venn diagram of "evolutionary strategies", "genetic algorithms", "deep Q networks", "deep RL algorithms" and "random search"... there is clearly a lot of overlap here.    + The proposed deep GA has a "deep Q network" (or is it a policy... the paper does not make this clear), forms a type of "evolutionary strategy" and, at its heart is a type of "random search", but is it not also a "deep RL algorithm"?    + It is not until page 3 that we get a proper definition of the algorithm, and it's hard to keep track of the differences the authors want to highlight compared to the "baselines".    + What do we gain from the claim "old algorithms work well"... gradient descent is also an old algorithm, as are seemingly all of the alternatives? Is age in-of-itself an asset?    + Statements like "compression rate depends on the number of generations, but in practice is always substantial" are very loose... what does the "practice" refer to here, and why?- There is very little insight/analysis into *how* or *why* this algorithm performs better/worse than the alternatives. I don't feel I understand if/when I should use this algorithm versus another apart from a wall of seemingly random Atari results. In fact, there is a large literature that explains why GAs give up a lot of efficiency due to their "black box" nature... what do the authors think of this?- This paper seems purely focused on results rather than insight, with many hacks/tweaks to get good results... should we believe that GA+novelty search is a general algorithm for AI, or is it just another tool in the arsenal of a research engineer?     + In the end, the algorithm doesn't actually seem to outperform state-of-the-art on these Atari baselines... so what are we supposed to take away from this.Overall, I don't think that this paper provides very much in the way of scientific insight.Further, the results are not even superior to existing algorithms with stronger groundings.For me, this leads it to be a clear reject... even though they probably have code that reliably solved Atari games in a few hours. PAPER SUMMARY:This paper introduces a biologically motivated black-box attack algorithm. The target model in this case is DNN applied to the ASR context (automatic speech recognition system). NOVELTY &amp; SIGNIFICANCE:The proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.This however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.It is also unclear how this mutation component improves over the existing work (more on this in the sections below).Another issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:Chen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.ZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACMCheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.Query-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457While these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.TECHNICAL SOUNDNESS:I find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.CLARITY:The paper is clearly written.EMPIRICAL RESULTS:I do not understand this statement:"That 35% of random attacks were successful in this respect highlights the fact that black boxadversarial attacks are definitely possible and highly effective at the same time"Why does 35% successful attack rate is a positive result? The result tends to suggest that this is an attack with low success rate. The 2nd paragraph in 3.2 seems to give a vague explanation: "the vast majority of failure cases are only a few edit distances away from the target. This suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity".Given the above statement, I do not see why the authors didn't actually "run the algorithm for a few more iterations" to verify it ...I am also curious why is the attack success rate of the targeted attack success rate of the proposed method is significantly lower than that of the existing system -- I assume "single word black box" is the work of (Alzantot et al., 2018).I find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?REVIEW SUMMARY:The paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental &amp; its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). Paper Summary: The idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. Paper Strengths:Unfortunately, there is not many positive points about the paper except that it explores an interesting direction. Paper Weaknesses: I vote for rejection of the paper due to the following issues:- It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all.- The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal.- According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well?- The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in "On Evaluation of Embodied Navigation Agents".- Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work:Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018- The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics.- What is the difference between this method and providing a large negative reward at a non-target object?- The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. - It seems the same environment is used for train and test. - SummaryThis paper presents a minor improvement over the previous Deli-GAN framework (Gurumurthy et al CVPR'17). Specifically, the work proposes to use the Fisher Integral Probability Metric as the divergence measure in our GAN model, instead of the Jensen-Shannon divergence. It shows little results (seems to be positive) using this new distance measure other than traditional ones.  Except that, I didn't see any other contribution from this paper.- SuggestionsThe paper is poorly written and seems to be a rush submission to ICLR. For example:* a lot of grammatical errors throughout the paper* only 6.5 papes out of 8 pages are utilized* the introduction is not convincing -- what problem are you going to address? any summary of your methodology? why it is expected to outperform existing frameworks? what distinguishes your work from existing works? and what're your main results? I cannot conclude after reading the intro.* the results are very minor and not convincing. It seems the authors conducted a very limited set of experiments and concluded that the proposed Deli-Fisher GAN is better. If you claim that the proposed framework can generate better images, at least the framework should be compared to the latest state-of-the-art GANs (e.g. spectral GANs, etc.)* The writing is not polished. Overall, the paper is far from ready to be submitted to ICLR, not mentioning acceptance. I would recommend the authors to conduct more experiments and comparisons and do a better job before submitting it to future conferences. Summary:Batch Normalization (BN) suffers from 2 flaws: 1) It performs poorly when the batch size is small and 2) computing only one mean and one variance per feature might be a poor approximation for multi-modal features. To alleviate 2), this paper introduces Mode Normalization (MN) a new normalization technique based on BN. It uses a gating mechanism, similar to an attention mechanism, to project the examples in the mini-batch onto K different modes and then perform normalization on each of these modes.Clarity:The paper is clearly written, and the proposed normalization is well explained.Novelty: The proposed normalization is somewhat novel. I also found a similar paper on arXiv (submitted for review to IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018): M. M. Kalayeh, M. Shah, Training Faster by Separating Modes of Variation in Batch-normalized Models, arXiv 2018. I didnt took the time to read this paper in details, but the mixture normalization they propose seems quite close to MN. Could the authors comment on this?Pros and Cons:+ Clearly written and motivated+ Try to address BNs weakness, which is an important direction in deep learning- I found similar papier in the literature- The proposed method aims to make BN perform better, but pushes it toward small batch settings, which is where BN performs poorly.- Misses comparisons with other techniques (see detailed comments).Detailed Comments:1. Multi-modality:It is not clear if the features are multimodal when performing classification tasks. Some histograms of a few features in the network would have help motivate the proposed normalization. However, it seems indeed to be an issue when training GANs: to make BN work when placed in the discriminator, the real and fake examples must be normalized separately, otherwise the network doesn't train properly. Moreover, when dealing with multimodal datasets (such as the one you created by aggregating different datasets), one can use the FiLM framework (V. Dumoulin et al., Feature-wise transformations, Distill 2018), and compute different means and variances for each datasets. How would the proposed method perform against such method?2. Larger scale:It would be nice to see how MN performs on bigger networks (such as the ResNet50, or a DenseNet), and maybe a more interesting fully-connected benchmark, such as the deep autoencoder.3. Small batch regime:It seems that the proposed method essentially pushes BN towards a regime of smaller mini-batch size, where it is known to performs poorly. For instance, the gain in performances on the ImageNet experiments drops quite a lot already, since the training is divided on several GPUs (and thus the effective mini-batch is already reduced quite a lot). This effect gets worse as the size of the network increases, since the effective mini-batch size gets smaller. This problem also appears when working on big segmentation tasks or videos: the mini-batch size is typically very small for those problems. So I fear that MN will scale poorly on bigger setups. I also think that this is the reason why you need to use extremely small K.4. Validation set:What validation sets are you using in your experiments? In section 4.1, the different dataset and their train / test splits are presented, but what about validation?Conclusion:Given the similarity with another paper already in the literature, I reject the paper. Also, it seems to me that the technique actually pushed BN towards a small batch regime, where it is known to perform poorly. Finally, it misses comparison with other techniques. This paper claims results showing ReLU networks (or a particular architecture for that) are NP-hard to learn. The authors claim that results that essentially show this (such as those by Livni et al.) are unsatisfactory as they only show this for ReLU networks that are fully connected. However, the authors fail to criticize their own paper for only showing this result for a network with 3 gates. For the same reason that the Livni et al. results don't imply anything for fully connected networks, these results don't imply anything for larger networks. Conceivably certain gadgets could be created to ensure that the larger networks are essentially forced to ignore the rest of the gates. This line of research isn't terribly interesting and furthermore the paper is not particularly well written. For learning ReLUs, it is already known (assuming conjectures based on hardness of improper PAC learning) that functions that can be represented as a single hidden layer ReLU network cannot be learned even using a much larger network in polynomial time (see for instance the Livni et al. paper, etc.). Proving NP-hardness results for proper isn't as useful as they usually are very restricted in terms of architectures the learning algorithm is allowed to use. However, if they do want to show such results, I think the NP-hardness of learning 2-term DNF formulas will be a much easier starting point. Also, I think there is a flaw in the proof of Lemma 4.1. The function f *cannot* be represented by the networks the authors claim to use. In particular the 1/\eta outside the max(0, x) term is not acceptable. In this paper, the authors propose a dynamic convolution model by exploiting the inter-scene similarity. The computation cost is reduced significantly by reusing the feature map. In general, the paper is present clearly, but the technical contribution is rather incremental. I have several concerns:1. The authors should further clarify their advantages over the popular framework of CNN+LSTM. Actually, I did not see it. 2.  What is the difference between the proposed method and applying incremental learning on CNN?3. The proposed method reduced the computation in which phase, training or tesing?4. The experimental section is rather weak. The authors should make more comprehensive evaluation on the larger dataset. Currently, the authors only use some small dataset with short videos, which makes the acceleration unnecessary. Major comments:This paper builds on previous work in hierarchical LMDPs and extends the core ideas to an online setting.  Essentially, we incrementally construct a hierarchy by adding new states to upper-level MDPs every once in a while; these are loosely initialized and the parameters are then refined with additional experience.Overall, I felt that this paper lacked a substantial enough contribution. * The key contributions over previous work seems to be entirely contained in Sec. 3.1 and 3.2: (1) when we have visited k new states, add a new state to the hierarchy, and (2) initialize its parameters with intuitive values.* To me, this level of contribution is below the bar for ICLR.  The ideas seem simplistic and likely to work only in the simplest of domains.* The improvement over previous work is marginal.* I think the paper is lacking in clarity.  I do not think I could re-implement the paper, given the level of detail presented.* I was very disappointed in the experiments.  Not only were they on gridworld-like domains (see below), but it was not clear if the improvements were significant in any way.* While I thought the discussion in Sec. 3.3 was interesting, it didn't seem to be a "contribution"; it felt like some adhoc thoughts.* Keeping per-state counts is only workable in small state space domains.Minor comments:Please fix the formatting of your citations.There are numerous typos and spelling errors.  Please correct them.I am strongly opposed to the use of gridworlds, or anything like them, in modern RL research.  While many ideas work fine in small, toy domains, they simply do not scale.  As a field, we need to move past them and focus more on algorithms and ideas that have more practical relevance.Pros:+ Core ideas seem promising+ Leveraging mathematical structure is a great strategy for constructing algorithms with desirable propertiesCons:- Very limited experimental results- Not clear if improvements are significant- Hierarchy construction seems to be too limited to work in any reasonably sized problem- No evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method SummaryThe authors consider RL with safety constraints, which is framed as a multi-reward problem. At a high-level, this involves finding the Pareto front, which optimally trades off objectives. The paper primarily introduces and discusses a discretization scheme and methods to model the Q-value function as a NIPWC (non-increasing piecewise constant function). NIPWC are stored as values over discrete partitions of state-action spaces. To do so, the authors introduce two data structures DecRect and ContDecRect to store Q function values over geometric combinations of subsets of state-action space.The authors discuss how to execute elementary operations on these data structures, such as computing max(f(x), g(x)), weighted sums, etc. The goal is to use these operations to compute Bellman-type updates to compute optimal value/policy functions for multi-reward problems. The authors also present complexity analysis for these operations. Pro- Extensive discussion and analysis of discrete representations of Q-functions as NIPWCs. Con- A major issue with this work is that it is very densely written and spends a lot of time on developing the discretization framework and operations on NIPWC. However: - There is no clear practical algorithm to solve (simple) multi-reward RL problems with the authors' approach.- No experiments to demonstrate a simple implementation of these techniques.- Even though multi-reward settings are the stated problem of interest, authors don't discuss Pareto front computations in much detail, e.g., section 4.3 computing non-dominated actions is too short to be useful.- The discussion around complexity upper bounds is too dense and uninsightful. For instance, the bounds in section 5 all concern bounds on the Q-value as a function of the action, which results in upper bounds as a function of |A|. But in practice, the action is space is often small, but the state space is high-dimensional. Hence, these considerations seem less relevant. Overall, this work seems to present an interesting computational scheme, but it is hard to see how this is a scalable alternative. Practical demonstrations would benefit this work significantly.ReproducibilityN/A In this paper, the authors try to interpret the prediction mechanism of Layered Neural Networks (LNNs). The authors proposed to first define a feature vector that represents the roles of each hidden layer unit, via computing Pearson correlation coefficient. Then a hierarchical clustering method is applied to the generated feature vectors, such that tree-structured relationships among hidden layer units are revealed.The purpose of the paper is to understand the prediction mechanism of Layered Neural Networks (LNNs). But based on the results in the experiments, I do not think the model achieves this purpose. Given the tree structure of LNN for the MNIST data set, I am still not able to understand how this LNN distinguishes the digit 0 from other digits. I am also not able to understand why a particular sample is classified as 0 rather than 6.In Section 1, the authors mension that there are existing clustering-based methods that interpret LNN. The authors do not compare the proposed methods with these existing methods, either quantitatively or qualitatively. So I am also not sure the contribution of this paper, provided the existing methods.In Section 3.1, the authors state that "there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit". I do not agree with this statement, because Ross et.al (2017) has proposed to measure it via gradient, although they are trying to solve a slightly different problem. Since the output of a hidden unit is a non-linear function of the input, I am not convinced that the proposed method that computes Pearson correlation coefficient is better choise than computing the gradient.The proposed method provides a tree structure to describe the relationships between the hidden layer units. The authors also do not illustrate why learning the tree structure is particularly important. We can also run k-means with cosine similarity on the generated vector $v$, and learn the number of clusters via Bayesian information criterion (BIC). The authors do not explain why the tree-structured clustering results are more superior than the k-means clustering results.In summary, I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.ReferencesRoss, Andrew Slavin, Michael C. Hughes, and Finale Doshi-Velez. "Right for the right reasons: training differentiable models by constraining their explanations." Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI). 2017. Pros1.The paper is fairly clear.2.The problem is important: analyzing the internal computations of layered networks.3.The method seems to be a slight improvement on an existing method: the use of hierarchical clustering is nice.4.Figs. 3 and 5 superimposing the analyzed clusters on top of the network diagram are cool.Cons5.The paper wastes valuable space writing out in detail the equations for backpropagation in a standard feed-forward MLP.6.The paper does not have an acceptable review of relevant prior work. This is particularly problematic as the proposal seems to be a rather small tweak to prior work of two 2018 papers by Watanabe et al. But there is extensive other literature attempting to address this problem, especially in the vision domain, where their main example - poor over-worked MNIST - resides.7.In my view, attempts to understand processing in NNs exclusively at the individual-unit level are essentially doomed at the outset. These networks crucially represent their information in distributed representations and it is joint action by multiple units rather than action by individual units that drives processing. Consider the effect variable analyzed in this paper, which is a simple correlation between the activity of a target hidden unit and the activity of a particular input or output unit. Suppose whenever hidden unit i is active, hidden unit j is also active, and vice versa. Now suppose j strongly drives output unit k via a connection with a large weight, while unit i has no connection at all to unit k. Then i will have a strong effect on k! The correlations between the activity of i and k is the same as the correlation between j and k, even though the causal interaction between i and k is nil, while the causal interaction between j and k is strong. In this especially transparent situation, it is the joint action of i and j that matters, and it so happens that this joint action has no contribution from i.8.So in addition to the problems arising from analyzing exclusively at the individual-unit level, there is the problem of defining effect by correlation instead of causation.9.I dont myself gain any insight into how the MNIST network is working by looking at the clusters diagrammed in Fig. 4. There is no discussion of the fact that nearly all of their input-effect maps look like a slanted oval which is either on-center-off-surround or the reverse (no comment on the superficial, at least, connection to the receptive fields of neurons in the early mammalian visual system). Just how do these cluster maps explain anything? 10.The maps for the other example, time-series of prices of root vegetables, are even more baffling, but, superficially at least, the input maps suggest the hidden units are doing Fourier analysis; even this obvious observation is not made in the paper, however. The paper proposes a generative model that combines VAE and GAN. The main idea of the paper is to replace the standard normal distribution used in VAE with a normal distribution centered at a feature representation of the input image. In other words, the prior distribution is data adaptive. The paper compares the proposed generative model to DCGAN and EBGAN for image generation quality using the CelebA dataset and reports better human preference score.Overall, the paper is poorly written with incorrect technical descriptions and vague expositions.  The two baselines (DCGAN and EBGAN) are also quite out-dated. Beating these two baselines are insignificant, particularly there are GAN methods that  can generate high quality images without an encoder. For example, the Progressive GAN by Terro et. al. (ICLR 2018), SNGAN by Miyato et. al.(ICLR 2018), and GAN with zero-center gradient penalty by Mescheder et. al. (ICML 2018). The paper also fails to give a literature overview of effort in combining VAE and GAN. For example, Zhiting et. al. ICLR 2018 and Liu et. al. NIPS 2017.Technical errors- In the related works section, the paper states that VAEs and GANs are both based on maximum likelihood. This statement is incorrect as GANs are based on distribution matching. Vague exposition- In Section 2, the paper states that "Larsen et al. (2015) used both VAE and GAN in one generative model. As they just mixed two models and did not analyzed a latent space, so that the manifold of data was hidden to us." Isn't the data manifold in this case a multivariate Gaussian distribution. The paper fails to explain what it means by the sentence.- In Section 3.1, the paper states that "Since any supervision is not in training process, the manifold constructed is hidden to us." Again, the reviewer fails to understand what the paper means.- In Section 3.2.1, the paper states that "it is not efficient to pre-train the G , because it depends on the parameters of the D." This sentence is confusing. Isn't pretraining just meaning using a pretrained decoder weight to initialize G? This paper proposes an alternative search procedure for A* sampling that, in contrast to the original optimistic search, doesn't rely on (possibly difficult-to-find) bounds for the log-probability function.The first major issue with this paper is clarity. The preliminary section describing the Gumbel process and A* sampling is very difficult to understand (despite my being quite familiar with A* sampling). The authors use undefined notation frequently throughout the introduction and refer to it in abstract terms. There are also numerous errors -- for example, when describing the "bottom up" approach to generating a Gumbel process, the authors suggest perturbing all points in the input space by independent Gumbel noise (which would result in Gp(S) = infty almost surely when omega is uncountable). The description of the main contributions in section 4 is equally unclear. This section starts by suggesting that sampling the next node to investigate according to the probability that it contains the maximum is reasonable, and then presents a lemma about regret in a bandit setting where the sampler never splits the subset. This lemma does not apply to the actual sampler proposed in the paper, so it is not clear why it is included. Section 4.2 is also very unclear -- I am not certain how both w and script Y are defined, nor why we need an "unbiased estimator of Y" (a random variable?) when we can simply sample from Y directly. As the definition of w is unclear, the purpose of 4.3 is unclear as well.The other major issue is more fundamental -- I am not convinced the sampler is correct. The algorithm simply terminates after some finite horizon (rather than having a conclusive proof of termination via branch and bound as in the original A*). There is no proof or argument included in the paper regarding this. Any proposed sampling algorithm must be correct to be acceptable in ICLR. The paper studies the use of embedding techniques in recommender systems, and shows that item2vec (an item vectorization method) can be replaced by user2vec, as users and items are interchangeable.This is a reasonable enough idea, though not sufficient for publication in ICLR. I'd suggest the authors address the following details:-- The methodological contribution is too small, and fairly obvious. Not sufficient for this conference.-- Only evaluated on one dataset, so unclear whether the results are really representative-- Comparisons against a very limited set of similar methods, which are probably not state-of-the-art for this dataset-- The results don't seem significant, all methods compared perform almost equally The paper deals with the idea to generalize the CapsNet architecture from Sabour. Under generalization the authors mean, to define a routing procedure without an iteration parameter.In general, your paper has a good length, is well explained and good organized. To be honest, I dont like your writing style. It seems to be a bit too casual and not formal enough for a scientific work. Additionally, note that:-You are not staring with Fig. 1 in the introduction&the counting should start by one.-AC-GAN is the abbreviation for? -Eq. 2 is outside the page space.I have several concerns about the contribution. My major concern is that it isnt new in general. If I break down your method, it is just the basic Dynamic Routing procedure with:-the number of iterations defined to be one;-trainable initial routing coefficients;-no softmax normalization over routing coefficients.The usage of trainable initial routing coefficients was already mentioned by Sabour. Thus, the only thing which is new in your method is that you skip the normalization and Im not sure that this has a positive effect on the process. Minor concerns/minor mistakes:1.You mentioned that the code is public available. Where is the link to a respective repository?2.Page 1: [&] so it makes sense to believe that CapsNet has a better generalization ability. Compared to what?3.Page 2: The routing iterations is a meta-parameter that needs to be set manually which limits the scalability of CapsNet [&]. Why it should limit the scalability? It has no effect on the model size, etc. Its just a parameter which has to be defined.4.Page 3: Are you sure that a linear transformation of a hyper-cube is defined in that way in general?5.Page 4: What is T_k?6.Page 7: Hinton et al. (2018) claimed that CapsNets [&] Are you aware that Hinton worked on Matrix Capsules and not on the CapsNet architecture of Sabour?7.Page 8: How you can guarantee the convergence of your method? Moreover, the convergence to what?8.Could you add some histograms plots of your c_ij values after the training?9.Why are your performance values so bad compared to CapsNet and Matrix Capsules?10.Could you add to your tables the inference, training times? If you remove the iteration parameter I would assume that your method should be faster, or?11.Is the parameter lambda in Eq. 2 the same as in Eq. 6? How you tune that parameter? This paper proposes a supervised learning method for predicting the connectivity of a graph based on both the features of nodes in the graph as well as the overall graph structure, rather than just the structure of the graph or just the node features. The approach is evaluated on two synthetic datasets, a community dataset and a geometric figures dataset.Unfortunately, I do not think this paper as it stands currently is ready for publication at ICLR for the following reasons: (1) the comparison and discussion with prior literature is lacking; (2) the experiments are rather weak; and (3) the significance and novelty of the method itself seems limited.1. I am very surprised there was no discussion of [1] (or even better, a comparison to), which is another method which uses information about the full graph (via message-passing) to infer the connectivity of the graph in an unsupervised way. The discussion of the literature on graph neural networks in general is a bit weak, missing important references such as [2-4]. Such approaches, especially message-passing style approaches like [4], do exactly what the current paper suggests has not been done: they make predictions based on information in the nodes while considering the structure of the graph as a whole (via message-passing). Although [4] does not explicitly make edge predictions the approach is straightforward to generalize to making edge predictions, see [5].2. The experiments in the paper only test the proposed method on very toy domains, and thus feel weak. The results in Table 2, for example, suggest that the proposed method has reached ceiling-level performance and thus to really tell the difference between GLN_f, GLN_c, and any other methods, more difficult problems are called for. The geometric figures dataset, in particular, does not seem to me like it would test the claim that the paper would like to make: that it is important to take into account the fully structure of the graph when predicting edges. Indeed, there is a very simple rule that can be applied in the geometric figures case which does not use global graph information (if the two nodes have the same color, connect them, if they are different colors, do not connect them). It is therefore unsurprising that GLN_c actually does slightly better than GLN_f (according to Table 2) on geometric figures.Additionally, the experiments do not provide much insight into the architecture itself. For example, the present architecture is meant to repeat the embedding and link-prediction steps some number of times, and in the experiments it seems that these steps are repeated four times. But how important is the repetition in practice? It would be nice to see the effect of repetitions on final performance, to demonstrate whether this is in fact an important component of the model or not. Similarly, there are several different loss functions but it is not obvious to what extent these losses contribute to the final performance of the model. It would be nice if there could be some ablation studies that train the model with different combinations of losses to see which are actually important.3. Finally, I have some concerns about the model itself. If I understand correctly, both f_l and c_l depend on a number of learned parameters which is a function of the number of nodes in the graph. This is unfortunate, as one of the strengths of the graph neural network approach is that GNNs usually have a number of parameters that is independent of the size of the graph, thus allowing GNNs to scale to graphs of arbitrary size. However, that is not the case in this model. Moreover, the architecture of f_l and c_l do not seem particularly novel. f_l just involves passing the node embeddings through a MLP to produce the link predictions. c_l involves something closer to message-passing, though where weighted combinations are learned on a per-node basis (rather than sharing the same function across all nodes). This could be interesting, even though it sacrifices the scale-free nature of GNNs, if it could be shown to actually outperform existing GNN approaches on more realistic datasets. However, given the lack of experiments demonstrating this, it is hard to say how significant the approach is.[1] Kipf, Fetaya, Wang, Welling &amp; Zemel (2018). Neural Relational Inference for Interacting Systems. ICML 2018.[2] Gori, M., Monfardini, G., and Scarselli, F. (2005). A new model for learning in graph domains. IJCNN 2005.[3] Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009). The graphneural network model. IEEE Transactions on Neural Networks, 20(1):6180.[4] Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212.[5] Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., ... Pascanu, R. (2018). Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261. This paper studies multi-agent reinforcement learning where the agents need to communicate information when observations are noisy.  The agents thus need to learn what information should be sent to other agents.The authors claim "we do not assume the existence of explicit rewards guiding the communication action," which however is questionable.  The "extrinsic reward" used to guide the communication action is simply the cumulative reward between two communication actions.  The reward is explicitly given.The key assumption is that communication is not performed every step.  Then standard cumulative reward until the next communication can be used as immediate reward for the previous communication.  Should this assumption be considered as an assumption of the domain where the proposed approach can be applied, or is this assumption rather a technique that one should use even when communication can be performed every step?  In the latter case, the effectiveness is sparse communication is not demonstrated.In addition, the intrinsic reward for guiding environmental actions is unclear.  In the experiment, the standard reward is simply used as intrinsic reward.  So, intrinsic reward is just standard (extrinsic) reward?  In general, how should we design intrinsic reward?  What is the advantage of not using the standard reward as intrinsic reward?The experimental settings are too ideal for the proposed approach, and it is unclear how the proposed approach work in practical settings.  In particular, sequential decision making is not essential in the experimental settings.  What are the real applications in mind? The authors propose an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy.Several key terms are used in ways that differ from the rest of the literature. The authors claim options are learned in an "unsupervised" manner, but it is unclear what this means. Previous work (none of which is cited) has dealt with unsupervised option discovery in the context of mutual information maximization (Variational intrinsic control, diversity is all you need, etc), but they do so in the absence of reward, unlike this paper. "Optimal policy" is similarly abused, with it appearing to mean optimal from the perspective of the current model parameters, rather than optimal in any global sense. Or at least I think that is what the authors intend. If they do mean the globally optimal policy, then its unclear how to interpret Equation 8, with its reference to a behavior policy and an advantage function, neither of which would be available if meant to represent the global optimum.Equation 10 comes out of nowhere. One must assume they meant "maximize mutual information" and not "minimize", but who knows. Why is white-noise being added to the states and actions? Is this some sort of noise-contrastive estimation approach to mutual information estimation? It doesn't appear to be, but it is unclear what else could motivate it. Even the appendices fail to shine light on this equation.The algorithm block isn't terribly helpful. The "t" variable is used outside of its for loop, which draws into question the exact nesting structure of the underlying algorithm (which isn't obvious for HRL methods). There aren't any equations referenced, with the option policy network's update not even referencing the loss nor data over which the loss would be evaluated.Some of the experimental results show promise, but the PPO Ant result raises some questions. Clearly the OpenAI implementation of PPO used would have tuned for the OpenAI gym Ant implementation, and the appendix shows it getting decent results. But it never takes off in the harder RlLab version -- were the hyper-parameters adjusted for this new environment?It is also odd that no other HRL approaches are evaluated against, given the number cited. Running these methods might be too costly, but surely a table comparing results reported in those papers should be included.A minor point: another good baseline would be TD3 with the action repeat adjusted to be inline with the gating policy.I apologise if this review came off as too harsh -- I believe a good paper can be made of this with extensive rewrites and additional experiments. But the complete lack of clarity makes it feel like it was rushed out prematurely. Summary: This paper proposes to reformulate the QA task in SQUAD as a retrieval task, i.e., using question as query and paragraphs as candidate results to be ranked.  Authors makes some modifications to elmo model to create better word embedding for the ranking task. Authors have mentioned and are aware of open domain QA methodologies (e.g., DrQA).Pros:- The general idea is interesting, to reformulate any QA task as a ranking taskCons:- The methodology and task are not clear. Authors have reformulated QA in SQUAD as as ranking and never compared the results of the proposed model with other QA systems. If authors want to solve a pure ranking problem why they do not compare their methods with other ranking methods/datasets.- The novelty: The novelty is not significant. Although modifications to ELMO are interesting. - Results: Why authors have not compared their work with DrQA? This paper tries to study retrieval methods for multi-paragraph / multi-document reading comprehension.  The basic approach is to embed the question and the paragraph and train a system to put the correct paragraph close to the question.  I had a very hard time following the details of the proposed approach for this, however, and I still don't really understand what the authors are proposing.This paper is not ready for publication.  The exposition is not at all clear and needs substantial rewriting.  Additionally, the evaluation done in the paper is not well-justified.  I do not know what "paragraph-side" means, but I assume that means you are trying to retrieve the question given the paragraph.  Why?  There were no standard baselines compared against, like a simple IR system (Lucene).  And I expected to see actual impact of the retrieved results on downstream QA performance of a system like Chen et al.'s, or Clark and Gardner 2018.  Even if you have a slightly better ranking of the retrieved paragraphs, it's not clear to me that this will improve performance, if the downstream method is properly calibrated to handle multiple paragraphs (see Clark and Gardner 2018).A few writing suggestions for the authors, for next time:This paper does not follow the typical flow of an academic paper.  It reads too much like a logbook of what you did, presented chronologically, instead of presenting the ideas in a coherent sequence.  Part of this is just simple wording fixes (e.g., avoid things like "it was time to compute ELMo representations" - this isn't a logbook).  Also, all of the shape comments and numerical details at the top of page 4 are out of place.  Describe your method first in general terms, then give experimental details (like corpus size, etc.) later.  I suggest reading the award-winning papers at various conferences to get a sense of how these papers are typically structured and phrased.Section 2: A full page dedicated to the history of word embeddings is entirely unnecessary for this paper.  This is not a survey on word embeddings.  It's much more useful to the reader to give pointers to multiple connection points between your work and the rest of the literature.  You could have given a paragraph to the most relevant embedding techniques, a paragraph to the most relevant retrieval / multi-paragraph techniques (e.g., Clark and Gardner 2018, which is very relevant, along with Chen et al., TriviaQA, others), and a paragraph to distance metric learning. This paper proposes an extension of deep image compression model to video compression. The performance is compared with H.264 and MPEG-4. My main concern is the limited technical novelty and evaluation:   - The main idea of the architecture is extending 2D convolutions in image compression networks to 3D convolutions, and use skip connections for multi-scale modeling. The 2D to 3D extension is relatively straightforward, and multi-scale modeling is similar to techniques used in, e.g., [Rippel and Bourdev ICML 2017].   - The reconstruction loss and the entropy loss are commonly used in existing work. One new component is the temporal consistency loss. However the impact of the loss is not analyzed in the Experiment section. - The evaluation isnt very extensive. Comparing the proposed method with state-of-the-art codecs (e.g., H.265) or other deep video compression codec (e.g., Wu et al. in ECCV 2018) would be valuable.  - Since the evaluation dataset is small, evaluation on multiple datasets would make the experiments more convincing.  - The evaluation is conducted in rather low-bitrate region only (MS-SSIM &lt; 0.9), which is not common point of operation.  - Finally I agree with AnonReviewer2 on limited description of evaluation details. Overall I think this paper is not ready for publication yet. This paper presents a spatiotemporal convolutional autoencoder trained for video compression. The basic model follows the logic of traditional autoencoders, with an intermediate quantizer:input -&gt; convolutional neural network with a skip connection as an encoder -&gt; quantizer -&gt; transposed convolutional neural network with a skip connection as a decoder.As the quantizer is a non-differentiable operation, the paper proposes to follow (Toderici et al  2016, Balle et al, 2018) and cast quantization as adding uniform noise to the latent variables. The pre-quantized variables are modelled as Gaussians with variance that is predicted by a second "hyperprior" network dedicated to this task. The final model is trained to minimize three losses. The first loss minimizes the difference between the true frame pixel values and the predicted pixel values. The second loss minimizes the entropy of the latent codes. The third loss minimizes the difference between neighboring pixels in subsequent frames, ignoring those pixels that are not linked between frames. The model is trained on 10,000 videos from the Youtub-8M dataset and tested on 10 videos from the MCL-V database, with rather ok results.Generally, parts of the proposed approach sound logical: an autoencoder like architecture makes sense for this problem. Also, the idea of using uniform noise to emulate quantization is interesting.  However, the paper has also weaknesses.- The added novelty is limited and unclear.  Conceptually, the paper is overclaiming. Quoting verbatim from the conclusion: "Our work is, as far as we are aware, the first end-to-end learned video compression architecture using DL.", while already citing few works that also rely on deep networks (Wu et al., 2018, Chen et al., 2016). In the related work section it is noted that these works are computationally heavy. However, this doesn't mean they are not end-to-end. The claims appear to be contradicting.- The technical novelty is also limited. What is new is the combination of existing components for the task of video compression. However, each component in isolation is not novel, or it is not explained as such.- Parts of the model are unclear. How is the mask M computed in equation (7)? Is M literally the optical flow between frames? If yes, what is the percentage of pixels that is zeroed out? Furthermore, can one claim the model is fully end to end, since a non-differentiable optical flow algorithm is used?- The purpose of the hyperprior network is unclear. Why not use a VAE that also returns the variance per data point?- Most importantly, it is not clear whether the model is trained as a generative one, e.g., with using a variational framework to compute the approximate posterior. If the model is not generative, how can the model be used for generation? Isn't it then that the decoder simply works for reconstruction of already seen frames? Is there any guarantee that the model generalizes well to unknown inputs? The fact that the model is evaluated only on 10 video sequences does not help with convincing with the generalization.- The evaluation is rather weak. The method is tested on a single, extremely small dataset of just 10 videos. In this small dataset the proposed method seems to perform worse in the majority of compression ratios (bits per pixel). The method does seem to perform a bit better on the very low bits per pixel regime. However, given the small size of the dataset, it is not clear whether these results suffice.- Only two baselines are considered, both hand-crafted codecs: H.264/AVC and MPEG-4. However, in the related work section there are works that could also be applied to the task, e.g., the aforementioned ones. Why aren't these included in the comparison?- Although it is explained that the focus is on the very low bitrates, it is not clear what part of the model is designed with that focus in mind. Is this just a statement just so to focus on the part of the curve in the experiment where the proposed method is better than the reported baselines? Is there some intrinsic model hypothesis that makes the model suitable for low bit rates?In general, the paper needs to clarify the model and especially explain if it is (or not a generative one) and why. Also, a more extensitve arrays of experiments need to be executed to give a better outline of the methods capabilities and limitations. Summary of the paper:This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. "structured" means that instead of just minimizing the L2 norm of the gradients, a "mahalanobis norm" is minimized. The covariance matrix is updated continuously to track the "structure" of gradients/perturbations. Whitebox attack and blackbox attack The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting.However, I believe the paper has major flaws in several aspects.The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challengeI noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST.In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because "intrinsic" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated.So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.There're also a few problems in the motivation / analysis. """A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations."""The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss."""Thus, under the assumption that \phi \approx \phi^* and of small perturbations (such that we can ignore higher order terms."""The Bayes optimal assumption seems to be arbitrary to me. If \phi is nearly Bayes-optimal, why would we worry about adversarial examples?Other relatively minor problemsIn the caption of Figure 1, """Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure."""PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse?In Section 3.1, the paper talks about both centered and uncentered adversarial examples.I assumed that the authors mean that the distribution of perturbations are centered?First, I think this the authors should make this more explicit.Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered?Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper.I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved?Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare. This paper is very similar to two previously published papers (as pointed by David Minnen before the review period was opened):"Learning a Code-Space Predictor by Exploiting Intra-Image-Dependencies" (Klopp et al.)  from BMVC 2018,and"Joint Autoregressive and Hierarchical Priors for Learned Image Compression" (Minnen et al.) from NIPS 2018.The authors have already tried to address these similarities and have provided a list in their reply, and my summary of the differences is as follows (dear authors: please comment if I am misrepresenting what you said):(1) the context model is slightly different(2) parametric model for hyperprior vs non-parametric(3) this point is highly debatable to be considered as a difference because the distinction between using noisy outputs vs quantized outputs is a very tiny detail (any any practitioner would probably try both and test which works better). (4) this is not really a difference. The fact that you provide details about the method should be a default! I want all the papers I read to have enough details to be able to implement them.(5+)  not relevant for the discussion here.If the results were significantly different from previous work, these differences would indeed be interesting to discuss, but they didn't seem to change much vs. previously published work.If the other papers didn't exist, this would be an excellent paper on its own. However, I think the overlap is definitely there and as you can see from the summary above, it's not really clear to me whether this should be an ICLR paper or not. I am on the fence because I would expect more from a paper to be accepted to this venue (i.e., more than an incremental update to an existing set of models, which have already been covered in two papers). Summary:The authors propose a technique for reducing the computational requirements of training BERT early in training to reduce the overall amount of resources required.Pros:The paper is well written and clear for the most part. The authors do thorough experimental evaluation.Cons:I have two primary concerns about the paper and the proposed technique.1. The positioning of the technique is not entirely clear to me. The authors pitch it as a technique for reducing the training time of BERT and use LayerDrop as a baseline technique that also removes network components. However, it feels like another baseline that should be considered is neural architecture search, which also seeks to automatically find a more efficient model to train. The difference here is that the authors find the model early in the training run, but it seems like the EarlyBERT procedure could be run once and the resulting model architecture could be saved and re-trained like NAS models are. 2. I found the experimental results to be lacking detail and breadth necessary to establish the value of the technique. Firstly, the rough time estimates in Table 2 are very odd given the primary value of the proposed technique is to reduce training time. The accuracy of EarlyBERT is close enough to LayerDrop that accurate training cost numbers are needed to differentiate between the techniques. Secondly, quoting training time reductions over the dense baseline when the EarlyBERT mode does not achieve the same accuracy makes the comparison very difficult to make. This problem shows up quite commonly in the model compression literature [1] and Id encourage the authors to show full accuracy-training time tradeoff curves so that the training time savings for a given accuracy can be more clearly established. Lastly, I found the use of reduced training epochs in EarlyBERT to be odd because you do not evaluate whether or not this can be done for the baseline models and there isnt clear evidence as to why your model would be able to do this while others (e.g., DropLayer) cannot. Figure 2 also does not seem to corroborate that higher learning rates can be used with shorter training time to achieve better accuracy. The data in your figure shows that the best learning rate achieves the best model quality independent of the number of training epochs.References:1. https://arxiv.org/abs/2003.03033 This paper argues that as the cross-entropy loss goes to zero, since the correct logit increases in magnitude the entries of the Hessian diminish to zero. Such overfitting on the training set and a small spectral norm of the Hessian should result in poor generalization error. Motivated by this, the paper experimentally evaluates the effect of weight decay on the Hessian controls the magnitude of weights, increases the spectral norm of the Hessian and improves the generalization.This paper presents the beginning of an interesting argument but the thesis of the study is unclear. Hessian-based measures of sharpness come with caveats, as has been explored in the literature cited in the paper. The conclusions of the paper have been widely discussed in these existing papers and it is therefore the incremental value of this papers results is difficult to ascertain.I do not believe I understand what the paper is arguing here. It is clear that exponential losses such as the cross-entropy loss result in weights that can have a large magnitude. It is also clear from Section 3 that entries of the Hessian go to zero if the weights increase. It is also known that the spectral norm of the Hessian can be made arbitrarily small/large without changing the input-output mapping of the deep network and thereby the generalization performance. So although the experiments are sound, the conclusion that flatness is a false friend does not follow from these experiments.The experiments are sound but the numerical accuracies are very poor; Fig 2 shows 3.3% validation error on MNIST, Fig 3 shows 45.2% error on CIFAR-100, Fig. 4 does have a stronger error of 19.4%. The authors are advised to use state of the art data augmentation and regularization techniques in these experiments.The paper advocates the need to understand the magnitude of weights in addition to spectral properties of the Hessian. The reviewer agrees with this point of view but the present paper does not offer any concrete insights into this issue. The paper proposes to use a CNN to compute an initial guess for the iterative Newton-Rhapson solution of a coupled PDE system used for semiconductor device simulation. To do so, the authors construct a "device template" which parametrizes the design space. The CNN then maps a device configuration in this 6-dim space to the predicted electrostatic potential in the form of a 64x64 grid. The authors provide an analysis showing why predicting the electrostatic potential alone is sufficient. Overall, this approach can provide a simulation speedup of 12x or more. The authors are also planning to publish the dataset generated for the paper.The idea to take a data-driven approach to compute the initial approximate solution in the context of the semiconductor simulation domain is interesting. Most prior work utilizing ML to accelerate PDE solution tried to generate such solutions directly. The path taken here, while seemingly easier, is interesting from an application point of view in that the solver is still used as before, potentially alleviating concerns about the validity of ML-accelerated solutions. The claimed order of magnitude speedup is significant (but see below for some comments on this). The main downside of the paper is the very limited novelty on the ML front (off-the-shelf generator from the DC-GAN paper) and demonstrated applicability to a specific and relatively narrow domain. As such, I recommend against publication at ICLR, and would advise the authors to submit the paper to a venue where the presented advances are likely to be of wider interest.One specific point that I believe should be addressed, is to clarify the cost of building the dataset (10k simulations) and training the network, and its impact on the effective simulation speedup. As is, the 12x speedup would seem to apply in the limit of a very large number of network inference runs, and it is unclear if this is a practically relevant setting.Suggestions for improvements:- Explain to the reader explicitly why this is a 2d problem while the device exists in 3d space.- What specifically was the validation set used for?- In the text it is stated that "grid spacing is adjusted". Please explain more how this is done.- The text states that the network was trained with backprop. Which specific optimizer was used, and with which hyperparameters?- Including parts of the explanations currently in appendix B could make the text clearer and easier to read for people from outside of your field (e.g. the importance of bias ramping, and its relation to the simulation cost). The work proposes a Rademacher type bound for the fine-tuned models based on the distance between the fine-tuned weights and the pre-trained weights. Since the distance term shows up in the upper bound on the generalization gap, the authors further propose to adopt it as the regularization term to boost the generalization performance of the model during the fine-tuning process. Some experiments are also done to show the effectiveness of the proposed regularization.I am seriously concerned about the correctness of the Rademacher-type bound the authors have proposed. The bound does not seem correct to me.The flaw comes from the function class F_* defined in section 3 of the draft. The function class F_*, by definition, depends on the pre-trained weights W_j^0. However, W_j^0 is not fixed, it is random! This is because W_j^0 depends on the data (W_j^0 is pre-trained using the data), which by the assumption of the draft, is random. As a consequence you cannot assume W_j^0 as fixed. The randomness of the hypothesis class F_* destroys almost all the derivations the authors are currently using in their proof. Another minor bug is the second term in the bound for theorem 1 seems to have some subscript issues. To me the product term related to B_j^\infty should go from i=1 to j instead of from j=1 to L. I may have missed something in this point but could the authors double check if the subscript of the B_j^\infty is correct? In particular the derivation from the second inequality to the third on page 13 of the appendix. The second issue is easy to fix. However the first issue seems like a fundamental flaw. I do not have a good way to handle it for now.  This paper considers adopting continual learning on the problem of causal effect estimation. The paper combines methods and algorithms for storing feature representation and representative samples (herding algorithm), avoiding drifting feature representation when new data is learned (feature representation distillation), balanced representation by regularization, etc. Consequently, the paper presents a system that makes use of existing methods as a loss function (the sum of losses and regularization terms). It is difficult to observe the novelty of the method---there is no apparent challenge arose in combining these methods as they can just be simply added. Further, the development of this framework for causal inference is mostly orthogonal to the problem of causal effect estimation (and the use of regularization does not add any novelty.) In addition to the lack of novelty, the paper has many non-negligible mistakes in describing causal inference. For example, consider a phrase "selection bias" between "treatment and control groups" in the first sentence of the second paragraph. "Treatment and control groups" usually are used to describe two groups under the context of RCT. Further, the bias we are talking in the paper is a "confounding bias" not a "selection bias". In Section 2, "Each unit ... received ... one of ... treatments" is also relevant to RCT not observational data. There are many other places the authors mentioning "selection bias".Question:What is "Real world evidence"?, which is used in the title, abstract, conclusion, and at the beginning of section 3? Is it just observational data? ## ContributionsThis paper presents SBEVNet, a neural network architecture to estimate the bird's-eye view (BEV) layout of an urban driving scene. Given an image captured by a stereo camera, SBEVNet performs an inverse perspective mapping (IPM) to obtain an initial feature volume, which is further processed to generate the BEV layout. The system is trained end-to-end in a supervised learning setup.## Strengths**S1** The problem considered here is very relevant to perception groups in the autonomous driving community. This area has only recently seen work crop up. Approaches like MonoLayout [A], MonoOccupancy [B], and PseudoLidar [C] are closely related to this submission.**S2** The paper is easy to follow, and provides a majority of the details needed to understand and assess the approach.**S3** The authors also seem to provide code (and promise a public release), which might help ensure reproducibility.## WeaknessesI see a few major and a number of other minor concerns that impact my perception of this paper. I'm hoping the discussion period helps address some of these, and I'm open to revising my score in light of evidence contrary to the following claims.It appears that this paper uses MonoLayout [A], MonoOccupancy [B], and PseudoLidar [C] as primary baselines. Much of my review stems from my understanding of [A, B, C] (and my 'surprise' at a few contradictory trends observed in this paper.)**Problem setup** It is unclear from reading the paper and supplementary material if the problem setup is infact "amodal" layout estimation (i.e., if scene points outside of the camera view are predicted in the BEV layout). Approaches like (Schulter et al., 2016) and (Mani et al., 2020) operate in this "amodal" setup, while others such as PseudoLidar [C] and (Lu et al., 2019) only predict points that are visible in the input image. Does this approach, for instance, hallucianate hidden intersections and roads? (It seems not, since a visibility mask is explicitly employed in the loss function -- cf. Fig. 1 and Eq. 12, 13).**MonoLayout baseline** The primary baseline considered in this paper is "MonoLayout" (Mani et al., 2020). Upon examining the MonoLayout [A] paper, I find a surprising and troubling trend. This paper reports very poor performances of MonoLayout on the KITTI dataset (the original MonoLayout paper reports mIoU for the "car" class to be around 26.08, while the current submission reports 2.43 -- cf. Table 2). I've noted that MonoLayout makes its code and models publicly available (its publicly available pretrained models claim an mIoU of 30.18 for the "car" class), as highlighted on their GitHub page. Also, other baselines like "MonoOccupancy" have surprisingly low scores in this paper (an order of magnitude), compared to scores reported in the MonoLayout paper. I wonder if there is something different in the experiment and/or training protocols employed in the current work, as opposed to those in the MonoOccupancy and MonoLayout papers? For example, the MonoOccupancy baseline as reported in the MonoLayout paper achieves an mIoU of about 24.16 (for the car class) (MonoLayout paper - Table 1), while the same baseline has a dismal performance (mIoU of 7.11 for car class) in Table 2 of the current manuscript.The fact that this performance gap is not explained in the paper makes it hard to analyze the merits of the proposed approach. Save for a single sentence "The results of MonoLayout ... and MonoOccupancy ... are inferior due to lack of any camera geometry priors in the network", I've not found any other discussion of this performance gap/discrepancy.I also find it a tad weird (and unexplained) that the performance of various baselines do not seem to follow a set pattern/trend across the CARLA and KITTI datasets. In the MonoLayout paper, I notice that changing the dataset from KITTI to Argoverse does change absolute mIoU scores a bit, but preserves the ranking of various baselines (i.e., MonoLayout > OFT > MonoOccupancy on both KITTI and Argoverse). In the current submission, the trends seem to be changing across the two datasets (cf. Tables 1, 2).Yet another set of baselines that seem to underperform here are the PseudoLidar variants. In the MonoLayout paper (cf. supplementary material, Table 5), Pseudolidar is evaluated on the KITTI dataset, and the reported mIoU for vehicles is 59, whereas in this paper the best performance on this class achieved by a pseudolidar model is 45.64. Further, the MonoLayout paper's version of the (stereo) Pseudolidar baseline seems to perform quite competetively (mIoU 59.0) to SBEVNet Ensemble (mIoU 60.17 for "car", cf. Table 2). This seems to indicate that well-tuned baselines could perhaps achieve better performance?In Appendix A.2, the authors seem to indicate that they used a very different process to train MonoLayout (i.e., using random images from the train set as opposed to using OpenStreetMap and/or adversarial training). I suspect this might have resulted in a performance gap?I feel that OFT [D] could be cited and used as a baseline, particularly to measure layout estimation accuracy for the "car" class.**Qualitative results** Unfortunately, there seems to be a dearth of qualitative result figures to get a better sense of the approach. In particular MonoLayout and MonoOccupancy seem to obtain crisp reconstructions of cars (cf. MonoLayout paper), while in Figure 2., cars are splayed throughout the image in the SBEVNet results. This is also surprising; in my opinion, these results do not adequately substantiate the impressive reported mIoU.**Missing mAP metric** Other papers such as MonoLayout and OFT seem to report the mAP (mean average precision) metric in addition to the mIoU metric, because mAP often turns out to be a more accurate estimate of prediction performance (due to integrating over various recall values). In practice, this leads to less-than-perfect predictions being scored well (and this could explain the splayed-out results in Fig. 2 scoring a high mIoU). Evaluating mAP would be a stricter criteria, and will allow an additional point of comparison with prior art.## Minor remarksThe following remarks have had no impact on my assessment of the paper, and as such I don't expect the authors to respond to these.Concurrent approaches such as [F] can be cited and discussed.The paper could be structured better. For instance, input image sizes and baselines could be moved over to the main paper, rather than being listed in the appendix.## References[A] Mani, Kaustubh, et al. "MonoLayout: Amodal scene layout from a single image." The IEEE Winter Conference on Applications of Computer Vision. 2020.[B] Lu, Chenyang, Marinus Jacobus Gerardus van de Molengraft, and Gijs Dubbelman. "Monocular semantic occupancy grid mapping with convolutional variational encoderdecoder networks." IEEE Robotics and Automation Letters 4.2 (2019): 445-452.[C] Wang, Yan, et al. "Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.[D] Roddick, Thomas, Alex Kendall, and Roberto Cipolla. "Orthographic feature transform for monocular 3d object detection." arXiv preprint arXiv:1811.08188 (2018).[E] A Parametric Top-View Representation of Complex Road Scenes. CVPR 2019.[F] Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D. ECCV 2020. Certified robustness approaches have been studied for single models based on interval propagation as well as randomized smoothing. The use of ensembles for empirical robustness has also been studied in the literature. This paper attempts to theoretically study the certifiable defense achieved by ensembles. The paper analyzes the standard Weighted Ensemble (WE) and MaxMargin Ensemble (MME) protocols, and proves the necessary and sufficient conditions for robustness under smoothness assumptions. The key idea is to show and utilize the diversification of gradients and large confidence margins. Pros:+ The paper addresses an important challenge of adversarial robustness via ensembles and attempts to develop theoretical bounds for these defenses. Cons:- Some theoretical discussion is rather straight-forward formulation of consequences of the definitions of robustness and ensemble methods. Proposition 1 and Theorem 1 follow directly from the definition of WE and MME, and r-robustness. - There are several notation lapses which make the paper difficult to read. In definition 2, it appears the intent is to take the partial gradient of the model at two different points x and y and then bound the ration of the difference of the gradients and the distance between two points (which would be the curvature). x,y are being used as values and the variable. The boldface font is not used consistently.  Another example is the missing relation between r and epsilon (which is described in the appendix). These make simple results difficult to parse, and negatively effect the readability of the paper and obscure the identification of novelty. - For Theorem 2, let us set N = 1 (that is the degenerate case when the ensemble is a single model), we see that the rate of change of difference between the top prediction and other predictions are now bounded by a linear spread of the difference divided by the robustness radius r and then we add or subtract (depending on sufficiency/necessity) the impact of curvature. Rearranging the terms so that first and third are on the same side, the theorem will read:r ( gradient term for prediction difference ) +/-  r^2 ( curvature term for prediction difference ) <=  prediction difference.Isn't this just a Taylor series approximation of the prediction difference using bounds on the curvature term (assumed as part of the definition)? Now, if we bring back any arbitrary N, the same would be applicable by the definition of how decisions are made by WE (the "prediction difference" term is now changed). Theorem 3 terms can be similarly rearranged to make the statement more obvious. Is the reviewer missing some non-obvious observation or challenge in proving Theorem 2/3?- Despite several other papers also using the general term of "certified robustness", it is important to note that this robustness is against a rather benign perturbation model. Typical perturbations (in particular adversarial attacks or other natural perturbations such as fog, rain for vision) do not fall into this class. But the reviewer is not concerned with this aspect heavily given the prevalence of the rather generic name of "certified robustness" for such approaches. - The empirical enforcement of diversity in ensembles has been studied in literature such as Pang et al 2019. https://arxiv.org/abs/1901.09981 uses cosine similarity.  Experimental evaluation with these methods is crucial to understand the value of the proposed approach. The current evaluation is very limited. Questions and suggestions to the authors:1. It might be a good idea to bring the relationship between r and epsilon in the main text from the appendix. r-robustness is the fundamental definition and in its current presentation r does not occur anywhere. 2. Could you help understand the concern in Weakness bit 3? This will help the reviewer better appreciate the theoretical novelty of the paper. Currently, the theoretical statements appear to make obvious statements once we get through the rather clunky notation and presentation choices. 3. Why is cosine used for diversity? There are several ways to enforce diversity. Was cosine selected arbitrarily or is it motivated by some theoretical insight? The paper addresses an important challenge of mathematically well-motivated defenses against perturbations. In its current form, the paper appears to formulate rather simplistic observations as theoretical results with a rather dense presentation. The empirical evaluation is significantly incomplete.  - pros    - The problem is well motivated. Estimating the causal effect with time-series data is a practical and important problem. Transparency is a desirable property.     - The idea connects works in econometrics to modern ML. - cons    - High level points        - The paper proposes to use the number of donor units as a metric for explainability. It is not clear why this matric is good a priori. It is possible that the network selects an arbitrary set of units to construct the control.          - For trustworthiness, It is quite confusing how synctwin is supposed to identify individuals whose ITE cannot be reliably estimated. This paper did not provide any formal procedures or guarantees on when identification is possible.        - Low rank is a key assumption for synthetic control. A discussion on whether this assumption is required for synthetic twins to work would be useful.        - The experiment section could use more clarification. In the synthetic experiment, the paper compares against a number of benchmarks (including robust synthetic control ) for model performance. However, to show that the method uses fewer contributors, the paper only compared against synthetic control, even though robust synthetic control is a more natural comparator, as it explicitly isolates a small subset of contributors during its data pre-processing.        - In the real world dataset, 1) while the train. test split is a common practice in ML, it doesn't generalize to inference. Namely, when the estimand is the ATT, we should not use just 1/3 of the data. 2) It is unclear how the ATT is measured.  Is it the average difference at the end of the trial? If so, it may not be a good metric to evaluate the time series model, since it's only the difference in the point estimate.     - minor details        - in figure 2, is there a missing arrow between the treatment and the outcome? ## Summary: The authors develop a method to estimate the effect of a static treatment on outcomes over time under temporal confounding, with an emphasis on making the method transparent. The authors define transparency as (1) the ability to explain the estimate for a given individual as a weighted compination of a small number of other individuals, and (2) trustworthiness meaning the model should be able to identify cases for which it cannot give a reliable estimate due to a violation of one of the main causality assumptions. ## Main comments:I would significantly increase the rating if the authors address the following comments.  (1) The authors tackle an interesting challenge. I am a bit concerned though that it trivially reduces to ITE estimation with a high dimensional outcome. It seems that the main novelty here is that the authors use a specific architecture that allows for varying lengths of pre-treatment variables, and hence confounders. My concrete questions are (a) suppose for a second that the outcome is measured for a single time period, why does this problem not reduce to the typical ITE estimation? Conditional on all the time varying confounders would simply be equivalent to the typical ITE estimation with a high dimensional set of pre-treatment variables. (2) The authors only model y(0) for individuals who received the treatment, and define the ITE to be the difference between the observed outcome and \hat{y}(0), and similarly for control individuals. This means that in order to use this model i.e., in order to obtain the ITE estimate for a new test patient, we need to first observe the outcome under t = 1 or t = 0. This makes the model not useful for making treatment decisions: it would only be useful to validated that some chosen treatment choice was good/bad. Can the authors clarify what the intended use for this model would be?(3) d_i^y is simply a test of whether or not we're able to model the outcomes under t = 0. I do not follow the logical jump from "there is a large estimation error" to there is additional hidden confounding or the data generating model is not right. There are several other things that could cause that (e.g., the model can be well specified, and no hidden confounding but errors arise due to finite sample analysis). To be more concrete, equation 15 in A.1.2 assumes that (a) the SEM is correctly defined. (b) Q, U are correctEither of these might be violated in practice. In fact, they will likely be violated in practice. The flip side is true, one can get a small estimation error and still have hidden confounding. In general, the assumption of no hidden confounders is statistically untestable (meaning it is impossible to design a statistical test that will answer this question). Can the authors highlight what I am missing here?## Minor comments: Addressing these would be helpful but not consequential for the rating(1) The authors make a statement that in general the error in ITE should be bigger than the error in y --> this is not true, for example it does not hold in situations where there is a large bias in opposite directions for estimates in \hat{y}(1), and \hat{y}(0). My comment here assumes the authors are operating in the "usual" setting where we don't observe the treatment and hence the outcome under treatment for a new test case, and need to estimate the ITE by estimating both potential outcomes. (2) In eq 5 m_{is} is not defined anywhere(3) The authors don't make a clear case (empirically or in the writing) as to why their chosen approach is transparent. This b vector can still be very high dimensional even with an imposed sparsity. In a sense, all what the authors do with this b vector can be done using a simple kernel approach. Would the authors be able to explicitly state how is their model uniquely equipped for transparency?(4) It is somewhat odd to have a causal graph where there is no arrow between the treatment and the outcome. After doing a bit of mental gymnastics, I arrived at the conclusion that this graph is assuming do t= 0, which makes sense but is highly unconventional, and confusing. It would be helpful if the authors follow the conventional notation for causal graphs, which does not condition on a particular treatment assignment (e.g., see graphs in publications by Pearl, Eric Tchetgen Tchetgen, Ilya Shpitser, Tyler VanderWeele)  This work proposes an efficient graph neural architecture search to address the problem of automatically designing GNN architecture for any graph-based task. Comparing with the existing NAS approaches for GNNs, the authors improves the search efficiency from the following three components: (1) a slim search space only consisting of the node aggregator, layer aggregator and skip connection; (2) a one-shot search algorithm, which is proposed in the previous NAS work; and (3) a transfer learning strategy, which searches architectures for large graphs via sampling proxy graphs. However, the current performance improvement over the human-designed models is marginal, which diminishes their research contribution.The paper organization is clear, but some expressions should be improved. The details are listed as below.Typos: In the Abstract, state-of-the-art should be abbreviated as SOTA, not SOAT.Typos: $L_\theta(Z)$ after Equation (4) is not defined. Should it be $L_W (Z)$ as used in Equation (4)?Clarity: The explanation before Equation (5) is a bit confused, which should be re-organized. There is grammar error (the absence of sentence subject) in the first sentence: however, in this work, to make use of the differentiable nature of L¸(Z), and design a differentiable search method to optimize Eq. (4).Clarity: The notations related to variable $Z_{i, j}$, i.e., $Z_{i, j}^T$ and $Z_{i, j}^k$,  are not defined well. What is the difference between the super-scripts: T and k?The pros of this work are summarized in terms of three components used in EGAN, which improves the search efficiency. The experiment results show that their framework greatly reduce time, comparing with the GraphNAS, Bayesian search and random search.Major questions:(1) In Introduction: we doubt that designing proper GNN architectures will take tedious efforts. As far as I know, the architecture parameters of the human-designed models do not require extensive tuning efforts on the testing benchmark datasets. Furthermore, most of the architecture parameters could be shared and used among the testing datasets to achieve the competitive performances.(2) It is unclear for the second challenge: the one-shot methods cannot be directly applied to the aforementioned dummy search space. There are some one-shot models with the parameter sharing strategy used for searching the hidden embedding size. (3) In Section 3.1, why is the dummy search space very large? The search space seems only to include the aggregators and hidden dimensions. It might be much smaller than the search space of CNNs.(4) Their search space assigns skip connections between the intermediate layers and the final layer, which is contradictory to the common case where the skip connections could be applied among the intermediate layers. As shown in [1], the skip connections may exist between any two layers. Could you provide reasons on the design of skip connection limitation? (5) In the node and graph classification of the experimental section, the performance improvement over the human-designed is marginal. This would not justify the motivation of applying NAS to search graph neural networks. The authors should provide more discussions on the contribution of this work in terms of research and industrial applications. (6) The marginal performance improvement might result from the search space. Currently, the authors search space is based on the traditional message passing approaches. They should consider more the recent developments in GNNs to further improve the performance. (7) The selection of baselines is unfair. The search space contains the skip connection components based on the JK-Network. However, authors excluded the important baseline in [2], which could achieve the comparable performance on dataset Citeseer and Reddit. For the graph classification task, authors also excluded a lot of pooling methods, such as the Graph-u-Net [3], which achieves the better performance than the proposed approach.[1] Rong, Yu, et al. "Dropedge: Towards deep graph convolutional networks on node classification." International Conference on Learning Representations. 2019.[2] Xu, Keyulu, et al. "Representation learning on graphs with jumping knowledge networks." arXiv preprint arXiv:1806.03536 (2018).[3] Gao, Hongyang, and Shuiwang Ji. "Graph u-nets." arXiv preprint arXiv:1905.05178 (2019) The paper proposes a hybrid imitation learning/reinforcement learning method for learning hierarchical policies where the top layer provides sub-goals and desired cumulative rewards and the bottom layer learns to meet these goals. The advantage of such a decomposition is interpretability of the learned policy. The algorithm is evaluated on MountainCar and LunarLander from OpenAIs gym. The authors show that their imitation learning/RL scheme is able to solve both tasks while producing reasonable sub-goals. I found the sub-goal for interpretability idea very interesting. However the paper lacks in clarity when presenting the algorithm and in depth when evaluating it. For the presentation, the algorithm box should be commented and its main components should be explained in the main paper. Since most of the learning is carried away by HER (which is not even properly referenced in the algorithm box), a background section should be added. The paper should aim to be more self-contained. The idea of adding the cumulative rewards as a goal is easy to grasp and its explanation can be shortened, leaving space for a more thorough explanation of the actual learning of the hierarchical policy. For the experiments part, important details are left out. For instance what is the ratio of imitation learning and reinforcement learning? How much imitation learning is needed for learning the task? How is the black-box policy obtained? What is its performance? To which extent the low level policy actually depends on the goals given by the higher level policy? Since imitation learning is involved for both layers, one could imagine that the top layer is learning to produce reasonable goals, the lower layer is learning to produce reasonable policies but there is only a weak connection between the two, and the lower level is only giving the illusion of tracking the top levels goals.Im also unsure about some of the motivations. It is stated in the introduction that the drawback of black-box policies is that they can have surprising behaviors when facing unexpected states. How is the proposed policy immune to this phenomenon? Finally, the number of intermediary goals should be discussed somewhere. In robotics, it is common to define the policy as generating a trajectory in joint space and to let a lower level position controller track the trajectory. One can also easily modify OpenAIs Mujoco tasks to be position controlled and not torque controlled. Would a policy defined in this MDP, where an action is a desired joint configuration of the next state, be de facto interpretable, or is there some constraint in keeping the number of subgoals small to be human readable? Overall, I like the direction and I think the plots showing the produced subgoals are very encouraging. However many details of the algorithm are left out and the experiments are not thorough enough to be fully convincing. The manuscript proposes (1) HAC-General with Teacher (GT), an extension of Hindsight Actor-Critic (HAC) that incorporates the environment reward as part of the goal formulation, and (2) "goal-based explanations", a framework in which the agent is tasked to produce intermediate goal states.Good things:+ The problem setting is important and interesting, and the paper lists a good number of applications where adding interpretability to policies would improve their applications to impactful real-life settings.+ The idea of extending HAC to goal-less environments is also interesting, as it could potentially shine a method for systematically utilise HRL algorithms on standard RL environments.Concerns:1. The paper overall doesn't flow well, which makes it difficult to evaluate the novelty and quality of the method.2. The concept of "goal-based explanations" is confusing. Most modern HRL work assumes the same problem decomposition, that that the agent needs to learn a goal-proposing policy (which might give additional signals such as rewards, embedding of the state, etc) and a acting policy that proposes environment-compatible actions. Thus it is unclear how this setup differs particularly from others.3. The paper doesn't do a good job at explaining how looking at a series of goals (which in this case are just states) may be interpreted, and how these would correspond to "explanations". Note that I do not wish to be nitpickinging about this particular naming choice, but "explanations" almost raises the expectation that the end user would see some sort of dense (and possibly language-based) description of the policy.4. GT builds on HAC, but HAC is never properly introduced. Section 3 can be mostly summarised as informing the reader that (a) HAC requires a hand-tuned goal proposal function, (b) that it doesn't utilise the environment reward (but it is not clarified why this would be a problem), and (c) that it has something to do with the problem of "non-stationarity". So, while section 3.1 does attempt to define then the differences between HAC and GT, it is extremely difficult to understand GT without clarifying how the manuscript intends HAC to look like.5. The terminology used in Section 3 is extremely confusing. What is a "goal/action" (or "action/goal")? What does it mean for a policy to reach it? What is the difference between a normal action and a "hindsight action"? It would be good if all of these things were properly defined, and not left only to graphical form in figures 2, 3, and 4 (which all look extremely similar, and not that clarifying).6. Section 3.2 declares that the goal of the work is to "create an explainable agent", and thus it is fair to use a pretrained teacher policy. However, (a) HAC does not require such a training setup, so it's unclear how to fairly assess whether GT is an improvement over it, and (b) it brings the method much closer to imitation learning, which the paper does not review at all. There are in fact methods that distill this sort of knowledge from teachers (e.g. https://arxiv.org/abs/1803.03835, https://arxiv.org/abs/1511.06295 https://arxiv.org/abs/2002.08037), and the proposed method looks extremely similar to them. It is also unclear how such a teacher policy would be trained to begin with, and how GT deals with e.g. probable sub-optimality of this teacher.7. The experimental setting can be greatly improved. The paper proposes to test GT against HAC on mountain car and lunar lander, but (a) both environments are extremely easy to solve, and (b the difference in performance is probably due to HAC being presented an extremely sparse reward setting.Due to these concerns, I currently cannot recommend acceptance, however I'd be willing to chance my score if the authors were to improve the manuscript by:- Providing a detailed explanation of HAC, and how GT fundamentally _improves_ it towards achieving better interpretability.- Providing a comparison (at least narrative-wise) against modern imitation learning literature.- Improve the experimental setup by:  a. At least having another modern RL algorithm as a baseline;  b. Providing a fairer adjustment to HAC (or at least making an argument on how it is currently fair);  c. Attempting a less trivial environment. SummaryThis paper proposes a hierarchical reinforcement learning algorithm in an attempt to improve the explainability of RL agents via the goals proposed by the higher level policy.  Strengths The problem of designing safer and more transparent RL agents is an important and rather neglected one, so it is nice to see papers on this topic.  Weaknesses However, I think the paper makes some unsubstantiated claims, lacks thorough empirical evaluations, and is not very well-motivated and situated in the broader RL literature. First of all, the proposed approach is only evaluated on two very simple tasks (Mountain Car and Lunar Lander) and it is only compared with two ablations of the proposed method. At the very minimum, the paper should include comparisons with a strong RL baseline (i.e. SAC, PPO etc.), a strong HRL baseline (i.e. feudal networks), and a behavioral cloning baseline (i.e. vanilla BC, Dagger, or GAIL) since the algorithm includes elements of all these. I would be quite surprised if SAC and BC dont achieve similar or much better performance than the proposed methods on these simple environments. Given that you have access to an expert, it is really unclear to me why you would want to use your method versus simple behavioral cloning. There needs to be better  motivation. I also do not think the choice of environments is well-motivated. Besides the fact that they are quite simple for current SOTA, they do not strike me as particularly good for emphasizing explainability or for using a hierarchical agent (since they can be easily solved without HRL). I also dont understand how the goals are useful in this particular case. Given that these are fixed environments, once youve trained a RL policy you can accurately predict the agents trajectory including all the visited states rather than only some of them (as the proposed method does). I think a more interesting use case would be to test the method on a new environment and show the goals selected by the higher level policy since a typical RL method wouldnt be able to predict what states it will visit in a new environment without a simulator or running the policy. I suggest using environments that are better suited for your goals and approach and where other methods might fail or would be hard to explain. In section 4.1 you make claims regarding the resistance to small errors and to dynamics / stochasticity which are not supported by any experiments or theory. I suggest either removing them or backing them up with some empirical evidence. Finally, I think the core idea of the paper needs to be better motivated. After reading the paper, I am not fully convinced that the proposed architecture to generate goal-based explanations is very useful for understanding and supervising an agents behavior. If you consider the environment in which you are training, then I dont see how predicting goals is better than simply using a standard RL to predict actions and use the simulator to predict future states. If you consider a new environment, I dont see how the proposed method can generalize if the state space is different so it wont know what goals to output or its predictions will be off. Also, what is the level of granularity for generating the goals (i.e how often the higher level policy acts relative to the lower level policy) and is there a way to tune this? This seems like an important design choice which is not discussed very much in the paper. RecommendationGiven the above points, I do not think the paper is ready for publication.  This paper introduces a new online convex optimization algorithm that operates in via the reduction to online linear optimization in which the regret is bounded by $\sum_{t=1}^T \langle g_t, x_t - u\rangle$ where $g_t$ is the gradient of the t^th loss at $x_t$. The algorithm is based on online mirror descent with non-decreasing quadratic regularizers $x^\top H_t x$, using the update $x_{t+1} = argmin_x \langle g_t, x \rangle + (x-x_t)^\top H_t(x-x_t)$ (or, equivalently using the terminology in the paper, $argmin \langle g_t, x_t\rangle /\sqrt(t) + (x-x_t)^\top H_t(x-x_t)$ where we replace $H_t$ by $H_t/\sqrt{t}$. The analysis is restricted to diagonal $H_t$, for which we can break the regret into a sum of $d$ 1-dimensional problems, so it suffices to do the analysis in the scalar case. The idea is to break out the standard analysis of mirror descent regret as the sum over all t of $D^2(H_t - H_{t-1}) + g_t^2 H_t^{-1}$, where $D$ is the $\ell_\infty$ diameter of the domain, and then choose $H_t$ to minimize each of these terms greedily subject to the non-decreasing condition. A regret bound is provided for this algorithm that achieves worst-case $\sqrt{T}$ regret, but in cases in which the gradients are small, the regret is much better. By employing this strategy on a per-coordinate basis one can obtain an adagrad-esque regret bound.As far as I can tell, the algorithm is equivalent to the update:$$x_{t+1} = argmin_x \langle g_t, x\rangle + (x-x_t)^2 max_{t \le t} \sqrt{t}|g_t|$$Where I remove the composite term and consider 1-d problems for simplicity.The analysis seems correct here, and the idea is a good approach. However, I am a bit concerned about the quality of the theoretical results obtained. In particular, it is extremely unclear to me that the main regret bound in Theorem 4.1 actually offers any advantage whatsoever over AdaGrad. In the paragraphs following the result, the authors offer the example in which $g_t = O(1/\sqrt{t})$. The authors then observe that in this setting, AdaGrad will obtain $O(\sqrt{\log(T)})$ regret. They then also note that Theorem 4.1 obtains regret $\ll \sqrt{T}$. This is true, but my reading of theorem 4.1 is that it will have $O(\log(T))$ regret, which is *worse* than AdaGrad by a $\sqrt{\log(T)}$ factor.I am not sure what is happening in Figure 1- my understanding is that this is plotting the analytical regret bound which seems actually smaller for AdaGrad using the provided example. I suspect the learning rates for adagrad are not tuned properly, but perhaps I am missing something.I have been unable to conceive of any sequence of gradients in which theorem 4.1 actually outperforms AdaGrad, and it is very easy to find sequences in which it does much worse (e.g. simply reverse the sequence of gradients in the provided example). It is, however, plausible to me that in reasonable settings in which the gradients decrease over time one might expect the bound to be within a constant (or maybe a log factor) of AdaGrads bound.Looking at the analysis, I suspect that this is a fundamental issue with the approach of bounding bregman divergences based on the diameter of the domain. Once we commit to this, it is clear that the final regret bound must be at least on the order of $D^2H_T$ by telescoping sum, since $H_T$ is increasing. Further, the $\sum_t g_t^2H_t^{-1}$ term can be lower-bounded by $\sum g_t^2 H_T^{-1}$. Clearly, the minimizing value for $H_T$ here is then $D\sqrt{\sum g_t^2}$ to yield a bound of O(D \sqrt{\sum_t g_t^2}), which is exactly what AdaGrad does.I am willing to believe that the analysis can be improved here, but I am pretty sure that one cannot due so by bounding the Bregman divergences with the diameter. This leads to a pleasant telescoping sum, but I think AdaGrad may already optimize that style of analysis.As for the empirical results, these seem a bit more promising so perhaps there is some improved analysis that could be made. There could be a few clarifications here though: is the value for $\alpha_t$ set to $\alpha/\sqrt{t}$ in these experiments, or is it tuned via some other schedule? In analysis of Adam and AMSGrad, the authors typically use the $\alpha/\sqrt{t}$ approach, but in practice I think this is not usually employed. If $\alpha_t$ is set via some other schedule, and momentum is used as well, then it becomes unclear if the $\alpha$ tuning and the momentum is not what is providing the gain over AdaGrad rather than other differences.I am less expert in evaluating the significance of the final numbers in the empirical study. They do seem relevant, but frankly I feel that the theoretical discussion is currently dragging the paper down quite a bit. <Paper summary>This paper focuses on the problem of knowledge transfer between deep learning models, with the goal of transferring some of the information contained in a (typically large) teacher model to a smaller student network, improving performance of the latter. In particular, this work proposes to require the sparse representations of the activations of the teacher model to be 'similar' to that of the student model, as way of knowledge transfer. The method is presented and empirically evaluated.<Review summary>This reviewer likes the general motivation of this work, proposing that in some cases it might be better to enforce similarity between activations (or features) in a different domain, and requiring these representations to be sparse under some transformation is natural. While the general idea is appealing, several of the motivating claims are vague and the way these ideas are implemented (e.g. via classification to enforce similarity between vectors) are questionable.<Details comments>Strengths:- the authors study an interest problem.- the proposed method obtains good empirical performance.Weaknesses:- The idea of matching the representations for two different data in order to enforce some similarity (or in this context, 'knowledge transfer') has been extensively used. However, this only makes sense if the dictionaries for one and other case are related (see [1,2,3]). Enforcing the representations to be similar (or, as it's done here, to use the same leading atom) is reasonable when the atoms from one dictionary share some properties (or 'code' for related things) in the other.  In this work, the authors learn dictionaries for the teacher and student networks (Ds and Dt) completely independently, as there's no connection between the atoms in one and other dictionaries. - The authors include an 'image-level labeling' which basically compares the mean value in the approximate features from the teacher network ($\tilde{t}$) to that of the vector of similarities of the student one ($k_s$). 1) I do not see how this is informative of relevant information between models, but more importantly 2) Comparing these two real numbers with a logistic loss makes little (if any) sense to me.- The idea of employing sparse representations for data (in this case, the intermediate representations of networks) is natural. There is a large body of work that the authors seem to ignore. For example: on pg 2 they mention that "[sparse representations learning] were not proposed to be jointly optimized with other objectives". Please see refs [4-6] below for examples of this.- Representations under redundant dictionaries are not unique, and the problem of finding sparse representations is NP-hard (see e.g [7]). Certainly, one can propose relaxations of this problem and even heuristic approximations (see e.g. [8]) but this is never discussed, and it is unclear how the obtained representations in this work fit in this context.Smaller comments:- The proposed method seems to be a pre-processing step: firs train teacher dictionaries, then student dictionaries and student weights, and then employ the KD method from Hinton et al. The authors should consider making this more explicit, perhaps detailing the full algorithm in a formal way.- On page 4, the authors motivate the use of the sigmoid as activation function saying that the 'gradients in the backward pass are stable'. What does this mean? Would they be unstable if a ReLU was used instead (as used in most deep learning models and in the other models in this work)?- At the end of Pixel-level labeling, stating the definition for $c_{n,i,j}$, the authors take the argmax over m, but there's no m in the expression.- 'Relaxing' the problem of requiring the representations to be similar simply by turning a regression problem into a classification problem seems unfounded: why would the latter be easier than simply allowing the representations to be similar (say, with small L2 norm)?- In defining the similarity kernel $\kappa$, I believe the $x$ and $y$ should be bold according to the authors notation.- On a subjective note, the notation is not standard and thus a bit confusing: calligraphic capital letters usually denote sets or distributions, whereas here they denote vectors (as do bold non-calligraphic letters).References:1] Wang, Shenlong, et al. "Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis." 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012.2] Qiu, Qiang, et al. "Domain adaptive dictionary learning." European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2012.3] Peleg, Tomer, and Michael Elad. "A statistical prediction model based on sparse representations for single image super-resolution." IEEE transactions on image processing 23.6 (2014): 2569-2582.4] Mairal, Julien, Francis Bach, and Jean Ponce. "Task-driven dictionary learning." IEEE transactions on pattern analysis and machine intelligence 34.4 (2011): 791-804.5] Sprechmann, Pablo, Alexander M. Bronstein, and Guillermo Sapiro. "Learning efficient sparse and low rank models." IEEE transactions on pattern analysis and machine intelligence 37.9 (2015): 1821-1833.6] Monga, Vishal, Yuelong Li, and Yonina C. Eldar. "Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing." arXiv preprint arXiv:1912.10557 (2019).7] Mairal, Julien, Francis Bach, and Jean Ponce. "Sparse modeling for image and vision processing." 8] Makhzani, Alireza, and Brendan Frey. "K-sparse autoencoders." arXiv preprint arXiv:1312.5663 (2013). The manuscript proposes a novel convolutional layer that computes a node embedding by selectively summing the nodes' embeddings neighbors at different hop distances (from 1 to m hops).  The aim of the Teleport graph convolutional layer is to solve the major limitations introduced by using the message-passing paradigm.In the introduction, the authors highlight the main issues related to message passing paradigms and deep GNN models. The authors state that stacking multiple layers involves massive trainable parameters, which consequently increases the risk of over-fitting.  It is important to note that the issue of having several different GNN stacked layers were already faced in some papers: The Graph Neural Network Model by Scarselly et al. (2019), and in Gated Graph Sequence Neural Networks by Li et al. (2016). In these papers, the authors use recurrent models that exploit the weights sharing mechanism in order to limit the number of trainable parameters. Moreover, several works that propose an alternative to the message passing paradigm were published in the last few years. For instance, several convolutional operators are designed to consider a larger receptive field. By exploiting power series of the diffusion operator or leveraging on a multi-scale operator:  Diffusion-convolutional neural networks, Atwood et al. (2016), LanczosNet: Multi-scale deep graph convolutional networks, Liao et al. (2019), Break the ceiling: Stronger multi-scale deep graph convolutional networks, Luan et al. (2019), SIGN: Scalable Inception Graph Neural Networks, Rossi et al. (2020). Since the Teleport GCN tries to solve the same issues, all these models should be discussed in the related works section, and also considered in the comparison of the experimental results.In this regards note that, even if the idea of using the features-aware or the structure-aware teleport function is interesting, the teleport convolutional layer defined in section 3.2 is not very novel in my opinion, since it seems very close to a multi-scale approach where in the same layer several exponentiations of linear diffusion operator are considered.The main problem of this work is the empirical evaluation of the proposed method. The authors validate the models using an unfair method. Indeed the authors state  The hyper-parameters in TeleGCNs are slightly turned on D&D dataset and are migrated to other datasets with slightly different selections' '. Using similar hyper-parameters tuned in D&D for the other datasets in my opinion is not correct. Note that the other datasets differ significantly from D&D (e.g. PROTEINS has 39.1 nodes vs 284.3; COLLAB has a significantly different number of classes and 5 times the number of graphs; etc.). It is also important to notice that all these datasets require a different type of classification task.Moreover, the method used by the authors differs from the common practice  used in (Xu et al., 2018; Ying et al., 2018; Gao & Ji, 2019; Lee et al., 2019) since in Xu et al., 2018 the authors state: The hyper-parameters we tune for each dataset are: (1) the number of hidden units  {16, 32} for bioinformatics graphs and 64 for social graphs; (2) the batch size  {32, 128}; (3) the dropout ratio  {0, 0.5} after the dense layer (Srivastava et al., 2014); (4) the number of epochs, i.e., a single epoch with the best cross-validation accuracy averaged over the 10 folds was selected. The validation phase has to be executed independently for each dataset, otherwise, the obtained result will be clearly biased. Moreover, note that the impact of the validation phase in evaluating the performance of a model is discussed in A Fair Comparison of Graph Neural Networks for Graph Classification by Errica et al. (ICLR 2019). The results reported in this paper show that performing a fair validation procedure is crucial to evaluate the model's performance.Minor comments:From section 4.2 to section 4.7  it is almost impossible to distinguish between tables' captions and sections text. Overview of the paper: This paper studies three empirically questions about the planning part in Muzero, an algorithm integrating direct learning, model-learning, and planning. These three questions, as written in the paper, are that 1) for what purposes is planning most useful? 2) what design choices in the search procedure contribute most to the learning process 3) does planning assist in generalization across variations of the environment. The paper answers all these three questions using experiments: 1) the major reason for the performance improvement using planning is maintaining a policy that approximates the policy found by the MCTS algorithm, 2) simpler and shallower planning is often as performant as more complex planning, 3)  search at evaluation time only slightly improves zero-shot generalization.Comments:Overall I think the paper is not ready to publish. While the 3 questions asked in the paper are quite general and apply to many model-based algorithms, the paper provided general answers to them using empirical results only for the Muzero algorithm under deterministic environments, which doesn't look appropriate to me. The Muzero algorithm is different from many other planning algorithms, such as Dyna-style planning, MPC, value iteration, etc. And deterministic environments are easy cases compared with stochastic or even partial observable environments. I would suggest the authors rephrase the questions and answers to make them more specific so that the results in the paper can support conclusions well.While there are multiple things making me confused, I would like to highlight the following one as an example because that almost makes me think their answer to their first question is incorrect. The answer is drawn from the results shown in figure 3, which illustrates how important three design choices (following MCTS policy in both training and testing, following MCTS policy in training and prior policy in testing, and following prior policy in both training and testing) are in planning. These results could tell us how much more performance the algorithm achieves by following MCTS policy in training or testing, but they can not tell us how much more performance the algorithm achieves by updating the prior policy towards the MCTS policy compared with other approaches. That is, in all cases, the algorithm updates its prior policy towards the MCTS policy. The other thing I feel not appropriate is, while the paper claimed that "we systematically study the role of planning and its algorithmic design choices in a recent state-of-the-art MBRL algorithm, MuZero", the planning part is not the only part that varies across different design choices. In particular, because the model parameters are learnable and all the variations of algorithms they tested have different updates to model parameters. The resulting learned models are different in these variations. Thus it is not appropriate to conclude that the performance difference between different variations is solely the result of planning. It might also come from differences in learned models. ## SummaryThe authors provide a new analysis of SGD and versions of RMSprop, taking into account possible non-stationarity of the gradient noise. In particular, the authors propose. (i) the convergence analysis of SGD with stepsizes dependent on the second moment of stochastic gradients and a "norm" version of RMSprop (Moment Adaptive SGD) in the convex non-smooth case and(ii) the convergence analysis of SGD with stepsizes dependent on the variances of stochastic gradients and a variance adaptive SGD in the non-convex smooth case with rates of gradient norm decrease.The rates are given in terms of high probability convergence.## Strengths1) The paper addresses an important and fundamental question for understanding why methods based on moment estimation (like RMSprop or Adam) work better in certain regimes.2) The experimental study of gradient noise non-stationarity shows interesting behavior of the gradient noise that can be useful for future research in this area.## Weaknesses1. The theory presented in the paper needs significant further development and refinement: some results are misleading, assumptions are too strong and not motivated well, several crucial statements are left without proof, and some of the presented proofs have inaccuracies/unexplained parts. Below, I provide a list of my major concerns about the paper. 1.1. **Assumptions.** While authors state in the first sentence of the abstract that they analyze stochastic optimization under "weaker assumptions on the distribution of noise than those used in usual analysis," this is not true. In fact, the main algorithms -- Algorithm 1 and Algorithm 2 -- are analyzed under Assumptions 1 and 6. These assumptions are strictly stronger (otherwise, according to Nemirovski and Yudin lower bounds for stochastic non-smooth optimization, it is impossible to get a better rate than SGD has) than the uniformly bounded second moment of the stochastic gradient and uniformly bounded variance of the stochastic gradient assumptions. Moreover, in the non-smooth convex case, which is considered as the main case of the paper, authors assume that $m_k = \mathbb{E}[||g_k||^2]$, where $g_k$ is an conditionally unbiased estimator of $\nabla f(x_k)$, is bounded and *does not depend on $x_k$*. Moreover, the authors do not explicitly state this as an assumption and do not emphasize this fact in the statements of Theorems 1 and 3. It is an important assumption and should be stated explicitly in the statements of these theorems. Next, $m_k$ depends on $x_k$ even when the noise is negligible, i.e., the stochastic gradients are almost gradients. 1.2. **Misleading results.** Unfortunately, some results require further explanations. For example, why can we choose $\eta_k$ dependent on $\sum_{t=1}^Tm_t$ in Corollary 2? It is misleading since $m_t$ for $t > k$ depends on $\eta_k$ in general. Next, it is unclear how can we know $m_k$ or even $\sum_{k=1}^T m_k$ to use these stepsizes? It seems, that in general it is not possible. 1.3. **Results without the proofs.** Some results that play a central role in the paper are left without rigorous proofs and even sketches of the proofs. In particular, the rates from Table 1 are not derived for the noise model considered in Example 1, and the most important results of the paper --- Corollaries 5, 6, and 7 --- are provided without the proofs as well. These results were meant to explain the main advantage of Algorithm 1 in comparison with vanilla SGD, but the proofs for them seem to be not that straightforward and should be added to the paper. 1.4. **Inaccuracies and unexplained parts in the proofs.** I have checked all proofs in detail. While in general, they are sound and mathematically correct, some places in the proofs of the main convergence results for Algorithm 1 and 2 are left without explanation and seem to be inaccurate (see my list with comments below).2. Clarity of the paper should be improved. Some claims require further clarification; see my comments below.3. The paper contains a significant amount of typos and grammatical errors. Here are some of them: - page 4, "The improvement factor ...": has depends $\to$ depends - page 13, "The iterate suboptimality ...": have $\to$ has - numerous missing periods after formulas (e.g., (10) and before inequality (11)) - page 19, "This assumption help us ...": help $\to$ helps - page 19, "In otherword ...": otherword $\to$ other words - page 19, "The through algorithm ...": the sentence is unclear and should be rewritten using academic English - page 19, "With the above assumption ...": algorithm $\to$ Algorithm; achieve $\to$ achieves - page 19, "The above theorem is almost ...": applies $\to$ apply## Questions and Comments1. It would be interesting to see the comparison with recent relevant papers, e.g.,  - Ogaltsov, Aleksandr, et al. "Adaptive gradient descent for convex and non-convex stochastic optimization." arXiv preprint arXiv:1911.08380 (2019). - Défossez, Alexandre, et al. "On the Convergence of Adam and Adagrad." arXiv preprint arXiv:2003.02395 (2020).2. page 2, "For CIFAR 10 ...", "While for language ...": These claims should be supported by relevant references.3. page 3, Definition 1: Should be $\mathbb{E}[g(x_k)\mid x_k] = \nabla f(x_k)$ instead of $\mathbb{E}[g(x_k)] = \nabla f(x_k)$. Do you use full expectations in (a) and (b)?4. page 3, "This is empirically justified ...": To justify this claim, authors should add at least plots comparing $m_k$ and $\sigma_k$ for different optimizers.5. page 3, Theorem 1: One should explicitly state in the theorem that $f$ is convex.6. page 4, Assumption 1: Why $M$ is used to estimate concentration and for bounding $m_k$? It can be the case that concentration is good (e.g., can be achieved via large bathes computed in parallel) while $\max m_k$ is large. Is it possible to get tighter results after the refinement of this assumption? Next, when does this assumption hold for Algorithm 1? It should be stated explicitly in the text with concrete examples of the problems. Why $D^2 = \Omega(M^2)$? The problem should be simpler when $D$ is small, even if $M$ is big. Do you mean $D^2 = O(M^2)$?7. page 5, Remark 4: it seems you mean standard amplification (the term from complexity theory), not restarts having a different meaning in optimization. One can do that, but this will require additional $O(log|\delta|)$ computations of the functional value of $f$ to choose the best point. For example, it is not possible for the general expectation minimization problem, which restricts the applicability of the results proposed in the paper.8. page 6, Corollary 5: The proof of this corollary is vital for the paper. Next, the explanation of why this assumption on $M$ and $m_k$ is reasonable should be added.9. page 6, Corollary 6: The condition $M/m_{avg} \leq T^{1/9}$ should be further explained and motivated. Next, the proof is also vital for the paper.10. page 7, "The convergence results are ...": This is not true since, in the variance oracle case, authors analyze only non-convex smooth problems and the convergence to the stationary points. It would be interesting to see the analysis for the smooth convex case and see what else we can get with smoothness in the convex case. The authors should explicitly write in the main part of the paper what results they have in the appendix. In the current form, this description is misleading.11. Experiments: How $m_k$ and $\sigma_k$ were estimated? By definition, these parameters are the full expectation. Do authors apply Monte-Carlo approximation for these parameters in the experiments? Next, there are two idealized and constant baselines in the text (in Sections 3 and F). What baselines did the authors use in the experiments? Have the authors tried all options? If yes, what were the results?12. page 14, formula for $\hat m_k^2$: Norms are missing for $g_{k-1}, g_{k-2}, \ldots, g_0$.13. page 14, after the formula for $\hat m_k^2$: independence $\to$ conditional independence.14. **page 16, inequality above (11):** It is not clear how this inequality was obtained from the above one and the definition of $m$. This part should be clarified.15. page 16, Remark 9: Why (8) holds in this case? The proof is required.16. page 16, Remark 9: What does this remark imply? It should be either explicitly stated or removed.17. **page 18, inequality above (16):** This part should be explained.18. page 18, application of Markov's inequality: Right-hand side is incorrect.19. page 19, Assumption 6: Why $D^2 = 4M^2$ is needed? $D$ can be significantly smaller in some cases.20. **page 21, the second inequality after (18):** It is not clear how this inequality was obtained from the above one and the definition of $m$. This part should be clarified.## Final remarksTo conclude, the paper focuses on a very important problem but suffers from a number of serious issues described above. Therefore, for me, it is clear that in the current shape, the paper should be rejected. Though there is a chance that the authors will address all my comments, I believe that the paper requires significant improvements and, as a consequence, a new round of reviews. The objective of the paper is to provide a theoretical justification for the value of using adaptive learning steps. The paper presents two results. The first is essentially of theoretical interest, and assumes that the noise level indicators defined in Eq. (1) [but which are difficult to understand at this level of the paper] are known. The second is more practical: it shows that a variant of the RMSprop algorithm achieves the same results as the "theoretical" algorithm.I perfectly understand the definition of the second moment $m^2(x)$ and $\sigma^2(x)$. These quantities depend on the current value of the parameter. This is a feature of the function we are trying to optimize, and this is perfectly clear. The Definition 1 of the non-stationary noise oracle is more far-fetched: "The stochasticity of the problem is  governed by a a sequence of second moment $\{m_k\}$ and variance $\sigma_k^2$". The definition is not even very well written and explained. This is an unconditional exceptation: it means in particular that the expectation is taken wrt to the distribution of the initial value  of the parameter (I guess therefore that the expectation is also taken on the initial condition, which can be concentrated on a point).  Even if we take a  this is very difficult to check, except in the situation in which the "noisy gradient" used in the procedure is the true gradient affected by some additive noise, independent from the current value of the parameter. The authors feel this embrassment in the sentence that their "goal is to demystify the correlation between the noise intensity and the performance of the algorithm" and two lines later "the noise intensity" (what does it mean "intensity" ?) is "decoupled" from the location, in strict contradiction of their earlier definiton of second moment and variance which are, of course, location dependent. We can of course argue that in most analysis the variance (or even the conditional variance) of the gradient is bounded, so that this may appear as a relaxation of the previous assumptions... Theorem 1 is not surprising, same three-lines proof where the bound on the gradient noise variance is simply replaced by a time-dependent variance. They illustrate the result with a very artificial example (Example 1), which substantiate an earier claim (that the adaptive stepsize can achieve a faster rate of convergence by a factor which is polynomial in T). In section 4, the authors start to consider the more interesting "adaptive" scenario. The algorithm is a simplified version of RMSprop in which a "global" scale is applied to the gradient instead of an "component-dependent" factor applied  on each individual component of the gradient.  The authors formulate Assumption 1, which a bit strange because the assumption is algorithm dependent (the unconditional variance of the algorithm after $k$-step depends on the algorithm itself, so we have to guess that Assumption 1 is formulated for the adaptive SGD algorithm... but then the quantities depend upon the constant $m$  and $c$ which are later optimized in the theorem. Everything is written as if the noise variance depends only on the iteration index, independently of the algorithm itself... Given this remark, it is very difficult to understand what theorem 3 is about... This really gives the impression of a snake biting its tail, because the definition of $m_k$ depends on the law of the  iterate and therefore on the SA algorithm, which we then allow ourselves to optimize the parameters of the algorithm but without affecting the law of iteration (because the m_k are always the same). Strange... Of course, the only scenario where this makes sense is when $g_k= \nabla f(x_k) + \sigma^_k Z_k$, i.e. when the gradient noise depends only on the iteration index but not on the current  value of the iterate.  I find the paper interesting, but the conclusions must be rewritten very clearly so as not to mislead the reader. You are in fact studying the stochastic approximation with an error that would have a variance profile, perhaps unknown, and depending on the iteration index but not on the law of iterations. As a result, the conclusions have much less force. ### Summary and ContributionsThe paper proposes a generative model for the motion of surface waves in open and closed geometries. While neural networks have been applied to simulate fluid dynamics before, they are purported to suffer from poor generalization to unseen geometries for long-time predictions. The paper proposes a U-net based model which is trained using a modified loss function incorporating the gradient into the loss function.  The results demonstrate generalization to unseen geometries with good predictions upto 80 time steps in the future. ### Detailed Review The following is the detailed review of the paper, organized into strengths and weaknesses subsections. ### Strengths #### Relevance and Significance The utilization of DNNs for modeling physical phenomena is compelling and gaining steam. In particular, the modeling of spatiotemporal phenomena like unsteady fluid dynamics for complex geometries should be of interest to the ML community.#### Clarity The paper Is written well and is easy to understand. #### Reproducibility Should be reproducible.  ### Weaknesses #### Relation to Prior Art The paper does a reasonable job of presenting the prior art but fails at identifying the unmet need that the presented work fulfills. The claimed speedup over numerical solvers has been achieved before (Guo et al, 2016). Further, DNNs have been used to model surface waves (Fotiadis et al, ICLR Workshop 2020, Sorteberg et al, NeurIPS Workshops, 2018, Kim et al Eurographics 2019,  etc.). (Sorteberg 2018) claim to use LSTM for modeling up to 80 time steps into the future on a dataset not seen during training. It is not clear what advantages the present work is supposed to have over the SOTA and why. #### MethodologyThe proposed approach is a straightforward application of U-net to predict a spatial field given past few spatial fields (stacked together). However, U-Nets, LSTMs, conv-LSTMs and other architectures have been tried before. It is unclear what the novel contribution in this paper is (gradient augmented loss function?) and why it would be instrumental in handling unseen geometries over longer periods of time.#### Novelty The presented approach is a straight-forward application of known techniques. Further, these approaches have been tried before (see prior art) #### Empirical Evaluation There is no evaluation against the state of the art. It is important to compare the performance against (Fotiadis et al, ICLR 2020 Workshops),  (Sorteberg et al, NeurIPS Workshops, 2018), (Kim et al Eurographics 2019),  etc. ### Assessment Though the problem seems relevant and of significance to the research community, the paper suffers from a lack of novelty and a non-existent comparison against the state of the art. It fails to identify the unmet needs it is addressing and what novel contributions allows them to achieve this. I do not recommend the publication of this paper.  This paper studies the relationship of correlation of ranking of networks sampled from SuperNet and that of stand-alone networks under various settings. They also study the how masking some operations in the search space and different ways of training effect the ranking correlation.Pros:The paper has a lot of experiments to substantiate the claims.Figure 3 where every operation is systematically masked, provides more insights about which operations are effective and how NAS behaves if one of the operation is masked.Cons:Several other papers have already published similar findings. Overall the paper is very incremental.More specifics in the questionsQuestions1. How is the SuperNet trained?2. Figure2: Yu et al [1] have already explored the correlation of ranks of networks sampled from SuperNet and that of stand-alone networks. How is Figure 2 different from that? 3. RobustDarts [2] has explored the possibility of how subset of NASBENCH search spaces behave. FAIRDarts [3] also explored the influence of skip connection by running DARTS without skip connection, running random search by limiting skip connection to 2 etc. Figure 4 seems to be inspired by that. While it is interesting, this might be a slight extension to the work done by Yu et al [1]4. Bender et al [4] postulate that the operations of a SuperNet are subject to co-adaptation and recommended techniques such as regularization, drop path etc to alleviate the same. RobustDarts also suggest some recommendations such as L2 regularization, drop path etc although in the context of DARTS. So while Figure 6 demonstrates this empirically, it is not a new finding.Overall, the empirical results in the paper are very useful for the NAS community. But the work is still very incremental. This might be better received as a workshop paper instead. The paper investigates the double descent phenomenon. It proposes the augmentation of the dataset via concatenating the covariate x and interpolating the label y, which increases the data size from n to n^2. The paper shows that the phenomenon of double descent can be mitigated via augmenting the input. The idea of investigating double descent from manipulating samples is novel and interesting.However, the paper is quite poorly-written. For example, I can't tell what the yellow and blue lines in Figure 2 and later figures represent, what is the difference between loss and error (which is never defined in the paper), and how the figures are generated from the concatenated data. The whole experimental setup is unclear to me. The authors are suggested to provide a concrete introduction and experimental details for it.Furthermore, the paper does not provide enough reasoning how simply concatenating the x and averaging y can mitigate the double descent phenomenon. First of all, are we supposed to train on the augmented data instead of the original data? Assume that we are running on completely synthetic data where y has a non-linear dependence with x (e.g. in the synthetic data y is the output of 3 layer neural network), then it seems that the pair ([x1,x2], (y1+y2)/2) will only hurt the performance since it adds extra complexity (one more linear transformation on the data) to really fit the model. It is also surprising why before and after concatenation, the error can be that much different with the same network width in Figure 4. The author mentioned at the end of Section 4 that one reason could be that the variance is smoothed due to implicit regularization from concatenation. Is it the case for all the experiments conducted above?The authors are suggested to analyze the effect of concatenation on double descent thoroughly (e.g. analyze the bias and variance separately in the linear regression case to show why the double descent phenomenon is mitigated), provide convincing reasoning and more experiment details. This paper focuses on the problem of Neural Text Degenerationwhere text sampled from a language model can either be too repetitive and bland or too random and nonsensical. The authors focus largely on the former problem, proposing a finetuning loss that specifically incentivizes the use of tokens that have not yet been decoded in the given document. The authors test whether this improves repetition and unique token coverage with greedy decoding in open-ended generation. A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT). Similarly good results are obtained on Image Captioning with and without trigram repetition blocking. On Abstractive Summarization BeamSearch is used and again outperform MLE and UT. Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited. The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad. Multiple hyperparameter settings are shown, with  some analysis on how gamma can be chosen to get a desired behavior. Finally, the authors analyze why UT may not be as effective: it penalizes gold repetitions too much and does little for other tokens.Strength:- The results are good for greedy decoding- The method is well motivated and well explained- The analysis regarding Unlikelihood Training is interestingWeaknesses-  The results shown do not make proper comparisons across models, baselines, and hyperaparameters- Image Paragraph Generation uses greedy decoding, but in practice BeamSearch or stochastic decoding would likely be used.- Results for stochastic decoding should have been shown across tasks.- Despite citing the need for awkward rules such as trigram repetition blocking as a reason to propose ScaleGrad, trigram repetition blocking still helps significantly.- Many details are hidden away in the appendix, which I had to read thoroughly in order to fully understand the comparison.I recommend to reject this paper, because the experimental comparisons made are simply too incomplete.The results in Table 1which show the main metrics of interest on open-ended generationare missing two key points of comparison: ScaleGrad is only show with gamma=0.2, even though gamma=0.5 & gamma=0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings. This is despite the fact that two hyperparameter options for Unlikelihood Training are shown. In a footnote on page 6, for directed generation, the authors state Although UL was originally proposed for open-ended generation, it is applicable to directed generation. We did  the same scale hyper-parameter search for UL. Details can be seen in Appendix E. However, in Appendix E two hyperparameter settings for alpha are shown, the same two as used in Table 1, but two hyperparameter settings for gamma in ScaleGrad are shown _neither of which are shown in Table 1_ nor are repetition or uniqueness numbers shown for these hyperparameters settings anywhere in the paper or the appendices. This makes me question whether the improvements shown in Table 1 hold across hyperparameter settings as the authors claim in their analysis of Figure 1.However, Figure 1 is missing necessary data points and comparison. First of all gamma=0.2 is not shown, though at least gamma=0.1,0.3 are so it can be somewhat inferred. That is suboptimal, but this graph does not even go up to gamma=0.8, which is what is used in the Abstractive Text Summarization experiment! Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1. Luckily, Figure 1 (c) can be compared, and if cross-referenced with Table 1, shows that Unlikelihood Training does better than ScaleGrad with a higher gamma. However, Figure 1 has no data on either Unlikelihood Training or a human baseline. It really should not be necessary to go looking through Table 1, Figure 1, and Appendix F to see that Unlikelihood Training is outperforming ScaleGrad on some metrics. Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.On page 4 the authors write Following Welleck et al. (2020), we apply greedy decoding in our experiments in this section. This allows us to evaluate the modeling capability exclusively. We will get into the matter of comparison to Welleck et al. 2020, but I would like to begin by addressing whether Greedy Decoding is a neutral choice that only tests modeling capability, because it is clearly not. There is a spectrum of generation algorithms between probability maximization and straight-forward sampling. Greedy is closer to probability maximization, but it only maximizes local probabilities and inevitably comes-up with lower probability outputs than Beam Search or Bound & Branch (Stahlberg & Byrne, 2019). Welleck et al. 2020 show that Greedy Decoding results in better text along their proposed metrics for open-ended generation. There is no reason to believe that this is the case for Image Paragraph Captioning, nor is Greedy Decoding a neutral choice that should be considered the default. Since Greedy Decoding is not a neutral choice, I do not believe it is appropriate to exclude stochastic decoding baselines from the given comparisons. Stochastic decoding algorithms such as sampling, top-k sampling, and Nucleus Sampling usually do very well on repetition and uniqueness metrics. Indeed, they can be seen to outperform all the other models on Table 16 in Appendix H. If the goal is to beat them without stochasticity, that is a perfectly appropriate goal, but simply not comparing to such algorithms makes it look like achieving these results was impossible before ScaleGrad. This is simply not true.In the analysis section, tables are quite limited in their coverage. In Table 6 no comparisons are made to systems that have not been trained with ScaleGrad, and these algorithms were not reported on in Table 1 so no comparison can properly be made even if the reader goes searching for the data.  In Table 8, Unlikelihood Training is not included in the comparison even though it does very similarly to ScaleGrad on the same task in Table 5. Finally, Table 5 shows that trigram blocking still helps significantly on ScaleGrad trained systems. This is understandable, but disappointing since getting rid of these kind of rules is described as the reason for proposing ScaleGrad.Altogether, I feel the comparisons made in this paper are incomplete. ## Summary of the paperThis paper proposes an improved method to calculate an upper bound of the spectral norm of the 2d-convolutional layer. The advantage of the proposed method is fast computation and easy gradient computation. Experiments on MNIST and CIFAR10 show that regularizing spectral norms using the proposed method improves generalization. Additionally, they showed that their method could extend and improve an existing method for computing certified robustness accuracy.## Review### SummaryThe paper is clearly written and easy to understand. However, the paper misses important literature and also seems to misunderstand existing methods. And I found the proposed method does not have the claimed advantages over existing methods.### More detailed comments1.  Misunderstandings of the existing methodIf I understand it correctly, Ryu et al. do not directly calculate the singular vectors' outer-product. The spectral norm calculation written in Ryu et al. is differentiable. And thus, you can directly take the gradient of the spectral norm, and major deep learning frameworks efficiently compute its gradients.2. Missing literature> this is the first work that derives a provable bound on the spectral norm of convolution filter as a constant factor (dependant on filter sizes, but not filter weights) times the heuristic of Miyato et al. (2018)It's at least mentioned in Cisse et al. (2017).Additionally, efficient and differentiable computation of linear operators appeared in the literature repeatedly (Tsuzuku et al. 2018, Scaman & Virmaux 2018). They are not limited to the 2d convolutional layers, but any linear layers implemented using deep learning frameworks. Tsuzuku et al. 2018 also applied the method to improve certified robustness accuracy. I was not convinced that the proposed method has more advantages over these methods.3. Weak experimentsThe experimental results seem noisy, and also the gain by the proposed regularizer looks marginal. Please write the number of each run and their standard deviations. Additionally, I am not convinced that the tighter bound results in better generalization. The proposed method is very similar to Yoshida & Miyato 2017, and it is not intuitive that the proposed method works better than it. Please add appropriate experimental comparisons with Yoshida & Miyato 2017. ## ReferencesCisse et al. Parseval networks: Improving robustness to adversarial examples. ICML 2017.Scaman and Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient estimation. NeurIPS 2018.Tsuzuku et al. Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. NeurIPS 2018Yoshida and Miyato, Spectral norm regularization for improving the generalizability of deep learning. ArXiv, abs/1705.10941, 2017. Summary:The paper proposes using the largest eigenvalue of the Fisher information metric (previously studied to identify adversarial perturbations in computer vision) as a measure to construct "difficult examples" for the IMDb sentiment analysis task. The authors suggest that one may perform such perturbations by replacing "important" words in the reviews. They further analyze those examples comparing them to counterfactually revised data (CRD; Kaushik et. al, 2020) and contrast sets (Gardner et al., 2020) and show that based on the FIM criteria, CRD and contrast sets are not too different from original reviews in the IMDb dataset.Pros:- This paper describes a simple method for understanding difficulty in terms of ease-of-perturbation.- The authors find that, interestingly, sentiment analysis datasets generated by counterfactually revising data are not "difficult" for classifiers.Cons:- While the idea of using FIM to identify examples closer to a classifiers decision boundary is interesting, it is a new application to NLP even though the idea has been discussed extensively in Zhao et al. (2019). However, the paper doesn't show why these examples may be useful. The paper reports no empirical results or analysis thereof to understand the practical efficacy of these "difficult examples" while discussing only the changes in eigenvalues.- It is not clear as to how generalizable this approach is. For instance, the paper uses a replacement strategy based on substituting synonyms, antonyms, and certain kinds of noun phrases (actress names etc.)  which may not directly translate to other tasks such as question answering, news classification, VQA, or even NLI. - The paper makes many philosophical claims without theoretical or empirical justification. At one point it suggests that FIM captures resilience to linguistic perturbations but provides no backing for the claim. The paper further suggests that evaluation sets should increase FIM but provides no theoretical/empirical justification for the same. At another instance it says, [t]ransfer learned models like BERT capture rich semantic structure & and tend to rely on semantically relevant words for classifying movie reviews without citing any prior work that supports this claim.- The approach is also very model specific. For one model some data points may turn out to be not "difficult" but for a different model they may, which casts further doubt on the general applicability of this method in this setting. Note Zhao et al use it to identify adversarial examples, which by definition are specific to a model. Furthermore, if we go by the authors' suggestion that evaluation sets must increase FIM, then by using such "difficult" examples for evaluation we end up discriminating (by design) against the model that was used to identify them, and unintentionally favor other models, leading to a potentially flawed comparison. I don't think that it is a desirable characteristic of an evaluation set.- Particularly given the lack of empirical evidence showing the benefits of "difficult examples" identified by this approach, the core claim of the paper is that examples generated by prior work are not "difficult" in terms of the discussed metric and perhaps they need to be so. However, the motivation expressed in Kaushik et al. is not to create "difficult" examples for training or evaluation, but to learn better models by intervening on causal variables of interest. A good counterfactual example generated by their approach would be one that modifies only sentiment-related variables and leave others intact (intervene on causal variables to d-separate labels from the spuriously correlated variables, see [1] for a detailed explanation), and it has no relation to any particular classifier. I agree that Gardner et al. do discuss their motivation as to construct examples closer to the decision boundary.- The writing can be significantly improved.Additional comments:- It is also not clear to me how replacements are sampled, it would be good to provide details on that.- It would be nice to see how well models perform in-sample and out of domain when trained on "difficult examples" and how models trained on CAD perform on previously identified "difficult examples" in test sets. Furthermore, are there overlaps between such examples if FIM is calculated with respect to a model trained on original data alone vs. one trained on CAD?Missing references:- Please cite the IMDb dataset: "Maas, Andrew, et al. Learning Word Vectors for Sentiment Analysis. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2011." - Prior work building on Gardner et al. and Kaushik et al. demonstrating the efficacy of their work: Teney, D., Abbasnedjad, E., & Hengel, A. V. D. (2020). Learning what makes a difference from counterfactual examples and gradient supervision. arXiv preprint arXiv:2004.09034.- There are also a lot of arxiv citations for papers that have been peer reviewed.Errata:- Page 2, line 3: BERTby -> BERT by- Page 2, second paragraph, line 1: Fisher information metric has already been defined in the previous paragraph so you may refer to it just as FIM over here.- Page 3: Glockner et al. should be \citep and not \citet. Same for Rajpurkar et al., Recht et al., and Beery et al.- Page 3: learning byMitchell et al. (2018) -> learning by Mitchell et al. (2018)[1] Kaushik, D., Setlur, A., Hovy, E., & Lipton, Z. C. (2020). Explaining The Efficacy of Counterfactually-Augmented Data. arXiv preprint arXiv:2010.02114. ##########################The paper proposes an algorithm and an analysis of its convergence.The algorithm propose to learn a model of temporal data y_1 , ... , y_T given input x_1, ..., x_TThe observations are assumed to arise from a deterministic latent h governed by a piecewise continuous ode (in between consecutive times t_i, t_i+1)with additional deterministic jumps at transitions.In the ODE-RNN paper, the latent h can be expressed as a single ODE for the whole time horizon (rewriting thejump with a skip transition).This paper appears to me as taking this expression and choosing a particular bounded form for thedynamics, jump and readout functionsA statistical asymptotic analysis of the convergence of the algorithms, for random times and inputs is given. ##########################Methodology:I find the paper quite difficult to read, I blame both its structure and my lack of ease with the mathematics used here.However from what I have understood of the algorithm proposed, I find the methodological contribution very limited.Clarity:I come from the machine learning community and read with no difficulty papers cited in the related work section.In comparison, I find this paper extremely difficult to read and parse despite containing the same kind of information.Asymptotic analysis.I leave to other reviewers the evaluation of the convergence analysis.My evaluation being partial, my confidence rating is set accordingly.* For a machine learning paper presenting in the end a 3 line simple algorithm, the paper containsa lot of superfluous mathematical notation that crowds the paper and make the reading very tedious.Many of the papers cited Brouwer 2019, Rubanova 2019, Li 2020, offer a much smoother read in that respect.As is, this paper feels better suited to a more specialist statistics venue.* For example, many elements are introduced in the main text and are not really necessary to understand what the paper doesThe detailed section on random inputs is used only in a theorem coming later, why have it in the main text in so much details.On the other end, a description of the method this paper builds on is left into appendices.#########################Additional comments:* the formatting of the references is very inconsistent, please update Summary: This paper aims to incorporate the attention mechanism into recurrent neural networks by using fixed point equations. In particular, the authors define a bidirectional RNN with attention by a fixed point equation and then transform it to a variant of the Transformer block. The proposed model StarSaber is shown to be more parameter efficient than the Transformer model and achieve competitive performance on three CLUE datasets.Pros: + This paper proposes a framework of transforming RNNs to Transformer-style models by incorporating the attention mechanism.+ Several ablation studies are conducted to support the design choices.Cons: -This paper aims to transform the RNN from the perspective of fixed point equations. But in Eq. (4), different weights are assigned to different layers, which means the model is actually iterating different functions at each timestep (layer). Thus, indeed its not a fixed point equation and the proposed model is actually a variant of the Transformer model with a modified structure, so the whole story does not make too much sense to me.-The difference between the proposed StarSaber block and the Transformer block needs to be highlighted and discussed more clearly in the main content.-The dataset and the pretraining task used for experiments are not commonly used and small-scale. It would be more convincing to include experiments on common and larger-scale benchmarks (e.g., SQuAD and GLUE).-The paper is not very well-written and is a bit tedious, for example:    1)The related work section is not well organized and includes some unnecessary contents like the discussions about pretraining and MLM. Some discussions about related works on implicit deep learning models could be included, e.g., Deep Equilibrium Models [1], Invertible Residual Networks [2], and Implicit Deep Learning [3].    2)Eq. (2)  Eq. (4) are three similar sets of equations and rewriting could be avoided.     3)The experiment section includes some experimental details that are not very important (e.g., the dataset statistics) which could be moved to the appendix.Additional Comments/Questions: 1.As in the StarSaber model, the linear transformation weights for all the future positions and all the past positions are shared ($U^{right}$ and $U^{left}$ respectively). It seems that it incorporates weaker positional information than the positional embedding of the standard Transformer model since it could not directly distinguish between those future positions (and past positions). Therefore, I feel surprised about the ablation study regarding the positional encoding. Could you provide more details about this experiment and explain more about the result?2.It would be great to include some comparison of the computational cost of the StarSaber model and the Transformer model.3.How do you tune the learning rate for all the models?4.Theorem 3.1 is for real-valued function, it would be better to use a vectorized version as the functions considered in the paper are all vector-valued.5.Typo: dadtaset - > dataset[1] https://arxiv.org/abs/1909.01377[2] https://arxiv.org/abs/1811.00995[3] https://arxiv.org/abs/1908.06315 The authors of this paper propose a unified optimisation objective for (sequential) decision-making (i.e., _action_) and representation learning (i.e., _perception_), built on joint (KL) divergence minimisation. As also mentioned by the authors, this is a concept paper and it includes no empirical study.In particular, the authors demonstrate how existing ideas and approaches to (sequential) decision-making and representation learning can be expressed as a joint KL minimisation problem between a target and "actual" distribution. Such examples are (a) MaxEnt RL, (b) VI, (c) amortised VI, (d) KL control, (e) skill discovery and (f) empowerment, which are all cases of the KL minimisation between a target and an ``actual'' distributions.**Concerns**:1. Although the proposed perspective and language is rich and expressive, I question the novelty of the proposed framework, since the information-theoretic view of decision-making and perception is a rather established and old idea, even the term/idea of perception-action cycle is already defined [1]!2. The power of latent variables for decision-making and their interpretation is also a known idea [1].**References**[1] Tishby, N. and Polani, D., 2011. Information theory of decisions and actions. In Perception-action cycle (pp. 601-636). Springer, New York, NY. SummaryRobustness in the multi-agent setting is a nuanced concept, as (a subset) of agents can act adversarially, while the non-adversarial agents can be trained to be more robust. The authors propose to solve a max-min problem, in which some agents are optimizing their rewards taking into account some agents maybe acting suboptimally/adversarially. The authors implement learning using QMIX and evaluate on SMAC. They train a model p(z | a, o) for a global latent variable (using a variational lower bound on mutual info between z and a) that each robust agent conditions its policy on. The authors claim this method can yield significant performance improvements over vanilla adversarial training.Strengths- Robustness is not well explored in the multi-agent setting, and using a correlated equilibrium seems like an interesting way to model coordinated robust training for (a subset) of MARL agents.WeaknessesThroughout, the writing is not very clear. - The notation in (3) and (4) is sloppy: which indices are "team" and which ones are "mis"? - Sect 4.1 talks about differences between correlated eq and decentralized equilibria, but the authors never formally define what the difference is between "cooperative agents" (which they claim emerges under centralized training, decentralzied execution), and a correlated equilibrium (in which some agents cooperate, presumably, but others don't). I assume the authors are getting at the normal definition of correlated eq: there is some (global) random variable that the policies are conditioned on, but the text is very unclear about this.- The propositions 1 - 2 seem out of place without a formal defn of correlated eq (see above).- The text around prop 3 never mentions that agents presumably now learn a policy pi(a | s, z). This should be clarified.- Eq 7 is missing indices i, or should clarify the "a" refers to a joint action, etc. Conceptual issue:- Adversarial agents are defined as taking actions that minimize their *own* Q-value (i.e., agent i executes argmin_i Q_i). But shouldn't the worst mistake be the action that minimizes other agents' value? It seems this definition of "adversarial" agent is rather weak, and counter to what is defined in Eq (1 - 3), where adversarial agents want to minimize other agents' value as well? This is where the notation again causes confusion.The experimental results are too thin to conclude the authors' method is indeed effective. - What is the significance (if any) of having different agents (6, 4, etc) as the "adversarial" ones in Figures 1, 3? - The authors do not explain what the structure of SMAC is (the experiment environment). - The authors should visualize the distribution of actions executed by the robust/adversarial agents to give some intuition of the qualitative differences. Questions- What is a "flat maximum" (above Eq 3)? Summary:The paper aims to formalize task similarity in meta-learning settings by making use of nonparametric kernel regression techniques; such similarity information is then proposed as a means to alleviate some of the current issues with meta-learning algorithms such as MAML/Meta-SGD, namely reliance on large sets of similar meta-training tasks. Experiments focus on standard toy regression tasks with the added meta-training data scarcity.Strong points: - Principled approach to a challenging and current problem of interest for the sub-field of meta-learning and beyond. However, formalization of meta-learning approaches in terms of the NTK is recent but not novel [see reference Wang 2020 in paper].- Good work in progress, but the attempt to publish is premature.Weak points:- Very poor representation of relevant and conceptually similar recent work, see [1] for a comprehensive review, and specifically [2, 3, 4, 5, 6] for similar approaches.- Inaccurate claims of novelty are made; they must be made more specific and put into context. For example, the claim that task descriptors have not been used in the design of meta-learning algorithms is false, see [5, 6]. That said, the current approach could be used to analyze such SOTA approaches and perhaps explain their performance.- Meta-training data reuse across tasks at test time has been proposed previously, e.g. [7], so it is also not novel to this paper.- Very weak experimental evidence. Please use some of the few-shot image classification datasets, or standard RL tasks available since MAML was published.- Proposed method needs extensive approximations to scale up to more interesting problems.Recommendation and Rationale:I believe the paper should be rejected in current form, but I strongly encourage the authors to add more experimental data and submit to a workshop.References:[1] Meta-Learning in Neural Networks: A SurveyTimothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey. https://arxiv.org/pdf/2004.05439.pdf[2] Recasting Gradient-Based Meta-Learning as Hierarchical BayesErin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, Thomas Griffiths. https://arxiv.org/abs/1801.08930[3] Bayesian Model-Agnostic Meta-Learning. Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, Sungjin Ahn. https://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf [4] Probabilistic Model-Agnostic Meta-Learning. Chelsea Finn, Kelvin Xu, Sergey Levine. http://papers.nips.cc/paper/8161-probabilistic-model-agnostic-meta-learning[5] Meta-Learning with Latent Embedding Optimization. Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell. https://arxiv.org/abs/1807.05960[6] Few-Shot Image Recognition by Predicting Parameters from Activations. Siyuan Qiao, Chenxi Liu, Wei Shen, Alan Yuille. https://arxiv.org/abs/1706.03466[7] Meta-Q-Learning. Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J. Smola. https://arxiv.org/abs/1910.00125 #### Summary:In more mathematical fields, theorem provers and similar systems can validate claims made about formal systems.  However, many research contributions come in the form of papers, and thus they are never validated in this way.  Math researchers can express their contributions in a special purpose language to do this, but that places an additional burden on them to learn this skill.An alternative would be to "translate" the research into formal languages which could be operated on by automated systems.  This seems to be the goal of this paper, which looks at using translation to disambiguate some expressions from STEM documents written in LaTeX, which maps it into an sTex document.  Previous research in this area used more hand-specified transformations, and was evaluated on different data.  This makes this work closely related to existing work, but not directly comparable.The main finding of this work is that pre-trained transformer models outperform more traditional fully-supervised translation systems on this task.  It is difficult to guage precisely to what extent the proposed method solves the task, or to fully grasp what aspect of the translation problem is being solved.#### Strong points: The proposed approach of using transformers and large in-domain pre-training is similar to a lot of recent work which has shown to work well in practice, and is therefore well-motivated.  The task itself is important and improvements in this area could have broad range of impact that would even be good to the ML field itself, so even a practical improvement using fairly standard ML would be a useful contribution.The authors are clearly knowledgable on the topic, and discuss and cite a great deal of the literature and libraries relevant to the problem.  It's a heavy paper -- there's a very extensive set of work that's been studied and referenced.#### Weak PointsIn the start of section 4, a number of systems were listed.  Each of these was an attempt to automate the formalization process, but there was no attempt to compare against these methods.As noted towards the end of section 4, the target transformation for much of the document is the identity.  Given that actually a lot of this text should not change, is phrasing it as translation the best choice?  It seems that phrasing it this way sets up the NMT baselines to perform poorly, since a lot of training is necessary to prime the model to learn this identity transformation, and that data is not given to those models.The evaluation methodology is confusing.  For instance, it seems some of the data is generated via an automated procedure, both in the supervised learning (Section 5) and in the Synthesizing Training Data section.  It makes it difficult as a reader to understand why this is not a chicken-and-egg type of scenario: if automated methods produce the dataset, which is then used to train the model, then why are those methods not sufficient for the end task?  This may be a problem that arises from introducing so many domain-specific libraries and formalisims, that it leaves the reader with a great deal of difficult to understand precisely what the transformation is accomplishing and from what type of data.It was also bizarre in the results section how the baselines were dismissed in writing, their results were never presented.  If the baselines are truly that bad, then do they suffice as baselines?  The authors choose these instead of the existing formalization methods, so why make the contrast with methods that are not in a position to perform the task well?#### RecommendationBecause the presentation makes it difficult to fully grasp the problem setting, precisely what is being learned, precisely what is failing, it is difficult to recommend the paper for acceptance.  It is actually very understandable that this particular paper has this problem, because the authors are forced to introduce many unfamiliar concepts -- the problem setting, the types of formalisms used, the libraries used in creating the data, etc.  These are all things that are outside the scope of the typical ICLR paper and thus warrant a clear introduction, but space is limited.  I could easily imagine this paper filling up 12-14 pages just with the same content presented here.  But ultimately the paper is not written in a way that can properly convey the scope of the work and narrow in on precisely the targetted problem and why it's difficult and important.Then the experimental section is quite short and lacks important comparisons.  Given the lack of suitable baselines, I would not be able to recommend accepting the paper without real comparisons to other work in this area.  Again this could be a space concern, but the paper overall spends too much time leading up to methodology/experiments, and then is very light on actual experimental content.  Factor in that the model is used in a very off-the-shelf way, and doesn't treat the problem setting really any different than a standard translation task, it is hard to see real novelty in the modeling contribution either.Overall I think the work is promising, but it is far too rough in its current state to be considered for acceptance without significant revision.  It would need major restructing and refocusing, more experiments, and more analysis.#### PresentationI feel like there's a lot of domain specific meanings to terminology that makes it more difficult than necessary to understand by a general ML audience.  Take for instance, formal and informal.  To most language users, a scientific paper is a formal document -- it uses formal language.  So it takes me some time as a reader to get into the actual data section and understand truly what is meant by informal here.  There are many things of this nature that would be better to clarify up-front, so the reader with the typical ML background and biases doesn't carry around incorrect concepts of what the paper is about, for longer than is necessary.The citation format is incorrect.Small typos throughout. This paper proposes to augment graph neural networks with awareness of global graph information based on memory neural networks.While this work attempts to leverage the memory networks for latent representation transformation, so as to preserve global graph structural information, there exist many recent developed graph neural models which aims to inject global-level graph structure into the embedding generation. To name a few for reference:Deep Graph Infomax, ICLR.Position-aware Graph Neural Network, ICML.However, the proposed models (MemGAT and MemGCN) are mainly compared to some representative graph neural network architectures, like GCN and GAT, which is insufficient to demonstrate the effectiveness of the new graph neural method.As a general graph neural network model, the evaluation experiments on only the node classification task can hardly comprehensively justify the rationality of the proposed approach. Other benchmark tasks, such as link prediction or node clustering, could be considered in the evaluation section. Additionally, it would be interesting to discuss the latent influence between memory dimensions and graph network depth.Another important dimension of evaluation lies in the model efficiency study. How is the computational cost of the proposed MemGNN (MemGCN and MemGAT) framework as compared to other alternatives, such as GCN and GAT, can be investigated. Due the pairwise relation learning with attention mechanism, the GAT-based neural architecture could be more time-consuming. What is the additional cost brought by the designed memory neural network with multi-dimensional latent representation projection.From the evaluation results, we can observe that GAT- and GCN-based graph neural networks provide different performance with respect to split methods (e.g., random split and standard split). It would be interesting to provide some insights and clarifications to discuss this point, in order to understand the memory graph neural network better. Intervention Generative Adversarial NetsSummaryThis paper proposes a GAN training procedure where images are encoded into latents, which are perturbed and passed through a generator/decoder, and the discriminator objective is augmented to encourage it to classify the perturbation. Results are presented for generative image modeling on several datasets and compared against DCGAN, LSGAN, and WGAN-GP.My takeThis paper is another one of a common variant of GAN papers which makes small changes to the objective and training setup and shows small gains relative to old baselines on small-scale tasks. This papers empirical results are unfortunately lacking and there are no substantial or new theoretical insights, making this paper a clear reject.Detailed reviewSignificanceThis paper proposes yet another GAN training procedure, based on an encoder-decoder-discriminator setup (as has been done many times before, especially in VAE-GAN setups). The change to the vanilla training setup is effectively quite small, but introduces a 100% compute overhead relative to its baselines. My primary complaint with this paper is that for such a small change to be of interest to the community (especially given its tremendous compute cost and any added implementation complexity) there must be substantial evidence that doing so is beneficial, and the empirical results in this paper are quite weak.This paper compares against old baselines and obtains numbers which do not justify the computational overhead even relative to these old baselines. This paper should include comparison against more recent work if it wishes to prove its relevance--a quick look at an online leaderboard shows CIFAR-10 (https://paperswithcode.com/sota/image-generation-on-cifar-10) has tons of recent work which the authors could compare to or build off of. WGAN-GP is far from state-of-the-art on any of the tested benchmarks, and that the authors have made any sort of SOTA claims is misleading, and not acceptable.ClarityThe paper is reasonably well written and easy to follow.Methodological soundnessThe papers methodology is reasonably sound, and not explicitly cause for concern.OriginalityThis paper is not especially original. There are dozens, if not hundreds, of GAN training papers which introduce small changes to the (now 6 year old) vanilla GAN or VAE-GAN objective, and there are no ideas in this paper which I would consider espeically new or novel.Misc-The authors seem to conflate mode collapse (when large regions of Z map to small regions of G(Z)) with mode dropping (when modes in X are not represented in G(Z)). These are two separate phenomena and while they co-occur, they should not be considered the same.-There is no discussion of previous work on intervention, or any of the related concepts. Being a critical element of the proposed method, this should be discussed in more detail (rather than just focusing on related GAN work).-There is an obvious connection to InfoGAN, which should be cited. Summary: The paper proposes a methodology to understand quantitively the VAE model, based on the Rate-distortion theory.Comments: I think that the writing of the paper is rather confusing. Unfortunately, I am not able to judge the proposed idea (which might be interesting) and provide a reasonable review, because I could not understand it after trying to read the paper couple of times. In general, I find the clarity and coherence of the paper lacking. In my opinion, the paper should be improved significantly along these lines, such that to be accessible from the reader. Some examples of the problems I think that should be fixed:- The text is super "wordy". There is too much compact information, but with low clarity. In general, I could not understand the main story of the paper.- Many terms are used without a definition e.g. what is the definition of isometric embedding? what is the definition of rate-distortion?- In general, the mathematical notation that is used is a bit unclear.- There are many places where the authors say: "We (will) show later ...". I believe that this causes a confusion. I think is better to avoid this tactic and directly show the result in place.- How the VAE model is perceived? In particular, I cannot understand what it means to "map the VAE"?- In my opinion the Figure 1 is hard to understand.- There is a lot of context and connections to related works, which the authors presume is already known from the reader. However, I think that this is not the case for a general reader.Overall: Unfortunately, I am not able to provide a constructive feedback. The main idea of the paper might be interesting, but the writing style does not help. Therefore, I believe that the paper needs to be improved significantly, such that to be accessible. This paper proposes TraDE, a transformer-based density estimator that is capable of learning a density of real-valued tabular data. Compared to previously proposed transformers, there are three main differences in TraDE model: 1) the output is modeled as a mixture of Gaussians, 2) maximum mean discrepancy (MMD) is added to the loss, and 3) Gated Recurrent Unit (GRU) is used to provide positional encoding. Tested on a suite of benchmark tasks, the proposed method shows promising results over baselines.Strengths:The paper is the first to apply transformers on continuous-valued tabular data. The motivation of using transformers for auto-regressive modeling of continuous data stated in Section 3 is persuasive.In addition to reporting the test likelihood, the authors conduct supplementary experiments to validate the effectiveness of the proposed model. The experiments include training a regression model with generated samples, two-sample testing using a classifier, detecting out-of-distribution samples, and learning on noise-corrupted data. These analyses provide a rich view of how the proposed model behaves and confirms the effectiveness of the proposed approach.Weakness:I reckon the contribution of the paper as a density estimation method is marginal. Compared to the previously proposed transformers, [1,2], The key architectural difference is the use of a mixture of Gaussian as an output distribution instead of a discrete distribution generated from a softmax function. This change of output parametrization seems trivial and straightforward, compared to architectural improvements presented in [1, 2]. Another difference is the use of maximum mean discrepancy (MMD) as a regularizer (or an auxiliary objective function). However, as shown in Table 3, the gain from the use of MMD is not consistent.The analyses conducted to evaluate generative models (contribution no. 2 on page 1) are valid, but the paper is not the first to perform such experiments and therefore it is not adequate to claim those experiments as a core contribution. Examining the predictive performance of a model trained on generated samples is used to evaluated generative adversarial networks [3, 4]. Also, measuring out-of-distribution detection performance is used widely in generative modeling literature [5, 6].Remark 4 on page 6 is wrong. The test likelihood is an estimator of cross entropy between the model distribution and the data distribution and therefore is connected to Kullback-Leibler (KL) divergence between them. As long as KL divergence is a meaningful measure of discrepancy, test likelihood gives meaningful information. Also, the maximizer of the objective in (1) is indeed p(x), given infinite data and a correctly specified model.There is no explanation of how the outlier classes in Pendigits, ForestCover, and Satimage-2 datasets are defined.There is no description of what transformers are used as the baseline in Table 3. Also, what is the "standard Transformer" used in Table 4? How exactly do these transformers differ from TraDE?Minor comments:- It would be more appropriate to use Proposition instead of Lemma for Lemma 1 and Lemma 3.- Currently, the numberings of lemmas and remarks are confusing. I suggest to number them separately.- It would be nice to mention that the datasets used in the experiments are tabular data, just in case if a reader is not familiar to the datasets.[1] Katharopoulos, Angelos, et al. "Transformers are rnns: Fast autoregressive transformers with linear attention." arXiv preprint arXiv:2006.16236 (2020).  [2] Child, Rewon, et al. "Generating long sequences with sparse transformers." arXiv preprint arXiv:1904.10509 (2019).  [3] Ye, Yuancheng, et al. "GAN Quality Index (GQI) By GAN-induced Classifier." (2018).  [4] Borji, Ali. "Pros and cons of gan evaluation measures." Computer Vision and Image Understanding 179 (2019): 41-65.  [5] Du, Yilun, and Igor Mordatch. "Implicit generation and modeling with energy based models." Advances in Neural Information Processing Systems. 2019.  [6] Grathwohl, Will, et al. "Your classifier is secretly an energy based model and you should treat it like one." arXiv preprint arXiv:1912.03263 (2019). I believe that this paper is addressing the problem of incorporating domain knowledge into the generation of symbolic music. I find this problem very interesting.The primary problem with this paper is its lack of clarity. Unfortunately, this is so severe, that I cannot tell what is being done. In particular, when reading the PDF, I highlighted every sentence that had minor or major writing issues (ranging from sentences that were nevertheless understandable to others that were impossible for me to decode), and by the end I found that I had highlighted nearly every sentence in the paper. It is the cumulative effect that becomes quite problematic.As an example, I will provide the second half of the paragraph given at the end of the Introduction (p1-2), in which I believe that the authors give an overview of their system (as far as I can tell):"Thus, based on a given music dataset, we combine how the sequence of musical skills can be attractive and how the bars created by using the simple RNN model, i.e. handling about flow of music. In addition, to make image size can be properly reduced and the Musical Skill can be maintained while processing MIDI Bar with Image we utilize image processing based on Relational Pitch Change. This approach allows the use of the music in the train without relying on the chord scale of given music in dataset, and provides a wider range of possibilities for the music produced by matching the music to the 12 basic chart scale (C, C#, D, D#, E, F, F#, G, G#, A, A#, B). Also, to distinguish major scale and minor scale, we can generate music with chord conditions with major scale. The resulting model, FLAGNet can use the musical skill contained in the music bar to understand the musical domain knowledge, analyze the sequence of the musical skills to control the overall flow of music, and process the generated images to make symbolic music, as well as to utilize all 12 basic chart scales and handling chord condition to distinguish major/minor scale."I find there to be several problems here that make this paragraph hard to follow. First, "musical skills" are not defined or explained, although they are frequently referred throughout the paper. I believe that they relate to domain knowledge, but I cannot figure out exactly what they mean, and I think that this actually matters in this paper. (Note: on a subsequent read, I believe they relate to the heuristics described in Appendix A. If so, then I would say that, given the paper's current form, Appendix A is absolutely essential for having a chance of understanding the paper.) Second, the term "Relational Pitch Change" may refer to intervals, or something to do with transposition, but the following sentence, "This approach [...] (...A, A#, B)." doesn't make much sense to me. I believe that it refers to transposition-invariance, but how is the music "matched" to the "basic chart scales"? (Note that the latter are not clearly defined, but again, I can guess). The next sentence refers to generating music with "chord conditions with major scale" but it is not clear how this is done-- is the music generated conditionally based on a chord? or based on a scale-treated-as-though-it-were-a-chord? or some other possibility? Finally, the last sentence puts some of these ideas together and suggests that the system analyzes the "musical skills" (heuristically-determined features) in a bar and then somehow uses those to synthesize new music that presumably has some of these features. Exactly how this happens is not at all clear to me, even by the end of the paper.Many paragraphs have this sort of opacity, and the cumulative effect is a paper which I find to be completely opaque.On p2, musical terms are defined. E.g. "Time signature 3/4 means 3 of 4th note construct 1 bar. So, if one bar at 4/4 beats is divided into 16 number of parts, one minimum unit is divided into 16th notes and the triplet note of 16th note is the smallest unit in the case of 24 number of parts. our study use these 2 minimum units."  Fortunately time signatures are familiar, so a bit of confusion here was no problem. Later, the authors write: "However, we only uses MIDI only with 4/4 time signature. The reason is that we judging that handling all kinds of time signature can reduce models performance a lot." I will note that in many MIDI datasets, 4/4 is a default time signature in the MIDI file, even if the piece itself is not actually in 4/4 (i.e. the default time signature does not actually affect the playback, and so it is left uncorrected).  This is not necessarily a problem, but if they were counting on the pieces being in 4/4, then the authors could indicate what they did to check or ensure that their MIDI files were indeed in 4/4 (not necessarily an easy task).For better or worse, Section 2.5 is missing entirely, other than a section title that suggests it was going to provide background on LSTMs. On its own, a missing background section might not be a problem, but again the cumulative effect is that of a sloppiness that runs throughout. This extends not only to missing words, but also to the logic of the writing itself, e.g. the last sentence of the first paragraph begins with the phrase "In other words", but I don't actually see the rest of the sentence as summarizing what preceded it in any way. Small and picky, I realize, but the pervasiveness of this logical sloppiness makes it hard to follow.In terms of quality, I think that the general intuition of incorporating musical heuristics is a sensible approach. But it is simply unclear to me exactly how it is being done here. Some key published references are missing, e.g. a few starting points for references include: "Sequence Tutor..." by Jaques et al (2017) which also aims to incorporate domain knowledge, and perhaps the transformer-based approaches by Huang et al and Payne et al as examples of recent methods. The evaluation is somewhat problematic as well. Table 1 is a bit unclear, and the user-study categories are confusing and I am not sure that they are measuring what they are intending to measure, nor am I even sure what they are intending to measure. E.g. "Creativity" was explained to the users as: "If the possibility of using a given rhythm is enormous, or if you think it has not existed before, it has highly creativity points."  Finally, the samples that I listened to were not very convincing either.In summary: I would be glad to read a clear version of this paper, and would not hesitate to change my score if appropriate. The samples I heard do not sound particularly effective, so unless I understand them, too, in a new light, I am unlikely to change my score by much, but I would certainly be open. I am very interested in this direction of work. I cannot accept the current version because I am often unsure of what is being done, and therefore I cannot assess if it is reasonable or not. In terms of my confidence score: I am absolutely certain that the paper is too unclear to be accepted, and I am very familiar with the relevant literature. However, I am not absolutely certain that the evaluation is correct, because in fact I can't evaluate the system itself very well based on the description given in the paper.  This paper proposes a music generation system consisting of A) an RNN over sequences of hand-engineered musical features, which are fed into B) a conditional GAN to generate pianoroll images which are then post-processed to be monophonic melodies.The paper has many issues that need to be addressed:1) The language of the paper needs a lot of editing and is quite difficult to comprehend.  I'm pretty sure I understand the proposed system at a high level, but a lot of the details are unclear to me due to the awkward language and at times non-standard vocabulary.2) The main "idea" in the paper is to use hand-engineered musical features as an intermediate representation for generation.  However, there's no experiment that tests whether or not this is even helpful; I would expect to see a comparison between the GAN conditioned on musical features and an unconditional GAN.  Using hand-engineered features makes sense for human control or to get around technical limitations, but in some sense the whole point of deep learning is that we don't need to use such features and can train end-to-end.  If the authors would like to continue to pursue this line of research, as a very first step I suggest performing the above experiment.3) The evaluation setup is insufficient for comparing the various experimental conditions.  With only 1-2 examples chosen per condition (and how were they chosen?), the human ratings provide very little information about the quality of the model's output in general.  It's easy to generate a large number of samples from such a model, and with platforms such as Mechanical Turk it's fairly inexpensive to run a large listening study with hundreds or even thousands of participants.4) Even if the model conditioned on hand-engineered features were clearly superior to an unconditioned model, this result would probably not be of broad interest to the ICLR community.  If the authors choose to continue this line of research, I recommend submitting to a music-specific conference. This paper studies the correlation between the flatness of the converged local minimum and the margin. The authors report experimental results that verify the positive correlation. They suggest using margin-based measures to assess the generalizability. Also, the authors argue that large-batch optimization does not have enough time to maximize margins and hence generalize worse and suggest using it to replace the misleading folklore that small-batch methods generalize better because they are able to escape sharp minima. In addition, the authors significantly narrowed the margin which would have violated the policy: Tweaking the style files may be grounds for rejection.Overall, I vote for rejection. The experiments are described in detail and seem correct. However, I was worried that (1) the reported results are not new; and (2) the authors argue existing results are misleading but did not give enough establishment to support their argument. Extraordinary claims require extraordinary evidence. It would be good if the authors can address thesis concerns in the rebuttal session.Pros:+ The authors conduct experiments which verify a positive correlation between the margin and the flatness.Cons:- The results are not new. It is well-known that (1) margin is a good measure for assessing the generalizability [1-4]; and (2) flatness has a strong correlation with the generalizability as the authors have stated. Combining (1) and (2), it is not surprising margin and flatness has a strong correlation.- The authors argue that large-batch optimization does not have enough time to maximize margins and hence generalize worse. This argument lacks evidence from either theoretical or empirical aspect.- The authors argue that it is a misleading folklore that small-batch methods generalize better because they are able to escape sharp minima, still without evidence. In contrast, this has been established in many works; e.g., Sagun et al. (2017) as the authors mentioned.- The authors significantly narrowed the margin. As stated in the template: Tweaking the style files may be grounds for rejection.Questions: It would be good if the authors can address the cons.[1] Vladimir Vapnik and Vlamimir Vapnik. Statistical learning theory. Wiley New York, 1:624, 1998.  [2] Peter Bartlett and John Shawe-Taylor. Generalization performance of support vector machines and other pattern classifiers. In Advances in Kernel Methods: Support Vector Learning, pages 4354, 1999.  [3] Vladimir Koltchinskii, Dmitry Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. The Annals of Statistics, 30(1):150, 2002.  [4] Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin Markov networks. In Advances in Neural Information Processing Systems, pages 2532, 2004. The authors suggest that training self-supervised protein sequence models with string manipulations as data augmentations will lead to better downstream performance.They study several different types of data augmentations, (1) dictionary replacement, (2) global/local random shuffling, (3) sequence reversion and subsampling, and (4) a combination thereof.The authors show promising results, where contrastive training improves benchmark results on 3 of the four tasks proposed in TAPE.Additionally, data augmentations show better results on all tasks, beating the TAPE baseline significantly.I find it exceptionally interesting that sequence reversal has such a negative impact on pretraining.I would never have expected the order of protein sequence has such a large effect on downstream tasks.Additionally, this is a very good line of work to pursue, since contrastive methods are known to work on images, but have not shown to beat MLM on NLP tasks.It is interesting to find out whether they do well on protein language modeling tasks.However, there are a few glaring weaknesses in the paper.(1)The authors do not even attempt to test on contact prediction, citing a data embargo.In fact, many of the CASP12 structures are available, and are tested on in various papers, including the TAPE baseline, trRosetta, etc.I believe contact prediction is a main problem to show gains on out of the 5 TAPE tasks, and without showing it in some form, this paper should not be accepted. (See CASP competitions)(2)The results are reported to be best across all data augmentations taking the best per task.This seems unfair, as the representations learned should not depend on the downstream tasks.Since each augmentation seem to have extremely different effects on each task, I would not say that on the whole data augmentation is actually useful in building a protein representation.For example, checking the best model for stability, RD(0.01), I find that this augmentation only does better than the baseline on fluorescence and RH@family.(3)I have a few worries about the augmentation strategies used.- For dictionary replacement, a much more natural choice is to follow probabilities outlined in BLOSUM or PAM substitution matrices.  I believe this would be a stronger result than Alanine substitution, though the hand designed replacement scheme is quite interesting.- GRS and LRS do not make much sense to me.  It's quite unnatural to shuffle amino acids, and it's surprising to me that these representations would do as well as it does on secondary structure.  In fact, I don't think GRS should be able to do better than a linear model on just amino acid identities on secondary structure prediction, so either there is a bug here or this is a quirk of the dataset.  I also think LRS with a percentage of the sequence length is more interesting, and probably ends up being similar to the MLM loss if you use 15%.- In figure 4 - it's not clear why the contrastive models all have a drop in performance at p=0.1. I don't see any reason that this should happen across multiple tasks and augmentation strategies.I also offer some small suggestions for a more clear and better paper:- Sec 2, last sentence of paragraph 1 claims protein sequences do not need to preserve contextual meaning.  This is simply wrong.   They must preserve the ability to fold into useful biological structures, and augmentation strategies like random shuffling remove this property - please remove this sentence.- The 'MT' task is more commonly referred to in literature as MLM (Masked Language Modeling), I suggest to use this term instead.- Figure 3's color scheme is counter-intuitive and not consistent across tasks.  Why is gray the best on two tasks, where as light red is best on remote homology and dark red best on fluorescence?  Please use a more intuitive color scheme, where perhaps the bold colors show where the best results are.- Please permute the numbers for remote homology so it is ordered (fold, superfamily, family) - this is the natural ordering for this task.- Table 7: why are the linear evaluation results now reported by cross entropy rather than classification accuracy as in previous tables?As a summary, my main criticisms of this paper is (1) not including contact prediction, (2) picking the data augmentation to use for each task, and (3) augmentations with unreasonable intuitions. A suite of data augmentations is presented for improving protein language models.Strengths- Data augmentation and contrastive models are underexplored areas in protein language models. The authors are able to improve upon the TAPE baseline. It would be interesting to see if these data augmentations could be used on recent models (ESM-1 and ProTrans) to achieve a new SOTA.Weaknesses- All evaluation was performed on the TAPE model, which was released >1 year ago. Since then, there have multiple models released that have much better performance (ProTrans from Elnaggar, et al.; ESM-1 from Rives, et al.).- Related work is incomplete. For example, it says SimCLR is the current SOTA contrastive learning technique. There is already SimCLR-v2 that should be cited, among others. Even though the authors use the models from Rao, et al. 2019, the authors should also cite Alley, et al. 2019, Heinzinger et al. 2019, and Rives, et al. 2019 which were first to introduce semi-supervised learning for protein language models.- The data augmentation is done on the validation set. This means there is no longer a validation set! Their model performs better than the ones presented by Rao, et al. but that could be because additional data was incorporated here. The authors include a baseline where they continued training Rao, et al. on the validation set, but it would be better practice to augment data from the training set.- The authors do not evaluate on contact prediction because CASP12 "has an incomplete test set due to data embargoes." However, CASP12 ended 4 years ago and the data is now available. In fact, the authors use CASP12 for secondary structure prediction, so clearly they have access to the data. Furthermore, the authors could consider additional structural sets, such as structural holdout at family level; temporal holdout to CASP12/CASP13; other at the very least a sequence based split.- The data shuffling will be predominantly shifted to the right because beta = min(N, alpha + 50).Additional- In the abstract, the authors refer to fine-tuning semi-supervised protein models. This is not very specific, as the authors actually continue the pre-training procedure with the data augmentations. - For the replacement augmentation, why do you randomly replace with a single amino acid instead of rotating between, e.g. all hydroxyl or all cyclic amino acids?- Specify that secondary structure is "3-class" - On your table, you should clearly write the three test sets used for remote homology. - The authors may want to see "Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization" by Lu, et al. which came out around the time of the ICLR deadline.- It would be helpful to summarize on a table which data augmentations worked best for each methodOverall, this is an underexplored area, but is incremental to previous work. If the authors had shown better results, I would have increased my rating. However, the lack of rigor, novelty, nor impressive results means this paper does not meet the high bar for an impactful paper at ICLR. Summary:The authors propose a hierarchical model of neural ODEs, which they fit to CIFAR10. They find performance on par with ResNets, and include qualitative analyses on the filters learned by the models, their ability to fill in occluded features, and robustness to contrast at test time.Strengths:The filter parameterization is interesting. I can imagine this improving sample efficiency in certain contexts  perhaps the authors should seek out those kinds of tasks to complement their CIFAR results?The discussion does a nice job of explaining the issues that neural ODEs have when scaling to large image datasets.Weaknesses:You spend time discussing spatio-temporal receptive fields throughout. Why? Your models are applied to 2d images.The authors are missing a huge literature on (a) recurrent convolutional networks, and (b) using these networks to simulate classical vs. extra classical receptive field effects.ResNet-Blocks is the original ResNet? Maybe change "blocks" to a citation? Or V1/V2 depending on which implementation it is (unclear from the text).There's essentially no difference between the performance of any of the models tested. Is it possible to scale to ImageNet? It is important to show that the proposed method does *something* different than the standard ResNet. The authors attempted to add some qualitative experiments towards this goal in Fig 4, but those results are not very convincing. I think to show filling-in you'd want to show reconstruction in RGB space. This paper introduces a method for improving robustness of neural networks to domain shifts by adversarially perturbing the feature statistics. This is a very interesting idea, by playing a middle ground between the worst case of PGD and not doing anything. My main problem about the paper is the evaluation and particularly the lack of evaluation of certain models and certain datasets.  Lets talk models first. It is unclear to me why the quite related AdvProp model is not evaluated here. Even if they are difficult to train the pre-trained models are available here: https://github.com/rwightman/pytorch-image-models. Same with the Noisy Student L2 model which doesn't have any sort of adversarially perturbation and performs much better on ImageNet-C than the best number reported here. For reference pretrained weights for both model types are available in the above link. With the availability of pretrained models it seems inexcusable to only have 5 arbitrary comparison points, especially when there are models with significantly better accuracy.  Furthermore, for stylized ImageNet and Instagram I would like to see what a resnet50 simply fine-tuned on those distributions look like. Next on the distribution shift side, I'd also like to see more than just 3 distribution shifts. There have been two recent papers that do a metastudy of many distribution shifts: https://arxiv.org/abs/2007.08558 and https://arxiv.org/abs/2007.00644. A thorough evaluation on other distribution shifts can give a more complete picture of the advantage of the proposed approach to distribution shift rather than just 3 numbers out of context.For these reasons I recommend rejection. ### SummaryThe paper proposed to use RL methods for control for hybrid vehicles fuelstrategy. The organisation of the paper is difficult to follow and I may havemissed some of the arguments the authors are making. The experimental section isvery thin with only a 2 d system simulation.### To improve1. How is this even possible ? "We now solve the open loop optimization problem   using a general non-linear programming solver without actually knowing the   exact form of the underlying dynamics" The cost doesn't have the dynamical system   equation but you need to enforce it either as a constraint or solve is via   typical single shooting methods. Constraint is mentioned in the section 2.1 but it   is still incorrect to says that gradient descent is without access to dynamics.   Do you mean to say that we learn dynamics from the data ? and model is   assumed to be unknown at the design time ?2. MPC and PID are not the same and I am not sure why they are clubbed together   as a baseline. Especially in the experiment shown the system is simple   enough to be learned by wide enough NN. Then we can use non-linear MPC with   out of the box optimisers as SOTA baseline  for MPC ? 3. Authors learn a NN based model as part of the proposed method, MPC would be   an algorithm that can use this model to optimise the control.4. If the hybrid system model is as described in the experimental section I am   struggling to see how this scholarship helps compared to standard optimal   control methods ?5. I found the overall paper very challenging to follow. There are large logical   jumps. I am open to changing my review, if authors can show the clear benefits of their proposal.### citations1. correct source for DDP is Mayne [1]### Language1. This has no impact on technical rating of the paper and it does not directly   contribute to the review score.2. I had started to write all the typos and grammatical errors in the paper and   I stopped as there are far too many of them. Please review this for language   errors as this breaks flow of reading.### Ref1. Jacobson, D. H., & Mayne, D. Q. (1970). Differential Dynamic Programming. American Elsevier Publishing Company. https://books.google.co.uk/books?id=tA-oAAAAIAAJ This paper proposed an ad hoc defense mechanism against white-box attacks, by duplicating the training data with original samples or adversarial samples and the number of prediction classes. The authors claim that this method achieves better results compare to baseline methods.I admit that this method could potentially defend against gradient-based attacks like CW and PGD if the attacker have no knowledge of the defense mechanism. However, since this method is defending the white-box  threat model, I believe a simple attack could break it:To generate adversarial sample for test data $x$ whose correct label is $y$ or $y+k$, run PGD algorithm with the objective function $\hat{x}= argmax_{x} l(f(x),y)+l(f(x),y+k)$, where $\hat{x}$ is the adversarial example, $k$ is the number of classes (before duplication) and $f$ is the network trained by target training. Namely, this is just maximize the loss for both the real class $y$ and the duplicated class $y+k$.I'm not sure why the authors did not include any adaptive attacks like this one in section 5.Other flaws:-The writing is confusing, a lot of details are omitted. For example, what is perturbation size for CIFAR10 under PGD attack?-In section 4.1, the authors say "Thus, the undefeated Adversarial Training defense cannot be used as a baseline becauseit uses adversarial samples during training for all types of attack." I don't buy it. The choice of baseline method should be based on the threat model, not the algorithm or training data used.-The authors say "Adversarial training assumes that the attack is known..." I believe this is not true.-Black-box attack ZOO shows only 81.5% accuracy on unsecured classifier, which basically means this is not an effective black-box attack. How could the authors use an ineffective attack to demonstrate the effectiveness of their defense method? In summary, given the execution of the experiment, I'm not convinced this is an effective defense against white-box attacks. This paper presents a method to solve the subgraph matching problem based on training a graph neural network to produce embeddings of 10-hop, apparently labelled, node neighborhoods in a way that allow for recognizing subgraph relationships among graphs via dominance relationships among their embeddings.The results presented in the paper appear occasionally simply too good to be true, while several aspects of the presented method are poorly defined, or totally undefined. Among them the following:1. Algorithm 1, which provides the main tool to be used in order to answer Problem 1, is defined as returning a subgraph of G_T that is isomorphic to G_Q. However, it is not clear how that is supposed to happen, and the algorithm's pseudocode provides no clue about it. Appendix D revisits this question, and offers that the Hungarian algorithm may be used in case when an explicit matching is desired, yet there is no elaboration on the topic and no results on it.2. The pseudocode states that a binary prediction is made based on the *average* score of all value of subgraph prediction function f(z_q, z_u); there does not appear to be any way of returning an subgraph as output, apart from this prediction. In itself, the idea that merely taking an average of a binary subgraph prediction function over all node pairs would result in a correct prediction appears simply too good to be true.3. The paper lacks a sufficient explanation of details regarding this predictive method. There appears to be a threshold t, or perhaps ± (both names are used for it) on the magnitude of a violation E(z_q, z_u) on the subgraph constraint among the embedding dimensions of q and u. This threshold is mentioned in Section 2.3 by two names, and then it is never mentioned again. No discussion is offered on what values it has in experiments.4. Appendix D mentions a sweep over hyperparameters, whose effect is supposed to be shown in Table 4. However, Table 4 shows the accuracy of matching on the ENZYMES data set in an unsystematic manner. No discussion is offered on the threshold parameter. The section on Hyperparameters lists a number of conclusions, which do not appear to relate directly to Table 4.5. The writing fluctuates several times between the idea of using categorical node features and not using them. Eventually, the matter is left undefined, and the reader cannot tell whether such features are used or not. Section 2.1 leaves the question open. Appendix D comes back to this question, and defines that, with the FastPFP method, a prediction score uses the features matrices of nodes. This is the first time when it eventually becomes apparent that experiments are indeed using node labels all the way, yet the matter is not previously discussed.6. The size of a k-hop subgraph is not explicitly specified. Section 2.1 suggests that k = 10 in experiments. The value of k is called number of layers. Table 4 in the Appendix also refers to a number of layers. It is not clear whether that refers to k or to a neural network architecture. Reasons for score:The idea of using small graphs to characterize local topologies and guide message passing in interesting. However, the graph isomorphism computation part has problem. Experimental results are not conclusive. The written need improvement in some part of the manuscript.  Pros: A new method (GSN: Graph Substructure Network) is proposed to a topologically-aware message passing method that better utilize graph substructure information. The method tries to tackles the limitation of traditional GNN in exploring graph structure.  It is a good idea to pass messages differently depending on their local topologies. This is done through using a set of predefine small graphs to characterize local topologies. The authors showed that GSN was more expressive than traditional GNNs. A good number of experimental evaluations were performed. Cons: It is not clear priorly how to define a good set of small graphs, especially when considering beyond immediate neighbors. In addition, node features are not considered in graph isomorphism, which can lead to incorrect subgraph matching. The experimental results on some of the datasets (such as, MUTAG, PTC, Proteins and NCI1 in table 1) do not appear to be significantly better than those of the previous approaches when considering the variances of different runs. In addition, much better results were reported on the ogb-molhiv leaderboard (https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-molhiv). Figure 1 is confusing.  Should the number in the yellow square on the left be 5? A more self-contained explanation of the figure is appreciated. It will help readers if the authors can visualize a few examples (e.g., contributions of small graphs) to explain why their approach works better. This paper concerns the problem of learning from single-label supervision, when this label is known not to be the truth. This is called complementary label learning. Some loss functions are proposed that are claimed to have the same theoretical minimizer as the one for standard labelling. The research agenda of the paper looks reasonable, even if it can be seen as a very specific instance of partial label learning (where one just considers the complement of the complementary label and tries to learn from it). A positioning with this latter approach therefore seems necessary. Also, after reading the paper, there are some unclarities left about the authors claim. Below are some more specific comments about that:* Introduction: it is claimed that getting complementary label is easier than getting true labels, however complementary labels have to be certainly false, and while there are indeed theoretically more wrong labels than right ones, it is not entirely clear whether getting certainly false labels is easier than getting true ones in practice. Are there applications or empirical studies demonstrating that? Most mentioned papers do not appear to have actually applied the setting. * Connection to partial label learning: the current framework can be seen as a peculiar case of partial label learning, as if I take a complementary label $\overline{y}$, then its complement $\mathcal{Y}\setminus\overline{y}$ is a partial label certainly containing the truth. It would then be necessary to connect the current work to this trend, for instance to Cour et al. "Learning from partial labels" (JMLR 2011) or the more recent works of, e.g., X Wu, ML Zhang "Towards Enabling Binary Decomposition for Partial Label Learning.". * Definition 2: I do not really follow definition 2. First, are \theta^* and \theta arbitrary parameters values? Why call it \theta^* (suggesting some kind of optimality)? If   \theta^* is a minimizer of one of the two losses, then either the premise or the conclusion is a tautology (making the definition kind of meaningless). * Proof of Theorem 1: I have some trouble with this definition. First, Equation (13) seems trivial if \theta^* is the finite sample optimal model (also, why not identifying the search space with the space of parameters?). I also do not really follow the next line, as it is unclear how realistic it is to modify the parameters for just one instance? It is also unclear what is to be proven here, as \inf \sum \geq \sum \inf, thus allowing for instance-specific parameters would always give something better than a global minimizer. In summary, I am not really convinced by this proof. * In the experiment, I wold expect a comparison with other approaches (complementary but also partial label learning), but more importantly with the optimal models obtained on learning from the initial true labels, if only to demonstrate that the proposed theorems are valid. The asymptotic accuracies displayed are also very far from state-of-art standards (less than half of it) for CIFAR 10, which seems to contradict the fact that complementary labels have the same minimizer (hence comparable performances) as the one obtained with true labels? * Finally, the paper contains an important numbers of typos or questionable grammatical structures. For instance in the first two pages only:- "supper" --> super- "A complementary-label is only specific that the pattern"- "in some questions refer to private."- "the best hyper-parameter by empirical risk since" (by empirical risk minimisation)- "can be summary as" SUMMARY:Sketching is a popular technique in numerical linear algebra for achieving various desirable properties (e.g., lower complexity, one pass methods). The present paper considers a particular kind of sketch for which the sketch matrix is learned from data. It shows how such learned sketches can be used in two types of problems: Hessian sketching (Sec. 3) and Hessian regression (Sec. 4). The authors give both algorithms and provide theoretical guarantees. They also apply these techniques to a number of both synthetic and real datasets in the experiments. For the most part, the experiments indicate that the proposed methods give a consistent, but not necessarily very large, improvement.I think the idea of using learned sketches is interesting, and it seems like it has not been applied to the problems considered in this paper before. I think the paper strikes a good balance between algorithms, theory and experiments. I like the paper, but there are a few issues that must be addressed before it can be published. Most importantly, there seems to be errors in both Lemma 3.1 and Theorem 3.2. These should hopefully be easy to fix though. In the current state, I vote to reject the paper. But if my concerns, especially the issues with Lemma 3.1 and Theorem 3.2, can be addressed, I'll be happy to increase the rating. ADVANTAGES:- Learned sketching is a fairly new and interesting idea.- It seems like learned sketching has not been used for Hessian sketching and regression before.- Good balance of algorithms, theory and experiments.CONCERNS/QUESTIONS:- In the list that describes performance improvements under "Our Contributions", it is not clear what the quantities $x^*$ and $X^*$ represent. In Sections 5 and 6, these quantities are used to represent the optimal solution to the unsketched problems, but here they seem to represent the solution for the sketched problem using one of the random sketches. If it's the latter, are $x^*$ and $X^*$ the best performing solutions produced by the competing methods? It would be helpful if you clarified this. - In Algorithm 1, how is $\alpha$ chosen? Is $\alpha$ updated adaptively, or is it just a fixed small number? Is there any rule of thumb for how to do this? - Lemma 3.1: Is seems like the bound on $\hat{Z}_2$ stated in the lemma is wrong. Based on the upper and lower bounds on $Z_2(S)$ towards the end of Section A, it seems like this bound should read$$\frac{Z_2(S)}{(1+\eta)^2} - 3 \eta \leq \hat{Z}_2 \leq \frac{Z_2(S)}{(1-\eta)^2} + 3 \eta.$$- Lemma 3.1: Since you use that $\max(\eta,\eta^2) = \eta$ when applying the result of Vershynin (2012) in the proof, you should add the condition that $\eta \leq 1$ in the lemma statement.- Proof of Lemma 3.1: In Section A, 2nd sentence, you say "Since $T$ is a subspace embedding of the column space of $A$...". I think you should add that this is true with probability at least 0.99, to clarify where the 0.99 probability of success in the lemma statement comes from. - Proof of Lemma 3.1: In Section A, you use the inverse of $R$ in multiple places. However, it seems like $R$ won't be invertible unless $A$ is of full rank. Can the proof be adapted for a case when $A$ is rank deficient? If not, you should add that $A$ is assumed to be full rank in the lemma statement.- Proof of Lemma 3.1: This is related to the previous point. In the second to last equation on page 10, it seems like the equation$$\min_{x \in S^{d-1}} || S U x || = \min_{y \neq 0} \frac{|| S U W y ||}{|| W y ||}$$only will hold if W is full rank, which requires A to be full rank (since $AR^{-1} = UW$ and $U$ has $d$ columns and is full rank). Can the proof be adapted for a case when $A$ is rank deficient? If not, you should add that $A$ is assumed to be full rank in the lemma statement.- Theorem 3.2: Given the error in Lemma 3.1, the bound in Theorem 3.2 needs to be updated accordingly. Also, in the proof in Section B, in the first inequality you use$$\frac{1}{\hat{Z}_1} \geq \frac{1}{\frac{1}{1+\eta} Z_1(S)}.$$There's a sign error here; it should read$$\frac{1}{\hat{Z}_1} \geq \frac{1}{\frac{1}{1-\eta} Z_1(S)}.$$- Proof of Theorem 3.2: In Section B, 2nd sentence, it would be helpful for the reader if you said "holds with probability 0.99" instead of "holds with high probability", and then also clarify that you do a union bound with the 0.99 probability from Lemma 3.1 to get a success probability of at least 0.98 in Theorem 3.2. This may be obvious to readers familiar with the area, but being clear with these things would make the paper accessible to a wider audience.- Solving (4) deterministically would cost $O(nd^2)$. For Alg. 3 to be worthwhile, it therefore seems like the number $\min(\sigma_1/\sigma_1', \sigma_2/\sigma_2')$ in Theorem 4.1 must be smaller than $d$. Could you say something about why we expect this $\min$ to be small? Is it the case that the $\min$ may be large with some small probability?- In the experiments, $m$ is chosen as $m = kd$ for some integer $k$. This is different from the $m = O(d^2)$ required for CountSketch to be a subspace embedding. Have you found that this choice $m = O(d)$ always works well in practice, or have you encountered datasets where such a choice has proven to be too small?- For the experiments in Section 5.2, is there any reason why you only use learned sketches in the first round for the Gaussian and Swarm behavior datasets, rather than use them all rounds as for the other datasets and experiments?- In Section 6, is there a reason why you choose $\eta = 1$ and $\eta = 0.2$ rather than using the rule for setting $\eta$ in Alg. 3?- In Section C, you refer to "standard bounds for gradient descent". Can you please provide a reference for those that are unfamiliar with the literature?- The paper ends abruptly with no conclusion.MINOR CONCERNS/QUESTIONS:- On page 2, in the 2nd paragraph, 3rd sentence, you say "If $A$ is tall-and-skinny, then $S$ is wide-and-fat...". Should it be "short-and-fat" rather than "wide-and-fat"?- The usage of R is a bit confusing. It is used to mean the R matrix in a QR factorization in Alg. 2, the inverse of the R matrix from a QR factorization in Alg. 3, and the upper bound on the nuclear norm in Section 5.3. It would be less confusing if a the inverses in Alg. 3 and the nuclear norm bounds were called something other than R.- In Section 5.2, for the Swarm behavior and Gisette datasets, you say that $B_i$ is of size $2430 \times 30$ and $5030 \times 30$, respectively. Should this be $2400 \times 30$ and $5000 \times 30$ since $n$ is 2400 and 5000 respectively for the two datasets? The paper proposed a learned variant of the well-known iterative Hessian sketch (IHS) method of Pilanci and Wainwright, for efficiently solving least-squares regression. The proposed method is essentially a learned variant of the count-sketch, where the positions of the non-zero entries are random while the value is learned. While getting a learned variant for IHS is an interesting direction, the current theoretical contribution of this paper is only incremental, and most importantly, the reviewer is unconvinced for the practicality of the current approach.The learned sketching matrix S^t is computed at each iteration, which should introduce a significant computational overhead. Then the plots comparing the learned IHS with unlearned IHS is unfair since it is regarding the iteration count. The iteration count does not reflect such overhead, and the reviewer believe that the wall-clock time comparision would be more sensible.The reviewer found that the authors are over-selling their numerical results. For example in Figures 1, 2, 6, 7, that the learned IHS has the same linear convergence rate as the unlearn IHS, and actually only slightly faster overall (only 1 iteration faster if we view the plots horizontally), but the authors' claims are like We can observe that the classical sketches have approximately the same order of error and the learned sketches reduce the error by a 5/6 to 7/8 factor in all iterations, We observe that the classical sketches yield approximately the same order of error, while the learned sketches improve the error by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset., which the authors seem to purposefully report from vertical views to exaggerate the results. This is very misleading to the readers which are not familiar with the related works. The paper studies the problem of differentiating through the policy return when the policy is a Gaussian mixture model. The main contribution of the paper is a heuristic approach for computing this gradient. Having defined the policy update, the authors integrate it to two RL algorithms: PPO and SAC. The experiments show that the algorithms with GMMs behave roughly the same as with a single Gaussian save for one (SAC) or two (PPO) environments. On the other side the authors show that their algorithm can learn multiple solutions on a reaching task and that exploration is better behaved on this task too.The main problem I had with the paper is the lack of formalism in tackling such a fundamental problem. Given a parameterized GMM model, the contribution of the paper is to study the gradient w.r.t. the GMMs parameters of an expectation, E[f(X)], where X is sampled from this GMM distribution. It seems like a fundamental problem and authors should discuss more thoroughly existing approaches to computing this gradient. For instance one could simply use the score ratio trick (i.e. REINFORCE) or use importance sampling as in PPO or perhaps a reparameterization trick (with Gumbel-softmax?) if applicable.  At least the two first one are perfectly valid alternatives and should be discussed and ideally compared to the current solution. In addition it is hard to understand if the proposed solution is an approximation or a heuristic with no guarantees. The authors propose a proof in the appendix but there is no formal statement of what the proof is supposed to show. Is it that the solution is a consistent estimator of the gradient? In Appendix A, I do not understand what the authors are saying. Somehow they go from a GMM (Eq. 12) to a MoE (Eq. 14-16), i.e. from summing densities to summing random variables. Since these two are very different, what ends up being used in the paper?  I am also confused by the role of the GMM compared to a single Gaussian policy. On one side, the GMM appears to find multiple solutions to an RL problem (Fig. 3), on the other side  the GMM appears to decompose a single solution into sub-policies that are active in different regions of the state-space (Fig. 9). But arent these two at odds with each other?Regarding the exploration advantage, is it known why a GMM should initially better explore than a unimodal Gaussian? Is it task specific or was it also observed on the Mujoco tasks?Why was there no PPO baseline in Fig. 2? Please also clarify the number of seeds for experiments in Fig. 1 and 2. and discuss statistical significance, since the plots are all very close to each other.For the Hopper cluster visualization, is the proposed algorithm able to learn different solutions? If that is the case, I think providing a video, or a sequence of images of two different solutions would be more impactful than Fig. 4.Overall, I think the paper needs significant changes to properly discuss the technical problem it tackles, discuss existing methods, and describe more formally the proposed solution including the theoretical statements provided in App. B. For the experiments, demonstrating that the algorithm is able to simultaneously find multiple solutions is interesting but should be extended to at least one of the highest dimensional locomotion tasks of the paper. For instance showing sampled trajectories for the Hopper as a replacement to Fig. 4 could be very encouraging.  ### SummaryThe paper focuses on the policy architecture of deep reinforcement learning algorithms. Specifically, the authors apply the probabilistic mixture-of-experts (PMOE) model in the policy of a reinforcement learning agent, where each primitive is a unimodal Gaussian distribution and the gating model is a simple state-conditioned categorical distribution. The authors derive the corresponding policy gradient objective for the PMOE policy.The authors apply the PMOE policy on top of SAC and PPO, and perform experiments on the continuous locomotion tasks in the MuJoCo environments. The results indicate that in some tasks, the PMOE policy outperforms the naive policy baseline. The paper also includes ablation studies and visualizations to demonstrate the diversity of the learned primitives of the PMOE policy.### CommentsThe paper is well written and the idea proposed in this paper is really easy to follow. The authors also include a wide suite of experiments with both on-policy and off-policy RL algorithms to demonstrate the performance of the proposed method, and various ablation studies and visualizations to demonstrate the behavior of PMOE policy. Despite these advantages, I cannot recommend acceptance of this paper due to the lack of novelty and significant performance improvement.First of all, as the experiment results in this paper suggest, the performance gain of the PMOE policy is marginal and highly task-specific. Only in one of the 6 tasks the PMOE policy exhibits significant benefit over baselines. Therefore, from the scope of experiments in this paper, it is hard to conclude that PMOE policy really has meaningful advantages over a naive policy parameterization.Moreover, as described in the paper, the PMOE model is a fairly well-studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting. Im not convinced that such a straightforward application has enough contribution, especially given the fact that the performance improvement is not significant.Therefore, due to the lack of novelty and significant performance improvement, I cannot recommend acceptance of this paper. This paper presents a modified version of a neural network-based MI estimator. They investigate a few of the issues of this specific estimator and propose a regularization to help with one of them. MI estimation is an important and difficult topic. Improvements in this area are of definite interest.Pros:The paper appears to be technically correct. The experiments are somewhat supportive of including the regularization, especially when the MI is higher which is a known issue with some MI estimators.Cons:There are some interesting ideas here but the paper feels unpolished. The presentation of the ideas is somewhat unconventional. Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them. The paper could benefit from a bit more focus in this regard. In the end, the authors really only propose a small modification to the MINE estimator to counter the supposed drifting problem and do some experiments showing some improvement. But it's not clear how much of a problem this drift really is. The authors show that it causes a bias but they do not present how much bias it adds.   In addition, the authors are severely neglecting some of the non-neural network state of the art MI estimators in their citations and comparisons (see the references below for some examples, which all have strong theoretical results). The theoretical work is also weak with regards to the convergence rates of the proposed estimator as well. While empirical results can confirm that an estimator can be useful in practice, they are easy to cherry-pick and ultimately theoretical guarantees are needed to know an estimator's general performance. Thus the results would be a lot stronger if convergence rate guarantees were given. Other comments/questions:The authors should define the MINE estimator in this paper.The second bullet point on page 2 says that "training with larger batch size reduces the variance of the MI estimate". Isn't this a good thing? That would lead to better convergence.In Section 3.1 the notation is technically incorrect. Instead of stating $I(X;X)$ it should be written as $I(X_1;X_2)$ where $X_1$ and $X_2$ are i.i.d. The former suggests that you're comparing the same random variables. On page 4, it's suggested that joint samples are sparse with reduced sample size. Why aren't joint samples simply included together during training?Does regular L2 regularization help with the drift problem? [R1] Moon et al."Ensemble estimation of mutual information," ISIT, 2017.[R2] Moon et al., "Information theoretic structure learning with confidence," ICASSP, 2017.[R3] Moon et al., "Ensemble Estimation of Information Divergence," Entropy, 2018.[R4] Singh and Poczos, "Exponential concentration of a density functional estimator," NeurIPS, 2014.[R5] Kandasamy et al., "Nonparametric von Mises estimators for entropies, divergences, and mutual informations," NeurIPS. 2015. This paper proposes a line-search for optimizing deep neural networks. The method is rather unprincipled and quite close to the approach proposed in (Vaswani et al, 2019). I do not think that the paper proposes new ideas that haven't been already explored in the deterministic optimization literature. All in all, the paper needs to better connect to the algorithmic ideas in the deterministic optimization literature, compare against the new optimization methods proposed recently and have more representative plots. Detailed review below. - The claim "adaptive methods concentrate more on finding good directions than on optimal step sizes, and could benefit from line search approaches" needs justification. At this point, there has been substantial work that shows that adaptive methods like Adam are quite robust to their step size and do in fact, work well across problems. - Please cite the literature relevant to SGD, for example, Robins-Munro and Bottou et al, 2016. In the deterministic optimization literature, using Polyak step size is an alternative to line-search approaches. Please also cite the recently proposed methods based on the Polyak step-size, Berrada, et al "Training Neural Networks for and by Interpolation" and Loizou et al "Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence" that have been used to train deep neural networks. - In Figure 1, it is not enough to show that the loss landscapes are similar in the batch gradient direction. they should also be the same in the stochastic gradient direction, which depending on the batch, can be very different from the batch gradient direction. A more convincing approach would be to choose random directions and compute a metric of similarity for all such directions. Moreover, please explain how is "t" chosen for this plot. We know that the loss landscape is different at the start vs the end of the optimization. Consequently, this is not enough evidence to show that line-search can be done on a stochastic batch. In this figure, the batch-size is another confounding factor. What is the effect of the batch-size? - "lemp has a simple shape and can be approximated well by lower order polynomials, splines or fourier series." This is indeed the motivation behind line-search techniques for convex problems. Please cite Noecedal-Wright 2006 or the original line-search paper by Armijo. - In Section 2.2, backtracking line-search is used to overcome the first challenge, i.e. the algorithm picks the largest step-size that satisfies a sufficient decrease condition. The complexity of this is proportional to the number of backtracking iterations which is small. In the proposed approach, a number of points is sampled, meaning additional function evaluations. Please justify why the backtracking approach is not sufficient in this case, and explain why the proposed method would be better.  - Similarly, for the second challenge, it is doing exactly the thing you cautioned against earlier "that the batch need not be representative of the full loss function".  Doing a backtracking line-search on a batch is exactly the approach adopted by (Vaswani et al, 2019). Please clarify this and also explain why you do not experimentally compare against them. - The idea of building a model of the loss function by using additional points (from the past) is well known in the deterministic optimization literature in the form of line-search with quadratic/cubic interpolation. This approach does not use additional function evaluations. Please explicitly make this connection and again, justify your approach against this less expensive method. Moreover, the paper proposes to build a highly accurate model of a stochastic loss which need not be representative of the full loss in any case. - "The test error is determined by a 5-fold cross-validation. The second last polynomial degree is chosen and the polynomial is againfitted on all loss values to get a more accurate fit. Consequently the closest minimum to the initial location is determined and additional losses are measured in a reasonable interval around it." This is clearly very computationally expensive for determining the step-size for one iteration. Please clearly state what is the computational complexity of determining the step-size in one iteration. - "ELF generalizes better if not performing a step to the minimum, but to perform a step that decreased the loss by a decrease factor" This is almost the same as checking the Armijo sufficient decrease condition with a factor of \delta. Why not just do this and say it explicitly?- Experimentally, since the proposed approach is closest to the work of (Vaswani et al, 2019), please experimentally compare against their method. - In Figure 4, please plot the training/test loss vs the number of iterations. One point of information in the form of the test accuracy is not representative, especially since the metric being optimized is the training loss. And there are multiple confounding factors that influence the test error corresponding to any optimization method. Since this is more of an experimental paper, it would make sense to compare against the newer variants of Adam, such as RADAM and AdaBound that have found to work well. This paper shows that aligning parallel text with fastalign and then randomly replacing source words with their aligned target words, or interpolating their embeddings, improves machine translation.This method is different from other data-augmentation methods that try to alter the source sentence without changing its meaning; here, the source sentence is altered into a mixture of the source and target. Thats interesting, but not very strongly motivated.The paper doesnt make clear whether the noise probability / coefficient is optimized on a development set or the test set. Based on Figures 3 and 4, it looks as though these hyperparameters may have been optimized on the test set, which is concerning. For both the baseline systems and your system, hyperparameters should be optimized on a development set and then tested using only a single hyperparameter setting on the test set. If this is what you did, please explicitly state this to reassure the reader.Not much attempt is made to explain why this method helps; the only analysis is a measurement of cosine similarity between five German-English word pairs. Do you tie word embeddings between the source and target languages (Press and Wolf, 2017)?- If so, one would expect that the transformer would already be able to place words with similar meanings close together, so the fact that your method improves this is interesting; do you know whether it helps more, e.g., for rare words, proper names, technical terms? Why is fastalign able to align some words better than the transformer? Would an even simpler method help, e.g., if (and only if) word f and word e both occur <= k times in the training data and they occur in exactly the same sentence pairs, then allow f to be switched to e?- If not, I'd suggest doing so and rerunning the experiments to see if you still get an improvement.Overall, this seems like a good trick for training NMT systems, but I would hope to see more insight either into why the proposed method works, or how NMT works or doesnt work. The paper seeks to understand how different padding modes and canvas colors affect the performance of a convolutional neural network in classification and semantic segmentation tasks. The question seems somewhat strange - surely a network should be able to counteract a consistent change in padding or background color. If there was a strong effect it would be an interesting finding indeed. Unfortunately, the paper fails to convince that any but the most obvious effects exist.There is an underlying assumption that knowledge of absolute spatial location should be helpful in some tasks and unhelpful in others. Why should absolute position matter for CIFAR-10 classification or semantic segmentation? Before jumping to this conclusion, this should be measured directly, e.g., by embedding horizontal and vertical positions as constant feature maps at every convolution. The findings in Section 2 are not very surprising. Assuming that images are not mostly black, the greatest contrast is found against a constant black background, and partial convolution only enhances this effect - it is equivalent to zero padding but brightening the image artificially when the convolution kernel overlaps it only partially.Table 2 shows that classification results vary wildly between padding and no padding, when comparing black canvas and zero padding. This makes no sense to me. Especially in large grids, a picture embedded somewhere else than the edge of a black canvas, without padding, should be fundamentally the same as having just the picture and zero padding. ResNet-18 that the paper uses has a global pooling layer at the end, ensuring that global position information is certain to be lost at that point at the latest.I'm extremely concerned about the paper's decision to use bilinear interpolation to match the resolution with skip connection in a ResNet architecture, when padding is not used. My understanding is that the convolution branch output is upscaled to the skip branch resolution before composition. This introduces a scaling factor to the convolution branch, and therefore moves the contents of feature maps towards the edges. The composite will thus not have the features from the two branches line up with each other  anymore. This shifting effect is largest near the edges and smallest near the center, which matches with the observed performance characteristics. The subsequent convolution layers may be able to undo some of this shifting, but only at the cost of location-dependent kernels that are tailored to fit the offset  caused at different parts of the image. I'm inclined to believe that many of the alleged problems with non-padded convolutions are due to this design. For example, Table 10 suggests that the padding itself is a much more important factor than if padding is with zeros or a reflection, which makes little sense if locating the boundary were the critical aspect.The paper is mostly clearly written, but some of the terminology is confusing. Section 3.1 talks about location-dependent classification, whereas the goal is to have classification be independent of location.  There are other cases of similar dependence vs independence confusion. What Horizontal and Gaussian mean in Table 1 should be explained. On page 7, formula $\frac{k-1}{k}$ should read $\frac{k^2-1}{k^2}$. In Figure 6, it is unclear how the activations are mapped to grayscale values. Regardless, high or low activations do not mean much with ReLU activation function, as any scaling can be counteracted in the weights of the subsequent convolution kernels, so the relative strengths of activations may not carry much information. In Figure 12, are the w/pad and w/o pad flipped? The effect seems inverse to Figure 1.In summary, I find the findings questionable and too counterintuitive - e.g., the big difference in classification power between black background vs. zero padding when a small image is embedded in a large black canvas. The paper fails to demonstrate convincingly that these findings are real instead of artifacts of the experimental design.Pros: mostly clearly written, many experiments.Cons: dubious choices for network design and experiments, counterintuitive results that are not properlyexplained or analyzed. Summary of the paper:This submission studies the effect of zero/one padding in convolutional networks when using images that are pasted on a canvas. The main thesis is that zero padding induces absolute position information and that this leads to better performance in many cases. The study is based on segmentation and classification where the input image is placed on a background canvas.Strengths:Padding is omi-present in current convolutional and "fully" convolutional architectures but its effect is not studied well.The manuscript clearly states the hypothesis that are investigated.Weaknesses:The motivation for using canvases in the way done within this manuscript is unclear to me. The introduction states this is due to the fact that images need to be rectrangular to be processed with a CNN. While in practice mostly rectangular images are used with CNNs it seems to me that convolutions could also be applied to non-rectangular images if required (with custom implementation). However, more importantly the manuscript does not just pad images to be rectangular but instead pastes a much smaller image onto a large canvas. This is not clearly motivated in the current form of the manuscript as far as I can tell.In Section 3 it is unclear if one network is trained for all grid resolutions or if the networks are trained per grid resolution.Unclear how no padding is conducted in ResNet. Section 3 indicates that a standard ResNet-18 is modified to a no padding version by removing the padding from the convolutions and using bilinear resize to match the sizes. The way I understand this is that for example an NxN image is convolved without padding and hence the output will be an (N-k) x (N-k) image, now either the output of the convolution or the original NxN image is resized to match the other resolution before adding the two. This seems like a very problematic implementation as in either version the spatial alignment between the two is not preserved and importantly the misalignment is spatially varying while the convolution kernel will not vary spatially. In my mind a no padding implementation of a ResNet would simply crop the original image in the consistent way to how the convolution does not produce outputs for the boundary regions. If my understanding of this implementation is correct I feel that this is a major flaw in this submission.Do the no padding segmentation networks contain upsampling operations and or u-net type skip connections? How are the no padding versions of these operations implemented? Especially for the city scapes segmentation results it is unclear to me how a full image segmentation was obtained without using any form of padding or related mechanism of retaining the information around the image boundary.In Table 3 the white padding seems to contain stronger location information than the black padding which contradicts the main thesis of the submission that says black padding contains more location information. However, Table 3 uses different grid sizes for black and white padding. Are the different grid sizes the reason for this discrepancy? What is the motivation for using different grid sizes?One point I find missing in this submission is that no padding networks will have spatially smaller feature maps which means less information might be stored in the intermediate layers. This could be a reason why the no-padding versions of Table 4 underperform compared to reflection padding.What happens for networks where the input/output resolution varies, i.e. "fully" convolutional networks? In these cases absolute position information intuitively would prevent generalization to different image sizes.Reason for score:This submission discusses an important and relevant topic. However, at this point I believe that there are potentially flaws in the experiments which might significantly influence the findings. Therefore the current rejection rating. If the rebuttal could further discuss the points in the weaknesses section and explain in detail how the no-padding versions were implemented that would be helpful to further assess the correctness of the above assessment.  1). The novelty and contribution are very limited. In literature, many papers have discussed the connection between different GNNs, typically, including aggregators and Updaters, such as discussed in Deep Learning on Graphs: A Survey. The submission only provides a kind of connection between GCN, GAT, PPPN and APPNP in the perspective of denoising. Compared with that, the survey paper actually connects many different GNNs. 2). The writing quality is low. There are many errors, for example, in section 3, the unnormalized version of Laplacian matrix with LDA==> the unnormalized version of Laplacian matrix with L=DA?3). In Eq. (1) to ease the discussion, the non-linear activation is not included. However, the nonlinearity is the key part for deep GNNs. In Formula (8), the GNNS AS GRAPH SIGNAL DENOISING actually oversimplified the topological smoothing over attributes, since the nonlinear transformation, especially with dropout will already conduct the signal denoting. So why use such additional effects to do that?4). The submission provides both Node classification task and ADVERSARIAL defense task to validate the performance. As for Node classification, the results however are not very promising compared with current SOTA. For example, in ICLR20 paper ADAPTIVE STRUCTURAL FINGERPRINTS FOR GRAPH ATTENTION NETWORKS, the cora dataset reports 85.4±0.3% compared with that reported by this submission only 84.59±0.8; in cite seer 74.0±0.4% compared with this submissions report 72.05±0.5. In Pubmed, this paper reports 79.70±0.4, however, in the ICLR paper, they report 81.2±0.3%. From that perspective, I did not see any advantage in the submission.5). As another task for validation, that is robustness to ADVERSARIAL attack. It is suggest to compare the recent SOTA Graph Information Bottleneck by Jure stanford in NeurIPS20. SUMMARY:This paper establishes a relation between different popular graph neural networks by mathematically proving that the feature aggregation operation of such networks can be understood as a graph-signal-denoising step. Moreover, the authors try to establish a general framework based on graph signal denoising that subsumes the studied architectures, developing new graph neural network (GNN) architecture under this framework.STRONG POINTS:Showing that different architectures are indeed using a similar approach for the feature aggregation which is closely related to graph signal denoising is an interesting idea which helps to gain insight into how these architectures work. Furthermore, the numerical results illustrate that the proposed architecture has a competitive performance under certain settings.WEAK POINTS:The mathematical notation of the paper is sometimes ambiguous and unclear, so it should be carefully revised.The relation between GAT and the graph signal denoising approach is not clear and should be detailed, since it is one of the main contributions of the paper.While proposing a unified GNN framework based on graph signal denoising is stated as one of the main contributions of the paper, it amounts to presenting the graph denoising formulation with an arbitrary regularization function. The paper should focus more on the relation between the proposed architecture and the different GNN with graph signal denoising schemes. The paper would benefit if the proofs of Theorems 1-4 were included in the main body, rather than in the appendix. The reason is twofold. Those proofs are likely to constitute the main contribution of the paper. Furthermore, the statement of the theorems (without the proofs) is not sufficient to fully illustrate the relation with graph signal denoising.The proposed ADA-UGNN network should be further analyzed. An MLP is chosen as the feature aggregation function without providing a motivation. Furthermore, the impact of C being learned instead of being a hyperparameter (Theorems 5 and 6) should be discussed in more detailed, since it implies that convexity of (14) is lost.ADDITIONAL COMMENTS:The symbol L is ambiguously used to denote different types of Laplacian matrices.In Section 2 says that each node is associated with a d-dimensional signal X of size N times d, but the signal associated with a node should be a vector, not a matrix.Equation (4) is not mathematically correct. The variables used as indexes of the summation are not present in the terms inside the summation. In fact, notation for the indexes of the summations throughout the entire manuscript is quite confusing (and in cases like (4) definitely incorrect). The edge-centric interpretation of the Laplacian regularization is never used. Also, both the edge-centric and node-centric formulations are not correct since they are missing the related term of the adjacency matrix A_ij. If A is binary, this should be stated clearly. Two different notations for the gradient descent algorithms are used in the paper (see, e.g., equations (21) and (24) vs. (25) and (30)). This should be unified.In eq (15), please clarify what d_i and d_j represent. This paper presents a model that takes in a keyframe from a video and emits the noun and verb best matching what is being done in the frame.At inference time, the noun and verb have never been seen in combination with one another, but have each been seen paired with other nouns/verbs at training time.The paper presents a complex, three part model (ArtNet) to tackle this challenge, as well as unimodal linguistic baselines.Notably, the evaluation does not include vision-only baselines or pretrained model toplines, making it difficult to assess exactly what ArtNet is learning and where its advantage lies.Questions:- There are multiple references to "creation", e.g., "create novel compositions" which makes the method sound like it's performing generation. From my understanding, though, ArtNet is purely discriminative, taking in a keyframe and predicting a noun and verb. However, there is one line in the paper that says "We also learn visual reconstruction via a regression task.", which makes it sound like there's a formulation of ArtNet that maybe takes in a noun and verb and produces a keyframe image (using a GAN, maybe? Or a nearest neighbor lookup?), and so does "create novel compositions". If that's the case, it isn't described, and this image reconstruction task is never mentioned again in the paper or described in any equations.- Were pretrained ViLBERT/UNITER run alone as a topline? Establishing how much is lost in performance due to lack of pretraining + how this method addresses that with sparse data would make a much stronger argument, I feel. In particular, we would want to see pretrained ViLBERT/UNITER and then pretrained + ArtNET to give a sense of how performance will change as models have seen huge amounts of aligned data.- [Related] In Table 1 what's the intuition for language pretrained mBERT/UNITER/ViLBERT falling behind from scratch? Why use language pretraining and not vision pretraining (e.g., topline)?- In Eq (1), why use cosine similarity for visual embeddings but then back off to surface forms for words? Was cosine similarity for word representations computed by UNITER tried? What is the intuition for this not working, if it did not?- Not sure in Eq (4) what the sequence input to the LSTM is; does c range over some sequence? Doesn't the sum already collapse that?- "To ensure the focus is on new compositions, rather than new words, we removed new compositions that contain new words not seen in the train set." All new words or just nouns/verbs? It's a big advantage/relaxation on the test set to have no OOV tokens.- Why no vision-only baseline? Strip word contexts at training time except noun/verb, then predict only noun/verb at test. A lot of this could be basically object recognition followed by activity recognition or strong priors on p(activity | object) (e.g., always "open" or "close" for cabinets).- "outperforms Multimodal BERT/BERT with 1.23%/4.42% improvements, which is significant"; what statistical significance test was used? How many random initializations were tried to establish the average performance numbers for comparison between performance populations?Suggestions for Improvement:- "We call attention to a challenging problem, compositional generalization, in the context of machine language acquisition, which has seldom been studied." This is poorly worded, since compositional generalization is well and commonly studied, to the point that even in this paper there is a section in the related work about it. Major workshops also list compositionality as a topic of interest, so I don't think it's fair at all to say that this has "seldom been studied" [ https://sites.google.com/view/repl4nlp2020/home ]. In the context of language acquisition specifically, emergent communication work focuses heavily on composition [ https://sites.google.com/view/emecom2019/home ].- ViLBERT in intro, UNITER in description of method, "Multimodal BERT" (mBERT?) in Table 1. What was used? Needs to be consistent in presentation.- "We discard the object labels due to strict constraints in the language acquisition scenario." Because Faster RCNN is trained on ImageNet, which is based on WordNet, the object categories still exist in the form of supervision. The model has a linguistically-motivated notion of what an "object" is that can be traced to the WordNet. This should be acknowledged; you can't actually "get rid" of linguistic information inherent in a pretrained Faster RCNN.- "But there are few works addressing this emerging and valuable challenge for language acquisition in a multimodal reasoning view." This paper does not really tackle language acquisition, though? There's a restriction so that the test set has no OOV words, even. I think the claims and presentation of the paper need to be carefully re-scoped.- There is a lot of focus on "learned arithmetic operations" but no analysis as to what exactly this component ends up doing or learning.Nits:- Typo Introduction "a language model that generate" S/V agreement.- Figure 1 doesn't feel like it communicates anything about the method, and does not seem tied to the caption. Are boxes (1, 2, 3) meant to represent the association, reasoning, and inference steps? What's happening in each?- "The results show that ARTNET achieves significant performance improvements in terms of new composition accuracy, over a large-scalevideo dataset." strange wording makes it sound like ARTNet is outperforming a dataset, not a method.- "We train the model to acquire words by directly predicting them." this sounds like the model will be predicting words unseen at training time, which is not so. In particular, "acquire" here sounds like the model will be exposed to the word at most once (at inference time) and then be able to memorize that exposure in sequence.- Typo? In 3.1 "by running faster R-CNN too" what is the "too" pointing to? Do you run Faster-RCNN somewhere?- Typo 3.3 "stringest baseline" strictest?- Figures 4 and 5 are so close together their captions bleed together and are really difficult to disentangle.- Typo "than our baselines 86.5" makes it sound like the baselines achieved 86.5.- Typo 4 "the goal of learned model" missing "the" This paper analyzes functioning of BERT by identifying gradient-based influence paths to track flow of influence between model inputs and outputs. Such analysis using influence paths has been established in prior work, but the present paper expands on this work with definition of "multi-partite" patterns, and by introducing a method for identifying strongly influential paths in the model. The authors evaluate on existing datasets for studying subject-verb agreement and reflexive anaphora, and they report measures of concentration of the influence flows, and performance of the model after compression based on the identified influence paths.Overall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. The reported results don't give us much to go on in terms of interpreting what we have learned from these analyses. The authors choose two measures: "concentration" of the influence paths, and drop in accuracy when compressing the model based on those paths. I'm not sure what we should be taking away from the concentration numbers, and the authors don't provide any substantive discussion of why this matters. There is also no meaningful baseline against which to compare these numbers. The second measure, accuracy of the compressed model, is a bit easier to interpret, in that it allows us to verify the extent to which critical information flow is indeed occurring within the identified paths, but while this verification serves to increase confidence in the method, it's not clear what it tells us about the functioning of the BERT model. (The authors also do not give a clear statement of what task these accuracies are in fact for, though I assume by default that they are referring to accuracy in assigning higher probability to the grammatical option over the ungrammatical option in the selected Marvin & Linzen datasets.) There is also no discussion of how we should interpret results from the attention-based versus the embedding-based influence paths.Zooming out further, there isn't a clearly identifiable question being asked in this paper, and in particular there is no clear connection between the analysis method being used and the particular linguistic phenomena embodied by the chosen evaluation data. What question is being asked about the model's handling of these linguistic phenomena, and why is this method appropriate for asking it? What output of the analysis will be used as an answer to the key questions? These connections should be defined clearly and from the start. As it is, I'm unclear on what these results tell us about the chosen linguistic phenomena (or, conversely, why this set of phenomena was chosen to showcase utility of this method). The measures reported have some variation between sentence types -- but what does it mean that there is higher positive embedding influence concentration for sentences with subject relative clauses, or higher negative embedding influence concentration for within sentence complement sentences? What is the meaning of the accuracies dropping more for subject relative clauses and number agreement among the embedding influence paths, and all accuracies seemingly dropping for the attention influence paths?  In Section 4.2 there are a couple of observations about the influence paths that make some concrete connections to potential questions about syntactic dependencies in the chosen datasets. However, because no formal connection between these things has been established prior to this point (no clear question, no clear linking hypothesis between the analysis and the phenomena) this set of observations comes a bit out of the blue, and leaves us still without clear takeaways. In sum, I think that there is potential for interesting insights about these phenomena to come out of this analysis method, but the paper would benefit from much more clearly defined questions and linking assumptions. ###############################################################################SummaryThe paper presents a novel method for learning branching strategies within branch-and-bound solvers, which consists in a graph-convolutional network (GCN) combined with a novelty-search evolutionary strategy (NS-ES) for training, and a new representation of B&B trees for computing novelty scores. The paper claims this new method provides a significant improvement over state-of-the-art branching strategies, either based on expert-designed rules or on imitation learning of strong branching.###############################################################################Pros and consPros: - the idea of learning branching strategies using reinforcement learning instead of imitation learning makes a lot of sense and seems like a promising direction to follow - the proposed representation of B&B trees is original - the presented results seem promisingCons: - the experimental setup followed in the paper is questionable - the presented experimental results are inconsistent with the literature, which I find suspicious - the overall method description comprises several blind spots and fallacious arguments###############################################################################RecommendationWhile I like the idea pursued in the paper, and I believe the proposed method might be promising, the paper is mostly experimental, and is not sound enough on that side. I therefore recommend rejection of the paper.First, I found the reported experimental results suspicious. The paper is mostly based on the work from Gasse et al., reuses the same code, (almost) the same problem benchmarks, and some portions of text which are identical. This is fine, but then why did the authors conduct experiments only on 3 out of the 4 available benchmarks ? And why are the results inconsistent with those reported in the original paper ? In the presented experiment the SVM model consistently performs better than the GCN model in terms of number of nodes. Why is it so, if literally the same code has been reused, as mentioned in the appendix ? This alone raises serious doubt in my mind about the validity of the reported numbers. At the very least the authors must provide an explanation for that. Also, Figure 4 indicates that a pre-trained GCN model results in a tree size of 350 on independent set problems, which does not coincide with the number 418 reported in Table 1.Second, the experimental setup itself is questionable. The authors consider two setups for the SCIP solver, "clean", with depth-first-search node selection and all other functions disabled (whatever that means), and "default". No explanation is given as to why those two setups are considered. Some experiments are conducted under the "clean" setup (Table 1, Figure 2 and 3), other are made under the "default" setup (Table 2), and for some experiments this information is missing (Figure 4). What is the purpose of the "clean" setup ? It is known that node selection and branching strategies do interact with each other, and all the evaluated branching methods were proposed in the context of a default, state-of-the-art solver. Why then comparing such methods in the "clean" setting ? What is the justification here ? Also, the paper lacks a proper ablation study. How much of the reported improvements come from the policy model ? From the RL training ? From the proposed novelty measure ? Third, I found several arguments to be fallacious. The authors claim their method results in non-myopic policies, without defining what they mean by that. Assuming they mean the policies have access to non-local information, they later on contradict this claim, by acknowledging that the proposed model only processes the local node's LP-based information. The authors also argue that imitating strong branching (SB) is not a good idea, as it results in small trees not because of its branching decisions, but because of side-effects. To back-up their claim, the authors present numbers showing that, once those side effects are removed, the SB tree size is much higher. But in the same table, the GCN model trained to imitate SB results in trees much smaller than the expert (418 vs 1304 on independent set), which again is suspicious, and most importantly contradicts the original claim of the authors. Given those numbers, imitating SB looks like a good idea.Finally, the proposed method comprises blind spots. The authors present their model as a primal-dual iteration policy, for what looks like a simplified GCN architecture for the one from Gasse et al. The same structure (bipartite graph) and the same features are used. The authors present a Lagrangian dual LP formulation in Equation 4, however I do not see the connection to the proposed model. How is the proposed model a primal-dual iteration policy ? The authors then present what is I believe the most original contribution of the paper, a distance metric for branch-and-bound trees. However, again I found the description of the method rather sloppy. Counting the number of integral points inside a polytope is a hard problem in itself. Is the counting performed over box relaxations only ? If so, how can the proposed representation process MILPs with unbounded variables ? This seems to be a strong limitation to me, which at least deserves a discussion.###############################################################################Questions to authorsI would appreciate if the authors could clarify why they conduct some of their experiments in the "clean" setting, why they don't report results on the complete benchmark from Gasse et al., and also comment on the performance of SVM vs GCN, which contradicts what is reported in Gasse et al.###############################################################################Additional feedbackp.2 §2: a linear programming -> a linear programp.2 §2: efficiency-effectiveness trade-off -> What do you mean by efficient and effective ? What is the difference ? Do you mean the decision quality / computation time trade-off ?p.2 §3: ignores the redundant information and makes high-quality decisions on the fly -> This is quite vague. How does your policy achieve that ? What redundant information are you talking about ? What is the efficiency-effectiveness trade-off ?p.2 §3: For exploration, we introduce a new representation of the B&B solving process -> How is this new representation related to exploration ? I am missing the argument here.p.2 §5: set covering, maximum independent set, capacitated facility location -> In Gasse et al. 2019, to which you compare as a baseline, there is a fourth benchmark, combinatorial auctions. I must say that not reporting experiments on this complete benchmark is suspicious, as I do not see a reason for not doing it.p.2 §4: primal-dual policy -> Where does this name come from ? Why is your policy primal-dual ?p.2 §5: the long-term overestimation of strong branching -> What does that mean ?p.2 §5: based on primal-dual iteration over reduced LP relaxation -> What does that mean ?p.3 §2: in line 2 -> line 3 I believep.3 §2: visulization -> visualizationp.3 Equation 3: I am not sure I understand this formula. I suppose that $\phi_t=(\theta_t,\sigma_t)$ ? Then $\sigma_t$ is never updated, but only $\theta_t$ is ? What is $\epsilon_i$ here ? I suggest that you clarify the meaning of this equation, which I could understand only after reading Wierstra et al. 2014, Natural Evolution Strategies. Making it explicit what it corresponds to (expected gradient $\frac{\delta R(\theta)}{\delta \theta}$ over the population) would greatly help the reader. At the very minimum, all terms should be defined properly.p.3 §4: if strong branching finds [...] -> I hardly understand that sentence. Do you mean that strong branching will induce additional domain propagation (Achterberg 2007, Constraint Integer Programming, §2.3) ?p.4 §2: we set the reward $r_t = -1$ with discount $\gamma = 1$ -> Maximizing such a reward is equivalent to minimizing the B&B tree size. And as a result it aligns with your evaluation metric (nodes). This should be mentioned, again, for clarity.p.4 Section 4.2: Primal Dual POlicy Net -> I don't really understand this primal-dual thing, given that you use the same features as in Gasse et al. (Table 3) and just replace their GCN model by a simpler, underparameterized version (A.2.2).p.4 Equation 4: I do not see the point of introducing a Lagrangian relaxation here. Neither Equation (4) nor $\lambda$ are ever used or referred to in the text.p.4 §4: $f_\mathcal{C}, f_\mathcal{C}$ -> $f_\mathcal{C}, f_\mathcal{V}$p.4 §4: one layer -> one hidden layer ?p.4 §4: For efficiency, we do not include problem set $S$, which makes it a partial observable MDP -> Then your model only has access to local information, and as such results in myopic policies, which contradict one of your initial claims. Myopic by definition means your vision is limited, which is the case here if your model can not observe the full state of the solver but only the local LP.  Unless you mean something else with "myopic", which does not have a formal definition in the paper. I think you confuse myopic policies with greedy policies.p.5 §1: as the collection of those leaf subproblems -> Which leaf subproblems ? The leaves of the complete B&B tree ? The partial tree ?p.5 §2: $w(R_i):= \dots$ -> I understand $Q$ is the original MILP, while $R_i$ is a box. $Q$ restricted to $R_i$ is therefore a regular MILP as well (the local MILP). Counting the number of feasible solutions for a MILP is I believe an NP-hard problem. How do the authors afford to do that ? Do you mean that you look for integral solutions within $R_i$ ? A second comment here: What if some variables in Q are unbounded ? Your weight function does not seem to handle that case...p.5 §2: the feasible solution -> a feasible solution ?p.5 §3: Notice that a polytope in the set representation is invariant with the generating order [...] -> This sentence is ambiguous, and is simply not true. Branching on $x_1$ then $x_2$, depending on whether it is the left child or the right child which is subsequently branched on, does not yield the same collection of polytopes.p.5 §3: pruning behavior -> What is meant here by pruning behavior ? Node selection ? A common procedure to compare branching strategies, is to removes potential side-effects from node selection by providing the algorithm with the optimal objective value from the start. See, e.g., Gamrath and Schubert, 2017, Measuring the impact of branching rules for mixed-integer programming. In this scenario where the goal of B&B is just to close the dual gap, the branching strategy remains a major component of B&B.  This seems to contradict your point here, that your novelty measure is both relevant for branching, while it is mostly driven by the pruning behavior.p.6 §3: we have two settings -> Why having those two settings ? What is the point of the "clean" setting ? Also, I think you mean depth-first-search, not deep-first-search.p.6 §7: under clean setting -> Why under that setting ? Would the "default" setting be more representative for evaluating the performance of branching strategies ? It is known that branching interacts a lot with node selection, which you arbitrarily changed here to depth-first-search. Is there a reason for that ?p.7 Table 1: I suggest that you group FSB and RPB together, since they can not be compared to the other methods in terms of number of nodes (unfair node counting). Also, the RPB $N_avg$ value should not be bolted, since RPB can not be compared to here in terms of nodes.p.7 §1: under the default setting -> Why do you suddenly switch to the "default" setting ?p.7 Table 2: Those numbers are very doubtful. Why does the GCN model result here in larger trees than the SVM model, while the opposite is observed in Gasse et al. ? This should, at the very least, be discussed.p.7 §3: founded -> foundp.8 §2: Then, we check dual value $\hat{c}$ -> If you want to assess the capacity of each strategy for closing the dual gap, why not simply reporting the evolution of $\hat{c}$ over time ?p.8 §2: our RL agent successfully employs a non-myopic policy to maximize $\hat{c}$ in the long term -> I do not see that from the curves... I only see that the tree size is smaller, and distributed differently that with the other methods.p.8 Section 5.5: This ablation study is missing a key ingredient: what is the performance of the PD model, trained via imitation learning ? Which part of your improvements comes from your PD model ? Which part comes from RL ?p.8 Figure 4: Why don't those numbers align with those in Table 1 ? Is the ablation study conducted in the "clean" setting ? Why do all curves in Figure 4 start at 350 nodes, when the optimal GCN model in Table 1 is at 418 nodes ? This paper is about representing functions $\psi : (\mathbb{R}^d)^n \rightarrow \mathbb{R}$ that are symmetric or asymmetric with respect to the permutation group $S_n$.  The aim is to consider neural networks giving only functions that symmetric or asymmetric, and to establish universality results.  The motivation comes from applications such quantum physics or computer vision with permutation symmetries. Honestly, I am very unconvinced by this paper.  In particular, the representation for symmetric functions is essentially due to Newton ($d=1$) and to Weyl ($d > 1$).  On the other hand, the representation for asymmetric functions -- dressed up in elaborate notation and the language of Slater determinants -- seems to just be the statement that an asymmetric function is divisible by the Vandermonde determinant (when $d=1$).  Maybe there are interesting results in this paper, but the writing is so informal and apparently rushed, I simply could not tell.  Itemized comments follow.Page 1, Footnote 4, terminology: I would suggest just sticking to one of covariant or equivariant throughout the paperPage 1, Definition 1, notation: Although I understand the authors are using Matlab notation when they write $\{1 : n \}$ is short for $\{1, \ldots, n\}$, it would be more standard to abbreviate $[n] := \{1, \ldots, n\}$.Page 2, usage of $d$, notation: Please define the meaning of $d$ before referring to this notation.Page 3: Functions on sets of fixed size n are equivalent to symmetric functions in n variables.  Isnt it symmetric functions in n variables restricted to the domain where no two variables take the same value?Page 4: Instead of averaging, the minimum or maximum or median or many other compositions wouldalso work, but the average has the advantage that smooth  lead to smooth  and  , and moregeneral, preserves many desirable properties such as (Lipschitz/absolute/...) continuity, (k-times)differentiability, analyticity, etc.  Could the authors expand on what they mean by minimum, maximum, median or other compositions to (anti)symmetrize functions?  Never heard of that.  Also, it should be remarked that the averaging operator is an orthogonal linear projection onto the subspace of (anti)symmetric functions known as the Reynolds operator in classical group invariant theory; in particular, averaging has the property that it is the identity applied to the functions that are already (anti)symmetric.Page 4, Footnote 6: which induces uniform convergence on compacta. I had to google compacta to find out it is the plural of compact subset or compact metric space. Suffice to say this word is not widely used.Page 4:  Do you mean $\mathcal{G}_{func} := \{g: \mathbb{R}^m \rightarrow \mathbb{R}\}$?  Also could you say more about how the basis templates $\eta_b$ are coming in?Page 5: It is stated that computing with elementary symmetric polynomials is numerically more stable than with power sums.  Why?  Also why do we need at least $m \geq n$ functional bases for a continuous representation?Page 5:  Every continuous AS function can be approximated/represented by a finite/infinite linear combination of such [Slater] determinants: Why?Page 6, Theorem 3, theorem statement: Thus you have reduced the representation of an antisymmetric function in $n$ variables to that of $n$ symmetric functions in $n-1$ variables, at the expense of an $n \times n$ determinant.  It is not clear how computationally useful this could be, since symmetric functions in $n-1$ variables are nontrivial computationally to represent already, e.g., Theorem 2s approach would involve $n-1$ power sums.  Page 6, Theorem 3, proof: The actual representation of $\psi({\bf{x}})$ that is constructed in the proof is almost certainly not useful, namely that it is the determinant of the matrix whose rows look like the following:$$\begin{pmatrix}\psi({\bf{x}}) / \Delta({\bf{x}})  & x_1 & x_1^2 & \ldots & x_1^{n-1} \end{pmatrix}$$$$\begin{pmatrix}\psi({\bf{x}}) / \Delta({\bf{x}})  & x_2 & x_2^2 & \ldots & x_2^{n-1} \end{pmatrix}$$etc. (Sorry, can't get multi-rowed matrices to render properly in OpenReview...)Crucial question: why is this a good way of expressing antisymmetric functions, in theory or practice?  The content here is that any asymmetric function is divisible by the Vandermonde determinant.  If the larger $d$ representation results depend on this, I don't know how much they tell me...Page 7: For $d > 1$, the Fermion nodes $\{{\bf X} :  \psi({\bf X}) = 0\}$ form essentially arbitrary $\psi$-dependent unions of (non-linear) manifolds partitioning $\mathbb{R}^{dn}$ into an arbitrary even number of cells of essentially arbitrary topology [Mit07].  This language is too intuitive for me.  What does this sentence mean??Page 7, Theorem 5, proof discussion:  Whether this generalizes to $n > 2$  and $d > 1$  is an open problem.  This is confusing.  Does the theorem have a restriction on $n$ and $d$?Page 78, Equivariant Neural Network subsection: work of Risi Kondor and coauthors needs to be cited here.Page 8, Theorem 6: It would help the reader to include in the body of the paper a more precise statement or diagram showing the construction of the approximating EMLP. # SummaryThis work concerns the metric learning between sequences using RNNs. The paper notices the similarity between a dynamical system and an RNN. Then it demonstrates that learning a pair of siamese RNNs is similar to learning synchronization between two subsystems of a dynamical system. Finally, the paper proposes to introduce coupling between the two RNNs in order to improve synchronization.The paper conducts experiments comparing the siamese GRUs with the proposed couples siamese GRUs.# QualityWhile this is an interesting idea to explore, I am not entirely convinced about its value. The main claim of the paper seems to be that the baseline siamese GRU exhibits chaotic behavior while the proposed architecture does not. The former has to be tested experimentally. I would like to see an experiment that shows this chaotic behevior at least qualitatively. Secondly, the proposed architecture intermixes two sequences. Therefore, it is not possible to learn embeddings.The experimental section is very scarce. The only dataset used in the paper, UCI HAR, is very small. Furthermore, there are not enough benchmarks for the metric learning on this dataset. The results reported here are not comparable with the supervised methods (~90-95%). This work needs more experiments on datasets with several published results.The section 4.2.1 seems to contain a logical error. The linear relation between the norm and the loss cannot show that the coupling helps or hurts the model. Instead, a model with coupling should be compared to a model without coupling. Would it be better to plot separately the norm for positive samples and the negative samples? If I understand correctly, the first is supposed to decrease and the second is supposed to increase. There is more chance to see a negative pair, therefore you observe that the norm increases.The section 4.2.3 aims to test the model for the hard positive samples. I can see several problems with the proposed approach to test this. Firstly, the absolute distance is meaningless here. I propose to normalize both curves. Secondly and most importantly, the average metrics cannot test the performance for the hard positives. Such metrics would be dominated by the majority of "easy positives". Therefore, I request a more formal description of what is a "hard positive" and an experiment  that better tests the claim.# ClarityIn general, the paper is hard to read. The logical flow of the paper is hard to follow. Some important aspects are only referenced in other papers or skipped through. Many typos hinder the understanding sometimes.The introduction seems to claim "an improvement over a classical GRU". In fact, the paper proposes an improvement to the *siamese* GRUs.Despite the fact that the proposed idea is quite simple, it was hard to follow the paper. Sections 3.1 and 3.2 are too general. This creates discrepancy between Sections 3.1-2 and 3.3. I don't see the point of introducing so general description of the synchronization. It would be clearer to define this concept for the specific case at hand.The abuse of notation made Definition 1 look nearly incorrect. I recommend either carefully introduce the specifics here (like in the original paper by Brown & Kocarev) or stick to the common mathematical notation. More specifically:- Definitions for "X", "Y", and "X (resp. Y)"- Definition for g(x)It is easy to loose a thread when reading the section 3.1. I would recommend to start with the siamese RNNs and then demonstrate the connection to the dynamical system, not other way around.The loss used in the paper has to be explicitly spelled out (perhaps, in the Appendix).# OriginalityThe paper is sufficiently original.# SignificanceThe paper has a potential to be have high significance. Unfortunately, the experimentation is too limited. The claims made in the paper are not sufficiently tested. Then, the performance is tested only on one dataset against a weak baseline. The presentation needs to be improved too (as noted above).# ConclusionThe idea is interesting, but the experimentation is very weak.Specific requests for improvement:- Experiments on more datasets- Compare to the published metric learning literature- Test the claim that the baseline is chaotic- Define hard positive samples and test the model on them- Improve Definition 1- Conduct ablation experiments# Some of typos- Fix \citep vs \citet- "further lose temporal dependency"- "in term of synchronization"- "system theory, an important result being ..."-  "recent approach <...> studied"- "further introduce"- "h and g are the same as in Equation 1" -- there are no h and g in Equation 1- "RNN are dynamical systems" -> "RNN is.."- "RNN are known" -> "RNNs are" or "RNN is"-  "3 axis" -> "3 axes"- "size of 128" -> "length of 128"- "a initial learning rate" -> "an initial"- "a SGRU" -> "an SGRU" #### Summary:Drawing inspiration from dynamic systems, the paper proposes a novel architecture that couple sequences. Such a system has easiness to bring two instances arbitrarily close and authors have shown the superiority of the approach over an action recognition dataset; but the results seem to far from state of the art on the dataset (see questions section). The authors also recognize that currently such systems need to calculate each pairs (can't be cached due to coupling) at inference time, which is slow. The main issue I found in this paper is its presentation. The authors claim to draw inspirations from dynamic systems (e.g. the important notion of  synchronized trajectories) in the abstract/introduction, cite related work and introduce a formal definition (with some errors, see section minor issues). However, the metric that ends up being used in eq(5) is a metric function of two RNN end states, which is common and wouldnt help to highlight authors contributions.Another concern is that the authors have tested on only one dataset where the claimed results are quite below state of the art on the dataset (see questions section). There are two baselines implemented (one very recent approach), however no simple baseline or other datasets to further support the approach.Finally, after reading the paper, I think the paper proposes a new neural architecture for similarity learning rather than focusing on metric learning. In this aspect, the paper misses references to papers in the area.#### Pros: Drawing inspiration from dynamic systems, the paper proposes a novel architecture that is not only dynamically involved with timesteps but is also coupled (between two instances), which has the capability of bringing two arbitrary sequences close. The authors have shown the superiority of the proposed approach over several metric learning baselines including the recently proposed RVSML (Su and Wu, 2019) over an action recognition dataset.#### Questions:Clarification question: Given the architecture, the methods can only be applied to sequences that have the same number of timesteps, is that correct?The approach seems to be far from the state of art: In Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope [Wong and Kolter, 2018], the baseline network investigated achieves 5% error rate; for papers more focused on performance, Human activity recognition with smartphone sensors using deep learning neural networks [Ronao and Cho 2016] archives accuracy 95.75%. Why is there such a discrepancy between the results in the paper and state of the art results please (as it is more convincing to build upon state of the art results when possible). #### Minor Issues:In Definition 1, the function two subsystems are synchronized if there exists a time-independent g (instead of a time dependent g as written). In fact, one can always find a trivial time dependent g satisfying the equation (by the way, it is written correctly with time independence in Brown & Kocarev (2000))#### Minor suggestions:Modifying neural architecture to capture better similarity has numerous works, one notable example in NLP is for example: A Decomposable Attention Model for Natural Language Inference [Parikh et al. 2016]. I would recommend including similar references in the paper to better situate the papers contribution.The loss function is missing in the paper, so leading some missing details for training. Finally, I want to detail my argument to change the papers highlight on synchronization, which ends being a distance between two RNN end states. First, this is very far from the formal definition in eq(1) with a quite weak link; secondly, it is one of the most commonly used metrics in NLP/CV applications (and very probably other areas that I am much less familiar with). Given that an ICLR main paper is only about 8 pages, I strongly recommend authors to reconsider the paper structure to highlight its main contributions. The paper is to measure each clients contribution to training the federated learning model. In particular, the contribution is measured by the distance between the local model and the global model in each iteration. The targeting problem is interesting, and the use of attention-based model divergence is also an interesting idea to measure the contribution. However, the paper lacks strict theoretical discussion to prove the proposed solution is a reasonable one rather than a heuristic method. Moreover, the experiment is too weak to support the claims. The papers technique contribution and originality are also limited. Below are some detailed concerns.1) The authors need to make a clear definition of the assumed application scenario so that the below problems can be avoided or solved. If the clients contribution is linked to rewards, it is unavoidable that some clients will produce fake data to gain more contribution to the commercial federation system. Therefore, the paper should discuss the prevention of attacking by fake data. For example, if the client randomly shuffles the index of neurons in the trained local model w_k, then the clients local model will get a bigger s_k^l calculated by equation 2. Thus, this client is likely to gain a big reward at every iteration.According to equation 5, the contribution at the early stage will be discounted. It is unfair for the clients to be selected at an early stage. Therefore, from a systematic perspective, some clients may refuse to contribute to the training process at an early stage. 2) Contribution is not enoughThe core method comes from the FedAtt algorithm  an attention-based federated aggregation method. The papers primary contribution relies on section 3.3 to measure the contribution according to the gradients.  3) The experiments are too weak to support their claim. More datasets and baseline methods are required, for example, the FEMNIST, FeCeleba.It is unclear how to define an objective metric to measure the quality of the proposed method. The contribution is a subjective feeling that various to different tasks and assessor. This paper designs an equation, i.e., equation (5) in the paper, to measure the impact or contribution of each participant/agent in federated learning. The designed measurement method is applied to attention aggregation algorithm of federated learning. Few experiments using Penn Treebank are conducted to support its claims.This paper should be rejected because (1) the paper is unpolished and thus is hard to read, (2) the novelty appears quite weak, and (3) the experiments are difficult to understand and generally do not support its contributionsConcerns:The paper is difficult to read due to the poor use of English. Many sentences are incomprehensible. Thus, it was often impossible for me to determine exactly what the authors would like to say or describe. Please have your submission proof-read for English writing style and grammar issues. Moreover, please treat the equations as the parts of sentences and make sure that the caption formats of Figures obey the ICLR format.I also have a serious concern about the novelty of this paper. If my understanding is correct (due to the aforementioned reason), Subsection 3.3 is the only new material proposed by the authors. However, the proposed equation, i.e., equation (5), seems like a design choice without any theoretical justification or providing any intuitive reason, which significantly degrades the novelty of this paper.Finally, the experiments should be refined to support its main claims. As claimed in Section 1, the proposed measurement method is real-time and has low computational complexity. However, no experiment nor quantitative comparison addressing the running time and complexity between the proposed method and Shapley Value. Actually, the authors compared their method with the method of approximating Shapley Value instead of exact Shapley Value. Furthermore, please cite for Shapley Value papers. Detecting anomalies is a notoriously ill-defined problem. The notion of anomaly is not a rigorous concept and different algorithms produce different results. The paper critiques a broad set of methods which involve likelihood (or density) estimations. It's main idea revolves around the 'Principle' set on Page 4. The principle claims that when data capacity and computational constraints are removed, an AD algorithm should be invariant to 'reparametrization' of the input. Roughly speaking, that means the algorithm should be invariant to arbitrary 'name changing' of the input - the result should not change if each data item x is replaced by f(x) if f is invertible. The paper then shows that density models do not satisfy this principle even when they are 'perfect'.My main critique of the paper is that this principal constitutes a completely unreasonable requirement of any AD algorithm, to the point where it is meaningless. It is trivial to observe (as the authors do) that any continuous distribution could be transformed to a uniform distribution with the correct f. Even if the domain is discrete we can make the input uniform: As an example think of f being a Pseudo-Random function, like say f(x) is a digital signature of x. If we believe cryptography then no efficient algorithm would be able to say anything useful. To sum up - I don't think the 'principle' is a useful prism by which to measure models and definitions, and therefore I don't find the contribution of the paper sufficient for publication.As a side remark, this principle may be useful if further constraints are put on f, for instance, if we may want the AD algorithm to be oblivious to unit change in the data, which translates to f which multiplies dimension be a constant.  ## SummaryThe authors propose an alternative method for finding informative latent variables in a model called General Incompressible-flow Networks (GIN). While previous work relied on the variance of the variables to assess informativeness, the authors argue that this is problematic when the scale of noise epsilon is large. They propose instead to use the mutual information between the variables and u instead.The authors evaluate their alternative identification method on two tasks: a toy example of a mixture of gaussians and EMNIST. They show that when the variance of epsilon is large, then their method outperforms the variance based method (figure 3). However when the variance of epsilon is small, then VAR performs similarly to MI (Figure 8).On EMNIST, it's clear that MI is able to identify the most important variables and outperforms the best VAR setting, however when more variables are included the MI method deteriorates quicker than VAR. ## ReviewIn general I find the paper difficult to follow. Many paragraphs have an unclear structure and sentences are not linked. There are also many unsubstantiated, or unclear/imprecise claims. The derivations and equations seem correct and the figures are well made and understandable. Especially figures 1 and 2 are informative and clear.I think the paper would be a lot stronger if it was positioned as "This is a failure case of using the VAR method with GIN, we propose a rigorous solution". Currently the authors make statements such as "Though Sorrenson et al. successfully establish the identifiability, their interpretation on the meaning of this result and the method they propose are incorrect", "it is unplausible to use the variances of the learned representation" and "while the original VAR criterion is not competent at all" - despite clearly showing that the VAR method works well in a number of situations. I think it's worth being more nuanced about alternative methods and explicitly showing when the other method does not behave as described/expected (which you also do!).Could the authors comment on how the MI was computed and in particular the computational trade offs between using MI and VAR to select variables? I'm also curious why the MI method degrades so quickly in figure 6, how did you set up the classification task?How did you "apply defence" in the adversarial defence experiment? You mention that VAR300 is only fooled by 56% of the cases without defence, but this cannot be compared because its "clean accuracy" is lower, could you expand on that? It seems to me that higher accuracy is always better? It seems in contradiction with the claim at the end of the paragraph that "all results indicate that ... features selected by MI criterion are more reliable."In general, a lot of the experiments seemed to be based on Sorrenson et al, but are not properly described in the paper. I would like to see at least a basic description.In summary, I think the authors have identified an interesting improved that is worth publishing. However I think that the paper in its current form is not ready: it can benefit from significant rewriting for clarity (both in structure, as well as spelling) and further analysis of the behaviour in figure 6.## Notes:"Incompressive" --> incompressible  "in nonlinear independent analysis theory" - missing "component"?  W_{d+1:n} should be W_{n+1:d} (below equation 4)?  "unplausible" -> "Implausible"?  "interpretation on" -> "interpretation of"There's a number of other, non-existent, words in the paper that will be caught by a spellcheck. This paper presents a new method to defend black-box attack based on an ensemble of sign activation neural networks. The authors demonstrate their method has much higher minimum distortion using HopSkipJump attack.However, I have many concerns regarding this paper:-The paper organization is very weird and confusing. The author did not put their main algorithm into the main paper. Instead, they put many unimportant results (e.g. Table 1) into the main paper. If there is not enough space the author should use simpler sentences to describe their algorithm and put some results into supp. Also, the figure and table style (i.e., unbounded table, screenshot figures) makes me feel this is an undergrad project report instead of an ICLR submission.-Besides transfer attack, the authors only evaluate the black-block robustness using HopSkipJump attack. Their claim is "Compared to other boundary attack methods it is known to give the best estimate of a datapoints minimum adversarial distortion." Is there any paper support this claim? I don't believe one attack method is universally better than other method among all datasets.I think the authors should evaluate a set of attack methods instead of only one method otherwise the results are not convincing to me.-What is the purpose of providing detailed results of 10 random images in Table 2, 3? Those results not only occupied a lot of space but also did not provide any useful insight. An average number of the entire dataset is enough.-Citing issue: when referencing a paper the author should use the published version not the arxiv version if the cited paper is published.E.g., Angus Galloway, Graham W Taylor, and Medhat Moussa. Attacking binarized neural networks. arXiv preprint arXiv:1711.00449, 2017.should beGalloway, Angus, Graham W. Taylor, and Medhat Moussa. Attacking Binarized Neural Networks. International Conference on Learning Representations. 2018.-The baseline comparison are all undefended networks. The author should compare to some other blackbox defense methods.  This paper proposes an MPC algorithm based on a learned (neural network) Lyapunov function. In particular, they learn both the Lyapunov function and the forward model of the dynamics, and then control the system using an MPC with respect to these models.Cons- Poorly written- Unclear connections to related work- Weak experimentsIt is unclear exactly what problem the authors are attempting to solve. In general, the authors introduce a large amount of notation and theory, but very little of it appears to be directly related to their algorithm. For example, they refer to the stability guarantees afforded by Lyapunov functions, but as far as I can tell, they never prove that their algorithm actually learns a Lyapunov function (indeed, Lemma 1 starts with Assume that V(x) satisfies (5) [the Lyapunov condition] ...).Similarly, they allude to robustness margins to model errors, but nothing in the algorithm actually takes into account model errors. Is the point of these margins just to show that they exist? If so, its not clear the results (either theoretical or empirical) are very meaningful, given that they depend on the unknown model error (which they assume to be bounded).In addition, the different loss functions they use (e.g., (10)) are poorly justified. Why is this loss the right one to use to learn a Lyapunov function?Furthermore, the authors approach is closely related to learning the value function and planning over some horizon using the value function as the terminal cost (indeed, the value function is a valid Lyapunov function, but not necessarily vice versa). For instance;Buckman et al., Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. In NeurIPS, 2018.The most closely related work Im aware of is the following:Deits et al., Lvis: Learning from value function intervals for contact-aware robot controllers. In ICRA, 2019.The authors should clarify their contributions with respect to these papers. More importantly, the authors should discuss their motivation for indirectly learning a Lyapunov function instead of simply learning the value function (which appears to be more natural and potentially more effective).Next, the authors experiments are very weak. They only consider two environments, the inverted pendulum and car, both of which are very simple. The inverted pendulum starts near the unstable equilibrium, which further trivializes the problem. In addition, they do not even appear to give the dynamics model of the car they are using (or the state space).Finally, this paper is poorly written and hard to follow. They provide a lot of definitions and equations without sufficient explanation or justification, and introduce a lot of terminology without giving sufficient background. This paper proposes to extract interpretable rules from a learned neural network. The authors claim that they are the first to propose rules connecting 1) multiple neurons together, and 2) do this at a dataset level. Their approach relies on using minimum description length and well known principles from the data mining community (e.g., downward closure lemma of apriori algorithm). The authors claim that experiments conducted on image data shows that their approach leads to more faithful, interpretable rules than other approaches such as prototyping or model distillation.Compiling rules from neural networks has been proposed before (e.g., see "Deep Logic Networks: Inserting and Extracting Knowledge from Deep Belief Networks" by Tran and d'Avila Garcez, 2018). I think the authors need to compare against such previous approaches to quantitatively show how their work extracts better rules. Otherwise, its difficult to appreciate the value of ExplaiNN. Also, the paper doesn't say anything about how faithful the rules are to the learned neural network. I mean, it seems possible that for some input the rules could produce a different output from the neural networks. There exist other works that also try to interpret neural networks consisting of affine layers with Relu activation that guarantee consistency (see "Exact and Consistent Interpretation for Piecewise Linear Neural Networks: A Closed Form Solution" by Chu et al in KDD'18). The authors should at the very least compare and contrast with such works to highlight the pros and cons. Another undesirable property of ExplainNN seems to be that it relies on a dataset to derive its rules. Is it possible that when run with a different dataset and the same learned network, ExplaiNN would produce a different set of rules? Then how much faith do we place on ExplaiNN's output?Writing wise, the paper is presented well enough. There's a few paragraphs in the Experiments section where the authors point repeatedly to the Appendices. In the best case this makes reading a chore. I would advise the authors to refrain from using the main body of the paper as a listing of contents and simply pointing to the appendices. The pictures in the experiments section were difficult to make out. I couldn't figure out from the image whether the husky's pointed snout had been identified as a defining feature. I would hope the authors find more compelling ways to make their point. Summary of paper:This paper proposes a GAN training method that involves keeping a table of historical losses for the generator and discriminator across training iterations and calculating the loss using a double-oracle framework inspired by game theory.Strengths:-- To my knowledge, this is a novel approach which uses new ideas in a related field to potentially help the unstable training of GANs.-- The background preliminaries are thoroughly explained, which is important, as much of this might be new to the target audience.-- The idea of computing losses across iterations to save on the total number of iterations that must be performed is a good one given the high computation costs of GAN training.Weaknesses:-- I found this paper to be poorly written and hard to understand. The game theory language used here is not standard GAN terminology and thus made it difficult for me to follow. -- If model weights themselves are compared/combined across iterations, for example, took me several read-throughs to understand because of unnecessary terminology like calling them "policies" and "meta-game" which are not used in work on GAN training.-- Not all GAN loss functions are zero-sum (in fact the best performing models e.g. Big-GAN do not use a zero-sum loss), but as I see that is required for this approach, or at least is the only thing  that is considered.-- Loss values themselves are not necessarily indicative of generative performance. There are stable equilibria that do not yield good generation, and a different equilibrium at a lower value may or may not be better. As such, only so much can be done with historical loss values.-- I don't understand how this is supposed to help mode collapse, which is claimed as a beneficial result several times in the results section. To combat mode collapse, it will have to change the way gradients are differentiated for different points. This seems prima facie unrelated to the proposed method, and if it really is causally related, that would be valuable but this needs to be investigated with deeper experimental analysis for it to be claimed.-- The experiments are weak. The toy example is entirely vacuous, as getting a regular GAN to match a handful of modes in 2D is quite possible with any number of small tricks that have been around for years and are not onerous.-- The natural image results are only on CIFAR and CelebA, and are low quality at that. Qualitatively, the generated images still look bad, and the use of a DCGAN as the main baseline is misleading as the vanilla DCGAN is nobody's standard to beat anymore. Models that have achieved state-of-the-art results in the last couple years like Big-GAN should be included.Conclusion: The motivation for this particular meta-game strategy is not very clear, and the results are not good enough to be relevant to where the current state of GANs are, and as such I vote to reject. This paper studies the top singular vector of the feature space learned by supervised and unsupervised deep learning models on CIFAR datasets. The hypothesis of converging feature spaces is interesting (converging both in terms of different models, and in terms of training epochs), but the conclusion from the current experiment results is overstretching.1. While the authors emphasize the convergence of subspaces, the P-vector defined in the paper is actually the top singular vector of the feature space, so it's actually about the convergence of the 1-dimensional principal subspace. A subspace refers to an arbitrary dimensional space in general. In the context of SVD, the literature often studies the top-$k$ dimensional subspace, which is represented by the $k$ top singular vectors, and the approximation error of the top-$k$ dimensional subspace: $E=\|X - U_k \Sigma_k V_k^T\|_F^2$, where $X$ would be the feature matrix in this paper, and $U_k, V_k$ are the first $k$ columns in the result of SVD. The authors didn't measure $E$, so the readers won't know how well the top-1 dimensional subspace represents the feature matrix. I recommend looking at $E$ as a function of $k$, and use some criteria to determine how closely you want the subspace to approximate the feature matrix. For example, we can say we want to keep the top-$k$ dimensional subspace such that $E < 0.1 \|X\|_F^2$. This way, you can rule out the possibility that the P-vector is a trivial vector that every model will converge to.(As an analogy for a trivial vector, we can consider the top-1 eigenvector of the similarity matrix defined in the classical spectral clustering method called Normalized Cut. No matter how the edge weights in a graph is defined, the similarity matrix used in Normalized Cut always has an all-one vector as the top-1 eigenvector.)And to measure the angle between general subspaces, many methods are available including classical ones (e.g. Åke Björck and Gene H. Golub, Numerical Methods for Computing Angles Between Linear Subspaces, 1973).2. This paper tries to emphasize the P-vectors found in the features from different deep learning models are very close (for example, "no matter what type of DNN architectures or whether the labels have been used to train the models, the P-vectors of different models would converge to the same one"). Actually it seems the angle typically converges to 10 to 20 degrees. It may be better to lower the tone, or quantify better (compared to the angles obtained by ..., the angles between P-vectors are smaller).3. The data in Fig. 7 looks quite noisy, though p-value shows statistical significance of the correlation. p-value can guide our findings but is not always meaningful. For example, comparing Fig.7(e) and Fig.7(l), we may argue the latter has a better correlation but the former has a much smaller p-value. It seems the very small p-value in Fig.7(e) results from some outliers. Intuitively I don't quite understand why the raw data and the features should have a correlated linear principal subspace, given that the neural network layers that generate the feature from the data are highly nonlinear.The only convincing data I found is in Table 1, which shows P-vectors can serve as an indicator of the model performance. But overall the readers would need more evidence as explained in #1 above. This paper proposes a data augmentation scheme (named PAD) to improve accuracy and calibration of NNs. The idea is to generate OOD data, close to the training data, where the model is overconfident, and force a higher entropy for their corresponding predictions.This topic is very relevant to the ICLR community, the paper is clear, and I was excited with the goal in a first place. However, the paper as it is has major drawbacks.1. The biggest drawback is that the proposed approach is ad-hoc, a heuristic with no guarantees that it will work as desired. In fact, recent work has shown that Data augmentation on top of Ensembles can be harmful, the authors should discuss this in the paper (see [Wen et.al, 2020: Combining Ensembles and Data Augmentation can Harm your Calibration). For this paper to be accepted, the authors should explore the properties of the proposed approach with careful controlled toy scenarios, and bring further insights on when the approach is expected (ideally, guaranteed) to work. 2. Using PAD on top of other probabilistic approaches destroys the probabilistic interpretation.3. Experimental results are extensive, but not convincing: Figure 1 lacks the GP reference, and shows bad performance on the left extreme; the Ablation study suggest that Equation (5) could be simplified; finally results in Table 2-4 suggest that the proposed approach hurts in high-dimensional scenarios (Energy and Kin8nm datasets), the reported numbers also strongly depend on model selection and tuning of PAD and other baselines, information which is currently missing.4. The authors do not compare nor mention recent advances on calibrating DNNs, for example:* (Antoran et.al, 2020) Depth Uncertainty in Neural Networks* (Liu et.al, 2019) Simple and principled uncertainty estimation with deterministic deep learning via distance awarenessMore comments:* the proposed model does not seem to scale to high-dimensions, as "filling the gaps" with the OOD data generator becomes infeasible (this is reflected in the tables, where both accuracy and calibration are systematically worse for the high-dim Kin8nm dataset). Up to how many dimensions would this approach be useful?* the OOD dataset produce an "equally sized pseudo dataset". Yet, one might think that the amount of data needed to robustify uncertainty would depend on the manifold geometry.* location of OOD samples is chosen as an interpolation of latent representations for the observed data. That means that many generated datapoints will NOT bee out-of-sample.* From the ablation study (Tables 4 and 5), "without AB" gives similar results to Regular (always within the reported error bars of "regular". That seems to indicate that terms A and B are not that relevant. Am I missing something?* Figure 1: the authors should include one column for the GP behavior, since the authors claim that the observed behavior is similar to that. Otherwise, it is unclear by eye what is best. In particular, PAD* Could the proposed approach suffer from the opposite issue, i.e., deliver too high uncertainty in the augmented OOD data? How do you avoid this issue?* How does the proposed approach compare to a DNN whose last layer is GP or Bayesian RBF network? (see http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-009.pdf)* The proposed method encourages a reversion to a *specific* prior (0 mean functions)Minor:* the authors mention limited expressiveness of GPs, but this is subject to a simple kernel. If the kernel is complicated enough, then GPs are as expressive as we would like to (see equivalences between DNN and GPs in [Neil, 1997] and [Lee et.al, 2017]). Please clarify this statement.* Figure 3 is hard to read, I suggest to highlight the PAD curves by changing the color scheme). This works aims at task-oriented fine-tuning from pre-trained ImageNet models. It proposes a Neural Architecture Search and Online Adaption framework (NASOA) to perform fast task-oriented model fine-tuning. The NASOA first employ an offline NAS to select a group of models and then pick up the most suitable model from this group via an online schedule generator. Weaknesses:The proposed method may be useful in some fine-tuning scenarios, however will have low overall impact. From the reviewer's view, lack of novel and interesting part. The proposed approach simply uses and combines existing methods, e.g., NAS, seems more like an engineering project. Summary:This paper proposes a method to do medical entity extraction from HER data by fine-tuning a transformer model pretrained on a large EHR dataset.  The model combines a two-step process of NER and NEN into a single step on a multi-label classification task by distantly supervised training. The main contribution of this paper is to exploit a single transformer model to perform NER and NEN for HER data simultaneously by using the representation of EHR for a single multi-label classification task.   Empirical studies are performed to show the expected recall.Pros: 1.The paper introduces distant annotation label data to avoid domain-expert costly annotations.2.Apply pre-trained transformer-based model to finetune the proposed model in EHR data to do medical EE.3.Large EHR data pre-processing Cons:1.The paper does not give the performance comparison with the state-of-the-art EE model.2.The originality and significance of this paper is not enough, as it applies RuBERT on EHR data to do medical EE.3.It is better to give additional evaluation metrics, such as precision and F score.Minor comments: 1.In the last sentence of the second paragraph in Related work, it should be " into NER and NEN tasks. " instead of " into NER an NEN tasks. ". 2.Brief introduction about RuBERT in model section.3.There are some typos and grammatic error.  Summary:The paper deals with offline aka batch RL for discrete actions. Three techniques ((i) behavior value estimation, (ii) ranking regularization, and (iii) reparametrization of the value function), which can be combined with each other, are presented. These techniques are compared with other methods in different experiments. Furthermore a new benchmark is being introduced.  It is claimed that in this new benchmark, the new techniques outperform state-of-the-art methods. Furthermore it is claimed that the presented method behavior value estimation, although it is only a one-step greedy optimization is typically already sufficient for dramatic gains.Strong points:The abstract and the first part of the introduction (the first 1.5 pages) are very well written and the problems of offline RL are very well presented. Also very good is the consideration that the existence of a behavior policy is a restriction that does not apply to every given dataset, as expressed in the terms "behavior policy(s)" and "coherent policy". However, it is not specified in the text what exactly is meant by "coherent policy".Weak points:The representation becomes increasingly unclear from page 2 onwards. None of the three techniques presented is sufficiently discussed and sufficiently tested. None of the statements is supported convincingly, although the paper already makes extensive use of references to the Appendix. There are 14 references in the main text to the Appendix and four to figures in the Appendix.Recommendation:In its current form, the experimental part of the paper is immature. A uniform structure is missing. The statements are not sufficiently substantiated. Therefore I recommend to reject the paper. It seems that there is not enough space to present and sufficiently verify all three techniques.The claim "this one step is typically sufficient for dramatic gains as we show in our experiments (see for example Fig. 9)" is not sufficiently substantiated, because one cannot speak of "typically", as only "Atari online policy selection games" are considered. And additionally Fig. 9 is located in the Appendix.The meaning of the error bars in Fig. 3 and Fig. 7 is not explained.The (on first sight) counterintuitive result that the performance of CQL and QRr at cart-pole is higher at 40% noise than at 0% must be explained in the text or caption.Because the presented ideas are not sufficiently examined and supported, no sound knowledge is generated on which the reader can rely.Questions:What is the meaning of error bars in Fig. 3 and Fig. 7?What is meant by "BC"?Additional feedback with the aim to improve the paper:The abbreviation BC is not introduced. It is unclear what is meant by it.It is unclear what is meant by "coherent policy.What is meant by "discrete offline RL algorithms?it is not clearly described that discrete actions are required.In the sum, i runs from 0 to 100 and is divided by 100, but from 0 to 100 we count 101 (unfortunately there are no line numbers in the manuscript, to locate the sum)."5e - 2" does not look nice, better is 0.05 or $5 \cdot 10^{-2}$.In Figure 8, the measurement results should not be connected by lines. Lines should only be used for fits or predictions of theory.In Appendix F the text width is not respected.Please do not use \pm for the standard deviation, but for the specification of the uncertainty (aka error of the measurement) e.g. the standard error. In this paper, the authors propose generative domain adaptation approach called EMTL. The key idea is to model a mediator distribution which can approximate the true target joint distribution. Specifically, the authors apply an E-M strategy to infer the model parameters. Experimental studies are done on both synthetic and real-world datasets. The paper is well-organized and easy to follow. My major concern is on the significance of the paper, which is not significantly novel especially compared with the recent DNN-based domain adaptation methods. Moreover, there are some technical flaws, which need to be further clarified. Here are the detailed comments:(1)One motivation of the paper is the sensitivity of the source data. Due to security or privacy issues, source data may not be accessible. While it is a practical and nice point of motivation, the paper misses one important research line on federated learning that is specially proposed for privacy issues. It is necessary to discuss with some federated learning related works. (2)Regarding the access of the source data, the proposed method still requires the source data to do source density estimation. In this sense, I am not convinced by the claim on the privacy preservation. From Algorithm 1, it can be clearly seen that D^s is still used for the initialization of \theta_s. (3)The authors highlight no access of source data in the adaptation phase. Could you elaborate on what is the adaptation phase? Based on my understanding, the whole Algorithm 1 is for adaptation, but it still needs source data. Is it better to claim that the proposed method only requires source model parameters trained previously? (4)The related work section can be further improved, by discussing more on both subspace-based and deep-learning based domain adaptation methods. (5)The key idea is based on Theorem 1, and aims to build a mediator distribution to approximate the target joint distribution. Most of existing domain adaptation methods share the same idea although they are not generative models, please highlight the main advantage of proposed generative model over existing subspace-based and deep-learning based methods. (6)Regarding Eq. (5), why \theta_s is used in the subscription of \mathbb{E}? It should be \thetha_m^(0), right? Moreover, how to obtain y_i for each target data point? (7)For Eq. (5), does it only hold for the first iteration where the source parameters are used as the initialization? For the following iterations, are you still using p^s(y_i = j | x_i^t)? or using p^m(y|x)? If the latter is used, does it mean y_i is updated in each iteration? (8)The proposed synthetic dataset is very naïve. It is more convincing to test on more complex datasets with higher dimensionality data.  For the real-world datasets, there are a lot of benchmark datasets for domain adaptation, e.g., office 31, office-caltech 10, and office-home etc. It is more convincing to test on these well-known datasets. More importantly, please compare with more state-of-the-art baselines, on both subspace-based and deep learning based. Even on the reported datasets, the improvements of EMTL over SA and DANN (these 2 are not state-of-the-art) are marginal.  Summary:This paper proposed a generative domain adaptation (DA) approach under covariate shift. Different from previous domain discriminator methods, this paper introduced a mediator distribution and adopted an autoregressive approach (RNADE) to estimate the distribution density. Empirical results on simple datasets (UCI and Amazon) verified its practical benefits.------------------------------------------------------Overall review Pros:[1] As far as I know, this is the first paper that used the autoregressive approach in DA. [2] The proposed adaptation algorithm does not require accessing the source data at the adaptation phase, which has some practical potential.[3] The high-level idea seems logical and correct (But some technical details seem problematic.)Cons:[1] The motivation of the proposed approach is unclear: it seems a simple plug-in approach with RNADE in DA. A thorough analysis is lacking.[2] The empirical significance of the paper is rather limited: the paper did not effectively show its practical utility.[3]  Some technical details are difficult to follow or flawful. Based on these reasons, I recommend rejection.--------------------------------------------------Detailed explanations[1] MotivationI am rather confused about the motivation of the proposed approach. As for the generative model, the particular reason to use RNADE is unclear. Is the discriminator unable to solve the source-target separation issue? An alternative approach is to train a model on source only and apply the unlabelled target for fine-tuning. (see recent paper [1]). Discussion on the benefits of these settings is highly expected. [2] ExperimentsSince it is an empirical paper, I am most concerned about the empirical results.[a] The current results are rather limited. The author only evaluated on UCI and Amazon review dataset. Both are simple datasets and linear models can achieve good results.[b] The compared baselines are NOT SOTA. DANN is the standard baseline.[3] Technical details[a] The RNADE is a high time complexity approach for high dimensional data. I would like to see an empirical and theoretical discussion on the high-dimensional dataset (such as the image)[b] The notation in the EM algorithm is rather confusing and difficult to follow. Besides, Eq (7) is the log-MLE approach, then it can be naturally decomposed in three terms. Eq (9) is not correct, $p(x)$ should be a continuous function (not discrete). Using the empirical counterpart to estimate the KL divergence is problematic in the high dimensional dataset. [c] Sec 5.3 As we will show in Section 6, by setting a large $\eta$ and doing more iterations, EMTL will reduce the weight on the Q function and allow us to escape from covariate shift constraints. This discovery is really important and interesting. I think it deserves a better justification.--------------------------------------------Suggestions I suggest extensive empirical results for showing the effectiveness of the proposed approach.[1] Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. ICML 2020 This paper studies challenges of offline RL with online fine-tuning and proposes an off-policy actor-critic method to address these challenges. The proposed method uses a supervised learning style to update the model parameters and avoids the behavior model estimation.  Empirical results show that the proposed method provides rapid learning with prior demonstration data and online experience.I have a major concern about the novelty of this paper. The major component of the proposed AWAC method, updating the model parameter using supervised learning, is exactly the same as AWR (Peng et. al., 2019). One minor difference seems that AWAC uses an off-policy policy evaluation, but this contribution is very marginal. This paper provides an analysis on challenges of combining offline RL with online improvement, which motivates this paper. However, most of the discussed challenges are quite well-known. For example, one major challenge is to estimate the behavior model in offline data. This paper does not discuss techniques/methods that address this challenge, like DualDICE (Nachum et al., 2019) and CQL (Kumaret al., 2020). A comparison to these methods is highly recommended. In addition, this paper may also include additional model-based offline RL methods, like MOPO. Most existing data poisoning attacks are objective driven, e.g., the poisoned model is inaccurate for certain inputs (called subpopulation). The authors define model-targeting attack, in which the attacker aims to poison training data such that the learnt model is a given target model. The authors define distance between the poisoned model and the target model using their loss difference (this reduces to objective driven poisoning attack). Evaluation on linear SVM is performed and comparison with one existing attack is conducted. 1. The model-targeted poisoning attack is an ill-defined problem. When measuring the model-targeted poisoning attack, one should use some distance between the poisoned model and the target model, e.g., L2 distance (it's good that the evaluation considers this). The loss based metric reduces the model-targeted poisoning attack to be similar to objective-drive attack. However, when measuring success using distance between the poisoned model and the target model, the attack is obviously unsuccessful when the loss function is non-convex. Many ML loss functions are non-convex. In such non-convex cases, you cannot get the same model parameters in multiple runs of the same algorithm and training data, even if there are no poisoned data points. So the attack could only be successful for strongly convex loss functions, which have a global optimal solution. I would suggest the authors to explicitly mention that the attack is limited to such setting. Moreover, redefine the success metric to measure the attack. However, once limited to such setting, the paper's contribution is also limited. 2. Insufficient evaluation. The evaluation does not compare with subpopulation attacks. I understand you study model-targeted attacks. Since the evaluation is for subpopulation attacks. It is still interesting to know the comparison results. 3. Only linear SVM is evaluated. I suggest evaluating other models and datasets. 4. Why is model-targeted poisoning attack relevant? Objective-driven attacks are more relevant. In fact, the evaluation is on subpopulation attack, which is an objective-driven attack. Typosis the setting use in many prior works -- usedalso allow arbitrary selectiong of the poisoning points -- selectionclassifier that that has 0% accuracy -- that The paper proposes to use a simple intermediate task - clustering - to improve the generalization ability of BERT in low resource text classification settings. The idea is simple. We can perform clustering using BoW representations to generate pseudo labels, which will be used to fine-tune the pre-trained BERT model. The results show that the proposed method is effective in topical classification problems.Pros: - the idea is very simple and we can easily adapt the idea to different classification tasks.- the results are promising, especially on topical classification problems.cons:- My main doubt about this paper is that I think the improvements in the topical classification problems were rooted in the fact that the authors used the "BoW representations" to conduct the clustering. The BoW representations are known to be more effective for topical classification problems compared to BERT-based sentence representations. So what the authors did maybe actually infuse/distill the BoW knowledge into the BERT representations.- I suggest the authors include a set of BoW representation based baselines. E.g., simple BoW based SVM model, average of GloVe embeddings of BoW representations based classification. Summary:This work is based on a recent work of [Bisi et al., 2019], where a per-step reward formulation is presented with some outstanding unresolved problems, e.g., the double-sampling issue and the policy-dependent reward issue. This paper proposes to use the Fenchel duality to solve the double sampling problem and extend it to the behavior-agnostic off-policy setting by leveraging the density ratio estimation technique.Major concerns:1 The derivations in several major equations are WRONG. The objective in Eq 5 is NOT the same as the objective in Eq 6. \E_{s, a ~ d_D}[\omega(s, a)r(s, a)] in Eq 6 is NOT equal to \E_{s~D}[Q^\pi(s, \pi(s)] in Eq 5. 2. The motivation and empirical demonstration of the variance regularization are unclear.First, the definition of the variance doesnt make sense to me. The variance in mean-variance optimization is a long-established term, which refers to the var of the return (either one-step or cumulative). So it is not clear why V_P makes sense without further motivation or reference. Moreover, the variance term defined in Eq. 2 is very weird. It is neither the variance of the return nor the so-termed variance of the marginalized IS (since it involves the reward r). By definition, the variance V_D is the variance of d_\pi(s, a)/d_\mu(s, a)r(s, a). This expression involves both \pi, \mu, and r, and its randomness comes from the randomness of (s, a). It is unclear why minimizing this variance is useful.Second, the empirical results are not convincing. As pointed out by Eq. 3, the overall objective is to achieve a trade-off E[Q] and V_D(\pi) through \lambda. The empirical results, however, do not show this trade-off. Then it becomes unclear where the empirical improvement comes from. I would like to see how changing \lambda influences V_D(\pi). 3 Several key deductions, which hold in on-policy cases, may NOT be true in this papers off-policy setting. For example, Eq . 8. lacks proof. In Bisis setting, this holds only for on-policy cases. I dont think it still holds for off-policy settings. The author needs to prove it.The MDP setting is unclear. The authors consider an infinite horizon MDP, so what does T mean? The proof of Lemma 1 also seems problematic. First, without a clear definition of T, there is no way to check the proof of Lemma1. Is T a random variable? Second, Eq 33 is wrong. Eq 33 is the same as Eq 24, but the definition of D^\pi is different, so how can they be the same?Moreover, It is hard follow the inequality in Eq 34. It looks wrong to me. Thee reviewer strongly suggest that the author write it step by step to make it clear? And also, it would be great to show how the products of IS in Eq 34 reduce to the density ratio in Eq 35.4. Theorem 2, which is the paper's major theoretical contribution,  is obvious and trivial. By definition, the first term of RHS of Eq 16 is exactly J(\pi). So what theorem 2 says is that J(\pi) \geq J(\pi) \sqrt{c * variance of sth}}. This is fairly obvious and does not bring in any insight.5. The entire Appendix B.2 is wrong, where the 3rd equality (aka, line 2 of Eq. 39) does NOT necessarily hold. The term d_\pi(\theta)s gradient is not computed at all, and therefore, any results afterward are not correct.6. There are some missing references as well. Using Fenchel Duality to solve the double-sampling issue in mean-variance optimization using variance as regularization has been solved by previous literature, e.g., Xie et al., (2018) and Zhang et al., (2020). The author should acknowledge this. Also, I encourage the authors to compare with them. Especially I think the authors may want to compare with Zhang et al., (2020). Algorithm 1 is very similar to the offline MVPI in Zhang et al., (2020). There is only a slight difference in computing the augmented reward.Xie, T., Liu, B., Xu, Y., Ghavamzadeh, M., Chow, Y., Lyu, D., & Yoon, D. (2018). A block coordinate ascent algorithm for mean-variance optimization. In Advances in Neural Information Processing Systems (pp. 1065-1075).Zhang, S., Liu, B., & Whiteson, S. (2020). Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning. arXiv preprint arXiv:2004.10888. The authors propose an RL framework, called ScheduleNet trained by clipped REINFORCE, for minmax multiple traveling salesman problem (minimax mTSP), which uses a clipping idea to stabilize learning process as PPO does. The authors empirically show the feasibility of the proposed framework.- Unfortunately, the proposed method has poorer performance than existing works, in particular, OR-Tool. This decreases the merit significantly.- In addition, it is hard to find contribution from proposing new RL method since the stabilizing effect of the clipped REINFORCE is shown in only limited environment (only minimax mTSP).- Table 2 is not completed.- The nature of "minimax" mTSP should be more clearly represented and exploited. Currently, the proposed RL framework seems to work for other formation of mTSP, and also it seems not to exploit the nature of minimax.- In order for showing novelty of the proposed method, it might be useful to devise and investigate actor-critic methods sharing the main idea. In the submission, only footnote 1 simply mentions the hardness of learning value function.- The behavior of ScheduleNet need to be studied further, e.g., when your algorithm works well, and not. This paper proposes a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.  I think the problem this paper deals with is very important and of great interest to the wide range of readers. In addition, the paper is generally clearly written and easy to follow.However, I found the novelty and the originality of this paper is not enough for the ICLR standard.  Although I am not aware of the paper that presents exactly the same concept as this paper, I feel the generalization this work presents is too straightforward that new insights, findings, and benefits brought by this paper to the community are very limited.For example, the micro average (or simply called "Accuracy" in this paper) can be regarded as a special form of equation (6), where weights or importance is given by the relative frequency of each class.This importance criteria can be regarded as opposite of one of the presented criteria, "Rarity".There may be some cases where the rarer a class is, the more important the class is as explained in the paper, but there may be other cases where more frequent classes are more important. As such, the concept of including importance of each class into an evaluation metric has been implicitly considered. It is true that the paper provides general form that includes the aforementioned case, but I'm afraid the formulation is not novel enough to bring a new value to the community as I mentioned above.Another weakness of the paper lies in the experimental analysis.  Overall, the analysis is not convincing enough to verify the benefit of the proposed metric.For example, at the end of "WBA_rarity vs. Class-insensitive Metrics" in page 7, the authors states "This result validates that WBA_rarity provides a more sensitive tool for assessing classification performance". I do not agree with this statement because it is no surprise that different evaluation metrics give different evaluation results, and this alone is not the ground for the validity of the proposed metric. The same argument can be applied to the "Impact of WBA in Model Training" in page 8. It is natural that a model trained with a specific criteria results in performing well on the criteria. This is again not the ground for the claim "our new framework is more effective than Balanced Accuracy  ... also in training the models themselves."The pros and cons of this paper can be summarized as follows.#### Pros1. The paper deals with practically important topic, and presents a simple, easy and intuitive solution.1. The paper is clearly written and easy to understand.#### Cons1. The novelty and the originality of this paper is not enough that there is only limited benefits to the community, which does not satisfy the ICLR standard.1. The experimental analysis is not convincing enough to support the usefullness of the proposed metric. This paper presents a weighted balanced accuracy to evaulate the performance of multi-class classification. Basically, the performance for a multi-class problem can be evaluated by decomposing the original multi-class problem into a number of binary ones based on one-against-rest manner, and then evaulating the performance scores for each of the binary ones using any well-known metric for binary classification, and then, aggregating the performance scores. The main aim of this paper is to present a weighting scheme when aggregating the scores.I'm inclined to rejection of this paper. Frankly speaking, I don't think this paper has a significant contribution. the weighting schemes to combine binary metric scores to evaulate the performance of multi-class classification have been well-studied, such as macro-averaging, micro-averaging, as well as importance weighting (manual or data-driven e.g. frequency). The proposed idea is just a simple natural extension of balanced accuracy with a weighting scheme. It is nothing new. In this manuscript the authors present a variant of stacked Gaussian mixture models they propose for modeling images called Deep Convolutional Gaussian Mixture Model. This model may contain analogues of convolutional layers and nonlinearities between the stacked Gaussian mixture models. This model can then be trained using stochastic gradient decent on the gradients propagated through the model. Finally the authors show some experimental evaluation on FashionMNIST and MNIST.Overall I vote for rejection. While the model the authors present seems to work in principle on images I do not think the authors present a good argument why their model should be used for modeling images and there are definitely other models the authors should compare their model to. Also I have doubts whether the model as presented is a proper probabilistic model.Pros:1) This is a new Gaussian mixture based model.2) It is stackable and inherits some of the benefits of DNNs.3) It is trainable with (stochastic) gradient descentCons:1) I disagree with the authors in the introduction. While GANs as they discuss are not fully probabilistic models and thus have limitations to their applicability, other models do have very clear probabilistic interpretations and apply to all the tasks discussed here. Examples of such networks are the numerous variations of the variational autoencoder, the invertable network based variations like FLOW or GLOW and diverse others. As these are ignored by the authors, I dont think they place their work well into the literature and do not see a particularly strong argument here that gaussian mixture models would be a great addition to the modeling of images.2) The authors do not compare their method against any competing methods outside the Gaussian mixture model framework. I think they would have to present some comparisons to state of the art methods for the tasks they test their model on. At very least compare to some basic models which generate some intuition where Gaussian mixture models overall lie in terms of performance. Without that I am completely lost whether the performance in these tasks is any good.3) If the authors instead want to focus on the conceptual level or advancing our understanding of the presented kind of model I still think a lot more could and should be done: For example, the authors decide to perform outlier detection based solely on the last GMM layer. While this might be somewhat sensible for the decision between different numbers or categories in MNIST, in general, this is not the probability of the observed data under the model. Why is this used? Similarly: What is the structure of the representations produced by the model?4) Given that the main claimed advantage of the model is its probabilistic interpretation, I find the probabilistic description and analysis of the model somewhat lacking:    - How exactly is the probability of a given sample computed in this model?    - How can convolution and pooling layers be interpreted as probabilistically given that they are not invertible. It seems to me, that for both types of layers, the input may even have zero probability to be produced by the described sampling processes. I.e. either some inputs have 0 probability under the model or the sampling methods do not actually sample from the model.    - just ignoring pooling and convolution steps in the calculation of the probabilities under the model as I guess the authors do here seems wrong.5) Each Gaussian mixture model layer in the proposed model converts a continuous input space into  a probability distribution over Group assignments. These assignment probabilities are then linearly mapped and pooled before another Gaussian mixture model layer again interprets the input as point in a R^n to be modeled by a mixture of Gaussians. This is technically possible to some degree if we ignore the restrictions on the support to achieve a valid distribution, but I do not get the intuition how this may well represent the composition of an image. It is stated as fact that the presented model is good for that, but I think this part requires some justification. Wouldnt we expect operations which further work on distributions over discrete spaces instead? Summary ====The paper proposes a model that combines hierarchical Gaussian Mixture Models with a convolutional architecture, supporting both estimation and sampling. The model is trained end-to-end via SGD and is composed of 3 types of layers: standard convolutional and max-pooling layers and a newly proposed GMM layer. The latter operates by modeling the stack of channels at each spatial location as vectors sampled from a GMM. The outputs of the layer are the component probabilities at each location, followed by channel-wise normalization. The loss function is the average log-likelihood of every location at every GMM layer. The paper argues for using it as an alternative to other, less interpretable, probabilistic models of images and demonstrates its capacity to model the MNIST and FashionMNIST datasets.==== Detailed Review ====Main strengths:A novel architecture inspired by both ConvNets and hierarchical GMMs, allowing for more interpretable representation of images.Demonstrates that the model can handle simple image datasets and provides the code to reproduce the results.Main weaknesses:There are no obvious theoretical advantages over other probabilistic models.Experiments do not compare to other methods beyond GMM, so it is hard to determine if there are any significant practical benefits. Additionally, the experiments are limited to only MNIST and FashionMNIST, which raises the question of whether this method is applicable to more complex datasets.The model does not represent a proper probability distribution. There is also a mismatch between the training objective and the sampling process.Missing references to other relevant probabilistic models of images that combine ConvNets and GMM.I do not recommend acceptance due to the lack of theoretical or practical benefit of the proposed method and the lack of appropriate comparisons to prior approaches. In more detail:The paper does not argue for any advantage to the proposed method over the alternatives beyond a general claim of interpretability. The experiments merely demonstrate that the method can model very simple image datasets and has a basic ability to detect outliers. Many models can accomplish the same, and yet they are not compared. The authors should explain why someone would prefer using this model over the alternatives (GAN, VAE, autoregressive models like PixelCNN, or even proper hierarchical graphical models).The model itself is not a proper distribution, as opposed to GAN, VAE, and autoregressive models, which do represent distributions. There is a lack of theoretical justification for the proposed loss function and its connection to the generative process (I believe you might be able to show your objective is a lower bound on the true log-likelihood). Regardless, if the only measure of success is image representation, then there are other non-probabilistic methods the authors could have compared to (e.g., plain AE or Generative Latent Optimization).It is claimed to be the first method to combine ConvNets and GMMs in an end-to-end manner, but prior works have already done this, though using different constructions. Specifically, Sum-Product Networks with Gaussian leaves [1,2,3,4] have been trained with convolutional architectures [2, 4] and SGD end-to-end [2, 3, 4] on several image datasets, including MNIST, FashionMNISt. These models are proper distributions, equally interpretable, and their samples are comparable to those produced in this paper. They also do not scale well to more complex image datasets (to the best of my knowledge), which is why it is so essential to show experiments on other datasets beyond these basic ones.[1] Sum-Product Networks: A New Deep Architecture. Pool et al., 2012. [2] Tensorial Mixture Models. Sharir et al., 2016. [3] Deep Convolutional Sum-Product Networks. Butz et al., 2019. [4] Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning. Peharz et al., 2020. Summary:This work presents two contributions towards cell membrane segmentation. First, it introduces a new labelled database for this purpose. The authors claim that this is the largest labelled database of high resolution Electron-Microscopy images for this purpose. Second, the work tackles the issue that the F1Score, Dice and IoU scores that evaluate segmentation performance by quantifying the overlap of 2 segmentations are not adequately describing quality of a segmentation with respect to what human experts would prefer for the task. This has been evaluated via employing humans (experts on the task of cell segmentation) to grade which segmentation they prefer, and analysed how their preference correlates with these scores. As a solution, the authors propose a metric, PHD, that can be described as the average Haussdorf distance between the skeletons of two segmentations, together with a threshold tolerance, which they show correlates better with the preference of experts with respect to segmentation quality on this task.###################################Reasons for score: I recommend a rejection of this work for the following reasons. On one hand, constructing a database and releasing it to the community is a great contribution. I am sure that this would be very well accepted. But, on the other hand, I dont think the article adequately describes the dataset or compare it adequately with existing databases (which makes less of a database article). Instead, half the article discusses a metric that is essentially an adaptation of the haussdorf distance (actually, of the average symmetric surface distance), adapted in a manner specific to the cell-segmentation task (applied on skeleton, and with a tolerance, the importance of which is questionable). This 2nd contribution has not been accompanied by a literature review on metrics (e.g. only discusses IoU/F1/Dice, missing related distance based metrics like ASSD completely), nor adequately evaluated with such related metrics (besides IoU/Dice/F1). Finally, the modifications, along with many claims in the article, are only relevant to the specific task of cell segmentation (and quite subjective).###################################Pros:  1. Great contribution by releasing publicly a new labelled database of high quality. Seems there was a lot of effort to construct good quality ground truth on a number of images much larger than the existing publically available databases. This is definitely interesting for the community that works in this problem.2. Interesting human-based evaluation of the usefulness of IoU/Dice/F1 scores for the cell-segmentation problem (by 20 humans, this is nice). It is known in the broader community that overlap metrics (IoU/Dice etc) are not perfect, hence there is a lot of work on other metrics [1,2 etc], but this substantiates/quantifies it very nicely.###################################Cons:  1. If I would judge the paper focused on the 1st contribution (releasing a database), I would say that it does not contain a sufficient analysis of the database itself, and especially not an adequate description and comparison with other public databases. I think this point could be sufficiently addressed in the rebuttal. 2. From the technical viewpoint, the claimed 2nd contribution is the derivation of a new metric, but the work has not performed any literature review on related work on metrics except IoU/Dice/F1. In fact, the work produces a metric (Eq.2) that seems to me the same as Average Assymetric Surface Distance (ASSD, see [1]), applied to the skeleton (thinned) segmentation, with a tolerance (task-specific modifications). I note that both skeleton-like operations have been previously performed for computing metrics in the cell-segmentation domain (e.g. for evaluation of ISBI2012 challenge: http://brainiac2.mit.edu/isbi_challenge/evaluation, notice the after thinning operation). Tolerance-based modifications have also been applied to various other metrics (e.g. [2] below) and are task-specific modifications (and not necessary for the metric to be appropriate in the general sense). In my opinion, this makes the value of the 2nd contribution very low. I think this point cannot be sufficiently addressed in the rebuttal, as I basically think that the contribution of the derived metric is low in comparison to existing literature.3. The skeleton part of PHD is not individually evaluated whether it actually adds substantially. I note that without it, the metric is essentially ASSD with tolerance (tolerance being task-specifically motivated in this context).4. Evaluation of the proposed metrics is limited, because it does not contain other metrics except IoU/Dice. E.g., it should have been compared with Haussdord, ASSD, etc.5. The scope of the paper is limited to the cell-segmentation problem.6. The work contains a number of statements that are not true and would need significant text alterations to reduce them.The above Cons are described in detail below, with the detailed comments I raise in the Questions for rebuttal period section.References:Paper with some review on related metrics (there are many more such reviews):[1] Yeghiazaryan and Voiculescu, Family of boundary overlap metrics for the evaluation of medical image segmentation, Journal of Medical Imaging, 2018Paper implementing tolerance (which motivates it, and is subjective to the specific task and needs):[2] Nikolov et al, Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy, arxiv 2018###################################Questions/points to address during rebuttal period: The abstract proposes a database with multiple iterative annotations. However, the actual database released only contains the last 3rd annotation. Please rephrase the abstract so that this is clear to the reader that only 1 annotation should be expected. Similar to the above, Sec 2.1 claims the existence of multiple (3) annotations as an advantage of the proposed database over other databases (Sec.2.1, Besides that, U-RICS produced 3 sets of annotations&, all of which can be applied in developing depe learning). But, these 3 annotations are not released, hence their existence is irrelevant to the reader and this claim/advantage simply does not hold.The authors claim that the intermediate annotations are are very valuable for learning (Sec.2) and therefore will not be released. In this case, if they are valueable, perhaps consider releasing them? Otherwise, I would suggest rephrasing the article, reducing the emphasis on these 3 labels across the whole manuscript, as they are of little relevance to the reader. You can spend the saved space to extend on more related points (e.g. related work etc, see below).Sec 1. claims: We found the human performance is far superior to these methods. I think this statement is very strong and not supported adequately by the current evaluation. I think the authors refer to experiments in Table 1., where the labels from the 1st and 2nd iteration where compared with the results in the 3rd iteration. But, naturally, image after 3rd iteration is conditioned (related) very closely to those from the 1st and 2nd iterations. Of course they will have very high agreement. Also, we note, the 2nd iteration is not a result of 1 human but of multiple (5 experts + annotator). Hence, for these 2 reasons, they cannot support the statement. For a correct assessment for such a claim, segmentations from a single annotator, who is not the same contributing to making the ground truth for the image, should be evaluated against the ground truth. Additionally, inter-rate segmentation performance (multiple humans) could also be evaluated to make such claims. I would recommend this claim to be altered, as well as state explicitly these factors (conditioning of 1st, 2nd, 3rd iterations) in sec 4 to explain how come agreement is so high in Table 1. The work has performed no literature review on related metrics for segmentation. It discusses and evaluates solely overlap based metrics (Dice/F1 & IoU), and then proposes a distance-based measure. The authors should have discussed and evaluated distance based measures too, and especially have a look at the Average Symmetric Surface Distance (ASSD) metric, which is very similar to what they propose (Eq.2). See [1] as a starting point for related metrics, but there are many more. Many of these papers raise the argument that no metric is enough for all tasks, and that quality of a segmentation is subjective to the task. Hence, for each task, one should chose the correct metric, while for objective and all-around evaluation of a general method (e.g. an arbitrary segmentation network), multiple complimentary metrics should be used. See [2] also with related discussion. Also see that ISBI2012 challenge itself implemented multiple metrics (http://brainiac2.mit.edu/isbi_challenge/evaluation) that are not duscussed here (and their paper also discusses appropriateness of metrics).Sec.1 Considering that image compression generally loses many texture details: Not necessarily. Depends on how much compression, what type of compression, and the actual content. If you would like to claim this, I would suggest you analyse what type of structures disappear if you do a 2x sub-sampling (based on the fact that ISBI has 2x less resolution). Otherwise, I would suggest this is rephrased a bit less strong.Sec.2.1 intends to do a comparison with ISBI database and SNEMI3D databases. However it still does not provide important information about them. For example, there is no mention of the actual resolution of the other databases, although the work emphasizes a lot in explaining it contains more info due to higher resolution. ISBI seems to be 4 nm x 4 nm x 50 nm / pixel. (from http://brainiac2.mit.edu/isbi_challenge/home), while the introduced one is 2.18 x 2.18 x 70 nm /pixel. Notice that ISBI resolution at z-axis actually is higher than the introduced. This should be made clear in text. Also, please add same information about SNEMI3D. Do you believe this difference in z-axis could make any difference with respect to what structures can be segmented? (In fact, in Sec 2.2, you say that thicker slice affect imaging quality.)The current database has approximately 2x resolution than ISBI in x,y plane. What type of structures do you believe are not capable of segmenting well in 4x4nm resolution, but capable at 2x2nm resolution? To support the claim that the higher-resolution is a significant advantage (and hence the contribution of releasing such a database is strong), perhaps the work should have performed an evaluation of how useful this extra resolution is in practice, to support the main contirbution.Please discuss in Sec 2.1 what anatomy are the images of each database coming from. As they are not coming from the same tissue (e.g. this is from retina, ISBI from Larvae cord etc), please discuss if you think this could be a factor for qualitative differences between the databases. If you think it may be, then perhaps claims about what database is more suitable should be adjusted, as perhaps the two have a bit different purpose / characteristics? Sec. 2.1: much more challenging: What evidence is this claim based on? I could make the argument that ISBI may be more challenging to segment due to the lower resolution (hence less information). Please ensure that you back up all claims with appropriate arguments. As it currently stands, this is an unsupported claim and should be removed.Same comment as above for the suitable in exploiting cell segmentation algorithms claim. Why more suitable to exploit algorithms? I think this needs a rephrase.Sec. 3.1: may not be consistent with human perception& tasks, an natural first instinct was that, and in Sec 3.3: humans are more sensitive to structure changes, instead of thickness changes.. I think these statements are not passing the correct meaning. It is not the human perception or the instinct of the humans that prioritizes thin (non-)existence of structures over thickness. In your experiments, the evaluators were clearly trained about what the task is. For example, perhaps they know that in cell-segmentation, where the ultimate goal is creating the connectomic, the connectomic can be created regardless the thickness, but a structure should not be missing. Hence, what wrong structures more important than thickness, is the task. Not human perception or instinct (in fact, for me, its clearly easier to identify thickness, than locating a small structure missing somewhere in the images). I think these statements pass a wrong meaning. I would suggest that they are rephrased, to emphasize that in every task, where the segmentation itself is not the ultimate goal, the quality of a segmentation should be judged with respect to what the actual ultimate goal is (e.g. here, creating the whole structure of how membranes are connected?). And this should be reflected in the evaluation metrics, where in each task, different metrics, appropriate for the specific one should be used. Please discuss your viewpoint and your recommended amendments.Same point as the above, in Sec 3.3:  humans are more sensitive to structure changes, instead of thickness changes.: I dont think this statement is in general true. I would say the opposite for me. I can immediately tell that the thickness differs among segmentations, but I have to focus explicitly on certain areas to find whether a specific area has been wrong segmented. I expect the fact that the humans that performed the evaluation were specifically trained (Sec 3.2) that they perform *the specific task of cell segmentation* is likely what made them emphasize the actual structure and not care about the thickness. In other words, what criterion/metric is most appropriate has to do with the actual task of interest. Please discuss. I would suggest all related statements about human perception, vision or sensitivity, to be rephrased in a way that is less generic, and instead perhaps passes the message that in each task, quality of segmentation should be judged with respect to the actual ultimate goal.Sec 3.2 does not describe on what data were the segmentation methods trained. Specifically, the paper should state explicitly if the training data were different images from those that were used to create the 200 groups of images that the 20 humans evaluated the results, or were they the same. Can you please clarify this here and in the text?Sec 3.1: What is the difference between F1 score and Dice Coefficient? As far as I know (I double checked), these two scores seem the same to me. Phrasing in sec 3.1 suggests they are different. Am I wrong? Please clarify. If they are the same, then perhaps one of them should be removed.Related to above: Figure4 c: I think that F1 score really is the same as Dice. After you double check, please check this figure. In Fig 4 c, the values of F1/Dice differ. How come? Please double check and clarify. Perhaps implementation detail? Or am I wrong? You see that in the end of Sec 3.2, F1-score and Dice also brought Exactly same results for correlation with human perception, agreeing with my view that F1-score/dice are the same. On the other hand, in Table 1, F1 score and Dice differs in *some* methods (humans, GLNet, Unet), but are absolutely the same for other methods (SENet, CASENet, Unet++, LinkNet). Please double check and clarify. I would suggest you check for a small implementation error? Sorry if I misunderstand something, I am happy to hear clarifications.Sec. 3.3: I think there is no strong technical argument given for introducing tolerance? Without the tolerance, for small offsets/differences, the distance metric will simply have low value. Humans dont "ignore it", it's just small so it does not "bother them enough to mention". Which is exactly what a low value from a distance metric means. From Fig 5, we can see that even without tolerance (=0), the metrics (ASSD on skeleton) behaves perfectly fine, giving higher PHD for the case (b) that has larger distance than case (a). I would recommend adding such explanation and discussion in the paper. What is your view on the above?Sec 3.3 & Fig 4: suggesting human vision does have tolerance: Sure, but this does not necessarily mean its the right thing, right? For example, factors for inducing human tolerance can be limited vision capability (our eyes are not as good as a computer in processing pixel-by-pixel) or subjectivity with respect to the task (e.g. if the human annotators know, or they have been trained, that 1-3 pixels is not a important *for the particular task* of cell segmentation). But this does not mean that a metric that has no tolerance is a bad thing, perhaps the metric is even more objective. Please discuss.Can F1-score& be improved& refutes this: This statement is wrong. Skeletonizing *does* improve all these metrics, as shown in Fig 6. They simply dont reach the result of PHD. Please rephrase.The evaluation could/should have included other metrics, such as basic Haussdorf distance, ASSD, or metrics used in related challenges, such as Rand etc (see http://brainiac2.mit.edu/isbi_challenge/evaluation) ###################Minors, or additional feedback for improving the work in the future (not subject to rebuttal):I would recommend rephrasing the phrase Surprisingly, we found in Sec.1, as it is actually a commonly discussed issue in the literature (see my previous comments on related work/references)Sec.2: provided by Marcs lab (Anderson et al. (2011)): I think this could be rephrased to a more canonical way of refering to a wold, and also be more accurately descriptive? One that clarifies whether the data are exactly those described in Anderson et al 2011? Were they made publically available together with the specific paper (Anderson et al)? Or have they been provided by Marcs lab (author of the cited work) to you personally for this current work? E.g. a rephrase like made publically available and described in the work of Anderson et al (2011) or something like that is more descriptive.at an x-y resolution of 2.18 nm/pixel: is the resolution the same along the 2 axes? If so, clarify something like 2.18nm/pixel across both axes, and 70nm&from different layers: What is a layer in this context? It has not been defined. Is it a slice? Remember that you are addressing this to a non-domain-specific audience of ICLR, so ensure to be clear about these terms.Fig 1: image number => number of images reads better.Sec. 3.2: and 2 segmentation results => 2 automatically generated segmentations?Abstract: proposes a dataset? Sounds wrong. I would suggest introduces a dataset.Sec 1: how robust if these methods are compared:  grammar # pros:- To the author's and reviewer's best knowledge, this paper includes the largest annotated public EM data set for cell membrane segmentation (in case it is published with this paper). Until now, the ISBI 2012 challenge (http://brainiac2.mit.edu/isbi_challenge/) dominates the evaluation of cell membrane segmentation in EM data, even though the performance is nearly saturated. New datasets can identify potential weaknesses in similar domains, that are not covered in current datasets and by state of the art methods, yet.- The discussion about suitable segmentation metrics for cell membrane segmentation is important and must be continued.- The article is written in a clear and comprehensive manner.# cons:- The discussion about appropriate metrics for cell segmentation, that do not depend on the thickness of the segmented cell membrane, has extensively been elaborated in "Crowdsourcing the creation of image segmentation algorithms for connectomics" by Ignacio Arganda-Carreras et al., Frontiers in Neuroanatomy 2015 (9) 142: pp. 1-13. However, this paper is not referenced and the therein proposed metrics are not mentioned or compared.- The evaluated "state-of-the-art" methods are not "state-of-the-art". They do not correspond to the top entries of the current ISBI Segmentation Challenge Leaderboard. Additionally, no parameters of the methods were adapted.- It is left unclear how the 20 human raters were instructed to evaluate the segmentation results. For the correct evaluation, not (only) intuitive human perception must be taken into account, but also the usability of the resulting segmentation. The segmentation results on high resolution EM data presented in this paper display many "unclosed" edges, which lead to severe problems, when using the segmentation as a basis for connectivity analysis. To the reviewer's understanding, the proposed Perceptual Hausdorff distance will hardly penalize these errors.- The dataset is not published in the format of a challenge, which would allow benchmarking on a private test set.- Spelling should be revised.# SummaryThe presented new high-quality dataset is highly valuabl to the community in order to improve and develop methods for instance segmentation, specifically cell membrane segmentation. The segmentation of thin cell boundaries imposes different challenges and includes different priors, than in other domains of instance segmentation.The use of appropriate evaluation metrics is crucial to identify suitable und successful methods in experiments and must be critically discussed including domain knowledge. However, in the presented paper, the discussion about suitable metrics is not appropriately linked to the existing literature. A metric is proposed, that is (more) consistent with "human perception". This is an interesting aspect, but its contribution to the successful analysis of neuronal connectivity from EM data remains unclear. This paper investigates incorporating shape information in deep neural networks to improve their adversarial robustness. It proposes two methods: the first one is to augment the input with the corresponding edge and then adversarially train a CNN on the augmented input. The second idea is to train a conditional GAN to reconstruct images from edge maps and use the reconstructed image as input to a standard classifier. 1. The description of the proposed defense in section 3 seems to be limited. It is not clear why the author applied a conditional GAN to reconstruct clean images from edge maps. In other words, what is the motivation for designing GSD on top of EAT?2. The authors use Canny edge detector to extract edges. Why not use neural network based edge extractors [2] as they give better edges? What is the motivation here? 3. Considering the possible obfuscated gradient issues of white-box attacks [3], the authors should explicitly describe their efforts to evaluate against strong custom adaptive attacks.4. In terms of the experiments, the authors claim that they investigated adaptive attack but I did not see any quantitative experiment results. They also claim that any adaptive attack would cause perceptible changes to the edges. This is not an excuse for not doing quantitative study; the authors already considered adversarial perturbations with magnitude as large as 64. Such magnitude can also cause perceptible changes to images as Figure 8 shows.5. For EAT, what is the performance if the model is not adversarially trained? Why use adversarial training in EAT but not in GSD? I believe these analyses are required for an in-depth understanding of how the proposed defense works.6. Last but not least, the algorithms proposed in this paper looks similar (almost the same) to this paper [1] from previous year: (a) The edge-guided adversarial training (EST) is basically applying adversarial training on EdgeNetRob in [1]; (b) The GAN-based shape defense (GSD) is exactly the same as EdgeGANRob in [1]; (c) Both of them use canny edge detector to extract edges. Can the authors highlight the differences? If this is a separate paper, given the previous work [1] that already proposed this idea, the contribution of this work seems to be limited. [1] Shape Features Improve General Model Robustness. https://openreview.net/forum?id=SJlPZlStwS, 2019.[2] Richer Convolutional Features for Edge Detection. Liu, et al TPAMI, 2019.[3] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. Anish et al, ICML 2018. This paper proposed a way of product representation learning via BYOL framework. Background introduced, model explained, experiments conducted. I don't think this paper is good enough for acceptance. See detailed comments:1. The writing. The model part is not clearly written, and neither is the experiments set-up.2. The novelty. It seems it's a derivative work from BYOL for this particular product embedding application.3. The sampling method. It is well known that sampling via browse data suffers from bias caused by the search engine. For example, two products are browsed together only if the search engine decides to show them to the shoppers together. The search engine quality will largely impact the quality of the sampled data.4. Missing important literatures and comparisons. The browse product pairs are essentially the degree-2 connection of products on the query-product bipartite graph. There are vast amount of work for learning representations on such graph.5. The experiment section is not convincing. By applying the embedding to specific problems doesn't justify the method's superiority.6. The results about different losses. Results are illustrated yet no discussions or insights are provided. What's the motivation behind? The authors study the problem of representation learning of marketplace products to apply in downstream tasks. More specifically, the authors extend the work of BYOL with a new objective function by adding a cross-entropy objective. The main hypothesis of this paper is that different products of the same browsing session can be thought of as different augmentations of the same session. Overall, the paper is clear and easy to follow. However, I have a few concerns and questions for authors.Concerns:The first concern is that the main hypothesis of this paper is not very clear to me. Why can different products of the same browsing session be thought of as different augmentations? I did not find clear explanations behind this hypothesis. This hypothesis is not validated in the experimental section, which leads that the hypothesis is not convincing to me. The second concern is about novelty. The authors claim that the main contributions of the paper lie in their novel deep encoder architecture. However, the bi-encoder architecture is not novel, and the novelty of added cross-entropy is also limited.Questions:1. Why can different products be thought of as augmentations to each other? Have you considered cases 1) complimentary products or 2) the users want to purchase several non-related products and browse them at the same time. Have you analyzed the similarities among products in the same browsing session?2. The sensitive nature of datasets can be understood. Have you considered conducting experiments on other public datasets for easy reproductions?3. The gap between the performance of the proposed model and the state-of-the-art models is large. The authors claim that one of the advantages is that the inference of their model does not rely on GPU. Why do the authors think that reliance on GPU is a limitation? Moreover, considering the inference efficiency, how does the proposed model compare to other baselines? This paper studies the data leakage issue in the federated learning. More precisely, when the servers have access to model parameters and gradients. It can recover the input data via gradient matching, and the authors claim that their method performs well even with large training batch sizes, e.g. over 40. Finally, the author also studies the possibility of attacking during learning, where they suggest that multiple updates of fake data helps. However, their contribution seems incremental, gradient matching is used in previous literature [zhu et al 2019], and their main modification is extra two regularization terms: total variation and internal representation regularization, and a data index alignment technique (whose exact meaning is unclear in the paper).The following are some questions:What does index alignment mean? Is that the server controls the indexes of samples chosen at each iteration? This seems to be very restrictive in practice, especially for horizontal federated learning.Does the server have access to the aggregated grads from each worker separately or the workers aggregate all the gradients before sending them back to the server? The second scenario cab be achieved while secure aggregation technique.In vertical federated learning, the gradients of part 1 of the network does not need to be exchanged with the server, as there is no average  operation needed, even the parameter itself does not need to be transferred to the server for the same reason, will your method work under this setting?Some terms are not properly defined, such as normalized gradient descent, batch ratio, et. al.Other questions:What does the iterations represent in table 1a? Is that the number of iterations need to reach a 35 PSNR?Using cosine dissimilarity decreases the PSNR, I assume this is because PSNR penalize the scale, is there noticeable degradation visually when using cosine dissimilarity?In the attack during learning scenario, is there any intuition why optimizing fake data multiple times works better? **Summary:** The authors propose PGPS as a minor variation on [CEM-RL](https://arxiv.org/abs/1810.01222). **Quality:** The quality of the paper is overall very low: misleading and unsustained claims, poor and incorrect literature review, cherry-picking results.**Clarity:** While the exposition overall follows a standard template, the writing is in dire need of intensive copy efforts prior to being considered for publication.**Originality:** The authors present a minor augmentation of CEM-RL while claiming some of its results as their own.**Pros:**- The experimental part seems to have received a lot of work- The setup and hyperparameter description is very complete, coupled with open sourced code, potentially allowing full experiment reproduction**Cons:**- The first sentence of the Conclusions claims the pairing of CEM and TD3 integrating policy gradient and population-based search as a novelty and contribution, while it was introduced in the (cited!) CEM-RL.- There is no clear, explicit definition of the paper's contributions, which given the circumstances is unacceptable.- Some results (particularly regarding the Q-critic filtering) are sketchy and not statistically meaningful.- The Related Work study in Section 2 is riddled with incorrectness in the evolutionary part. For example: stating EA stands for Evolutionary Approach rather than Algorithms; referring to CMA-ES by citing a 2016 tutorial instead of the [2001 paper](https://scholar.google.ch/scholar?q=completely+derandomized); equating EA to direct policy search; stating that previous-generation data cannot be reused; and many more. This overall shows a less than perfunctory understanding of the field. The parallels with the CEM-RL paper are once again obvious and misquoted.- The paper writing requires a thorough overhaul.**Comments:**The contributions are not made clear, and a quick read of the original CEM-RL paper shows an exact correspondence but for 1. the double-feedback loop and 2. the Q-critic filtering. These two contributions however are very minimal, and most importantly they are not tested in isolation: end of Section 5, 1. "The effect of EA guidance" does not compare with CEM-RL (or other hybrid methods), and 2. "The effect of Q-critic filtering" drops the variance information in favor of what looks like single runs with statistically indistinguishable results.**Final remarks:**This work is a barely-augmented, error-riddled rewriting of a 2019 paper, and its state is nowhere near publishable. I strongly advise the authors to decide if their contributions are sufficient for publication, and if so to write a paper dedicated to proving their point to the scientific community. Finally, the un-anonymized GitHub link on Section 5.1 breaks the code of conduct; the fact that the author (NamKim88) is less known does not excuse the breach of anonymity. This work presents TaskSet, a collection of optimization tasks consisting of different combinations of data, loss function, and network architecture. The tasks are useful when choosing and evaluating different optimizers (e.g. ADAM) for learning tasks. The usefulness of this collection is demonstrated for a hyperparameter search problem. The main question I had about this work is why is the chosen collection the right set of tasks to be considering? Do I have any assurance that an optimizer chosen using TaskSet will be any good on future tasks? How can we know that we don't overfit to these particular tasks when choosing an optimizer? Is there any notion of two tasks being drawn from the same distribution? However, my main concern with this paper is that, while TaskSet may be a useful tool for facilitating future research, it is not clear to me that it itself represents an advancement of novel research, which I think should be the bar for acceptance to a major conference. The work does not make any claims, or present any results beyond a use-case for the set of tasks. That's not to say that TaskSet isn't a useful tool, helpful for future research. But it itself does not represent such research. Because of this I recommend the work be rejected.  In order to improve robustness of analog written weights to circuit variation, quantization is used. My first question is why is that the case? The claim that quantization reduces analog noise does not seem to be correct. As far as I can tell, this setup leads to quantization noise on top of analog noise. This is a very serious point since the authors just assume this claim to be true (first para of page 3) and that is the whole motivation. The claim should be justified, or at least a reference should be provided. P.S., I do not think the claim is correct; two noise sources (quantization + analog) is worse than one noise source (analog only).Second issue: what is the contribution of this paper? As far as I can tell, two old methods (LLoyd max and dithering) are used and that's it. Sure, the experimental section is massive but this paper does not claim to be a pure empirical study. In fact, the claim is that for the first time, a general quantizer is used. Those methods have been around for decades!The experiments only use quantization. One would expect that CACIM would be emulated somehow and we would see that indeed the proposed method improves CACIM's accuracy (which I don't think it would as discussed above). Even the conclusion talks about digital systems. So CACIM was (erroneously) used to motivate the work but then completely forgotten.  Summary: This paper proposes a model for verifying semantic equivalence  between symbolic linear algebra expressions. Expressions are represented by trees and equivalence is proven by a sequence of axioms applied to the first expression. The proposed model encodes the expression/program trees as nodes on a graph connected by edges representing one of a set of axioms being applied to one of the elements in the first expression to yield a node in the second expression. The output of the model is a path, a sequence of edges, on this constructed graph that correspond to a sequence of axioms applied to the first expression to arrive at the second.Strengths:* The paper investigates an interesting problem of provably-correct equivalence proof generation for symbolic expressions.* Results on incremental versus comprehensive supervision over the entire proof sequence motivate the training method proposed by the work.Weaknesses:* Its not clear how this work fits into the context of the field and experiments are only conducted as ablation studies on the architecture choices and training regimes, with no other existing methods for expression rewriting or proof generation considered.* The system is trained and tested on expressions that are very few rewrite steps away. While datasets were generated for up to 5 and 10 rewrite steps, even the 10-step dataset contains mostly pairs that have fewer (1-5) rewrite steps. In any case, 5 and 10 steps are very few and claims of robustness and generalization are weak with such few steps in the required proofs and such a large overlap in the steps required for pains in both datasets.* The system relies strongly on training one step at a time by requiring samples generated for every intermediate step in the proof of equivalence between two expressions.* There is no motivation for using a deep model over another path-finding method on the constructed graph that connects the input programs.Recommendation:I recommend against acceptance of the paper in its current form. The idea of GNN based path-finding for expression/program equivalence proving is promising, but the methods are not clearly and rigorously explained and the experimental settings are too weak to support the claims of generalizability and applicability to realistic scenarios. (Questions:1. The development of the equivalence proofs on page 3 is very unclear. What does it mean to say $x \in v_i$? This notation is not well explained. What is $x$ and what does it mean for $x$ to be _in_ $v_i$?2. The text says that models were run twice, does this mean trained twice with different initializations? If so, it would be more informative to report average performance rather than the one from the best random seed. If this is not the case, could you please clarify the training set-up?3. What are the evaluation metrics used and reported? What measure was used for the validation score that determined the best-performing model?Minor comments:* It is best practice to define acronyms the first time they are used. This will avoid confusion, especially for readers from a wider machine learning audience. In particular, GNN, GGNN and AST are not defined* There are many cases where citations should be within parentheses, please check for these. If the work is not referenced as part of the text of the sentence, it should be in parentheses for clarity.Typos:Pg 5, production rulse" should read production rulesPg 5, there is a sentence that begins with E.g. which should be replaced with the grammatical For example,. Similarly, on page 5, the sentence that begins Fig. 1 should read Figure 1. The authors propose a VAE-type generative model approach to characterize the hidden factors, with a divided focus on the global and local reconstructions. The claim is that the learnt hidden representations are disentangled (which is not defined clearly) using two reconstruction terms. The setting of the problem adopts the graph VAE setting in [1,2] (which I think the authors should mention in the related work), and the ELBO & local aggregation (convolution) approaches used in this paper are relatively standard in the generative modelling and graph representation learning domain. Apart from the limited novelty, which would not have affected my evaluation if it solves the problem as claimed, I have several major concerns about this paper:1. The notion of disentanglement is not well-defined in the first place. In the VAE setting where the hidden factors are stochastic, does disentanglement refer to independence? Or they are orthogonal under a specific measure induced by the graph itself? The claims made by the authors can never be examined rigorously (the visual results do not constitute supportive evidence as I shall discuss later). 2. There is no guarantee that the so-called global and local factors are not confounded. Both the global and local reconstruction terms involve the two types of factors. Given the high expressivity of deep learning models, the local factors can easily manage both tasks, or the global factors are merely enhancing the signals of the local factors. There no mechanism to prevent the cross-terms during the optimization, so the learning process of the global and local factors confounded as a result of how the authors design the objective function.3. Unclear interpretation of the visual results. It seems that the visual results showcase a similar pattern among the local and global factors, despite the difference that the signal is stronger for the local factors (which is evident as they play a more critical role in the objective). In the absence of a clear definition of disentanglement, more persuasive numerical results and interpretations are needed. [1] Kipf T N, Welling M. Variational graph auto-encoders[J]. arXiv preprint arXiv:1611.07308, 2016.[2] Xu, Da, et al. "Generative graph convolutional network for growing graphs." ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019. The paper presents an approach for object-centric representation learning for planning to accomplish complex tasks. The learned representations are at an abstract level, resulting in desirable knowledge transfer capabilities between tasks. The learned action knowledge is represented using PDDL. Experiments have been conducted using blocks world and Minecraft domains. Results show that the agent was able to learn useful operators (actions) and that learned actions can be applied to different tasks. The object-centric idea is highlighted in the paper, though the work is more about learning symbols for abstraction, which is not new. The issue is that many of the "learning abstraction" works have been demonstrated in much more complex domains. For instance, the work of Konidaris et al 2018 (cited in the paper) has enabled robots to learn action preconditions and effects in the real world. In comparison, this work does not go beyond toy problems. The developed approach is more like an integration of a few existing methods, and the connections among the pieces are rather weak (see Figure 1). For instance, once the actions are learned, it looks like the agent faces a planning problem, and the planner does not have a way to go back to improve its learned representation. Since each component introduce its own errors, there's the cumulative error that can potentially make the whole system rather unstable. Some co-learning functionalities will be good for future work. The experiment section is relatively weak. For instance, the blocks world domain was mostly used as a demonstration platform. There were a couple of examples presented, while there were no statistical results discussed in the paper. Figure 6 is on the Minecraft domain, where the baseline (such as no transfer) is very weak. The results are not convincing to support the claims on transferability and learning efficiency.  The authors discuss several ideas aimed at improved semi-supervised learning by adopting an appropriate "plate model" with probabilistic content, and then examining various techniques and variants. The theme is relevant but the whole effort seems a bit preliminary: there are many keywords and many discussed techniques, but the whole picture is not clear in terms of concrete contributions (and the provided testing does not clarify whether gains are realized). The paper contains a somewhat long introduction that in a sense includes Sections 1 and 2, then quickly goes through the proposed Expression (6), and derives consequences that are not always clearly articulated; for instance what is the point of Table 1? Also the connection with neuro-symbolic learning is interesting but it feels a bit too much; why exactly is it needed in this framework? Or is it just an optional add-on? (Besides, for the proposed approach to work I believe more testing is needed.)A problem: as far as I can tell, Figure 1 left and center are identical. What is the difference?A problem: is the citation (Van Engelen and Hoos 2020) indeed in the references? I could not find it. **1. Summary and contributions: Briefly summarize the paper and its contributions**This work analyzed the optimal solutions for the Generative adversarial training (GAT) and the convergence property of the training algorithm. This work also compared the minimax and maximin games, both theoretically, and empirically, with the help of a nice 2D toy example. This work also developed an unconstrained version of GAT, and evaluated it on image generation and out of distribution detection tasks. ##########################################################################**2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.**I found the theoretical analysis to be interesting, and I especially like the 2D toy experiment in Figure 2, it strongly and clearly justified the importance of using a uniformly distributed p_(-k) distribution. I also liked the thorough discussion of the distinction between the minimax and maximin games. The paper has interesting ideas and a lot of content, it also introduced adversarial OOD samples, which are all very interesting to me. ##########################################################################**3. Weaknesses: Explain the limitations of this work along the same axes as above.**Memorization: I think optimizing the D approach is problematic in terms of memorization, theres nothing stopping the model to memorize the data, especially in the high dimensional space which makes the mass distribution to be very sparse. In the Celeb A results in the middle of Figure 3, notice how the images (4,1), (4,3) look almost exactly the same, and they are also very similar to (1,3), (3,1) and (3,4).  In figure 2b and 2c, notice how the distribution all collapses to 1 point, I wonder whether this is one perspective or intuition on this problem. Unverified claim: At the bottom of page 7, These results suggest that with a high capacity model and proper training, a robust OOD detection system is within reach. where these results referred to reducing the data complexity. I think the results in this paper are not enough to make this claim. The authors logic here is that, if their model performs better on a simple dataset, then it implies the problem is insufficient model capacity. The assumption here is that the model can scale, which is completely unverified. Just because a model works well on MNSIT does not imply it is even possible to scale this model on ImageNet. An unverified claim like this is always a warning sign as it may mislead the readers and community. Lack of ablation study: I really liked the toy experiments in Figure 2, which justified the use of uniform distribution in the data space. However this only provides intuition for the higher dimensional cases, it is still necessary to conduct an ablation study to verify this is indeed the case for higher-dimensional cases, for scientific rigor.  Unfair comparison: For image generation results, the authors method was compared with GANs, and the motivation is that both of the methods are trained adversarially. However, the generators of GANs never got to see the real data during train time. Here authors are optimizing D, which is trained on real data, thus I dont think it is fair to compare with GANs.  Weak baseline for OOD (outlier exposure) and insufficient comparison:Outlier exposure is no longer the state of the art OOD detection methods and there are so many other methods that perform better than outlier exposure on CIFAR10, for example, see Detecting Out-of-Distribution Examples with Gram Matrices: https://arxiv.org/abs/1912.12510  Lack of Related Work section: This paper does not have a related work section. The related works are very briefly discussed in the introduction, but I think that is far from enough. I understand there is a page limit, but even putting a related work section in the appendix would be very helpful for readers to get a more complete understanding of where this work stands. Unclear writings: See section 4. ##########################################################################**4. Clarity: Is the paper well written?**Typos: At the top of page 3: as a results -> as a resultBottom of page 8: a OOD -> an OODAmbiguity: elementary mass was not defined before being used, itd be helpful to the readers to briefly define what elementary mass means in this specific context. In order to cause more local maxima to be eliminated could be worded better. ##########################################################################**5. Reasons for score**In conclusion, the ideas are very interesting but I think there is a lot more work to be done before this paper could be accepted.  Overview: ========The paper suggests a modification to graph neural networks, which is claimed to overcome GNN expressiveness issues recently shown by Loukas (ICLR 2020).Comments:=========I had multiple difficulties in following the content of the paper, some are detailed next.-- "Unfortunately, [GNNs] require large depth as proved in Theorem 6 above"The lower bound in Theorem 6 is for approximation alpha < 1.5, but here you discuss approximation O(log n), doesn't this void the lower bound? Why cannot usual GNNs produce an O(log n) approximation (and specifically Bourgain's embedding)?-- "While there do exist semidefinite programming based algorithms (Linial et al., 1995) for computing the embeddings required for Bourgains theorem, they are not suitable for implementation via efficient neural architectures. Instead [...] we adapt the sketch based approximate shortest path algorithms of Das Sarma et al. (2010)"In fact, the algorithm you implement is the one from Bourgain and Linial et al., not Dar Sarma et al. All those works use roughly the same sketching algorithm, which measures the distance of each point to randomly chosen clusters. However, the distance estimation procedure you implement (specifically ~d_G(s,t) = max_i|v_s^(i) - v_t^(i)|) is Bourgain's (this is just an ell-infinity embedding). Dar Sarma et al.'s estimation procedure is different and (building on the well-known work of Thorup-Zwick) relies on computing the common nearest neighbors of the given pair s,t in the random clusters. Note that this bears on the correctness of the proof of Theorem 8 (I think the statement still holds due to Matousek's analysis of Bourgain's embedding, but not for the reason you cite). Also note that in Theorem 8 (as well as all aforementioned results) c needs to be an integer (in particular, it is known that any approximation less than 3 is impossible with less than ~Omega(n^2) parameters).-- For min-cut, you write that traditional GNNs require Omega(sqrt(n)) depth/rounds, citing Loukas (2020). But doesn't that lower bound entail both the depth and the width (d*sqrt(w) = ~Omega(sqrt(n)))?-- I am unable to follow the proof of Theorem 9. Could you please explain the correctness of your construction.(On this note, the sentence "Karger & Stein (1996) implies that with probability at least 1/n^2 there exists a prefix L' of L such that..." seems like an unfortunate inaccuracy; the "prefix" exists deterministically, and their guarantee is that the iterative random contraction algorithm finds it with probability at least ~1/n^2.)Conclusion:=========I am currently unable to recommend accepting this paper, due to what seems like multiple inaccuracies, misinterpretations of prior work, unclear statements, and possibly technical correctness issues. I will await clarifications from the authors on the points detailed above. This paper aims to apply Fuzzy c-Means (FCM) clustering to persistence diagrams and prove convergent subsequence of iterates tends to a local minimum or saddle point. The motivation of the work is direct and clear. This paper addresses the problem of persistence diagram clustering via using  weighted Frechet mean . This is a incremental work though replacing Euclidean distance in FCM by Wasserstein distance. The contributions in this work are not quite promising. For instance, the weighted Frechet mean in Sec3.2 is the well-known Wasserstein Barycenter whose behavior is well studied in optimal transport works. Thus Theorem2 can be not be considered as the contribution of this work. Another drawback is that experiment in this work is very weak. There are only three dataset tested in this work. It's not quite convincing. It would be promising if the proposed work valid in other shape datasets such as SHREC2010 or SHREC2014. These datasets were frequently used for testing algorithms of topological data analysis .I cannot recognize the merits of this work compared with previous papers, such as: Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport. This paper revisits ObjectNet dataset closely and found applying classifiers on object bounding box significantly reduces the gap between ImageNet and ObjectNet. The authors further investigates the robustness of CNNs against image perturbations and adversarial attacks, and found limiting the object area to their segmentation mask significantly improves model accuracy (and robustness). Qualitative evaluation is also performed over confident and less-confident / incorrect model predictions and find it correlates with human perception.Pros:- More analyses like this paper does would help bridge the gap between ML/CV model performance in staged datasets and real-world scenarios- The experimentation conducted in this paper is comprehensive, accompanied with many in-depth inspection over a large dataset. The insights drawn from this paper would be invaluable for researchers working in this field.Cons:- My major concern with this paper (and the main factor of rating it clear rejection) is the experimental setup used in section 3.3. From authors they "selected images from ten classes of the ObjectNet dataset ... manually annotated the object of interest in each image". Then "Models were trained on 70 images per category".  (also from Figure 39 "In total we annotated 1K images across ten categories of the ObjectNet dataset."). If interpreted correctly, the models are trained on part of ObjectNet images which clearly violates dataset license "ObjectNet may never be used to tune the parameters of any model." (https://objectnet.dev/download.html). - While appreciating the authors conducting study on model robustness, the conclusion drawn from several experiments seems to confuse "performance" v.s. "robustness", where the former indicates model have better accuracy, and the latter measures how model accuracy varies with increasing noise / perturbations. See more details below.Some (minor) comments:- Sec 3.1. 1) I share the concern from authors that "object detector" should be not confused with "object recognition" (or commonly used "image classifier"). Hopefully vision community could use more consistent terms across literatures.- Sec 3.1 1) While detection dataset would surely have more scale variation (and truncation / occlusion due to many objects are not in the center), it is not entirely clear that object in detection datasets "vary more in parameters such as lighting, ..., and blur".- Sec 3.1 1) It would be great to see more analysis on detection datasets (authors mentioned they will discuss in section 4, but only with very little analysis).- Sec 3.1, 2) While ImageNet and ObjectNet have distinct characteristics, having some stats on object size / spatial placement might better illustrate the gaps between these datasets.- Sec 3.1 3) Agree top-5 might make classifiers' life easy, but it is more of an eval metric rather than training loss (the model still need to predict top-1 class correct during training). Meanwhile, it is not very clear why multi-label annotation would bias against model that "is spatially more precise but misses some objects". That model should be evaluated against detection benchmarks, rather than object recognition (image classification) datasets.- Sec 3.1 "bounding box annotation". For multiple objects nearby with the same category, the annotation would include all of them in one bounding box. This might leads to bad aspect ratio? (in general, bounding box would also vary more in aspect ratio than images, and feeding bounding box into a square CNN seems to be less ideal)- Sec 3.1 "object recognition results". "AlexNet, VGG-19, and GoogLeNet have also been used", GoogLeNet should be "ResNet-152"? ObjectNet uses inception (GoogLeNet) v4 while authors use v3.- Sec 3.2.1 "The higher the prediction accuracy, the higher the robustness against natural distortions". This is not necessarily true. Looking at Figure 3, it seems all models and both image / bounding box schemes would have decayed performance w.r.t distortion severity, and their slopes seem similar.- Sec 3.3 "Despite a large body of literature on whether and how much visual context benefits CNNs, the majority being focused on model accuracy, the matter has not been settled yet". "the matter" means context for robustness? please clarify.- Sec 3.3 " Around 35.5% and 12.6% of the image pixels fall inside the object bounding box and the foreground object, respectively. Around 58.5% of the bounding box pixels fall inside the object mask.". here foreground object and object mask should be the same thing? if so, the numbers seem not matching (would expect box-to-image ratio * mask-to-box-ratio = mask-to-image ratio, or these numbers are normalized differently)?- Figure 5: it is clear that seg-mask actually isn't robust to adversarial attacks (accuracy dropped significantly), which contradicts with the claim from authors.- Sec 3.3 "An attempt was made to tune the parameters to attain the best test accuracy in each case", could authors elaborate? - Sec 3.3.1. here the results seem to indicate segmask is more robust (less variations). Would the segmentation mask itself be indicative for object categories? It might be interesting to predict object classes directly from those masks as a baseline. This paper explores the problem of machine translation for African languages. The authors show that trilingual models outperform bilingual models for a set of languages spoken in Cameroon, and explore two strategies to find the best auxiliary language for a given translation pair: a manual one based on their linguistic relationship, and an automatic one based on language modelling.I agree with the authors that African languages have been underrepresented in NLP research in general and machine translation in particular. In that regard, I think that their motivation is truly admirable, and I would like to encourage them to keep working on this direction.Unfortunately, I think that the work itself does not meet the standards of the conference in its current form. While the focus on African languages is new and interesting, I find that the paper has little substance beyond that. According to the authors the paper makes 3 main contributions, which I feel are too narrow or otherwise questionable as discussed next:1) "Our first contribution is the creation and public release of a new dataset." The dataset in question is a parallel version of the Bible. There have already been some efforts to extract parallel corpora from the Bible, which are not even discussed in the paper. For instance, Mayer & Cysouw (2014) [1] report covering 830 languages, which are likely to include the ones explored in the paper. Moreover, the authors extracted the corpus from an existing website with Bible versions in >1000 languages, so this was just a scraping effort. In connection to that, I am not sure if the authors considered potential copyright restrictions when releasing their corpus.2) "Our second contribution is the proposal of a new metric to evaluate similarity between languages". The proposed method measures the similarity between two languages, L1 and L2, by applying a language model trained on L1 to L2. This looks like a rather simplistic approach and, in the lack of any other baseline, it is hard to get a sense of how good it is. Also, I am not familiar with this topic, but there is certainly some work on automatically identifying similar languages. For instance, [2] shows that the degree of overlap in the BPE vocabulary of different languages is already a good indicator of the their linguistic proximity.3) "Our third contribution is the set of empirical evidences that doing multi-task learning on multiple similar languages generally improves the performances on individual pairs." There were already in-depth studies showing that multilingual training is helpful for low-resource machine translation, see for instance [3]. Unfortunately, this is not even cited in the paper.In connection to the last point, the authors limit themselves to trilingual models and the main focus of their work is on identifying the most suitable auxiliary language. An obvious baseline that is missing in the paper is to train a multilingual model on the combination of all languages.In addition, the models used by the author are usually trained at a large scale, and might not work well with very small datasets like the Bible. Even a phrase-based statistical machine translation system might work better in this setup, and I overall miss better baselines.[1] http://www.lrec-conf.org/proceedings/lrec2014/pdf/220_Paper.pdf[2] https://aiide.org/ojs/index.php/AAAI/article/view/4677[3] https://arxiv.org/abs/1907.05019 This paper studies the optimization and generalization properties of a two-layer linear network. The considered setting is over-parameterized linear regression where the input dimension is D, number of samples is n<D, and the target dimension is m. The hidden width is h. The paper has two main results. The first result is exponential convergence of gradient flow to global minimum, where the convergence rate depends on the (m+n-1)-th singular value of an "imbalance" matrix. The second result shows that the solution found is close to the minimum L2 norm solution if certain orthogonality assumption is approximately satisfied at initially; then it was shown that if the width h is sufficiently large, then under a random initialization scheme, the solution found is close to the minimum L2 norm solution with a distance $1/\sqrt{h}$.pros:The results are not previously known to my knowledge. The proofs appear to be correct as far as I can tell.cons:My overall concern is the significance of the results. The results, while correct, do not contribute much to our understandings of optimization and generalization in deep learning. The ways in which the authors interpret the results are unsatisfactory or even misleading.1) Thm 1 shows a convergence rate of $e^{-ct}$, where $c$ is the (m+n-1)-th singular value of an imbalance matrix. On the appearance this result seems to suggest that a larger $c$ is beneficial for convergence. However I believe this suggestion is incorrect and can be very misleading. Indeed, previous work (e.g. Arora et al. 2018a) has shown linear convergence under zero imbalance ($c=0$), as cited in the paper, but Thm 1 fails to capture that. I think in general this $e^{-ct}$ is a very loose bound that does not capture the real convergence rate (unless the authors can provide convincing evidence that suggests otherwise).That said, I do think Thm 1 is an interesting theoretical result and the proof is clever. I'm concerned about the practical relevance and the possibly misleading message it sends.Another weakness is that Thm 1 only considers gradient flow but not gradient descent. 2) Thm 2 and its interpretations are unsatisfactory in a number of ways.First, we know that just doing a normal linear regression using gradient descent (starting from 0) leads to the minimum L2 norm solution. So now we go through all the trouble in the 2-layer net and finally show we can find a solution that's almost as good as linear regression -- what's the point of doing that?Of course, one may argue that we are studying a toy model in order to better understand deep learning. However, the main message from this result can be also conveyed in linear regression -- as shown in Sec 4.1, the main step is to find an invariant manifold for gradient flow such that the minimizer in that manifold must be the min-norm solution; for linear regression, such manifold also exists, which is just the span of the data points. Second, the initialization used ($1/h$ variance in both layers) is unconventional. It's different from the standard 1/fan_in initialization or the NTK parameterization. What happens if we use those more standard initializations? And what happens if we make the initialization smaller, e.g. $1/h^2$, or $1/h^{100}$? Would those change the result? The scale of the initialization is very important in this line of work (such as NTK), so this should be addressed clearly.(The authors actually claim that as $h\to\infty$ we would get the NTK solution, following Jacot et al (on page 7). I actually don't think Jacot et al.'s work directly implies this, because this paper uses a different initialization scale.)Third, the authors try to differ this result from all the NTK results, but the theorem is exactly showing that the final solution is close to the NTK solution. Isn't this a bit ironic?Fourth, the authors claim "this is the first non-asymptotic bound regarding the generalization of linear networks in the global sense." Maybe check out these papers:Implicit Bias of Gradient Descent on Linear Convolutional Networks,Implicit Regularization in Matrix Factorization.Also, many NTK papers also have non-asymptotic bounds. For 2-layer linear networks, one should be able to easily get a bound on the distance of the learned model and the min-norm solution -- might be better than Thm 2. This work tackles an existing phenomenon that is often ignored in real-world control problems -- stochastic-lengthed delays. While the motivation and examples are clear, I feel I'm completely baffled by the theory that follows. I find the definitions and, consequently, the results, extremely hard to follow. Namely, in Def. 1  why is the state-space a product of $\mathbb{R}^2$? Is this due to the action and observation delay values? If so, why are they continuous and not discrete (as mentioned in the paper)? Then, in the same definition, comes the most confusing equation regarding $f_\Delta$. What exactly is it? How can it be part of a transition probability while according to Def. 2 it is an expectation? Also, what is $s^*$ there and why is $r-r^*$ a relevant term? And most puzzling to me is the fact that $f_\Delta$ itself is recursive. That is new and surprising but barely receives any attention in the text and gets me wondering what does that imply on the process and algorithms. Similar to the confusing definitions, the text itself is very hard to comprehend as well. For example, one paragraph before Sec. 3.1 and the first one discuss an off-policy partial trajectory resampling method using very vague arguments, and the relation to what was presented up to that point in the paper is loose. Honestly, I read those two paragraphs a couple of times and couldn't understand them. Then, the following definition and theorem 1 that follow are as confusing to me as the text. At that point, I felt I could not follow the paper anymore. Lastly, the experiments apparently exhibit good results, but I cannot say anything smarter on it. Since I couldnt understand the analysis that preceded the algorithm, I cannot appreciate its qualities.Additional comments:1. The literature review is very scarce. Two examples of prior art dealing directly with RL with stochastic delays are [1, 2]. Additional multiple recent citations using SOTA algorithms for constant delay are also missing.2. Often, unclear sentences are either not backed up by references (e.g., in Sec. 2.1, it is also possible to do much better when the delays themselves are also part of the state-space), or when the reader is referred to the appendix, but there, no compelling argument to the original claim is found (e.g. Sec. 2.1,  $r-r^*$ explanation with reference to B.2). [1] Katsikopoulos, K. V., & Engelbrecht, S. E. (2003). Markov decision processes with delays and asynchronous cost collection. IEEE transactions on automatic control, 48(4), 568-574.[2] Campbell, J. S., Givigi, S. N., & Schwartz, H. M. (2016). Multiple model Q-learning for stochastic asynchronous rewards. Journal of Intelligent & Robotic Systems, 81(3-4), 407-422. This paper proposes an easy positive sampling method for deep metric learning which aims to reduce the class collapse problem which is found to harm the performance of existing DML methods.Pros: 1. This paper is well-written and easy to follow. 2. The idea is simple but makes good sense. The author also provide solid theoretical analysis of the flaw of existing methods and the advantage of the proposed easy positive sampling strategy. Cons:1. The idea of sampling easy positive for deep metric learning is actually not new. [1] already proposed an easy positive sampling method and the motivation is quite similar (to relax the constraints of intra-class variations). [1] should be cited in this paper.2. The authors only provide theoretical analysis on the binary case and claims it can be easily extended to the multi-label case, which I find not trivial.3. A concern is the limited batch size, which might cause the easy positive sample of one particular sample at different iterations to be different (and possibly from different subcluster). This might lead to inconsistent effect of pushing the same sample to different subclusters. 4. Similar to the last one, a more general problem is the theoretical analysis only consider the optimal situation but neglects the nature of batch-based training, which might bring unexpected problems.5. For the experiments, the performance improvement using the proposed easy positive sampling is not strong. Specifically, the best performance on the Cars196 and CUB200 dataset is achieved with EPS + margin, but the authors did not report the performance of margin loss  with distance-weighted sampling. Comparisons with other sampling methods on the same loss should be provided.6. The authors should design an experiment to better demonstrate the class collapsing problem on a regular dataset like CUB. The toy experiment on the MNIST dataset is not convincing.In summary, I think this paper is solid and well-motivated, but I find the idea not new and the experiments not satisfying. The latter weighs more in my decision. [1] Xuan H, Stylianou A, Pless R. Improved embeddings with easy positive triplet mining[C]//The IEEE Winter Conference on Applications of Computer Vision. 2020: 2474-2482. ##########################################################################Summary:The authors propose a framework, GenQu, for learning classical data using quantum computation. The classical computer would encode the classical data into quantum circuits. The quantum computer would then run the quantum circuit, measure the resulting quantum state, and feed the measurement data back to the classical machine. This process would repeat until the classical computer output the final result.##########################################################################Reasons for score: This framework is not new and has been widely adopted in the quantum machine learning community. It is unclear to me what is being proposed by this work. The framework is known as a variational quantum-classical algorithm and there is extensive literature for different applications in quantum computing, such as quantum chemistry, simulating quantum field theory, optimization, and machine learning. For example see references [1, 2] for existing proposals for machine learning applications. Due to the lack of meaningful contributions, I would not recommend acceptance.##########################################################################Pros: Cons: 1. The framework is not new. It is not scientifically correct to claim the proposal of a new framework "GenQu" when this has already been widely adopted in the quantum machine learning community.2.  The authors did not provide any new theoretical insights into how quantum computation can learn classical data better.3. The numerical experiments were not strong enough to justify any form of advantage using the quantum computer. Furthermore, these numerical experiments have already been presented in the literature. For example, a tutorial in Tensorflow Quantum [3] has also included such an experiment.#########################################################################[1] Havlíek, Vojtch, et al. "Supervised learning with quantum-enhanced feature spaces." Nature 567.7747 (2019): 209-212.[2] Farhi, Edward, and Hartmut Neven. "Classification with quantum neural networks on near term processors." arXiv preprint arXiv:1802.06002 (2018).[3] Peruzzo, Alberto, et al. "A variational eigenvalue solver on a photonic quantum processor." Nature communications 5 (2014): 4213.[4] https://www.tensorflow.org/quantum/tutorials/mnist This paper presents GenQu, a hybrid and general-purpose quantum framework for learning classical data through quantum states. By encoding two dimensions of data per one qubit. they demonstrate the effectiveness of their framework via two classical classification tasks, where 1 and 2 qubits are used, respectively. This paper is more like an entry-level tutorial, rather than a technical paper.  More technical contributions are needed towards a paper.1. One of the key contributions claimed by the author is that "they show the power of encoding two dimensions in one qubit ...This thereby reduces the quantum state dimensionality by 2^(n/2)." I am super surprised by this claim. What is your baseline? One qubit per one dimension of classical information? Could you refer to the paper from which you get this baseline? There are just too many papers[1] about how to encode classical data into quantum states.  The coding scheme proposed in the paper is not novel and not even state-of-the-art.  2. What is the key difference between your framework and TensorFlow Quantum[2]? For me, TensorFlow Quantum is a much stronger framework. For example, the SINGLE QUBIT KERNELIZED CLASSIFICATION case study in section 3.3 is just an illustrative example in [2].[1] Biamonte, Jacob, et al. "Quantum machine learning." Nature 549.7671 (2017): 195-202.[2] Broughton, Michael, et al. "Tensorflow quantum: A software framework for quantum machine learning." arXiv preprint arXiv:2003.02989 (2020). This paper introduces NDMZ, short for nondeterministic MuZero, a deep reinforcement learning algorithm for model-based RL that doesn't use the rules of the game to perform search. The paper's contribution is mostly focused on describing how to construct the algorithm, and experimental results are provided at the end. A good analogy is that of a player that must play a (physical) board game by not only making decisions, but also acting out the game: producing random events, such as die rolls, and moving pieces on the board. Overall, I enjoyed this paper and thought it was quite clear, but it does not feel substantial enough for a conference publication. The main contribution is the detail of how to implement stochasticity in a MuZero architecture. While interesting, there's relatively little discussion of why these are the right choices, why this is particularly challenging, or in fact what value it adds compared to existing algorithms.My main concern with the paper can be summarized as: What is this work's impact? What is the key premise that makes it a reasonable line of work? It seems that AlphaZero is just as well equipped to deal with games (where a simulator *is* available; the restriction imposed here is artificial). Demonstrating that a deep learning system can model stochastic events isn't too surprising either. I would have liked to see more discussion and empirical support arguing why this particular work brings new insights to deep RL. The easiest way to do so is to demonstrate a problem where NDMZ or MZ outperforms AlphaZero, or where a model isn't available, or is too cumbersome to be used.A minor issue also concerns the presentation. The algorithm is described in terms of 'chance policy' and 'identity policy', but I would call these just a transition model. To present them as additional players is a little surprising.Minor points:- PUCT: spell out the name - Why did you omit the L2 term in Eqn 1?- Why is the root node necessarily a choice node? This paper aims to address the issue of mitigating side effects in policy learning. The authors propose an algorithm SARL, which uses a safe policy to define a regularization term for penalizing the agent's actions deviating from the safe agent in policy learning. In the experiments, four variations for SARL are shown and compared with a baseline method based on reward penalty. The proposed algorithm is competitive across the experiments presented in the paper. I think side effects and safety in reinforcement learning is an important issue. However, this paper does a bad job in describing the problem it wishes to address and, therefore, it's unclear whether the proposed algorithm really achieves that goal. 1. The main motivation of this paper is to mitigate the side effects in learning. However, the definition of side effects were never given. It's only until Algorithm 1 is presented where the paper mentions a safety metric that the safe agent aims to optimize (is this the same s appearing in A(s|theta) and Z(s|psi) in Sec 3.2?), which however is not defined. Therefore, I do not fully understand what the objective of this learning algorithm wants to achieve. From the paper's vague description, it seems like the goal is that the learner should have high performance in the original reward while not causing high side effects. This is a multi-objective MDP problem or at least can be framed as a constrained MDP. However, the proposed algorithm, based on simple regularization with a constant weight, can address neither of these two criteria. I am wondering if the authors consider to more explicitly outline the solution concept they wish to obtain. Current hand-wavy description makes me difficult to judge whether the proposed algorithm actually solves the problem they wish to solve.2. In Algorithm 1, since the safe agent Z is updated independently of the progress of the learner agent A, when there's only a single environment, there is no point of distinguishing the so-called "zero-shot" and the online version, as in high level this dependency allows us to pretrain the safety agent alone beforehand and get the same results. Or do the authors mean zero-shot in the sense that the safe agent is trained on a different set of environments and the online version means they're trained on the same environment? 3. In the paper, the authors write multiple times that a difficulty in this problem setup is that the side effects are difficulty to define. But it seems that the proposed algorithm assumes some safety metric. How are the two related precisely? And what is that used in the experiments?4. What is S[\pi_theta] in (3)?Overall, I think the paper is rather incomplete and therefore I do not recommend acceptance. ***Summary and general comments:***This paper presents a method to parametrize a set of constraints via a linear space that may change. This method was already studied in much more depth in [1], where this idea is explored through the lens of vector bundles and retractions on them, with a convergence result appearing in the follow-up work [2].The authors then instantiate these ideas in the setting of normalizing flows via the ShermanMorrison inversion formula.***Questions:***1) For instance, parameterizing the whole orthogonal group (via the Cayley parameterization or matrix exponentials of skew-symmetric matrices) is computationally expensive. In contrast, orthogonal matrices can be cheaply perturbed into other orthogonal matrices using double Householder transforms or Givens rotations.Why is that the case? Could you please justify the first assertion?What do you mean by "it can be cheaply perturbed into an orthogonal matrix"? The best orthogonal approximation to a given matrix is the orthogonal projection given by the polar decomposition. That is expensive.2) "In contrast to this related work our method reparameterizes the update step during gradient descent rather than the parameter matrix itself"This is not really different as explained in [1]. Using a dynamic trivialization changes the metric that you are working with, but you can still use it as a parametrization. In your example, it would account for changing the problem$$\min_{x \in \operatorname{GL}(n)} f(x)$$to$$\min_{u,v \in \mathbb{R}^{2n}} f(X_0 + uv^\intercal)$$for a fixed $X_0$ and changing the problem via the dynamic trivialization framework every $N$ steps. Using this formulation, you can let the optimizer do all the heavy lifting for you. See also 5)3) Trivializations are known to have a problem with the momentum and adaptive term. I see that you use Adam in the training of the 2D distributions. How do you solve the problem of the incorrect momentum and adaptive terms when you change the base?4) A general question. I do not understand what is the motivation behind using the low-rank update besides the fact that it allows for an (amortized) low cost of the inversion.5) A follow-up of the previous question: How is $R_{X}(u,v) = X + uv^\intercal$ a sensible map to use? This map does not have its image on $\operatorname{GL}(n)$ but on all $\mathbb{R}^{n\times n}$! For example, $R_{-uv^\intercal}(u,v) = 0$ which is clearly not invertible. As such, I do not understand the claims in section 2.3 about this being a "fully flexible invertible layer". This is the reason behind having to use Algorithm 2 in PtInv. Given that Algorithm 2 is used, what is the reason behind using this parametrization at all over just doing unconstrained optimization?***Citations:***- Lezcano-Casado & Martínez-Rubio do not use Givens rotations but the exponential. The first to use Givens rotations in the context of ML was [3].- When it comes to Riemannian gradient descent, it is probably better to cite Absil's book [4] as a general reference rather than two recent papers, as this is a well studied topic.- I would recommend to clean-up the bibliography, as there are many citations that point to the arXiv when the articles have indeed been published in peer-reviewed venues.Minor:- "which is a R-diffeomorphism" -> "an"***Conclusion:***I really like the first experiment for its simplicity, trying to elucidate the behavior of the layer. It is also nice to see the improvement that this idea gives over RNVP. At the same time, when it comes to the experiments, I believe that it would have been of interest to compare this approach with other known ways to parametrize invertible linear layers, such as those that use QR, SVD or Choleski factorizations.That being said, as mentioned in 4) and 5) I do not see the reason for this being a good way to obtain invertible layers, given that even the image of the parametrization does not lie in GL(n). Furthermore, the ideas behind the framework presented in this paper were already studied in previous papers in much more depth.[1] M. Lezcano-Casado. Trivializations for gradient-based optimization on manifolds. NeurIPS, 2019[2] M. Lezcano-Casado. Curvature-Dependant Global Convergence Rates for Optimization on Manifolds of Bounded Geometry. https://arxiv.org/abs/2008.02517[3] U. Shalit, G. Chechik. Efficient coordinate-descent for orthogonal matrices through Givens rotations. ICML, 2014[4] P.-A. Absil, R. Mahony, and R. Sepulchre.Optimization algorithms on matrix manifolds. PrincetonUniversity Press, 2009- . Summary:The authors proposed a WAE-based algorithm for outlier detection, aiming at mapping outliers to a low probability region and inliers to a high probability region. The training objective is based on WAE by replacing the reconstruction error with the prior weighted loss. Experiments were performed to show the effectiveness of the proposed algorithm.  ################Strong points:. The authors proposed a training objective, which intends to map outliers to a low probability region.. The authors theoretically showed that under certain conditions the outliers and inliers can be separated by a distance metric.################Weak points:. In the abstract the authors claimed that ``We formally prove that OP-DMA succeeds to map outliers to low-probability regions''. This is not true. This conclusion is only verified in Experiment 3 using a synthetic dataset. A formal proof in Theorem 2 is regarding the separation between outliers and inliers under a distance metric. As the claim is the main contribution of the paper, more theoretical/experimental work is needed to demonstrate that.. The authors claimed that the reconstruction error can hardly separate outliers from inliers and showed an example in Figure 1. However, such conclusion may directly depend on the percentage of outliers in the training set. As an extreme case, if the training set only includes inliers, then the reconstruction error may work well in the testing phase.  . In the proposed network architecture, is the encoder deterministic or non-deterministic? It seems that the authors adopt the deterministic one but both Q(X) (deterministic) and Q(Z|X) (non-deterministic) are adopted, which is very confusing.. The authors claimed that the reconstruction error term in Equation 1 ensures the one-to-one correspondence between input, latent, and reconstructed input. This claim is questionable. Consider the loss function as the L2 norm. It is possible that different reconstructed inputs lead to the same reconstruction error. Also, it is unclear why one-to-one correspondence is critical to outlier detection.. The expression in Equation (2) is incorrect: the optimization problem should not be subject to P_Q = P_Z as the divergence penalty has already been included.. The contamination parameter \alpha is required as an input in the algorithm, which significantly limits the practical use of the algorithm. In reality, the value of \alpha can only be estimated, thus inaccurate estimation of \alpha inevitably degrades the algorithm performance. This is also confirmed in Experiment 2: if \alpha is underestimated, F1 score decreases significantly.. In experiment, the authors compared with benchmarks using F1 score. Another commonly used metric in outlier detection is AUROC. Do the authors have any comparison results regarding that?################The paper needs to be proofread. There are many grammar mistakes and typos. The paper looks at the idea of building models of natural variation of an input and then using these models to develop robust training algorithms that are less susceptible to adversarial attacks. Strength:+ The paper addresses a very important topic of adversarial robustness of DNN models and is accompanied by diligent evaluation over different datasets. Weakness:- The paper adopts an approach which is very reminiscent of Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold defense: Adversarial training using generative models. arXiv preprint arXiv:1712.09196, 2017. The key difference seems to be adoption of the idea of using auxiliary transformations (called natural perturbations) which is also very well-studied in literature, for e.g. see https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Auxiliary_Training_Towards_Accurate_and_Robust_Models_CVPR_2020_paper.pdf  So, the approach presented here is quite incremental for a premier venue such as ICLR.- The attack in Jalal et. al. paper can be launched against this approach once the attacker also access to this generative/natural model (which would be easy to build for an attacker)- The reviewer will strongly recommend reviewing the advices in https://arxiv.org/abs/1902.06705 for writing papers on defense and how to self-evaluate its robustness by suitably designing the attacker. If the defense approach uses some background/auxiliary knowledge, one must consider the attacker with this knowledge if it is accessible to the attacker. Questions to authors:- Can authors explain why an attacker can't build similar natural model to defeat the proposed defense? - Any clarification on incremental novelty from  Jalal et. al. and Zhang et. al. (and references therein) would be also useful.  This paper studies test-time adversarial robustness through a maximin framework and illustrate non-trivial robustness (under transfer attack) using domain adversarial neural network (DANN) to Linf-norm and unseen adversarial attacks. While I agree that test-time adaptation is an important and practical approach for adversarial robustness, the current version, in my opinion, does not deliver significantly novel insights, nor considering a reasonably practical threat model. My main concerns are detailed as follows.1. Impractical threat model: At test time, the maximin threat model (Def. 2) assumes the attacker to make move prior to the defender's action, which limits the attacker's ability and weakens the robustness evaluation. More importantly, it may create a false of security/robustness, as pointed out by (Athalye et al. 2018), that the robustness gain may actually come from information obfuscation and thus the results may fail to provide meaningful robustness evaluation. Although the authors mention the maximin threat model is a weaker (attack) model and part of the goal is to find an adaptation method that is "good" in this attack-move-first scenario, I couldn't see the practical utility and contributions form these results. Even in the test-time adaptation setting, the "defender-move-first" setting should be more practical. Better motivation and use cases are needed to justify why the considered setting is important.2. Due to the assumption of the considered maximin threat model, the experiments are limited to comparing accuracy on transfer attacks, which provide limited understanding of the true robustness of the victim model. Moreover, the baseline models in comparison are too weak and unfair. To have a fair comparison, the authors are suggested to compare robustness on robust models such as TRADES [R1] and adversarially trained models with unlabeled data [R2,R3], so that the baseline models also use unlabeled data. If DANN shows limited robustness against white-box attacks but stronger robustness against transfer attacks, one can only conclude that transfer attack is a weaker threat model, which is a known result. I do not see new insights from the reported results.[R1] https://arxiv.org/abs/1901.08573[R2] https://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness.pdf[R3] https://deepmind.com/research/publications/Are-Labels-Required-for-Improving-Adversarial-Robustness3. In addition to the issue of impractical threat model and lacking motivation, this paper contains too many high-level discussions accompanied with limited or even no empirical evidence. The theorem presented in the paper is a natural use of maximin inequality.  In my opinion, the current version requires significant revision. I suggest the authors carefully motivate the research goals (especially answering why the studied problems and settings are important), consolidate the claims on test-time robustness with convincing evidence, and make a broader connection to other test-time defenses other than DANN. The paper presents a tutorial to a video analysis platform software, i.e., VideoFlow, which represents a video analysis task as a computation graph, provides common functions like video decoding and database storage, integrates deep learning frameworks, e.g. Caffe/Pytorch/MXNet as built-in inference engines, and supports heterogeneous hardware such as CPU/GPU/FPGA. VideoFlow also allows the customers to develop operator, decoder, and model inference extensions. The paper presents an example application of person ReID using the VideoFlow platform. The paper claims this VideoFlow software could be used in both academic and industrial scenarios. The VideoFlow platform software is certainly a great development tool for video analysis tasks. The major concern is that if it is appropriate for ICLR to publish this tutorial which may be regarded as an endorsement to this software. Perhaps, VideoFlow could demonstrate in the exhibition of academic conferences to promote the usage in academia. Some detailed comments:1)Is VideoFlow a commercial software or a free open-source software? What kind of license does VideoFlow follow? As it integrates some functions of other software like Caffe/Pytorch/MxNet. 2)The major issue of adopting VideoFlow could be the learning curve how to use and debug applications on VideoFlow. VideoFlow appears a powerful and comprehensive video analysis platform, which abstracts and defines the interfaces among the components in a computation graph. For most of academia users, PyTorch/TensorFlow may be good to develop and verify some new deep model or algorithms. For industrial users, debugging any problem on VideoFlow requires a profound understanding of the platform. Deployment of an application could encounter issues on run-time library dependency, cross-compilation, resource limitation, e.g, on an embedded system. It would be a critical decision to adopt an open-sourced development platform for commercial applications for industrial users.3)Some minor questions:  As there is already a software named VideoFlow (de Armas, 2019), why not choose a different name for this software?  Vector<void*> appears a C++ implementation of a C-style concept, which implicitly requires the caller to agree with a protocol specified in other documents. The users may make mistakes that are hard to debug.  Overall, the proposed VideoFlow software is a great development tool for video analysis tasks using deep learning models. I encourage the authors to promote this software in the open-source community and demonstrate it in conference exhibition and trade shows. This manuscript describes a framework, VideoFlow, for deploying and analyzing video execution flows. VideoFlow abstracts the workload into computation graph and resource, and dynamically executes the graph given the resource.Although this work seems helpful for system integration and deployment, I did not see a strong connection between it and the topic of this conference. In this framework, learned models are treated as an atomic node and executed on other DL libraries. The graph mentioned in this paper, as well as terms like forward/backward, has nothing to do with the graph in machine learning concept. Therefore, I do not see this paper as a good candidate for machine learning community, but a better fit for system communities.There are some additional comments:1. Figures should come with captions to illustrate themselves, instead of just a title.2. Figure 2 is confusing. It is hard to infer from the figure what forward function is. The authors need to emphasize this forward/backward function is different from DL models' forward/backward. Summary-------This paper introduce a framework for visual analysis (so-called VideoFlow) that will be open-sourced. VideoFlow is introduced with providing a flexible, efficient, extensible, and secure visual analysis framework for both academia and industry. It has been adopted in tens of intelligent visual analysis systems.However, through the whole paper, the key contributions of the VideoFlow should be only counted as engineering efforts rather than any novelty in the scientific or research perspective.Therefore, ICLR would be not an appropriate venue for the submitted paper to be published. It is more valuable for authors to submit to other venues such as OSDI (USENIX Symposium on Operating Systems Design and Implementation).Strengths---------- The paper introduces the architecture of the proposed framework (VideoFlow) in detail, including operator, data flow, resource, graph construction, execution scheduling, etc. These details should be helpful for users to understand the logic of its working flow and how the framework should be used.- The paper also presents the evaluation of person re-identification (ReID) task on VideoFlow to show its efficiency. Weaknesses----------- The motivation of presenting such a practical framework is clear and understandable, however, as a research paper, scientific contributions are required to prove its novelty rather than the only engineering efforts. For such a purpose, the following points could be considered to improve the current paper towards a more scientific-like research paper.- Address some scientific issues that were made in the architecture of VideoFlow: original methods solving the computation/IO intensive problem, theoretical efforts for graph construction and the design of execution scheduling.- Provide some theoretical or mathematical guarantees of the efficiency in the working flow of execution scheduling (Fig. 2).- The evaluation is not sufficient to show the advantages (flexible, efficient, extensible, and secure) that claimed by authors. In this paper, only the evaluation of ReID task is presented, but how about its efficiency and other advantages for other computer vision tasks such as object detection, segmentation, face recognition, and so on.  This paper points out the fact that just developing models using deep learning frameworks is not the end of the story when it comes to building end-to-end visual pipelines. The paper introduces VideoFlow which is a framework that aims to improve the development process of streaming pipelines.Compared to the widely used TensorFlow, PyTorch, and MXNet, VisualFlow focuses on more coarse-grained blocks like a "whole network" itself instead of "layers." As such, the work is developed around a rather different units of data compared to Tensors in DNNs. This work also incorporates a GUI that lets the user edit the computation graphs. I believe this work in full fledged form may help the productivity while building visual analysis applications.Sadly, there are many shortcomings of the paper. First, the paper literally spends over 60% of the paper to describe the implementation details which does not lead to much intellectual insights. This looks more like a technical report than a research paper.Furthermore, the paper lacks on evaluation in many aspects. For example, there is no evaluation about the potential overhead of the framework. I have listed a number of questions below with my comments.The paper began with a luring abstract; however, after reading through the paper, the reader is left with only minor insights. Up to this point, the paper seems to be an amalgamation of various libraries and frameworks with a GUI wrapper. While I do believe the paper has some prospect as it does touch upon a real problem that does indeed take up a lot of resources in industry, I believe the paper is yet in a premature state to merit a publication at ICLR.Also, I believe the paper would receive a more pertinent evaluation from a systems community like OSDI, SOSP, ATC. This is because these frameworks should not be just about putting things together to make something working, but should also entail a through experimentation of the performance and overhead.Questions:- Could you please provide running examples of applications that have graphs with a complex topology?- How does the resource management behave for different scenarios such as underutilization? Could you show some visualization of that? For example, it would be very interesting to see how Dynamic Batching dynamically improves the utilization and the performance at runtime.- How can this be used for on-device scenarios, or cases where there are numerous devices? For example how does this compare to NNStreamer [1]?- If this framework were to be used in cases like Inference-as-a-Service scenario, how would this perform in terms of various QoS metrics?[1] NNStreamer, https://nnstreamer.ai  Summary:This paper uses theoretical grounding, starting with Lipshitz continuity-based assumptions on residual connections, to show why such architectures are more susceptible to adversarial inputs. In the process, the authors draw a parallel between these residual connections and neural ODEs, showing how the latter can circumvent the main reason that leads to adversarial susceptibility for the former. Finally, via empirical evaluations, they show how neural ODEs have "natural" robustness to adversarial examples: they have a non-trivial performance on adversarial inputs, despite not being explicitly trained for robustness. ##########################################################################Reasons for score: This draft in its current form lacks strong adversarial evaluation and makes strong claims without any experimental evidence. The title and the body of the paper suggest "natural adversarial robustness", but evaluated only against ($L_2$: this is my guess since it is not specified in the paper) PGD and FGSM (which, as has been seen recently in the literature, is not useful to see) attacks. Moreover, the Lipshitz-based assumptions in the proof for deriving conditions for residual connections seem a bit too strong. Keeping strong assumptions and weak empirical evaluations in mind, I feel this paper needs a lot more work before it can be considered for acceptance.  ##########################################################################Pros:   - The analysis of existing variants of network architectures that include residual connections seems interesting. It helps to see why these models are theoretically unfit to achieve adversarial robustness. Some empirical evaluation on these models would also have been nice to see: just to see how one is more/less problematic than the other. ##########################################################################Cons: - My biggest concern is the weak evaluation of the mentioned models. Even though the paper's title (and most places in the paper) talk about "natural robustness", evaluation is performed only against one kind of attack (FGSM is just PGD with the number of steps = 1) and that too for just one norm. Please include more extensive evaluation, perhaps like second-order gradient attacks, gradient-free attacks, and augmentation based adversarial attacks, or change the title to reflect your current findings. - A large body of related works seems to have been missed out in this paper's literature review. For instance, there have been several works on Lipshitz-constraint-based robustness for neural networks are highly relevant, but seem to have been missed out ([example](https://arxiv.org/pdf/1704.08847.pdf), [another example](https://openreview.net/pdf?id=HkxAisC9FQ))- The second paragraph sets up the flow of the paper to hint at "designing a deep neural network that has natural robustness", whereas the main focus of this paper goes to the extent of only evaluating existing architectures. Additionally, there is no clear evidence to suggest that adversarial robustness is even possible just via a well designed neural network.- Residual connections are not limited to skipping only one connection (Eq 1), and using this as a base assumption should either be explicitly stated or worked out for the general case.- The function $f$ is related to one specific layer and is thus parameterized by its associated weights $\theta_n$. However, Eq 7 talks about the Lipschitz continuity for the general layer. Does this mean that this model assumes that __all__ layers under consideration are Lipschitz continuous, that too with the same constant? In an ideal scenario where all layers somehow indeed, using the same constant $K$ would require taking the largest one of all layers, which can make it ridiculously large. I think this is a fatal flaw in the derivation process, and the authors should address it.- In what norm are all these examples operating? What are the parameters for the PGD attack? Configuration details like the number of steps, step size, and the number of random restarts, can make a significant difference in evaluation metrics. Also, since these attacks include randomness (especially PGD), please run them multiple times and report mean/std values.- Many numbers in Table 2 do not make any sense (my guess is this is because of the randomness in these attacks, which makes it even more important to have multiple runs). For instance, accuracy __increases** for neural ODE on CIFAR10 for FGSM when the perturbation budget is **increased**? Attack success rates should be strictly non-decreasing with increasing attack budgets since the adversary can copy-paste smaller-budget attacks and get at least the same attack success rate. The same problem holds for Resnet50 on CIFAR10 ($\epsilon=0.4$ vs $0.5$), neural ODE on MNIST $\epsilon=0.3$ to $0.4$. Please address and clarify these cons.##########################################################################Minor issues:- Page 1, last paragraph "...which has been used to solve ordinary differential equations". Reference missing.- Section 2.1, last paragraph: "...rely on using external models". This statement is not true. Popular adversarial defense techniques like feature squeezing([ref](https://arxiv.org/pdf/1704.01155.pdf)) do not augment the dataset or use an external model.- Ambiguity in notation: next to Eq 1, what does $N(h)$ signify? - The move from Eq 7 to Eq 8 is a bit non-trivial: please add more steps in between to show the process explicitly - Section 3.3.1, just above Eq 16: "It has been show that ...". Reference missing- Just above Eq 18: "can be formally rewrite" -> "can be formally rewritten as". Also, the $(1+x)^{-1}$ Taylor series expansion holds only when $|x| <1$. Is that the case here, *i.e.* is $|hf|<1$ ?- Section 3.3.2, just above Eq 20 "resembles the Runge-Kutta method of order 2". Reference missing- Is the x-axis for Figure 1 on the log-scale? Please clarify Please do not refer to adversarial inputs as "adversarial test set", as it is likely to be confused with an adversarial test set that has been generated offline and is used for evaluation.- What are the numbers in Table 2? Accuracy ($f(\hat{x}) = y$), or 1 - error success rate ($f(x) = f(\hat{x})$)? Please clarify.  The paper claims that neural ODEs are more robust to adversarial examples than ResNets and offers both empirical evidence and a theoretical explanation. However, I don't believe the theoretical analysis shows what is claimed; in fact, it doesn't seem to say anything about the magnitudes of Lipschitz constants or a relative comparison between them in the case of ResNets vs. neural ODEs. Below are the main issues with the analysis:Claims: In the statement of Thm. 3.1, it is claimed that in ResNets, the distance between z_n (output corresponding to perturbed input) and y_n (output corresponding to the original input) is bounded above by some constant times the amount of perturbation to the input. The statement does not upper bound the constant and only lower bounds it trivially with 0. Therefore, this says nothing about how robust or not robust the model is to adversarial examples -- if the constant is large, the model could be sensitive to adversarial examples; if it's small, it could be robust. So, this statement is unrelated to the main claims of the paper. In the statement of Thm. 3.2, a very similar claim is made about neural ODEs, except that the constant in front of the magnitude of perturbation (\hat{c}) is different from the constant in Thm. 3.1 (c). No claims on the relative magnitudes of \hat{c} and c are made, and so nothing can be said about the relative robustness or lack thereof of neural ODEs compared to ResNets. Proofs:If I understand correctly, Thm. 3.1 is supposed to make a claim about ResNets, which is when n (the number of steps/layers) is held constant as the step size (h) changes. In this case, the last step of the proof (eqn. 11) is incorrect, since nh does not necessarily equal to b - t_0, i.e. when n is fixed and h decreases, Euler's method will not reach b. On the other hand, Thm. 3.2 seems to aim at making a claim about neural ODEs, which is when n increases as h decreases. In this case, nh would be equal to b - t_0. However the claim that the distance between z_n and y_n would be upper bounded in the limit of h -> 0 is odd, because it holds even when no limit is taken: nh is always equal to b - t_0 regardless of how small or large h is. So this cannot serve as an explanation for the differences in behaviours between neural ODEs and ResNets (where h = 1). Significance:The theoretical analysis is trivial and is a simple application of the definition of Lipschitz continuity, which is assumed. Additionally it has nothing to do with the main claims of the paper. Conclusion: While the empirical phenomenon has been convincingly demonstrated and is intriguing, the theoretical analysis doesn't contain anything substantive and isn't much of an explanation. No other attempt at explaining this phenomenon (empirically or otherwise) was made other than the provided theoretical analysis. So I believe the paper is incomplete and will need substantially more effort at explaining the phenomenon.  The paper discusses a model for learning the architecure of a convolutional networks starting from a fully connected graph. The idea is to learn the adjacency graph of the model together with the weights of the networks.Strenghts:- The idea of thinking out-of-the-box by imagining new architectures is very attractive and interesting.Weaknesses:- The actual advantages in the model do not look apparent given the results in the evaluation section.- The theoretical foundation of this work is unclear. For example, it is unclear how the proposed solution will work in practice in terms of back-propagation.- The performance results show that the proposed method is characterized by performance close to those of existing methods.In general, I really welcome this type of work: non-conventional, experimental and quite radical in terms of approach. However, unfortunately, the authors do not provide a convincing description of their approach. Unfortunately, it does not appear that the method is developed on a sufficiently strong theoretical basis. For example, it is unclear how back-propagation work in these circumstances when you don't have a stacked architecture.The choice of the thresholds and the actual learning of the adjacency matrix is not described in sufficient detail.The actual computation complexity and the trade-offs in terms of computational complexity/accuracy is unclear.In the experimental results, the actual performance of the method appear very similar to the other methods. In some cases they might be the same since the confidence intervals are overlapping.Questions:- What is the theoretical foundation of the method? Given the fully connected nature of the graph, how does back-propagation work in this case? In a sense, in fact, you have a DAG, how do you deal with cycles? When you do you stop the back-propagation if you do not have a stacked architecture?- Could you please provide the confidence intervals for all the results you presented? In fact, it seems that the values related to your approach are better than existing techniques in some cases but they look very close?- Is the computational complexity justified? Also note: you probably need a larger number of samples to learn the additional adjacency graph. What is the trade-off? Given the gain in terms of performance, the actual additional complexity might not be completely justified. SummaryThe authors address the multi-agent learning issue of agents needing to infer other agents' policies to estimate their expected reward correctly. This can be done synchronously (agents see other policy models), or asynchronously (agents cannot see other agents' policies) -- if I understand the authors' nomenclature/illustrations correctly.Certainly improving the stability of multi-agent learning with just first-order methods and without the need to infer other agent policies is an important and interesting problem. However, I found the manuscript hard to follow and experimental proof unconvincing.- Clarify issues: It seems that the authors end up with a multi-agent variation of TRPO in Eq 16. They seem to basically assume that agent policies are independent, so they can rewrite the advantage A(s, u^a) of agent a as marginalization of A(s, u^joint) over the actions/policies of the other agents. From this, they then apply TRPO to each agent individually, using the most conservative trust region radius among the N agents. I'm not convinced this is that novel.- The dashed lines in Figure 1 make the relationships between agents a bit hard to read. - There are minor English grammar and syntax issues throughout, e.g., ' each agent has a local trajectory Äa consists of historical observation and action'. - The authors should visualize and better explain why in certain situations their algorithm seems to do better or not (e.g., 2c_cs64zg vs 2m_vs_1z: what's the difference? The authors only mention that 2m_vs_1z is "hard").- The results in Figure 3 seem to use only 1 seed? The authors should repeat experiments with more (5 or more) random seeds to show how significant the gains are. I encourage the authors to add more experimental proofs and make the presentation clearer. The paper studies embeddings of directed graphs based on SVD. It proposes an interpretation of the embedding obtained from the normalized adjacency matrix in terms of the forward-backward random walk on the graph. In such random walk odd steps are taken using the edges of the graph while even steps are taken using reverse edges. Such an interpretation seems to be a trivial extension of the work for undirected graphs. For example, consider bipartite graphs, in such graphs the forward-backward random walk can be seen as a standard random walk on the graph with directions removed. Indeed, if the walk starts in part A then remove all edges from B to A and make A to B edges undirected. Strengths:+ A variational approach to meta-learning is timely in light of recent approaches to solving meta-learning problems using a probabilistic framework.+ The experimental result on a standard meta-learning benchmark, miniImageNet, is a significant improvement.Weaknesses:- The paper is motivated in a confusing manner and neglects to thoroughly review the literature on weight uncertainty in neural networks.- The SotA result miniImageNet is the result of a bag-of-tricks approach that is not well motivated by the main methodology of the paper in Section 2.Major points:- The motivation for and derivation of the approach in Section 2 is misleading, as the resulting algorithm does not model uncertainty over the weights of a neural network, but instead a latent code z corresponding to the task data S. Moreover, the approach is not fully Bayesian as a point estimate of the hyperparameter \alpha is computed; instead, the approach is more similar to empirical Bayes. The submission needs significant rewriting to clarify these issues. I also suggest more thoroughly reviewing work on explicit weight uncertainty (e.g., https://arxiv.org/abs/1505.05424, http://proceedings.mlr.press/v54/sun17b.html , https://arxiv.org/abs/1712.02390 ).- Section 3, which motivates a combination of the variational approach and prototypical networks, is quite out-of-place and unmotivated from a probabilistic perspective. The motivation is deferred to Section 5 but this makes Section 3 quite unreadable. Why was this extraneous component introduced, besides as a way to bump performance on miniImageNet? - The model for the sinusoidal data seems heavily overparameterized (12 layers * 128 units), and the model for the miniImageNet experiment (a ResNet) has significantly more parameters than models used in Prototypical Networks and MAML.- The training and test set sampling procedure yields a different dataset than the one used in e.g., MAML or Prototypical Networks. Did the authors reproduce the results reported in Table 1 using their dataset?Minor points:- abstract: "variational Bayes neural networks" -&gt; variational Bayesian neural networks, but also this mixes an inference procedure with just being Bayesian- pg. 1: "but an RBF kernel constitute a prior that is too generic for many tasks" give some details as to why?- pg. 2: "we extend to three level of hierarchies and obtain a model more suited for classification" This is not clear.- pg. 2: " variational Bayes approach" -&gt; variational Bayesian approach OR approach of variational Bayes- pg. 2: "scalable algorithm, which we refer to as deep prior" This phrasing is strange to me. A prior is an object, not an algorithm, and moreover, the word "deep" is overloaded in this setting.- pg. 3: "the normalization factor implied by the "" sign is still intractable." This is not good technical presentation.- pg. 3: "we use a single IAF for all tasks and we condition on an additional task specific context cj" It might be nice to explore or mention that sharing parameters might be helpful in the multitask setting...- Section 2.4 describes Robbins &amp; Munro style estimation. Why call this the "mini-batch" principle? First, I apologize to the authors and ACs for the late review, since this paper desearves much more time to judge the quality. Summary: This paper proves that the gradient descent/flow converges to the global optimum with zero training error under the settings (1) the neural network is a heavily over-parameterized ReLU network (i.e., requiting Omega(n^6) neurons); (2) the algorithm update rule ignores the non-differentiable point; (3) the parameters in the output layer (i.e., a_is) are fixed; (4) the data set has some non-degenerate properties and comes from a unit ball. The proof relies on the fact that the Gram matrix is always positive definite on the converging trajectory. Pros: The proof is simple and seems to be correct. The paper is paper is written clearly  and easy to follow. Cons:The problem setting considered in this paper does not seem to be difficult enough. The difficulty of analyzing the landscape property of a ReLU network and proving the global convergence of the gradient descent mainly lies in the following three perspective and this paper does not try to tackle any one of them. First, it is very hard to characterize the landscape or the convergence trajectory at/ near the non-differentiable point and this paper fails to touch it. The parameter space is separated into several regions by the hyperplanes and the loss function is differentiable in the interior of each region and non-differentiable on the boundary. I believe the very first question authors need to answer is wether there are critical points on the boundary and why the sub-gradient descent escapes from  any of these points. However, in this paper, authors avoid this problem by defining an update rule used  in practice and this rule does not use the sub-gradient at the non-differentiable point. Thus, it is totally unclear to me wether this global convergence result comes from the fact that this update rule can generally avoid the non-differentiable  points on the boundary or the fact that the landscape is so nice such that there are no critical points on the boundary or the fact that all points on the convergence trajectory is differentiable only in this unique problem.Second, the problem is much easier if the loss is not jointly optimized over the parameters in the first and second layer. Having parameters in one layer fixed does not seem to be a big problem at first glance, but then I realize it indeed makes the problem much easier, which can be seen in the following example. If we randomly sample the weight vector w_i from N(0, I) and only optimize  over the parameters in the second layer, then it is straightforward to show the following result.Result: If \lambda_\min(H^\inf)&gt;0 and m=\Omega(n\log n), then with high probability, the loss function L is strongly convex with respect to a=(a_1,&, a_m) and the loss function is zero at the global minimum.The above result shows that if we fix the parameters in the first layer and only optimize the parameters in the second layer, it is easy to prove the global convergence with a linear convergence rate. In fact, this result does not require the samples coming from a unit ball and the network size is only slightly over-parameterized. Therefore, if we are allowed to fix the parameters in some layer, how are the result presented in this paper fundamentally different from the above result. Authors may say that the loss is not convex with respect to the weights in the first layer even if the second layer is fixed. However, when the second layer is fixed, the loss function is  smooth and convex in each parameter region and some recent works have shown that in this case, the loss function is a weakly global function. This means that the loss function is similar to a convex function except those plateaus and this further indicates that if the initial point is chosen in a strictly convex basin, the gradient descent is able to converge to a global min.  However, the problem becomes far more difficult if the loss is jointly optimized over  all parameters in the first and second layer. This can be easily seen since in each parameter region, the loss is no longer a convex function and this may lead to some high order saddle points such that the gradient descent cannot provably escape. Furthermore, the critical points on the boundary can be much more difficult to characterize for this joint optimization problem. Third, the dataset considered in this paper does not seem to be a fundamental pattern and it seems more like a technical condition required by the proof. It is easy to see that a linearly separable dataset does not necessarily satisfy the conditions that 1) the gram matrix is positive definite and that 2) samples come from the surface of a unit ball. Therefore, I do not understand the reason why we need to analyze this pattern. Clearly, in practice, the data samples is unlikely sampled from a ball surface and it is totally unclear to me why the gram matrix is necessarily positive definite. I understand that some technical assumptions are needed in a theoretical work, but I would like to see more discussions on the dataset, e.g., some necessary conditions on the dataset such that the global convergence is possible.Last, I understand that the over-parameterization assumption is needed. In fact, I expect the network size to be of the order Omega(n*ploylog(n)). I am wondering wether Omega(n^6) is a necessary condition or wether there exists a case such that Theta(n^6) is required. Above all, I believe this paper is a half-baked paper with some interesting explorations. In summary, it cannot deal with non-differential points, which is considered a major difficulty for analyzing ReLU. In addition, it makes an un-justified assumption on some matrix, it requires too many neurons, and fixed 2nd layer. With so many strong assumptions, and compared to related works like [1], Mei et al., Bach and ..., its contribution is rather limited.[1] https://arxiv.org/abs/1702.05777 This paper proposed a way to detect a skew in the distribution of classes in a stream of images and reweight the class priors accordingly, to estimate the final posterior probabilities of present classes. This probability re-calibration is referred to as the probability layer. A simple algorithm is proposed to detect the class distribution skew. The proposed benefit of this method is that they do not require fine-tuning any network parameters using newly skewed data. Overall the method is quite simple and heuristic. The technical contribution - i) updating class priors online ii) detecting class skews, is marginal. The evaluation is performed on a contrived setting of skewed imagenet images. I would have liked to see some evaluation on video stream data where the skews are more natural. In real scenarios, the class specific appearances P_{X|Y}(x|i) as well as class distributions P_Y(i) change online. The method seems incapable to handle such problems.  In these situations, there is no simple fix, and one needs to resort to transfer. The paper describes an interesting tweak of the standard GAN model (inspired by IPM based GANs) where both the generator and the discriminator optimize relative realness (and fakeness) of the (real, fake) image pairs. The authors give some intuition for this tweak and ran experiments with CIFAR10 and CAT datasets. Different variants of the standard GAN and the new tweak were compared under the FID metric. The experimental setup and details are provided; and the code is made publicly available. The results are good and their tweak seems to help in most of the cases. The paper, however, is not very well written and is not of publication quality.  All the insights given in Section 3 are wrong, incomplete and unsatisfying. For example, in Section 3.4, the authors suggest that gradient dynamics of the tweaked model (with some unrealistic and infeasible assumptions) is same as that of an IPM-GAN and contribute to stability. This is wrong. Similar dynamics (even under the unrealistic assumption), does not imply similar performance. In fact, if one is trying to move towards IPM dynamics, then one should try to tweak an IPM model directly. Section 3.2 also seems wrong from my understanding of GAN training. Section 3.3 could also be improved. In fact, any explanations based on minimizing JS divergence is incomplete without answering as to why JS divergence minimizing is the best thing to do. The author should have provided more comparison images to rule out the fact that the tweak is not overfitting for the FID metric. The benchmarks are also weak and more experiments need to be done (Eg, CelebA). The paper aims to connect "distributionally robust optimization" (DRO) with stochastic gradient descent. The paper purports to explain how SGD escapes from bad local optima and purports to use (local) Rademacher averages (actually, a  generalization defined for the robust loss) to explain the generalization performance of SGD.In fact, the paper proves a number of disjointed theorems and does very little to explain the implications of these theorems, if there are any. The theorem that purports to explain why SGD escapes bad local minima does not do this at all. Instead, it gives a very loose bound on the "robust loss" under some assumptions that actually rule out ReLU networks.The Rademacher results for robust loss looked promising, but there is zero analysis suggesting why these explain anything. Instead, there is vague conjecture. The same is true for the local Rademacher statements. It is not enough to prove a theorem. One must argue that it bears some relationship to empirical performance and this is COMPLETELY missing.Other criticisms:1. One of the first issues to arise is that the definition of "generalization error" is not the one typically used in learning theory. Here generalization error is used for what is more generally called the risk.  Generalization error often refers to the difference R(theta) - ^R(theta) between the risk and the empirical risk (i.e., the risk evaluated against the empirical distribution). (Generally this quantity is positive, although sometimes its absolutely values is bounded instead.)  Another issue with the framing is that one is typically not interested in small risk in absolute terms, but instead small risk relative to the best risk available in some class (generally the same one that is being used as a source of classifiers). Thus one seeks small excess risk. I'm sure the authors are aware of these distinctions, but the slightly different nomenclature/terminology may sow some confusion.2. The unbiased estimate suggested on page 2 is not strictly speaking an estimator because it depends on \lambda_0, which is not measurable with respect to the data. The definition of K and how it relates to the estimate \hat \lambda is vague. Then the robust loss is introduced where the unknown quantity is replaced by a pre-specified collection of weights. If these are pre-specified (and not data-dependent), then it is really not clear how these could be a surrogate for the distribution-dependent weights appearing in the empirical distributionally robust loss.Perhaps this is all explained clearly in the literature introducing DRO, but this introduction leaves a lot to be desired.3. "This interpretation shows a profound connection between SGD and DRO." This connection does not seem profound to a reader at this stage of the paper.4. Theorem 2 seems to be far too coarse to explain anything. The step size is very small and so 1/eta^2 is massive. This will never be controlled by 1/mu, and so this term alone means that there is affectively no control on the robust loss in terms of the local minimum value of the empirical risk.5. There seems to be no argument that robustness leads to any improvement over nonrobust... at least I don't see why it must be true looking at the bounds. At best, an upper bound would be shown to be tighter than another upper bound, which is meaningless.Corrections and typographical errors:1. There are grammatical errors throughout the document. It needs to be given to a copy editor who is an expert in technical documents in English.2. "The overwhelming capacity ... of data..." does not make sense. The excessive complexity of the sentence has led to grammatical errors.3. The first reference to DRO deserves citation.4. It seems strange to assume that the data distribution P is a member of the parametric model M. This goes against most of learning theory, which makes no assumption as to the data distribution, other than the examples being i.i.d.5. You cite Keskar (2016) and Dinh (2017) around sharp minima. You seem to have missed Dziugaite and Roy (2017, UAI) and Neyshabur et al (NIPS 2017), both of which formalize flatness and give actual generalization bounds that side step the issue raised by Dinh.6. "not too hard compared" ... hard?7. Remove "Then" from "Then the empirical robust Rademacher...". Also removed "defined as" after "is".8. "Denote ... as an" should be "Let ... denote the..." or "Denote by ... the upper ..."9. " the generalization of robust loss is not too difficult" ... difficult? 10. "some sort of solid, " solid?11. "Conceivably, when m and c are fixed, increasing the size of P reduces the set c". Conceivably? So it's not necessarily true? I don't understand the role of conceivably true statements in a paper.[This review was requested late in the process due to another reviewer dropping out of the process.] &gt; Even though the paper details the underlying Markovian setup in Section 2, it is unclear to the reader how this knits with the FFNN architecture, for example what are the Markovian functions at hidden layer and output layer. Are they all conditional probabilities? How do you prove that this is what occurs within each node?&gt; Why is the functional form of f_\theta in Eq 1? &gt; How many hidden layers are in place?&gt; What is the Stochastic dynamical process in Figure A and how is this tethered to DyMon? &gt; The authors mention an nth-order Markovian process implemention but is this not the case with any fully connected neural network implementation? What the reader fails to see is why DyMoN is different to these already-existing architectures.&gt; In the teapot example, the authors mention a DyMoN architecture. (Page 8). Is this what is used throughout for all the experiments? If yes, why is it generalizable and if not, what is DyMoNs architecture? You could open the DyMoN box in Figure 10 (1) and explain what DyMoN consists. Section 2 is the crux of the paper and needs more work - explain the math in conjunction to the deep architecture, what is the 'deep' architecture and why it is needed at all. Then go on to show/prove that the Markovian processes are indeed being realized. Edit: changed "Clarity"[Relevance] Is this paper relevant to the ICLR audience? yes[Significance] Are the results significant? no[Novelty] Are the problems or approaches novel? no[Soundness] Is the paper technically sound? okay[Evaluation] Are claims well-supported by theoretical analysis or experimental results? marginal[Clarity] Is the paper well-organized and clearly written? noConfidence: 3/5Seen submission posted elsewhere: NoDetailed comments:In this work, the authors compare several state-of-the-art approaches for high-resolution microscopy analysis to predicting coarse labels for the outcomes of pharmacological assays. They also propose a new convolutional architecture for the same problem. An empirical comparison on a large dataset suggests that end-to-end systems outperform those which first perform a cell segmentation step; the predictive performance (AUC) of almost all the end-to-end systems is statistically indistinguishable.=== Major commentsThe paper is primarily written as though its main contribution is as an empirical evaluation of different microscopy analysis approaches. Recently, there have been a large number of proposed approaches, and I believe a neutral evaluation of these approaches on datasets other than those used by the respective authors would be a meaningful contribution. However, the current paper has two major shortcomings that prevent it from fulfilling such a place.First, the authors propose a novel approach and include it in the evaluation. This undercuts claims of neutrality. (Minor comments about the proposed approach are given below.) Second, the discussion of the results of the empirical evaluation is restricted almost solely to repeating in text the what the tables already show. Further, the discussion focuses only on the top line numbers, with the exception of a deep look at the Gametocytocidal compounds screen. It would be helpful to instead (or additionally) identify meaningful trends, supported by the data acquired during the experiments. For example: (1) Do the end-to-end systems perform well on the same assays? (2) Would a simple ensemble approach improve things? if they perform well on different assays, then that suggests it might. (3) What are the characteristics of the assays on which the CNN-based approaches perform well or poorly (i.e., how representative is Figure 5)? (4) What happens when the FNN-based approach outperforms the CNN-based ones? in particular, what happens in A13? (5) How sensitive are the approaches to the number of labeled examples of each assay type? (6) Are there particular compounds which seem particularly informative for different assays?A second major concern is whether the binarized version of this problem (i.e., assay result prediction) is of interest to practitioners. In many contexts, quantitative information is also important (how much of a response do we see?). While one could imagine the rough qualitative predictions (do we see a response?) shown here as an initial filtering step, it is hard to believe that the approach proposed here would replace other more informative analysis approaches.  === Minor commentsAre individual images from the same sample image always in only the training, validation, or testing set? that is, are there cases where some of the individual images from a particular sample image are in the training set, while others from that sample image are in the testing set?I did not find the dataset construction description very clear. Does each row in the final, 10 574 x 209 matrix correspond to a single image? Does each image correspond to a single row? For example, it seems as though multiple rows may correspond to the same image (up to four? the three pChEMBL thresholds as well as the activity comment). What is the order in which the filtering and augmenting happens? It would be very helpful to provide a coherent, pipeline description of this (say, in an appendix).Do all the images in the dataset come from the same microscope (and cell line) at the same resolution, zoom, etc.? If so, it is unclear how well this approach may work for images which are more heterogeneous. There are not very many datasets of the size described (I believe, at least) available. This may significantly limit the practical impact of this work.How many epochs are required for convergence of the different architectures? For example, MIL-net has significantly fewer parameters than the others; does it converge on the validation set faster?=== Typos, etc.The references are not consistently formatted.not loosing -&gt; not losingdoesnt -&gt; does not This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of interesting states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.Clarity:I found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? Its not clear from the algorithm box. Some important details are also left out. The section on Montezumas Revenge mentioned that the goal set was initialized using a custom edge detection algorithm. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I cant find it anywhere in the paper. Omissions like this make the method completely unreproducible. Novelty:The idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of Dynamic Abstraction in Reinforcement Learning via Clustering by Mannor et al. or of Learning Options in Reinforcement Learning by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.Significance:I was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezumas Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, its not clear how this method could address scalability issues. Repeated clustering could become expensive and its not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.Overall quality:The proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) Im not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.Minor comments:- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I dont understand what it means for a problem to have scaling issues.- Please check your usage of \cite and \citep. Some citations are in the wrong format.- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\gamma max Q) are held fixed in Q-learning. The paper presents a new evaluation platform based on massive multiplayer games, allowing for a huge number of neural agents in persistent environments.The justification evolves from MMO as a source of complex behaviours arguing that these settings have some characteristics of life on earth, being a competitive game of life. However, there are many combinations with completely different insights and implications. The key characteristics for the setting in this paper seem to be:1.        Cognitive evolution with learning, rather than physical or just genetic evolution (all bodies and architectures are equal)2.        Changing environments (tasks), between parameter updates3.        Survival-oriented rewardsAnd for some experiments some agents share policy parameters to simulate species.From the introduction and the rest of the paper, its not clear whether the same platform can be used with agents that are not neural, or even agents that are hardcoded (for the sake of diversity or to analyse specific behaviours). This is an important issue, as other platforms allow for the definition of some baseline agents, including random agents, agents with simple policies, etc.The background and related work section covers MMO and artificial life, but has some important omissions, especially those ideas in the recent literature that are closest to this proposal.First, why cant Yang et al., 2018 be extended with further tasks? Second, conceptually, the whole setting is very similar to the Darwin-Wallace setting proposed in Orello et al. 2011:@inproceedings{hernandez2011more,  title={On more realistic environment distributions for defining, evaluating and developing intelligence},  author={Hern{\'a}ndez-Orallo, Jos{\'e} and Dowe, David L and Espa{\~n}a-Cubillo, Sergio and Hern{\'a}ndez-Lloreda, M Victoria and Insa-Cabrera, Javier},  booktitle={International Conference on Artificial General Intelligence},  pages={82--91},  year={2011},  organization={Springer}}The three characteristics mentioned before are the key elements of this evaluation setting, which changes environments between generations. Also, the setting is presented in the context of evaluation and experimentation, as this manuscript.Third, regarding multi-agent evaluation setting, Marlo over Minecraft (Malmo) is covering this niche as well. https://marlo-ai.github.io/Although it is episodic and the number of agents is limited, this should be compared too.Nevertheless, the authors should make a more convincing argument about why we need *massively* multiplayer settings. Why is it the case that some behaviours and skills appear with thousands of agents but cannot appear with dozens of examples? In evolutionary game theory, for instance, some complex situations emerge from very few agents.Finally, the use of agents that have to survive with health, food and water and its use as experimental setting can be found in Strannegård et al. 2018.   https://www.degruyter.com/downloadpdf/j/jagi.2018.9.issue-1/jagi-2018-0002/jagi-2018-0002.pdfFigures are not very helpful. Especially the captions do not really explain what we see in the figures. For instance, Figure 2 doesnt show much. Figure 3 left and middle show some weird dots and patterns, but they are not explained. Also, the one on the right tries to show ghosting, but colours and their meaning are not explained. Similarly, it is not clear what the agents see and process. I assume it is a local grid as the one seen in figure 4. But this is quite an aerial view, and other grid options might do the job as well.Similarly, some actions are mentioned (it seems that N, S, E, W and Pass? plus some attack options, but they are not described). In the end, I understand many choices have to be made for any evaluating setting, but many choices are very arbitrary (end of section 3 and especially experiments) and there is a lot of tuning, so its unclear whether some of the observations happen just in a particular combination of choices, but are more general. The authors end up with many inconclusive observations and doubts (perhaps) about small changes, at the end of section 5.Other things such as the spawn cap and the server merge are poorly explained, with clear definitions and proper justification of their role. Similarly, Im not sure about how reproduction takes place or not, and if so, whether weights are inherited or reinitialised. Something related is said about species.I found the statement about multiagent competition being a curriculum magnifier, not a curriculum itself, very interesting, but is this really shown in the paper or elsewhere?In general, I miss many details and justifications for the whole architecture and mechanism of this neural MMO.Pros:-        Designed to be scalable-        Goes in the right direction of benchmarks that can capture generally variable (social) behaviour.Cons:-        Poor comparison with existing platforms and similar ideas.-        Too many arbitrary decisions for the setting and the experiments to make it work or show complex behaviours-        The paper needs extensive rewriting, clarifying many details, with the figures really helping for the understanding.Typos and minor things: -        Susan Zhang 2018 is named a couple of times, but the reference is missing. Also, it is quite unusual to use the given name for this researcher while this is not done for any other of the references.-        as show in Figure 2 -&gt; shown-        impassible -&gt; impassable This paper presents a strategy to overcome the limitation of fixed input image sizes in CNN classifiers. To this end, the authors incorporate some local and local attention modules, which fit inputs of arbitrary size to the fixed-size fully connected layer of a  CNN. The method is evaluated on three public classification benchmarks: CIFAR-10, ImageNet and Kaggle-Furniture128. The results are better than those of the baseline architecture with fixed input size.Even though the need of handling arbitrary input size is an interesting problem, I have several major concerns about this paper:- One of the main problems of this paper is its presentation, both the writing and methodology. The writing is very poor, with continuous errors and many wrong definitions and concepts. For example, authors talk about data argumentation, pooling reduces the size of the hidden layers,back-to-back convolutional layersFurther, the paper is not well structured, which makes it very hard to follow.Methodology:Another major concern is that I do not see how this approach allows the network to be input-size independent. If one looks at table 1, in both AIN-121 and AIN-169 the GAIL module employs kernel sizes equal to M/32xN/32, with M and N denoting the input image sizes. In this case, for each image, the kernel size will be different and, consequently, the number of learnable parameters. It is not clear to me how this is solved in this paper, as it ultimately results in a different architecture for each different input size.When doing the sum on the proposed module, what does the result represent? absolute sum? mean of the sum? I also believe that a lot of information is lost when performing this operation (for example going from 32 to 1), in addition of the other spatial reductions during the network forward pass. Please comment on this and give a more detailed information about the proposed module.Evaluation: In CIFAR-10, authors say that keep MOST of the setting similar to ResNet. What is then difference with the training with ResNet? For a fair comparison both settings should remain the same. In addition, what is the benefit of evaluating this approach on CIFAR-10, as the images are all of the same size? Furthermore, improvement is marginal with respect to the baselines (and it is not clear what is the reason behind the improvement), while increasing the model complexity by nearly 50%.Kaggle-Furniture128: Why the learning is stopped exactly at epochs 38 and 53? Is this the same for all the networks? DenseNet and ResNet are pre-trained with what dataset?ImageNet: In table 4, while the results for the baselines are evaluated on the validation set, the test set is used for evaluating the proposed approach. Furthermore, some results on the test set are obtained with augmentations. The reported values should correspond to the original test set without any kind of modification.Minor comments:The authors assess the input fixed-size problem as a main problem in image processing. Despite being a limitation, some other image processing tasks, such as semantic segmentation, do not suffer from this problem, as CNNs are fully convolutional, and can accommodate images of arbitrary size.Many inconsistencies between terms: LAIL and then LAIN and GAIL and GAIN.' This paper presents a feature quantization technique for logistic regression, which has already been a common practice in  many finance applications.  The text feels rushed. From the current presentation, I find it difficult to understand what is the motivation of adopting the proposed relaxation of the optimization method, and how is the neural network-based estimation strategy connected to the logistic regression model. It seems the difference lies in the parameterized nonlinear transformation such that the cutting points can be somehow optimized.  The quality of the experiments performed is way below the expectation for ICLR. Although numerical experiments are performed on both simulated data and credit scoring data, it is still unclear whether the proposed method has superiority over competitors.  Question: In the test phase, how would the proposed method handle features that are not seen in the training phase? This paper presents an empirical analysis of the convergence of deep NN training (in particular in language models and speech).Studying the effect of various hyperparameters on the convergence is certainly of great interest. However, the issue with this paper is that its analyses are mostly *descriptive*, rather than conclusive or even suggestive. For example, in Figure 2, it is shown that the convergence slope of Adam is steeper than that of SGD, when the x-axis is the model size. Very naturally I would be interested in a hypothesis like Adam converges quicker than SGD as we increase the model size, but there is no discussion like that. Throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice. I dont have a good take-away after reading it.The writing of this paper also needs to be improved significantly. In particular, lots of statements are made casually without justification. For example,If hidden dimension is wide enough to absorb all the information within the input data, increasing width obviously would not affect convergence -- Not so obvious to me, any reference? Figure 4 shows a sketch of a models convergence curve ... -- its not a fact but only a hypothesis. For example, what if for super large models the convergence gets slow and the curve gets back up again?In general, I think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results. Overview.The authors present an algorithm for lowering the variance of the score-function gradient estimator in the special case of stochastic binary networks. The algorithm, called Augment-REINFORCE-merge proceeds by augmenting binary random variables. ARM combines Rao-Blackwellization and common random numbers (equivalent to antithetic sampling in this case, due to symmetry) to produce what the authors claim to be a lower variance gradient estimator. The approach is somewhat novel. I have not seen other authors attempt to apply REINFORCE in an augmented space and with antithetic samples / common random numbers, and Rao-Blackwellization. This combination of techniques may be a good idea in the case of Bernoulli random variables. However, due to a number of issues discussed below, this claim is not possible to evaluate from the paper.Issues/Concerns- I assess the paper in its current form as too far below the acceptable standard in writing and in clarity of presentation, setting aside other conceptual issues which I discuss below. The paper contains many typos and a few run-on sentences that span 5-7 lines. This hinders understanding substantially. A number key terms are not explained, irregularly. Although the paper assumes that readers do not know the mean and a variance of a Bernoulli random variable, or theof  definition of an indicator function, it does not explain what random variable augmentation means. The one sentence that comes close to explaining it seems to have a typo: "From (5) it becomes clear that the Bernoulli random variable z < Bernoulli(Ã(Æ)) can be reparameterized by racing two augmented exponential random variables ...". It is not clear what is meant by "racing," here, and I do not find it clear from equation (5) what is going on. Unfortunately, in the abstract, the paper claims that variance reduction is achieved by "data augmentation," which has a very specific meaning in machine learning unrelated to augmented random variables, further obfuscating meaning. Similarly, the term "merge" is not explained, despite the subheading 2.3.- Computational issues are not addressed in the paper. Whether or not this method is useful in practice depends on computational complexity- No effort is made to diagnose the source of the variance reduction, other than in the special case of analytically comparing with the Augment-REINFORCE estimator, which does not appear in any of the experiments. - No effort is made to empirically characterize the variance of the gradient estimator, unlike Tucker et al (2017) and Grathwohl et al. (2018).- The algorithm presented in the appendix appears to only address single-layer stochastic binary networks, which are uninteresting in practice.- Figure 2 (d), (e), and (f) all show that ARM was stopped early. Given that RELAX and REBAR overfit, this is a little troubling. Overal, these results are not very convincing that ARM is better, particularly in the absence of variance analysis (empirically, or other than w.r.t. the same algorithm without the merge step). All algorithms should be run for the same number of steps, particularly in cases where they may be prone to overfitting.- Figure 1 I believe contains an error for the REINFORCE figure. In my own research I have run these experiments myself, with a value of p close to the one used by the authors. REBAR and RELAX both reduce to a REINFORCE gradient estimator with a control variate that is differentiably reparametrizable, and so the erratic behaviour of the REINFORCE estimator in this case is likely wrong.- There is a mysterious sentence on page 6 that refers to ARM adjusting the "frequencies, amplitudes, and signs of its gradient estimates with larger and more frequent spikes for larger true gradients"-The value to the community of another gradient estimator for binary random variables is low, given the plethora of other methods available. Given the questions remaining about this methodology and its experiments, I recommend against publication on this basis also.- Table 2 compares results that mix widely different architectures against each other, some taken directly from papers, others possibly retrained. This is not a valid comparison to make when evaluating a new gradient estimator, where the model must be fixed. The paper addresses the problem of truncating trajectories in Reinforcement Learning. The scope is right for ICLR. The presentation is pretty good as far as the English goes but suffers from serious problems on the level of formulating concepts. Overall, I believe that the issue addressed by the paper is important, but I am not sure whether the approach taken by the authors addresses it.I have the following complaints.1. The paper suffers from a fundamental confusion about what problem it is trying to address. There are two straightforward ways to formulate the problem. First, we can formulate the task as solving an MDP with infinite trajectories and ask the question of what we can learn training from finite ones. The learned policies would then be evaluated on the original MDP (in practice using trajectories that are, say, an order of magnitude longer or, for a simple MDP, analytically). Second, we can consider the family of episodic MDPs parametrised by trajectory length T and ask the question of what we can learn by training on some values of T and evaluating on others. These problems are similar, but not the same and should be carefully distinguished. Right now, the introduction reads like the authors were trying to use the first approach but Section 4 reads like they are doing the second: "If during training we just used times up to T, but deployed the agent in an environment with times greater than T". Either way, the concept of bias, which appears throughout the paper, isn't formally defined anywhere. I believe that a paper that claims to address bias should have an equation that defines it as a difference between two clearly defined quantities. It should also be clearly and formally distinguished which quantities are deterministic and which are random variables. The introduction seems to (implicitly?) define bias as a random variable, section 3.1 seems to talk about "bias introduced by initializing the network". As the paper stands now, the working definition of bias used by the authors seems to be that some quantity is vaguely wrong. I do expect a higher standard of clarity in a scientific paper,2. The paper confounds the problem of learning the value function, specifying the initial estimates of the value function and exploration. The analysis of exploration is entirely informal and suffers from the lack of clear problem formulation as per (1). Of course, one can influence exploration by initialising the value function in various ways, and this may respond differently to different truncations (different values of T), but I don't see how it is related to the "bias" problem that the paper is trying to address. In any case, I wish the authors either provided a formal handle on exploration or shift the focus of the paper and remove it,3. I don't see what hypothesis the experiments are trying to test. Clearly, if I train my agent on a different MDP and test it on a different one, I get a mismatch. The lack of clear definitions as per (1) comes back with a vengeance.4. Section 1.1 seems to exist purely to create a spurious impression of formality, which bears little relevance for what the paper is actually about. RL, as traditionally formulated, uses discrete time-steps so the Brownian motion model developed in this section doesn't seem very applicable - it is true that it is a limiting case of a family of discrete chains, but I don't see how this produces any insights about RL - chains are easy to simulate, so why not test on a chain directly? In any case, the result shown in Section 1.1 is entirely standard, can be found in any textbook on stochastic processes and was likely introduced purely to cover for the lack on any substantial theory that comes form the authors.To summarize: if the authors address these points, there is a possibility that the ideas presented in this draft may somehow lead to a paper at some later point. However, I feel that the required changes would be pretty massive and don't see how the authors could make it during the ICLR revision phase - the problems aren't details or technicalities but touch the very substance of what the paper is trying to do. Basically, the whole paper would need to be re-written.another minor point:  sloppy capitalization in section 2.1 Summary:The paper attempts to link the known mismatch between infinite horizon MDP values and finite trajectory sums to the problem of exploration.  Trajectories in environments requiring exploration (mountain car and a number-line walk) are shown and the effects of changing trajectory lengths and initial values are discussed.  Potential solutions to the problem are proposed though the authors did not deem any of the solutions satisfactory.Review:The paper brings up a number of important issues in empirical reinforcement learning and exploration, but fails to tackle them in a manner that convincingly isolates the problem nor proposes a solution that seems to adequately address the issue.  Specifically, several issues seem to be studied at once here (including finite-horizon MDPs, function approximation, and exploration), relevant work from the exploration and RL community is not cited, the early experiments do not reach a formal theoretical claim, and the proposed solutions do not appear to adequately address the problem).  These issues are detailed below.First, the paper is considering many different issues and biases at once, including those introduced by initialization of the value function, exploration policies, function approximation, and finite/infinite length trajectories.  While the authors claim in several places that they show one bias is more important than another, no definitive experiment or theorem is given showing that finite-length trajectories are the cause of bad behavior.  While it is well known that infinite-horizon MDPs do not exactly match value functions for finite horizon MDPs, so many other factors are included in the current work (for instance the use of neural networks) that it remains unclear that the finite/infinite mismatch is an issue.The paper also fails to cite much of the relevant work on these topics.  For instance, the use of infinite-horizon MDPs to study finite learning trajectories is often done under the guise of epsilon-optimal guarantees, with  epsilon derived from the discount factor (see Near-Optimal Reinforcement Learning in Polynomial Time).  In addition, the effects shown in mountain car when changing the values or the initialization function, mirror experiments with Q-learning that have shown that there is no one initialization scheme that guarantees optimal exploration (see Strehls thesis Probably Approximate Correct Exploration in Reinforcement Learning ).  Overall, the paper seems to confuse the problems of value initialization and trajectory length and does not show that they are particularly related.In addition, the early sections covering theoretical models such as Wiener Processes and Random Walks lay out many equations but do not come to a specific formally proven point.  No theorem or proof is given that compactly describes which exact problem the authors have uncovered.  Therefore, when the solutions are presented, it remains unclear if any of them actually solve the problem.Finally, several of the references are only ArXiv pre-prints.  Papers submitted to ICLR or other conferences and journals should only cite papers that have been peer-reviewed unless absolutely necessary (e.g. companion papers). This paper proposes the use of Bayesian inference techniques to mitigate the issues of miscalibration of modern Deep and Conv Nets. The presentation form of the paper is unsatisfactory. The paper seems to imply that Bayesian Deep Nets are used to calibrate Deep/Conv Nets, so I was expecting something like post-calibration using Bayesian Deep Nets. After reading through the paper a few times, it seems that the Authors are proposing the use of Bayesian inference techniques to infer parameters of Deep/Conv Nets in order to improve their calibration compared to non-Bayesian counterparts. This is the only contribution of the paper, and I believe it is insufficient. Guo et al., (2017) already points out that regularization of modern Deep/Conv nets improves calibration, so the fact that Bayesian Deep/Conv Nets are calibrated is not surprising, giving that the prior over the parameters act as a regularizer. It is surprising to see ECE values above one - unless these have been scaled by a factor of 100 - but this is not mentioned anywhere.Previous work shows that Monte Carlo Dropout for Conv Nets offers well calibrated predictions (https://arxiv.org/abs/1805.10522), so I think a comparison against this inference method should be included in the paper. The paper makes a number of imprecise claims/statements. A few examples:- "Bayesian statistics make use of the predictive distribution to infer a random variable by computing the expected value of all the possible likelihood distributions. This is done under the posterior distribution of the likelihood parameters" - very unclear and imprecise explanation of Bayesian inference- "When using neural networks to model the likelihood" - the likelihood is a function of the labels given model parameters -- Paper summary --The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration.-- General Commentary --The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isnt post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time?I have plenty of concerns with the submission itself, listed below:- First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didnt properly check the paper before submission.- While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand whats being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least.- The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (uncalibrated, temp-scal' and BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot.- As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks (Bradshaw et al, 2017), and Calibrating Deep Convolutional Gaussian Processes (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models.(- With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion.(- I cant disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the alternative hypothesis being mentioned, and the investigation is entirely limited to the offline setting, so Im not entirely sure what distinction the authors are trying to make here.(- In the same section, the authors then remark that Our results open new perspectives to improve the variational approximation& and we believe our results might foster further research in&, before proceeding to list a dozen or so papers which might be inspired by this work. However, I cant really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers.(- The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the papers lack of identity and focus.- There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient.(- There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by conf in Equation 4.(- Referring to datasets as databases in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by uses BNNs? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission.- In their discussion of the results, the authors state that We cannot conclude that BNNs are calibrating at the cost of losing accuracy, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score.-- Recommendation --Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conferences standards.Pros/Cons summary:+  The proposal yields good results in the provided experiments-   Minor contributions that are not convincing enough-   Muddled presentation of ideas-   Dubious or weakly motivated design choices-   Poorly written with plenty of typos-   Difficult to follow Summary:The paper studies the effects of preconditioning on generalization properties in deep learning. By using a bias-variance decomposition of the expected risk, the paper determines optimal precondition matrix $P$ for bias and variance. Then the paper analyzes the generalization performance via the aspects: clean labels, well-specified model and aligned signal. Finally, it extends the analysis to the reproducing kernel Hilbert.Pros:1. The theoretical results provide guidelines of choosing precondition matrix for practical problems.Cons:1. The paper contains a number of unclear / undefined terms such as well-specified and aligned signal, that make it difficult to read.2. The paper uses a lot of vague and unverified claims / statements which are usually the explanations after each theorem / proposition. For example, after theorem 1, it says that "Theorem 1 implies that preconditioning with the inverse population Fisher results in the optimal stationary variance... In other words, when the labels are noisy so that the risk is dominated by the variance term... We emphasize that this advantage is only present when the population Fisher is used, but not its sample-based counterpart". For me, it would be more clear if these statements could be explained in detail.3. The paper is not well-organized. For me, it is a collection of results that are unconnected. For example, after reading the analyses of bias and variance, I have no idea how they support the study of generalization or why section "3.3 misspecification" is placed along with bias and variance analyses, etc. I am not saying these results are irrelevant, however, there should be a better way of arranging / writing them so that they can support well the ideas of the paper. The authors propose the Quantile Regression GAN (QRGAN) to minimize the 1-Wasserstein distance between the real and generated data distributions. The proposed method avoids the mode collapse problem and obtains an improvement in the FID score compared to some existing GANs.-  Pros:  - Compared with NSGAN and LSGAN, the proposed method avoids mode collapse and achieves better performance than them in the FID score.  - In addition, compared to WGAN-GP, it achieves better FID with less training iterations while maintaining comparable performance.- Cons:  - Overall, there are so many typos and grammatical errors in this paper that they make it difficult to understand the content of the paper. The authors must look for these errors and correct them.  - Also, the way of citing figures and tables is inappropriate. For example, Fig. 1 is not cited in the main text, and figures and tables in appendixes such as Fig. 7 and Table 2 are cited without specifying that they are in appendixes. Reviewers don't need to read the appendixes, so the content should be complete in the main text.  - The authors state that the relationship between quantile regression and 1-Wasserstein distance is shown in section 2.1, but this is not explicitly shown. In particular, the authors state in section 2.1 that "Here, minimizing 1-Wasserstein distance is same to minimizing distance between quantile values", but Eq.1 and Eq.2 are simply p-Wasserstein distance and 1-Wasserstein distance, so it is unclear which equation represents the relationship. Also, the period in Eq. 3 should be a dot (multiplication).  - In Eq. (4), you state that a and b are set to + and - respectively, but how were these infinities implemented in practice?  - Why is there no WGAN-GP result in Figure 2? My understanding is that QRGAN minimizes 1-Wasserstein like WGAN-GP, so the result is almost the same. And why didn't the authors include unrolled GAN and VEEGAN results for comparison, even though they performed the same experiments as these papers?  - If the authors claim that WGAN-GP is computationally expensive, they should show how much less expensive it is in QRGAN. QRGAN also requires the sum of multiple quantile values, so the more of them, the longer it should take to compute them. Also, as far as I read, there is no indication in the paper of how the number of quantile values was set up in the experiment.  - Looking at Figure 4 and Figure 5, GRGAN appears to be less stable than WGAN-GP. Why is this?  - In section 3.2, the authors should show the image actually generated by GANs.- Minor comments:  - It is difficult to read because the author's citation is not enclosed in parentheses.  - Some parts of the random variables are in bold type and others are not. These notations should be consistent. The paper proposes a novel approach for solving MORL problems while considering uncertainty in the Pareto frontier.The contributions are interesting and novel, but the paper has several flaws which make it not ready for acceptance, especially regarding experiments.First, the authors overlook many Bayesian MORL algorithms which also consider the Pareto frontier uncertainty. For instance, Calandra et al, "Pareto Front Modeling for Sensitivity Analysis in Multi-Objective Bayesian Optimization"Calandra et al, "Bayesian Multiobjective Optimisation With Mixed Analytical and Black-Box Functions: Application to Tissue Engineering")Hernandez-Lobato et al, "Predictive entropy search for multi-objective Bayesian optimization"Olofsson et al, "Bayesian multi-objective optimisation of neotissue growth in a perfusion bioreactor set-up"In the introduction the authors say "In addition, most approaches still only work in domains with low-dimensional and discrete action spaces." This is not true. Simply, all cited algorithms have just been tested on low-dimensional problems. Since they were not evaluated on larger problems, we do not know how they behave. The authors do not even include any of them in the evaluation to show that they actually fail.Furthermore, the authors claim that their experiments have large action spaces. How big are they? This is not mentioned in the paper. For instance, the Mujoco environments tested in the paper do not have that large action spaces (eg, Swimmer has 15 actions).The writing can be also improved. The sections feel a bit disconnected, and sometimes it is not easy to highlight the contributions. For instance, Section 4.3.1 takes quite some space and seem to be part of the novelty contributions, but the losses are taken from Yang et al.My biggest issue is with the experiments. First, the authors should say right away on which environment they are testing the algorithms, and not just "Mujoco" and "two provided by Xu et al". Furthermore, these two environments are never actually tested, since the experiments are only on SUMO, Swimmer, Walker, and HalfCheetah.And why are results for the Swimmer shown as figure, while Walker and Cheetah have tables? And why these environments out of all Mujoco ones? (these three are known to be the easiest).Moreover, Mujoco environments are single-objective. How did you turn them into multi-objective? This is crucial and not mentioned.Finally, there is no evaluation against any of the MORL algorithms mentioned in related work. The paper makes the point that these should fail with large action spaces and uncertainty, but this is not shown.Overall, the experiments section feels rushed and incomplete, and the paper is not ready for publication. The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the "Discrete Autoencoders for Sequence Models" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:* comparing with a stronger NMT architecture and larger data-set* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.* comparison to fully unsupervised latents and some other system, e.g., the system from [1] aboveIn the absence of these comparisons and with little novelty, the paper is a clear reject. The paper introduces a GaLU activation function, which is the product of a random gate function and a learnable linear function. The authors argue that empirically, neural networks with the GaLU activation is as effective as that with the ReLU activation, but theoretically, the GaLU activation is easier to understand because of the separation of the non-linearity and the learnable parameters. The the paper analyzes neural networks with one GaLU layer. Essentially, the network is a random transformation followed by a linear projection. This property enables analysis that are well known for the linear models.Although the definition of GaLU is new, the idea of combining a non-linear projection with a linear transformation is an old one. [1] shows that many kernel SVM models can be written in this form. [2] [3] show that neural networks with various of activation functions can be relaxed to this form. However, these methods have never achieved performance that is as good as the state-of-the-art CNN models in challenging datasets (ImageNet or even CIFAR-10).In section 3, the accuracies on MNIST (98%) and MNIST-fashion (88%) are quite low. They are not even as good as a classical kernel SVM, though the non-linear projection version of the kernel SVM has been well-studied in theory.In section 4, the analyses are mostly standard for linear models and convex optimization. To the best of our knowledge, it doesn't introduce new insight on the understanding of non-convex optimization.Overall, I think the paper and its theoretical analysis is built on an unsolid claim that the GaLU activation is a good replacement for traditional non-linear activation functions. The empirical study doesn't seem to support this claim. I cannot recommend accepting the paper.[1] Random Features for Large-Scale Kernel Machines [2] Learning Kernel-Based Halfspaces with the Zero-One Loss[3] Convexified Convolutional Neural Networks The paper proposes an alternative to commonly used ReLU activated networks. The "gating" and "amount" effects of the weights are decoupled. The authors claim that such architectures are easier to theoretically understand. That might be the case indeed, but I fail to see much value in obtaining such understanding of very contrived objects that are not being used in practice. Unless such architectures can be proven to be interesting from a practical standpoint I do not think there is much of a point in studying them. The argument provided by the authors that they can - in a simple situation - have as much expressive power as a standard ReLU activated architecture is insucfficient, in my opinion, to justify researching them. Also, if a strong, deep theorem was proven using GaLU networks was proven, I would be inclined to recommend the paper to be accepted. As is - I do not find the paper to be a contribution significant enough for ICLR. Summary: this paper discussed an incremental improvement over the Random Walk based model for sentence embedding.Conclusion: this paper is not ready for publication, very poor written and well below the bar of ICLR-caliber papers. More:This paper spent the majority of its content explaining background (those paragraphs were very poor written and difficult to read), and very briefly introduced their methodology with some mathematical derivations and equations, most of which can be put in the supplement instead of main context. The author didn't quite explain how the proposed method, such as why using non-extensive statistic in this context,The experiment results aren't convincing and lack sufficient information for reproducibility. PAPER SUMMARY:This paper introduces a non-extensive statistic random walk model to generate sentence embedding while accounting for high non-linearity in the semantic space.NOVELTY &amp; SIGNIFICANCE:I am not sure what the main focus of this paper is. It seems accounting for non-linearity in the semantic space while generating sentence embedding has already been achieved by existing LSTM models -- the goal seems to be more about interpretability and computational efficiency but the paper did not really discuss these in detail (more on this later).In terms of the proposed solution, I am also not sure what is the significance of using non-extensive statistic in this context. In fact, the background section gave the impression that the non-linear form of q-exponential is the main reason to advocate this approach. But, if it is only about handling non-linearity, there are plenty of alternatives and it is important to point out exactly what advantages non-extensive statistic has over the existing literature (e.g., why is it more interpretable than LSTM). Please expand the respective background section to clarify this. TECHNICAL SOUNDNESS:There are parts of the technical exposition that appear confusing and somewhat incoherent. For instance, what exactly is this confounding effect of vector length &amp; why do we need to address this issue if according to the Section 2.2, it has already been addressed in the same context?Section 2.2 seems to discuss this effect but the exposition is unclear to me. The authors start with an example and a bunch of assumptions that lead to a contradiction. It is then concluded that the cause of this is due to the linearity assumption (what about the other assumptions?) in estimating the discourse vector. I do not really follow this reasoning and it would be good if the authors can elaborate more on this.CLARITY:The paper seems to focus too much on technical details and does not give enough discussion on its positioning. The significance of the proposed solution with respect to the literature remains unclear. EMPIRICAL RESULTS:I am not an expert in this field and cannot really judge the significance of the reported results. I do, however, have a few questions: in all benchmarks, are the algorithms tested on a different domain than the domain it was trained on? Have the authors compared the proposed sentence embedding framework with the LSTM literature mentioned in the introduction? I noticed there was a LSTM AVG in the comparison table.Is that the simple averaging scheme mentioned in the introduction when the authors discussed transferrable sentence embedding?Is there any reason for not comparing with RNN (Cho et al., 2014)? In terms of the computation processing cost, how efficient is the proposed method (as compared to existing literature)? This paper while presenting interesting ideas, is very poorly written. It seems as though the authors were in a rush to submit a manuscript and did not even bother with basic typesetting.Firstly, the paper spends too much time motivating and re-introducing the model of Arora et.al. Note to the authors here, they cite the same paper from Arora et.al for 2017 twice. The first time the model they refer to was introduced by the paper "RAND-WALK: A latent variable model approach to word embeddings", this is probably what the authors mean by the 2016 reference?Now coming to the experiments, the results are presented in a table that is poorly formatted. The section partitions are not clearly delimited, making for a hard read. Even if we overcome that and look at the results, the presented numbers are incredibly confusing. On the STS 13 and 15 data sets, Ethayarajh 2018's numbers are much better at 66.1 and 79.0. Coming to STS14 Ethayarajh attain 78.4 while the proposed method achieves 78.1. If we discount this for the moment, and look at the results on STS12 where the proposed method achieves 71.4, this is the only data set where the proposed method does better than the other baselines.So almost on 3 of the 4 datasets Ethayarajh 2018 does better. This makes me question what exactly is the proposed model improving?Coupled with the fact that there is no motivation to explain results or future work, this makes for a very poorly written paper that is very challenging to read.It is very likely that there is some merit to the proposed methods that introduce non linearity, but these points simply get lost in the mediocre presentation. This paper proposed the concept of state-action permissibility (SAP). Given a user-defined type 1 SAP function, the algorithm learns a classifier to predict whether an action at a given state is permissible or not. Based on this prediction, the reinforcement learning (RL) algorithms can limit the exploration only to the permissible actions, and thus greatly reduce the cost of learning. The proposed algorithms are tested on two simple tasks, both of which have the same flavor of following a predefined track.Although the results of the experiments show that SAP helps to speed up RL, I think that the application of SAC is very narrow. It is extremely difficult to define an AP1 function in general. For example, for most of the OpenAI gym environments (such as half-cheetah, ants or humanoid), it is not clear to me how to manually define an AP1 function. It would be more convincing if the paper can apply the proposed techniques to some of the benchmark OpenAI gym environments.Even for the lane following task described in the paper, the AP1 function in eq. 5 is limited and eliminates many good solutions. It constrains that the action should not lead to more deviations to the center line in the next time step. This greedy constraint will not work in more interesting driving scenarios. For example in a sharp turn, if the curvature of the lane is too large for the car to follow, a common strategy (that can be learned by vanilla RL algorithms) is to first drive to the outer side of the lane before the turn, cut to the inner side at the turn and exit the turn to the outer side. This optimal solution to negotiate a tight turn is completely eliminated by the user-defined AP1 function (eq. 5).The idea of AP1 is somewhat contradictory to the philosophy of reinforcement learning. AP1 is a greedy decision based on the next step while RL optimizes for the accumulated reward over many steps. RL allows taking an action that will sacrifice the immediate reward (e.g. deviate from the center line of a lane) in the next step but can accumulated more reward in the long run (successfully drive along a tight turn). In most of cases, by looking at the next state, it is just not possible to predict whether a specific action cannot lead to the optimal long-term reward (SAP).For the above reasons, I think that the application of SAP would be very narrow, especially for reinforcement learning. I would not recommend accepting this paper at this time. In this work authors benchmark a biologically plausible network architecture for image classification. The employed architecture consists of one hidden layer, where input to hidden layer weights W1 are either trained with PCA or sparse coding, or are kept fixed after random initialization. The output layer units are modeled as leaky integrate-and-fire (LIF) neurons and hidden to output connections W2 are tuned using a rate model that mimics STDP learning dynamics in the LIF neurons. The authors compare classification results on MNIST and CIFAR10 datasets, where they also include results of an equivalent feed-forward network that is trained with standard error backpropagation.The authors find that in the bio-plausible network with a large hidden layer, unsupervised training of input to hidden layer weights does not lead to as good of a classification performance as achieved through fixed random projections. They furthermore find that localized patch-style connectivity from input to hidden layer further improves the classification performance.Overall the paper is well-written and easy to follow, but I fail to see any significant contribution in this work. As compared to the findings of Hubel &amp; Wiesel, how bio-plausible are random projections for low-level feature representation? One may also argue that unsupervised tuning of W1 may require a lot more training data than available in MNIST or CIFAR10. The authors also need to take the capacity of their network into account; they draw conclusions based on a biologically-plausible network, but one that only has two feed-forward layers. It is hard to imagine that a more accurate biologically-plausible vision model would prefer random projections over low-level feature extractors that are well-tuned to the input statistics.Regarding the observation that localized fields perform better than densely connected layers, I find it simply in line with physiological findings (starting from the work of Hubel &amp; Wiesel) and artificial neural network architectures they inspired like CNNs. This article compares different methods to train a two-layer spiking neural network (SNN) in a bio-plausible way on the MNIST dataset, showing that fixed localized random connections that form the hidden layer, in combination with a supervised local learning rule on the output layer can achieve close to state-of-the-art accuracy compared to other SNN architectures. The authors investigate three methods to train the first layer in an unsupervised way: principal component analysis (PCA) on the rates, sparse coding of activations, and fixed random local receptive fields.  Each of the methods is evaluated on the one hand in a time-stepped simulator, using LIF neurons and on the other hand using a rate-approximated model which allows for faster simulations. Results are compared between each other and as reference with  standard backpropagation and feedback alignment.  The main finding is that localized random projections outperform other unsupervised ways of computing first layer features, and with many hidden neurons approaches backpropagation results. These results are summarized in Table 8, which compares results of the paper and other state-of-the-art and bio-plausible SNNs. PCA and sparse coding work worse on MNIST than local random projections, regardless if the network is rate-based, spike-based or a regular ANN trained with the delta rule. Feedback Alignment, although only meant for comparison, performs best of the algorithms investigated in this paper.In general the question how to train multi-layer spiking neural networks in a bio-plausible way is very relevant for computational neuroscience, and has attracted some attention from the machine learning community in recent years (e.g. Bengio et al. 2015, Scellier &amp; Bengio 2016, Sacramento et al. 2018). It is therefore a suitable topic for ICLR. Of course the good performance of single-layer random projections is not surprising, because it is essentially the idea of the Extreme Learning Machine, and this concept has been well studied also for neuromorphic approaches (e.g. Yao &amp; Basu, 2017), and versions with local receptive fields exist as well (Huang et al. 2015 "Local Receptive Fields Based Extreme Learning Machine"). While the comparison of different unsupervised methods on MNIST is somehow interesting, it fails to show any deeper insights because MNIST is a particularly simple task, and already the CIFAR 10 results are far away from the state-of-the-art (which is &gt;96% using CNNs). Another interesting comparison that is missing is with clustering weights, which has shown good performance for CNNs e.g. in (Coates &amp; Ng, 2012) or (Dundar et al. 2015), and is also unsupervised.The motivation is not 100% clear because the first experiment uses spikes, and shows a non-negligible difference to rate models (the authors claim it's almost the same, but for MNIST differences of 0.5% are significant). All later results are purely about rate models. The authors apparently did not explore e.g. conversion techniques as in (Diehl et al. 2015) to make the spiking results match the rate versions better e.g. by weight normalization.I would rate the significance to the SNN community as average, and to the entire ICLR community as low. The significance would be higher if it was shown that this method scales to deeper networks or at least can be utilized in deeper architectures. Scrutinizing the possibilitites with random projections on the other hand could lead to more interesting results. But the best results here are obtained with 5000 neurons with 10x10 receptive fields on images of size 28x28, thus the representation is more than overcomplete, and of higher complexity than a convolution layer with 3x3 kernels and many input maps.Because the results provide only limited insights beyond MNIST I can therefore not support acceptance at ICLR.Pros:+ interesting comparison of unsupervised feature learning techniques+ interesting topic of bio-plausible deep learningCons:- only MNIST, no indications if method will scale- results are not better than state-of-the-artMinor comments:The paper is generally well-written and structured, although some of the design choices could have been explained in more detail. Generally, it is not discussed if random connections have any advantage over other spiking models in terms of accuracy, efficiency or speed, besides the obvious fact that one does not have to train this layer. The title is a bit confusing. While it's not wrong, I had to read it multiple times to understand what was meant.The first sentence in the caption for Fig. 2 is also confusing, mixing the descriptions of panel A and B. Also, in A membrane potentials are shown, but the post-membrane potential seems to integrate a constant current instead of individual spikes. Is this already the rate approximation of Eq. 2? Or is it because of the statement in the caption that they both receive very high external inputs. In general, the figures in panel A and B do not make the dynamics of the network or the supervised STDP much clearer. Principal Component Analysis and Sparse Coding are done algorithmically instead of using a sort of nonlinear Hebbian Learning as in Lillicrap 2016. It would have been interesting to see if this changes the comparatively bad results for PCA and SC.In Fig. 3, the curve in the random projections case is not saturated, maybe it would have been interesting to go above n_h = 5000. As there are 784 input neurons, a convolutional neural network with 7 filter banks already would have around 5000 neurons, but in this case each filter would be convolved over the whole image, while with random projections the filter only exists locally. In Eq. 1, the notation is a bit ambigous: The first delta-function seems to be the Dirac-delta for continuous t, while the second delta is a Kronecker-delta with discrete t.In A.1 and A.4.2 it is stated that the output of a layer is u_{t+1} = W u_t + b but I think in both cases it should be W a_t + b where a_t = phi(u_t). Otherwise, you just have a linear model and no activations. In Table 3, a typo: "eq. equation" The paper describes an interesting idea for using Vashwani's transformer with tree-structured data, where nodes' positions in the tree are encoded using unique affine transformations. They test the idea in several program translation tasks, and find small-to-medium improvements in performance. Overall the idea is promising, but the work isn't ready for publication. The implementation details weren't easy to follow, the experiments were narrow, and there are key citations missing. I would recommend trying some more diverse tasks, and putting this approach against other graph neural network techniques. In this paper, the authors propose a new technique called Teaching to Teach via Structured Dark Knowledge for curriculum learning.  See my comments below.I dont like this paper because it is full of buzzwords, and it is really poorly written. The author first started in the abstract on hyper deep learners, which confuses me a lot. It is unclear to me what type of deep learning models that you are aiming for and what exactly hyper deep learners are. Also, in the abstract, the authors mention Structured Dark Knowledge, but they have not discussed it in the introduction, which makes it extremely hard for the readers to understanding the relationship between this work and the so called Structured Dark Knowledge. The 3.2 is also poorly written and insufficiently motivated. The term Structured Dark Knowledge sounds fancy, but I fail to see what the structures are. It sounds like the authors just propose Training Subset as Structured Dark Knowledge, which is extremely misleading. This work gives people an impression that you are trying to leverage external knowledge for curriculum learning, it turns out it is not the case.In section 4.1, the hyperparameters setting seems to be mysterious. It is unclear to me how the authors come up with magic numbers like 0.04, 0.1, 15%. In Figure 2, the improvements from SDK do not look like it is very impressive. And also 4.2 the datasets are too small. The metric in Table 4 is poorly chosen. I dont understand what the numbers mean un Table 4 and how significant they are. Please use standard metrics. The future stock price regression experiments are also poorly presented. In Table 5, the authors do not explain what the price, percentage and error mean. It only says Results. Please pick common benchmark datasets and well-known metrics. Overall, this paper is poorly written, and it has not met the requirement of ICLR. The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.2) Properties of the minimizerThe authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.3) Scaling propertyI find this section confusing. Specifically,a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?c) The idea of restarting is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I dont see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this property seem to be used to simply rescale the a and w parameters.d) The authors claim that the scaling law (Proposition 3.2) should play a significant role to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?4) Convergence rateIt seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldnt a constant step size also yield convergence in that case?b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.5) Saddles for neural netsThe authors claim they have not encountered convergence to saddles for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?6) Extension of the analysis to deep neural networksThe analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?7) ExperimentsHow would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network? The paper looks at the problem of generalization across physical parameter varaition in learning for continuous control. The paper presents a method to develop a sampling based curriculum over env. settings for training robust agents. * The paper makes an interesting observation on inadvertent generalization in robust policy learning. However, the examples in both the cartpole and the pendulum cases seem not to be watertight. For instance, the authors claim that But from a dynamical system perspective in both cases, the controller is operating near limits. The solution and subsequent generalization depend more on the topology of the solution space. A heavy Pendulum is an overdamped system and required the policy to operate at the limits of action to generate momentum for swing up. Hence a solution for a lighter pendulum in implicitly included. Similarly, the rolling ball is an underdamped system, and where the policy operates near zero limits in light ball case to prevent the system from going unstable. Adding mass results in damping which makes it easier. In this case, as well the solution space is implicitly contained.But this is not a novel observation. Similar observations have been made for Robust control and Model-Reference Adaptive Control. The paper also overlooks a number of related works in model-free randomization [4], adaptive randomization [3], adversarial randomization [5,6]. The method also does not compare with model-based methods for adaptive policy learning and iLQR based methods to handle this problem [2, 7].The argument that the method is model-free is perhaps not as acceptable since the model parameters need to be known apriori for adaptation. The policy itself may be model-free but that is a design choice. A good experimental evaluation for this is generalization across known unknowns and unknown unknowns. * The algorithm itself is reasonable but the problem setup and choice of a discrete dynamics parameter choices are questionable. The bandit style method operates over a discrete decision set. It also assumes in the multi-parameter setting that they are independent, which may not be true very often. The algorithm proposed itself isnt novel, but would have been justified if the results supported the use of such a method. * Experiments are quite weak. Both the experimental domains are rather simplistic with smooth nonlinear dynamics. There are more sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2].  It would be useful to see how tis method works in more complicated domains and how the performance compares with simpler methods such as joint brute-force randomization both in performance and in computation.  Questions: 1. Please provide details of Algorithm 1. How are the quantities K and M related? 2. What is the process of task initialization? What information is required and what priors are used. Uniform prior over what range?In summary, the authors explore an interesting adaptive curriculum design method. However, in its current form, the work needs more thought and empirical evaluation for the sake of completeness. References:1. Model Reference Adaptive Control [https://doi.org/10.1007/978-1-4471-5102-9_116-1]2. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems [https://arxiv.org/abs/1707.04674]3. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles [https://arxiv.org/abs/1610.01283]4. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World[https://arxiv.org/abs/1610.01283]5. Certifying Some Distributional Robustness with Principled Adversarial Training [https://arxiv.org/pdf/1710.10571.pdf]6. Adversarially Robust Policy Learning: Active Construction of Physically-Plausible Perturbations [http://vision.stanford.edu/pdf/mandlekar2017iros.pdf]7. Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization [https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf] This paper is clearly written and identifies an important point that exploration is dangerous in the autonomous driving domain. My key objection to this paper is that, even though the method is intended to deal with problems where exploration is dangerous and therefore should not be done, the method relies on negative examples, which are presumably dangerous. If simulations are used to generate negative examples and those are used, then the benefit of the presented method over standard reinforcement learning goes away.I have several questions/comments/suggestions about the paper:1. Can one perhaps present only mildly bad examples (e.g., mild swerving) to reinforcement learning in a way where the algorithm can understand that significant swerving, like what is shown in figure 2, is even worse?2. The backseat driver feedback described seems to granular. I think that, to be realistic, the algorithm should allow for feedback that is less precise (e.g., turn further, turn the other way), without requiring information on proportions.3. Please add an architecture diagram.4. In figure 4, what is the difference between the first and fourth items? They have exactly the same description in the legend.5. The experiments are not convincing. They lead one to conclude that negative examples are beneficial, which is good, but not surprising. Because negative examples are generated, a comparison with regular reinforcement learning should be done. The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring. The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation. In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives. The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset. In sum, the paper has a very good application but not good enough as a research paper. Some of the problems are listed as follows:1.Lack of technical novelty.  It seems to me just a combination of several mature techniques. I do not see much insight into the problem. For example, if the rule-based concept extractor can already extract concepts very well, the problem retrieval should be solved by searching with the concepts as queries. Why should we use embedding to compare the similarity? Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap. 2.Data size is too small, and the baselines are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models. Some clarity issues. For example, Page 6. is pre-trained on a pure set of negative samples what is the objective function? How to train on only negative samples? [Summary]PU learning is the problem of learning a binary classifier given labelled data from the positive class and unlabelled data from both the classes. The authors propose a new  GAN architecture in this paper called the Divergent Gan (DGAN) which they claim has the benefits of two previous GAN architectures proposed for PU learning: The GenPU method and the Positive-Gan architecture. The key-equation of the paper is (5) which essentially adds an additional loss term to the GAN objective to encourage the generator to generate samples from the negative class and not from the positive class. The proposed method is validated through experiments on CIFAR and MNIST.[Pros]1. The problem of PU learning is interesting.2. The experimental results on CIFAR/MNIST suggest that some method that the authors coded worked at par with existing methods.[Cons]1. The quality of the writeup is quite bad and a large number of critical sentences are unclear. E.g.a. [From Abstract] It keeps the light adversarial architecture of the PGAN method, with **a better robustness counter the varying images complexity**, while simultaneously allowing the same functionalities as the GenPU method, like the generation of relevant counter-examples.b. Equation (3) and (4) which are unclear in defining R_{PN}(D, ´)c. Equation (6) which says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense.d. The distinction between the true data distribution and the distribution hallucinated by the the generator is not maintained in the paper. In key places the authors mix one with the other such as the statement that supp(Pp (Xp )) ) supp(Pn (Xn ))  In short even after a careful reading it is not clear exactly what is the method that the authors are proposing.2. Section 2.2 on noisy-label learning is only tangentially related to the paper and seems more like  a space filler.3. The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs worse than other PU learning methods which does not make sense. Because of this I am not quite convinced by the experiments. This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues.*****The clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not.Moreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me.*****The paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in "learning classifiers from only positive and unlabeled data", KDD 2008; the latter problem setting was proposed in "presence-only data and the EM algorithm", Biometrics 2009 and formalized in "analysis of learning from positive and unlabeled data", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method.The huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See "learning from corrupted binary labels via class-probability estimation", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled "on the minimal supervision for training any binary classifier from only unlabeled data" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting.Furthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see "learning classifiers from only positive and unlabeled data" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in "presence-only data and the EM algorithm" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)&lt;=P(X) following "estimating the class prior and posterior from noisy positives and unlabeled data", NIPS 2016. BTW, "mixture proportion estimation via kernel embedding of distributions" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later.In summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue.*****The novelty is to be honest incremental and thus below the bar of ICLR. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN. Authors present a study to compare global translation invariance capabilities of CNNs and CapsuleNets.The paper doesn't introduce any novel concept or technique but it simply compares two established techniques on MNIST dataset. The interest on this paper is rather limited. The authors of this paper compare the robustness of CNN and CapsNet to global translation on the MNIST dataset. Both models were trained on the standard training set of MNIST, and then tested on a set with digits shifting from the upper left corner to the lower right corner. The results of both models were poor. To improve it, the authors add some shifted digits to the training set, and the performances of both models were significantly enhanced. Moreover, the performance of CNN was better than that of CapsNet in the experiments. Generally speaking, the work presented in this paper is clear and straightforward. However, the work is not significant enough to publish as a ICLR paper. Below is my major comments.1. There are lots of typos and grammatical errors everywhere in the paper. Thus, the manuscript was not well prepared.2. It is unclear which CapsNet and what settings were used in the experiment.  3. It is well-known that convolutional networks are good at capturing local patterns from the images, while capsule networks enhance it to consider global configurations of the local patterns, and robust to affine transformation. Obviously, the experiments presented in this manuscript is too simple. Lots of work should be done in the investigation. For example,o on the training set and shifted test set, the authors can enlarge the background and keep the digits in the original size to make it as a local pattern in the image. Will it be detected by CNNs with larger receptive fields for the images? How is it compared with CapsNets? 4. How are both models compared on other (perhaps more complicated and larger) datasets?In summary, the work presented here is interesting, but lots of work should be done in order to make it publishable. The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).There is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models? PRO's:+good problem: generating polyphonic music with long-term structure+reasonable approach: modification of SampleRNN: makes senseCON's:-doesn't work. My fundamental critique of this paper is that, while the authors claim that their system " generates polyphonic music which maintains long-term dependencies", in fact what it generates it is not really polyphonic, nor-- more importantly-- does it demonstrate the kind of long-term structure present in the training set.1) Polyphony: The model predicts a combination of monophonic melody plus chords (i.e. chord names such as "A+", "C7" etc). This is different from polyphony, in which the model would predict the actual voicing used for those chords. However, this could be seen as an error in terminology; if the authors claimed that they were predicting a monophonic musical voice plus chords, and they did that well, that would be absolutely fine. Generating a coherent melodic line that continues, along with the chords underneath it, would be a great achievement. However, that is not what happens here. For examples, in the provided examples, e.g. Measure 19 of Fig 4, Measures 1,3,4,5, ... of Fig 6, contain stylistically unusual combinations of chords and melodic lines. By "stylistically unusual", I simply mean that those examples are not consistent with the Nottingham dataset.  Furthermore, in my subjective opinion, the examples that I listed above also just don't really musically work. There is no question that in the right context, any of those particular combinations of chords and melody notes *could* be made to work: for example, the first measure of Fig 6 would be perfectly fine as the beginning of a different song. (E.g. it could be taken as a slight reharmonization of the opening of "lullaby of birdland", but that would require a coherent continuation. )2) Long-term structure: It seems to me that one of the key things that this paper sets out to do is to get strong long-term dependencies. The motivation for the SampleRNN-inspired approach is to have generation at multiple time scales, for example. However, there is no evidence in the presented examples of long-term structure. Consider Fig 6, for example. Where is the long-term structure? A D major chord is frequently repeated with occasional A7. That is reasonable but it does not necessarily demonstrate long-term structure, anymore than learning that "q" is often followed by "u" demonstrates long-term structure. There is no melodic motif, there is no sense of 4-bar phrasing (or any other recurring such pattern that I can tell). In fact, all of the samples shown (Fig 4, 6, 7, 8) all end up with the chord D major played most of the time, after what appears to be a bit more variation in the first few chords.The results of the listening test are strange to me (beyond some of the apples-to-oranges comparisons). I cannot comment on those without hearing the pairs of examples that were actually played. How were those pairs selected?At the moment, it does not seem worthwhile for this review to get into details about exactly how the system works, in light of the problematic output. If there is reason for me to do so, I would gladly oblige. The authors do make a variety of choices that appear to be fairly sensible. I would very much look forward to seeing a revised version of the system in future that produces the  kind of output that the system is intended to produce (and described as producing). The authors propose a hierarchical model of symbolic music that takes explicit advantage of measures and chords to construct the hierarchy. Their model is very similar to SampleRNN (2-level RNN Autoregressive Model) but with an additional cross-entropy loss for chord labels at the higher level and a summarization connection passing back to the high level from the low-level at the end of each bar. They show that given monophonic music with chord labels their model is able to produce reasonably coherent chords and note samples, and improves the NLL over a low-level model alone. The core of their approach (using measures as a natural hierarchy for a multi-level RNN) is a good one, but not new in of itself as it was the basis for the prior work of Roberts et al. (http://proceedings.mlr.press/v80/roberts18a/roberts18a.pdf). The authors highlight in section 3.3 that their work is distinguished by the summarization connection, but do not provide any evidence in their results that the connection is useful. They find in Table 1 that connection hurts NLL on the note level, and do not compare summarized to non-summarized models in the listening tests. The area for most improvement in the paper is the evaluation, especially the listening tests. The authors compare samples from four models that generate different types of outputs and were trained on different datasets. Because of this, the notion of user preference is completely convoluted with external factors. In particular the comparisons to DeepBach and SequenceTutor are inappropriate and give little information about the quality of the model architecture itself. To be useful comparisons should be restricted to model architectures that are trained on the exact same data as HAPPIER, and output both chords and melodies like HAPPIER does. Given that the novelty of the paper rests on the summarization connections, and they were not shown to help NLL, it would be natural to try and compare the different model variants in the paper and see if the NLL misses some element of larger structure that listeners may care about. My rating is thus based on the lack of novelty and poor quality of evaluation justifying the actual novel aspects of the paper. Some minor comments that could also help improve the paper:* Including NLL for chords is important to compare summarization (does it help in chord prediction?)* The input representation could use further clarifying. What is the dictionary of chords to predict from? Are they just chord names or individual notes (the figures imply notes, but that doesn't seem what's happening). In Figure 2, clarify the meaning of tick, what 1, 0 means in terms of time progression.* Provide quantitative evidence for the claims in 4.2 that the notes and chords belong to the same key. Compare real data and generated data for those statistics. * Provide explanation for why Note NLL is higher for Summarization.* Minor notation problems: Eq 1, f should not be a function of n_i. Similar, in Eq 2, p(n_{ij}) should be a function of c_i. Eq 3 doesn't define what the hat represents. The authors propose an approximate MCMC method for sampling a posterior distribution of weights in a Bayesian neural network.  They claim that existing MCMC methods are limited by poor scaling with dimensionality of the weights, and they propose a method inspired by HMC on finite-dimensional approximations of measures on an infinite-dimensional Hilbert space (Beskos et al, 2011).  In short, the idea is to use a low dimensional approximation to the parameters (i.e. weights) of the neural network, representing them instead as a weighted combination of basis functions in neural network parameter space.  Then the authors propose to use HMC on this lower dimensional representation.  While the idea is intriguing, there are a number of flaws in the presentation, notational inconsistencies, and missing experiments that prohibit acceptance in the current form.The authors define a functional, f: \theta -&gt; [0, 1], that maps neural network parameters \theta to the unit interval.  They claim that this function defines a probability distribution on \theta, but this not warranted.  First, \theta is a continuous random variable and its probability density need not be bounded above by one; second, the authors have made no constraints on f actually being normalized.  The second flaw is that the authors equate a posterior on f given the data with a posterior on the parameters \theta themselves.  Cf. Eq 4 and paragraph above.  There is a big difference between a posterior on parameters and a posterior on distributions over parameters.   Moreover, Eq. 5 doesn't make sense: there is only one posterior f; there are no samples of the posterior. The third problem appears in the start of Section 3, where the authors now call the posterior U(theta) instead of f.  They make a finite approximation of posterior U(\theta) = \sum_i \lambda_i u_i, which is inconsistent with Beskos et al.  I believe the authors intend to use a low dimensional approximation to \theta rather than its posterior U(\theta).  For example, if \theta = \sum_i \lambda_i u_i for fixed basis functions u_i, then you can approximate a posterior on \theta with a posterior on \lambda.The fourth, and most important problem, is that the basis functions u_i are never defined.  How are these chosen? Beskos et al use the eigenfunctions of the Gaussian base measure \pi_0, but no such measure exists here.  Moreover, this choice will have a substantial impact on the approximation quality. There are more inconsistencies and notational problems throughout the paper.  Section 4.1 begins with a mean field approximation that seems out of place.  Section 3 clearly states that the posterior on theta is approximated with a posterior on lambda, and this cannot factorize over the dimensions of theta.  Finally, the authors again confuse the posterior on weights with a posterior on distributions of weights in Eq 11.   \tilde{U} is introduced as a function of lambda in Eq 14 and then called with f in line 4 of Alg. 1.  These two types are not interchangeable. These inconsistencies cast doubt on the subsequent experiments.  Assuming the algorithm is correct, a fundamental experiment is still missing. To justify this approach, the authors should show how the posterior approximation quality varies as a function of the size of the low dimensional approximation, D.I reiterate that the idea of approximating the posterior distribution over neural network weights with a posterior distribution over a lower dimensional representation of weights is interesting.  Unfortunately, the abundance of errors in presentation cloud the positive contributions of this paper. PROS:* The paper was well-written and explained the method and the experiments wellCONS:* The problem seems ill-posed to me. Sound is temporal and the problem should probably be sound-to-video conversion not sound-to-image. * A link to generated images from sounds where one could actually evaluate the generations would be useful. Currently the only way to evaluate the results is via labels.* Similarly, a baseline where images are generated given the classification labels of the sounds would probably produce better looking images. Such baseline is not provided, and it is not clear to me what a multi-modal feature extraction is providing on top of this.  For example, in the case of StackGAN, the GAN that was converting text to images, the text was describing something about the image that one could quantify in the resulting generation (eg a blue bird as opposed to a yellow one). Here such an advantage is not clear and if there is one, it should be clearly stated and discussed.* The results in Fig. 3 seem particularly poor and on par with current GAN generations. I think this part of the model should be improved before attempting to improve the rest.* In Figures 6 and 7, it is not clear what we are expected to see. Also, the labels do not correspond to the real images in many of hte cases (eg pajama, wing, volcano etc).Finally in the discussion, DiscoGAN is mentioned as something to look into for future work. I should note that DiscoGAN is converting samples between domains of the same modality (vision), in the context of domain adaptation, similarly to other works. The authors formulate policy optimization as a two step iterative procedure: 1) solving a constrained optimization problem in the non-parameterized policy space, 2) using supervised regression to project this onto a parameterized policy. This approach generally applies to both continuous and discrete action spaces and can handle a variety of constraints. Their primary claims is that this approach improves sample-efficiency over TRPO on Mujoco tasks and over PPO on Atari games.The method proposed in the paper has strong similarities with existing methods, but lacks comparisons with these approaches. The authors have not clearly demonstrated that SPU provides novel insights beyond the existing literature. I'm happy to change my score if the authors can convince me otherwise.Main comments:The focus of the paper is sample-efficiency, but the intro restricts to the on-policy setting. The authors should justify this choice. It is well known that off-policy algorithms (e.g., SAC for continuous control and Implicit Quantile Networks for Atari) are much more sample-efficient.In Sec 4, what is the advantage of breaking the problem up into these 3 steps versus directly trying to solve (9),(10)? In fact, if we convert (10) into a penalty and take the derivative, we arrive at nearly the same gradient as (17). As this is central to the SPU framework, this needs to be justified.MPO (Abdolmaleki et al. 2018) is very closely related to SPU. It is unclear if SPU provides any additional insights or benefits over MPO. This needs to be discussed and compared.The experimental section could be strengthened by:* Given the similarity to SPU, comparisons to MPO and GAC should be made, or clear justification for why they are not comparable must be given.* Why is the comparison on Mujoco to TRPO in the main text and the comparison to PPO relegated to the appendix? It would make more sense to compare to PPO, so the authors need to justify this decision.* The results on Mujoco are quite poor compared to state-of-the-art methods (e.g., SAC). The authors should justify why we should care about their results.Comments:In Sec 2, the authors should be careful about the discounting. For example, they are almost surely not having A_{it} approximate \gamma^t A^{\pi_{\theta_k}}, rather A^{\pi_{\theta_k}}.In Sec 2, the KL is denoted as KL(\pi || pi_k), but in the text is described as the KL from \pi_k to \pi (reversed). From the equations, it appears that is an error, and it should read KL from \pi to \pi_k.In Sec 3, the description of NPG/TRPO is not accurate. The main goal of NPG/TRPO work was to establish monotonic improvement.In Sec 3, computational speed is cited as a major deficit of GAC, especially the solving linear systems with the Hessian (wrt to the actions). This seems rather surprising. Inverting a 1000x1000 matrix on a modern computer takes &lt;1 second, so it doesn't seem like this should be the limiting step for any of the problems encountered.The KL penalty version of PPO seems closely related to SPU. Can the authors mention differences with that version of PPO in the related work?In Sec 4, step iii is described as supervised learning. Can the authors elaborate on why? I would typically think of the other direction as supervised learning as that leads to MLE.In Sec 5.1, what is the justification/reasoning for setting \tilde{\lambda_{s_i}} = \lambda and introducing the indicator functions?Sec 5.2 is not evaluated and Sec 5.3 produces inferior results, so it may make sense to move these to the appendix. Otherwise, the authors should explain situations where we would expect these to be useful or provide some additional insight. It also should be noted that the proximity constraints in TRPO/PPO follow from a theoretical argument and are not arbitrary choices.Sec 5.3 seems to deviate quite a bit from the SPU framework. In addition to the differences pointed out in the text, the "supervised" loss changes too. Can the authors justify/explain the reasoning for these changes? This paper proposes a new method to input data to a conditional discriminator network. The standard setup would simply concatenate the "condition" image with the "output" image (i.e., the real image, or the generator's output corresponding to the condition). The setup here is to feed these two images to two separate encoders, and gradually fuse the features produced by the two. The experiments show that this delivers superior performance to concatenation, on three image-to-image translation tasks.I think this is a good idea, but it's a very small contribution. The entire technical approach can be summarized in 2-3 sentences, and it is not particularly novel. Two-stream models and skip connections have been discussed and explored in hundreds of papers. Applying these insights to a discriminator is not a significant leap. The theoretical "motivation" equations in Sec. 3.1 are obvious and could be skipped entirely. In summary, the paper makes sense, but it does not present substantively new ideas. I do not recommend the paper for acceptance. This paper presents an a particular architecture for conditional discriminators in the cGAN framework. Different to the conventional approach of concatenating the conditioning information to the input, the authors propose to process them separately with two distinct convolutional networks fusing (by element-wise addition) intermediate features of the conditioning branch into the input branch at each layer.Pros:+ The writing is mostly clear and easy to follow.+ I feel that exploring better conditioning strategies is an important direction. Quite often the discriminator discards additional inputs if no special measures against this behaviour are taken.+ The proposed method seem to outperform the baselinesCons:- Im generally not excited about the architecture as it seems a slight variation of the existing methods. See, for example, the PixelCNN paper [van den Oord et al., 2016] and FiLM [Perez et al., 2017].- Theoretical justification of the approach is quite weak. The paper shows that the proposed fusion method may result in higher activation values (in case of the ReLU non-linearity, other cases are not considered at all) but this is not linked properly to the performance of the entire system. Paragraph 3 of section 3.1 (sentence 3 and onward) seems to contain a theoretical claim which is never proved.- It seems that the authors never compare their results with the state-of-the-art. The narrative would be much more convincing if the proposed way of conditioning yielded superior performance compared to the existing systems. From the paper its not clear how bad/good the baselines are.Notes/questions:* Section 3.1, paragraph 1: Needs to be rephrased. Its not totally clear what the authors mean here.* Section 3.1, paragraph 4: We observed that the fusion & -  Could you elaborate on this? I think you should give a more detailed explanation with examples because its hard to guess what those important features are by looking at the figure.* Figure 4: I would really want to see the result of the projection discriminator as it seems to be quite strong according to the tables. The second row of last column (which is the result of the proposed system) suspiciously resembles the ground-truth - is it a mistake?* Figure 5: It seems that all the experiments have not been run until convergence. Im wondering if the difference in performance is going to be as significant when the model are trained fully.In my opinion, the proposed method is neither sufficiently novel nor justified properly. On top of that, the experimental section is not particularly convincing. Therefore, I would not recommend the paper in its present form for acceptance. This paper connects continual learning and GAN training together, and propose to use standard continual learning schemes (EWC etc) to improve GAN training.Continual learning for GANs is certainly an important problem to look at. Even though I like the problem, I'm not convinced with the solution provided by the paper. The paper in it's current form, in terms of technical contributions and experimental analyses presented to support the hypotheses it started with, is not good enough to be accepted in ICLR. My comments:1) Catastrophic forgetting of discriminator: Interesting view about mode collapse. I have following concerns:(a) I like the view in which the training is shown as a sequential process. I wonder if we could solve the issue of catastrophic forgetting of discriminator by storing sufficient fake examples from previously generated samples from the generator. Storing previous generations has already been explored, however, just to prove the point that forgetting is an issue, it would be interesting to store enough samples for the mixture of Gaussian setting and analyse.(b) Why not thinking of catastrophic forgetting of generator? What if we constrain the generator to not-to-forget about previously generated samples by Fisher or something similar? In this case, every new task in the training process will have sufficient fake samples from all the modes.2) Lack of technical contributions: The approach, in which EWC or IS is being used to regulate the discriminator's parameters, seems to be straightforward. The issue of multiple parameters is being resolved by storing one Fisher/Score vector using moving average type scheme. This, to me, is almost same as Online EWC or EWC++. Both these papers have already addressed this issue and discussed them in great detail. How is this approach different?3) Not sure what exactly Fig 2 is conveying as D_1^{gen}, \cdots, D_T^{gen} should almost be the same, so training on one and testing forgetting on other doesn't actually say much. I think we can't conclude anything from this figure, at least using MNIST experiments.Minor:4) I'm assuming that you call your method EWC/IS GAN (I think it wasn't explicitly mentioned in the paper). Why don't you call Seff et al. 2018 (they used EWC with GAN for the first time) work EWC-GAN? I personally feel that it's important to give acronyms so that it doesn't undermine previous works. Just to clarify, I'm not advocating the work by Seff et al..5) Please provide citations for mode collapseOnline EWC: Progress &amp; compress: A scalable framework for continual learnin, ICML 2018.EWC++: Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence, ECCV 2018. This paper presents a way to induce low-rank representations in a deep neural network and study its effect on adversarial attacks.QualityThe analyses are conducted on several types of problems, first classification tasks for confirmation of the low-rank structure, and then on adversarial attacks. Unfortunately, there is only a very few number of experiments per analysis, making it virtually impossible to infer reliably any trends in the data. Table 1, 2, 3 and 4 contains at best enough information for a proof of concept, but it is not possible to make any conclusion out of them. Also, VGG results only appear in figure 2 where they appear to contradict the conclusions held by the authors. Is it difficult to understand why results from VGG do not appear in any table. ClarityThe paper is difficult to follow. Introduction gives too much details and even contains methodological information, all of which obfuscates the main message which does get more clear in the latter sections. The sections 2 and 3 are confusing because they do not follow the logic presented in the abstract. The latter states that observations on the low-rank structure of the representations will be done prior to experimentally impose low-rank. However, section 2 presents the low-rank structure imposed on models while section 3 presents the observations of low-rank-representations jointly with the results of imposed low-rank structure.There is no clear definitions of what the "intriguing properties" are beside the fact that forced low-rank representations yield similar results on classification and are more robust to adversarial attacks on a very limited number of experiments.OriginalityUsing low-rank representation is not something new and has already been explored in [1] for instance.SignificanceThere would be an important contribution to make if the author would analyze the effect of low-rank by varying the constraint. However, the current analyses are not pushed far enough to get any useful insight using only a fixed rank and making a minor modification by adding one or two LR-layers. Experiments in table 2 is a good step in this direction nonetheless.[1] Luo, Ping. "Learning deep architectures via generalized whitened neural networks." In International Conference on Machine Learning, pp. 2238-2246. 2017. This paper compresses neural networks via so called Sparse Binary Neural Network designs. The proposed idea is naïve, directly using a slightly modified sign function to quantize network weights into 0 and 1 instead of commonly defined -1 and 1. Experiments on small MNIST and CIFAR-10 datasets with two shallow and old neural networks are provided.This paper has obvious weaknesses.--- Limited noveltyThe proposed method is naïve. The authors merely replace  binary weights {-1, 1} by {0, 1}, using common quantization tricks for binary neural networks, such as straight-through estimator (STE) and a slightly modified sign function. The authors claim that such a modification can bring significantly improved compression. However, it is problematic, as it will force all quantized network weights to be non-negative, leading to serious accuracy drop. For example, in Table 3, a shallow VGG-like network on CIFAR-10 dataset shows about 10% absolute accuracy drop compared to the binary weight counterpart. Furthermore,  in the optimization, the authors add two non-negative constraints, which makes the training with STE even more challenging. I believe, experiments on large-scale image classification dataset such as ImageNet with modern CNNs will lead to more serious accuracy drops. Actually, more impressive neural network compression yet with good accuracy can be achieved via combing quantization and pruning. There exist numerous works in this field,  e.g., "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding" in ICLR 2016, "Clip-q: Deep network compression learning by in-parallel pruning-quantization" in CVPR 2018 and "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy" in CVPR 2020. Unfortunately, they are missed by the authors. --- Poor writingThe paper is poorly written, including introduction (messy), related works (poor), proposed method (tedious) and experiments presentation (weak). --- Weak experimentsThere is no comparison with state of the art CNN compression methods, combining quantization and pruning for improved compression.The authors only conduct toy experiments, a 3-layer fully connected LeNet on MNIST dataset and a shallow VGG-like network on CIFAR-10 dataset.Even with toy experiments, results are very weak, showing serious accuracy drop even for a shallow VGG-like network on CIFAR-10 dataset, compared to the binary weight counterpart.    The paper propose to address the heteroscedastic regression problem using deep neural networks. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.The major issue to me is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method. See Chapter 10 of http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdfThis paper failed to cite any relevant reference and clarify the novelty.To apply the method to a deep learning setting, some interesting problem can be how to estimate the variance with deep network in a reliable way (this was done previously using classic models). However, this paper did not tackle this harder (and more interesting) problem. Instead, it assume the variance is simply given during training. This is not very realistic in real world setting. The experiments are all synthetic and are not particularly convincing. Finally, the paper claims a lot of connection with privileged information (LUPI). But I found it hard to consider this variance a similar concept as privileged information, which is realistic and interesting. The paper contains two curriculum learning algorithms of which one assume knowledge of the parameters found by the baseline, uniform-sampling, model to push updates in that direction, and the second orders images according to an increasing stddev/entropy of pixels. While the first approach is impractical because of the strong assumption, the second approach demonstrates small gains that lie within random variance (Fig. 5, Fig. 6) and would be not straight-forward to apply to non-image data e.g. text. These reasons make the paper hard to accept.The main problem is knowing the parameters of the baseline, SGD, optimization. It's not clear why would one even need optimization again, if (a good enough) result is already known and gains from this re-optimization do not significantly improve over this baseline. The speedups mentioned in the abstract (45% and 43%) could not be located in the results in main body of the paper. How were they measured? Even if aligning updates with the SGD-trained parameters does speed up convergence, re-training from scratch will cost 143% of baseline time instead of 43%, as the standard training needs to be counted too.Issues include:- How to sample using \rho_{t,i}? It's not a distribution and can be negative.- Figure 1: Judging from the plot, the vanilla curve converges faster than the curriculum. How can one see the >40% curriculum speed up?- Abstract's claim of removing noise is only supported in Section 4 through citing related works. Also, more evidence would be needed to call k a regularizer.- lines 9-10 in Algorithm 1 would interfere with bucketing in seq2seq applications and adversely affect performance.Regarding related work in Sec. 3: I couldn't confirm in (Graves et al, 2017) that they also sort examples by difficulty.The last approach to define curriculum through statistical quantities makes sense, in principle, although the difference between curves in Fig. 5 is very small and could be caused by random variance as the error bars on Fig. 5 and Fig. 6 show. Another problem is that it's straight-forwardly applicable only to images and not categorical data, like text. One suggestion of possible paper improvement: consider swapping and reworking sections 5 and 3, so that the content of sec. 5 becomes the main proposal and a reworked sec. 3 - its analysis. There one could analyze if the example ordering according to stddev does bias updates towards some "good" point of convergence, with one possible definition of "good" according to (now, unknown during optimization) SGD results.Other minor remarks:- "greedy approach" is mentioned multiple times before being explained on page 4. Consider deferring the use of term to that place.- Contributions: useful is a vacuous word, consider dropping it.- notation: square brackets used to denote several objects - sequences [B1, B2, ..] , ranges [T] and vectors [x1, x2, .. ]. Using different brackets could be better.- well-known concepts:  * no need to define stddev and mean in (2)  * (Arora, 1981): if entropy requires a citation at all then citing Shannon directly would be more appropriate.- Sec. 2: curriculum is defined by two functions -> we define curriculum by two functions- conclusion: display -> show- while CL indicate that -> while CL indicates that- judicial ordering -> judicious ordering Summary:This paper studies curriculum learning and proposes two methods to order the examples by (1) gradient information and (2) statistical measures like standard derivation and entropy. The experiment results show that the proposed curriculum learning strategies can speed up the convergence by a large margin and the authors provide some insights about why curriculum learning works.Strengths:1. The proposed "dynamic curriculum algorithm" can speed up the convergence by ~45% and the proposed task-specific curriculum strategy based on standard deviation and entropy can yield an average speedup of ~43%.2. The code and data are shared and helpful for reproducing the experiments conducted in the paper.Weaknesses:1. The paper is poorly written. There are even no sections to discuss related works and experimental settings in the main paper. Although some related works are discussed scatteredly in the paper, it might be helpful to have a specific section to compare related works with the proposed methods, which makes it much easier to identify the contributions and novelty of this work. Besides, although I found the experimental settings in the supplementary, the main paper at least should have discussed the basic experimental setup to understand how the experiments are conducted.2. The proposed DCL algorithm requires an optimal weight or a local optimal weight to calculate the difficulty scores. This requirement is unreasonable and renders the proposed methods useless.3. The proposed scoring function (the equation at the end of page 3) requires to compute the gradients on each sample. Perform back-propagation and computing gradient are highly prohibitive. The learning curves against the time cost should also be reported, in complement to the learning curves against the training steps in Fig.1.4. The pace function is just a constant, dependent on a tunable hyperparameter k. From Fig.2, it seems that the value of k has a large impact on the testing accuracy. It is not mentioned in the paper how the value of k is selected.Suggestions for improvement:1. It might be better to have a specific section to discuss related works and compare them with the proposed method.2. Index all equations. Questions:The questions to be addressed in the rebuttal are listed below:1. Where does the optimal weight in DCL comes from? Can the authors justify why a given optimal weight can be used during the training?2. What is the time cost of the scoring function?3. How is the value of k in the pace function tuned? Summary:This submission works on the neural machine translation problem. The authors extend the previous works on leveraging language statistics or prior knowledge (SMT model or whatever) in LSTM based NMT models in self-attention based NMT models, Transformer model. The authors propose two alternatives to incorporate prior knowledge, which are the word frequency information for the monolingual data and the prior translation lexicon information for the bilingual data. These resources are integrated into the hidden representations from the self-attention computations and then the two output hidden representations are gated together for upper computations. The experiments are conducted on two typical NMT datasets: WMT14 En->De and WMT17 Zh->En, the results show that the proposed method can improve the NMT model performances. General comments:The submission works on the neural machine translation tasks, which are important in recent years and have achieved great breakouts. This work extends the previous work on how to leverage the information from the statistical data knowledge into the NMT model. Overall speaking, there is no big tech flaws or incorrect points, but this paper is not suitable for ICLR conference, better for NLP related conferences. The main pros and cons of my side are listed as follows.Strengths:* The paper writing is good in an overall reading, no big mistakes existed in this submission.* The experiments are conducted on two different large datasets, with extensive ablation studies to show the effectiveness of each part of the model. Weaknesses:* The motivation of this work is not presented in a very clear way. The authors claim that previous works can not explore other prior knowledge in a universal way. This is somehow too overwhelmed, without clear examples or explanations, I feel hard to be convinced of this point. Besides, the authors do not present that the methods utilized in this submission are much better than previous works in terms of the differences and contributions. * The contribution from both technique insights and the studying problem is limited. First, for the studying problem, how to leverage the information from the statistic data knowledge is widely studied in the MT committee previously and in recent years. The authors must be very familiar with the works conducted on LSTM models about incorporating SMT knowledge or others. Second, as for the technique contribution, the authors propose word frequency information and the translation lexicon knowledge. These two are typical methods used in MT research, which makes the submission not so insightful from the different prior knowledge. Therefore, the contributions of the submission are not important or enough. * As for the specific utilization of the information, there are also several concerns. The authors seem to emphasize the prior word frequency from the "monolingual" data, which looks unnecessary. In current modeling, there is nothing related to extra monolingual data, but only the source data. The authors want to compare with the lexicon information from the bilingual dataset, but this will mislead the readers to think of the extra monolingual data. As for the bilingual translation lexicon, the extra costs increase and the authors should discuss this. * There are several unclear parts of the presentation. First, the dimension of the matrix $M$ is not clear. For example, $M_F$ seems to be a $J\times J$, but for a bilingual lexicon, it is $J\times K\times d_{model}$ for $M_T$. Therefore, what is induced $K_F, V_F$ and $K_T, V_T$, and how are they converted to $K$  and $V$. By the way, in the bilingual lexicon, $L$ is leveraged, but not show in $M_T$, this also makes it to be unclear. It supposes that $M$ should be a similar size as $H$, but seems not.* In current modeling, the prior knowledge is only incorporated in the encoder-side and the training phase, if I understand correctly. This is also limited. I acknowledge that the word frequency information is from the training data, therefore only the training phase is influenced, but the authors should clearly talk and discuss this. By the way, the encoder-side only modeling again makes the contribution to be limited. More efforts are encouraged than this submission. * One another point is about the experiment results. The authors said that they reimplement the baselines of the Transformer base model and Transformer large model. However, according to the settings they described in the submission, the baselines are reported at a low level, not a very convincing score. The authors used attention dropout and residual dropout as $0.1$, and also they use $8$ GPU cards (since $4096\times 8$ batch size), and checkpoints are averaged. Therefore, according to the experiences, the baseline results should be higher, and it makes me not be convinced about "which makes the evaluation convincing". However, I acknowledge that the method proposed is effective and the results of the proposed method are in reasonable scores, what I want to mention is only the baseline results. Minor suggestions:* Section 5.7 about length experiments is not necessary, it is not related to the claim, motivation, contribution of the submission. So it is recommended to remove this part. In a word, this paper is okay for NLP related conferences, but maybe not enough for ICLR.  The introduction and the title does not match. Metric learning does not require to specify the dimension; while the embedding has to specify the reduced dimension. I feel confused that the authors mix these two concepts.The objective in (1) is very close to that of t-SNE[5], where it uses the KL as the objective. Then other update formula are similar.  This paper facilitates the effect of temperature in the Softmax function to heuristically learn a compact and spread-out embedding. However, such an idea have been widely used and investigated in Reinforcement learning [1], Knowledge distillation [2], classification [3] and discrete variable optimization [4] and t-SNE visualization [5] etc. Thus, the insight about the temperature effect on the embedding from the second last layer, cannot be novel any more. Based on this, the proposed ``heating-up strategy to leverage its effect on the embedding is heuristic, since the temperature parameter is manually set instead of automatically learning. In this case, I do expect the authors should provide more in-depth theoretical analysis. The authors do not present more experimental results on the correlation between the final performance and this temperature setting. Besides, as the alpha increases or decreases, the side-effect on the learning rate setting for the optimization have not clearly analyzed, which leaves more concerns on tuning performance. [1] Sutton, R. S. and Barto A. G.&nbsp;Reinforcement Learning: An Introduction. The MIT Press, Cambridge, MA, 1998.[2] Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. NIPS 2015.[3] Guo, Chuan, et al. "On calibration of modern neural networks."&nbsp;ICML 2017.[4] Jang E, Gu S, Poole B. Categorical reparameterization with gumbel-softmax. ICLR 2017.[5] Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of machine learning research, 2008, 9(Nov): 2579-2605. The paper describes a framework for training a self-driving policy by augmenting imitation loss with additional loss terms that penalize undesired behaviors and that encourage progress. The policy takes as input a parsed representation of the scene (rather than raw images) and outputs pose trajectories for a down-stream controller. The method is trained on simulated data that includes perturbations to improve generalizability. The framework is evaluated in simulation through a series of ablations to better understand the contribution of the different loss terms.STRENGTHS+ Paper acknowledges the difficulty of end-to-end (pixels-to-torque) learning for autonomous driving and instead reasons over pre-processed inputs in the form of lower-dimensional images (and image sequences) that capture obstacles as bounding boxes and simple lines for routes, grayscale intensities, etc. Similarly, the output is a trajectory that is then fed to a controller responsible for tracking this trajectory.WEAKNESSES- The insufficiency of behavioral cloning is not surprising, as noted, given the covariate shift. It would be interesting to consider a  no-regret formulation analogous to Ross et al., 2011, even though it would require interaction with a human.- The limitation of producing paths in this way is that the network does not explicitly reason over the feasibility of the path, which is important for non-holonomic vehicles. Instead, the network must learn the kinematic and dynamic constraints.- Perturbations of the simulated trajectories are used to expose the model to collisions and other rare events, but is not clear that simple trajectory perturbations such as those used here provide a sufficient exposure to these rare events.- The fact that the 2D image that expresses the vehicle's position is absolute limits the environment in which the network is valid. The experiments are conducted on images corresponding to an 80m x 80m environment, which is trivially small.- The proposed framework is highly specific to self-driving and the extent to which it provides insights for other domains is not clear.- The ablation experiments are not very compelling. In the case of the nudging experiment, all models result in collisions with M4 being the best model with a 10% collision rate. The trajectory perturbation results are better. In the case of the slowing experiment, M3 is the only version to not result in collision, whereas M4 collides 5% of the time. It isn't clear than which model is preferable since, while M3 never collides in the case of the slowing down experiment, it collides 45% of the time in the nudging experiment, almost as frequently as the M0 baseline.- The paper claims that the model was run on a real robot, but there is no experimental evaluation of the results, only a reference to videos. The results of these experiments should be quantified and discussed or the reference to running on a real vehicle should be toned down, if not removed.- Equation 3 requires knowledge of the ground-truth distribution. How is this determined? Summary:The authors propose a novel adversarial learning framework consisting of a coarse-to-fine generator and a multiple instance discriminator to generate high-quality sentences without supervision, along with two training mechanisms to make the generator produce higher-quality sentences.The coarse-to-fine generator is implemented as a two-stage decoder where the first stage decoder is initialized with a noise vector z and produces an initial sequence by sampling from its distribution at each step. The second stage decoder is able to attend to first stage decoder hidden states when generating its sequence and is initialized with the last hidden state of the first state decoderTwo training frameworks are defined to help the coarse-to-fine generator learn to produce coherent text. In both frameworks, a multi-instance learner is used as a discriminator and the overall score for the sequence is an average of word-level scores. In DelibGAN-I, the first stage decoder is just trained to minimize the negative log likelihood of producing the sampled sequence. Im not sure I understand how G1 learns to generate coherent text by just maximizing the probability of its samples.In DelibGAN-2, the word-level scores from passing the first stage decoders output through the discriminator is used to score the sequence. This makes more sense to me w.r.t to why the first stage generator could learn to produce coherent text.Review:The writing could be clearer. Spacing is consistently an issue in w.r.t. to citations, referencing equations an figures, and using punctuations. Equations are difficult to understand, as well. See Equation 5. Im confused by how the discriminator score of x_t is computed in equation 5. It seems to be the score of the multi-instance learner when applied to the concatenation of the previous generated tokens and the most recently generated token. This isnt really a roll-out policy, however, since only a single step is taken in the future. Its just scoring the next token according to the discriminator. In this case, Im not sure what the summation over j is supposed to represent. It seems to index different histories when scoring word x_t. The authors should clarify exactly what the scoring procedure is for sequences generated by G2 and make sure that clarification matches what is written in the equation.Because deliberation has previous been explored in Xia et al., 2017, the novelty of this work rests on the application of deliberation to GAN training of generative text models, as well as the development of the multi-instance discriminator for assigning word-level scores to the first-stage decoder's actions. However, its difficult to know how much of the improvement in performance is due to the modeling improvement because the evaluations are missing key details.First, no information about the models hyperparameter setting were provided. Naturally, having two generators would double the number of parameters of the total model. Was an ablation run that looked at doubling the parameters of a single-generator? How powerful are the models being used in this new setup. With powerful models such as the OpenAI GPT being available for generative modeling, how powerful are the base units in this framework?Second, I dont have much intuition for how difficult the tasks being performed are. Because the authors arent evaluating on particularly difficult language datasets, they should provide statistics about the datasets w.r.t to vocabulary size, average length, etc.Consequently, a lot of information that is necessary for evaluating the strengths and weaknesses of this paper are missing from the write-up.Questions:Equation 2 is slightly confusing. Is the representation of the word at that time step not conditioned on previous and future hidden states?Is the multi-instance discriminator used in DelibGAN-II or is G2 only scored using the sentence level discriminator score?Small:-Wang &amp; Wan, 2018a are definitely not the first work to discover that using word-level losses to train models for sentence-level evaluations is suboptimal-Please fix issues with spacing. There are issues when citing papers, or referencing equations, or using punctuation.-Equation 3 is missing a closing parenthesis on the softmax In computing the gradient of the ELBO, the main challenge lies in computing the gradient of the reconstruction loss with respect to the encoder parameters. VAEs traditionally rely on reparameterization in order to obtain a low-variance estimate, but there are a number of other gradient estimators that one can apply. The authors here proprose to use a trick that is known, but perhaps not widely known: If we introduce an importance sampling distribution, then we can use samples from this distribution to compute an importance-weighted estimate of the gradient. The idea is now that we can compute the gradient w.r.t. the encoder parameters as a simple importance-sampling estimate, which obviates then need for reparameterization, or likelihood-ratio estimators. The authors then apply this trick to train VAEs with discrete latent variables.While I think that the idea that the authors present in this paper is worth further exploration, the paper in its current form is not sufficiently mature to appear at ICLR. The two areas where this paper would benefit from improvement are1. Discussion of related work. While the authors seem to suggest that there has been no work on VAEs with discrete latent variables, there has in fact been quite a lot of work, including work on VAEs that contain both discrete and continuous variables (e.g. [8-10], but I'm almost certainly missing further references). There has also been a large body of work on continuous relaxations of discrete variables that are amenable to reparameterization (e.g. [6-7], and references therein). There has also been a line of work relating importance sampling to variational objectives (see [1-3] as key references). Finally, there is also related work on reweighted-wake-sleep style objectives (see [4]) which similarly don't require reparameterization. From what I can tell, none of these references are cited or discussed as related work. In order to place this work in context, I would rewrite 2 to discuss approaches to gradient estimation in this space, which then makes it much easier to explain how this approach differs. 2. Empirical evaluation.The authors only evaluate on MNIST and F-MNIST, and don't compare to any existing approaches. More than a couple of reconstructions, what I would like to see is an analysis of gradient variances, asymptotic ELBO estimates. I would also like to see a larger set of problems. Finally I would like to see a clear comparison to other methods based on, e.g., continuous relaxations. References[1] Y. Burda, R. Grosse, and R. Salakhutdinov, Importance Weighted Autoencoders, arXiv:1509.00519 [cs, stat], Sep. 2015.[2] T. Rainforth et al., Tighter Variational Bounds are Not Necessarily Better, arXiv:1802.04537 [cs, stat], Feb. 2018.[3] G. Tucker, D. Lawson, S. Gu, and C. J. Maddison, Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives, arXiv:1810.04152 [cs, stat], Oct. 2018.[4] T. A. Le, A. R. Kosiorek, N. Siddharth, Y. W. Teh, and F. Wood, Revisiting Reweighted Wake-Sleep, arXiv:1805.10469 [cs, stat], May 2018.[5] A. Mnih and D. J. Rezende, Variational inference for Monte Carlo objectives, arXiv:1602.06725 [cs, stat], Feb. 2016.[6] G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein, REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models, in Advances in Neural Information Processing Systems, 2017, pp. 26242633.[7] W. Grathwohl, D. Choi, Y. Wu, G. Roeder, and D. Duvenaud, Backpropagation through the Void: Optimizing control variates for black-box gradient estimation, arXiv preprint arXiv:1711.00123, 2017.[8] J. T. Rolfe, Discrete Variational Autoencoders, arXiv:1609.02200 [cs, stat], Sep. 2016.[9] E. Dupont, Learning Disentangled Joint Continuous and Discrete Representations, arXiv:1804.00104 [cs, stat], Mar. 2018.[10] B. Esmaeili et al., Structured Disentangled Representations, arXiv:1804.02086 [cs, stat], Apr. 2018. Summary:This paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \theta) \log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.Critique:The gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih &amp; Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  Conclusion: Due to lack of novelty, I recommend rejection.Miscellaneous points:...there exist no simple solutions to circumvent this problem.  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. ...after training q(z|x) is a very good approximation to the true posterior p(z|x).  Thats not necessarily true.  Equation #2 should be just equal to Equation #1.Kingma &amp; Welling (2013) proposed to minimize L(\theta) using stochastic gradient descent on a training set.... First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma &amp; Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.Before Equation #11, the reference to Equation #4 should be to Equation #5....the weighting...depends only on \theta_D and not on \theta_E (p 4). D and E should be switched. The paper discusses the topics of predicting out-of-vocabulary tokens in programs abstract syntax trees. This could have application in code completion and more concretely two tasks are evaluated: - predicting a missing reference to a variable (called FillInTheBlank) - predicting a name of a variable (NameMe)Unfortunately, the paper proposes strange formulations of these tasks, which are overly complex, heavy implementation with unnecessary neural architecture and as a result, does not demonstrate state-of-the-art performance or precision. Figure 1 shows the complexity of the approach, with multiple steps of building a graph, introducing the vocabulary cache to then produce a vector at every node of the input tree of the program, which is unnecessary.The FillInTheBlank task is badly defined already on the running example. The goal is to select a variable to fill in a blank and already in the example on Figure 2, one of the candidate variables is out of scope at the location to fill. The motivation for the proposed formulation with building a graph and then computing attention over nodes in that graph is unclear. For example, [1] (also cited in the paper) solves the same problem more cleanly by producing scores for the variables in the scope and taking the one with maximum score. There is no experimental comparison to that work, but it is unlikely it will perform worse. Also [1] does not suffer from vocabulary problems for that task.The NameMe tasks also shows the weakness of the proposed architectures. This work proposes to compute vectors at every node where a variable occurs and then to average them and decode the variable name to predict. In comparison, several prior works introduce one node per variable (not per occurrence), essentially removing the need to average vectors and enforcing the same name representation at every occurrence of the variable [name]. While not on the same dataset, [2,3] consistently get higher accuracy on a related and more complicated task of predicting multiple names at the same time over multiple programming languages and with much simpler linear models. This is not surprising, because they propose simpler architectures that are better suited for the NameMe task.[1] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programswith graphs. ICLR 2017[2] Veselin Raychev, Martin Vechev, and Andreas Krause. Predicting program properties from BigCode[3] Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav. A General Path-Based Representation for Predicting Program This paper presents a self-training algorithm based on GCN to improve the semi-supervised node classification on graphs. The key idea is to add new nodes with high confidence as supervision to enlarge the labeled nodes. Although the experimental results show the proposed method outperforms or performs similarly to baseline methods, the paper has several weaknesses. First the presented approach is not clearly introduced, with inconsistent statements on building the checking part, and lack of details on how to calculate the confidence to add the new nodes. Second, the novelty of the presented approach is limited, as adding unlabeled samples with high confidence is not a novel idea. Third, the paper writing should be improved, as there are errors.  In Table 2, GAT has better or similar performance comparing to feat5+SVM. Why not using GAT feature aggregation for building the checking part? And, in page 5, it says At the beginning, we concatenate features from feat0 to feat10 and put them into a linear SVM to build the checking part. .  So, it is confusing how the checking part is built. In Algorithm 1,  feat1 to feat9 were concatenated in line 2. The statements are not consistent in the whole paper.In the line 6 of Algorithm 1, Train GCN model and get predictions and confidence matix:   How the confidence is calculated? There are writing errors to correct, such as   Due to its its excellent,   confidence matix:  This paper empirically studies various CNN robustifying mechanisms aiming to achieve rotational invariance. The main finding is that such robustifying mechanisms may lead to lack of robustness against pixel-level attacks such as FGSM and its variants. The paper does a comprehensive job in studying relevant robustifying schemes and attacks strategies. However, the paper does not present sufficiently new information worthy of a regular conference paper, it can be a good workshop paper though for the Robust Learning community. Some analytical insights would really strengthen the work. Also, from an empirical standpoint, the authors need to consider other data sets beyond just the MNIST data set. This paper describes a search space reduction method for neural network based keyboard input methods. The paper discusses two different sampling methods to restrict the vocabulary size during beam search.  Title: The title of the paper is too generic to describe what is actually being done in the paper. Input methods in mobile devices could have also meant speech based input or handwriting based input or swipe based input. It would be very convenient for the readers if the authors use more specific wording in the title to clarify that they are talking about neural network based keyboard typing input.Comparison with prior work: Neural network based on-device keyboard input is a research topic with a lot of previous contributions and the existing literature survey seems lacking. Further it does not even cover popular techniques for inference speed-up like hierarchical softmax computation. It would be easier for the reader to appreciate the  contributions of this paper if the authors compare and contrast with more relevant prior work. The paper demonstrates the main challenge of using LSTM-based language models for input method in real time is the huge amount of computation in the softmax. The authors present a system to speed up the inference by avoiding computing the full softmax in the Japanese conversion task, where the number of output words can be limited from the mapping of the input sequence through a lexicon. The experiment result is encouraging in that the proposed incremental selective softmax approach significantly reduces latency over the standard inference with the full softmax computation while not hurting accuracy much. The paper also evaluates the effect of quantization for LSTM LM model compression in terms of size and accuracy.However, there are a few major problems of the paper as follows:1. The main weakness in the experiment setup is that it misses a few competitive baselines in terms of inference speed, notably hierarchical softmax[1] and self normalization[2]. In the Japanese conversion task in the paper, it only needs to evaluate the scores of limited output words that are given from the mapping of the input sequence through the lexicon. This is exactly like the rescoring setup in speech recognition and machine translation, where self-normalization is typically used for efficient inference to avoid computing the expensive softmax normalization term [2,3]. Assuming the number of selected output words is K and the entire vocabulary size is V, then the time complexity is O(K logV) for the hierarchical softmax, O(K) for self normalization, but O(V) for all the baselines in the paper. Self normalization is simple to implement and works well in practice, while the proposed incremental selective softmax approach in the paper needs an additional step to sample most frequent words to adjust the normalization term. Without showing the self normalization result, I am not convinced that the proposed approach is better and needed.[1] F. Morin and Y. Bengio. "Hierarchical Probabilistic Neural Network Language Model," in Proc. of AISTATS, 2005,[2] J. Devlin et al., "Fast and Robust Neural Network Joint Models for Statistical Machine Translation," in Proc. of ACL, 2014.[3] Y. Shi, W. Zhang, M. Cai and J. Liu, "VARIANCE REGULARIZATION OF RNNLM FOR SPEECH RECOGNITION," in Proc. of ICASSP, 2014.2. The proposed approach would only be useful in speeding up the conversion task, but not applicable to the prediction task where it needs to evaluate all words and choose the top hypotheses. Also how is the latency of the prediction task compared to conversion task? Please also add it to the experiment result.3. The idea of using quantization for neural network model compression is not novel (even for language model), although it is listed as one of the main contributions in Section 1.So in general, I think the paper is insufficient in novelty and missing competitive baselines.Some specific comments:4. Figure 2(b) is not clear what it means, and not referenced anywhere in the paper.5. The last 3 lines in Section 3: "as each path has different missing vocabularies": Why is that? The candidates of the output words should only depend on the input sequence and the lexicon, based on Eq(1)(2).6. It is not clear how to adjust the probability in the second pass of incremental selective softmax. The description "we compute a union of all missing vocabularies, and then recompute the logits of them in batch." is unclear what it means.7. Section 4.2: "is measure with numpy" -&gt; "is measured with numpy".8. Section 4.4: It is not clean how "76x speedup" is computed from Table 2 since all the time numbers are rounded. Consider also showing one digit after the decimal point. Summary: Authors proposed a model for input method for mobile or desktop devices. The goal is to convert the input sequence (from one language to another) or predict the next word. Their model is based on an LSTM with modified softmax activation function that is adjustable for large vocabulary sizes. They showed experimental results on Japanese BCCWJ data set.Clarity: Paper is well-written and well-organized. Notions and methods are clearly expressed. Originality: This paper builds on an LSTM model without enough work or idea to show novelty. Significance: It is below average. Using LSTM is a well-known method for these types of tasks in the literature. Incremental selective softmax is potentially a good approach, however, this work lacks showing significant improvement. The experiments are limited and are done only on one data set.More detailed comments:- My concerns about this work are both on modeling aspects and experiments. Authors mainly focus on highlighting the benefits comparing to n-gram models, and briefly discuss the ongoing developments in neural based models. For example sequential modelings using RNN's have shown promising results in capturing long-term dependencies [1]. Unfortunately authors did not include any discussion on how their approach would compare to that framework nor did they present any experimental comparisons to them.- Although mentioned briefly in the introduction and related work sections, no analytical or experimental comparisons are made to machine translation approaches when their work is closely related to it. I strongly suggest that authors compare their experimental results to some of benchmarks in neural based machine translation discussed in the related works.- In the incremental selection softmax, they use "match" to return all lexicon items matching the partial sequence. How is this done and what are the effects of it on the computational time of the algorithm? Also, It is not clear how authors correct old probabilities in IS softmax step. As mentioned, they add logits of missing vocabulary to the denominators, how do they keep the properties of softmax so that it sums up to 1? And later in the discussion authors mentioned that in practice they compute union of all missing vocabularies, it is not clear how this is done since the advantage of using IS softmax is expressed to be incremental increasing. [1] A.B. Dieng, C. Wang, J. Gao and J. Paisley. TopicRNN: A recurrent neural network with long-range semantic dependency, International Conference on Learning Representations (ICLR), 2017. This paper interprets the optimization of deep neural networks in terms of a two phase process: first a drift phase where gradients self average, and second a diffusion phase where the variance is larger than the square of the mean. As argued by first by Tishby and Zaslavsky and then by Shwartz-Ziv and Tishby (arxiv:1703.00810), the first phase corresponds to the hidden layers becoming more informative about the labels, and the second phase corresponds to a compression of the hidden representation keeping the informative content relatively fixed as in the information bottleneck of Tishby, Pereira, and Bialek. A lot of this paper rehashes discussion from the prior work and does not seem sufficiently original. The main contribution seems to be a bound that is supposed to demonstrate representation compression in the diffusion phase. The authors further argue that this shows that adding hidden layers lead to a boosting of convergence time.Furthermore, the analytic bound relies on a number of assumptions that make it difficult to evaluate. One example is using the continuum limit for SGD (1), which is very popular but not necessarily appropriate. (See, e.g., the discussion in section 2.3.3 in arxiv:1810.00004.)Additionally, there has been extensive discussion in the literature regarding whether the results of Shwartz-Ziv and Tishby (arxiv:1703.00810) hold in general, centering in particular on whether there is a dependence on the choice of the hyperbolic tangent activation function. I find it highly problematic that the authors continue to do all their experiments using the hyperbolic tangent, even though they claim their analytic bounds are supposed to hold for any choice of activation. If the bound is general, why not include experimental results showing that claim? The lack of discussion of this point and the omission of such experiments is highly suspicious.Perhaps more importantly, the authors do not even mention or address this contention or even cite this Saxe et al. paper (https://openreview.net/forum?id=ry_WPG-A-) that brings up this point. They also cite Gabrie et al. (arxiv:1805:09785) as promising work about computing mutual information for deep networks, while my interpretation of that work was pointing out that such methods are highly dependent on choices of binning or regulating continuous variables when computing mutual informations. In fact, I don't see any discussion at all this discretization problem, when it seems absolutely central to understanding whether there is a sensible interpretation of these results or not.For all these reasons, I don't see how this paper can be published in its present form. The paper proposes using structured matrices, specifically circulant and diagonal matrices, to speed up computation and reduce memory requirements in NNs. The idea has been previously explored by a number of papers, as described in the introduction and related work.  The main contribution of the paper is to do some theoretical analysis, which is interesting but of uncertain impact.The experiments compare performance against DeepBagOf`Fframes (DBOF) and MixturesOfExperts (MOE). However, there are other algorithms that are both more competitive and more closely related. I would like to see head-to-head comparisons with tensor-based algorithms such as Novikov et al: https://papers.nips.cc/paper/5787-tensorizing-neural-networks, which achieves huge compression ratios (~200 000x), and other linear-algebra based approaches.  The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder. The resulting model is evaluated on three very simple tasks, showing small improvements over baselines.I'm not entirely sure what the contribution of this paper is supposed to be. The technical novelty seems to be limited to new notation for existing work:- (Sect. 3.1) The separation of forward/backward edges was already present in the (repeatedly cited) Li et al 2015 paper on GGNN (and in Schlichtkrull et al 2017 for GCN). The state update mechanism (a FC layer of the concatenation of old state / incoming messages) seems to be somewhere between a gated unit (as in GGNN) and the "add self-loops to all nodes" trick used in GCN; but no comparison is provided with these existing baselines.- (Sect 3.2) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al; no comparison to these baselines is provided.- (Sect. 3.3) This is a standard attention-based decoder; the fact that the memories come from a graph doesn't change anything fundamental.The experiments are not very informative, as simple baselines already reach &gt;95% accuracy on the chosen tasks. The most notable difference between GGS-NNs and this work seems to be the attention-based decoder, but that is not evaluated explicitly. For the rebuttal phase, I would like to ask the authors to provide the following:- Experimental results for either GGS-NN with an attentional decoder, or their model without an attentional decoder, to check if the reported gains come from that. The final paragraph in Sect. 4 seems to indicate that the attention mechanism is the core enabler of the (small) experimental gains on the baselines.- The results of the GCN/GG-NN models (i.e., just as an encoder) with their decoder on the NLG task.- More precise definition of what they feel the contribution of this paper is, taking into account my comments from above.Overall, I do not think that the paper in its current state merits publication at ICLR. The authors provide new generalization bounds for recurrent neural networks.Their main result is a new bound for vanilla RNNs, but they also havebounds for gated RNNs.They claim that their vanilla bound improves on an earlierbound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.It appears to me that the bounds are incomparable in strength, thatthe new bound has an improved dependence on the size of the outputsequence, but a worse dependence on the number of hidden nodes.I think that the root cause of this difference is that this paper,at its core, adapts the more traditional analysis, used in Haussler's1992 InfComp paper.  New analyses, like from the Bartlett, et alNIPS'17 paper, strove for a weak dependence in the number of parameters,but this proof technique appears to lead to a worse dependence on thedepth.  I think that, if you unwind the network, to view the functionfrom the first t positions of the input to output number t as adepth t network, and apply Haussler's bound, you will get a qualitativelysimilar result (in particular with bounds that scale polynomially withd and t).  I think that Haussler's proof technique can be adapted totake advantage of the weight sharing between layers in the unrollednetwork.It is somewhat interesting to note that the traditional bounds havea better dependence on depth, with correspondingly better dependenceon the length of the output sequence of the RNN.I also do not see that substantial new insight is gained through theanalysis that incorporates gating.I do not see much technical novelty in this paper. The idea of image classification based on patch-level deep feature in the BoF model has been done before.  Just list few of them:Wei et al. HCP: A Flexible CNN Framework for Multi-label Image Classification, IEEE TPAMI 2016Tang et al. Deep Patch Learning for Weakly Supervised Object Classification and Discovery, Pattern Recognition 2017Tang et al. Deep FisherNet for Object Classification, IEEE TNNLSArandjelovi et al. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition, CVPR 2016The above papers are not cited in this paper.There are some unique points. This work does not use RoIPooling layer and has results on ImageNet. However, the previous works use RoIPooling layer to save computations and works on scene understanding images, such PASCAL. These differences are not able to make this paper novel. The paper considers a simplistic extension of first order methods typically used for neural network training. Apart from the basic idea the paper's actual algorithm is hard to read because it is full of lacking definitions. I have tried to piece together whatever I could by reading the proof. The algorithm box is very unclear. For instance the * operator is undefined. To the best of my understanding which the paper changes the update by first checking whether the gradient has the same direction as the previous gradient if yes it uses the component wise maximum of the new gradient and the previous gradient in the update and otherwise it uses the new gradient. Now whether this if condition is checked component wise or an angle between the two vectors is completely unclear. I will really suggest the authors to at least write their algorithm with clarity. Further while stating the theorem there are undefined parameter and even the objective Regret has not been defined anywhere. Further the theorem which I could not verify due to similar unclarity shows I believe the same convergence result as AMSGrad and hence there is no theoretical advantage for the proposed algorithm. In terms of practice further I do not see a significant advantage and it could result be a step size issue . The authors do not say that they do a search over the hyper parameters. On a philosophical level it is unclear what the motivation behind this particular change to any algorithm is. It would be good to discuss what additional advantage is added on top of acceleration. Note that the method feels very much like acceleration. This paper present a spatio-temporal (i.e., 3D version) of Cycle-Consistent Adversarial Networks (CycleGAN) for unsupervised video-to-video translation. The evaluations on multiple datasets show the proposed model is better able to work for video translation in terms of image continuity and frame-wise translation quality. The major contribution of this paper is extending the existing CycleGAN model from image-to-image translation and video-to-video translation using 3D convolutional networks, while it additionally proposes a total penalty term to the loss function. So I mainly concern that such contribution might be not enough for the ICLR quality. The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.In summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable. Summary:This paper presents a framework for generating adversarial perturbations for videos. Specifically, the paper proceeds by using a standard image-based adversarial noise generation setup (such as the FGSM scheme), and applies it to the motion stream of a two-stream action recognition pipeline; this motion stream typically using optical flow images. As such flow generation is usually done offline and thus is not differentiable, the paper resorts to the recent FlowNet 2.0 scheme that uses an end-to-end learnable deep flow generation model. Three variants of the scheme are provided, (i) that perturbs all frames in a flow stack, (ii) that perturbs only a sparse set of frames as decided by the importance of a frame to action classification, and (iii) a variant of (ii) that recalculates the gradients for all frames if the ones selected in (ii) were not adversarial. Experiments are provided on UCF101 dataset and show promise. Analysis is presented on the transferability of  the learned noise to flow images generated via external means.Strengths:1) The different variants of the scheme and sparse selection of the frames to be perturbed are interesting. 2) The paper makes some interesting observations, namely that (i) only a single frame perturbation might be sufficient to make the video adversarial, and (ii) perturbations computed via FlowNet models are not transferable to those with flow computed via external software -- which is often the practice.Weaknesses:1) I think the main weakness of this paper is the lack of any surprise/significant novelty in the presented approach. The main idea follows the common trend in adversarial noise generation for image classification problems, except that the inputs are a stack of frames instead of  a single one; however, such a setting do not seem to bring along any non-trivial challenges. In the second contribution of this paper -- on the sparse selection of frames to attack, there is a lack of clarity in how one would do the iterative attacks at test time, given such sparse frame selection is done via computing the frame level saliency values via the classification loss, which depends on ground truth class labels, which are unavailable at test time. 2) There are previous works that have attempted video level adversarial perturbation generation, which the paper do not cite or contrast to; such as a few below. Further, the literature survey fails to provide any compelling motivation as to why video perturbation generation is any difficult than image based noise generation -- it does not appear so from the subsequent text that this problem deserves any special treatment in the considered context.[a] Learning Discriminative Video Representations Using Adversarial Perturbations, Wang and Cherian, ECCV 2018[b] Sparse Adversarial Perturbations for Videos, Wei et al., arxiv, 20183) It is unclear why the paper chose to consider flow produced by a FlowNet model as their inputs for the attack? Why not consider the flow images directly? Of course, the optical flow algorithm may not be differentiable, but that is perhaps besides the point; the focus should be in perturbing flow, in whatever way it is generated. To that end, given that flow (on static camera images) can be sparse, it would be interesting to see how would a perturbation be generated that needs to operate on local regions (where motion happens). In my opinion, using a FlowNet model for flow generation trivializes the proposed algorithm.4) It could have been interesting if the paper also provided some qualitative results of the optical flow images generated by FlowNet after adding perturbations to the input frames. Are these flow images also quasi-impercitable? Overall, I think the paper has some observations that may be slightly interesting; however, it lacks novelty and the analysis or presentation are unconvincing. PIE extend NICE and Real NVP into situations which require having a smaller dimensionality of the latent variable (d) compared to the dimensionality of the observed variable (D), i.e. d &lt; D. This is done by learning an extension function g(z) from R^d to R^{D-d} and then using the change of variables formula on x and [z, g(z)]. To model probabilistically the deterministic function g(z) is replaced by Normal distribution with mean g(z) and a small variance.PIE is used to build deep generative models and trained on the MNIST dataset. The authors show that the models learnt via PIE produce sharper samples than VAEs and Wasserstein autoencoders (WAEs). No comparison to real NVP is made, which should be the main baseline of comparison to answer the question of "what is the advantage of having d &lt; D?". Further MNIST is no longer a good enough benchmark to evaluate deep generative models. Most representative work in this literature use CIFAR-10, downsampled Imagenet, or Imagenet at 256x256.This work falls short of the standards of ICLR in a few ways:1. The presentation is unclear. The explanation of the extension-restriction idea is overly complicated. Further, the paper does very little to properly contextualize this work in the literature. Real NVP and flow-based models are mentioned but the proposed technique is not compared to it. The authors say they "introduce new class of likelihood-based Auto-Encoders", but this is false as far as I understood. The technique is not even an autoencoder since a separate decoder is not trained, and is obtained by exactly inverting the encoder as in real NVP.2. The experiments are weak. The samples shown are of poor quality, and on a very simplisitic dataset (MNIST). The authors compare with vanilla VAEs, but ignore more recent improvements to VAE such as VAE-IAF, flow-based models, and also autoregressive models. A heuristic is used to measure sharpness and only used to compare against VAE and WAE. Since all these models allow likelihood evaluation, likelihoods should also have been compared.3. The technique itself is a small change over real NVP and it's not clear whether this change brings any improvements or provides any insights about generative modeling. The paper proposes a new objective function for learning disentangled representations in a variational framework, building on the beta-VAE work by Higgins et al, 2017. The approach attempts to minimise the synergy of the information provided by the independent latent dimensions of the model. Unfortunately, the authors do not properly evaluate their newly proposed Non-Syn VAE, only providing a single experiment on a toy dataset and no quantitative metric results. Furthermore, even qualitatively the proposed model is shown to perform no better than the existing factor-VAE baseline.I commend the authors for taking a multi-disciplinary perspective and bringing the information synergy ideas to the area of unsupervised disentangled representation learning. However, the resulting Non-Syn VAE objective function is effectively a different derivation of the original beta-VAE objective. If the authors want to continue with the synergy minimisation approach, I would recommend that they attempt to use it as a novel interpretation of the existing disentangling techniques, and maybe try to develop a more robust disentanglement metric by following this line of reasoning. Unfortunately, in the current form the paper is not suitable for publication. This paper proposes a new approach to enforcing disentanglement in VAEs using a term that penalizes the synergistic mutual information between the latent variables, encouraging representations where any given piece of information about a datapoint can be garnered from a single latent.  In other words, representations where there is no information conveyed by combinations of latents that is not conveyed by considering each latent in isolation.  As the resultant target is intractable to evaluate, a number of approximations are employed for practical training.The high-level idea is quite interesting, but the paper itself is quite a long way of convincing me that this is actually a good approach.  Moreover, the paper is a long way of the level of completeness, rigor, clarity, and polish that is required to seriously consider it for publication.  In short, the work is still at a relatively early stage and a lot more would need to be done for it to attain various minimum standards for acceptance.  A non-exhaustive list of specific examples of its shortfalls are given below.1. The paper is over a page and a half under length, despite wasting large amounts of space (e.g. figures 3 and 4 should be two lines on the same plot)2. The experimental evaluation is woefully inadequate.  The only quantitative assessment is to compare to a single different approach on a single toy dataset, and even then the metric being used is the one the new method uses to train for making it somewhat meaningless.3. The introduction is completely generic and says nothing about the method itself, just providing a (not especially compelling) motivation for disentanglement in general.  In fact, the motivation of the introduction is somewhat at odds with the work -- correctly talking about the need for hierarchical representations which the approach actually actively discourages.4. There are insufficient details on the algorithm itself in terms of the approximations that are made to estimate the synergistic mutual information.  These are mostly glossed over with only a very short explanation in the paragraph after equation 15.  Yes there are algorithm blocks, but these are pretty incomprehensible and lack accompanying text.  In particular, I cannot understand what A_w is supposed to be.  This is very important as I suspect the behavior of the approximation is very different to the true target.  Similarly, it would be good to provide more insight into the desired target (i.e. Eq 15).  For example, I suspect that it will encourage a mismatch between the aggregate posterior and prior by encouraging higher entropy on the former, in turn causing samples from the generative model to provide a poor match to the data.5. The repeated claims of the approach and results being "state-of-the-art" are cringe-worthy bordering on amusing.  Writing like this serves no purpose even when it justified, and it certainly is not here.6. There are a lot of typos throughout and the production values are rather poor.  For example, the algorithm blocks which are extremely messy to the point where they are difficult to follow, citep/citet mistakes occur almost every other citation, there is a sign error in Equation 16.This is a piece of work in an exciting research area that,  with substantial extra work, could potentially result in a decent paper due to fact that the core idea is simple and original.  However, it is a long way short of this in its current state.  Along with addressing the specific issues above and improving the clarity of the work more generally, one thing in particular that would need to address in a resubmission is a more careful motivation for the method (ideally in the form of a proper introduction).  Though I appreciate this is a somewhat subjective opinion, for me, penalizing the synergistic information is probably actually a bad thing to do when taking a more long-term view on disentanglement.  Forcing simplistic representations where no information is conveyed through the composition of latents beyond that they provide in isolation is all well and good for highly artificial and simplistic datasets like dsprites, but is clearly not a generalizable approach for larger datasets where no such simplistic representation exists.  As you say in the first line of your own introduction, hierarchy and composition are key parts of learning effective and interpretable representations and this is exactly what you are discouraging.  A lot of the issue here is one of the disentanglement literature at large rather than this paper (though I do find it to be a particularly egregious offender) and it is fine to have different opinions.  However, it is necessary to at least make a sensible case for why your approach is actually useful.  Namely, is there actually any real applications where such a simplistic disentanglement is actually useful?  Is there are anyway the current works helps in the longer vision of achieving interpretable representations?  When and why is the synergistic information a better regularizer than, for example, the total correlation?  The experiments you have do not make any inroads to answering these questions and there are no written arguments of note to address them.  I am not trying to argue here that there isn't a good case to be made for the suggested approach in the context of these questions (though I am suspicious), just that if the work is going to have any lasting impact on the community then it needs to at least consider them. GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications. Unfortunately, GANs often show unstable behaviour during the training phase. The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication. Summary: The manuscript introduces a dataset filtering technique for the purpose of speeding up training of machine learning models.The technique filters the training set, yielding a subset of examples that are as diverse as possible, according to an autoencoder embedding of the input space. First, one trains a deep autoencoder, whose code layer is used as embedding of the input space. Then, for each element of the embedding the top k training samples are selected which activate that element. This reduced training set is then used for rapid training of the model in the first optimization stage, followed by slower fine-tuning on the complete data set. The experimental section presents a comparison of accuracies after training a simple CNN on CIFAR10 with and without the proposed data filtering under several constraints.Strengths: The proposed technique addresses the important problem of long training times. The description is very clear and detailed.Weakness:My main criticism of this manuscript is that the experimentation is not nearly sufficient to support the central claim that dataset filtering via embeddings, as described in this manuscript, is a general technique that any tasks [...] could in principle benefit from [...].The evidence from the presented experiment is rather weak, as only one architecture and one dataset is selected. Furthermore, there are quite a few confounding factors that I dont think are compensated by averaging the performances of four training runs. A few recommendations on how to improve the experimental section:- How are hyperparameters selected? For a fair comparison, separate hyperparameter searches should be performed for training with the full training set and with the filtered set. Simple hyperparameters can influence the performance strongly for a given dataset.- It requires extensive experimentation to show the techniques merits as a generally applicable technique that is not bound to certain types of architectures, for instance trying different types of architectures and datasets. Of course the technique can be applied to most architectures and datasets. However, the question is whether it often helps, not whether it is technically possible. Does it for example improve convergence speed or performance in a state-of-the-art network trained on ImageNet? - The heavy use of data augmentation is a confounding factor which adds randomness that is not likely to be compensated by averaging a few training runs. Maybe you could present performances without augmentation.- You mention momentum is used. For reproducibility, it would be good to state the coefficient used.- Testing is performed on checkpoints with some form of weighted averaging of final weights. Could you describe the steps in detail for better reproducibility?- Is the result stable over multiple autoencoder trainings?- It would be interesting to see the performance before the finetuning stage!I feel the discussion section could benefit from a few thoughts on the limitations of this approach. For instance, the method might not be the best choice for highly imbalanced classification datasets. Literature on dataset resampling for such scenarios might be worth mentioning in the related work section. Also, the autoencoders embeddings are trained to reconstruct the whole image, an objective that gives more importance to patterns that occupy a larger portion of the image. If the downstream task needs attention to detail (e.g. counting of small objects, segmentation in remote sensing or medical imaging, street-number or road-sign detection), the filtering method might also not be much better than random subsampling.The related work section could also be improved. I see only one work on data set optimization. Ive seen work using a reducedMNIST dataset, which is probably created by random subsampling, but still more relevant than many of the aspects of embeddings cited in this section (the paragraph about arithmetic operations for instance). Katharopoulos and Fleuret (2018) seems like highly relevant recent work, which should be cited and contrasted against. The evaluation in that work seems very thorough in comparison.A general recommendation on writing: Try to limit the content to relevant details. For example, a description of hardware specifics (support for NVLink, which is not used) or stating the well-established speed-up when using GPUs for CNNs are not relevant.Figure 6 could be improved by marking on the time axis, when the fine-tuning sets in.To summarize my feedback, I like most of the presentation and it is good to see effort towards reducing training times by selecting good training samples, but I think the manuscript requires significant effort to justify acceptance. This paper presents the idea of splitting the training process into two phases: fast training on a subset of the original dataset and finetining on the full dataset. To find a good subset of the training dataset it is proposed to train an autoencoder and use its embeddings to choose examples that have large values of the embedding features. The experiments show that on CIFAR-10 dataset this may speed up the convergence.In general, I like the idea of being smart about which data and in which order to feed to the learner.Nonetheless, I disagree with several premises of this paper. The paper claims that by making the dataset smaller one can speed up the training by the means of fitting the dataset into the accelerator memory and thus avoiding slow memory copies from CPU to accelerator memory. However, modern deep learning data pipelines are built in a way that has virtually zero overhead, since the data is loaded from disk and preprocessed on CPU and then copied on the accelerator asynchronously (i.e. the GPU doesnt have to wait for the data, it can process the current batch and at the same time load the next one). Moreover, moving the data to GPU will introduce additional overheads in the case of random data augmentation, since this additional work would have to be done by GPU (while current deep learning frameworks asynchronously do this work on CPU). And finally, the authors claim that most modern datasets can fit to the accelerator memory if reduced 10x, but in my experience the network and its activations (which are stored during training) occupies most of the GPU memory even on high end accelerators, not leaving enough space to store a large dataset even after data reduction.The paper cites Dunner  et al. (2017) as related work that focus on the similar problem: how to find a subset of the dataset to fit it into the GPU memory. However, I would argue that their setup is very different because they are using linear models (such as SVM): their learning steps are very fast compared to CNNs (which makes the memory bandwidth much slower in comparison), they dont have to store activations of the layers (which allows them to fit much more training samples into GPU memory), and they dont use data augmentation.Also, I dont think that the experimental comparison provides a strong enough evidence supporting the benefits of the proposed scheme. First, the experiments are only done on a small scale dataset (CIFAR-10), which is OK in general, but questionable when the proposed method explicitly targets big data regime and making the training faster. Second, the only baseline considered is choosing subset of data randomly. Third, the optimization method is plain SGD with momentum, while when presenting techniques for faster convergence it would make sense to compare on at least several standard optimization algorithms (e.g. Adam). Finally, the presented results are weak: in Fig 4 any improvements over random baseline are noticeable only after degrading the performance of the network by a large margin (to less than 67% accuracy on CIFAR-10);Fig 5 looks like it has an error: training on the full dataset performs on the level of random guess after some training, which contradicts the fact that the same network converged to something reasonable in Fig. 6. Also, I believe that the training on the full dataset is strictly better than training on a random subset of data for a few epochs and then finetuning on the full dataset (with the same time budget). The latter sees the same number of updates but with less data, which should only decrease the test performance. If its correct, the results in Fig 5 for the full training should look similar (or better) than the results in Fig 4 for the random subset baseline, but its very far from being the case.In Fig 6 Im not sure what is being compared. Is a train or test loss? If its train, then its not a fair comparisons, since the two network are optimizing different train losses. Im also very surprised not to see the moment of training mode transition on the plot (i.e. the moment when the model switched from restricted dataset to the full one), the lack of it can indicate an implementation error.And finally, I would like to see the text being improved. Right now the language is confusing, for example: this technique is shown to be effective (what does it mean effective and compared to what?), Unfortunately, while these techniques may be viable for smaller networks or datasets, large datasets have shown that they do not scale well. (who have shown that the techniques doesnt work on large dataset?), the testing network is initialized using a weighted average of the final weights learned during training (what does it mean?), Qualitatively, the trained autoencoder succeeded in learning an adequate embedding. (what does it mean?), etc.Also, there is a typo in formulas 1, 2, and 3: it probably should sum up to n-1.And I didnt get what formula 4 means, what is union of (for all i in n)?This bit I also didnt get: This simple loss function, in essence, forces the network to learn to extract the key features from the input, so that it can reproduce it using said features only. If desired, one could elect to use a more sophisticated loss, such as the Wasserstein distance metric (Gulrajani et al., 2017; Arjovsky et al., 2017), that takes more into account than raw pixel values.. How can you substitute L2 loss in an autoencoder with Wasserstein metric (which is a metric between probability distributions, not images)?There is also some missing related work, e.g. the idea mentioned in conclusion on augmenting the dataset in the latent space is presented in DeVries et al. Dataset augmentation in feature space.It would be interesting to connect this work with importance sampling off-policy RL (see e.g. Prioritized Experience Replay) and look into sampling dataset points proportional to some importance probability with importance sampling correction.On the positive side, I really enjoyed the look of the figures and diagrams. SummaryThis paper propose to learning a dynamics model with future prediction in video and using it for reinforcement learning.The dynamics models is a variants of convolution LSTM and it is trained mean squared error in the future frame.The way of using dynamics model for reinforcement learning is similar to Weber et al., 2017, where K step prediction of the dynamics model is uses as an augmented input of the policy.StrengthTraining dynamic model to understand physic and using it for reinforcement learning is an interesting problem that worth exploring. This paper tackles this problem and demonstrated experimental setting based on physics games. WeaknessThe part for understanding dynamics model is very close to existing convolutional LSTM model (Xingjian et al., 2015), which is a popular baseline in video modelling community and how pretrained dynamics model is used for reinforcement learning is similar to Weber et al., 2017, but this paper does not provide comparison to any of these two baseline. Since the difference with these existing method is subtle, clear comparison with these method and difference in characteristic is essential to show the novelty of the paper. Overall commentThis paper address the interesting problem of understanding dynamics for solving reinforcement learning, but the suggested method is not novel and comparison with existing close methods are not performed. This is a paper with scattered potentially interesting ideas. But the execution is limited and the writing poor with critical details lacking.  A major limitation of the paper is that it is not clear what contribution it makes. Some of the analyses are indeed interesting but 1) these analyses are mostly descriptive and 2) they are limited to one particular (outdated) architecture. How would batch norm or residual connections or any of the developments that have happened since AlexNet affect these results?As a side note, the references/comparisons between AlexNet and recurrent nets (see abstract, etc) are misleading. This is based on the claim that Bowers et al (2014) qualitatively different results but this is for entirely different domains (words). Indeed what could have made potentially the work more relevant would have been to show some kind of benchmarking between AlexNet and alternative architectures (possibly RNNs). As such the current study does not contribute much except for comparing different semi-arbitrary measures of selectivity for one specific (outdated) network architecture trained on a particular problem (ILSVRC).****Minor points:The study is limited to correctly classified images as stated on page 3. This seems like a major confound in a study aimed at understanding the visual representations learned. It seems to me that the conclusions of the paper could be heavily biased because of this (when computing any measure based on inter and intraclass responses).In general, this is a relatively poorly written paper which would be hard to reproduce. For instance, the image generation for activating units (assuming it is novel) could be interesting but it is not even described with sufficient details so as to reproduce the results. This paper suggests a new architecture for representing and predicting properties of molecules ("Pixel Chem"). The authors report that this architecture produces better results than previous methods on one of the QM9 dataset properties (U0) but performs worse than many of the other reported methods on other quantities. Overall, this paper is extremely unclear, contains typos in most sentences, and provides insufficient justification for the both a) the design choices made and b) the claims made wrt the performance of the model. Additionally, at least one critical baseline [1] is missing which outperforms the proposed model. In order to improve this paper, I would suggest the authors do the following: 1) Heavily edit and rewrite the paper focusing on clarity of communication.2) Provide justifications for why design choices were made. For example, the authors state that they include a charge and energy matrix to "mix all the things up." Why does it make sense to combine these factors in this way? 3) Provide a more comprehensive evaluation of the model, showing improved performance across more than one target and including appropriate baselines, including [1]. [1] Gilmer, Justin, et al. "Neural message passing for quantum chemistry." arXiv preprint arXiv:1704.01212 (2017). This paper proposes a neural network architecture PCnet for the prediction of intensive and extensive chemical properties of molecules and materials. The authors claim that the use of prior chemical knowledge such as Mulliken electronegativity, bond strength and orbital information improves prediction accuracy. While the idea of incorporating chemical domain knowledge in the interactions of an atomistic neural network is interesting in principle, this paper has severe issues ranging from presentation over the proposed approach to the results.First and foremost, I would like to point out that the results in Table 1 are cherry-picked since the authors fail to cite neural network architectures that outperform their approach, e.g. for U0: 0.45 kcal/mol [MPNN, Gilmer et al 2017, ICML], 0.31 kcal/mol [SchNet, Schütt et al, NIPS 30, 2017], 0.26 kcal/mol [HIP-NN, Lubbers et al., JCP 148, 2018]. This is especially apparent since Figure 4 is obviously inspired by Fig. 1 in [SchNet, Schütt et al, JCP 148, 2018]. The authors use a variety of heuristics and approximations such as a "charge transfer ability", bond strength, exponential decay of distances and overlaps of atomic orbitals which are multiplied "to mix all things up", to arrive at the PixelChem representation which is then fed into an atomistic neural network (PCnet). Combining these chemical features in such a way is neither well-motivated, nor does it lead to an improvement in accuracy compared to state-of-the-art networks.Even for the intensive properties (gap, HOMO, LUMO), where PCnet is supposed to have an advantage due to its use of orbital information, MPNN, SchNet and even GC and GG-NN in Table 1 outperform the proposed approach. Parameterization of the chemical features and training the PCnet end-to-end might have improved results and seems like a missed opportunity.Further issues:- The manuscript is riddled with typos, grammatical errors as well as confusing sentences.- The authors claim that PCnet is applicable to periodic structures, however, this is never demonstrated. Beyond that their definition of periodic PixelChem does only include adjacent cells, while for a unique representation more cells might be required.- The "benefits" listed in Section 2.3 compare selectively to previous work. E.g., invariances, uniqueness, asymmetric interactions are also fullfiled by the neural networks listed above. A comparison of the PixelChem representation to the Coulomb matrix is not sufficient here.- The PCnet architecture uses PReLU nonlinearities. While this is fine for equilibrium predictions, for other configurations this prohibits the prediction of a smooth PES.Overall, I believe that it is important to incorporate chemical knowledge into neural networks. However, neither the approach nor the results convince me that this has been achieved here. The idea of analyzing embedding spaces in a non-parametric (example-based) way is well-motivated. However, the main technical contribution of this paper is otherwise not clear - the methodology section covers a very broad set of techniques but doesn't provide a clear picture of what is novel; furthermore, it makes a strong assumption about linear structure in the embedding space that may not hold. (It's worth noting that t-SNE does not make this assumption.)The visualization strategies presented don't appear to be particularly novel. In particular, projection onto a linear subspace defined by particular attributes was done in the original word2vec and GloVe papers for the analogy task. There's also a lot of other literature on interpreting deeper models using locally-linear predictors, see for example LIME (Ribeiro et al. 2016) or TCAV (Kim at el. 2018).Evaluations are exclusively qualitative, which is disappointing because there are quantitative ways of evaluating a projection - for example, how well do the reduced dimensions predict a particular attribute relative to the entire vector. Five-axis polar plots can pack in more information than a 2-dimensional plot in some ways, but quickly become cluttered. The authors might consider using heatmaps or bar plots, as are commonly used elsewhere in the literature (e.g. for visualizing activation maps or attention vectors).User study is hard to evaluate. What were the specific formulae used in the comparison? Did subjects just see a list of nearest-neighbors, or did they see the 2D projection? If the latter, I'd imagine it would be easy to tell which was the t-SNE plot, since most researchers are familiar with how these look. The authors propose a momentum based approach for batch normalization and provide an asymptotic convergence analysis of the objective in terms of the first order criterion. To my understanding, the main effort in the analysis is to show that the sequences of interest are Cauchy. Some numerical results are reported to demonstrate that the proposed variant of BN slightly outperforms BN with careful adjustment of some hyper parameter. The proposed approach is incremental, and the theoretical results are somewhat weak.The most important issue is that the zero gradient of the objective function does not imply that it attains an (even local) minimum point. As for the 2-layer case, the objective function can be nonconvex in terms of the weight parameters with stationary points being saddle points, it is crucial to understand whether an iterative algorithm (GD or SGD) converges to a minimum point rather a saddle point. Thus, the first order criterion alone is not enough for this purpose, which is why extensive studies are carried out for nonconvex optimization (e.g., using both first and second order criteria for convergence [1]) and considering the specific structure of neural nets [2].The analysis is somewhat confusing. The authors assume that the objective of interest have stationary points (\theta*, \lambda*), and also show that the sequence of the norm of gradient convergence to zero, with the \lambda^(m) converges to \bar{\lambda}. What is the relationship between \lambda* and \bar{\lambda}? It is not clear whether they are the same point or not. Moreover, since there is no converge of the parameter, it is not clear what the convergence for the \lambda imply here, as we also discussed above that the zero gradient itself may mean nothing.In addition, the writing need improvements. Some statements are not accurate. For example, on page 3, after equation (2), the authors state The deep network &, though they mentioned it is for a 2-layer net. Also, more explicit explanation and definitions are necessary for notations. For example, it is clearer to define explicitly the parameters with \bar (e.g., for \lambda) as the limit point. [1] Ge et al. Escaping from saddle pointsonline stochastic gradient for tensor decomposition.[2] Li and Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation. Beyond Games: Bringing Exploration to Robots in Real-world===========================================================This paper tackles the laudable goal of making an algorithm for efficient exploration in "real-world" RL.To do this, they augment the "curiosity" algorithm of Pathak et al with a differentiable approximation to the reward prediction model.They motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE.There are several things to like about this paper:- The problem of making "real-world" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning.- The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines.- The authors clearly make an effort to survey a wide variety of recent papers in the fieldHowever, there are several important places where this paper falls down:- In a paper that posits a new, groundbreaking, real-world application of "exploration" there is remarkably little discussion of the key issues of "efficient exploration". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration.  + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds?  + Of course, this is not a paper designed for "tabular MDPs", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches?- There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond "sample-efficient exploration formulation") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled.- A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me:  + "... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ..."  + "Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient"I would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not!- Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper.Overall, it is clear that this is an interesting area to do work in.The goal of making a practical algorithm for real-world exploration tasks is exciting.However, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR. Summary:This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL.The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot.Comments:The paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details.The experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is "multi-step learning" in Table 2?).Without these details, the results will be difficult to validate and reproduce independently.The approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space?The authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak.The paper contains many factual errors and unsupported claims. For example:- "the field of RL was born out of need to make our robots learn"- "none of the recent advances have translated to success in the field of robotics" (see e.g. the proceedings of CoRL 2017 and 2018)- "Building a good model will require enormous number of interactions" (see e.g. PILCO)- "[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot" (PILCO and many others)- describing Pathak et al. curiosity as a "gaussian density model" in eq1; it's a deterministic forward model- in sec3, "regress r^i_t to learn value estimates", this is probably meant to be the discounted sum of rewards- also sec3, "[REINFORCE] gives no signal as to what action to take"; the signal has high variance but it works (see all policy gradient work)These errors can be easily corrected. However, the contribution of the paper is based on a more serious error:- sec3.1, "If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high."This is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al.But in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.As a result, the gradient obtained does not actually move the policy toward higher prediction errors.To understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors.Instead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}.This is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing.It's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by.I would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion.Finally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.).Conclusion:The paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance. This paper proposes a new loss based on the Recall metric to deal with imbalance problems in several visual recognition tasks (i.e., classification and segmentation). Authors show, in several public benchmarks, that the proposed recall loss outperforms other losses in the tasks of classification and segmentation. Please find below my comments.Strengths.-The paper is easy to follow.-Proposing new losses to deal with imbalance problems is an interesting venue.-Results show that the proposed loss outperforms the other losses compared in the paper.Weaknesses.-The methodological contribution is marginal/incremental. The proposed loss is a straightforward extension of prior losses. -More fundamentally, I wonder about the motivation of this loss wrt to the use of other metrics as losses. For example, the F1-score of a class is given by the harmonic mean of precision and recall, combining both. Why a recall loss is better than an F1-based loss? For example, the paper in [1] (not included in related work), propose a novel balance function balancing these two terms in the context of highly unbalanced segmentation. Related to this, if model predictions obtain low recall and low precision for a given class, that class is poorly handled by the model, which again motivates the use of something beyond just the recall as objective function.-There exist missing literature in the losses to deal with imbalance. Particularly, recent works (e.g., [2]) have introduced boundary-based losses for segmentation, which deal with the problems of region-based losses (e.g., large gradients or issues to weight classes bases on their frequency). Authors should discuss these papers, as well as include them in the results.-I also believe the experiments are poorly conducted. Authors resort to mean-IoU and mean-Acc for segmentation, and accuracy for classification. Nevertheless, F1-score is a popular choice on imbalance problems. Authors should reconsider other metrics to better show the impact of the proposed loss. Furthermore, in both classification and segmentation, the results are averaged over all the classes, when comparing to prior work (not just CE and CBCE). This makes that one does not know which is the impact of the proposed loss in the under-represented classes. Is the improvement due to the model improves the performance over all the classes? Or is it actually coming because the model better handles minority classes? With the current results, it is not possible to evaluate the contribution of this loss in imbalance problems. -Related to my previous comment, authors only show the per-class mIOU and Acc on the segmentation task, and comparing to CE and CB-CE (Fig. 2a and 2b). Looking at the Fig 2a, which is the metric typically employed to evaluate the segmentation task on these datasets, we can observe that the proposed loss only improves one class over the standard CE (i.e., bike), falling behind the standard CE in all the rest. These results, together with the values reported in Tables 1 and 2 (where CE outperforms the proposed loss in 1 case, and in the other 3 cases the improvement over CE is marginal), make me wonder about the usability of the proposed loss. [1] Hashemi et al. Assymmetric similarity loss function to balance precision and recall in highly unbalanced deep medical image segmentation IEEE 2019.[2] Kervadec et al. Boundary loss for highly unbalanced segmentation. MIDL 2019. The authors propose to upper bound the generalization gap via three quantities, namely robustness gap, rationality gap and memorization gap, shows that the memorization gap can be bounded via standard learning theory arguments, and empirically show that all of the three terms are small. The authors also argue that if the rationality gap is large, then the performance can be improved.1. First of all, I think this paper is highly over-claimed. I dont see how the proposed methods provably indicates the generalization. In fact, there is no theoretical conclusion on bounding the rationality gap and little theoretical discussion on robustness gap. Instead, the authors only show the empirical estimation on the robustness gap and  rationality gap. I would like to say, such inaccurate claim makes me feel uncomfortable.2. I would like to argue that, we cannot know the exact rationality gap, as we dont have the data distribution at any time, thus we need a generalization bound to describe the performance of algorithm on unseen data. How do the authors deal with the rationality gap? I dont feel empirical estimation on test set is an acceptable choice, as test set is only a batch of sample of real data distribution. The bound proposed by authors is not a generalization bound, thus it is meaningless to talk about the bound is vacuous or not.3. Moreover, as we dont know the exact rationality gap, the claim by Theorem 3.1 is also not meaningful. In other words, even if we find the rationality gap is large when evaluating on test data, what we really do is tuning the model using the test data, not improving the performance of the model on data distribution.4. The authors argue in the abstract that the bound is independent of the complexity of the representation. However, several properties of the representation, e.g. the dimension, will definitely influence the generalization bound. I dont feel this argument well-supported.Overall, the decomposition itself may motivate new idea on improving the current algorithms. However, theoretically, I dont think this paper is a rigorous paper considering the generalization bound. If the authors want to argue the decomposition have some insight on improving algorithm, the authors should focus more on the intuition, algorithm design and empirical justification. If the authors want to argue the decomposition indicate tight generalization bound, then the authors should give rigorous proof on the bound of all three terms and calculate the bound based on the theoretical prediction instead of empirical simulation. There can be some misunderstanding on some of the points in the paper, but overall, with the current presentation, I think this paper is not ready for acceptance. This paper describes an approach for learning a representation U(X,Y), V(X,Y) of data (X,Y) such that U and V are causally meaningful, and U causes V. The approach relies on observing data from two domains, P and Q, where P(V| U) = Q(V |U) (reflecting the causal structure). The approach is a modification of Bengio et al. The main new idea is that the objective function can be tweaked by replacing a KL divergence term with a term involving domain-shift induced generalization errors.The paper covers an interesting subject, and the idea to directly use domain generalization error learn causal structure is exciting. However, the paper is clearly not yet baked. The writing is generally poor, and the key ideas have not been formalized. The main ideas of the paper are unclear, as are the validity of the core insights. In particular, there is a fundamental confusion between estimators and estimands.  The paper will require extensive revision and formalization before it's ready for public consumption.Below I including some free form thoughts I had below. These give a flavor of my issues with the paper, and will hopefully provide some direction for the authors. However, I stress that these issues are only examples. The paper requires extensive revision.1. the phonemes/acoustics example just shows that structured learning may be beneficial, it doesn't rely on causality2. the explanation of equation 1 is unclear (or, possibly, wrong). Presumably, the actual aim is to compare the likelihoods of two distinct models corresponding to A->B and B<-A. The paper argues that the higher complexity model will have lower likelihood. But this is false in general; a very flexible model will simply memorize the training data.3. in general, the paper suffers from a confusion in the notation between population parameters and finite-sample estimators. The notation generally suggests the former, but the prose, appealing to sample-complexity handwaving, suggests the later. 4. Is proposition 1 meant to be a theorem? The text doesn't reference any estimator, much less an 'unbiased' one.(update: I read the appendix, and the intended statement is simply, "If P(A|B)=Q(A|B), but P(B|A)!=Q(B|A) then 0 = KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A))"5. In section 2.3, \script{L} has changed from denoting log-likelihood to denoting risk (incorrectly called loss in the prose) This paper proposes to use a mixture of distributions for hazard modeling. They use the standard censored loss and binning-based discretization for handling irregularities in the time series. The evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead. If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead. Moreover, the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling. The authors also dont report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling.A major baseline for mixture modeling is always non-parametric modeling. In this case, given that there are existing works on deep Cox hazard modeling, the authors need to show the advantages of their proposed mixture modeling against deep Cox models.Overall, the methodology in this paper is quite limited and the evaluation is non-standard. Thus, I vote for rejection of the paper.[1] Ranganath, Rajesh, et al. "Deep Survival Analysis." Machine Learning for Healthcare Conference. 2016.[2] Katzman, Jared L., et al. "DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network." BMC medical research methodology 18.1 (2018): 24.[3] Lee, Changhee, et al. "Deephit: A deep learning approach to survival analysis with competing risks." AAAI, 2018. The paper "Neural Distribution Learning for generalized time-to-event prediction" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data.  First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like "Modeling the intensity function of point process via recurrent neural networks" should be considered in the experiments, though they do not explicitely model censoring but  with slight adapations should be able to work well of experimental data. This paper proposes a method for learning how to explore environments. The paper mentions that the exploration task that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.&lt;&lt;Pros&gt;&gt;-The paper is well-written (except for a few typos).-The overall approach is simple and does not have much complications. -The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.  &lt;&lt;Cons&gt;&gt;**The technical novelty is not significant**-This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. **The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. **-Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters?  Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? -The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup.-What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map.-The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the learning for Navigation section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both exploration strategy and imitation learning : Pathak, Deepak, et al. "Zero-shot visual imitation."&nbsp;International Conference on Learning Representations. 2018. is missed in navigation comparison. The aforementioned paper is also missed in the references.  -Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning.-The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. ** Technical details are missing or not explained clearly**- Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? -The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? -Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isnt it only 2.5D (information obtained by depth sensor) used in the proposed method?**Presentation can be improved**-The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. - Interpretation of green vs white vs black in the reconstructed maps is left to the reader in Fig. 1. - Last line in page 5: there is no need for reiteration. It is already clear.**Missing references**-Since the paper is about learning to explore, discussion about exploration techniques in RL is recommended to be added in at least the related work section. -A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. This paper discusses the addition of a regularizer to a standard sparse coding/dictionary learning algorithm to encourage the atoms to be used with uniform frequency.    I do not think this work should be accepted to the conference for the following reasons:1: The authors show no benefit of this scheme except perhaps faster convergence.  If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.  SPAMS (http://spams-devel.gforge.inria.fr/) can train a model on image patches as the authors do here in a few tens of seconds on a modern computer.  On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.  It is not even clear that the final compression of the baselines would not be better.  Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.   The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset.  Even without the "train to convergence" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true. This paper suggests an exploration driven by uncertainty in the reward space.In this way, the agent receives a bonus based on its squared error in reward estimation.The resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.There are several things to like about this paper:- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.- The algorithm does appear to outperform the basic DQN baseline on their experiments.Unfortunately, there are several places where the paper falls down:- The authors wrongly present prior work on efficient exploration as "exploration in state space" ... rather than "reward space"... in fact prior work on provably-efficient exploration is dominated by "exploration in value space"... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to "counts" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853- So this algorithm falls into a pretty common trope of algorithms of "exploration bonus" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around "randomized value functions"- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.Overall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.It could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests "exploration by uncertainty in value space"... e.g. "deep exploration via randomized value functions" The paper proposes a modification of the saliency map/gradient approach to explain neural networks.# Method summaryThe approach is as follows:For each layer, the gradient w.r.t. it's input layer is computed for multiple images concurrently.Then for conv layers, the activations are averaged per feature map (over space).As a result, for both fully connected and convolutional layers there is a 3D feature map.From these at most b positive outliers are selected to be propagated further. What is a bit strange is that in the results section, guided backpropagation is mentioned and clearly used in the visualizations but not mentioned in the technical description.# RecommendationThe current evaluation is definitely not sufficient for acceptance. The evaluation is done in a purely qualitative matter (even in section 4.1 Quantitive justification of outliers as relevant neurons). The results appear to be interesting but there is no effort done to confirm that the neurons considered to be relevant are truly relevant. On top of that, it is also evaluated only on a single network and no theoretical justification is provided.# Discussion w.r.t. the evaluationTo improve section 4.1,  the authors could for example drop out the most important neurons and re-evaluate the model to see whether the selected neurons have a larger impact than randomly selected neurons. Since the network is trained with dropout, it should be somewhat robust to this. This would not be a definitive test, but it would be more convincing than the current evaluation. Furthermore high values do not imply importance. It might be possible that I misunderstood the experiment in Figure 2. So please correct me if this is the case in the reasoning below. In figure 2, FC2 is analyzed. This is the second to last layer. So I assume that only the back-propagation from logits (I make this assumption since this is what is done commonly and it is not specified in the paper) to FC2 was used. Since we start at the same output neuron for a single class, all visualisations will use the same weight vector that is propagated back. The only difference between images comes from which Relu's were active but the amount if variability is probably small since the images were selected to be classified with high confidence. Hence, the outliers originate from a large weight to a specific neuron. The interpretation in the second paragraph of section 4.2.1 is not scientific at all. I looked at the German Shepherd images and there are no teeth visible. But again, this is a claim that can be falsified easily. Compare the results when german Shepherds with teeth visible are used and when they are not. The same holds for the hypothesis of the degree of danger w.r.t. the separation. Finally, there is no proof that the approach works better than using the magnitude of neuron activations themselves, which would be an interesting baseline. Additional remarks---------------------------The following is an odd formulation since it takes a 3D tensor out of a 5D one and mixes these in the explanation:"... the result of equation for is a 5D relevance tensor $\omega^l_{n,i,..} \in R^{H\times W\times K} ....."The quality of the figures is particularly poor. - Figure 1 b did not help me to understand the concept.- Figure 2 The text on the figure is unreadable. - Figure 4a is not readable when printed. Summary:The paper introduces a new approach for interpreting deep neural networks called step-wise sensitivity analysis. The approach is conceptually quite simple and involves some interesting ideas, but I have some serious concerns whether the output produced by this method carries any meaning at all. If the authors were able to refute my concerns detailed below, I would raise my score substantially.Strengths:+ Potentially interesting heuristic to identify groups of feature channels in DNNs that encode image features in a distributed wayWeaknesses:- Using the magnitude of the gradient in intermediate layers of ReLU networks is not indicative of importance- No verification of the method on a simple toy exampleDetails:Main issue: Magnitude of the gradient as a measure of importance.I have trouble with the use of the gradient to identify "outliers," which are deemed important. Comparing the magnitude of activations across features does not make sense in a convnet with ReLUs, because the scale of activations in each feature map is arbitrary and meaningless. Consider a feature map h^l[i,x,y,f] (l=layer, i=images, x/y=pixels, f=feature channels), convolution kernels w^l[x,y,k,f] (k=input channels, f=output channels) and biases b^l[f]:h^l[i,:,:,f] = ReLU(b^l[f] + \sum_k h^(l-1)[i,:,:,k] * w^l[:,:,k,f])Assume, without loss of generality, the feature map h^l[:,:,:,f] has mean zero and unit variance, computed over all images (i) in the training set and all pixels (x,y). Let's multiply all "incoming" convolution kernels w^l[:,:,:,f] and biases b^l[f] by 10. As a result, this feature map will now have a variance of 100 (over images and pixels). Additionally, let's divide all "outgoing" kernels w^(l+1)[:,:,f,:] by 10.Simple linear algebra suffices to verify that the next layer's features h^(l+1) -- and therefore the entire network output -- are unaffected by this manipulation. However, the gradient of all units in this feature map is 10x as high as that of the original network. Of course the gradient in layer l-1 will be unaltered once we backpropagate through w^l, but because of the authors' selection of "outlier" units, their graph will look vastly different.In other words, it is unclear to me how any method based on gradients should be able to meaningfully assign "importance" to entire feature maps. One could potentially start with the assumption of equal importance when averaged over all images in the dataset and normalize the activations. For instance, ReLU networks with batch norm and without post-normalization scaling would satisfy this assumption. However, for VGG-16 studied here, this is not the case.On a related note, the authors' observation in Fig. 4b that the same features are both strongly positive and strongly negative outliers for the same class suggests that this feature simply has a higher variance than the others in the same layer and is therefore picked most of the time. Similarly, the fact that vastly different classes such as shark and German Sheppard share the same subgraphs speaks to the same potential issue.Secondary issue: No verification of the method on simple, understandable toy example.As shown by Kindermans et al. [1], gradient-based attribution methods fail to produce the correct result even for the simplest possible linear examples. The authors do not seem to be aware of this work (at least it's not cited), so I suggest they have a look and discuss the implications w.r.t. their own work. In addition, I think the authors should demonstrate on a simple, controlled (e.g. linear) toy example that their method works as expected before jumping to a deep neural network. I suppose the issue discussed above will also surface in purely linear multi-layer networks, where the intermediate layers (and their gradients) can be rescaled arbitrarily without changing the network's function.References:[1] Kindermans P-J, Schütt KT, Alber M, Müller K-R, Erhan D, Kim B, Dähne S (2017) Learning how to explain neural networks: PatternNet and PatternAttribution. arXiv:170505598. Available at: http://arxiv.org/abs/1705.05598 This paper proposes a new approach to understand the theory of RELU neural networks. Using a teacher-student setting, this paper studies the batch normalization and the disentangled representations of neural networks. However, the definitions of some of the concepts and notation are not sufficiently clear. In addition, the assumptions that the main results of this paper depend on do not have clear intuitions.Detailed comments:1. It seems that this paper over claims its contribution. It is not clear why the "teacher-student setting" can be called a theoretical framework, even the definitions of the teacher and the student are not clear. It seems that the new framework is just a way to compute the relations of the gradients of neurons based on a few assumptions (Theorem 2).2. I found it very hard to follow the notations given in this paper. The main reason is that many of the terms appear without a definition, and the reader has to guess what they stand for. For example, in equation (2), w_{jk} seems to be the weight between nodes j and k, where k is a child of j. But this term is not defined. As another example, all the matrices in Theorem 9 are not defined. They just suddenly appear. In addition, S(f) in (11) is not defined. I would suggest the authors to spend one section to carefully define everything. 3. The theorems all depends on some assumptions that are unclear whether will hold in practice or not. For example, in theorem 2, it is hard to see what kind of data distribution satisfy these three conditions. Although in Theorem 3 the author gave a sufficient condition, we still don't know what kind of $X$ satisfies this. For example, does Gaussian distribution satisfy those? This problem also happens to other theorems. It would be much better to make sure that these assumptions are unrealistic. This paper aims to address the problem of lacking sufficient demonstrations in inverse reinforcement learning (IRL) problems. They propose to take a meta learning approach, in which a set of i.i.d. IRL tasks are provided to the learner and the learner aims to learn a strategy to quickly recover a good reward function for a new task that is assumed to be sampled from the same task distribution. Particularly, they adopt the gradient-based meta learning algorithm, MAML, and the maximal entropy (MaxEnt) IRL framework, and derive the required meta gradient expression for parameter update. The proposed algorithm is evaluated on a synthetic grid-world problem, SpriteWorld. The experimental results suggest the proposed algorithm can learn to mimic the optimal policy under the true reward function that is unknown to the learner. Strengths: 1) The use of meta learning to improve sample efficiency of IRL is a good idea.2) The combination of MAML and MaxEnt IRL is new to my knowledge. 3) Providing the gradient expression is useful, which is the main technical contribution of this paper. (But it needs to be corrected; see below.)4) The paper is well motivated and clearly written "in a high level" (see below). Weakness: 1) The derivation of (5) assumes the problem is tabular, and the State-Visitations-Policy procedure assumes the dynamics/transition of the MDP is known. These two assumption are rather strong and therefore should be made explicitly in the problem definition in Section 3.2)  Equation (8) is WRONG. The direction of the derivation takes is correct, but the final expression is incorrect. This is mostly because of the careless use of notation in derivation on p 15 in the appendix (the last equation), in which the subscript i is missed for the second term. The correct expression of (8) should have a rightmost term in the form  (\partial_\theta r_\theta) D  (\partial_\theta r_\theta)^T, where D is a diagonal matrix that contains \partial_{r_i} (\E_{\tau} [ \mu_\tau])_i and i is in 1,...,|S||A|. 3) Comparison with imitation learning and missing details of the experiments. a) The paper assumes the expert is produced by the MaxEnt model. In the experiments, it is unclear whether this is true or not, as the information about the demonstration and the true reward is not provided. b) While the experimental results suggest the algorithm can recover the similar performance to the optimal policy of the true reward function, whether this observation can generalize outside the current synthetic environment is unclear to me. In imitation learning, it is known that the expert policy is often sub-optimal, and therefore the goal in imitation learning is mostly only to achieve expert-level performance. Given this, the way this paper evaluate the performance is misleading and improper to me, which leads to an overstatement of the benefits of the algorithm. c) It would be interesting to compare the current approach with, e.g., the policy-based supervised learning approach to imitation learning (i.e. behavior cloning). 4) The rigorousness in technicality needs to be improved. While the paper is well structured, the writing at the mathematical level is careless, which leads to ambiguities and mistakes (though one might be able to work out the right formula after going through the details of the entire paper). Below I list a few points.     a) The meta-training set {T_i; i=1,...,N} and the meta-test set {T_j; i=1,...,M} seems to overload the notation. I suppose this is unintentional but it may appear that the two sets share the first T_1,.., T_M tasks, e.g., when N&gt;=M, instead of being disjoint.     b) The set over which the summation is performed in (4) is unclear; alpha in (4) is not defined, though I guess it's a positive step size.    c) On p4, "we can view this problem as aiming to learn a prior over the intentions of human demonstrators" is an overstatement to me. At best, this algorithm learns a prior over rewards for solving maximal entropy IRL, not intention. And the experiment results do not corroborate  the statement about "human" intention.    d) On p4,  "since the space of relevant reward functions is much smaller than the space of all possible rewards denable on the raw observations" needs to be justified. This may not be true in general, e.g., learning the set of relevant functions may require a larger space than learning the reward functions.    e) The authors call \mu_\tau the "state" visitation, but this is rather confusing, as it is the visiting frequency of state and action (which is only made clear late in the appendix).     f) On p5, it writes "... taking a small number of gradient steps on a few demonstrations from given task leads" But the proposed algorithm actually only takes "one" gradient step in training.     g) The convention of derivatives used in the appendix is the transpose of the one used in the main paper.Minor points: 1) typo in (2) 2) p_\phi is not defined, L_{IRL} is not defined, though the definition of both can be guessed.3) T^{tr} seems to be typo in (11)4) A short derivation of (2) in the Appendix would be helpful. This paper presents an extension of Capsule Networks, Siamese Capsule Networks (SCNs), that can be applied to the problem of face verification. Results are reported on the small AT&amp;T dataset and the LFW dataset. I like the direction that this paper is taking. The original Capsules work has been looking at fairly simple and small scale datasets, and the natural next step for this approach is to start addressing harder datasets, LFW being one of them. Also face verification is a natural problem to look at with Capsules.However, I think this paper currently falls short of what I would expect from an ICLR paper. First, the results are not particularly impressive. Indeed, SCN doesn't outperform AlexNet on LFW (the most interesting dataset in the experiments). Also, I'm personally not particularly compelled by the use of the contrastive loss as the measure of performance, as it is sensitive to the scaling of the particular representation f(x) used to compute distances. Looking at accuracy (as in other face verification papers, such as DeepFace) for instance would have been more appropriate, in my opinion. I'm also worried about how hyper-parameters were selected. There are A LOT of hyper-parameters involved (loss function hyper-parameters, architecture hyper-parameters, optimizer hyper-parameters) and not much is said about how these were chosen. It is mentioned that cross validation was used to select some margin hyper-parameters, but results in Table 1 are also cross-validation results, which makes me wonder whether hyper-parameters were tuned on the performance reported in Table 1 (which of course would be biased).The paper is also pretty hard to read. I recognize that there is a lot of complicated literature to cover (e.g. prior work on Capsule Networks has introduced variations on various aspects which are each complicated to describe). But as it currently reads, I can honestly say that I'm not 100% sure what exactly was implemented, i.e. which components of previous Capsule Networks were actually used in the experiments and which weren't. For example, I wasn't able to figure out which routing mechanism was used in this paper. The paper would strongly benefit from more explicitly laying out the exact definition of SCN, perhaps at the expense of enumerating all the other variants of capsules and losses that previous work has used.Finally, regardless of the clarify of the paper, the novelty in extending Capsule Networks to a siamese architecture is arguably pretty incremental. This wouldn't be too much of a problem if the experimental results were strong, but unfortunately it isn't the case.In summary:Pros- New extension of Capsule Networks, tackling a more challenging problem than previous workCons- Novelty is incremental- Paper lacks clarity and is hard to read- Results are underwhelmingFor these reasons, I'm afraid I can't recommend this paper be accepted.Finally, I've noted the following typos:- hinton1985shape =&gt; use proper reference- within in =&gt; within- that represent =&gt; that represents- a Iterated =&gt; an Iterated- is got =&gt; is obtained- followed two =&gt; followed by two- enocded =&gt; encoded- a a pair =&gt; a pair- such that to =&gt; such as to- there 1680 subjects =&gt; there are 1680 subjects- of varied amount =&gt; of the varied amount- are used many =&gt; are used in many- across the paper: lots of in-text references should be in parenthesis The focus of the submission is GANs (generative adversarial network), a recent and popular min-max generative modelling approach. Training GANs is considered to be a challenging problem due to the min-max nature of the task. The authors propose two duality-inspired stopping criteria to monitor the efficiency and convergence of GAN learning.  Though training GAN can have some useful applications, the contribution of the submission is pretty moderate. i) Duality-inspired approaches, embedded also in optimization have already been proposed: see for example 'Xu Chen, Jiang Wang, Hao Ge. Training Generative Adversarial Networks via Primal-Dual Subgradient Methods: A Lagrangian Perspective on GAN. ICLR-2018.'.ii) The notion of generator and discriminator networks with unbounded capacity (which is an assumption in 'Proposition 1') lacks formal definition. I looked up the cited Goodfellow et al. (2014) work; it similarly does not define the concept. Based on the informal definition it is not clear whether they exist or are computationally tractable. Minor comments:-MMD is a specific instance of integral probability metrics when in the latter the function space is chosen to be the unit ball of a reproducing kernel Hilbert space; they are not synonyms.-mixed Nash equilibrium: E_{v\sim D_1} should be E_{v\sim D_2}.-It might be better to call Table 1 as Figure 1.-References: abbreviations and names should be capitalized (e.g., gan, mnist, wasserstein, nash, cifar). Lucic et al. (2017) has been accepted to NIPS-2018. This paper proposes two gated deep learning architectures for sensor fusion. They are all based on the previous work Naman Patel et al's modality fusion with CNNs for UGV autonomous driving in indoor environments (IROS). By having the grouped features, the author demonstrated improved performance, especially in the presence of random sensor noise and failures.#Organization/Style:The paper is well written, organized, and clear on most points. A few minor points:1) The total length of the paper exceeds 8 pages. Some figures and tables should be adjusted to have it fit into 8 pages.2) The literature review is limited.3) There are clearly some misspellings. For example, the "netgated" is often written as "negated".#Technical Accuracy:The two architecture that the author proposes all based on the grouped features, which to my point of view, is a very important and necessary part of the new model. However, the author failed to rigorously prove or clearly demonstrated that why this is effective to our new model.  Moreover, how to make groups or how many groups are needed are not clearly specified. The experiments used only two completely different datasets, none of them are related to the previous sensor fusion method they are trying to compete. I'm afraid this method cannot generalize to a common case.In addition, if we look at Table 4 and Table 5, we can find the first Group-level Fusion Weight actually increases, which seems contradictory to the result shown in Table 6.#Adequacy of Citations: Poor coverage of literature in sensor fusion. There are less than 10 references are related to sensor fusion.Overall, it is not an ICLR standard paper. This paper propose a hierarchical Bayesian model to cluster sparse sequences data. The observations are modeled as Poisson distributions, whose rate parameter \lambda_i is written as the summation of \lambda_{ik}, a Gamma distribution with rate equal to the mixture proportion \alpha_{ik}. The model is implemented in Pystan. Experimental results on a real-world user visit dataset were presented.The format of this paper, including the listing in the introduction section, the long url in section 2.3, and the model specification in section 3.2, can be improved. In particular, the presentation of the model would be more clear if the graphical model can be specified. The motivation of choosing the observation model and priors is not clear. In section 3, the author described the details of model specification without explaining why those design choices were appropriate for modeling sparse sequence data.Experimental results on a real-world dataset is presented. However, to demonstrate how the model works, it would be best to add synthetic experiments as sanity check. Results using common baseline approaches should also be presented. The results should also be properly quantified in order to compare the relative advantage of different approaches. This paper considers self-maps of metric spaces where the range is strictly smaller than the domain.  Under this condition this tries to show that such a map has a fixed point.  Now the paper suggests that such a "weakly contractive" map has a fixed point and tries to use such maps to find the global minima of functions.  Even if all the proofs in this paper were right I do not see what this has anything to do with learning and why such a paper has been submitted to ICLR! This paper should probably be submitted to an optimization journal!The basic proofs here are completely unclear. Like in Lemma 1.2, its not even clear what the variable "x" is in the limit! The limit is being taken over the sequence index as far as I can see. Top of page 3 tries to describe an algorithm which can leverage weak contraction to get the global minima if it exists. But this description is hardly making any sense to me. I don't see how the function to be optimized is being used to define the weakly contractive "T" map in the paragraph just below the proof of Lemma 1.4. (How is that parameter "h" even chosen in the definitions of X^{&gt;} and X^{\leq}?) Without a clear pseudocode there is almost nothing concrete in the paper to judge correctness by. The experiments are all set-up on standardized functions which have nothing to do with learning setups. So the relevance of the experiments is completely unclear, let alone the fact that the description is too muddled up.  Also the notation used in the paper is highly non-standard and that makes reading very difficult. For example "D" seems to be the symbol for diameter of the metric space. So D is a property of the metric space (X,d) and its not a part of the definition of the metric-space as the weird notation "(X,d and D)" seems to suggest!  Also the definition 2 is ambiguous because it uses a {\cal R} which doesnt seem to have been defined anywhere! This paper proposes an approximate second-order method with low computational cost. A common pitfall of second-order methods is the computation (and perhaps inversion) of the Hessian matrix. While this can be avoided by instead relying on Hessian-vector products as done in CG, it typically still requires several iterations. Instead, the authors suggest a simpler approach that relies on one single gradient step and a warm start strategy. The authors points out that the resulting algorithm resembles a momentum method. They also provide some simple convergence proofs on quadratics and benchmark their method to train deep neural networks.While I find the research direction interesting, the execution is rather clumsy and many details are not sufficiently motivated. Finally, there is a lot of relevant work in the optimization community that is not discussed in this paper, see detailed comments and references below.1) MethodThe derivation of the method is very much driven on a set of heuristics without theoretical guarantees. In order to derive the update of the proposed method, the authors rely on three heuristics:a) The first is to reuse the previous search direction z as a warm-start. The authors argue that this might be beneficial if If z does not change abruptly. In the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature where the direction of z might change quickly from one iteration to the next.The "warm start" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?b) The second step interleaves the updates of z and w instead of first finding the optimum z. This amounts to just running one iteration of CG but it is rather unclear why one iteration is an appropriate number. It seems one could instead some adaptive strategy where CG with a fixed accuracy. One could potentially see if allowing larger errors at the beginning of the optimization process might still allow for the method to converge. This is for instance commonly done with the batch-size of first-order method. Gradually increasing the batch-size and therefore reducing the error as one gets close to the optimum can still yield to a converging algorithm, see e.g. Friedlander, M. P., &amp; Schmidt, M. (2012). Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3), A1380-A1405.c) The third step consists in replacing CG with gradient descent."If CG takes N steps on average, then Algorithm 2 will be slower than SGD by a factor of at least N, which can easily be an order of magnitude".First, the number of outer iterations may be a lot less for the Hessian-free method than for SGD so this does not seem to be a valid argument. Please comment.Second, I would like to see a discussion of the convergence rate of solving (12) inexactly with krylov subspace methods. Note that Lanczos yields an accelerated rate while GD does not. So the motivation for switching to GD should be made clearer.d) The fourth step introduces a factor rho that decays z at each step. Im not really sure this makes sense even heuristically. The full update of the algorithm developed by the author is:w_{t+1} = w_t - beta nabla f + (rho I - beta H) (w_t - w_{t-1}).The momentum term therefore gets weighted by (rho I - beta H). What is the meaning of this term? The -beta H term weights the momentum according to the curvature of the objective function. Given the lack of theoretical support for this idea, I would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.This is especially important given that you claim to decay rho therefore giving more importance to the curvature term.Finally, why would this be better than simply using CG on a trust-region model? (Recall that Lanczos yields an accelerated linear rate while GD does not).2) Convergence analysisa) The analysis is only performed on a quadratic while the author clearly target non-convex functions, this should be made clear in the main text. Also see references below (comment #3) regarding a possible extension to non-convex functions.b) The authors should check the range of allowed values for alpha and beta. It appears the rate would scale with the square root of the condition number, please confirm, this is an important detail. I also think that the constant is not as good as Heavy-ball on a quadratic (see e.g. http://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf), please comment.c) Sub-sampling of the Hessian and gradients is not discussed at all (but used in the experiments). Please add a discussion and consider extending the proof (again, see references given below).3) Convergence Heavy-ballThe authors emphasize the similarity of their approach to Heavy-ball. They cite the results of Loizou &amp; Richtarik 2017. Note that they are earlier results for quadratic functions such as Lessard, L., Recht, B., &amp; Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1), 57-95.Flammarion, N., &amp; Bach, F. (2015, June). From averaging to acceleration, there is only a step-size. In Conference on Learning Theory (pp. 658-695).The novelty of the bounds derived in Loizou &amp; Richtarik 2017 is that they apply in stochastic settings.Finally, there are results for non-convex functions such convergence to a stationary point, seeZavriev, S. K., &amp; Kostyuk, F. V. (1993). Heavy-ball method in nonconvex optimization problems. Computational Mathematics and Modeling, 4(4), 336-341.Also on page 2, "Momentum GD ... can be shown to have faster convergence than GD". It should be mentioned that this only hold for (strongly) convex functions!4) Experimentsa) Consider showing the gradient norms. b) it looks like the methods have not yet converged in Fig 2 and 3.c) Second order benchmark:It would be nice to compare to a method that does not use the GN matrix but the true or subsampled Hessian (like Trust Region/Cubic Regularization) methods given below.Why is BFGS in Rosenbrock but not in NN plots?d) "Batch normalization (which is known to improve optimization)" This statement requires a reference such asTowards a Theoretical Understanding of Batch NormalizationKohler et al& - arXiv preprint arXiv:1805.10694, 20185) Related WorkThe related work should include Cubic Regularization and Trust Region methods since they are among the most prominent second order algorithms. Consider citing Conn et al. 2000 Trust Region,  Nesterov 2006 Cubic regularization, Cartis et al. 2011 ARC.Regarding sub-sampling: Kohler&amp;Lucchi 2017: Stochastic Cubic Regularization for non-convex optimization and Xu et al.: Newton-type methods for non-convex optimization under inexact hessian information.6) More commentsPage 2Polyak 1964 should be cited  where momentum is discussed."Perhaps the simplest algorithm to optimize Eq. 1 is Gradient Descent". This is technically not correct since GD is not a global optimization algorithm. Maybe mention that you try to find a stationary pointrho (Eq. 2) and lambda (Eq. 4) are not definedPage 4: Algorithm 1 and 2 and related equations in the main text: it should be H_hat instead of H.BackgroundMomemtum GD exhibits somewhat better resistance to poor scaling of the objective functionTo be precise the improvement is quadratic for convex functions. Note that Goh might not be the best reference to cite as the article focuses on quadratic function. Consider citing the lecture notes from Nesterov.Section 2.2This section is perhaps a bit confusing at first as the authors discuss the general case of a multivalue loss function. Consider moving your last comment to the beginning of the section.Section 2.3As a side remark, the work of Dauphin does not rely on the Gauss-Newton approximation but a different PSD matrix, this is probably worth mentioning.Minor comment: The title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small e.g. in Fig 1. # SummaryThis paper proposes to learn behaviors independently from the main task. The main idea is to train a behavior classifier and use domain-adversarial training idea to make the features invariant to sources of behaviors for transfer learning to new behaviors/tasks. The results on Atari games show that the proposed idea learns new behavior more quickly than the baseline approaches. [Cons]- Some descriptions are ambiguous, which makes it hard to understand the core idea and goal of this paper. - The experimental setup is not well-designed to show the benefit of the idea. # Comments- This overall idea is a straightforward extension from domain-adversarial learning except that this paper considers transfer learning in RL.- The goal/motivation of this paper is not very clearly described. It seems like there is a "main task" (e.g., maximizing scores in Atari games) and "behavior modules" (e.g., specific action sequences). It is unclear whether the goal of this paper is to learn 1) the main task, 2) learning new behavior modules quickly, or 3) learning new (main) tasks quickly. In the abstract/introduction, the paper seems to address 3), whereas the actual experimental result aims to solve 2). The term "task" in this paper often refers to "main task" or "behavior" interchangeably, which makes it hard to understand what the paper is trying to do. - The experiment is not well-designed. If the main focus of the paper is "transfer to new tasks", Atari is a not a good domain because the main task is fixed. Also, behavior modules are just "hand-crafted" sequences of actions. Transfer learning across different behaviors are not interesting unless they are "discovered" in an unsupervised fashion. - The paper claims that "zero-shot" transfer is one of the main contributions. Zero-shot learning by definition does not require any additional learning. However, they "trained" the network on the new behavior modules (only the main network is fixed), which is no longer "zero-shot" learning. The authors try to build a deep neural network model based on observations from the human brain Pre-Frontal Cortex connectivity. Based on a DQN network, the authors add additional fully connected layers as Behavior Module to encode the agent behavior and add the Discriminator to transfer information between behavior modules. The authors experiment on four different games and evaluate based on metrics game scores and behavior distance.Overall the quality of the paper is low and I recommend to reject it.[Weakness in Details]1. I am not convinced that the proposed algorithm actually solves/works as described in the motivation. Moreover, the whole framework just adopts existing algorithms(like DQN and adversarial training) which provides little technical contribution.2. I am skeptical about the motivation whether mimicking the human brain Pre-Frontal Cortex connectivity can really result in a better neural network model. The poor execution and insufficient evaluation of this work prevent me from getting a clear answer.3. It is very strange that the authors emphasize that "This property is particularly useful for user modeling (as for dialog agents) and recommendation tasks, as allows learning personalized representations of different user states." while in the end doing experiments with video games playing. There are tons of public recommendation data sets out there, why not experiment on recommendation, which has much clear(well-established) evaluation metrics and public-domain datasets that can make it easier for others to repeat the experiments.4. The experiments are insufficient and the baselines are weak. Lots of state of artworks are left out.5. The writing of this paper needs further improvement and parts of this paper is not clearly written which makes it challenging for readers to follow the authors' ideas. Proposal to use polar regression for prediction problems. To do so, one maps the target variable into "maximally separating prototypes" laid in the D-hypersphere. For classification, the learning problem reduces to minimizing the angle between D-dimensional feature vectors and the associated D-dimensional polar prototype. A similar strategy applies to regression, where the continuous target variable is squeezed to the range of the hypersphere.The authors claim that their method unifies, as opposed to much prior art, classification and regression approaches. I disagree with this claim, since we usually approach classification as a (normalized!) regression problem. In some cases the normalization is on the entire output space (single-label classification as in ImageNET), and in some other cases this normalization happens separately in each component of the output space (multi-label classification as in COCO). It is even possible to train an ImageNET classifier using mean squared error given unit-norm feature vectors (Tygert et al, 2017). As such, the "unification" proposed by the paper seems a bit blurry to me.I am unconvinced about the impact shown by the experiments. Table 1 shows accuracies far from the state-of-the-art (91% for all methods in CIFAR-10 versus 97% SOTA, 65% for the proposed method versus 75% SOTA) and throw some separation statistics without a clear correlation to accuracy. The experiment on semantic priors is inconclusive, as all non-baseline results are within error bars. The impact of Section 3.3. is also unclear, since obtaining semantic (digit rotation) interpolations in MNIST is a common feat achieved by unsupervised learning algorithms with decent feature learning.The results from section 3.2 are interesting, although I would be interested in seeing a reduction-to-classification baseline, where the years are clustered to set up a classification problem, and the prediction is fine-tuned by a local regression.Note: Regressing to (random) polar prototypes was proposed in https://arxiv.org/abs/1704.05310 Summary:This paper considers the task of trail navigation task recently explored by Giusti et al. and Smolyanskiy et al. The authors describe their setup for physical experiments with a drone, and compare three neural network architectures for trail navigation on the IDSIA dataset. Experiments in a simulator are also reported.Good aspects of the paper:The pairing of simulation with trail navigation is an interesting idea, though it is not explored much in this paper.Bad aspects of the paper:Although the presence of physical experiments is suggested by pages 3 and 4, there are no physical experiments actually reported in the paper. In Section 5, this is revealed to be due to a hardware bug. The authors should not include these descriptions if they are not tied to reported experiments.One of the main contributions of the paper is stated to be the comparison between neural network architectures. The two architectures compared to the TrailNet model from Smolyanskiy et al. are selected for their performance on the ImageNet classification task, and are shown to outperform TrailNet on salient metrics. However, comparing only three architectures is a very small comparison, and is not much of a contribution to the research problem.This paper does not introduce new methods for approaching the problem of trail navigation. In its current form, it is a small comparison of existing classification architectures on the IDSIA dataset.The paper also contains a number of minor errors. For instance, in Table 2 there is a footnote that leads nowhere, introduced in Sif is cited incorrectly, in recent times jet (2014) is cited incorrectly, and the figures are grainy (this isnt really an error, but do try to make figures crisp in the future, e.g. with pdf images). The paper initiates a comparison between different SOTA convolutional neural networks for UAV trail guidance with the goal of finding a better motion control for drones. They use a simulator (but not a physical UAV)  to perform their experiments, which consisted on evaluating tuned versions of Inception-Resnet and MobileNet models using the IDSIA dataset, achieving good results in the path generated.  I think that the authors have perform an interesting evaluation framework, although not novel enough according to the literature. It is also great that the authors have included an explicit enumeration of all the dimensions relevant for their analysis (which are sometimes neglected), namely, computational cost, power consumption, inference time and robustness, apart from accuracy. However, I think the paper is not very well polished: there are quite a lot of grammatical, typing and aesthetic errors. Furthermore, the analysis performed is an A+B approach from previous works (Giusti et al.2016, and Smolyanskiy et al, 2017) and, thus, it is hard to find the novelty here, since similar comparisons have been already performed. Therefore, the paper needs major improvements in terms of clarity regarding the motivations in the introduction.Also, one third of the paper is devoted to the software and hardware architecture used in the study, which I think it would be better fitted in an appendix section as it is of no added scientific value. Another weakpoint is that the authors were unable to run their DNN models on a physical drone in real time due to a hardware bug... I think the paper would benefit from a more robust (real) experimentation since, as they are, the presented results and experiments are far from conclusive. This paper proposes autoencoder architectures based on Cohen-Welling bases for learning rotation-equivariant image representations. The models are evaluated by reconstruction error and classification in the space of the resulting basis on rotated-MNIST, showing performance improvements with small numbers of parameters and samples.I found most of this submission difficult to read and digest. I did not understand much of the exposition. Ill freely admit I havent followed this line of work closely, and have little background in group theory, but I doubt Im much of an outlier among the ICLR audience in that regard. The Preliminaries section is very dense and provides little hand-holding for the reader in the form of context, intuition, or motivation for each definition and remark it enumerates. I can't tell how much of the section is connected to the proposed models. (For comparison, I skimmed the prior work that this submission primarily builds upon (Cohen &amp; Welling, 2014) and found it relatively unintimidating. It gently introduces each concept in terms that most readers familiar with common machine learning conventions would be comfortable with. It's possible to follow the overall argument and get the "gist" of the paper without understanding every detail.)All that being said, I dont doubt this paper makes some interesting and important contributions -- I just dont understand what they are.Here are some specific comments and questions, mostly on the proposed approaches and experiments:* What actually is the tensor (product) nonlinearity? Given that this is in the title and is repeatedly emphasized in the text, I expected that it would be presented much more prominently. But after reading the entire paper Im still not 100% sure what tensor nonlinearity refers to.* Experiments: all models are described in long-form prose. Its very difficult to read and follow. This could be made much clearer with an algorithm box or similar.* The motivation for the Coupled Autoencoder model isnt clear. What, intuitively, is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa? The empirical gains are marginal.* Experiments: the structure of the section is hard to follow. (1) and (2) are descriptions of two different models to do the same thing (autoencoding); then (3) (bootstrapping) is another step done on top of (1), and finally (4) is a classifier, trained on top of (1) or (2). This could benefit from restructuring.* There are long lists of integer multiplicities a_i and b_i: these seem to come out of nowhere, with no explanation of how or why they were chosen -- just that they result in learn[ing] a really sharp W_28. Why not learn them?* How are the models optimized? (Which optimizer, hyperparameters, etc.?)* The baseline methods should also be run on the smaller numbers of examples (500 or 12K) that the proposed approach is run on.* A planar CNN baseline should be considered for the autoencoder experiments.* Validating on MNIST alone (rotated, spherical, or otherwise) isnt good enough in 2018. The conclusions section mentions testing the models with deeper nets on CIFAR, but the results are not reported -- only hinting that it doesnt work well. This doesnt inspire much confidence.* Why are Spherical CNNs (Cohen et al., 2018) a good baseline for this dataset? The MNIST-rot data is not spherical.* Table 1: The method labels (Ours, 28/14 Tensor, and 28/14 Scale) are not very clear (though they are described in the text)* Table 1: Why not include the classification results for the standard AE? (They are in the Fig. 6 plot, but not the table.)* Conclusions: We believe our classifiers built from bases learnt in a CAE architecture should be robust to noise -- Why? No reasons are given for this belief.* There are many typos and grammatical errors and odd/inconsistent formatting (e.g., underlined subsection headers) throughout the paper that should be revised. This paper makes two different contributions in the field of adversarial training and robustness.First the authors introduce a new type of attack that exploits second-order information while traditional attacks typically rely on first-order information.Another contribution is a theorem that using the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels.Overall, I find that the paper lacks clarity and does not properly contrast their work to existing results. They are also some issues with the evaluation results. I provide detailed feedback below.1) Prior worka) Connection between adversarial defense and robustness to random noiseThis connection is established in Fawzi, A., Moosavi-Dezfooli, S. M., &amp; Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems (pp. 1632-1640).b) Connection between minimal perturbation required to confuse classifier and its confidence was discussed for the binary classification in Section 4 ofFawzi, Alhussein, Omar Fawzi, and Pascal Frossard. "Analysis of classifiers robustness to adversarial perturbations." Machine Learning 107.3 (2018): 481-508.c) The idea to compute the distribution of classifier outputs when the input is convolved with Gaussian noise was already anticipated in Section V of the following paper which relates the minimum perturbation needed to fool a model to its misclassification rate under Gaussian convolved input:Lyu, Chunchuan, Kaizhu Huang, and Hai-Ning Liang. "A unified gradient regularization family for adversarial examples." Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015.These papers should be discussed in the paper, please elaborate how you see your contribution regarding the results derived there.2) Second-order attack introduced in the paperI think they are a number of important details that are ignored in the presentation.a) Regarding the assumption that the gradient vanishes in the difference of the loss, I think the authors should elaborate as to why this is a reasonable assumption to make. If we assume that the classifier has been trained to optimality then expanding the function at this (near-)optimum would perhaps indeed yield to a gradient term of small magnitude (assuming the function is smooth). However, nothing guarantees that the magnitude of the gradient term is negligible compared to the second-order information. The boundary of the classifier could very well be in a region of low-curvature.b) The approximation of the second-order information is rather crude. However, the update derived is very similar to PGD with additional noise. In optimization, the use of noise is known to extract curvature, see e.g. (Xu &amp; Yang, 2017) who showed that noisy gradient updates act as a noisy Power method that extracts negative curvature direction.Xu, Y., &amp; Yang, T. (2017). First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. arXiv preprint arXiv:1711.01944.3) Issue of "degenerate global minimum": The authors argue that multistep attacks also suffer from this issue. However, the PGD attack of Madry is also initialized at a random point within the uncertainty ball around x, i.e. PGD attack first adds random noise to x before iteratively ascending the loss function. This PGD update + noise at first iteration seem rather similar to the update derived by the authors that uses random noise at every iteration. It could therefore be that the crude approximation of second-order information is not so different from previous work. This should be further investigated either theoretically or empirically.4) Lack of details regarding some important aspects in the papera) Note the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedureThe authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which I think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.b) the noise is not necessarily added directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer, one can still calculate an upper bound using our analysis. We omit the details here for simplicityWhat exactly changes here? How do you estimate the Lipschitz constant in practice?5) Main theorem needs to be contrasted to previous resultsThe main Theorem uses the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels. There are already many results in the field of robust optimization that already derive similar results, see e.g.Namkoong, H., &amp; Duchi, J. C. (2017). Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980).Gao, R., &amp; Kleywegt, A. J. (2016). Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint arXiv:1604.02199.Can you elaborate on the difference between your bounds and these ones? You do mention some of them require strong assumptions such as smoothness but this actually seems like a mild assumption (although some activation functions used in neural nets are indeed not smooth).6) Adversarial Training Overfit to the Choice of normsThe main theorem derived in the paper uses the l_2 norm. What can be said regarding other norms?7) Experiments:a) the authors only report accuracies for attacks whose l2-norm is smaller than a fixed constant 0.8. However, this makes the results difficult to interpret and the authors should instead state the signal to noise ratio, i.e. dividing the l2-norm of the perturbation by the l2-norm of the image. Otherwise, it is not clear how strong or weak such perturbations are. (In particular, the norm depends on the dimension of the image, so l2-norms of perturbations for MNIST and CIFAR10 are not comparable).b) In Section 6.2, the authors state that an l_infty trained model is vulnerable against l_2 perturbations. Why not training the model under both l_infty and l_2 perturbations?c) Figure 1Based on the results predicted in Theorem 2, it seems it would be more interesting to evaluate the largest L for which the classifier predictions are the same. Why did you report a different results?8) Other commentssection 2.1: Note this distribution is different from the one generated from softmax. Why/How is this different?connection to EOT attack: authors claim: E_{d<N(0,Ã2I)} [_x L(¸, x, y)|x+d] = _x E_{d<N(0,Ã2I)} [_x L(¸, x, y)|x+d]. There is a typo on the RHS where _x is repeated twice. This is also the common reparametrization trick so could cite Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Review of "Neural Network Cost Landscapes as Quantum States"Paper summary:The paper proposes a new algorithm "quantum amplitude amplification"for training and model selection in binary neural networks. (in whichboth weights and activations are restricted to the set -1, 1)Section 2 references related work and gives some motivation, that somequantum algorithms scale better (in terms of big-O notation) thanclassical algorithms.Section 3 explains the basics of quantum computing (qubits and quantumgates).Section 4 explains the proposed method. There are two toyproblems. The binary neural network has 8 weight parameters. There arehelpful Figures 1-2 which explain the network structure and thequantum circuit.Section 5 explains the results of using the proposed method in aquantum computer simulator (not an actual quantum computer). On thetwo toy problems the paper observes quadratic speedups with respect toa brute force search.Comments:A strong point is that the paper is very well-written and easy tounderstand. However there are several weak points which should be addressed beforepublication. Major weak points are (1) only (noiseless?) toy data setsare used, (2) some terms in the paper are unclear/undefined, and (3)results are unconvincing.It is not clear that this article should be published in the machinelearning literature. One of the hallmarks of machine learning is afocus on algorithms for real data sets / problems. In contrast thefocus of this paper is quantum computations on toy data /problems. Maybe this paper would be better suited for publication inthe quantum computation literature?The toy problems are explained in section 4.2. Is there any noise orare these noiseless simulations? How does your model/algo perform as afunction of the noise level? How many data points did you simulatefrom the model? (e.g. what is the number of observations in the training set?)The paper uses the terms "cost landscape" and "meta-cost landscape"without explicitly defining them. Equations should be added to clarifythese terms.Results could be made more convincing by1. using a real quantum computer.2. using real data rather than toy data.3. adding error bars or confidence intervals to Figures 4-5.4. using a more appropriate baseline -- why not try the algorithms mentioned in section 2.1?Figure 3 could be clarified by providing ticks and labels on the xaxes. Summary:The paper presents an SSL method extending FixMatch by introducing an auxiliary loss motivated from the metric learning literature. For example, the triplet loss is utilized to define the loss, where any triplet of anchor, positive and negative examples, either using ground-truth labels for labeled data or pseudo-labels for unlabeled data, are used to compute the loss. Variants using hard triplets of the mean as well as using the contrastive loss are proposed. The proposed methods are evaluated on standard SSL benchmarks.Review comments:While authors presented ablation study with various metric learning losses proposed in this work, the biggest concern is that none of these losses shows the clear performance improvement over FixMatch when combined with FixMatch losses. This begs the essential question on the effectiveness of the proposed losses on top of FixMatch. If RankingMatch is not clearly more performant than FixMatch, is there any scenario that FixMatch cannot be applied but RankingMatch can? It is also unclear to me the motivation for introducing auxiliary metric learning losses -- why is it a good addition to the FixMatch loss? Which aspect of FixMatch loss is problematic and how does adding metric learning loss fix that issue?"We argue that the images from the same class do not have to have similar representations strictly, but their model outputs should be as similar as possible" --> There seems no supporting empirical evidence to this statement.Reason for decision:Given the fact that the proposed method is constructed incrementally to the previous method (i.e., additional loss to FixMatch loss), the initial decision criteria is its empirical effectiveness. Unfortunately, I am not convinced that the proposed method is clearly improving upon previous methods in any settings considered in the paper, so I would recommend for rejection. ##########################################################################Summary:This paper proposes a method to train a neural network by selecting a weight from a set of $K$ randomly generated weights for each edge in the network. Each edge has a different set of random weights. Quality score is assinged to each of $K$ random weights, which determines the weight used in the forward calculation. Instead of optimizing weights directly, the proposed method optimizes the quality scores with the straight-through gradient estimator. Experimental results show that the neural network trained by the proposed method achieves high accuracy compared to random initialization even when $K=2$.##########################################################################Reasons for score: Overall, I vote for rejecting. The authors say in Sec. 3 that the goal is to construct non-sparse neural networks with completely random weights that achieve high accuracy. However, the model obtained by the proposed method is no longer a network with completely random weights, because the authors optimize quality scores instead of original weights. It is empirically shown that a neural network can achieve a high accuracy by properly selecting weights from a set of random weights prepared in advance. However, such a result is not so surprising from the viewpoint that the quality scores are optimized.Also, this paper has few practical implications. I would like to see if the network can still achieve a high accuracy when every edge has a common set of $K$ random weights. If this is the case, the proposed method may lead to a network that is efficiently compressed.##########################################################################Minor concerns:(p.1) a fixed a set of random weights -> a fixed set of random weights-- 1. Paper contribution summary    This paper proposes a new one-point zeroth-order gradient estimation method for online optimization. Comparing to previous methods in the same scenario, this paper's method is shown to have smaller variance, which can improve the learning rate in certain cases including classes of convex Lipschitz functions, convex smooth functions, non-convex Lipschitz functions, as well as non-convex smooth functions. 2. Strong and weak points of the paper    1) Strong points: The paper's focus is on one-point zeroth-order gradient estimate, which is a more realistic setting in non-stationary online optimization problems as compared to most popular two-point estimator due to the queried function is time-varying. The new estimate method is based on residual feedback from previous time's perturbed objective value, which can help improve the regret order when function variation is small. It also extends the finding to online non-convex optimization problems with different regret definitions. And the proposed new one-point zeroth-order gradient estimation method is shown to have smaller variance and improved performance in the two numerical examples.     2) Weak points: The proposed zeroth-order update rule for constrained convex case in Eq.(4) is problematic. For the constrained case, it uses a projection to make sure x_{t+1} is feasible. However, when doing gradient estimation, it actually uses the perturbed x_{t+1} + \delta u_{t+1}, which may violate the constraint, making this update rule not feasible sometimes. The claimed improvement for convex cases is only in the constant order instead of the order of T even under this problematic update rule. The regret metrics used in the two online non-convex cases: non-convex Lipschitz, non-convex smooth are different from each other, which is very weird. 3. My recommendation    I would suggest a rejection due to the problematic update rule in Eq.(4) and the constant order improvement.4. Supporting arguments for my recommendation.    First, the problematic update rule as discussed above. Second, the only constant order improvement of regret in convex cases. Third, the weird different regret metrics used in non-convex cases.5. Questions     1) Can the author/s confirm if the update in Eq.(4) is problematic?     2) Why using two different regret metric in non-convex problems?     3) For the example used in the paragraph under Eq.(8) to show that the assumption 3.1 is weaker than previous works' uniform boundedness, i feel that the uniform boundedness assumption in prior works can be improved to have only bounded first or second order expectation. Then the example's statement won't hold anymore. Can the author/s make any comments on this? [Summary]This paper studies the training of multi-branch networks, i.e. networks formed by linearly combining multiple disjoint branches of the same architecture. The core contribution in this paper is the STAM aggregation rule which is to set the combination coefficient to $1/\sqrt{C}$ for a network with $C$ branches. This aggregation rule is justified by (1) theoretical analysis on the function values and gradient norms at initialization, and (2) experiments on residual networks and transformers showing that this rule performs better than the baseline rules (such as sum or average).[Pros]--- The problem this paper focuses on (multi-branch architectures) is important and I think worth more studies since they are used more widely these days. From a theoretical perspective, the number of branches seems like a similar thing as the width of a network, yet the understanding is not yet as comprehensive as our understanding on the width (for which we know things like what are their effects, how should we initialize a wide network, etc).--- The experimental results are interesting and could be a good reference point for designing these multi-branch architectures. Specifically, it is interesting to see it is better to explicitly do a fixed $1/\sqrt{C}$ scaling, rather than making it trainable or embedding it into the individual linear layers in each block. --- The paper is generally clearly written and easy to follow. The notation between the theory and empirical parts matched quite well.[Cons]--- I feel like the core contribution of the $1/\sqrt{C}$ scaling rule is rather straightforward and not necessarily justified as a new method or theory, especially with the analogy to width in mind: It is standard to initialize weight matrices to have scale $1/\sqrt{d}$ where $d$ is the either the input or output dimension as this gives the right normalization. Even with the other baselines (making the $1/\sqrt{C}$ trainable or embedding it into the individual blocks) performing worse according to this paper, I still feel like a researcher or practitioner who encounters the issue of choosing the aggregation rule could still end up with the same solution proposed in this paper. --- The theoretical results also build on the exact same normalization idea ($1/\sqrt{C}$ scaling gives the right normalization) as for width, and do not seem to provide us with much new theoretical understandings for what is unique about the multi-branch architecture. From my (maybe biased) perspective, I would have been more interested in some more in-depth understandings (either theoretical or empirical), such as what is the effect of adding more branches (and where does diminishing return kick in). Reading this paper leaves me more questions than answers about these multi-branch networks. # IntroductionFirst off, thank you (authors) for taking the time to put this together.The problem the authors are trying to tackle is meaningful (+1) and relevant. Contrastive learning is in dire need of removing the dependency of using transforms. This paper goes one step and even tries to generalize it across domains which is desperately needed in the field.## Paper summaryThe authors have taken an idea from supervised learning (MixUp) and derived the matching application for contrastive learning in hopes of removing the need for domain-specific augmentations.They show how to apply their i-Mix to SimCLR, Moco and BYOL.Finally, authors show results for vision, tabular and speech data using standard datasets for each.Results have a few sections:1. Evaluate cifar-10, cifar-100, Commands and CovType using Moco and BYOL in addition to i-Mix.2. Evaluate larger datasets, imagenet and Higgs.3. Evaluate how depth of a resnet and length of training are affected with i-Mix.4. They show ablations with models trained without their standard data augmentations and i-Mix.5. They evaluate transfer learning on vision (cifar-10 <=> cifar-100) and (Imagenet -> VOC detection)## Strong points.1. The overall goal they are looking at is impactful and critical.2. They show results on small and large datasets.3. They are attempting 3 domains with the same method.4. Their most promising result is Table 3 which shows a nice gap between models trained without data augmentation but with i-Mix instead.## Weak pointsTheir general results are not significant enough. Most results look like this (82.5 vs 82.7).This hints at these results being achieved largely via hyperparameter tuning, instead of meaningfully providing an advantage.## Suggestions for improvement1. Show distributions for results instead of a single number. (ie: not 82.5 vs 82.7, but instead run 50-100 times and plot the histogram)2. Many of the ideas mentioned here are also mentioned in non-cited relevant work (for example: [YADIM](https://arxiv.org/abs/2009.00104)).3. Provide code. Since some of these claims come down to implementation details, it's important to see the code as well.## RecommendationReject This paper analyzes the effect of choosing a large step-size on the generalization of deep networks. They suggest that starting with a large learning rate, the loss initially increases before converging to a flatter minimum with improved generalization (catapult effect). When the learning rate is above a certain threshold, the authors observed this catapult effect along with a decrease in the curvature of the landscape. These observations were empirically demonstrated through various experiments and analytically studied for a two-layer linear network.Pros:The paper tackles an interesting problem in a vibrant field of research.Several experiments that demonstrate the authors claims were presented in the paper. These results can be further used to propose algorithms designed to converge to flatter solutions. The related material is referenced and well-discussed in the paper.Cons:This paper lacks theoretical evidence for supporting the raised claims. The model studied in Theorem 1 is very simple. Moreover, the effect of choosing the step-size and batch-size on the sharpness of the computed solution has been observed in the literature. For instance, Keskar et al. (2017) justified the improved generalization achieved when using small batch sizes by the ability of such methods to converge to flat minima. The choice of the batch-size was formally related to the choice of the step-size in the work of Patel (2017) which provides a learning-rate lower bound threshold for the divergence of batch SGD. This threshold is also a function of the curvature. The higher the curvature around a critical point, the lower the threshold required for divergence. Hence choosing a large step-size tend to escape sharp minima and potentially converge to flatter minimizers. The last paper was not cited in the submitted manuscript.The paper does not provide any intuition on how to compute the thresholds $\mu_{\mbox{crit}}$ and $\mu_{\max}$, and how is this related to the choice of non-linearity, structure of the network, &Comments:1.For the gradient descent update rule in equation (1): why do we have $f_t$? Minor Comments:1.What is meant by ``compute budget?2.In the definition of $\Theta$ in Page 4: I think it should by $\sum_{\alpha = 1}^m$ instead of $\sum_{\mu=1}^p$.3.In Figure S1, the images are small and not very clear.4.Expression (S12) is missing a term.5.S(16), what is $u_{ia}$.6.Figure S10, the colors in the label do not match the plots.   The paper presents a lossy video coding scheme using autoregressive generative models. + It is good to see that the paper includes comprehensive reviews regarding neural network based compression schemes and autoregressive models.- The contribution has been depicted in aspects of 1) a new framework, 2) a new model, and 3) a new dataset. However, beside to 3), it is hard to see the other two aspects have significant novelty both in a neural video coding architecture (Agustsson et al. 2020) and in temporal autoregressive models, etc. - Furthermore, current video codecs (i.e. HEVC) are developed for YUV4:2:0 because the current broadcasting system has supported the video format. The evaluation should be conducted in YUV 4:2:0 domain for fair and reasonable comparisons.   Clarity:The paper is relatively clearly written except for the experimental section. Originality:The content of this paper is not novel. Significance:The claimed contribution of this paper is not significant. Cons:This work tries to rigorously characterize the conditions for a rather intuitive task (reconstruction). There are a few serious issues around it. First, without having the learned weights satisfying certain conditions exactly, the theorems (e.g. Thm1) are way too conservative, i.e. the bounds are too loose in order to catch all corner cases. In addition, in reality, the activations are not necessarily sparse, especially for generative models. The combination of these caveats make these theorems' indications of little meaning. Second, in reality, we use much more complex architecture (CNN, attention, ReLU, Normalization) but these theoretical works can't easily go beyond simple MLP with ReLU, making these results even less meaningful. Thirdly, despite all the above, it is hard for me to reason why the theorems are significant in anyway in the context of ML, e.g. why uniqueness is important. I can understand the significance of existence and uniqueness in the context of differential equation but not ML. Other comments:What is "sparse representation theory"? I know "sparse representation", and "representation theory", but not sure about "sparse representation theory".Lack of novelty: the proofs are direct extensions of existing compress sensing literature. Experiments: -the experimental setups are quite contrived and not optimized for model performance but for the papers' plots. E.g. a larger scale b etter designed MLP can model MNIST dataset quite well (e.g. 4 layers of 1024x1024 with LeakyReLU)-the evaluation metric is suboptimal. Eventually, we want to measure the recovered signal, not the latent code error.  The authors of this manuscript propose an unsupervised learning framework for 3D segmentation of biomedical images. Specifically, the proposed method learns effective representations for 3D patches using variational autoencoder (VAE) with a hyperbolic latent space. Its main contribution lies at that it introduces a new unsupervised learning framework including hyperbolic convolutional VAE and hierarchical triplet loss. This work conducts experiments on toy dataset, the Brain Tumor Segmentation dataset, and cryo-EM data. The experiment demonstrates competitive performance of the proposed method.Major Strengths of The Paper1. The idea of hyperbolic 3D convolutional VAE is interesting.2. The experiment shows that this idea performs favorably against the state-of-the-art.Major Weaknesses of The paper 1. The manuscript contains some grammatical errors, and the writing could also be improved. For example, there are some grammar mistakes like our work is a first to introduce&. The paper will be more clear if the authors could further polish the manuscript.2. The proposed model is not novel enough. In my personal opinion, the proposed model generalizes the gyroplane layer and hyperbolic representations in previous literature to 3D biomedical images. The multi-scale sampling and triplet-loss are also commonly used mechanisms. It would be better if the authors could elaborate the novelty of the proposed model.3. The biologically-inspired toy dataset does not look close to a biomedical dataset. Although we totally understand there should be differences between simulated dataset and real dataset, the toy dataset (i.e. composed of regular shapes) oversimplifies the biomedical image, and makes the experiment results less convincing. It will be better if the authors can simulate images close to real biomedical images, at least with irregular shapes or fuzzy boundaries. 4. For the baseline selection, the authors used the model in paper Unsupervised Segmentation of Hyperspectral Images Using 3D Convolutional Autoencoders, which is published in IEEE Geoscience and Remote Sensing Letters. There should be a domain gap between remote sensing images and biomedical images. I personally think it may not be a fair comparison for the baseline methods. It will be better if the authors could select some of the state-of-the-art unsupervised 3D segmentation methods as baselines. #### SummaryThis paper proposes a new SGD algorithm with heavy-ball momentum and adaptive coordinate-wise stepsizes, called SEHB. It is based on a very recent algorithm called AEGD, and when the momentum parameter gamma_t is set to 0, SEHB becomes identical to AEGD. I find the paper to be of quite low quality for several reasons. Firstly, the paper does not motivate the algorithm at all: it simply lists the ideas that the authors combine in this work, and the derivation of the stepsizes is done under the assumption that $\gamma_t=0$, which corresponds to the case with no momentum. Secondly, it proposes a momentum modification and tries to analyze it, but the theoretical momentum parameter has to decrease exponentially. Finally, the work claims superior performance to other adaptive and non-adaptive algorithms on nonconvex problems, but the tests are insufficient to draw this conclusion. I elaborate on these points below.#### MotivationI can see how the authors derive the stepsizes, and every step of the derivation is clear, but the motivation behind the steps is missing. For instance, why was the function $g()$ introduced? Why are we trying to estimate its values and based on that produce stepsizes? Similarly, why the stepsizes are not changed when the authors introduce momentum? #### TheoryThe theory is disconnected from practice since the momentum parameter in the analysis assumed to be decreasing exponentially. The authors argue that this is similar for Adam and AMSGrad, but in a recent work [1] there is an analysis of AMSGrad with constant momentum ($\beta_1$). Therefore, I find the decreasing momentum schedule of this submission to be a significant flaw, especially since introducing momentum to AEGD is its main contribution. In addition, as far as I can see, the authors show no advantage of the proposed bound compared to that of AEGD.The nature of Theorem 1 is also concerning: before proving anything about the iterates, the authors assume that the iterates difference are bounded, $\|\theta_n - \theta_m\|\le \mathrm{const}$ for any pair of the indices $(n, m)$. This is equivalent to assuming that the iterates themselves are bounded and the only way it can be guaranteed is by introducing a projection step in the algorithm, which the authors didn't do. Of course, this issue is common in regret analysis, but it is an issue nevertheless. Moreover, convergence of many stochastic algorithms such as SGD can be shown without assuming bounded iterates or gradients, see for example [2].#### ExperimentsEven if the theory is not good, the paper still had a chance if it showed a solid justification of improvement over state-of-the-art optimization methods. Unfortunately, I see no code attached to this submission, which makes me a bit concerned about reproducibility, especially as so many works have claimed to be better than both SGD and Adam but have not become commonly used in practice. What makes me additionally concerned about the experiments is that only one seed seems to be used per each method, which makes the results random. Even SGD may converge to different accuracy with a different random seed when everything else, including the stepsize and the architecture, is fixed. The next issue is that only deep networks for vision were tested, while some methods fail to outperform Adam on NLP tasks, GANs, etc. A more diverse selection of problems is required to guarantee that the proposed algorithm is practical.Ideally, more than a single minibatch size needs to be tested, but this is a smaller issue. To improve the experiments, I would recommend the authors to consult a paper concerned with benchmarking optimization methods, for instance [3].The first experiment is on Rosenbrock function, which is rarely used these days as a benchmark since no one uses it in practice. I suggest the authors remove this experiment from the paper and use the space for other things such as additional experiments on real-world applications, proof sketch, etc.In the experiment details, the authors wrote numerical constants as "1e2". I suggest writing it as $10^{-2}$ or 0.01, which is more readable.#### Minor issuesI recommend writing in the algorithm caption that G^2 denotes the coordinate-wise square. Currently, the notation is explained in a separate section, which is convenient but might be overlooked by someone who just wants to know what algorithm you propose.Lemmas 3 and 4 have no commentary around the equations. Please add at least some minimal explanations of the steps performed in the proofs and their motivation.##### Typospage 1, "SGD havs"Equation (7), missing a period at the end.page 4, Remark cites (Nesterov, 1983) for the convergence of SGD. This paper does not have any result about SGD and instead is his work with the accelerated gradient algorithm. Moreover, the citation for the paper of (Nesterov, 1983) has the incorrect title: it mentions $1/\sqrt{k}$ convergence instead of $1/k^2$.page 6, "default ³ often have" -> "has"page 7, "performance on the test set have" -> "has"page 7, "has a excellent" -> "an"page 9, Equation (9) and other equations, $\theta_{,i}^*$ should be replaced with $\theta_i^*$page 9, Equation (9) and other equations, when writing the $i$-th coordinate of the gradient of $f_t$ at $\theta_t$, you probably meant to write $\nabla_i f_t(\theta_t)$ or $(\nabla f_t(\theta_t))_i$ rather than $\nabla f_t(\theta_{t,i})$, which doesn't make any sense unless $f_t$ is a linear function[1] Alacaoglu et al., "A new regret analysis for Adam-type algorithms", ICML 2020[2] Gower et al., "SGD: General Analysis and Improved Rates", ICML 2019[3] Sivaprasad et al., "Optimizer Benchmarking Needs to Account for Hyperparameter Tuning", ICML 2020 The paper presents a semi-supervised model to predict the vitality of beehives. The inputs of the model are data from sensors (audio on one hand and environmental on the other hand such as temperature, humidity ...).  The objective is to predict simultaneously 3 values of interest: the frames state of beehives, the potential diseases and their severity. The architecure is composed of two modules, the first one is an auto-encoder in charge of embedding the audio spectrogram in a low latent dimensional space and the second one a MLP to predict the outputs from the latent spectrogram and the environmental data. The paper presents results of the proposed architecture on a small dataset, an ablation study to show the benefits of the auto-encoder module and the role of the environmental data and a latent space analysis to understand the ability of the model to capture relevant audio information linked to the diseases. In my opinion, the paper is off topic. The presented model is very simple with nothing innovative (except for the application to beehives), the ML aspect of the paper is quite small (just 1 reference is ML oriented, all other references are from biology field) and the choice of the model is not really explained (why not something sequential ?), the experimental data are very small and it is hard to judge the relevance of the approach (only ablation studies is presented, no other approaches/baselines are considerd), I do not see how the approach can be generalized to other similar applications (with multi-modal data, audio combined to environmental time series). I am sure that the subject is interesting and important but I don't think that ICLR is the right conference for this paper. In addition, the experimental part is hard to follow : the outputs of the model and the human labeling information are not clearly stated (what are the different diseases type ?  it is the same with diseases status ? the diseases severity seems to be discrete - bottom of page 3 - but a continuous Huber loss is used ?). The results are also very quickly analysed : Fig. 3 gives the impression that disease severity is always under estimated when the severity is high, but no analysis is provided. In summary, I can not state if the problem that the authors addres is a hard one or with simpler models same results can be achieved. Minor remarks : add color to figures 1,4 (and not black vs gray) for more visibility.  ##########################################################################Summary:This paper proposes an anchor-free object detector that does bounding box regression in the polar coordinate instead of in the Cartesian coordinate. The motivation of doing this is because there are larger variance in offset vectors in the Cartesian coordinate (the extreme case when a point is on one of the four corners of the bounding box, the offset vector becomes [0, w, 0, h]). The authors propose a solution to regress to the pair of corners (either TL+BR or TR+BL) in the polar coordinate, and select the corner pair that gives the smallest variance during training.##########################################################################Pros: Experiment results show using polar coordinate is effective.##########################################################################Cons:1. The authors did some analysis on the variance of the offset vector in Section 3.1, however, I think the analysis is not enough. First, the analysis only contains the worst case analysis, that is, the range of offset targets. And the author directly concluded from this: PolarNet "significantly reduces the variance" (page 4 last line). What is the ratio of variance under cartesian and polar coordinate to make it "significant"? I do not see any number either theoretically proves it or empirical analysis of the variance during training.2. The term "keypoint" used in this paper is confusing. Sometimes the "keypoint" refers to corner points of the box ("Keypoint Position") and other times the "keypoint" simply refers to any point within the bounding box ("Keypoint Offsets").3. The introduction of PolarNet is not clearly presented, specifically there are several confusing points:- Section 3.4.1, what is the usage of t_{x,y}? I don't see how t is used during inference.- Section 3.5, "we select the optimal box from b_{x,y} as the final output of the predicted box", what is "optimal box"?- Figure 3, why the "corner supervision" comes from the feature map? I don't see how corner supervision uses any feature.- The proposed corner supervision is simply the L1 loss, and there are methods that already use it with IoU loss. I don't think it is a contribution and section 3.4.2 and 3.4.3 should be combined.4. Experiments are not solid:- There are ways to reduce variance of offsets under cartesian coordinate, e.g. only use points within the center region of the bounding box to learn offset. Such experiment should be compared.- The importance of extra loss function is also not studied, what are the benefit of using more losses?- From reading Section 3, I feel the method is exactly as applying FCOS + polar coordinate, but Table 2 shows there is still some gap. Where does the extra gain come from?- I checked the FCOS paper and found the R101 results in the paper is 43.2 but the number in this paper is 41.5.##########################################################################Reasons for score: Overall, I vote for rejecting. This paper proposed an interesting idea, but I think way it is presented is not good enough to be accepted. Specifically, I think the paper still misses analysis on the variance of offset prediction, and also misses some important ablation studies. Furthermore, the paper is not well-written and requires some revision. In this paper, an extension to deep kernel learning is proposed. A linear kernel is used as the base kernel, which enables exact optimization of the kernel hyperparameters. There is a universal approximator theorem stating that the deep neural network with linear kernel could approximate any kernel function, which is quite obvious from the perspective of random Fourier features as well. Also, multiple neural networks are used to produce features and the features are concatenated.  I am not sure why the word ensemble is used, but it is really a concatenation of features instead of ensembling predictions. Besides the exact inference, standard variational inference is also proposed for the linear base kernel. Experiments are conducted on synthetic data and UCI datasets, with comparison to DKL with linear kernel and deep ensembles. In general, I could not find very interesting contributions from the paper. The framework follows closely from DKL with a linear kernel. But I have a question about the proof of the universal approximator theorem, to approximate the different eigenfunctions, I would assume the hidden layer of the neural network to be at least of O(B) where B is the number of eigenfunctions to be approximated. So potentially, the neural network needs to be very wide which might not be practical. Also, I am confused why the concatenation of features is referred to as ensemble in the paper. It is super confusing to me unless I am not understanding how the features are used jointly. Also, the authors mention that a learner with M output is simply a concatenation of M single-output learners, suggesting that multi-output learner may help to further reduce the number of H of required learners. With the same number of nodes in the hidden layers, I dont think a multi-output learner is equivalent to M different single-output learners, therefore the argument made here might not be valid.In terms of experiments, it is weird that for DKL linear kernel is used instead of the original spectral mixture kernel with random Fourier features. It makes the most sense to compare the linear kernel with something like RBF or spectral mixture kernel. Also, UCI regression tasks seem to be rather easy and do not necessarily need a deep neural network to do feature engineering. It would be necessary to conduct experiments on more complicated tasks such as images to validate the effectiveness of the proposed linear kernel approach.  This paper proposes a programming language, RASP, as a computational model for transformer encoders, and discusses how analysis in terms of this language could be used to understand the behavior of transformer models.The idea of finding a computational model for transformers is interesting, and (as discussed in section 4) could lead to insights in terms of how to build better models.However, this paper lacks any results or experimental analysis, which makes it difficult to judge the validity or value of the claims presented. Section 4 discusses how recently proposed transformer variants could be understood (post-hoc) in terms of the RASP language. However, in order to justify using the RASP language to reason about transformers, I think it is necessary to demonstrate experimentally that insights from RASP can translate to new empirical findings. For example, in section 3.1, the paper makes the claim, For any given RASP program, we can compute the minimal number of layers required to implement it in a transformer, and upper bound the number of heads this implementation requires. Can this be verified experimentally, by building a synthetic task, and testing performance as the number of heads is varied?Similarly, section 4.2 provides an analysis of the recently proposed sandwich transformer model. Could similar analysis be used to make claims about novel, untested architecture variants? Could these claims be verified experimentally? Results such as this would be of high value to the ICLR community.Because of the lack of experiments, I recommend rejection. I think this is an interesting line of work which could prove valuable to the ICLR community if supported by rigorous experimental evidence.Minor details:pg 1: that is requires -> that is required Authors propose addition of group sparsity regularizer into the FTRL framework, and derive update rules of AdaGrad/Adam. They demonstrate the effectiveness by inducing sparsity on several models used in the benchmarks.Reason to Score: Weaker experimentation, lack of standard baselines -- including them can improve the paper.I have listed my concerns below and hopefully authors can address them during the rebuttal period.Questions/Comments:1. Could authors contrast their work with algorithm presented in: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf which includes an implementation in: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/FtrlIs the contribution an extension by using group sparsity?2. Missing word in sentence in abstract ("not only can the")... the loss functions, not only can the dimensions of features be effectively and efficiently reduced ...3. Incorrect citationAny regret minimizing algorithm can be converted to a stochastic optimization algorithm with convergence rate O(RT /T) using an online-to-batch conversion techniquePlease cite:N. Littlestone. From On-Line to Batch Learning. In Proceedings of the 2nd Workshop on Computational Learning Theory, p. 269-284, 1989.4. A major concern was on experiment sections. Authors do not mention what type of groups were used clearly, which made it hard to judge the results. I also suggest authors include several baselines comparing with existing work:a) block l1 (l2 of the norm of the group) as penalty to the objectiveb) standard magnitude pruning. https://arxiv.org/abs/1902.09574  This paper focuses on developing an RL learning algorithm that is simple and can significantly improve performance over existing algorithms. The paper presents the algorithm AWR, which is an extension of the algorithm reward weighted regression. The primary extensions of RWR are using the advantage function instead of the q function and the ability to use experience replay. Experiments on common environments are conducted to evaluate the performance of AWR and compare it to other algorithms. There are ablation experiments to justify the choice of some of the extensions. The development of simple, effective off-policy algorithms for reinforcement learning is still an open problem, and this paper tries to take a step towards addressing it. The paper gives a detailed derivation of the proposed algorithm, and the ablation studies provide some information as to what components in the algorithm make it useful. However, I do not believe this paper is ready for publication because the extensions are minor, and experiments lack scientific rigor, making it unclear what is to be learned from the paper. There are also some errors in the paper. There are some essential questions the paper should address but it did not.What is the benefit of using the baseline in RWR? Does it change the weighting of the update, or is it just a variance reduction technique?The beta term in RWR is adapted during learning. How does the choice of beta in AWR affect the weighting of rewards during learning?The paper says there is a theoretical analysis of the algorithm, but I only see the algorithm's derivation. What analysis is this statement referring to? The extension of RWR to using experience replay is formulated as using a mixture policy of past policies using weights w_i for each policy pi_i. How are weights w_i chosen? The algorithm, as defined, performs a global maximization at each step. However, neural networks are used, and this maximization cannot be guaranteed. How are the updates performed? There are other changes to make the algorithm used in the experiments, but they are not connected to the algorithm's design, e.g., the normalized advantage function. Experiments:The primary motivation and claim of this paper is the design of a more effective algorithm. Therefore, one should expect the presented algorithm to be a significant increase in performance. However, this cannot be concluded based on the experimental methodology used. The main issues are with how hyperparameters are selected and the lack of statistical analysis of the results. Some undefined processes set the hyperparameters for AWR, and the parameters for the other algorithms were left unspecified. Based on this experimental setup, it is impossible to tell if AWR is better or worse than the other algorithms or if it is just due to the specific setting of hyperparameters.  The performance results are given along with standard deviations over ten trials. However, this does not directly quantify how likely these results are achieved. Furthermore, it has been pointed out in several past works that the performance of RL algorithms are highly stochastic, and more trials and proper statistical analysis is needed to gain confidence in the outcomes (Colas et al., 2018, Henderson et al., 2017).  There exist more rigorous evaluation procedures that may be useful in improving this paper (Dodge et al., 2019, Jordan et al. 2020, Sivaprasad et al. 2020).Mathematical errors:The jump from (9) to (10) is not correct. Sampling from a fixed-sized dataset is not equivalent to sampling from the state distribution of a policy. In the algorithm, samples of states and actions are drawn according to the data distribution. However, this does not correspond to the empirical samples of discounted state distribution; i.e., there needs to be a gamma^t term in the expectation (Thomas, 2014). ReferencesColas, C., Sigaud, O., & Oudeyer, P. Y. (2018). How many random seeds? statistical power analysis in deep reinforcement learning experiments. arXiv preprint arXiv:1806.08295.Dodge, J., Gururangan, S., Card, D., Schwartz, R., & Smith, N. A. (2019). Show your work: Improved reporting of experimental results. arXiv preprint arXiv:1909.03004.Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.Jordan, S. M., Chandak, Y., Cohen, D., Zhang, M., & Thomas, P. S. (2020). Evaluating the Performance of Reinforcement Learning Algorithms. In Proceedings of the 37th International Conference on Machine Learning.Sivaprasad, P. T., Mai, F., Vogels, T., Jaggi, M., & Fleuret, F. (2020). Optimizer benchmarking needs to account for hyperparameter tuning. In Proceedings of the 37th International Conference on Machine Learning.Thomas, P. (2014, January). Bias in natural actor-critic algorithms. In International conference on machine learning (pp. 441-448). This paper presents a reinforcement learning algorithm that applies advantage-weighted regression. In each iteration, it samples trajectories from a mixture of previous policies, estimates the value function and then computes the advantage value to estimate the policy. The idea is very similar to the work published in Neumann, Gerhard and Peters, Jan R, Fitted Q-iteration by advantage weighted regression, Advances in neural information processing systems, 2009, starting from reward-weighted regression and further developing to advantage weighted regression.  The difference in this paper is to add a constraint on the policy search, requiring the policy to be similar to the sampling policy. However, this constraint has also been studied in the paper "Christian Wirth and Johannes Furnkranz and Gerhard Neumann, Model-Free Preference-based Reinforcement Learning, AAAI 2017" (It seems not in reference). Overall, it may enhance this paper if it has more technical novelty when developing a new algorithm. My major concern is that this paper has been submitted last year and resubmitted this year without new additions. Moreover, this paper has been available to the public since last year with information on authors and affiliations, which may violate the double-blind review policy (e.g. the version updated on Oct 7, 2019 at https://arxiv.org/abs/1910.00177). Overview:The paper studies the effect of important sampling approaches in the context of budgeted training. They empirically show that important sampling does not provide consistent improvement over uniform sampling. Instead they find that the budgeted training benefits from variety in the sampled introduced by data augmentation.Strengths:++ Both the average accuracy and the standard deviation are reported across 3 runs. This makes the experiment results more statically convincing.++ The literature survey on budgeted training as well as importance sampling is detailed and clear.Weaknesses:-- Inappropriate title. The paper argues that " under budge restrictions, importance sampling approaches do NOT provide a consistent improvement ..." and the useful part is the data augmentation. Then why the title is the importance of importance sampling (instead of data augmentation)?-- The motivation of this paper is unclear.  The majority of the introduction looks like a duplicate related work to me. -- The paper is not well organized. For example, the reason for including core-set selection is unclear. In Sec 4, the authors conduct experiments by adapting importance sampling approaches in the setting budgeted training. Therefore, I don't see any necessity to spends two paragraphs in both introduction and related works on discussing core-set selections.-- Flaw in the formula: For example, Eq (2) is wrong: If the prediction $ h^k_\theta $ is exactly the same as the gt $ y_i $, then $ p^t_i $ is being maximized but this is the easiest example (therefore $ p_i $ should have be minimized). Indeed, in Chang et. al. (2017), what they used was $ P_S(i \vert H, S_e, D) \propto 1 - \bar{p}(y_i \vert X_i) +\epsilon_D $. There is a negate in the front. This paper focuses on contagious disease prediction with the consideration of observed data bias and patient exposure. The authors present a Model for Infections under Incomplete Testing (MIINT) and the experimental results show the proposed model outperforms baselines on *some* metrics.Problems:1. The paper is not well-written and pretty hard to follow.  - It is unclear why the authors use w^1(X) in eq (1). Why should t be set as 1?- It would be better if the authors gave the definition of \hat{w}^t_i in eq (2).- Q(D^t_1) is a set of imputed distribution. It is unclear how to generate Q.- It would be better if the authors use consistent notations in different places. such as f(X,Y) and f(X^t, Y^{t+1})2. The authors want to consider the exposure relationships between patients. An intuitive solution is graph-based models. It would be better if the authors compared the proposed model with graph-based models, e.g. [1]. Moreover, exposure states are not fully considered. The authors just simply use the count of exposure of observed true infection patients at the last time point. The exposure before two time-points is ignored.[1] Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. "Semi-supervised learning using gaussian fields and harmonic functions." Proceedings of the 20th International Conference on Machine learning (ICML-03). 2003.3. In Table 1, the two simple baselines (OM, NEM) and POM outperform the proposed model a lot on AUPRC. It is worth explaining why. 4. Due to missing technical details (especially how to generate Q), it is hard to re-implement the proposed models. It is necessary to provide code and data as supplementary materials. This paper proposes a new procedure for continual supervised learning from a non-IID data stream that assumes ability to maintain some of the stream examples in a buffer and use the buffer to improve updates of a prediction model. The proposed procedure is called Continual Prototype Evolution (CoPE), which controls evolution of prototypes to prevent catastrophic forgetting. Strenghts:- the paper presents very detailed experimental resultsWeaknesses:- the paper is not easy to read- the underlying assumptions about the data stream are not clearly defined. Thus, it remains unclear what problem the proposed algorithm is trying to solve.- the proposed algorithm is a collection of heuristics that are not clearly justified. There is no attempt to provide a theoretical insight about the behavior of the algorithm.- the experiments were performed for one particular synthetic setup on one benchmark data set and the proposed algorithm is compared to a very limited class of baselines. Thus, even the empirical evaluation is not very insightful.- there is no discussion about the computational cost Overall rating:This paper has too many weaknesses and is not ready for publication Summary========This paper investigates the role of pre-training as an initialization for meta-learning for few-shot classification. In particular, they look at the extent to which the pre-trained representations are disentangled with respect to the class labels. They hypothesize that this disentanglement property of those representations is responsible for their utility as the starting point for meta-learning. Motivated by this, they design a regularizer to be used during the pre-training phase to encourage this disentanglement to be even more prominent with the hope that this pre-trained solution is now closer to the optimal one, thus requiring less additional episodic training which is time-consuming. They show experimentally that their modified pre-training phase sometimes leads to better results as an initialization for Prototypical Networks compared to the standard pre-trained solution, and sometimes converges faster.Pros====The topic of study of this paper is very interesting. I definitely agree that the role of each of the pre-training and meta-learning phases are not yet well-understood, and making progress on understanding this will shed light on the most promising directions for few-shot classification.Also, using the Soft-Nearest-Neighbor-Loss is an interesting property to measure (and to try and reinforce) in the pre-trained representations.Cons====[A] The biggest weakness of this work, in my opinion, is the lack of connection with previous work that is very similar. Specifically, the property that is referred to in this paper as disentanglement is very related to previous notions that have been studied in this context of pre-training for few-shot learning. Specifically, [2] argued that the success of the cosine classifier during pre-training (compared to a standard classifier) is due to explicitly minimizing the intra-class variance of each class (which leads to better clustering, and to better disentanglement in the way that the term is used in this paper).Pushing that direction further, [1] proposed regularizers whose purpose is to directly encourage the pre-training phase to have this property of better clustering: minimizing the intra-class variance and maximizing the inter-class variance. This paper should be discussed as related work and should be compared to experimentally since their approach is very similar to the one proposed here.[B] Unfortunately I also found the writing to be of poor quality. Some minor grammatical or wording errors did not distract me too much from understanding the intended meaning, but there were certain statements which I found hard to understand, or disagreed with. Some examples are below:Due to episodic training, meta-learning methods generalize better than traditional transfer-like methods for the novel classes. The jury is actually still out on this, so I dont think its appropriate to make this claim. Better generalization was the motivation of episodic models, indeed, but in practice non-episodic approaches have been shown to perform quite well, as pointed out in this paper too.Episodic sampling is time-consuming. Can you explain why that is? I dont disagree (based on my experience too) but I dont believe it is obvious, and it would be useful to explain this.In the previous section, we conclude that the last layer in the backbone would be more disentangled after episodic training. Its unclear to me how that conclusion follows.[C] Another weakness is that the proposed method does not perform too strongly compared to the baselines / previous methods. It seems that the gain is larger for smaller architectures, which is in line with the observation in [2] that minimizing intra-task variance is most beneficial for small backbones. On the other hand, for larger architectures, RP-Proto is not better than plain proto (notice the overlap in the confidence intervals in the respective entries of Table 1). [D] I also found the experiments to be weak in terms of the analysis of disentanglement during pre-training and episodic training. Contrary to the authors observation, it doesnt really seem to me that the disentanglement loss is going down too much during episodic training by looking at Figure 1. The conv and resnet18 curves are mostly flat. The resnet10 one does go down noticeably but then starts going up again. Im also not sure what causes the large discrepancy in the behavior of the resnet10 curve compared to the other two? My initial thought was network capacity, but resnet 10s capacity is in between that of the other two networks if I understand correctly, so its hard to draw a conclusion there.Overall======I vote for rejection of this paper in its current form, mostly due to the missing comparison with the very related method mentioned above, the quality of the writing and the weakness of the experimental results, as described above.Suggestion for additional experiments============================Table 1 shows that MetaOptNet outperforms RP-Proto (Im looking at the entry of RP-Proto with the ResNet12 backbone for an apples-to-apples comparison with MetaOptNet). I would be curious to see an RP-MetaOptNet variant too. More generally, does the proposed regularizer also lead to improvements in episodic approaches that are closer to state-of-the-art compared to Prototypical Networks?Further, as an additional data point, it would be useful to also report the performance of the pre-trained network itself on the few-shot test tasks (without a meta-learning phase at all). For an apples-to-apples comparison with the reported Prototypical Network variants, Prototypical Networks can be used to solve each test task still, but operating directly on top of the representation learned from pre-training, instead of the representation produced by the episodic phase.Additional comments for fixing minor issues and improving clarity===============================================Below are a few more recommendations and singled-out sentences that I think should be re-written to improve clarity.In the Mixed Framework section, [3] should also be cited among the papers that used a pre-trained solution as the initialization for the meta-learning stage as this is how the meta-learners in that paper were trained as well.The provided reference for fo-MAML is incorrect. fo-MAML was actually introduced in the original MAML paper (Finn et al, 2017). The provided (Nichol et al., 2018) reference introduced Reptile, which is similar but not the same as fo-MAML.Episodic training is key to make meta-learning prominent. Im not sure what this sentence means. Are there other ways of meta-learning in this context without episodic training?The description of optimization-based meta-learning: [...] try to get an embedding that could easily fit subtasks by adding some extra layers. This is not entirely accurate. MAML, for instance, does not add any extra layers per task. Instead, the entire network is rapidly fine-tuned within each task as well as meta-learned across tasks.References=========[1] Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks. Goldblum et al. ICML 2020.[2] A Closer Look at Few-shot Classification. Chen et al. ICLR 2019.[3] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. Triantafillou et al. ICLR 2020. This work proposes to use cascade decision tree models to come up with shorter explanations for the predictions made by the decision trees.The proposed technique focuses on explaining one class in a binary classification task, i.e., positive samples. The idea is to build a decision tree with predefined depth to classify positive samples, remove those classified positive, as well as negative samples in leaf nodes that are dominated by negative samples, from the dataset, and then repeat this process until the sequence of tree models is built.Explainability of ML models is an important topic, especially for medical scenarios. In addition, shortening the decision tree paths to improve explainability is a promising direction. The writing of the work is also clear and easy to follow.Having said that, I have the following concerns about this work:W1. The application scenario is narrow. And it is not clear why the explanation path is not the concatenation of all the paths of the cascading trees.D1. The proposed approach only applies to binary classification task. It is not clear how this can extend to multi-class and regression models.D2. Also, even for binary classification, it only explains one class. It is counterintuitive that you need to build different models to explain the classification of the two classes from the same dataset.D3. Since the process of building a subtree is independent of the previous subtrees built, i.e., the features and splits of the tree being built does not consider the features and splits in previous subtrees, it seems to me that the subtree is pre-conditioned by the previous subtrees. It is not clear why it is correct to only use the subtree that the prediction ends for explanation instead of all the subtrees that are used before the prediction ends. W2. The complexity of this decision tree can be high, leading to high inference time. And the model can be overfit to positive samples.D4. Because the cascading model tries to construct leaves with most pure positive samples, the total depth and the number of trees in the model can be quite high. This will especially impact the overhead in the inference.D5. Also, given this tree is built for optimizing the classification of one class with very high accuracy, the model can be overfit to that class, and the prediction accuracy of the other class is not guaranteed.W3. The evaluation needs to be enhanced.D6. The average path length of classic decision trees is already small, i.e., less than 4. The evaluation needs to perform on more complex datasets.D7. In medical scenarios, we would expect the negative data samples are much more than positive data samples. It is already easy to overfit for the positive samples in this scenario, and as mentioned in D5, the technique itself also tends to overfit. Overfitting may not be a good idea for this skewed dataset.D8. It is unclear if the proposed technique gives better explanation without doing a user study. As mentioned in W1 and D3, it is not entirely clear why the explanation does not need to concatenate all the paths in the subtrees before the prediction ends. This is especially a concern since the classic decision tree only has a single tree. It will be great to conduct some user study to understand why the proposed technique impacts the Explainability. # SummaryThis paper introduces a new type of classification model called the "cascading decision tree." The cascading decision tree is a rule-based classifier designed to have an overlapping hierarchical structure between its nodes to produce succinct explanations. The paper introduces these models, presents an induction algorithm to learn them from data, and includes an empirical evaluation on three UCI datasets as well as a propietary dataset. The submission includes code.# Pros1. The paper introduces a new kind of classification model. This model form is a contribution in and of itself (i.e., regardless of the algorithm used to fit cascading decision trees from data). 2. The paper highlights an innovative approach to the design of machine learning models  i.e., training models that are constrained to have particular "explainability" properties.# Cons3. The cascading trees produced by the algorithm in this work have little to no formal guarantees regarding their optimality or generalization produces. It is unclear if this is the best way to learn cascading decision trees.4. The paper does not provide a pruning routine. The authors suggest that the algorithm can be paired with any generic pruning method. However, the empirical results do not showcase how the trees perform after pruning. 5. The experimental section is lacking in multiple ways. Ideally, this section should include comparisons on more than three datasets, and consider other baseline models such as "rule lists" (i.e., a special kind of decision tree) and sparse linear models (i.e., a type of model that does not require explanations). Finally, I would recommend the authors to include a plot that shows the distribution of explanation depths for all the examples in a dataset. This would allow readers to have a far better understanding of how each method affects the explanation depth (as compared to a comparison of the means).6. The paper does not make a strong case to motivate why "shorter explanations are better." This is unfortunate given that succinct explanations are the primary motivation for using cascading decision trees. At a minimum, the paper should include a clear demonstration the advantages of using succinct explanations in a modern application. Ideally, this would include: (i) comparisons of the explanations produced for the same point by competing methods; (ii) a study of how the properties of explanations change based on other relevant phenomena (e.g., explanation depth for seen/unseen points).# RatingOverall, I was convinced that "cascading decision trees" were a valuable model class. I was also convinced that this work was valuable in that it highlights a novel approach for supervised learning (i.e., training models with explicit constraints on explainability such as in https://arxiv.org/abs/1703.03717)My current rating (3) is based on the fact that the submission fails to analyze, validate, or motivate cascading decision trees sufficiently. Ideally, the paper should include a thorough analysis of the tree induction algorithm (as discussed in 3 and 4) as is standard in other work on decision trees. It should include more robust evidence that the proposed method produces succinct explanations (as discussed in 5), as well as a convincing demonstration of the utility of succinct explanations in modern applications (as discussed in 6).# QuestionsQ1. How does one measure the "quality of a cascading decision tree"? Is it only in terms of "explanation depth?"Q2. Did you use a pruning routine in your experiments? In this paper, the author proposes Meta_abd which is a hybrid model that learns a deep recognition model and FOL rules the same time. The goal of this work is to learn FOL rules from raw data such as digits presented in image patches in an end-to-end fashion. The model is evaluated with 3 induction benchmarks associated to the MINST digit dataset.Personally, I find this paper to be difficult to read and many details in methods and experiments are missing, making it hard to understand the authors' contribution. Given its current state, I would recommend rejection. My concerns are as follows:Motivation:- Neural-symbolic integration usually refers to combining logic reasoning into deep model's decision process for better interpretability or sample-efficiency, or to use deep models to help the logic reasoning tasks such as ILP or deduction.- With that being said, I find the claim for this hybrid model to be unjustified. In Meta_abd, NN is only used for data pre-processing which is completely agnostic to the later logic component.- In fact, diff-ILP (Evans & Grefenstette, 2018) also uses NN for pre-processing MNIST digits for ILP task which I personally find to be similar to the proposed method, though I'm happy to be proven wrong.Claims: I find many of the claims in the paper to be ambiguous and lack justifications- In section 2, the author claims that differentiable ILP methods rely on fully trained NN for pre-processing -  this is untrue, for example NeuralLP is an end2end model that can be extended to the MINST benchmark with a perception module that's jointly trainable- The author also claims that most existing NeSy systems only utilize a pre-defined knowledge base. I find this claim to be confusing and the author does not discuss how the proposed method has addressed this limitationMethod: I find the method to miss many details- The author defines the learning problem with Eq1 and Eq2 in Section 3.1 and claims to it would be learned through EM. However, Eq1 and 2 do not reveal details about the method, and the exact procedure of EM is unclear to me- In section 3.3 the author claims the proposed method is an extension of MIL, but this concept is not formally introduced in the paper.- I find Figure 2 to be difficult to understand - why is Prolog used here? What's the connection of Prolog to the proposed method? It seems to suggest the proposed method is using Prolog for solving the constraints?Experiment: - The proposed method is only compared to LSTM and RNN.  The author should include some ILP baselines such as diff-ILP or NeuralLP and substitute the digit image into its ground-truth digit symbol for reference, though I personally think adding a NN pre-processing module is straightforward  as well.- 3 benchmarks are fairly small consisting of only a few predicates and rules. How does the proposed method scale with the number of predicates and the size of the grounding space? This paper proposes two fully-connected layers based neural graph pooling methods for graph neural networks, named Neural Pooling Method 1 and Neural Pooling Method 2. The first method uses a first FC to reduce the feature dimension and then FC2 to compute the weights to do weighted-average over features for different nodes. The second method uses two FC to reduce the dimension and then compute second-order statistics by Flatten(H^{\top}H). Experimental results on four datasets (PTC, PROTEINS, IMDB-BINARY, IMDB-MULTI) of two tasks (bioinformatics, social networks) show that the proposed graph pooling method can improve the performance by 0.5%-1.2% accuracy while decreasing the std. Strengths:- The proposed method is simple and motivated by several limitations of current graph pooling methods such as average and summation, DIFFPOOL, SORTPOOL, TOPKPOOL, SAGPOOL, and EIGENPOOL. - The proposed approach is simple and the experimental results can deliver improvements on several tasks and datasets. Weaknesses:- My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second-Order Pooling for Graph Neural Networks (Ji and Wang, 2020) - Based on the author's writing, it is unclear what is the second-order statistics for graph pooling, why it is important to have second-order pooling, and how the proposed method can capture the second-order statistics. - The proposed graph pooling method is only experimented with 1 underlying particular choice of GNN (Xu et al., 2019), so it is unclear how well the method can perform on other GNN architectures. - The four datasets only have 2 or 3 classes and upto 620 nodes. So it is clear how well the method can generalize to large-scale graph classification problems.  - The improvement of the proposed methods compared with SOPpool is marginal. For example, On PROTEINS, the accuracy is improved by 0.5% with the same std. On other datasets, the improvements are only at most 1.2%. To show the proposed approach is better, more datasets or tasks should be used. For example, there are five bioinformatics datasets (MUTAG, PTC, PROTEINS, NCI1, DD) and five social network datasets (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K).- There is not enough discussion and analysis of the results. Especially, there should be some analysis to compare the method 1 and method 2: For different datasets, when one method is better than the other? Some examples would be helpful, too.  - While the author explains the proposed method has lower complexity, there is still no formal analysis or quantitative measures of running time from experiments. - The writing can be improved, In the abstract and introduction, the author should describe the approach briefly and explain its characteristics including why it can handle variable number of nodes, invariant to isomorphic graph structures, capture information of all nodes, and especially why it can collect second-order statistics. - Furthermore, there is a lot of repetition of problem statements. The problem and notation is introduced formally in section 3.1, but is repeated again and again at the beginning of section 3.2 and section 3.3 Questions:- Do both of your method 1 and method 2 capture second-order statistics? My understanding is that only method 2 captures second-order statistics by computing Flatten(H^{\top}H). Is this correct?- How do you compare your method with SOPpool (Ji and Wang, 2020)?- Have you tried other datasets or other tasks?- Have you tried your graph pooling approaches on other underlying GNN models?- Is your standard deviation in Table 2 based on 1 run of 10 folds or multiple runs of 10-fold cross-validation? Minor:- Please give better names for your approaches and give a better title. "Neural Pooling Method" is too general and thus not particular enough to summarize your method. In this manuscript, the authors propose two novel methods using fully connected neural network layers for graph pooling, namely Neural Pooling Method 1 and 2. compared to existing graph pooling methods, the authors think their methods are able to capture information from all nodes, collect second-order statistics, and leverage the ability of neural networks to learn relationships among node representations, making them more powerful.Pros :1. This work studies an important topic but less explored topic, graph pooling.2. Propose to perform graph representation learning with Neural Pooling. 3. Experimental results are interesting.Cons:1. The main concern is the lack of novelty, and the technical contribution is very limited. The essential difference between the authors manuscript and this article is unclear -- Z. Wang and S. Ji. Second-order pooling for graph neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.? Just one more layer? And there is a lot of overlap in content with this article.2. It is actually a stealth exchange of concepts that the authors attribute the success of the methods to the neural networks. On the one hand, the reason $H^{'^T}Q$ or  $H^{'^T}H^{'}$ can capture topology information is not that the neural networks can learn $H'$ that contains topology information, but that $H$ itself contains local topology information and $H^{T}H$ is capable of capturing second-order statistics. In fact, the neural networks dont use topological structure. They play a vital role in reducing dimension and parameters, which is proven in Section 6. On the other hand, $Q$ can be thought of as the weight of the node, not as the correlation among the node representations. Therefore, the statement Neural networks can learn the correlation among the node representation lack of further explanation.3. About the experiment of this manuscript:a.Several advanced pooling methods are ignored, especially EigenPooling. Why does it appear in the method comparison in Section 4.3, but disappear in Table 2.b.Datasets are not enough. Just 4 of the 9 data sets in this article are used -- Z. Wang and S. Ji. Second-order pooling for graph neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.4. About the clarity of this manuscript:a.Figures 1 and 2 are not beautiful and have minor issues. $H^{'^T}Q$ and $QH^{'^T}$ are not equal, which can be corrected by slightly changing the diagrams.b.Repeat a paragraph and an equation many times. For example, in Sections 3.1, 3.2, 3.3 and Section 6, there are redundant.c.The lack of punctuation, such as the end of the second paragraph of Section 1 on the first page, and the end of the second last paragraph of Section 3.3 on page 5, etc.d.Many long sentences make understanding difficult. For example, in section 3.2, there is only one long sentence in a paragraph.  ## Review### SummaryThe authors propose an alternative explanation to the fact that Convolutional Neural Networks (CNNs) learn oriented bandpass filters. They suggest that it is due to the architecture of neural network and not the structure of natural images.The two main contributions are:1. the alternative explanation2. empirical support that oriented bandpass filters are learned at each layer### Strengths* The paper is well written and easy to follow.* This is a good summary of Linear Operators and Fourier Analysis for non-expert### Weaknesses* To me, the contributions are not real. The paper intends at formulating a problem where there is none: the theory is known already and empirical evidences already exist (the repeated observations: yes CNN learns oriented bandpass filters and it is not a surprise). * It mixes contributions such as Olshausen and Field (1996), Karklin and Lewicki (2009) with the litterature of CNNs and the theoretical work of Bruna and Mallat (2013). The first two papers show how localized oriented bandpass filters emerge from unsupervised learning of a non-convolutional dictionary under sparsity constraints. This is a strong indication that these localized bandpass filters are appropriate to represent natural images and a strong confirmation for the use of convolution (a translation invariant representation). It appear that this is also what we observe when evaluating neuron receptive fields. Finally, the work of Bruna and Mallat pushes further the idea of invariance, trying to find a theoretical explanation of why stacking Linear and Non-linear operation provide an efficient representation. They build on wavelet transforms (oriented bandpass filters) because it is a translation invariant representation known to be sparse.* The theory is not rigorously presented. This is a good enough overview of a graduate course on Fourier analysis and linear operators.* Empirical evidences are not convincing. How would you get if you were fitting random 3x3 filters with oriented bandpass filters ?### Minor comments* The text in the figures is too small... The authors present a method that combines Monte Carlo tree search (MCTS) and random rollouts. The authors their relate this to the bias-variance tradeoff observed in n-step temporal difference methods. The authors evaluate their method on Sokoban and the Google Football League environment. The results show that the authors' method leads to marginal improvements on these domains.I do not think what the authors are doing is very novel as MCTS combined with rollouts was already used in AlphaGo. Furthermore, I believe the small difference in results can be made up by using only MCTS with a different exploration parameter (i.e. like the one that was used in the AlphaGo paper).I would like to know what benefits this method brings that cannot be obtained from combining MCTS with rollouts as in AlphaGo or from a hyperaparameter search with MCTS. Is there an anaylsis of the bias variance tradeoff of this method? summary:This paper introduces Shoot Tree Search (STS), a planning algorithm that performs a multi-step expansion in Monte-Carlo Tree Search. Standard MCTS algorithms expand the search tree by adding one node to the tree for each simulation. In contrast, the proposed STS adds multiple nodes to the search tree at each simulation, where each node corresponds to the state and action that are encountered during rollout. By multi-step expansion, the evaluation of the trajectory is less-biased, which can be analogous to n-step TD. In the experiments on Sokoban and Google research football domains, STS outperforms baselines that include Random shooting, Banding shooting, and MCTS.Overall, my main concerns are technical novelty and presentation quality.The most common MCTS methods assume that the leaf node is expanded one at a time in each simulation (and its evaluation is performed either by rollout policy or by function approximator), but this common practice does not necessarily mean that MCTS should always do that. The main reason for only expanding one node per simulation in standard MCTS is memory efficiency: if we fully expand the rollout trajectory and retain its information to the search tree, we may get slightly more accurate value estimates. However, the nodes located deep in the tree will not be visited more than once in most cases, thus its effect is usually not significant, leading to the common practice of one-step expansion. More importantly, multi-step expansion has already been used in existing works (e.g. in [1], the tree is expanded by adding the whole rollout trajectory), thus I am not convinced that this work introduces a technical novelty.It seems that the relative benefit of the STS over MCTS observed in the experiments comes from the bias of the value function approximator. However, to show the effectiveness of 'multi-step' expansion compared to 'single-step' expansion, I think that more thorough ablation experiments should have been conducted. For example, we can consider the setting where both STS and MCTS perform leaf-node evaluation (i.e. UPDATE in Algorithm 5) by executing rollout policy rather than by using value function approximator. By doing so, we can focus only on the benefits of STS's retaining information of full rollout trajectory (i.e. multi-step expansion), compared to MCTS's retaining one-step information (i.e. single-step expansion) while eliminating the effect of biased value function estimation.To relieve too much bias in the current MCTS's leaf node evaluation, mixing MC return of rollout policy and the output of the value network could also have been considered, as in AlphaGo (Silver et al. 2016). It would be great to see if STS still has advantages over MCTS in various leaf node evaluation situations.Also, more writing effort may be required, and the current version of the manuscript seems premature to be published. There are some unclear or questionable parts.- Algorithm 3 and Algorithm 4 are not the contributions of this work, thus they can be removed or moved to the Appendix. Instead, more discussions regarding the proposed method should have been placed in the main text.- In Algorithm 2: the definition of CALCULATE_TARGET is missing.- In Algorithm 5: In SELECT, the tree policy is defined by CHOOSE_ACTION that selects purely greedy action. If this describes the MCTS used in the experiments, I would say this is wrong. To make MCTS be properly working, an in-tree policy that balances exploration vs. exploitation is required (e.g. a classical choice is UCB rule).- In Algorithm 6: In UPDATE, $N(s,a)$ and $quality$ are increased by $c$ times more, which means that the longer rollout length, the more weight is given. What is the reason for assigning more weight to the trajectory that has a longer rollout length? If the entire planning horizon is limited to finite length, this means that early simulations (short $path$ length, long $rollout$ length) have more weight than later simulations (long $path$ length, short $rollout$ length), but I do not think this is desirable. Is my understanding correct?- For the Sokoban experiments, the pre-trained value function would significantly affect the performance of MCTS and STS, but I could not find the way how the value function was pre-trained.- In Appendix A.2., the hyperparameters for Shooting and STS are very much different. Why did you set Shooting's hyperparameter differently from STS (e.g. VF zero-initialization, action sampling temp, etc.)?- It seems that the choice of zero-initialization of the value network is rather arbitrary. I am not convinced that this would always work better. In some situations, optimistic initialization of the value network may be helpful to encourage exploration of the uncertain state regions.- In Table 2, Why does RandomShooting-PPO underperform PPO? Since RandomShooting-PPO puts additional search efforts upon PPO, I expected that RandomShooting-PPO must outperform PPO.- Table 5 could have been moved to the main text, replacing Table 2.[1] Soemers et al., Enhancements for Real-Time Monte-Carlo Tree Search in General Video Game Playing, 2016 IEEE Conference on Computational Intelligence and Games (CIG 2016) The architecture of the tracker is standard siamese. The novelty is at a technical level, modules of the "cross-guided" type have been proposed. It does bring an improvement, but not to the state-of-the-art level.  There is no significant insight, training, updating novelty or theoretical. Recent short-term trackers output segmentation, the proposed tracker outputs a bounding box.The performance of the tracker is evaluated on UAV, VOT 2018 and VOT 2019. UAV is saturated. The performance VOT 2019 is worse than state-of-the-art, which is not reported -- the trackers selected for comparison do not include the best performing ones. It is not clear why  VOT 2020 was not included. Overall, this is "yet another siamese tracker", which is not sufficient for ICLR acceptance. This paper present an experiment of safe reinforcement on a 2D grid-word where the safety constraints are specified in natural language instead of being specified formally. The justification of this system is to allow non-experts to train agents using safe-RL.According to the authors: "The key challenge lies in training the agent to interpret natural language and naturally adhere to the constraints during exploration and execution".  The proposed system is made of two parts: a constraint interpreter that is (mostly) trained in a supervised way with Amazon Mechanical Turk to translate natural language orders into grid-world ad-hoc constraints and a policy that is trained through PCPO, a TRPO-like constraint-aware policy optimization algorithm.That's certainly a nice piece of engineering, but honestly my first sentiment when reading the motivation of this paper was astonishment. If the agent is already able to understand complex natural language statements about its environment during its exploration/training phase, why would it need further training to apply these constraints on a simple grid-world ?If the natural language understanding (NLU) task has to be handled before the agent's exploration/training phase, we are facing a concatenation of two problems: NLU then Safe-RL, not a new problem involving tightly NLU and safe-RL.Even if some publications were already made on that topic, training a agent to properly follow natural language orders, that may include constraints, during its execution phase is yet an unsolved problem that requires a real fusion between NLU and RL that is of scientific interest. The authors propose a deep RL solution for the communication problem of user scheduling and resource allocation. The deep RL solution uses deterministic policy gradient + quantile regression + dueling + deep sets and the authors demonstrate it outperforms classical solutions on benchmark tasks. Recommendation:I quite frankly have no prior knowledge of the task this paper aims to solve. It didn't come across as a grandstanding AI challenge in any capacity (feel free to debate this) so I'm looking at this paper from the angle of significance to the deep RL community. In general, the questions I'm asking are: Does this paper introduce a novel deep RL algorithm? Is the knowledge produced by this paper generalizable to other problems or algorithms? Does this paper provide value to anyone who isn't concerned with the specific application? As of now, I feel like the answer is no to all three and so I would recommend rejection to this conference. However, there are presumably venues which are a better fit and I hope the authors consider submitting elsewhere. There were other issues with the paper. In general, clarity and organization were a big issue for me. The authors were very rigorous with their supplementary material, so I believe most of the information is there, but was not presented in an easily digestible manner. Strengths:- Thorough supplementary material and code is provided. - The use of deep RL to tackle the problem felt well-motived and a good fit. - The performance of the agent seems strong but I'm not clear on the significance of some of the results. Weaknesses:- I think the problem set up was clear in the sense that I understood the overarching objective. However, the specifics of the problem, specifically in the context of RL was not. The paragraph about 3.2 is a generic description of the RL problem and left me wondering the connection to the actual application. I realize many of the details are contained in the supplementary material but the statement "the problem can be modeled as an MDP" was not defended & the following description did not clarify the problem statement. For example, immediately after, in 3.2, "high variance randomness" is discussed but its not clear to me why this is the case or how this randomness affects the problem- reward? transitions?- The structure of the paper did not feel helpful to me. 3.2 is categorized into "Policy Network" and "Value Network" for somebody who is comfortable with RL a lot of the details felt unnecessary but more importantly, this organization doesn't provide a solid presentation of the algorithm. Somebody who is interested in the application and is not an RL expert, won't necessarily follow why "Policy network" is being presented or what that even means necessarily. There isn't a clear overview of the algorithm. - Novelty of the algorithm is low in the sense that it is a combination of prior, existing ideas. - I felt like many of the algorithmic choices were not well-justified. For example, the use of QR is justified by an analogy? The use of the dueling architecture also seems unusual when the authors also propose a much simpler solution. It was also unclear what the issue with "The main problem was that the distribution Z was far away from 0 making it very difficult for the policy network to well approximate them" exactly meant. Minor Comments:- There are a few latex issues with reversed quotations. - "In Figure 2 we provide additional element to support the choice" -> an additional- The objective of Figure 2 is nice but its confusing to have two sets of experiments presented in the same graph. Class is not explained in the description of the graph and the significance of the graph is not explained in the figure description. SummaryThe paper introduces improved deep learning architecture for solving stochastic differential games by fictitious play. Compared to previous best model it uses LSTM instead of MLP to capture forward dynamics of the system, importance sampling and order invariant encoding of other agents to improve sample efficiency of learning.Strong points* Order invariant encoding uses inherent structure of the problem that improves sample efficiency compared to previous approaches. * Batching of some operations speeds up the algorithm.* Two different domains are used to evaluate the model.Weak points* Time complexity claims are problematic.* The paper is sometimes difficult to follow. It seems like 8 pages aren't enough to describe the whole approach in detail. Many things in appendix seem central for the presented system. * Importance sampling that is claimed to be important ingredient isn't described well enough neither in the main text nor in the appendix.* Quantitative evaluation is presented only for domains that can be already solved analytically. In other domains only qualitative results are presented.RecommendationI recommend rejecting the paper due to lack of clarity and lack of evaluation against other methods suitable for solving problems that can't be solved analytically.Questions* It seems to me that "propagate by batch" time complexities in Table 1 assume constant time computation of batches. Is that correct? That would however be a simplification, while batched computation is much faster on GPUs it isn't in general constant time. This view is further supported by Fig 4 where empirical time complexity also isn't constant as is suggested by Table 1.* My understanding of empirical results is that the model is run in domains with analytical solution only as sanity check. The true value is in domains without analytical solution (e.g. Superlinear Simulation section). Current evaluation for superlinear setup and autonomous racing is only qualitative. I would like to see comparison to previous methods applicable to these domains if there are any (numerical solvers?). If there aren't any the paper should explain why this is the case. I understand that the model is an improvement over Han & Hu however does it allow us to do something we couldn't do before when we consider even non learning methods? Please reflect on this point to help me better understand main contribution of the paper. This broader context would help readers (like me) that aren't experts in stochastic differential games.Possible improvements* Since most of the results compare presented model against Han & Hu 2019 it would help to add a paragraph that briefly summarizes algorithmic/model differences. Is it true that current system is Han & Hu with LSTM instead of MLP + importance sampling + invariant layer? * In table 2 unit of "Total time" isn't specified.* "memory complexity of LSTM with respect to time is O(1)" --- this is true for inference time, at train time O(T) activations still have to be stored. (If they aren't to be recomputed again for each t < T). The text can be more clear on that.* In Figs 5 and 6 captions say that it compares current system against Han & Hu however the legend shows FC and LSTM with and without invariant layers. I assume that Han & Hu = FC and current system is LSTM + invariant layer. However being more explicit about that would  help. Does FC model here use importance sampling or not?Typos* solutions exist only few special -> only FOR few* stochastic optional control problem -> OPTIMAL control* drive fast than -> drive fastER* global augment -> augmentED* The paper would benefit from further proofreading.  ##### Summary ##### This paper investigates conditions under which communities of cooperative agents are stable. Communities in multi-round bargaining games with evolutionary dynamics are evaluated in three main setups. The first imposes no restrictions on the agents' behavior and is shown to be easily invaded by deceitful agents. The second enables agents to refuse to bargain with deceitful agents. Nevertheless, such communities are shown to be invadable. Finally, in the third setup, a global punishment system is shown to be able to drive out deceitful invaders. The main take-home message is that, when lying is an option, agents(' communities) need to be prepared for it.  ##### Reasons for score #####I vote for rejecting this submission. The main reason, further detailed in "Cons" below, is that I am not convinced that the problem this manuscript addresses is not an artifact of the rather strong assumption of selfish individuals and communities driven solely by functional pressure. I agree with the authors that we need to start looking at more naturalistic setups (e.g., communities instead of two-player games). However, this also relates to the evolutionary dynamics we take into consideration, and to a careful motivation of the setups we analyze.     ##### Pros #####+ Language emergence is a topic that has drawn renewed interest from multiple disciplines. Studying the conditions under which agents' acquired behaviors are (not) stable in a dynamic setting is important to understand their underpinning.+ The setups are well explained and easy to follow   ##### Cons ######- Much of the motivation of this paper relies on the argument that there are strategies that functionally dominate cooperation. However, the view that functional pressure is all there is to evolution is rather outdated. If the authors wish to stick to this line of argumentation, I encourage them to give strong arguments for why we should disregard other factors, such as transmittability/acquisition (e.g., Kirby et al. 2015, "Compression and Communication in the Cultural Evolution of Linguistic Structure", or Brochhagen et al. 2018, "Coevolution of Lexical Meaning and Pragmatic Use") or neutral models (e.g., Reali & Griffiths 2011, "Words as alleles: connecting language evolution with Bayesian learners to models of genetic drift.", or Perfors & Navarro 2014, "Language Evolution Can Be Shaped by the Structure of the World"). Along the same lines, the view that functional pressure needs to apply to only individuals, and that these are fully selfish, also requires evidence or needs to be clearly marked as a rather strong assumption. The claim that "[...] in order to ensure the survival advantage of many competitors, animals are selfish in nature" flies in the face of the whole line of research on animal alarm calls (e.g., Zuberbuehler 2009, "Survivor Signals: The Biology and Psychology of Animal Alarm Calling"); and, for that matter, the existence of any social animal. In sum, as it stands, I am not convinced that the problem this manuscript addresses is not an artifact of the assumption of selfish individuals and communities driven solely by functional pressure.- Terminology is very sloppy at times. What is "successful evolution" (p. 1) or "naive natural selection" (p.8)?- Writing could be greatly improved. There are a lot of typos and borderline grammatical sentences- References are wrong (e.g., Lewis' work on convention is not from 2008 but from 1969. The same goes for all the references on seminal work in game theory)- Many assumptions in the setup are not motivated. For instance, is there a reason to use a discrete dynamic over a continous one? (p. 3) Why use a multi-round bargaining game to model language evolution by contrast to, say, an established signaling game-style setup with conflict of interests? How is the "credit mechanism" and the "market" motivated/grounded in light of this being an investigation about language emergence? (p. 3)- The lines in Figure 3a and Figure 3b are very hard/impossible to read. Particularly Figure 3b.- The y-axis in the plots in Figure 3 are not aligned, rendering a comparison across conditions hard (and visually misleading) Strength:+ The paper proposes a general framework to deal with constraints in optimization problems using neural networks. In my opinion this is an important problem since there exists no standard method in many existing deep neural network frameworks to deal with constraints, which are also inapplicable even if the constraints are only slightly nontrivial. The paper proposes to deal with equality and inequality constraints differently which may be often easier in large scale settings.+ Comparison with CVXPY indicates that the algorithm can indeed be practically useful, and applicable for a broad spectrum of problems (this is not discussed in the paper).Weakness:- While the related work from the recent years has been discussed to some extent, the paper fails to show how the technique proposed is better than them. The main idea of the paper has a rich history in optimization literature and is referred to as "elimination" of constraints. See Chapter 15 in Numerical Optimization by Nocedal & Wright (2006), and references within for more details. From my understanding, the discussion in Amos & Kolter (and their implementation) makes it clear that a reduced/partial Cholesky factorization is sufficient. This is equivalent to the completion procedure suggested here. Moreover, the correction procedure simply reduces a penalized form of constraints, and is not a projection operation. To this end, the paper does not have any discussion regarding the convergence aspects of the framework which is a crucial subject for this paper.- For a generic optimization framework (proposed), the paper only provides experiments on well studied optimization problems - all the experiments provided in the paper are with quadratic programming problems which are known to be easy both in theory and practice. It will be interesting to see how the framework performs in different problems. Also, it is not clear whether the experimental benefits will carry to other networks that are used in practice. Indeed, running experiments on all known network architectures is feasible but showing experiments on a few more relevant architectures can add a lot of value to the paper and readers. This paper proposes a method to strictly enforce hard constraints during a neural network, without compromising differentiability. The method has two stages 1) From a smaller set of predicted variables, compute the remaining ones so that equality constraints are satisfied; 2) Take a few gradient steps (w.r.t soft constraint) in case inequality constraints are violated. They perform experiments on synthetic and also somewhat applied instances of quadratic programs. The results look very promising.While most of this review will focus on the negative aspect, I want to begin by stating that I like the paper very much.I think it solves an important problem (from a fundamental standpoint but with potentially large impact), it solves it with a method that is non-trivial, yet quite simple (both of which are positives) and shows performance clearly better than that of naive -- but popular -- baselines. The writeup is very nice and the experiments are sufficient.Having said all of that, I cannot recommend acceptance of the paper in its current stage. In several places, the paper claims to have orders-of-magnitude faster runtimes than standard solvers. It is even the only claim of the paper appearing in boldface. Since the time measurement methodology is not described in the paper, I went into the code attached to the submission. Below is a list of reasons why I believe the current methodology is flawed and does not permit making strong claims from the paper.- CVXPy was never developed to be a fast solver. I checked this with the CVXPy authors and often times the solver is not even the computational bottleneck.- Even if other bottlenecks are factored out of CVXPy, there is an option to use an almost-SOTA QP solver on the backend with prob.solve(solver=cp.OSQP), which the authors **do not use**. For these two reasons, it would probably be better to compare against the freely available OSQP directly, without incorporating CVXPy -- (I feel, CVXPy, in general, shouldn't be promoted as a baseline for runtimes)- I didn't go through all the code but the time measurements I found were around large blocks of code, not the pure runtime of the solvers. This leaves a lot of room for inaccuracies.- The baselines optimizers do not enjoy any parallelization over the instances. On the other hand, the authors write (in a footnote on page 11) that their method was timed with *full parallelization* (all instances in one batch). **This is massively unfair to the baseline** and **may fully explain the orders of magnitude difference in runtimes**.- Runtime of its own is a bit of a compromised quantity due to hardware (cpu/gpu) differences, maybe FLOPS would be fairer (but I understand it might not be feasible)There are two ways forward:a) The authors remove/strongly-tone-down all claims about runtimes. In this case, I am ready to dramatically improve my evaluation as I believe the paper is safely above the acceptance threshold even without them.b) The authors insist on claims about runtime. If this is the case, the authors need to address the points made above, update the results, and add a section that demonstrates the soundness of the methodology (possibly into supplementary). In such a case, I am happy to reevaluate but I will still insist on a proper justification of the claims made in boldface. The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.The main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. ### Questions / CommentsThe paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.In this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.The results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?As the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).The description of the baseline they call "Naive" is also not very clear. If would be good to have more details on this baseline.In Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.To summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.### TyposFigure 1b:Comapring -> ComparingTable 1 caption:performs almost the same with naive in Parkinson giventheir standard errors -> grammar should be improved Summary:The authors propose PeVFA: a value function able to evaluate the expected return of multiple policies. They do so by extending the conventional value function, allowing it to receive as input the parameter (or a representation) of the policy. The authors study the local generalization property of PeVFA, propose possible ways of encoding the policy parameters and compare traditional PPO with an extended version of PPO using PeVFA. While the idea of generalization among many policies is an interesting topic in RL, there are many theoretical and experimental issues that prevent acceptance. Moreover, the authors do not at all compare their approach to recent work which also uses value functions with policy parameters as input.Review:Below the major problems I found:- State and action value functions receiving as input the parameters of a policy, which generalize to unseen policies were proposed in a work on Parameter-based Value Functions [1] (PVFs, see version v1 from 16 Jun 2020). Recent work on Policy Evaluation Networks (PENs) proposes policy embedding for a value function receiving as input the parameters of a policy [2]. These works introduce value functions which generalize to many policies. The authors must relate their Surface Policy Representation to the fingerprint mechanism in PENs and introduce their V and Q in the PVF framework.- Since the state space is not finite, $f_{\theta}(\pi)$, the approximation loss of $V_{\theta}$ should be defined considering an expectation over the state space, which is weighted by a distribution over the states (e.g. the on-policy stationary distribution). Since the loss proposed does not include a distribution over the states, it is not clear if the contraction assumption should hold in expectation or for every possible state. The former would imply that all the results should be stated in a probabilistic framework; the latter would be false in a continuous state setting.- The authors assume that the class of value function considered can achieve zero approximation error. Where is this assumption used in the proofs and why is it useful?- Theorem 1 seems quite trivial. It is trivial that if the loss is decreasing in one point and it has some smooth properties, then there exists a close enough point such that the loss decreases also there. Assumption 2 should be stated in a clearer way, differentiating the cases of Lipschitz continuous function, Lipschitz continuous gradient and Lipschitz continuous Hessian more clearly.- Assuming that $\gamma_g$ in Corollary 1 is lower than 1 during the training process is quite unreasonable, and the authors do not check if this holds in their experiments. I expect that for Corollary 1 to hold, the learning rate should be extremely small, thus preventing learning.- In Appendix A.2, there is an additional proof assuming that also $f(\pi_2)$ is Lipschitz. The authors start from the bound in eq (8), which is strictly less tight than the assumption $f(\pi_1) \leq f(\pi_2)$. By assuming only $f(\pi_1) \leq f(\pi_2)$ one would get the same final condition for $f(\pi_1)$, except for the term L_0 which would be zero. Hence the condition could be even less restrictive.- In Corollary 2 it is assumed that the sum of the losses over 2 consecutive policies is lower than the distance between the optimal value function of the two policies. Is there an interpretation of this assumption or is it just a technical requirement to complete the proof? Please discuss why this assumption should hold during the learning process.- When the policy representation is learned, the problem of mapping the policy parameters to the expected return becomes nonstationary, i.e. the same policy representation over time would be optimally mapped onto different values. How is this addressed in the experiments and how does nonstationarity affect the theoretical claims?- From the experiments it is not clear if a representation for the policy is necessary. I would have expected to see a comparison between PeVFA with raw policy representation (RPR) as input and PeVFA using the learned representations. The authors should provide strong evidence that RPR is not enough and the policy representation is needed. Furthermore, the policy used is very small (2 layers and 2 neurons per layer). I would expect to see benefits in using a policy representation when the policy is bigger (e.g. 2 hidden layers, 64 or 128 neurons per layer).- The authors provide no details about the hyperparameters used and about how they tuned their methods. Policy representation learning methods are only in part explained. Without further details, it is difficult to assess if the experiments provided were fair, and it is not possible to reproduce the results.- In appendix B.1. the transition function is not reported. The state includes sinusoidal terms that do not appear in the reward function. Why is it necessary for the agent to observe these terms? Are not just the x-y coordinate sufficient?- In Figure 3 and 7 it is not clear why there is only one learning curve for the policy policy and 6 value functions losses in standard PPO.- Algorithms 1 and 2 make almost no distinction in training when the update is on-policy or off-policy, because the authors consider PeVFA as just a replacement for standard value function when the algorithm is already derived. However, if PeVFA is introduced before deriving the algorithm, the derivation leads to different policy gradient theorems (see [1]). Please discuss this issue.- Figure 8 represents $\pi(a|s)$ for each possible a, s. Therefore I would expect that for each s, the integral over A of the curve is 1. However, the area under the curve is much lower than 1. What is the explanation for this?- Last page in the Appendix is truncated.I think the most interesting contribution of this paper is the proposed policy representation. I would like to see a revised version published in the future. However, a lot of additional work is needed to address the aforementioned problems in the theory and the missing experimental evidence & implementation details & comparisons to very similar related work. Minor:- The title is grammatically incorrect.- "with a limited parameter space" -> what does limited mean? Finite? Or that $\Theta$ is only a subset of the space of value functions?- same for "unlimited"- Appendix C.1 "Advantage Acotor-Critic" -> Advantage Actor-Critic[1] Francesco Faccio and Juergen Schmidhuber. Parameter-based Value Functions. arXiv preprint arXiv:2006.09226v1, 2020.[2] Jean Harb, Tom Schaul, Doina Precup, and Pierre-Luc Bacon. Policy evaluation networks. arXiv preprint arXiv:2002.11833, 2020. *Summary of the paper*: One drawback of the transformer architecture is that they often fail to capture local interactions within the sentence. In this paper, the authors propose the PhraseTransformer architecture which incorporates Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results on three semantic parsing datasets show that the phrase level transformer is better at capturing the local information than the original transformer.*Strength of the paper*:  This paper proposes to use phrase-level information to capture the local dependencies of the transformer architecture and the experimental results show that semantic parsing could benefit from such local dependencies. The authors also conduct empirical experiments -- section 4.3.2 to show why their method is helpful. The paper is well-structured and easy to follow.*Weakness of the paper*: (1) The idea of using different-granularity representation including phrase-level representations of transformer architecture has been proposed before. For example, Hao, et al. 2019 study multi-granularity representations for self-attention in machine translation; Yang, et al. 2018 study the localness of the self-attention mechanism; Nguyen, et al. 2020 study the tree-structured representations of the self-attention networks. The authors should have done a detailed literature review along this line of works. (2) The proposed method is quite empirical and from Table 2 it is unclear how to set the n-gram size for different layers. A more detailed experiment demonstrating how different n-gram sizes in different layers affects the model would be quite helpful. (3) The improvement over the original transformer is marginal except on the Atis dataset, the claims would be more convincing if the authors could conduct similar experiments on other tasks. (4) For the error analysis part, I would like to see a more systematic analysis rather than two examples posted in the paper: is there a specific type of error being corrected by the phrase-level representation? In what scenario will the phrase-level representation help most?*Reason for score*: Overall, I vote for rejecting this paper. I like the idea of trying different-granularity representations for the transformer. However, such kind of ideas has been proposed before and this paper does not bring new insights into using such representations.  *Reference*: Hao, Jie, et al. "Multi-Granularity Self-Attention for Neural Machine Translation." arXiv preprint arXiv:1909.02222 (2019).Yang, Baosong, et al. "Modeling localness for self-attention networks." arXiv preprint arXiv:1810.10182 (2018).Nguyen, Xuan-Phi, et al. "Tree-Structured Attention with Hierarchical Accumulation." arXiv preprint arXiv:2002.08046 (2020). This paper extends Transformer to integrate representations of ngrams for semantic parsing. The key idea is to split input sentences into ngrams of different orders and utilize LSTM to build their representations before feeding them to the layers inside a transformer. The experimental results show that this modification leads to marginal improvement on three benchmark datasets of semantic parsing.The encoders of the previous neural semantic parsing methods only learn the dependencies between tokens but ignore the local context around the tokens. Therefore, to exploit the local context information, this work provides some main contributions: -- This work introduced a novel Transformer encoder to encode the n-grams in each utterance.  -- This work showed the effectiveness of this new model on three benchmark datasets. -- This work displayed the model capacity by visualizing the alignment between the source tokens and the target logical forms, and the similarity of the phrase representations.  The method is evaluated on three datasets, Geo, Atis and MSParS with two evaluation metrics, Exact Matching and Logic Matching. Compared with Exact Matching, Logic Matching is able to compare the variants of the logical forms.Strengths:-- It is a good method to exploit the local context information with the Transformer architecture. And this method seems to be easy to implement.-- There is a thorough evaluation which displays the model performance, and visualizes the attention alignment and the similarity of the phrase representations. Weakness:-- The first thing I am concerned about is the novelty. Although in semantic parsing, there is no previous work that utilizes local context information with a Transformer, there are many similar architectures in machine translation. The proposed Transformer-based model is also totally applicable to machine translation scenarios, which makes it necessary to compare the model performance with the machine translation models that exploit local context as well.-- The second is that the performance is not significant enough. Although this work claims that the performance is superior to the baselines on two benchmark datasets, it should be noted that the other baselines report only the Exact Match accuracy while this work reports the Logic Match performance. Considering only the Exact Match comparison, the proposed method is only 0.4% and 0.02% higher than the baselines on Atis and MSParS, respectively, which is not significant at all. If using Logic Match as the main metric, it would be better to re-evaluate the baselines with Logic Match as well for fair comparison.--  The description of the model in Sec. 3 is not clear enough.How does the model split an input sentence into ngrams? Do the adjunct ngrams have overlapped subwords/characters or not?-- I found this contribution too close to the following work.Hao, Jie, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. "Modeling recurrence for transformer." arXiv preprint arXiv:1904.03092 (2019).-- In addition:---- The citation of logic matching seems to be incorrect.---- The sentence W is parameters, LayerNorm, FeedForward are the functions by proposed by & . is misleading because LayerNorm and feed forward networks are not proposed by Vaswani et al.---- What is model 4 and model 5 in Table 3?---- Figure 5 (b) is hard to readMinor issues and improvement suggestions:-- Compare the proposed method with the baselines which are from machine translation fields which also exploit the local context.-- Improve the model performance. Reasons to accept: -- Proposed an interesting Transformer-based encoder to exploit the local context information.Reasons to reject:-- Novelty is not enough. The evaluation results can not show the superiority of this model. This paper studies if gradient descent will affect the compositionality generalization. It attempts to prove the results by information theory and demonstrated several experimental results. Unfortunately, I think the proof has mistakes, and the conclusion doesn't hold.The major claim of the paper is that the gradient descent tries to use all available and redundant information from input. This is not true. Let's assume that we have two input $x_0$, and $x_1$, and the ground truth is the identity function $f(x_0, x_1)=(x_0,x_1)=(y_0,y_1)$. Now we assume the neural network be a simple linear transformation $f_{\phi}(x_0,x_1)=(a_0x_0+a_1x_1,b_0x_0+b_1x_1)$, and we initialize it with the optimal solution $a_0=b_1=1,a_1=b_0=0$. The partial gradient is then $\frac{\partial y_0}{\partial x_0}=\frac{\partial y_1}{\partial x_1}=1; \frac{\partial y_1}{\partial x_0}=\frac{\partial y_0}{\partial x_1}=0$. The gradient descent will not use any redundant information. As for random initialization case, simple experiments show that the neural network can learn the identity mapping with enough data under the MSE loss (this is obvious as linear regression is a convex problem..). The neural network will have good compositional generalization.The mistake might be in section 4.3. In the proof, the authors assume the output $\hat{Y}_j$ for any $j\neq i$ is fixed given that $X_j$ has information to reduce the loss. However. if $X_j$ correlates with $\hat{Y}_j$, one can't fix $\hat{Y}_j$ as it may change according to $X_j$. $X_j$ can reduce the loss $\mathcal{L}(Y,\hat{Y})$ by changing $\hat{Y}_j$, then $\frac{\partial \hat{Y}_i}{\partial \hat{X}_j}$ can be zero. So, I think it's a clear rejection. Summary-------The authors adapt an existing RL approach to combinatorial optimization to be used for their particular application of optimizing a fleet of UAVs (simulation) to deliver supplies. For this the problem is presented as a graph, a cost function is defined and an optimization is applied. Critique, Questions, and Discussion-----------------------1) In order to follow the explanation better I would recommend describing the task in more detail in 2.1: What are the actions? What is the exact optimization problem that is being solved? Figure 1 shows that the outputs of the architecture are the action probabilities, but the reader still does not know what the actions are and has no clear picture of the problem that is being solved.2) Turns out the paper never mentions what the actions are and does not specify the MDP to which the reinforcement learning approach is being applied.3) It is not explained how the competitor methods (AM) is adapted to the problem at hand.4) I would recommend to clearly state somewhere in the beginning of the paper what are the specific contributions of this work from methodological standpoint.5) It is not clear from the description of the task why formulating the problem as a graph is beneficial in this scenario. How does the distance information, for example, enter the decision-making of the agents? Maybe a clearer explanation of the task and the exact optimization problem would help the reader see it.6) How efficient would be a classical (non-RL) solution on this particular problem? It would be interesting to see some numerical estimation of this that would explain the necessity for heuristics in this problem.Recommendation and justification--------------------------------In my opinion the potential impact of this paper is not sufficient (even if perfectly executed) to be presented at ICLR. This work does not offer methodological novelties, and the particular application is of limited significance as the experiments are conducted on a toy problem, not on the real task that is given as motivation. To the best of my understanding this work is interesting from engineering standpoint as application of RL-based combinatorial optimization to a particular problem, but does not constitute scientific contribution.Additional remarks------------------Typospg 3: "consist of multiple tasks located in different locationS"pg 3: "Therefore its possible" -> it'spg 4: "ThereforE we define a matrix" This paper develops a stochastic MM-type algorithm to minimize a finite sum. Essentially, the stochastic method draws one sample at each iteration, and find a majorization surrogate for the corresponding loss, and find the minimizer for the updated total loss.Overall, I don't find the paper well-developed and doesn't meet the bar of a top conference like ICLR for the following major concerns:1. The major flaw is that in each iteration, the algorithm requires us to find the minimizer of the updated total loss (Step 8 of algorithm 2). This step is computationally as expensive as the update step in a batched MM algorithm. For a stochastic-type algorithm, I would expect the update only finds the minimizer of the stochastically picked individual surrogate function.2. By minimizing a stochastically picked individual surrogate function, the convergence follows by existing literature on stochastic proximal gradient method, there Theorem 2 follows without much difficulty.3. The convergence rate of the proposed method is not derived, which shouldn't be too difficult to derive. ## ReviewGiven as set of pre-specified policies, this paper proposes a Bayesian method to estimate the posterior distribution of their average values, by estimating posterior distributions of their discounted stationary distribution ratios. These posterior distributions are used for off-policy evaluation in various ways. ## Positives+ The idea focusing on estimating nonlinear functionals of multiple policy values is appealing. ## Major concerns+ The results in Figure 2 are a bit surprising. We expect that methods based on concentration inequalities like Bernstein or student-t to be somewhat conservative, but the results suggest that their confidence intervals are extremely wide. For example, in the Bandit case, even after 200 samples, the interval log-width would suggest that Bernstein's confidence intervals is more than 7x the confidence intervals suggested by BayesDICE. What explains these results?+ Again on Figure 2, if "Bernstein" and "Student t" are unbiased methods, then having a very wide confidence interval should translate into over-coverage. However, they seem to be *under*-covering the true value. Are these methods somehow heavily biased? If not, what explains the under-coverage?+ The paper proposes a method for evaluating non-linear functionals of policy values, such as ranking scores over their values. However, it seems to me that in order to evaluate such nonlinear functions one would require knowledge about the *joint* distribution of values over all policies of interest. In the notation of the paper, one would require knowledge of $q(\bar{\rho}_1, ..., \bar{\rho}_N)$. However, it is not clear from the method description in Section 3.2 how one is able to estimate this joint distribution. Instead, it seems to me that all we get is $q(\bar{\rho}_i)$ for each policy $i$ -- that is, their marginal distributions. If that is the correct interpretation of what's going on in Section 3.2, then that raises the question of whether these distributions are independent.  ## Minor concerns+ In appendix C.1, I did not understand the description of the "bandit" environment. Are rewards binary?+ Several symbols are not formally defined. E.g., on page 4, (lowercase) r(s,a) is not defined.+ In Algorithm 1, what is the role of quantity L*? Why do we need it as a stopping rule?+ Some notes on exposition.  - The authors take some time to reveal what is their estimand --- the discounted stationary distribution ratios. As a reader, I would have benefited from having that explained much earlier, even before the conversation about ranking evaluation.  - Section 3.2: the authors could have dedicated some more space developing the intuition for their method (e.g. an abridged version of Nachum and Dai 2020), even if that meant relegating some of the mathematical details to the appendix. As it stands, the section makes the paper incomprehensible as a standalone piece of research.## Typos+ The indices on the sum in the definition of "stationary visitation" (p.4) are wrong. On the next line, the last conditioning should have been s[i+1] ~ T(.|s[i],a[i]) instead of s[i+1] ~ T(.|s[t],a[t]).+ Philip Thomas' "High-confidence off- policy evaluation" citation shows up twice in the bibliography.+ indentical --> identical (Pg. 5) Summary: I think that what is presented is a promising method for human interpretation of high-dimensional models. However, the experiments feel too much like toy examples, without rigorous attempts to validate the resulting feature attributions or to compare to other ways of getting concept-level attributions.Objective: Explain the output of high-dimensional ML models in a human-interpretable way by using Shapley values on semantically meaningful latent features.Strengths:* This is an extremely important problem; the shortcomings of e.g. pixel-based methods for images are important and well-noted.* The proposed method is a promising way to attribute to (and in some cases, learn) semantically meaningful latent features.* dSprites serves as a good ground truth where the generative process is known, and serves to validate some of the human-interpretable patterns found.Weaknesses: My overall concern is that while the experimental results are interesting, they are not thorough and don't demonstrate the marginal value of this method relative to other possible methods in the space; for example, concept bottleneck or causal concept effect models.* I didn't see a reference to Koh et al's "Concept Bottleneck Models" (https://arxiv.org/abs/2007.04612), which do supervised learning of semantic concepts then train a classifier on top of those concepts, which is very similar/relevant work.* Similarly I didn't see a reference to Goyal et al's "Explaining Classifiers with Causal Concept Effect (CaCE)", which trains a VAE to learn a meaningful latent space and report causal effects of modifications to the latent variables.The existence of this prior work sharpens some specific questions I had when reading the paper:* For the non-Fourier tasks, why use a VAE latent space? It's impossible to know that VAE latent dimensions (when they do look like they correspond to a concept) correspond *only* to that concept. The bar charts stating "most importance goes to vertical position/digit identity" seem overconfident, since that is only our guess of what the latent dimension means. In contrast, a concept bottleneck model has dimensions with clear meaning (they were trained with supervision). This is acknowledged to some degree in the text.* Also, for the non-Fourier tasks, why use the Shapley framework? CaCE is a very similar approach, using a VAE to get explanations in terms of an interpretable latent space ,but doesn't use Shapley. What are the pros and cons of each approach? Also, given a meaningful latent space, why not use gradient-based methods (i.e., integrated gradients) to attribute to it? Overall, I don't think this method needs to blow the others out of the water; there are legitimate counterpoints to be made -- for example, concept bottleneck models require concept labels, though I believe such labels are available in the CelebA experiments. But a thoughtful discussion of the relationship of this work to methods like concept bottlenecks or CaCE is essential -- and not present in the current version.Other weaknesses:* The quality of the resulting explanations is not assessed rigorously. The fact that the dSprites example picks up on vertical position and the MNIST example picks up on digit value is useful, but many interpretability papers look more exhaustively, across the dataset, at what happens to the model output when the features reported as important are at least perturbed or ablated, or if a model is retrained with the features altered. I would particularly like to see such a comparison with concept bottleneck and/or CaCE: if this method (Shapley with VAE) is the right way to do things, perhaps it will outperform the other methods at predicting how much certain concept shifts will affect model output or model retraining.* The Fourier examples are interesting but feel a bit like toy examples to me because they are a specific case with a convenient invertible feature map and mostly recapitulate known properties of adversarial examples (they rely on high-frequency patterns).* Minor comment: The "landscape of semantic representations" could be shortened, potentially leaving room for more comparisons to other methods/benchmarks of the attributions. This paper introduces a new QA model based on BERT, which is called Span-Image Network. The paper first points out that previous span extraction models model independent probability of the start and the end of the span, making the extension to multi-span extraction harder. Span-Image Network model the joint probability of the start and the end by deploying a 2-D convolution, enabling multi-span extraction.I think the paper is not ready for publication for a few reasons.First, the limitation of independent modeling of the start and the end of the span has been reported in the literature, and there have been a few works that use joint probability instead. I will only list some of them: work in QA [1] [2] as well as QA baselines for pretrained LMs [3][4][5]; [6], which is concurrent work to thsi work, includes detailed survey. Such literature has not been mentioned or discussed in this manuscript.Second, although multi span extraction is the main motivation for this work, as described in the Introduction, there is no evaluation on public dataset. Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation. The only experiment on public dataset is SQuAD which does not require multi-answer extraction and the proposed model does not show superior results compared to the baseline. (The top-k experiment in the paper is not convincing - it is synthetic and does not align with the goal of predicting multiple answers.) If the authors want to include public datasets for multi-span extraction, they can consider multi-answer portions of Natural Questions dataset [7] or AmbigQA dataset [8].Third, as mentioned above, calculating alignment of the start and end position has been already incorporated in the previous work (such as [2]). The convolutional component is the one I have not seen in the previous literature - however, as there is no ablation with and without convolution, its effect is not shown in the paper.Fourth, even with Amazon internal dataset, the gains are not significant compared to bert baseline with postprocessing. Based on Table 5, the baselines best EM is 88.5, whereas the best performance of the proposed model in Table 4 is 89.1. In fact, the baseline number in Table 4 is not the best number of the baseline, which will mislead the audience to believe that the gap is significant - another major issue of this paper.Lastly, this is a minor point, but I believe Span-Image in the name of the model is largely misleading. There is no image involved in the model architecture or training. Something like 2D or convolution might be a better term.[1] Lee et al. Learning recurrent span extractions for extractive question answering. 2016. [2] Seo et al. Real-time open-domain question answering with dense-sparse phrase index. 2019.[3] Yang et al. Xlnet: Generalized autoregressive pretraining for language understanding. 2019.[4] Lan et al. Albert: a lite bert for self-supervised learning of language representation. 2019.[5] Clark et al. Electra: pre-training text encoders as discriminators rather than generators. 2020.[6] Fajcik et al. Rethinking the objectives of extractive question answering. 2020.[7] Kwiatkowski et al. Natural questions: a benchmark for question answering research. 2019.[8] Min et al. Ambigqa: answering ambiguous open-domain questions. 2020. This paper studies the implicit regularization effect of SGD from the Thermophoresis perspective. The authors find several quantities scale with square of learning rate over batch size, including activation rate and gradient variance. The theory, to my understanding, requires several strong assumptions, e.g., (1) zero true gradient, and (2) certain implicit independence condition. The empirical results in real dataset like CIFRAR-10 do not seem to support the conclusion, either. As someone from CS/Applied Math community, I personally encourage the authors to write the theory in a more mathematical manner: I find several of the current reasoning in the main text hard to grasp ---- maybe I am missing important Physics backgrounds.Below are my detailed reviews ---- it is highly possible that I misunderstand something; if so please do clarify.# Theory- Contradicting statements in Page 2 the second paragraph vs. Page 2 Eq. (3). In specific you study steady state and let J to be zero. However when the dynamic reaches its steady state, it is no longer directly related to the "early phase", which is claimed as the focused object in Page 2 the second paragraph. - Eq. (5). I am super confused here. Does Eq. (5) imply the expectation of the gradient is zero??? This is not at all true for GD/SGD. I mean how can you effectively optimize an objective with zero (expected) gradient?- Eq. (9) and Eq. (10). I cannot understand the paragraphs for Eq. (9) and (10). Recall the definition of U in Eq. (7), I believe U should also depend on time t? Then what do you mean by projecting the flow on to the subspace U, where the subspace U is not even fixed? I tried to look at the appendix, but sorry I am unable to follow the abbreviated derivation.- Property 4.2. What is W here?- Below Eq. (18). Why P(p-y = a) = P(p-y = -a)??? To my understand, if we write down the complete formula, it reads P ( p(f(x_i, theta)) - y_i = a ) = P( p(f(x_i, theta)) - y_i = -a ), where theta is (W, V, b) the parameters. Even the dataset is unbiased, i.e., P(y = 0) = P(y=1), after theta being injected, we no longer have independence for p, thus I do not understand the above equality.- Below Eq. (22). Why can you consider a transformation that maps V to -V??? Intuitively this transformation changes a descent iterate to an ascent iterate and vice versa. Does not it change the behavior of the original dynamic??# ExperimentsThe results in toy datasets seem to be good. But when I look at Appendix A.6, the plots for CIFAR-10 do not seem to agree with your theory in my humble perspective. # Vague Statements- Page 7, the third paragraph. I understand sparsity of weights implies small capacity; however I do not see why sparsity of the activations also imply small capacity. In specific, it can be that the rate of activation is small, but for each data the activated neurons are different. In this case can you prune the network as stated in the paragraph?- The title and abstract emphasize the paper concerns SGD. But I fail to find a direct connection between SGD and the so called "mass flow" in Eq. (1). I suspect the tile and abstract are over claimed. Overall, I cannot give high scores to this paper. Again, I can miss important facts because of lacking Physics background.  This paper developed methods for resampling from the hindsight experience replay buffer. The resampling strategy was developed based on the current policy, and the overall distribution of the relative goals. As the distribution over goals evolves over time, the multi-goal agent's replay curriculum is adjusted throughout the learning process. The developed approach, called hindsight curriculum generation (HCG), was applied to DDPG, and evaluated using a set of four robot control problems. Results show that HCG performed better than a few baseline methods, and its performance was claimed to be insensitive to the choice of hyper-parameters. The paper has a few issues. The sampling from hindsight experience is partially based on the likelihood of the corresponding state-action pair under the current policy. The reviewer is not sure that this strategy makes sense when the developed approach was applied to off-policy RL methods (DDPG in this case). It seems to be suggesting that, without exploration (completely following current policy), off-policy RL methods get the best results. Using only recent policies limits the variance over the collected experience. Some more discussions and justifications are needed for "the likelihood of the corresponding state-action pair under the current policy."  The two baselines of CHER and HER-EBP were not mentioned in the experiment section. The reviewer had to search the whole paper, and found the acronyms mentioned in the introduction section. The results are suspicious: how come CHER and EBP performed even worse than naive HER in Figure 2? The results are inconsistent to those reported in the CHER and EBP papers. It's stated that "Results in Figure 3 indicates that the choice of L is robust." This is apparently not the case from Figure 3. In the pick-and-place task, when L=5, it reached 0.8 success rate in 15 epochs, whereas the agent couldn't succeed at all when L=1 or L=0.5. The conclusion was not supported by evidence or experimental results. The paper mentioned "Appendix" in a few places, but there is no appendix in this submission. It's suggested to experiment with RL methods other than DDPG. There's the potential of applying the developed approach to on-policy methods that have been evident to performing better than DDPG in robot control tasks.  The authors introduce a wealth of changes to the standard HER agent and obtain a performance improvement in 3 multi-goal tasks. The main observation made by the paper is that relabeled experiences may be very off-policy / out-of-distribution, and so value estimates for such experiences will be bad. To help with this the authors propose the following:- apply K-means to cluster real (state, goal) tuples. - estimate the likelihood of a given (state, goal) tuple X by finding the closest cluster center in real experience, sampling n real experiences Y^i in that cluster, and taking the minimum of 1/d(X, Y^i). - they change the Bellman targets in case the likelihood estimate is low. In particular they change it to a lower bound based on some nearby real experience minus the distance times Lipschitz constant.- they use relative goals (g_original - g_current) instead of absolute goals (g_original).- there is some kind of curriculum on goal relabelingThis paper is hard to follow. The word usage and sentence structure is unnatural, and I find myself guessing at what exactly the authors mean. This carries through to the math. I think I understand what the modified Bellman backup above equation (6) is doing, but I'm still not entirely sure. I'm also not really following the Section that includes equation (8). As a result, the contributions are a bit unclear. Theorem 1 is not trivial and there is indeed doubt in my mind. A proof should be provided upon revision. The related works section can be greatly improved. You should be relating the related work to your own.An appendix was not provided, despite being referenced, and so the paper is missing additional results (the 3 environments are insufficient), implementation details, and hyperparameter details. Without these, this paper cannot be reimplemented and is not in a publishable state.Nits:- Isn't Equation (5) just the definition of Lipschitz continuity, so I'm confused by what is meant by "it's reasonable to claim that [it] holds".-  Major weaknesses of the paper:- My understanding is that these are surrogate models that are meant to simulate a real-world task. However there is no description as to how these surrogates were created or trained, nor how their fidelity to the original task was vetted.- The most simple and naive algorithm seems to provide similar speed-ups to the much more complicated proposed T2PE; especially when considering the much larger improvement of the naive method (see Fig. 11), none of the other proposed methods seem justified to me.- Furthermore, I don't consider this naive approach (Best First) as being an HPO approach that leverages transfer, since it does exactly what anyone would do when faced with a slightly altered set of hyperparameters.- This last point suggests that at least one of the following must be true:  * non-trivial transfer is not as important as intuition would lead us to think;  * this benchmark suite does not provide a good testbed for assessing an HPO method's ability to transfer; or  * none of the non-trivial proposed algorithms do a good job transferring and can therefore not argue against the previous point.  Given this important contradiction, I must recommend a rejection. My recommendations would be to:- focus on creating a good benchmark suite (perhaps focus on a single or two domains as introduce many variants, instead of four domains with only two variants);- focus on vetting the surrogates in terms of their fidelity to the task they are meant to simulate; and- focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do.For the record, I don't think this is an easy task.Minor points:- Justify geometric mean. I'm not saying it's the wrong way to compare these, I just think it requires at least a sentence of justification.- Same for the violin plots. For such simple plots, simple boxes and whiskers, with perhaps data points to show the spread of measurements across seeds, would do just fine.- Figure 4, and indeed any mention of the two methods therein, can be entirely removed from the paper; other than to perhaps mention that they were tried and failed---results in the appendix.- A much more interesting replacement for that figure would be Figure 11.- Not sure what is the point of comparing random search to TPE in the appendix unless this means Best-first then Random-search/TPE? If the latter is true, please clarify. This paper studies federated learning with quantization. The problem setting is very standard, including both iid and non-iid cases. This work proposes a new algorithm, called lossy FL, to save the communication costs, especially from the broadcasting direction. To my understanding, the algorithm is new but still very similar to double-squeeze (Tang 2019). The presentation of the paper is generally good. However, there are several major issues regarding the quality and significance of this work.1. The convergence analysis is based on the assumption that the problem is strongly convex. But from the motivation of this work and numerical results, the problems would be nonconvex. The current analysis is restrictive.2. There is no analytic result of quantifying the upper bound of \tau, which is one of the key differences between federated learning and distributed training. There are already many works there.3. The comparison with double-squeeze is definitely not fair, since the aggregation rule of the double-squeeze did not consider the \tau step local update.4. The numerical results are very limited, where the CNN network is very small. There is no need to perform quantization for this neural net. Also, MNIST and CIFAR-10 are all small datasets. I dont know why there were 40 devices used. To show the advantage of federated learning, the speed up in terms of the training time should be compared and plotted. Paper Summary:The paper builds on previous work like Slalom to propose a new secure training and inference protocol in the TEE+GPU paradigm. The main technical contribution of this work is a new blinding algorithm that dramatically reduces the memory required to store the blinding parameters (decoupling it from the input/model size). The authors then build on this to extend their blinding scheme to the training use-case.Score Rationale:- The proposed blinding scheme does indeed provide a ~1.5x performance improvement over the Slalom baseline- There two concerns about the correctness of the security argument provided by the authors  - The security proof as argued in the paper does not extend from the single pixel to multi-pixel case and as such does not apply to real-world images  - The reviewer believes that this not a mere gap in the security proof. Infact for certain allowed settings of parameters there are practical attacks.Detailed Comments:- The core security argument is rooted in Theorem 1. This theorem in prose rightly claims that the blinded pixels at any specific index, leak almost no information about the pixels at that specifc index in the source images.- This statement critically makes no claims whether all the pixels in all the blinded images in their totality will leak information about a given pixel index in the source images.- The authors then argue that the total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels at that particular index).- The statement is not true. The total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels across all indices)- The next concern to evaluate is whether this is just a technical/theoretical gap in the proof or whether it leads to a practical attack.- Consider the case where the parameters are set as in section 5.2 with K=1. Note that reducing K improves the leakage bound in principle.- In this particular case we can rewrite the blinding equations s.t. $x^{(1)} = \frac{\overline{\mathbf{x}}^{(1)}\alpha_{2, 2} - \overline{\mathbf{x}}^{(2)}\alpha_{2, 1}}{\alpha_{1, 1}\alpha_{2, 2} - \alpha_{1, 2}\alpha_{2, 1}}$- Thus the source image can be represented as a simple linear combination of the outputs and attacker job reduces to the task of guessing the two weighting coefficients- Given the strong priors on natural images this seems to be a very tractable problem.Additional Comments:- There are three potential ways to work around to address the above comments  - Devise a new proof for information theoretic security  - Argue the computational hardness of the search problem  - Devise a new blinding scheme that does not have this issue This paper presents an approach to deep subspace clustering based on minimizing the correntropy induced metric (CIM), with the goal of establishing when training should be stopped and generalizing to unseen data. The main contribution over the existing S2ConfSCN method is a change from squared error loss to CIM when optimizing over the affinity matrix. A key benefit of CIM as a loss is that it does not decrease arbitrarily with training epochs, so it provides a means of estimating when training should cease without needing ground truth labels. The authors argue that CIM "ensures a smooth decrease of the loss function that enables the use of label-free stopping criterion." However, this claim is only justified through a minimal empirical evaluation. The authors also include a means of enforcing block diagonal structure in the learned affinity matrix.While the ideas presented in the paper are important for the area of deep subspace clustering, the overall contribution of this paper is quite limited. Further, the results in Table 1 do not improve on the results of shallow subspace clustering methods such as SSC and EnSC. The authors argue that these methods are tuned using ground truth labels, but even tuning with the KSS Cost (as in Lipor and Balzano 2020) results in better performance on the given datasets. Given that shallow subspace clustering methods maintain strong theoretical guarantees, the empirical results presented here do not justify the use of deep methods. The authors propose a novelty detection module to help unsupervised class-incremental learning. The novelty detection relies on the percentage of accuracy drop during a model update when treating incoming data as a new class. If the model maintains high accuracy, then the module treats the incoming data as familiar, thereby choosing one of the existing classes as the correct label. The paper investigates the effectiveness of the proposed method on MNIST, SVHN, CIFAR-10, and CIFAR-100.The main weakness of the submission might be that the proposed novelty detection is not well motivated. The bottleneck of the proposed pipeline comes down to whether the accuracy drop on the selected subset is a good indicator of out-of-distribution (OOD) detection. The submission does not provide theoretical insights nor direct references that show it is actually the case. In fact, in the experimental section (Sec 4.1), the authors have to use several different accuracy threshold settings (0.46, 0.63, 0.57, 0.62) for different datasets, demonstrating that accuracy drop might not be a reliable indicator.Besides, the direct competing method CURL (Rao et al.) is cited but not compared. iCarl [R1] should be quite related as well. The authors also use quite a different backbone network (ResNet-18) than other competing methods. Therefore, it is hard to justify whether the proposed approach is more effective than other baselines. The method described here is also quite similar to the field of active learning. It would be great to discuss the relationship between the proposed novelty detection and other active learning literatures.Sec 1 first sentence continually learning systems remains to be a major obstacle in the field of artificial intelligence is quite a strong statement. I believe there are other major obstacles in AI and they should be discussed as well.Sec 1 paragraph 3, an agent must conduct two procedures successfully. The authors do not clearly define what is an agent in the context. It is hard for the readers to follow through the manuscript.Sec 3.4, After obtaining the correct label. Actually the label technically is not correct but assumed to be correct for the class-incremental learning.[R1] Rebuffi et al. Icarl: Incremental classifier and representation learning. In CVPR 2017. This article proposes a method for predicting whether a batch of data is of the same class as one of the classes already seen by a classifier or whether it contains data from another class. The idea is to then be able to incorporate this batch to the previous training set, in an unsupervised learning context. It is assumed that each batch contains data from only one class. Experiments are there to show the interest of this method for anomaly detection or incremental learning.I find it difficult to formulate an opinion on this paper because I don't think I have managed to understand the detail of what is actually done. For example with regard to the detection of out of distribution data, the classic problem is whether a data is out of a distribution. Here it is not a data but a batch of data that is considered. I don't really see, under these conditions, how to compare to classical OOD methods. As far as incremental classification is concerned, I don't understand the definition of the metric given in section 4.3 and therefore I'm not sure I understand what the task is really about. The fact that it's unsupervised makes it away from standard problems.It seems to me that the paper lacks a clear definition of the tasks addressed and the means to evaluate performance.  This paper proposes a method to learn the cloth deformation of a t-shirt giventhe skeletal pose of an upper body. The method skins a thick tetrahedral mesh tothe skeleton and embeds the t-shirts cloth within. At inference time a networkpredicts a rest pose displacement before conducting skinning via barycentriclookup in the tet mesh. The method is trained (as far as I can tell) on somegroundtruth cloth simulation method (this is not revealed).I recommend rejecting this paper from ICLR 2021 on several grounds: 1) theresults are poor, 2) the description is hard to follow, 3) the methodologicalchoices are not well motivated, 4) the method as written is not reproducible,and 5) the claims are too general.1) In terms of topic and methodology this paper would be an appropriatesubmission to SIGGRAPH or SCA. Callibrating for the expected result quality ateither venue, I would recommend acceptance. Since the machine learning componentof this paper is not a contribution besides being an application of"off-the-shelf" tools, I do not see reason to lower these standards for ICLR.2) After reading the paper, I eventually feel I understand this method. Important details are left out effecting replicability (see below). The paperdoes not clearly state what the input and output is. The paper does not describehow groundtruth data is generated. "This is done by first sorting the tetrahedra on the list based on their largestminimum barycentric weight, i.e. preferring tetrahedra the vertex is deeperinside" I don't understand this. Barycentric weights are largest when near avertex. Meanwhile tetrahedra can be very pancake/sliver-shaped, so thatregardless of the barycentric coordinates, a point is never deep inside. Usingbarycentric coordinates to measure depth of penetration is misguided.I didn't understand "method 2". In that method is the tet mesh entirely ignored?Is Figure 4 showing training data or withheld testing poses?3) The paper immediately jumps into the idea of embedding the cloth of a t-shirtin a bulbous tetrahedral mesh around the upper body. This isn't questioned untillater when all sorts of issues appear due to overlapping and inverted elements.There was no reason to think that skinning such a thick tet mesh was a good ideain the first place. So the "INVERSION AND ROBUSTNESS" section is describing adhoc heuristics to a problem that could have been avoided by starting with a moresound premise.The working premise is that pose space deformations can be used for efficientcloth simulation. This is reasonable and traces its heritage to "A powelloptimization approach for example-based skinning in a production animationenvironment," which should probably be cited. From there, the choice of using athick tet mesh comes without solid motivation. Why not, for example, insteadlearn the skinning weights and displacements directly? So, that for a point p onthe cloth the final deformation is: wi(p,¸) Ti(¸) (p + d(¸,p))?To generalize across body types etc., rather than the heavy handed proposedapproach of sharing this mysteriously skinned tet mesh, the learned w and dfunction could be predicted based on some relative position to the rest pose andt-shirt size etc.4) This paper is far from replicable. How is the groundtruth data computed? Somecloth simulation? Which method? Are collisions handled in that method? Why does the surface boundary in Figure 1 (c) look so spiky yet the input levelset is smooth? This does not appear in the results of the red/greentetrahedralization results. This looks more like a simply clipped regular gridtetrahedralization.How are the weights wkj determined? Manually? Automatically? Optimized duringtraining? This appears to be crucial to the method but left out.5) Finally, this paper claims to provide a "Skinning a parameterization ofthree-dimensional space for neural network cloth". Even if I accepted this paperas successful in its results (I do not), then this paper could at best claim tohave skinned a parameterization of t-shirt deformations for upper-body motions.The fragility of the method as discussed above makes this overclaimingespecially dubious. The authors propose a neural architecture search (NAS) algorithm inspired by brain physiology. In particular, they propose a NAS algorithm based on neural dendritic branching, and apply it to three different segmentation tasks (namely cell nuclei, electron microscopy, and chest X-ray lung segmentation). The authors share their codes with the scientific community, which is highly appreciated. - I have a concern about one of the main motivations of the paper. The authors say that they "take inspiration from the brain because it has the most efficient neuronal wiring of any complex structure" (lines 7 and 8 in the Abstract). My question is: What is the problem with using biologically implausible NAS methods or with the fact that some approaches are incompatible with current understandings in neuro-biology? I consider that all proposals to improve neural networks (NNs) training and design are welcome, but I do not see the relevance of explicitly searching for biologically realistic algorithms in NNs. As Yann LeCun would say (http://matt.colorado.edu/compcogworkshop/talks/lecun.pdf): "Let's be inspired by nature, but not too much". It is indeed nice to imitate nature, but we mainly need to understand what is actually relevant for our practical purposes. For instance, as pointed out by LeCun, in the case of airplanes, we developed aerodynamics and compressible fluid dynamics, and we figured out that feathers and wing flapping were not crucial. In this sense, I'd like to see a better justification and contextualization of this dendritic branching approach.- The paper, in my humble opinion, is a bit confusing, because the different contributions and overall structure of the paper dilute the main focus of the paper. The paper first starts with the presentation of a novel bioinspired NAS method. Then, we move towards medical imaging segmentation and the limitations of U-Net, and later we discover that there are also evolutionary computation algorithms involved (as a primary contributions that is not even cited in the Abstract). I would encourage the authors to rewrite the paper in a clearer way, identifying the core contribution and challenges, as well as the motivation and the rationale behind the approach they use. In particular, I'd like to better understand how the submitted paper compares (quantitatively and qualitatively) to prior art, and what are the conceptual or empirical advantages of the proposed approach.- Why other NAS algorithms were not included in the experimental comparison (apart from Weng et al. (2019))? Regarding the competitor methods, why not to introduce in the comparison methods like Neuron-Evolution for Augmenting Topology (NEAT) [1], HyperNEAT [2], ENAS [3] or [4], that represent popular methods in neural architecture search? Without a more extensive experimental comparison with prior methods and different strategies is difficult to elucidate the actual empirical contribution of the proposed method. [1] Kenneth O. Stanley, and Risto Miikkulainen. "Evolving neural networks through augmenting topologies." Evolutionary computation 10(2): 99-127, 2002.[2] Kenneth O. Stanley, David B DAmbrosio, and Jason Gauci. "A hypercube-based encoding for evolving large-scale neural networks". Artificial life, 15(2):185212, 2009.[3] Pham, Hieu, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. "Efficient neural architecture search via parameter sharing." arXiv preprint arXiv:1802.03268 (2018).[4] Zoph, Barret, and Quoc V. Le. "Neural architecture search with reinforcement learning." arXiv preprint arXiv:1611.01578 (2016).- Evolutionary computation techniques have been employed many times for both training neural networks and or designing their architecture and building blocks (e.g., activation functions), with papers on the subject already published in the 80's and 90's. From this point of view, statements like "Within the last five years, advances in NAS have branched into different areas, including evolutionary algorithms (Real et al., 2018)" look insufficient. The related works section is, in my opinion, not complete and clear enough. See the following papers as example:  [5] Miller, Geoffrey F., Peter M. Todd, and Shailesh U. Hegde. "Designing Neural Networks using Genetic Algorithms." ICGA. Vol. 89. 1989.[6] de Garis, H. (1990). "Genetic programming: Modular evolution for Darwin machines". In Proceedings of the 1990 International Joint Conference on Neural Networks (pp. 194-197).[7] Fogel, David B., Lawrence J. Fogel, and V. W. Porto. "Evolving neural networks." Biological cybernetics 63.6 (1990): 487-493.[8] Yao, X. (1993). "Evolutionary artificial neural networks". International journal of neural systems, 4(03), 203-222.[9] Angeline, Peter J., Gregory M. Saunders, and Jordan B. Pollack. "An evolutionary algorithm that constructs recurrent neural networks." IEEE transactions on Neural Networks 5.1 (1994): 54-65.[10] Yao, X. (1999). "Evolving artificial neural networks". Proceedings of the IEEE, 87(9), 1423-1447.- In relation to the proposed model itself (called Bractivate), there are many aspects that remain unclear:(1) What do the authors mean by semi-random evolutionary algorithm?(2) What evolutionary algorithm do they use? It appears that they just apply a mutation operator on the most active block in the best architecture found so far. (3) The authors propose a loss that includes the number of model parameters and the training time per epoch. In this sense, what do the authors mean by BCE in Equation 3? I guess they actually refer to BCL, right?(4) If I understood correctly, Bractivate does not allow to create or insert new blocks but it only branches already existing blocks (using two skip connection operator types: concatenation or addition). This seems a severe limitation in the neural architectures it can generate. Am I correct? (5) The authors state that "Our ablation study most strongly confirms our hypothesis that dendritic branching to active blocks significantly improves segmentation performance". Did the authors employ several independent runs, some experimental validation protocol (like Cross-validation) and statistical tests to verify the existence of statistically significant differences between the methods under comparison?   Overview: This paper proposes to apply ensemble methods to make use of sub-optimal configurations saved from Hyperband. In particular, the authors propose to use Dijkstra's algorithm to choose the best combination of size K among n models.Reasons for the score:Overall, I vote for reject. There are several reasons that will be detailed a bit in the next. Roughly, I think the paper requires a huge effort of rewriting before being published. Beyond that, I'm not fully convinced by the technical novelty or difficulty of this paper.Pros:- The idea of collecting sub-optimal configuration to boost the performance has not been considered in the context of HPO (to the best of my knowledge).- The experiments in Table 1 and Table show that the workflow do improve the final performance.Cons:- My  first major concern is on the writing:  - There is no clear problem formulation of HPO.  - Personally, I'm not very comfortable with the authors not presenting formally what is Hyperband, what is Dijkstra's algorithm, and also how is the problem of finding the best ensemble modeled as a knapsack problem. I think these details can at least be provided in the appendices. I don't think it's appropriate to assume that readers are all familiar with every notion presented in the paper. Especially the main purpose of this work is to propose a new workflow that combines existing techniques, providing enough detail on how each component works and how they are connected in a precise way seems essential to me.- Now regarding the experiments:  - First of all, a general question, how many trials have you run for each experiment?  - Also, I don't see error bars on figure 6, 7, 9, 10.  - Could you explain a bit more why are you interested in the experiments of Table 2? I don't understand very well why it is useful to support the purpose of this work.  - Could you please more precise on when do you stop the experiments? It looks like for some experiments, a fixed time horizon is given. I guess it's the same for others, but again, some precise and formal descriptions are more than welcomed.General questions and remarks:- Do you have any intuition on why you use averaging other than other ensemble strategies?- Although ensemble has not been considered in the context of HPO, I'm not sure that the scientific contributions in this paper is significant enough. Could the authors highlight a bit more the technical difficulties if I have missed anything?- In general, I feel like the paper is more like an engineering trick than a scientific discovery, which could be a nice contribution of course. But then in that case, I think more solid experiments should be provided. For example, in the context of HPO, we often want to see the evolution of performance over time, not only the final performance.Minor comments and grammar issues (non-exhaustive):- In general, I would suggest the authors to review a bit the writing style of the paper. Sometimes I feel like the authors have personal claims regarding some previous work without any support (which can be citations or even some intuitions). To cite one of them as example, in Section 3.2: Its pure-exploration nature combined with conservative resource allocation strategies can sweep better the hyperparameter space than other strategies like blackbox bayesian optimization. Maybe, but how is it compared to evolutionary algorithms for example? My point is that we should be careful about this kind of claims.- The citation style is weird, authors can refer to Section 4.1 of the template file of ICLR.- In the abstract: capable of doing sth. instead of capable to do sth.- Section 2, paragraph Multi-objective goal, I don't really understand the sentence: Literature lets us imagine that the hyper-parameter function topology has two plateaus.- Section 3.2: bayesian -> Bayesian.- Section 3.3.3: hyperband -> Hyperband.- Section 4.1: It shows very effective to detect... <- this is not correct grammatically.- Section 4.3: Different combinations algorithms -> Different combinations of algorithms.- Section 4.3: SP-MCTS do not falls into -> SP-MCTS does not fall into. This paper is proposing to build ensembles of deep models, components of which have different hyperparameter (HP) configurations. This is done by first running Hyperband to create a large pool, and then run a greedy algorithm to construct an ensemble. This algorithm is termed Dykstra's algorithm on a certain graph, but it is of course simply just the default greedy algorithm, which is almost by default used to create an ensemble from a pool. The correct reference for this is [1], and this is just what people do when they create ensembles. The paper also misses a number of relevant recent work to build ensembles of deep models, at least [2, 3]. There is nothing new here, except maybe that Caruana's algorithm can now also be called Dykstra's.In the very unlikely case I missed something, and what they call Dykstra's method here (not detailed in the paper) is different from the obvious greedy method of Caruana, then the paper fails by not comparing against this obvious baseline.[1] Caruana, Niculescu-Mizil, Crew, Ksikes  Ensemble selection from libraries of models  ICML '04: Proceedings of the twenty-first international conference on Machine learning[2] Deep ensembles:  B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NIPS), pages 64026413, 2017.[3] Batch ensembles:  Y. Wen, D. Tran, and J. Ba. Batch ensemble: an alternative approach to efficient ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020 This paper proposes the use of Dijkstra's algorithm for ensemble construction, where the pool of classifiers to chose from is models trained during a hyperparameter optimization procedure. Authors show that the performance of the constructed ensembles improves upon the performance of a single classifier. Strong points:- Novel and interesting ideaWeak points:- The paper would benefit significantly from proofreading and copyediting.  - The choice of the Dijkstra algorithm is poorly justified and might be incorrect- More baselines for comparison are requiredRecommendation:I strongly recommend that this paper be rejected. While the idea of using Dijkstra for post hoc ensemble construction is interesting, I don't think it is justified and I am not convinced that it is a correct choice. Furthermore, the paper is riddled with mistakes, unneeded details and bad explanations. This paper needs to be entirely rewritten and the experiments require proper baselines for post hoc ensemble construction. Extra comments:There is no justification provided for using Dijkstra. The context is different from shortest path searching, as multiple classifiers in an ensemble have complex interactions that are lost via the majority voting and empirical risk estimation. In other words, the "weight" of the edge from one node to another will vary depending on the nodes visited beforehand, which is not a condition normally present for shortest path estimation. I think it means that it's incorrect to apply Dijkstra here, or at least that Dijkstra offers no guarantees. The same goes for the parallel with the Knapsack problem (i.e. I don't think this is an instance of the Knapsack problem).Other baselines should be added, some simple ones would be forward/backward greedy search, stacking and genetic algorithms. Section 3.3.1: showing with a single example that combining three models with a majority vote does not lead to optimal performance is hardly evidence that "smart algorithms are needed".The related works section is needlessly long. For example, no need to mention work on multi-objective optimization. Formatting of citations is incorrect, see the conference author guidelines.Missing citations:The correct citation to introduce boosting is "Y. Freund, and R. Schapire, A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, 1997." not Schwenk & Bengio. Amongst others, Caruana et al. (2004), Feurer et al. (2015) and Levesque et al. (2016) have already applied ensembling to hyperparameter optimization, such references are missing from your relevant works section. References: Caruana, R., Niculescu-Mizil, A., Crew, G., & Ksikes, A. (2004, July). Ensemble selection from libraries of models. In Proceedings of the twenty-first international conference on Machine learning (p. 18).Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., & Hutter, F. (2015). Efficient and robust automated machine learning. In Advances in neural information processing systems (pp. 2962-2970).Lévesque, J. C., Gagné, C., & Sabourin, R. (2016). Bayesian hyperparameter optimization for ensemble learning. arXiv preprint arXiv:1605.06394. This paper presents an argument that model capacity differences are not necessarily the root reason for the performance gap between the student and the teacher, and the distillation data matters when the student capacity is greater than a threshold. Based on this, the authors develop KD+ to reduce the performance gap between them and enable students to match or outperform their teachers. In addition, this paper designs experiments to confirm the proposed arguments. However, this paper should be rejected because:(1)the results of Table 1 cannot confirm the authors argument that the widely used students are CSTs. It also depends on the model capacity differences whether a CST is able to fully fit the teacher outputs or not. (2) some arguments in this paper are unclear and lack verification. For example, the paper statesThis suggests that these students have well captured the knowledge on sparse training data points but have not well captured the local shapes of the teachers within the data distribution. in Definition 4.2. How did you come to this conclusion?(3) the most important or relevant references are not cited, for example [1] Xu G, Liu Z, Li X, et al. Knowledge Distillation Meets Self-Supervision, 2020. And the proposed KD+ is not superior to the approach of [1], for example, some results in Table 4 and Table 8.(4) This paper lacks experimental settings and details.Please refer to the above comments and answer these questions. SUMMARY This paper introduces GP-NC, a new methodology that allows Gaussian processes to stay far from certain data points (referred to as negative datapairs). These negative datapairs complement the standard training points to be fitted. The authors show enhanced performance when using this technique with standard GPs, SVGP and PPGPR.###################################################PROS1) I am not aware of previous works in the GP literature dealing with this scenario, i.e. data pairs to be avoided during training.####################################################CONS1) I think this work is not sufficiently motivated. The authors claim that the brand-new negative datapairs can be useful for modelling navigation problems with GPs. However, this type of problem is not addressed in the experimental section. I would expect to see how the proposed method compares to the previous approaches that have modelled the navigation problem with GPs. 2) In fact, due to the lack of real problems that require the presented technique, in the experimental section they just simulate some negative datapairs (following the so-called random shuffling technique). Moreover, this simulation technique is pretty poor in my view, since different inputs could have very similar outputs, which may lead to a simulated datapair almost identical to a standard training datapair. 3) Some arguments are too vague/ambiguous. For instance, the equivalence between the scale of magnitude of both terms in eq. (5). It is stated that, by using the so-called logarith trick, both terms have the same "scale of magnitude". How is that defined? and how can the equivalence be proved? Or the sentence that says "A Gaussian process is a Bayesian non-parametric approach that fits Gaussian distributions to functions". 4) Low quality writing in general. I have come across many typos and sentences that are not well written. See e.g.:* caption of Fig. 1 "is been given".* after eq. (5) "minimize",* "Hewing et al. (2020) are some of the works using sampling based techniques for trajectory prediction"* "We trained a sparse SVGP model to regress a curve on the using the classical GP framework and the one with negative constraints GP-NC"* Caption of Figure 4: it says five datasets, but there are six* Many references are missing the journal/conference/arxiv reference. Format not homogeneous across references. Summary--This paper in concerned with Gaussian process regression under constraints that aim to discourage the model from learning certain values (negative constraints). These are called negative data pairs, and the authors propose an extension to the standard GP methodology to incorporate these constraints in the model. This is done by iteratively training a standard GP and maximising the KL between the GP and blobs of the negative data pairs.The problem tackled in the paper is of relevance in fields such as spatial statistics and robotics, with possible applications also in more general ML tasks. The paper could be of interest to a narrow audience at ICLR. The presentation is rather clear and the main idea is easy to follow.Concerns--1. My main concern is originality and novelty of the approach presented in the paper. The authors tackle an important practical problem (or limitation), but the presented contribution alone is small. The proposed approach for including negative data pairs is straightforward, and something that one might find in an application paper (where the main interest is in solving a task related to an application). 2. Limitations. The limitations of the approach should have been discussed in detail. Questions related to uni- vs. multimodality (splitting), bias influenced by the parameter \lambda, behaviour in multi-output (2d, like Fig. 1), would perhaps best have been illustrated with suitable simulated test cases and included as figures (including random samples from the GP posterior). Some of the issues are visible in Fig. 2, but these are not discussed in detail.3. Experiments. The experiments do not appear convincing. Rather than artificially changing standard test data sets to fit your task setting, it would have been more interesting to see actual problems, where the model class would have been beneficial.4. Presentation. Even if the method itself is easy to understand from how it is presented, the paper would deserve improvements in the presentation. As a practical suggestion: Background material could be presented more concisely, the methods section refined to be more concise, and the additional space used for improving experiments and discussion.5. From how Alg. 1 now reads, I would interpret \lambda not to have any effect on the training. This should not be the case. Alg. 1 does not agree with the objective in Eq. (6).6. Minor: Typo in Eq. (4). ## Paper summaryThis paper proposes *Unsupervised Robust Representation Learning* (URRL), a framework that combines several data augmentation schemes and a similarity-based loss. The goal is to improve the robustness of visual representations to image perturbations. A further goal is to maintain the robustness properties of pre-trained representations after fine-tuning the network to downstream tasks.The proposed method consist of two components: A scheme for sampling combinations of image augmentations, and a loss that is applied during transfer learning and penalizes deviations of the pairwise similarity between datapoint representations from the similarities before fine-tuning.The method is evaluated on clean data performance and adversarial robustness, as well as calibration error.## Arguments for acceptance1. The paper is well written and clear. 2. The proposed method improves on MoCo-v2 and Augmix in ImageNet accuracy, ImageNet-C corruption error, RMS calibration error and adversarial robustness.## Arguments against acceptance3. The proposed augmentation approach appears to be very similar to the methods used in MoCo, AugMix, and related work. Section 2.1 of the paper argues that these methods represent two different families of augmentation ($\mathcal{T}_\text{rep}$ and $\mathcal{T}_\text{rob}$). The paper suggests that augmentations need to be sampled from both of these families in a particular way (Eq. 2) to overcome the issue that one of the families affects the color histogram too much. This approach is only a small and heuristic variant of existing methods. It has been known since long before MoCo and AugMix that image augmentations improve unsupervised representation learning and robustness. Formal arguments for the distinction between $\mathcal{T}_\text{rep}$ and $\mathcal{T}_\text{rob}$ are not given. I am not sure whether the proposed variant is original enough for publication at ICLR. Further, the performance improvement of URRL over MoCov2 is small, less than one percentage point for most metrics.4. The proposed similarity regularization leads to more significant improvements in fine-tuning performance. However, this method is not compared against baselines. Regularization approaches for improving transfer performance have been proposed before, e.g. see [Li et al., 2018](https://arxiv.org/pdf/1802.01483.pdf). These should be compared to the proposed method. In particular, well-tuned weight decay towards the frozen pre-trained weights ($L^2$-$SP$ in Li et al.) is an essential baseline. I suspect that the proposed similarity regularization may be functionally equivalent to decaying weights towards their initial values.## ConclusionWhile the paper is well organized and clear, in its current form it does not meet the originality and significance standards of ICLR and lacks crucial baselines. Suggestions for improvement: 5. Provide a more formal argument for the proposed augmentation framework to show that it is a significant conceptual advance over the current knowledge that augmentations are important.6. Provide a more formal motivation for the similarity regularization, including a formal comparison to $L^2$-$SP$.7. Compare the similarity regularization to a well-tuned $L^2$-$SP$-decay baseline. ### SummaryThis paper focuses on the problem of training a neural model to understand source code. The authors argue that both graph information (such as the parsed abstract syntax tree) and sequence information (such as the raw program tokens) are useful for understanding code, and describe a particular method of adding raw program tokens to a graph called SCS. They also describe a modification of a transformer (called a GN-Transformer) that uses this SCS graph representation, and present results on code summarization tasks.The proposed encoding does not seem novel, and the authors appear to be unaware of significant prior work in this space. In particular, the SCS graph representation seems like a simpler version of the representation described by Allemanis et al. (2018) [1]. The GN-Transformer model also seems to have only minor differences from previously-used architectures for code understanding. Additionally, the paper has multiple issues with clarity and style that make it hard to understand, and also has some claims that seem unsupported by the evidence.The experimental results are somewhat interesting, and show that this kind of method can achieve good results on source code summarization tasks (whereas most of the closely-related prior work that I am aware of has instead focused on automatic fixing of bugs). Overall, however, given the lack of novelty of the proposed method and the issues with clarity, I recommend rejection for this paper.### Detailed commentsIn "Learning to Represent Programs with Graphs" [1] (see section 4 heading "Program Graphs"), Allamanis et al. describe a graph representation very similar to the SCS representation described here, which includes both raw tokens and AST nodes, linking the AST nodes to the raw tokens they contain, and connecting each token to the previous and following tokens. The graph representation in [1] also includes a large variety of additional edges with explicit edge labels, based on static analyses of program behaviors. The SCS representation seems to be simpler than this, only including AST and token edges, and not including next/previous relationships for tokens (although the authors do describe a variant where each token is connected to all other tokens) or having any notion of different edge types or directed edges.On page 3, the authors claim that "structural information contained in the AST is lost" when represented with a structure-based traversal. However, Hu et al. state that the SBT traversal is lossless and that the AST can be recovered exactly from it, so this claim seems to be incorrect.I don't understand the discussion of "optimal graph structure" in section 3.2. The authors cite work by You et al. (2020), which describes a way to analyze the structure of the connectivity of arbitrary neural networks (including feedforward nets, convolutional nets, etc); they then analyze various properties of these graphs to draw conclusions about the corresponding networks, and find that certain connectivity patterns work better than others. I don't see how this argument translates to graph neural networks, for which the graph is the input to the model. It's not obvious that connectivity of network layers and connectivity of the input graph are the same kind of thing, and also just because connectivity correlates with accuracy in some sense on one dataset that doesn't mean that those connectivity patterns are *necessary* to achieve good accuracy. Furthermore figure 4b in this work suggests that the SCG encoding has a clustering coefficient of exactly 0, which is strange and does not seem to be the same kind of connectivity pattern analyzed by You et al.The description of the GN-Transformer model is cluttered and hard to follow, but from my understanding it is only a minor variation of existing models. In particular, it appears to be the same as a vanilla transformer, but with masking applied so that each graph node can only attend to neighboring nodes. (Or, equivalently, it is a standard multi-head GAT block except that dot-product attention is used instead of leaky ReLU attention, and a dense feedforward layer is added after the attention mechanism.) There are also similarities to the GREAT model described in "Global relational models of source code" [2], which is also a transformer-based model applied to nodes of an "early-fusion" graph (containing source tokens and AST nodes), but the GREAT model seems more powerful because it is allowed to attend to nodes that are not neighbors (avoiding the long-range dependency problem that the authors of this paper mention in section 3.2).Also, if my understanding of the GN-Transformer model is correct, I don't see why it is necessary to invoke the formalism of GN blocks (proposed by Battaglia et al). The "edge update function" seems to discard all previous information about each edge, and is just a more complicated way of writing the dot-product attention computation. While it may be true that GN-Transformer fits into the GN block framework, I think using the terminology of GN blocks obscures what the proposed model is doing.The experimental results seem somewhat promising, and suggest that using graph structure and source tokens together when doing code summarization works better than just operating on sequences or doing a late-fusion approach. However, it seems to me that you could likely obtain even better performance by using a more powerful graph representation and model such as the GREAT model in [2]; it would be important to compare against that approach to determine what, if anything, the SCS representation and GT-Transformer models add.The claim that the experiments with graph variants "explain" the results of Ahmad et al. (2020) seems too strong. These experiments are conducted with an entirely different architecture and input structure. Maybe these experiments suggest a possible explanation, but there could be many other alternate explanations.The paper contains some grammatical and notational errors; please carefully reread for these. I have listed some of the things I noticed below, although there may be others.### Minor errorsIntroduction: The sentence "Programming languages are context-free formal language, an unambiguous representation, Abstract Syntax Tree (AST), could be derived from a source code snippet" seems grammatically incorrect.Figure 2 caption: Some quotation marks face the wrong direction.Last paragraph of 2.1: some of these sentences are sentence fragments.Third paragraph of 2.2 "its a representation without noise on how the tokens interact": What do you mean by noise here?Second paragraph of 3.2 "For example, One graph neural networks (GNN) layer": typo in capitalizationFirst paragraph of 4.2 "This is done by two sub-blocks an edge block": should there be punctuation between "sub-blocks" and "an"?Second paragraph of 4.2: The notation used here seems inconsistent; $\phi^e$ is defined but never use it, then ${E'_{ij}}^{(\gamma)}$ is described and used but never defined. Are those the same?### References[1] Allamanis, Miltiadis, Marc Brockschmidt, and Mahmoud Khademi. "Learning to represent programs with graphs." International Conference on Learning Representations. 2018.[2]: Hellendoorn, Vincent J., et al. "Global relational models of source code." International Conference on Learning Representations. 2019. This paper presents the problem formulation for a hypothesis testing within a reinforcement learning paradigm. In particular, given a hypothesis represented as a formal, templated language, an agent needs to perform a sequence of actions and check whether the hypothesis holds true or false. The paper proposes multiple methods since it is shown that traditional RL is insufficient for an agent to learn to test hypotheses. One of methods is to use pre-training taking advantage of more detailed information (represented as a triplet of pre-condition, action sequences, and post condition) about the hypothesis verification. Authors further investigated training after a pretraining step, and hypotheses that cannot be represented as a triplet.Hypothesis verification using RL framework seems an important research direction. Based on the experiments presented in this paper, my understanding about the hypothesis testing is that it is about performing a complex task relevant to the hypothesis, and answering whether the hypothesis turns out to be true or false. Providing a triplet of information seems to be similar to the idea of breaking up such a complex task into comprehensible smaller tasks (like goal vs. subgoals) since the way pre-training works in this paper is precisely nudging the agent to perform subtasks. In this regards, I am not quite convinced the novelty of the formulation or methods. In MineRL competition 2020, for example, the goal of the competition is to mine diamond. Agents receive rewards for obtaining items which are necessary to mine diamond. Hence, one may think that the agent needs to implicitly generate hypotheses and test the hypotheses to finally obtain diamond. This seems a harder task than hypothesis verification tasks this paper considered. How about "Montezuma's Revenge"? Unfortunately, I found this paper paper does not adequately provide neither novel methods, results, nor insights. I would like to reexamine the paper once the authors respond focusing on which parts are particularly novel.Minor points.Figure 2. "the the" for color switchPage 23, missing reference "??"One switches (pg 5, color switch)Question"RL agents struggle to solve the problem." Is this because of fundamental flaws in RL formulation (sparse reward, credit assignment problem, etc.) or just not having enough time steps (say, 1e9, 1e10 ...). Without having experiments on a simple hypotheses and smaller grid-world, it is difficult to examine the performance of standard RL agents.   The paper "Empirically Verifying Hypotheses Using Reinforcement Learning " proposes learning strategies of agents where goals are to validate or invalidate hypotheses about the world. This is done via RL, using the structure of triplet {pre-condition; action-sequence; post-condition} to drive the learning with intrinsic rewards. While the problem is interesting, I found the paper difficult to read as the task is ill-defined in the section 3 where many notation definitions are missing and some notations are reused in different contexts with different definitions (e.g.  o=sw,h in the first sentence of last paragraph of section 3 and o is an observation from the world in the beginning of section 4, a is {false,true} in section 3 but can also be a move in the world later). Also, the state includes the dynamics L_W ? I understand that there are two levels of actions, states and observations (moves in the world and final decision w.r.t. the hypothesis) but this presentation could be made much clearer for the reader. More importantly, my main concern is about the experimental setting. In the setting formalized in section 3, I understood that we get hypotheses and ground truth about their validity in the sampled world. Ok, but if we got the ground truth, what the learning could be used for. The only setting that matters in that case is from my point of view a meta-rl setting where we train agents to decide from a training set of worlds (for which we know the ground truth of hypotheses), which we hope well generalize for other worlds with similar properties. However, I cannot see any discussion about this, neither in the formalization section nor after in the experiments. Reported experiments only give results at train time from what I understood, which is clearly not sufficient to demonstrate the benefits of the approach. How the strategies generalize, and in which conditions they do is of crucial matter here. In the motivation from section 3, authors justify their approach by claiming that triplet hypotheses can be useful for interfacing symbolic solvers. It would also have been nice such kind of setting. Also, I do not understand the justification of natural language hypotheses (i.e., "scaling annotations to untrained humans").   It would have been nice to have experiments more related to these announced interests,  to highlight the usefulness of the approach.At last, do we use text encoding for natural language hyoptheses described in the special cases setting ? Please detail.  Globally, I feel that the paper needs a full rewritting to improve clarity. And that experiments must include generalization results.   ###############################################################Summary:This paper provides a new method for estimating the generalization performance of neural architectures. This method used the sum of training loss as a criterion. The paper gave some intuitions about the method from the perspective of Bayeian model selection. ###############################################################Reason for Score:Using training loss to improve generalization performance is unreasonable. And the paper didn't give some convincing reason to this method. The method has no theoretical guarantee, and its analogy with Bayesian model selection seems problematic.###############################################################cons:1, The method in this paper purely used training loss as a criterion for generalization performance. This is unreasonable. And the paper didn't give some convincing reason to this method.2, The analogy with the Bayesian model selection is problematic. In Bayesian model selection, the parameter in the following step is the posterior estimator based on previous data. This paper think of the SGD optimizer as a way to find the posterior estimator. This is problematic. The paper studies recent approximation bounds of coreset-based pruning strategies for neural networks.  It concludes that the bounds in certain cases can be very conservative (guarantee much lower accuracy than seen in practice), especially if the network can be fine-tuned after pruning,  and should not be used as a guide to estimate expected accuracy.  While the conclusion that the bounds can be conservative is not unreasonable -- that was not the point of the papers it tries to criticize, bounds do not have to be good estimates.  Furthermore, I find the paper to be poorly and confusingly written, with many questionable choices done in setting up error metrics, and the experiments (please see details below), so I recommend to reject it. A bound on performance and an estimate of performance serve different purposes.  Even tight bounds that can not be improved may give poor estimates, but they are still useful in providing theoretical understanding of a problem, and possibly in safety-critical applications where one needs to guarantee worst case results.  So the criticism of the paper may be besides the point of the papers it analyzes.  They are among the first to show that coreset-based methods allow any kind of theoretical approximation guarantees, but they do not claim that the bounds are good predictors of performance.   The present paper would have more value if it attempted to improve the bounds, or perhaps develop accurate estimators of accuracy vs. pruning level, or maybe propose a pruning method which is more effective empirically. Furthermore, for a given neural network that has been pruned it is fairly inexpensive to numerically evaluate the approximation accuracy w.r.t. to the full model on a validation set -- so I do not see much practical value of the exercise of trying to infer it from bounds.  Furthermore, for pruned networks that have later been fine tuned,  indeed the results of the papers do not apply, so the bounds unsurprisingly may have little to do with empirical results. Detailed comments (non-exhaustive list of issues, but sufficient for my rating):1) You go from additive and multiplicative bounds to error rates in lemma 3.1.  Just because epsilon > (f(x)i - f(x)i')/2  does not guarantee that it will make a mistake (it has the ability to make a mistake, but it does not have to).  Lemma 3.1. is on the one hand elementary, but on the other hand it introduces another layer of approximation between accuracy and pruning. 2)  "guaranteeing that 50% of the development set labels that are correct will remain correct"...   The goal of approximating a neural net is to make sure that the output of the approximate NN matches the original one -- how is it relevant whether the original was correct or not w.r.t. to the target labels?  Why do you condition on predictions being correct?  3) 'fine tuning can recover large amounts of accuracy while simultaneously maintaining of increasing approximation error" ... That doesn't make sense.  Do you mean 'improving' or 'reducing' approximation errors? 4)  Sections 2.1 (and same for 2.2).   A lot of important details are left to appendix -- this section gives very little useful information to understand the method.  Where is algorithm 1 -- is it in your appendix or is it in Baykal's paper?  Where are the theorems -- in your paper / appendix / Baykal's paper?  What is the 'importance', how is it computed? A more informative intro would be much appreciated. 5) The easy problem is a NN with 90 to 95% of weights set to 0.  The network is refitted, and 0-weights are re-enabled?  What does it mean for 0 weights to be re-enabled?  If the pruning strategy keeps the 0-edge -- does it mean the edge is active and the network is non-sparse?  This is very confusing (either poorly described, or this is an artificial corner case). It seems hard to imagine that a coreset pruning method that is given a sparse network will make it less sparse.  If on the other hand the accuracy bounds somehow ignore edges which are already 0 -- then it seems like an easy opportunity to derive better bounds?6) For the above easy network (with mostly zero weights) -- the coreset algorithm gives more than 50% of elements with probability 0 of being sampled, so the conclusion is that if we ask for 50% sparsity -- it will run forever?  This whole discussion just seems very strange and confusing. 7) does 90% sparsity mean that there are 90% zeros or 90% non-zeros?  Maybe use clearer language. 8) The discussion of scaling up the bounds due to total homogeneity --  also seems very strange: the paper claims that positive homogeneity allows to 'arbitrarily increase or decrease sample complexity by scaling the appropriate weight matrices by a constant factor'.  Is this for the additive or multiplicative bound?  The paper says "investigating the full implications of this observation are out of scope of this paper".  I would expect some discussion here.  For the additive bound -- if you scale up the weigths in NN, the relative approximation error will be much smaller, so isn't it natural to require larger sample complexity?  - Summary    - This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function     - The authors demonstrate their method on 3 Atari games and use A3C as the training algorithm- Strengths    - To the best of the reviewers knowledge, this is the first work to apply this type of visual explanation to RL    - The interpretable agent performs on par with the black box one.- Weaknesses    - How were the points/frames in figure 2 chosen?    - To my untrained eye, the attention masks in figure 2 aren't very interpretable.  Human studies to verify that the explains actually help the humans understand (or predict) the agent's decision would be very helpful in this regard.    - I am uncertain that the contribution is enough to warrant publication at ICLR.  While this is the first work I am aware of to apply this type of visual explanation to RL, using attention masks is well known in the literature (Mascharka et al, 2018; Fukui et al, 2019) and it doesn't appear like any considerable modification necessary to apply it to this domain.    - Using the attention masks to to interpret the decision of the agent based on just the current frame is misleading.  This is because the attention is conditioned on s_t, not o_t, where s_t the output of ConvLSTM(o_t, s_{t-1}).  The consequence is that we do not know whether attention is high for a given location because of the visual information in o_t or the visual information in any other observation. While it is entirely plausible that the most influential location in the ConvLSTM output is most correlated with the current frame, this hasn't been shown.- Suggestions    - Show both the frame and the frame with attention in Figure 2.  Currently it can be hard see the content of the frame.- Overall    - Overall, I am not convinced the contribution is enough for publication at ICRL.  More importantly, without additional verification the attention masks cannot be used to explain the decision based on the current frame as they are conditioned on the current frame __and__ all previous frames.  - References    - Mascharka et al, 2018: Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning    - Fukui et al, 2019: Attention Branch Network: Learning of Attention Mechanism for Visual Explanation **Summary**This paper presents a novel method of incorporating structural and relational constraints into the generative process of sequences. This approach is explored in both the music and poetry domains using several generative approaches. An analysis of the resulting sequences finds that they successfully incorporate structure.**Strengths**Novel procedure for extracting constraints from training dataResults explored in multiple domainsExample of interactive user modifications, showing that the constraints are human-interpretable and useful for guiding generation toward a creative goal**Concerns**My first big concern is a lack of comparison against other methods for using structural constraints in the generative process. A couple examples:For music generation, I feel a comparison against StructureNet: INDUCING STRUCTURE IN GENERATED MELODIES by Medeot et al. (http://ismir2018.ircam.fr/doc/pdfs/126_Paper.pdf) is needed. The method for extracting structure is different, but the goals are very similar. I think its important to demonstrate that the method your paper proposes is more effective.For poetry, Id like to see a comparison against a method similar to the one described in Combining Learned Lyrical Structures and Vocabulary for Improved Lyric Generation by Castro and Attarian (https://arxiv.org/pdf/1811.04651.pdf).Both of these methods have the advantage of using a relatively simple technique to extract structure from the training data, so Id like to see evidence that the more complex method proposed in this paper provides an advantage.My other big concern is the lack of rigour in the evaluation process. This paper both proposes a new method for generating sequences with improved structural characteristics and several new methods for evaluating high level structure. However, very little time is spent validating that these evaluation methods actually accomplish their goals. I would like to see something like a systematic human evaluation of outputs that shows human perception agrees with the automated evaluations.A few more concerns:Sections 3 and 4 are very thorough in their descriptions, but I found it difficult to keep all the variables and their uses straight in my head as I was reading through. I think adding some diagrams and a few simplified concrete examples would greatly improve this section.Section 4.1: Either a citation or more architectural and training details are needed about the LSTM-VAE.I found Tables 1 and 2 confusing. The bolding doesnt seem to match up with the best scores. Am I correct that in Table 1, the lowest Low-Level score should be best and the highest High-Level score should be best? For example, shouldnt the MusicAutoBot row be bolded in the first column? Similar issues for Table 2. I think providing more visual separation between the different scoring mechanisms and explicitly saying higher is better or lower is better would help clear this up.Last 2 paragraphs of Section 5: There are a couple references to Figure 5, but there is no Figure 5 in the body of the paper.The user modifications discussed in the final paragraph of Section 5 are very interesting, and show a real advantage to having interpretable structural constraints. I think this would be much more clear if you included a concrete example of what it looks like to modify the program to produce the new output. Even just a snippet of what the original program looked like, then the modification, then the new output. This is very much related to my comment on Sections 3 and 4: having some concrete examples would really help clarify exactly whats happening.**Additional minor feedback**In the abstract, second sentence, s/generate/generation/.Section 2, 2nd paragraph, indicates whether w and w satisfy. I think the second w was meant to be w.Section 4.2, Approach 1. s/rejecting sampling/rejection sampling/.**Questions for the rebuttal period**Please address the concerns above.Id also like to hear your thoughts on how this can be expanded to music more complex than monophonic folk melodies.**Recommendation**My recommendation is to reject due to the concerns listed above, in particular the lack of comparisons to existing structural constraint methods and rigour of the evaluations. There has been an ongoing debate about the role and importance of explicit and implicit regularization in deep learning. This paper attempts to address this issue by arguing that explicit regularization is required for the generalization of predictive probabilities, which may not be observed under the 0-1 loss. The paper provides some discussion and numerical evidence to support the claim.Although the paper makes some interesting remarks, the idea of looking at the predictive probability rather than classification error is certainly not new. This may be part of the story, but is unlikely to a central one. In particular, this does not explain why overparametrized models may still generalize well in the regression setting. The only theoretical analysis in the paper is based on eq. (3), which is almost trivial and does not reflect any characteristic of deep models. I find the arguments totally heuristic and not convincing enough to justify the necessity of explicit regularization. In terms of methodology, the paper examines only complexity control through weight decay and two existing explicit regularizers and does not propose any new regularization strategy.Minor comments:1) The predictive probability should not be referred to as predictive confidence. In particular, level of predictive confidence and confidence control may be confused with the standard use of confidence in level of confidence and confidence interval.2) The coined terms stochastic switch and deterministic score are also misleading. The score usually means the derivative of the log-likelihood. This paper aims to improve the "reliable predictive probability" when using a single deterministic deep net. The paper proposes explicit regularization methods to achieve this goal.Overall, I found the paper hard to follow, often unclear in what the goal is, jumping quickly from one point to another one. Also many crucial technical steps seem incorrect or at least not well motivated/discussed. A few examples : 1. What do the authors exactly mean by reliable predictive probabilities, which is at the core of this paper? In the introduction, calibration, overconfidence of the model and uncertainty representation (like in Bayesian statistics) are discussed. Does reliability refer to all of this at once, or is it some other concept? How is it defined? Given that this is not clear to me, it makes it hard for me to understand if  the paper provides some approaches to improve it.2. The inequality in Eq 3 could be discussed in some detail. My understanding is that each \phi_k is replaced by 1-\phi_mx in the second part, i.e., the maximum possible probability 1-\phi_mx is used for each misclassification. This seems very extreme to me.  Moreover, these new probabilities do not sum to one any longer: \phi_mx + (K-1) * (1-\phi_mx) = 1+ (K-2) * (1-\phi_mx) >> 1, which grows linearly with the number of classes K (if \phi_mx <1), which seems weird. I wonder if this extremely loose bound itself causes the high cost of misclassification that is later derived as a key problem.3. It would also be good to discuss why it is useful to consider an upper bound on a quantity that gets maximized (log likelihood). Maximizing a lower bound would seem more natural.4. The divisor \alpha_x,y is introduced in Eq 3 as a mechanism to achieve equality. As I understand Eq 3, however, \alpha_x,y cannot be a function of y at the location where it is introduced in Eq 3, as it is  outside of the expectation E_y|x. It would be good to clarify this before deriving further insights.5. The L2-norm of function f^W is discussed in several places before it is defined in Eq 5.6. Below Eq 4, \tau^\star is defined in a maximization problem. However,  \tau can be moved outside of the expectation: E[ \log(\phi/\tau)]=  E[\log(\phi)] - \log(\tau), hence this is trivially maximized when \tau approaches 0, which does not make sense to me.Apart from that, also the language of the paper could be greatly improved. The idea proposed in the paper is simple - "predict" future attention weights using past attention weights. A 2D CNN is used to mix previous N layer's attention maps. Strictly, this is also not "predicting" but instead generating". There is no supervised loss here. The authors introduce a PA-Transformer model. The idea is to use previous attention maps to augment future attention weights. A stack of N previous attention weights is modeled with 2D CNN to generate future attention maps. The idea of predicting attention weights (or generating them) is not new (see https://arxiv.org/abs/2005.00743). The difference here is that there is a 2D CNN to model relationships between N previous layers. This is somewhat a pretty incremental extension of the Synthesizer-Transformer model. There is also insufficient convincing evidence that using previous layer's attention to generate future attention weights is beneficial. I think the experiments are lacking. The experiments on GLUE are only comparing against BERT (the least the authors could do is to compare side-by-side with at least a few other models). Machine translation datasets are tiny and ablation studies are unconvincingly run on SST and SNLI. The authors only run experiments using a preloaded checkpoint of BERT and do not apply their architecture to actually pretrain BERT which is also one weakness of this work. Hence, the paragraph beginning with "pretraining" is misleading". The results on GLUE are also weak and could be a result of variance over the existing BERT model. The authors should also discuss how this can be implemented in a decoder setting since the current setup will disable causal attention.Overall, I recommend a clear rejection. I think the key selling point and hypothesis behind this paper (using prev N layer attention) is not well supported. Experimental settings are also weak and there are insufficient convincing experiments to feel that this architecture is doing something useful.  Overview:This paper proposed two methods, K-starts and dissipating gradients approach to sparsity the network before training and achieve better performance than a random dropout.Strength bullets:1. The idea is interesting, but needs more effort to complete it.Weakness bullets:1. poor writing, for example, citation error in abstract line 3 //? exist in 2.1.2 line 2 and the last line of 3.1  // the explanation of sparsity is colloquial in 2.1.3 2. As for SparseMatrices in equation 1, does it mean the explanation of K? It's easy to confuse with a minus sign.3. because the classification on MNIST and FashionMNIST is too simple, the improvement of k-starts is marginal4. As dissipating gradient dropout needs training for a couple of epochs, it's unfair to compare with random dropout.5. The fatal limitation is the lack of comparison with previous methods and related works, need to compare with other pruning from scratch methods, like GraSP (https://arxiv.org/abs/2002.07376), SNIP (https://arxiv.org/abs/1810.02340). Summary:The paper starts from the Linear Symmetry-Based Disentanglement (LSBD) in [1]. The existing approaches to evaluate disentanglement require the supervision of the dataset and it is impossible for the unlabeled data. In this regard, the authors propose the new quantifying metric for disentanglement under the 'limited supervision' setting. Also, under this metric, authors provide a VAE-based framework that exploits limited supervision for the proposed metric.==============================================================================================================Reason for score:Overall, I vote for clear rejection. I was hooked by the title and abstract. However, I could not find any novelty and insights to compare several measures for the disentanglement score. Furthermore, experiments are not enough to support the proposed method (see cons). I suggest the resubmission with more experiment results and justifications of their proposed methods. =============================================================================================================Strong points (pros) :(1) Mathematical definitions to interpret disentanglement are clear (but complex). The quality of writing is not bad.(2) I like the trial to quantify disentanglement under a limited-supervised setting since it could be applied to real-world settings.==============================================================================================================cons : (1) The comparison with the other disentanglement metrics under supervised data is crucial. The proposed metric which can be applied under a weakly supervised setting should have a certain amount of consensus with other metrics. The authors need to show through experiments and provide justification by pointing out similarities and differences with the other metrics. This part is very important to persuade reviewers and readers. (2) I was caught off guard at the experiments since there doesn't exist any baselines in Figure 4. The authors need to add baselines to compare with  5I5458 .(3) The idea underlying paper is quite overlapped with [2]. As a baseline, [2] can be used to analyze the performance of  5I5458 . Also, the authors need to emphasize the novelty of their method by comparing it with [2].=====================================================================================================Minor :The word "limited supervision" is quite confusing. Instead, I suggest "weakly-supervised". ======================================================================================================References [1] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework, ICLR 2017.[2] Weakly-Supervised Disentanglement Without Compromises, ICML2020. The paper introduces a series of new datasets and task and investigates the inductive bias of seq2seq models. For each dataset, (at least) two hidden hypothesis could explain the data. The tasks investigated are count-vs-memorization, add-or-multiply, hierarchical-or-linear, composition-or-memorization. The datasets consists of one sample with varying length (amount of input/output pairs), which is denoted as description length. The models are evaluated on accuracy and a logloss. An LSTM, CNN, and Transformer are all trained on these datasets. Multiple seeds are used for significance testing. The results suggests that LSTM is better at counting when provided with a longer sequence, while the CNN and Transformer memorizes the data, but are better at handling hierarchical data. What this paper excels at is a thorough description of their experimental section and their approach to design datasets specifically for testing inductive bias, which I have not previously seen and must thus assume is a novel contribution. However, I lean to reject this paper for the following reasons- The paper tries to fit into the emerging field of formal language datasets for evaluating the capacity of deep learning methods. However, they do not build on any of the recent papers in the field. A new dataset, especially a synthetic one, should be well motivated by shortcomings of previous datasets and tasks in the field. I find the motivation and related works section lacking in that sense.- We already know that LSTMs can count https://arxiv.org/abs/1906.03648 and that transformer cannot https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00306- It is not clear to me why these results are important? Who will benefit from this analysis? Why are the current AnBnCn and DYCK languages that formal language people work with insufficient?- LSTMs do not have the capacity to perform multiplication. I dont know why your results suggest otherwise. You would need to incorporate special units that can handle multiplications in the LSTM, such as https://arxiv.org/abs/2001.05016 This paper proposes to use an intrinsic reward based on uncertainties calculated from temporal difference errors. The approach, called Temporal Difference Uncertainties (TDU), estimates the variance of td errors across multiple (bootstrapped) parameters, for a given state, action, next state and reward, where variability is due only to variance in parameters. The other addition is to learn a separate set of action-values that use this intrinsic reward, from the bootstrap set. Actions are then taken by randomly sampling an action-value function from the combined set. The idea of using td uncertainties as an intrinsic reward is interesting, and should help avoid the fact that bootstrapping value functions alone likely provides insufficient exploration. However, the paper in its current form is not yet ready for publication for two main reasons. First, there are significant gaps in motivating and detailing the TDU technique. In sections 1 and 2, TDU is motivated as an exploration method deriving an intrinsic reward from the agents uncertainty over the value function.  In particular, it is heavily implied that TDUs uncertainty estimate does not suffer the bias indicated by Lemma 1.  But it is not detailed how the quantity \sigma(\tau) estimates value function uncertainty, nor how it is unbiased. Why is it a better choice than, for example, directly using the bootstrapped set of action-values? There is some intuition provided, as well as an informal argument following from the distribution p(\delta|\tau) to uncertainty over value function parameters \theta.  But this intuition and lack of formality is at odds with the formality highlighting that estimating value function uncertainty is hard.Second, the empirical results appear to be obtained with 3 runs and do not provide significant evidence of improvements. Considering the apparent variance in all techniques, no meaningful conclusions may be drawn from comparison between the average of so few samples. The reported shading is also not explained, though I suspect it is standard errors. For only 3 samples, standard errors are not reliable, and in either case here are quite large. It would be better to run on few environments, and try to provide a stronger claim about the role of tdu. Even better would be to also highlight if results change significantly with changes to hyperparameters.    Below, more detailed comments are given about the paper. It would be useful to better discuss the use of Bootstrapped DQN (BDQN), and why this approach improves on BDQN. In Janz et al. 2019, there is a thoughtful and detailed analysis of one particular failing of BDQN methods (cf. section 5.3) including with a specific demonstrative environment (the binary tree MDP with randomized actions.) While this TDU paper does make a compelling argument that TDU addresses certain challenges of BDQN methods, TDUs performance on the binary tree MDP (or one sufficiently similar) is not included.  This is a nontrivial omission.  If TDU succeeds on the environment, then a significant challenge is overcome. And if TDU does not succeed, then it is still worthy of publication, just with some discussion.The theoretical contribution seems like it could be interesting, but it is not fully clear. Is this a statement about all methods that try to estimate some form of value uncertainty? Or those based on PSRL? Lemma 1 is summarized as the harder it is to generalise, the more likely we are to observe a bias in value function estimation, which is helpful, but warrants further detail.  A worked, demonstrative example may be edifying here.It would be useful to discuss more why estimating TDUs is easy (as per the title of Section 3). It looks like the quantity in Equation 4 relies on p(theta), which was stated to be difficult to maintain. A bootstrapped distribution is used to get, as described in Section 4, but Section 3 did not make it clear to me why estimating sigma(tau) was straightforward. There are two additional statements here that could use clarification:1. Why does using the standard deviation put this bonus on the same scale as the reward?2. The paper says: We compare TDU to (a) a version where Ã is defined as standard deviation over Q and (b) where Ã(Q) is used as an upper confidence bound in the policy, instead of as an intrinsic reward (Figure 2).  These choices are very interesting, but should be explained in more detail.  Especially why their results indicate that the key to TDUs success is that the intrinsic reward relies on a distribution of TD-errors to control for environment uncertainty.Minor comments:1. neural networks, which are prone to overfitting and tend to generalise through interpolation (e.g. Li et al., 2020; Liu et al., 2020; Belkin et al., 2019)  This is not an accurate representation of the cited results.  Those papers do not argue that neural networks are prone to overfitting.  Indeed, they argue the opposite: neural networks do not seem to overfit in the overparameterized limit, despite their tendency to interpolate the training set.  That is, generalization is maintained, where high generalization is characterized as per convention: low prediction risk w.r.t. an independently sampled test set.  While this is a significant mischaracterization, it does not appear to be critical to the paper, so we choose not to include it in the rejection justification.2. The colored bar chart to render parameter sweeps (Fig 2 left, center left, and center right) is a bit distracting. The use of color does not seem to be helping here. 3. The related work section describes and cites some of the same work as the introduction, and is hence some of it is a bit redundant.  There is no need to duplicate that part of the survey in this section.4. There is an incomplete sentence: While this can be effective in sparse reward settings, ... it can also lead to arbitrarily bad as the exploration (see analysis in Osband et al., 2019).5. In appendix section D.1, there appears to be a missing citation: Finally, instead of n-step returns we utilize Pengs Q(») as was done in (Value-driven Hindsight Modelling: citation needed).6. This work looks at computing variances directly, and using that variance for uncertainty, rather than using the bootstrapped action-values. This citation seems relevant: "Context-Dependent Upper-Confidence Bounds for Directed Exploration", Kumaraswamy et al., 2018.   This paper analyzes the current limitations of existing magnitude-based pruning methods. First, the paper focuses on the similarities between three methods and then focuses on the redundancy in large networks. The paper also analyzes the weight distribution for a well-trained network and propose CDWA as a way to prove this distribution.  My main concern with this paper is the contribution to the field. Two main comments:1. If I understand correctly, the paper focuses on the distribution at the parameter level and shows it follows a Gaussian Distribution. If that is the case, that is a well-known result especially when the network is trained with a weight decay to regularize.Then, the paper shows the similarities between three magnitude based methods (L1, L2 and GM). I guess I am not very surprised the first two are similar in the structure and slightly different in performance as that is expected. The rank correlation is interesting tho. Would be more interesting to show the same comparison with importance based methods or other methods not using the magnitude only. 2. In terms of redundancy, I am not sure I understand the point there. It is also known that training of neural networks leads to redundancy and that is the main reason for some particular postprocessing or compression algorithms based on that (see Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation -- Denton et al; or Compression aware training of Neural Networks -- Alvarez and Salzmann; and some others in the literature). Magnitude based approaches do not consider the interaction between parameters and therefore still lead to redundancy. Methods aiming at reducing redundancy usually do not provide good compression ratios as the architecture is harder to prune. The paper also suggests l2 is regarded as importance. I think that should be reconsidered as L2 is the magnitude and there are many works using other criteria referred as importance (for instance gradient *magnitude). All in all, not sure the real contribution to the community.  Summary and contributions: Briefly summarize the paper and its contributions:The paper considers node representation learning in the setting of incomplete node attributes. The proposed method transforms node features to a latent space endowed with the Wasserstein metric, uses the Wasserstein barycenter of nodes neighbors to recover missing features, and finally transforms them back to their original space using a proposed inverse mapping. The algorithm has been tested on benchmark classification tasks as well as matrix completion tasks.Strengths: + The paper appears to be a novel application of ideas from both matrix completion and optimal transport. + In the node classification experiments, multiple benchmark datasets were used and multiple methods to complete missing values were considered as baselines. Weaknesses: - While the experiments show promising results, the authors do not explain the choices  behind their method. - Why do the authors use SVD and define the distance between transformed features in the way they did? Is there a reason for using the exponential function in the transformation other than for inducing positiveness of probability? How does the inverse mapping come up and why does it work? - In the classification task experimental section, it is not clear why methods, like IGMC (used in Section 4,2) paired with an MLP, werent tested for the classification tasks. As for the choice of architectures, is there a reason for using 7 WGD layers versus 2 GCN layers for the baselines?- Because many choices are not justified or described, a number of questions are left unanswered. What is fundamentally different between a WGD layer and a GCN layer? What is the influence of the number of WGD layers? And what is changing in the representation after each WGD layer? What does the induced latent space look like? How does it relate to the complete feature information setting? Is the dimension of the latent space equal to that of the original space? Why does feature recovery using Wasserstein barycenter update trumps simple average or k-means of neighbors, what makes the metric be more adequate than Euclidean distance? - The authors claim that their work is the rst work to compute embeddings of a graph with incomplete attributes directly. but their proposed algorithm is two-stage, first matrix completion and then neural network training. The paper seems to be more relevant to data pre-processing than representation learning itself. - The authors conducted experiments in two settings, partially missing and entirely missing. It seems that the performance of the proposed method is exactly the same in both settings. (Figure 1) This is curious. Can the authors comment on what is happening in this case?- In multiple instances, the authors introduce experiments but dont comment on them or explain the similarity or differences in performance between all of these experimental settings.Clarity: Many terms are used without properly defining them; the equivalence of p-Wasserstein distance to eq. (4) is unclear unless readers are already familiar with optimal transport; the iterative Bregman projection and Gram-Schmidt process have not been elaborated clearly. The statement However, most of the methods, which embed nodes into a lower-dimensional Euclidean space, su I think the use of QPyTorch for the experiments here invalidates the results since the intermediate matrix multiplies are done in single precision (FP32), and so are more optimistic than a pure 16-bit implementation. (This is both according to the authors Sec 4, experiment setup; and according to the QPyTorch paper arxiv:1910.04540, Sec 3 intro.) For these kinds of experiments to be meaningful, they have to be done on native 16-bit hardware which luckily is becoming more common, e.g., Google's TPUs or the newer NVIDIA GPUs.There are two other problems. First, it is not clear how stochastic rounding would be implemented in hardware. Doing it for every MAC operation could likely be even more expensive than just doing 32-bit MAC operations, since it involves the generation of random numbers, division, etc. Second, Kahan summation takes up twice the weight storage, so a more detailed calculation is needed to compare any hardware/energy savings to use that instead of just 32-bit.As an aside, it may be interesting in Figure 1 to zoom in on the initial part of training to understand where the difference between 32-bit and standard 16-bit comes from in early training since at that point, the gradients are generally larger than later on in training. The paper deals with debugging of black-box deep reinforcement learning (RL) agents to better understand and fix their policies. The authors propose diverse tools for, among others, visualizing the state space in terms of calculated statistics, analyzing the taken actions across learning episodes or exploring the replay buffer. The authors also propose a workflow for using the proposed tools. The resulting Vizarel tool is evaluated in terms of an exemplary walkthrough.The approach follows an interesting direction towards explaining RL agents, but I am missing concrete design decisions and empirical evaluations for the proposed set of visualizations. While evaluating interpretability/explainability is difficult in general, it is still essential for such kind of contribution. I feel that the authors should explore some kind of user study (as conducted in cited works, such as contrastive RL explanations [1]), where end-users need to solve a challenging RL-related task and use the tools for actual debugging/search for improvements. I am aware that other explainable RL approaches based on counterfactuals or attentions might be easier to evaluate (and might not require end-users for acceptance at a conference), but I still feel a deeper evaluation is necessary here. To this end, an evaluation then should include mentioned related works on explainable RL in order to empirically prove the superiority for specific tasks / use cases. To this end, I am wondering it which situations the tool would be beneficial over other explainable RL approaches. For example, does the approach work well for procedural, hard-exploration tasks? More specifically, which tools of the framework would I use and which potentially not? Again, I feel like such questions can only be answered by asking actual end-users. Lastly, there seem to be numerous minor errors and in the references, as publication years are often missing. [1] van der Waa, J., van Diggelen, J., Bosch, K.V.D. and Neerincx, M., 2018. Contrastive explanations for reinforcement learning in terms of expected consequences. arXiv preprint arXiv:1807.08706. 1. I am confused on the motivation part. Although the authors provide two examples (the prediction of a users social class mainly relies on his/her social network structure) to illustrate the benefits of disentangled representation for certain downstream tasks and I agree on the statement, it raises another question that what kind of representation, structure component or attribute component should be used for a specific task. 2. If we take a further through on the relationship between structure component and attribute component, it should be separated into three parts, a common part, structure-specific part and attribute-specific part. In my eyes, the authors put the common part and structure-specific part together. Why? What is the benefit? So, the name "structure/attribute component " is not accurate. 3. I do not see too much novelty in the technical part (edge reconstruction and mutual information minimizing). I am fine with this. But I do not see a deep insight of the problem-solving philosophy, either.4. It is nice to see the performance gain in the experimental part. But  I want to know the authors use one type or two types of representation. ### Contributions ###* The authors study a new kind of problem, the robustness of few-shot learners against adversarial attacks * The paper propose a new meta-learning algorithm ADML that uses adversarial and clean examples during meta-training (both for train-train and train-test). ### Significance ###Combining meta-learning with adversarial robustness is an interesting research direction that might inspire follow-up work. However, it remains unclear if any practical applications would benefit from adversarially robust few-shot learners. A clearer motivation would be desirable.### Originality ###In principle, combining gradient-based meta-learning such as MAML with adversarial training (similar to the proposed baseline MAML-AD) is a natural fit that does not require novel research. The novel methodological contribution of ADML is that meta-train-train is done separately for clean and adversarial examples and that meta-train-test evaluates the loss of the adversarially trained parameters on clean train-test data (and vice versa). While this is an interesting idea and empirical evidence seems to support this idea (see below for some major caveat), the paper lacks any theoretical justification for ADML. At least a well motivated intuition would be required.### Clarity ###Some details remain unclear:* From Algorithm 1, it seems that 2K samples are used for meta-train-train and 2k for meta-train-test, because clean and adversarial examples are constructed based upon different samples. This would make it an uneven comparison to standard meta-learning, that would do actual K-shot training.* For adversarial attacks like PGD, hyperparameters like step-size and number of steps are not stated### Quality ###The paper does not follow established procedures for evaluating adversarial robustness:* the threat model is not clearly stated* the main evaluation in the paper is done based upon a weak attack, namely FGSM* most importantly, the authors are using only transfer attacks for robustness evaluation (and also during meta-training). That is: adversarial examples are computed based upon a pretrained network  (paragraph after Equation 1). This is not at all motivated; the natural thing would be to compute adversarial examples always based on the current model (\theta in Algorithm 1)In summary, the presented results need to be taken with care because all observed effects might be due to using weak attacks. ### Recommendation ###Because of the lack of motivation for adversarial meta-learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me. The authors introduce a pretrianing paradigm based on contrastive learning between multiple syntactic views of the same sentence. The method maximizes representations between different setence encoders when given the same sentence, and minimize the similarity to all other sentence repre sentations. The results on the infersent benchmark show competitive performance of the approach when compared to non-syntactic pretraining methods.There are a couple of concerns I have with the paper and some questions I will elaborate on in detail. In particular, this paper doesn't convince me that incorporating explicit structure is i) desirable and ii) necessary to achieve good results on the chosen benchmarks. I also have doubts about the comparison and the bechmark in general, especially given the fact that random encoders (wo/ explicit syntax) on pretrained word embeddings can do a pretty good job on those [2]. Therefore, at the current state of the paper I cannot recommend acceptance.Detailed comments:1) Are explicit, human designed syntax frameworks really needed? The authors argue that explicit syntax can help generalization, which I could maybe agree with, if trained parsers were perfect. Since they themselves are far from that and regularly fail on complicated sentence structures, I don't believe that this will assist in generalization. Rather the errors in parsing can lead to more systematic failures that the models might have a harder time to correct downstream. Prior work shows that Transformers trained on a (masked) LM objective learn about sentence structure implicitly [1]. In fact, although the authors "hypothesize" that explicit structure can help -- which they do not properly ablate in my opinion (see 2-3) --, the results in the paper show no clear advantages on their benchmarks when comparing to BERT. The increased complexity of the method (ie., relying on other trained models) should be warrented by showing a clear advantage in doing so.2) Fair comparisons: The models in the paper are trained on another pretraining corpus. Other baselines should have been trained on this corpus to have an apples to apples comparison. Furthermore, BERT models use much lower dimensional sentence embeddings which typically hurts performance. A way to make comparison fairer would be to apply a random projection of the 768 features to 4800 features. Note that this can have a dramatic impact even for BoW models on these benchmarks [2]. I also wonder how well a randomly initialized model would perform, that is, how much does the pretraining actually help? [2] shows that it might not be required at all. I think given the inductive bias coming from the parser and the use of TreeLSTMs, this effect might even be greater, because much of the information about the structure sentence is given apriori, which might even help random models to achieve better performance.3) What can we expect the pretraining to learn? For instance, wouldn't a satisfactory solution to the pretraining task for the 2 encoders to simply learn a bag-of-words representation? I believe that the large majority of the setences could easily be distinguished by that. Just memorize the exact words that occured in the sentence. I am not convinced from the paper that the model learns anything more semantic than that. More rigorous ablations are required to show the benefit of the pretraining method.My personal opinion on introducing explicit syntax into neural nets (note that this will not be part of my decision): Given all the evidence we have so far on pretraining LMs using generic model such as transformers, I just don't see any reason why we should still try to explicitly fit our potentially faulty and biased syntactic frameworks into neural networks when more generic models (such as transformers) can learn structure directly from the data. Our linguistic frameworks can still be super useful for understanding and systematically testing our models, but I think we should refrain from  introducing our potentially limited understanding of language into those models. A random thought: How would pretrained parsers help a model on twitter like text or child speech? [1] Jawahar et al., What Does BERT Learn about the Structure of Language? ACL, 2019.[2] Wieting et al., NO TRAINING REQUIRED: EXPLORING RANDOM ENCODERS FOR SENTENCE CLASSIFICATION. ICLR 2019. Summary: This paper proposes a new regularizer that can be plugged in gradient-based learning algorithms, which aims at solving the problems induced by unobserved confounders. And the authors provide the upper bound for one specific kind of distributionally robust optimization problem, whose uncertainty set is defined as the affine combinations of training distributions. And based on this the algorithm is proposed to deal with the problem of unobserved confounders. Experiments on three medical datasets validate the effectiveness of the method. Strengths: 1.The authors provide the upper bound of a group-DRO-like problem whose uncertainty set is the affine combination of training environments. 2.The authors provide the moment conditions for each pair of environments under linear settings with unobserved confounders and show that the gradients should not be forced to be zero. 3.Three medical experiments validate the effectiveness of the proposed method. Weaknesses:In spite of the strengths mentioned above, there are a few questions that are confusing. 1.As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2.As for the theoretical analysis: a)For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b)For Theorem 1, the core assumption is the expected loss function as a function of beta belongs to a Sobolev space, which is confusing. Could you provide some explanations of this assumption or give some examples of it?c)Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3.As for the proposed objective function:a)As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing?b)I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4.As for the experiments:a)The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives. Summary: The authors propose a connection between abstention and robustness to adversarial examples. Specifically, the authors contend that without the ability to abstain, any classifier can be fooled by an adversarial perturbation in the feature space. They additionally provide results and experiments concerning the proper selection of a hyperparameter that tunes the abstention.Pros: The authors include many theoretical analyses. A good effort is made to address the problem from several relevant angles. Cons: The paper is very difficult to read.  - The introduction does not provide a clean line of thought motivating the paper. I am not aware that the attack model considered, that of an adversary allowed to make arbitrarily large moves in a subset of feature space, with no constraints on the input space, exists elsewhere in the literature. The work cited (Brown et al. 2018) requires that unrestricted adversarial examples remain unambiguous to human judges. - The abstract cites results for "any classifier" when in fact the result seems to be for a nearest-neighbor style classifier, which is unusual given that the setting is deep networks.- The variables used throughout are difficult to keep track of.Major issues:- Theorem 4.1: This is obviously not true for all classifiers. The proof is very difficult to follow, and there are no useful details given in Figure 1. The result seems like it may be true by virtue of the fact that KNN with K=1 defines a hyperplane between any two points. Then a randomly chosen vector with probability 1/2 + epsilon inevitably crosses such a hyperplane eventually, but there's no reason to believe such a value in feature space could be reverse engineered or even lies within the range of F.- The testing in Section 7.1 does not seem to include the use of non-adversarial test samples. In evaluating whether or not a threshold is too strict to be of use, it would be necessary to evaluate on in-domain test samples as well as the training set. These results seem likely to be overfit to the training data. - The testing setup is unclear. Perhaps there are further details in the referenced papers, but it is not even clear to me how many classes are in the set. Is it only two, given the baseline of a linear classifier?- The clarity of the paper is severely lacking. It is difficult to follow the contribution of many of the theorems, especially concerning their generality or lack thereofGiven that I find these issues too concerning to recommend publication, I have not carefully checked all of the proofs. Summary:In this paper, the authors provide a series of experimens where they show that when dealing with a very small dataset, a single very deep network is outperformed by an ensemble of multiple more shallow networks. More specifically, the authors artificially create training sets from CIFAR10 and CIFAR100 datasets where the number of images per category is limited to 10-250 samples. Then, they compare the test performance of ResNet101, an ensemble of 5 ResNet 20 and an ensemble of 20 ResNet8, trained for classification with different loss functions, i.e. cross-entropy and cosine distance. The bottom line is that the ensembles work better and have a comparable computational complexity in FLOPs.Strengths:- The topic of the paper fits well in the paradigm of representation learning.- The work demonstrates that the community does not have a good understanding of what kind of models must be used when little data is available for training and brings attention to classical techniques for variance reduction.Weaknesses:- The paper is basically a compilation of experiments with no explanations of the observed phenomena. The authors perform a set of experiments with already known methods and merey propose the reader to look at the results. I would like to know not only that we need to do ensembles of small networks but also why these ensembles are more efficient than a single deep network in the low data scenario. Why do we observe the difference between using cross-entropy and cosine losses, depending on the network, dataset and its size?- The novelty of the paper is limited. It is already known from [1] that using ensemble methods in few-shot problems helps the performance a lot. Even if the authors propose a different evaluation strategy, referencing existing work in this field is still required.- Abblation studies are missing. To be more convinced by the experiments I would like to see how the performance differes if you vary the ensemble size and the single network's capacity. That may improve our understanding of the phenomena too. Experimenting with more datasets may help to answer the question of why the behavior of different loss functions is so different between the two used datasets.- A question of wheather a single ResNet101 with vanilla training is a fair baseline. Its been known [2] that to achieve better results on a small-data task it is beneficial to train deeper networks with proper regularization rather than shallow networks. Using an ensemble of N networks is identical to using a single network where each layer is N times wider; each convolutional layer will have N times more filters (that could be obtained by concatenating the weights of the original network), however the convolution operation now changes from a standard to a grouped one (has N groups). The resulting output of the fused network must be averaged across the groups to match the ensemble definition exactly. The group-separated convolutions restrict representational power of the network and introduce stronger regularization, which is most likely the reason for the ensemble to perform better. If we speak about regularizing ResNet101 what kind of regularization did you introduce to adapt it to the small size dataset? It is possible that vanilla training with higher weight decay and more data augmentation is not actually efficient in the case of ResNet101 on the small datasets. Instead, it may require introducing more aggressive data aufmentation [3,4] or some structural changes must be introduced, as for example in [5].Even though the direction of reserch is interesting and deffinitely useful for the community the work still needs development to be recommended for acceptance.[1] - Dvornik et.al "Diversity with Cooperation: Ensemble Methods for Few-Shot Classification"[2] - Geiger at.al "The jamming transition as a paradigm to understand the loss landscape of deep neural networks"[3] - DeVries et.al "Improved Regularization of Convolutional Neural Networks with Cutout"[4] - Zhang et.al "Mixup: beyond empirical risk minimization"[5] - Gastaldi "Shake-Shake regularization" The paper studies the performance of ensembles of deep networks on small-data tasks taken from subsets of Cifar10 and Cifar100, with either the cross-entropy loss or the cosine loss. The authors conduct extensive experiments on these datasets with various choices of sample size, and are careful of evaluating models with comparable computational budget, by considering ResNet architectures of varying depths and with different numbers of models in the ensemble. They find that ensembles of small models tend to outperform single large models.The approach seems promising, and the extensive experiments provide a comprehensive picture of the performance of various choices of model and ensemble sizes on the Cifar datasets.Nevertheless, the proposed method is not compared to any existing models and regularization approaches which are applicable to small datasets, including the cited references or other approaches (e.g. [1-4]). This makes the statement of "improving the state-of-the-art" questionable. The evaluation also does not seem to perform adequate cross-validation (e.g. the authors use the "best test performance" across any epoch).I thus encourage the authors to perform a more comprehensive evaluation of the proposed approach, and further compare to other methods. Some confidence estimates for comparing methods would also be useful, as such small datasets may lead to large variance across different choices of samples. Also, it would be interesting to see how the approach performs beyond just the Cifar dataset, perhaps in other domains where data is more scarce. Other ways to control for computation in each model of the ensemble would be interesting, e.g. how would controlling width instead of depth affect performance?[1] Arora et al "Harnessing the power of infinitely wide deep nets on small-data tasks."[2] Bietti et al. "A Kernel Perspective for Regularizing Deep Neural Networks"[3] Drucker and Lecun "Improving generalization performance using double back-propagation"[4] Miyato et al. "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning" The authors present a method for incorporating basic concepts from linear systems theory into the standard structure for training artificial neural networks. They compare results of their approach against standard approaches for 3 simple datasets.Broadly speaking the work is quite unclear, and takes several passes over to have a basic sense of the approach. There are too many shortcomings to enumerate them all, so I will just present one. Figure 5 is presented in the section "example architecture" which might lead one to believe the authors implement this network (which it appears they do not). I believe this is included only to indicate a hypothetical architecture, but the presentation is too poor to glean this with any quickness. This is of course, in itself, not sufficient grounds for rejection, but speaks broadly to the poor presentation of the work. It does not seem ready for publication.As for the significance, the work clearly falls short. Although the motivation of constructing a "more explainable model" is a good one, this should not come at an extreme cost of model expressivity. It seems obvious that richer models, such as LSTMs etc., correctly trained, should be able to account for the linear transformations the authors include in their "novel layer." That their work is competitive with these richer models is simply an indication of the simplicity of the tasks they chose, which (as far as I can tell) can all be accounted for using linear systems analysis (although it's hard to say, since they work so poorly explains the second two tasks). It's completely unclear how effective the authors' approach would be over standard, richer models, on tasks that cannot be accounted for by linear systems analysis, and I am doubtful that the suggested approach could offer much over these richer models.Likewise, an alternative view of the authors' work is as a learnable filter bank applied to data to create a representation of the data better suited for post-hoc learning with a richer model, which is certainly an useful idea, but it is not clear to me (and the authors haven't shown) that their choice for this filter-bank is superior to many other choices (e.g. convolutional layers applied prior to FC layers, which is standard for deep networks). This paper aims at defining a new architecture, Dynamic Recurrent Neural Network, that would be based on discrete differential equations of basic linear system transfer functions known from dynamic system identifiation. They show also an application example.The paper is correctly written, and the subject is of interest. But I don't really see the point as it looks like the method removes all of what is 'recurrent', as no weights from the new unit are learned during the training. The learning is only on some fully connected layers afterwards: I feel that the authors just made an (interesting) 'feature extraction' instead of a recurrent unit, by using standard transfer functions without any weights to be learned (or haven't I understood correctly?).The claim that the paper is the first to present a method where the sequence sampling rate, or 'delta t', is a parameter that can be modified without needed to re-train the network seems odd. In fact, I am not very familiar with this field but it looks to me that a lot of works that are now combining neural nets and dynamical systems are by definition able to do that. For example, I know this work 'Learning Dynamical Systems from Partial Observations', Ayed et al. 2019, which has a section called 'Benefits of Continuous-Time.', where I can read 'this allows us to accommodate irregularly acquired observations, and as demonstrated by the experiments, allows interpolation between observations.' So it is not something 'new' to the community and I would guess that papers closer to yours would also have the same feature?Questions/ remarks:- please explain better how your network is still recurrent : where is the 'hidden memory' that is passed? And where are the weights inside the DYRNN that are learned during the training? For me it looks like only the FC model placed after, that is moreover without any non-linearity (ReLU, etc), contains weights that are updated during training.- 'and are state of the art layer types for text based sequence-to-sequence problems like machine translation or text processing --> no, the recurrent NN are not state of the art in translation anymore... transformers are. But RNN could be state of the art in other problems.- 'The number of output channels per layer n layer amounts to (base component count * n oc = 5 * n oc ), as shown in Figure 3b.' --> not clear: there is 5 output channels, so noc = 5, but what is this 'base component count'? Why do we have to multiply it, and what is a layer? Or do you mean that there are 5 DYRNN units? or that every 'component' (P, I, ..) outputs the 5 outputs? This is not what is represented here.- Fig. 6 might not be necessary, we can understand the concept without it.- Modell --> Model This paper considers the maximum entropy (MAXENT) method for estimating underlying probabilities over a finite alphabet, i.e., the multinomial model. The authors compare MAXENT with the regularized maximum likelihood, that is the Bayesian estimator under the Dirichlet prior with a common hyperparameter, and the Bayesian estimator with a general Dirichlet prior in terms of the Bayes risk, i.e., the KL-divergence from the true distribution to the estimated distribution averaged over the prior. The authors also consider the case where the prior is extended to the mixture of Dirichlet distributions. These comparisons are done numerically with synthetic data.Although the practical performance of MAXENT is of interest, the paper provides little novel knowledge about it.- The numerical experiments are at a too small scale. Without theoretical results, the experiments are required to be more comprehensive. The reported experiments are small scale both in terms of the alphabet size n and the data sequence length M.- Isnt it possible to include any discussion on hyperparameter estimation, for example, empirical Bayes since the hyperparameter estimation of the Dirichlet prior has long been studied?- p.7, last paragraph: I wonder why the random perturbation of the ordering of Z devastates MAXENT since the hyperparametes \alpha can also be permutated accordingly.Minor:Right after eq. (15): \alpha_k^{[1]},..., \alpha_k^{[L]} (The last one should be \alpha_k^{[L]}.)p.7, l.12 from the bottom: be be --> be The paper's starting point is the question whether the episodic training is beneficial, or not, for FSL / Prototypical Networks. The work can be seen as a follow-up of the recent works showing that simple baselines can outperform rather sophisticated few-shot learning models. Towards answering this question, this paper points out that Prototypical Networks (PN) are related to Neighborhood Component Analysis (NCA), and NCA can be considered as an episodic training-free alternative of PN.In more detail, PN aims to learn per-class prototypes based on sample averaging in the feature space. NCA, in contrast, aims to maximize the ratio of total similarity between same-class example pairs to the total similarity between different-class pairs. Due to their similarities in terms of their formulations, the paper claims that NCA loss can be considered as an alternative to PN loss to do non-episodic representation learning for few-shot learning purposes. In addition, the paper has a few strong claims, such as episodic training is detrimental to learning and under no circumstance beneficial to differentiate between support and query set within a training batch". Clearly, these are intriguing claims. However, there is a gap between the claims and the experimental validation. First, even if ProtoNet loss and NCA loss seem to be similar to each other, they're nevertheless different models, and it takes quite a significant manipulation to convert PN to NCA. Therefore, the fact one particular non-episodic-training based model gives superior results compared to those of PN with episodic-training, does tell us much about detrimental effects of episodic training for PN or in general. Second, while the paper's observations that NCA has the advantage of using more pairwise similarities within a batch compared to PN is indeed insightful, it rather points out to certain weaknesses in the way per-batch / per-episode data is being utilized by the PN formulation, instead of problems about episodic training.Overall, the paper has interesting observations about PN's weaknesses and shows why one particular simple non-episodic training / non meta-learned approach (NCA) can yield superior results compared to PN, which is a relatively mode sophisticated & well-established approach. However, the paper's (over-strong) claims remain mostly unsupported, which makes the otherwise interesting work poorly framed. The paper, with more water-tight arguments only, could otherwise be a valuable contribution but it requires quite significant & fundamental revisions throughout the paper, therefore, is not ready for publication in its current form. Summary: The paper presents a new model for the task of language modeling especially suited for longer sequences. This new model dubbed as Memformer consists of Transformer encoder-decoder and a memory module to store the past information from the encoder outputs. The encoder bidirectionally attends to the immediate previous sequence/segment information and to the memory module, which is designed to capture useful information from the past history of the full sequence. The idea is that by bidirectionally attending simultaneously to the previous input segment and to a memory module, the decoder should be able to improve its generation capabilities.Pros:The motivation of the proposed model is interesting, which is to bidirectionally encode the past segment information and a memory module to capture rich signals from the entire history. Cons:- The main drawback of this paper is the lack of controlled experiments in the results section. Due to this, fair comparisons to the previous language modeling results in the literature is not possible and thus merits of the approach are neither convincing nor clear.- To evaluate performance of the Memformer model, the authors present results on the Wikitext-103 dataset. However, to compare with the perplexity results from baseline models such as Transformer-XL and Compressive Transformer, the authors don't include the results from the original papers and instead re-compute it under simplified settings. However, to report progress in a widely studied task such as language modeling, it is not fair to not compare against highly-cited state-of-the-art results.- In Section 3.1, it is mentioned that byte-pair encodings are used to represent words. Are the perplexity results also computed over BPE tokens? If so, then I feel this evaluation scheme is inconsistent with the standard language modeling evaluation of tokens which is done over words (or linguistic units).- Although the results in Table 3.2 show that Memformer models obtains small performance gains in perplexity over baselines, but these results are rather marginal improvements and in the absence of statistical significance testing results, it can't be really understood if these are actual performance gains or due to randomness in the training process.- As the core of the experimental results are on Wikitext-103, it is unclear if the experimental findings would generalize. Currently, the paper lacks comparisons on other datasets such as PG-19 from Compressive Transformers.- The authors propose a multi-task training approach for language modeling. However, the motivation and benefits of why doing this is actually needed is not clearly illustrated and the perplexity gains seem to be small.- In Section 2.1, the motivation of simplified relative position encoding is not presented. - In paragraph 3, it is mentioned that due to uni-directional attention is Transformer-XL style language models, the memory may not have enough capacity to retain important information. However, this does not necessarily holds true for different tasks. Can the authors include a more detailed explanation or cite a prior work that illustrates this phenomenon?Writing Issues:- abstract 1st line: The mention of "remarkable accomplishments" is very vague in the context of applicability of Transformer models.- compatible with other self-supervised tasks: compatibility does not necessarily imply that such language models would be useful in self-supervised tasks.- Introduction section is not well-written. For example, the transition from first paragraph to second paragraph is rather abrupt, paragraph 3: Transformers and its followers: this is a very informal writing style, something that is not really suited when submitting to a publication.- Sec 2.2.1: The first sentence in third paragraph - "Figure 1b is an assumed language model." is grammatically incorrect.  **Summary**:This work proposes to attack object detectors by targeting their relevance maps of the different detected objects. The proposed RAD attack shows better black box transferability across different detectors on MSCOCO dataset. The relevance maps are calculated based on SGLRP act as an attention mechanism to the attack to focus on relevant regions in the more meaningful image and hence produce more transferable attacks. **Strengths** :- Good attack performance and transferability between detectors, which poses a security threat for SDV applications that use object detectors- Eight different detectors and three segmentation models are used in the RAD attack, which shows good generalization.**Weaknesses**:- Missing important references [a,b,c]. All of these works attack object detectors and target transferability.- The paper is poorly written and ambiguous. Variables are introduced without proper definitions. It is not clear how to obtain the gradients in eq(3) with respect to the relevance maps.- No use of the proposed dataset. The authors propose a new dataset of adversarial objects but never mention or showcase the dataset's usefulness. A straightforward way to show the dataset's usefulness is by performing adversarial training and making robust detectors against the proposed attacks. - No enough ablation is performed. The only ablation to the proposed method is in table 7 regarding the way to pick the detection target. The relevance maps based on LRP are expensive and worse than recent saliency maps like CAM and grad-CAM. The attack budget $\epsilon =16$ picked in the experiments is not justified ( it might be big or small for attack success ), and a plot of mAP vs. $\epsilon$ for different detectors would give more information about the effect of the attack.- All the attacks in the paper are performed on YOLOv3 and transferred to other models. It would be more informative to show transferability matrices of attacks performed on all models and transferred to all others.- The novelty of the proposed methodology is limited. While the use of relevance maps to improve the transferability of attacks on object detectors is novel, no proper explanation is provided. The attacks are based on PGD, and the relevance map is adapted from SGLRP. The paper offers no theoretical results or exciting insights. [a] Huang et al. "Universal Physical Camouflage Attacks on Object Detectors", ( CVPR 2020)[b] Wu et al. "Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors", (ECCV 2020 )[c] Xu et al. "Adversarial T-shirt! Evading Person Detectors in A Physical World" (ECCV 2020).Minor issues :- Many grammar mistakes:" because they possess multiple-output.", "Among the classification attacks and detection ones, cross-domain attack (Naseer et al. (2019)) is the most effective, but RAD is more aggressive" ..etc.- No question marks in titles 3.1-3.5.- Table 2-5 could have been visualized better by using a bar chart, for example, to observe the relative performance of attacks and defenses.  ## Summary  - The authors propose a method for "zero-shot navigation" that learns to navigate mazes from a map of the maze and the start and goal location in the maze. ## Strengths - This work builds on previous work in similar environment setups such as "Learning to Navigate in Complex Environments" (Mirowski et. al. 2017) to the "zero-shot" case, i.e., where the agent need not do any exploration when presented with a new environment but can immediately navigate to the goal. ## Weaknesses - My major concern about this work is that I just don't understand why this is a hard problem. You argue that "one cannot simply perform graph search on the 2-D map" but I fail to see why. Any classical planning method would have no trouble generating a plan for agent navigation from the 2-D map provided. The problem then, it seems to me, is reconciling the plan on the 2-D map with the actions that are needed to be taken in the actual environment. But given that there is a deterministic scaling between occupancy grid map and the agent world ("Each cell on the abstract map corresponds to 100 units in the agent world.") AND your agent's transition function is deterministic, then this problem is solvable zero-shot with a traditional planner (e.g. one based that generates a roadmap and then searches it but others would work)  and then executing the plan. Learning the dynamic model is also straightforward because you assume that you can directly observe the the joint state $o_t$.   - Related to the above, you refer to the 2-D map as "abstract" but I fail to see what's abstract about it. It is a metric occupancy grid map that gives you full information about the layout of the environment.  - As a result, my impression is that the solution is "over-engineered". There are many complex components and "hyper" models when something much simpler would have easily solved this problem. ## Minor Questions/Comments - At the onset you frame with this work with the classical SLAM literature, but this seems puzzling since you are providing the agent with both the map, and its initial location within the map, so both the mapping and localization are solved.  The authors present a method to optimise the _loading time_ of a deep neural network on e.g. a mobile device. With ever more complicated and bigger networks (notwithstanding approaches to prune networks etc.), loading parameters can cause a noticeable initial delay before the first inference step. The authors propose an RL-based approach to optimise loading parallelism by splitting the model into smaller submodels and show that this speeds up loading time.I've got several concerns with this submission:- The paper was very difficult to read and follow in general, and I'm still not 100% certain my current interpretation is correct. While the language itself could use some polish, the notation is also pretty inconsistent (e.g. $S$ as state space or specific state) and the figures are hard to read. A few terms are not clearly explained: what is a block exactly? A contiguous set of weights in memory? A number of network layers? There's also mention of nodes in Section 4.2.1, are those the same? Furthermore some of the equations also seem to contain mistakes (e.g. $Block_M$ in Eq.2 should probably be $M_k$, final term in Eq. 5 should still be in the brackets).- The problem statement does not feel like the right one to me. The problem the authors try to solve is loading delay, however the problem they are actually solving is perfect loading parallelism. Is it not more important to reduce overall delay than to make sure that each submodel can get loaded in exactly the same time? Even if the former would eventually result in the latter (which would be a nice result!), why not optimise total delay directly? Moreover, the authors allow the number of submodels to exceed the number of hardware threads, keeping the rest of them in a load buffer, which can potentially even add more delay compared to pre-specifying the number of submodels to the number of hardware threads. Also more important than the number of hardware threads, I suspect, is the number of simultaneous streams from the secondary storage medium you can effectively use due to I/O contention, which I imagine is usually lower. This would potentially also explain why in Table 1 there's only a 2.5x speed-up, where one would expect 4x with 4 submodels.- The authors compare to an existing ("state-of-the-art") method but don't explain what this is. How does the proposed method compare to naively loading N chunks of M/N weights, with N being the number of hardware threads (or available streams from the storage medium) and M the total number of weights?- The authors propose an RL-based approach (table-based Q-learning) to solve the problem, though provide no real motivation why such a comparatively complicated method is a good idea in this case. While RL is very flexible, it is not always the right solution.- Though I'm still not certain, it seems like the authors use a somewhat unusual formalism in between a bandit and a full MDP, where the "action sequence" (the number and indices of the splits) is selected at the start of the episode, and then the state incrementally updated. This would explain the reward definition, which would only make sense if the number of splits $C_{split}$ would be fixed at the start of the unroll. as well as the appearance of what I interpret as the state variable $s$ in Algorithm 1. Is this interpretation correct? If so, this choice should definitely be elaborated upon in the text, as it is pretty unusual. I would also think that in this case a pure bandit-style formulation might make more sense.- Why the choice for a sparse reward function with an $\epsilon$-margin instead of a dense reward, which are usually easier to learn?- What is meant with "Vendor's site" and "Developer's site" in Figure 2? Are these separate geographic locations?- While the results span different DNN models, it's unclear how the RLU controller was actually trained besides with a general Q-learning approach. How many iterations? Separate Q-tables per DNN model? What hyper parameters? Perhaps more worryingly, Table 3 contains 5 "observations". Are these simply 5 different timing measurements of the same model? Does this imply that for Table 1 only a single timing measurement was taken?Overall I find this submission too hard to follow and agree with to vote for it to be accepted. This paper aims at optimizing the the loading latency of DNN models by re-partition the model into modules that can be loaded in parallel. The paper adopts reinforcement learning to find the optimal partition. The experiments show that the proposed method achieved significant improvement on several popular models.Strength of the paper: the problem this paper aims to solve is very meaningful and not addressed by previous work, as far as I know.  Weaknesses of the paper: 1/ This paper is not written clearly. For example, in the abstract and the beginning of the intro, the paper aims at optimizing the model loading latency, but in the later part of the intro, it also aims at optimizing on-device training? It is not clear how these two problems are closely related. 2/ The problem formulation and methods are not expressed in clear and rigorous mathematical terms. For example, what's the meaning of equation(1) and (2)? In equation (3), what is a "split"? what is the C_split and I_split? There are verbal explanation to this, but the definition is too vague to understand. The author might want to consider representing a model as a directed-acyclic graphs (DAGs), and use the graph terminologies to describe the problem and methods.3/ It is not clear to me what is the motivation of using RL to solve the model partition problem. Since the model is a static graph, the optimization problem can be solved by searching for an optimal partition that minizes the overall latency? The paper decomposes this process into steps, and formulate the search process as a markov decision process and use RL to search for the solution. However, it is not clear if this is necessary, or optimal. Overall, I think this paper is not well written and should be further polished.  Authors present a spatial transformer layer that modelsdiffeomorphisms. Modelling diffeormophisms with neural networks is not verynew. Prior work successfully utilized stationary vector field prediction andfast solvers through scaling-and-squaring techniques. These prior work have beenused to tackle registration and segmentation problems. The technical noveltyhere is to use time-dependent vector fields for modelingdiffeomorphisms. Experiments on MNIST classification, using the proposed spatialtransformer as a layer, and breast tissue segmentation, using the proposedtransformer as a single network to deform a template, are presented and comparedwith conventional spatial transformer layers using thin-plate-splines astransformation models.While the introduction of time-dependency is interesting, more from a technicalperspective than a conceptual perspective, I believe there are several aspectsof the paper that needs improvement. 1. Since the main innovation is integration of time-dependency in modellingvector fields with neural networks, I suggest directly comparing time-dependentwith stationary vector field approaches.a. Experimental comparison can focus on the differences between stationary andpiece-wise stationary vector field modelling. I could not see this comparison inthe experiments, only a single mention in the conclusion stating betterperformance of the proposed model.b. Analysis of computational time would be interesting. Predicting andintegrating a stationary vector field will be faster I assume. But by how much?2. MNIST classification experiments raise some concerns:a. The increase in classification accuracy of the proposed model compared toCNN + Field-STN is small. This difference may as well be due to randomness ofthe optimization. This result is not motivating for the proposedmethod. Achieving lower number of negative Jacobian determinants is interestingbut its value is questionable.b. What is CNN + Field-STN? This is not defined.3. Segmentation results raise the following concerns:a. The method proposed here does not seem to be better than the methodproposed in Lee et al. 2019. The lower HD distance can be motivating but valueof this may be better justified. If the proposed innovation was substantial,lower accuracy would have been completely fine. However, since the innovation issubtle, such a lower accuracy lowers the enthusiasm for the paper.b. The proposed method achieves lower number of connected components but thevalue of this is not very well motivated from an applicationperspective. Providing such a motivation would be helpful.4. I suggest editing section 3 to present the proposed method much moreclearly. The method only becomes clear in Section 4.  This paper proposes a method to compress large-scale neural language models (e.g., BERT) without losing too much downstream performance. In contrast to prior work that seeks to compress just the parameters W in the standard linear layer h = Wx + b, this work incorporates the input by approximating Wx instead. The authors also extend this idea to approximations of attention, which enables them to experiment with Transformer architectures. Experiments across the GLUE benchmark confirm that the method does not significantly harm downstream performance, and it also seems to result in inference speedups. However, the biggest flaw with this paper is that its timing experiments are reported on a single CPU thread, while the majority of practical scenarios rely on GPUs. There are other issues with the choices made for the timing experiments (large batch size despite using CPU?) which make me skeptical that the method works as well as claimed.  As such, I cannot recommend the paper's acceptance.comments:- i'm not quite sure how the proposed method is more "generic" than existing related work. why does having to "train the proposed structure from scratch" (pg 1) reduce the practicality of a method? in general, these types of applications prioritize inference speedups over training time, so i don't get the criticism. i feel like touting the method as "generic" is a little misleading; the proposed method here is complementary to prior work on compression / distillation and should be presented as such.- the related work comparisons are similarly exaggerated / misleading (sec 2). implying that pruning methods "might not" reduce inference speed is strange given that much prior work on pruning BERT-like methods yields significant inference speedups. - this method assumes that the data lies in a low-dimensional subspace, which might not be a valid assumption for many tasks / datasets. i would have liked to see more discussion of this, although the paper does evaluate over many different tasks. - the GLUE speedup ratios are interesting; do they correlate to something like the complexity of a particular dataset? - a HUGE concern with the timing experiments is that they were done on a single thread of a CPU, not a GPU. especially with a large batch size (100 is fairly large), this definitely disadvantages the CPU computation for larger models. DRONE is not very relevant if the same speedups don't occur on a GPU; no one is serving BERT models on CPU. this makes me skeptical of the value of the proposed method, and is by itself enough to reject the paper. - it's even more strange because "DRONE-retrain" performs 1 epoch of fine-tuning, presumably on a GPU, so why would a CPU be used for timing? This paper proposes regularizing the conventional MARL learning objective with a mutual information term to encourage more correlated behaviors among different agents. The contribution is clearly stated. However, the similarity to previous works is not sufficiently discussed, and the paper leaves out some important related works.The paper maximizes the mutual information between agents' policies, given the current states. To allow policies to be conditional dependent, the authors assume there exists a dummy variable. The contributions of the paper about the lower bound of the mutual information (Sec. 4.2) and policy update (Sec. 4.3) are not significant. The lower bound of the mutual information has recently been extensively explored in multi-agent settings, used for encouraging role emergence, minimized communication, and exploration. The policy update and policy improvement guarantee can be easily obtained based on soft reinforcement learning literature. ** Major concern: About related works. My major concern is not about the two contributions mentioned above. Instead, I think that the first and main contribution of this paper is a subset of previous works' contributions. EDTI [1] discusses how to maximize the mutual information between the \emph{trajectories} of different agents. Their discussion already covers the correlation of policies of different agents at \emph{a single timestep}. More importantly, the authors of that paper also point out that only optimizing mutual information between trajectories is not enough because the reward signal has to be considered for better policy learning. They even discuss the second-order influence between these mutual-information-based intrinsic rewards. The contribution of this paper seems to be the first part of [1]. Frans Oliehoek and other researchers also did lots of excellent works on this topic [2,3,4,5]. However, the discussion about these related works is absent from this paper.Additionally, the difference between the proposed method and Jaques's paper (social influence) is not significant. The only difference is whether the action of other agents, $a_j$, is $a_j^{t}$ or $a_j^{t+1}$ when calculating the mutual information (In Jaques's paper, they prove that their formulation is equivalent to a mutual information formulation). I do not think the author's definition is an improvement of that of Jaques. At least, the authors should provide a more serious discussion about this point, perhaps providing a matrix game to show that different timesteps do indeed make a difference. The authors may argue that their experiments show that their method has better performance. The problem is that SSD used by Jaques is a more challenging task than those used in this paper. Moreover, social influence is sensitive to hyperparameter settings and needs fine-tuning to reach its full potential. I will change my mind regarding the experiments if the authors could provide SSD results (adopting their methods to tasks with discrete actions is not very difficult). Besides, Jaques et al. also discuss how to make condition dependency clear between agents and how to carry out influential communications, which are not discussed in this paper. [1] Wang, T., Wang, J., Wu, Y. and Zhang, C., 2019. Influence-based multi-agent exploration. ICLR 2020 spotlight[2] F. A. Oliehoek, S. Witwicki, and L. P. Kaelbling. Influence-based abstraction for multiagent systems. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, pages 14221428, July 2012* Also see a recent longer version: https://arxiv.org/abs/1907.09278[3] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman. Transition-independent decentralized Markov decision processes. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, pages 4148, 2003.[4] Miguel Suau de Castro, Elena Congeduti, Rolf A.N. Starre, Aleksander Czechowski, and Frans A. Oliehoek. Influence-Based Abstraction in Deep Reinforcement Learning. In Proceedings of the AAMAS Workshop on Adaptive Learning Agents (ALA), May 2019.[5] Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs, SpringerBriefs in Intelligent Systems, Springer, May 2016. Manuscript Summary====This paper constructs a disentanglement problem from a temporally causal view, where data are observed in sequences, and where actions cause those observations to change as the sequence progresses in time. Their stated objective is to recover the specific actions and their parameters (e.g. rotation and translation, and their magnitudes/signs).The authors thus construct a "Fractional VAE" (FVAE), and then construct sequences from Dsprites and Chairs based on their statement of the problem.Initial Decision (from this reviewer), Review, and Reasoning====I think this paper should be rejected; were this a journal, I would suggest at least major revision.Overall the concept of bringing temporal causality (which for some cases *is* valid as the causal diagram/frame) into the disentanglement problem statement is a good idea. However, after that point in the manuscript, I cannot understand what has done. For example, section 2 is a restatement of previous work, and section 3 begins with an explanation of the dataset construction. Section 3.3, section 4, and Figure 4A I think describe the paired asymmetric autoencoder method, but a sparse few paragraphs are given at this point. What they _do_ describe is a set of "sub-encoders" with varying compression rates $\beta$. However, beyond varying the rates, it's not clear how particular "ground truth" factors (e.g. $\theta, L$) can be selected for and locked in to specific latent factors in an unsupervised manner, or even if this should happen.By varying $\beta$ we receive different amounts of information in the representation, but how can we ensure across "learning phases" disentanglement? Further, if these KL divergences are set to different $\beta$, this means we don't have a divergence for the joint representation? (the concatenation of the sub-encoders) So how can we ensure that these are disentangled themselves? While successively learned encodings would optimally not include previously encoded data, why would these encoders learn separate concepts instead of coarse grained representation with all concepts to successively finer representations (or refinements to those coarse grained representations) with each successive sub-encoder.  Or are these separate phases repeated?Perhaps these questions have answers in the positive, but they should be answered by the manuscript.I further cannot make a connection between the actions sequences and the training methods/arch. I think I have understood both (...save for the above highlighted problems), but I cannot understand where the sequences come in practically speaking, even modulo the aforementioned issues. How does the FVAE or its training scheme use this information? Does it use this information?I think the positive experimental results in Figure 6c mean that there is something here. However, I cannot tell given section 4 what is actually being done.Suggestions====I suggest a clear procedure section with numbered steps. It is of vital importance that the reader understand what has been done. If it is already there, it should be made much more obvious/clear.I think the connection between sequences of images under actions and the proposed method needs to be made, or, if I missed this connection, should be made clear.The initial portion of section 3 concerning dataset construction might also be moved to much later.There are philosophically challenging sections which I did not comment on in the review portion. I think these are approximately orthogonal to the method due to the scope of the problem: rotation/translation may be disentangled. Can dog breeds *be* disentangled, even in theory (example from Section 3)? Disentanglement of simple mechanisms/"actions" are perfectly acceptable at least in my opinion for the state of the field at this moment. Using more complex examples may not be helpful. Similarly, the discussion in section 1 raises questions that are unrelated to the later method. Since a literature review is undertaken in Section 2, the paper could have started at "In this paper, we first demonstrate that instead of the ground-truth factors the disentangling approaches [should] learn [disentangled] actions.", with an update for phrasing.I would also give the paper another read through for grammar. 1. SummaryThis paper proposes an augmentation of the Ladder VAE model (LVAE [1]) using 1. more flexible variational distributions using normalizing flows (denoted $f$) 2. an autoregressive component (because of the generative dependency $p( x_{l} | z_{l}, x_{l-1}) $ (*data layers*), although this is not directly stated as a contribution. It is argued the proposed factorization of the inference network (top-down inference) matches the factorization of the true posterior, that follows the top-down design of the generative model. The authors recorded competitive performances on dynamically binarized MNIST. 2. [a] Strong Points- the authors introduce a very flexible architecture for DGMs by combining recent improvements from the literature. 2. [b] Weak Points- the paper overall lacks clarity: the idea could be explained in much simpler terms- the contributions are weak: augmenting an LVAE model using flows brings little novelty - the autoregressive structure introduced in the paper (which can be interpreted as an instance of state-space models [2]) is not well discussed and is contradictory with the claim that no *autoregressive layer* is used.- an ablation study is required to disentangle the effect of 1. the flow augmentation and 2. the autoregressive structure- aiming for a better inference network aims at tightening the variational bound, which should be measured empirically - the diagonal Gaussian assumption for the variational family is not necessarily so limiting [3, 4] (as assumed in the paper). - the results reported for CIFAR-10 are not competitive with sota models [3, 4] (SeRe-MAF) $\approx$ MAF (10) = 4.31 (original paper) $\gg$ 3.08 (BIVA [4]) > 2.91 (NVAE without flow [3]) .3. RecommendationUnfortunately, I recommend rejecting this paper.4. Recommendation Arguments SeRe-VAE combines multiple architecture improvements (top-down model (LVAE), flexible variational distributions using flows, autoregressive components) into a final model, which is tested using a single dataset (excluding cifar10 results which are not competitive) and without performing an ablation study. This prevents the community from understanding the effect of each of the architecture choices and does not guarantee that such an architecture could be effectively adapted to other contexts. 5. Questions to the Author- Figure 1: why considering the prior independent? the generative model adopts a hierarchical structure $p(z_l  | z_{l+1})$6. Feedback Your work is an engineering prowess, I am saddened to recommend rejection. I think your work could greatly benefit from an ablation study and from defining the model in more minimal terms. A few comments:- The inference network is conditioned on $\mathbf{x}$, not the entire $\mathcal{D}$.- you can measure the variational bound using the identity: $\operatorname{KL}(q(z | x) | p(z | x)) = log p(x) - \mathcal{L}(x)$- please report CIFAR10 results in bit per dimension, as stated in the literature- please report results on the more widely accepted Statistically binarized MNIST first, use dynamic MNIST as a second option.[1] Sønderby, Casper Kaae, et al. "Ladder variational autoencoders." Advances in neural information processing systems. 2016.[2] Fraccaro, Marco, et al. "Sequential neural models with stochastic layers." Advances in neural information processing systems. 2016.[3] Vahdat, Arash, and Jan Kautz. "Nvae: A deep hierarchical variational autoencoder." arXiv preprint arXiv:2007.03898 (2020).[4] Maaløe, Lars, et al. "Biva: A very deep hierarchy of latent variables for generative modeling." Advances in neural information processing systems. 2019. The paper is proposing to evolve datasets of (inputs, outputs) in the context of programming by example (PBE), where the goal is to infer a computer program that is consistent with the association of (inputs, outputs). The justification of the approach is to figure out instances that would allow to learn a model with PBE that would generalize better, compared to learning from synthetic datasets that are randomly generated. The approach followed consists to use an evolutionary algorithm to generate this new dataset, using the learn model as a guide, by picking the points where the model at hand performs poorly. Such adversarial approach proceed iteratively, adding extra pairs of (inputs, outputs) to the training set, to get it to perform better in situations where it has trouble.The proposal is relatively straightforward, using an adversarial optimization loop to enhance the training set for better (at generalization) PBE approach. Such solution is in line with many other adversarial approaches. The proposal is quite specific to the context of PBE, and has been tested in a rather synthetic setting (toy problems / synthetic problems). Performance shows good general improvement of the performance. However, the overhead of generating the adversarial samples is not provided, nor the overhead caused by adding more samples to the training set. If we have trained the model with more random samples (same size for all approaches), would the results be the same -- I mean is there any correction made to ensure that we are always comparing approaches with the same number of samples?The overall proposition is analogous to different proposals made with coevolution in evolutionary algorithms in the years 2000s. Here again, the training set was "co-evolved" with the program inference task in a competitive (adversarial) way, with a population of samples vs. a population of programs, trying to obtain the samples that would mislead the most the programs, in order to infer more robust solutions. See for instancehttps://doi.org/10.1162/evco.1997.5.1.1https://doi.org/10.1162/106365604773955139Although we are not talking of the same kind of PBE -- evolutionary algorithms vs. PCCoder -- I think linking the current work with this older one would be required, as the idea is not new (but applied to a different PBE approach).The quality of writing is not as sharp as we would expect for an ICLR paper. The writing stay somewhat vague at times, we would expect more formal and precise presentation. The algorithms presentation in Appendices A and B are not very clear and not very useful to grasp all the details. The presentation of the methods in Sec. 4.1 and 4.2 are very textual, I am far from being convinced that someone who what to reproduce the approach will be able with the information provided. For instance, how is exactly computed the fitness function is not given explicitly, from what I get I would have a hard time to implement it, or at least valid it in its correctness. It appears strange of proposing an optimization solution as the main contribution without explaining in detail the objective function optimized.In brief, the proposal does not pass the threshold in terms of significance of the contribution, overall quality of the paper and how exhaustive the experimental evaluation has been made. The paper is not well-written, so it is hard for me to fully understand the whole paper. It seems to me that the authors tried to model user preferences with smooth functions for collaborative filtering and results on two Movielens datasets showed that the proposed method is comparable to several baseline methods. Pros:1.The idea of capturing frequency domain information for collaborative filtering seems interesting to me, although I did not quite understand how this information is obtained in this work.2.The authors mentioned that their method can be interpretable, which may be more interesting if they can provide some case studies for explaining recommendations using the proposed method.Cons:1.The motivation is not clear to me, i.e., why smooth functions are important in modelling user preferences in collaborative filtering is not well explained. 2.The authors seem to have a narrow understanding of collaborative filtering. For instance, the authors mentioned that the utilization of deep architectures is limited to shallow networks in exiting CF literature. However, there are a lot of recent works building upon state-of-the-art deep learning techniques as far as I know. 3.The idea of this paper is very similar to a recent work (Harald Steck, Embarrassingly Shallow Autoencoders for Sparse Data, WWW 19). The main difference may be that this work adopted clustering of users instead of individual user when modelling user interests.4.The experimental results are not encouraging. The proposed method is actually much worse than Bayesian TimeSVD++ (Rendle et al., 2019) and IGMC (Zhang & Chen, 2019). So, it is hard to understand why this alternative method is promising in the area.5.The presentation could be improved. The authors mentioned that their code has been published but no link was provided in the paper.  Overall, I think this paper is a preliminary work and is not ready for publication. I think the authors should revise the paper and submit the revised version to another conference. This paper proposes an approach based on Fourier transforms to predict ratings in collaborative filtering problems. The papers scope (smooth reconstruction functions) gets immediately narrowed down to Fourier transforms--it would be nice to provide some motivation for this choice over alternative smooth functions. The paper then clusters the users as a way to reduce the number of parameters in the model, given that the Fourier transform itself does not reduce it. As a further step, the clustering is replaced by a soft-clustering learned by a neural network. In the experiments, the RMSE of the rating prediction problem is worse than some baselines and better than others. Besides these technical steps, from a more big-picture perspective, I am not sure if the problem of rating prediction as cast in this paper, misses a key point. The key point I am concerned about is that the observed ratings are missing not at random [a]. For this reason, the collaborative-filtering literature abandoned the minimization of RMSE on the OBSERVED ratings ten years ago. Two different avenues have been pursued since then: most of the papers switched to ranking the entire catalog of items, e.g, see [b] to get started. A few papers continued with rating prediction, but stated the problem correctly by taking into account the fact that the ratings are missing not at random, eg., [c,d].In this submission, the problem statement at the top of page 4, and Eq. 6, was not clear to me: while s was defined clearly in the Fourier transform earlier in the paper,  I did not find a definition of s_u in Eq 6 in the context of rating prediction, i.e., is this the vector of ratings of user u? Only the observed ratings? How are the unobserved/missing ratings of user u treated in the proposed approach? Given the RMSE-values in the experiments, my best guess is that the model was trained on the observed ratings only, ignoring the key problem that the ratings are missing not at random. I feel like a rating-prediction paper that ignores the key problem of collaborative filtering, i.e., the fact that ratings are missing not at random cannot be accepted (and should actually be desk-rejected). I encourage the authors to modify the approach to account for this key problem of collaborative filtering. Alternatively, this approach may be useful for different applications, like compressive sensing problems where the observations are truly random.[a] Collaborative Prediction and Ranking with Non-Random Missing Databy B. Marlin and R. Zemel (RecSys 2009 Best Paper)[b] Training and testing of recommender systems on data missing not at randomby H. Steck (KDD 2010)[c] Probabilistic Matrix Factorization with Non-random Missing Databy J.M. Hernández-Lobato, N. Houlsby, and Z. Ghahramani (ICML 2014)[d] Modeling User Exposure in Recommendationby D. Liang et al. (WebConf 2016) SUMMARYThis submission extends Scobee and Sastry's ICML-2020 work, which formulates the problem of learning (hard) constrains in RL as inverse constrained RL setting, by generalizing it from tabular to continuous settings and, empirically, increasing the latter's scalability.HIGH-LEVEL REMARKSI believe that the paper, while incremental, might be making a valuable contribution. At the same time, the magnitude of this contribution is difficult to assess, because paper makes a number of unsubstantiated claims about them, doesn't thoroughly analyze the limitations of its approach, doesn't provide an empirical comparison to existing alternatives, and has several other presentation issues as detailed below.DETAILED COMMENTS-- The paper claims, "[Scobee and Sastry] assume ability to modify the environment (specifically, to add arbitrary constraints to it)." I'm not sure what is meant here -- adding a constraint doesn't mean modifying an environment. The constraints apply to a policy.-- The paper also claims, "our approach [...] does not suffer from the curse of dimensionality". This claim is unsubstantiated and doesn't sound true. Is the approach really oblivious to observation space dimensionality? Do you have a theorem showing this?-- The problems in the experiments are too small to support the above claim about the approach being impervious to the curse of dimensionality empirically, if that was the intent. In particular, the paper doesn't experiment with environments involving visual observations.-- How many expert trajectories were used in the experiments in Figures 3 and 4? -- The paper claims that its approach can handle arbitrary constraints. This is a misleading claim, at best. There are rich temporal-logic based constraint languages such as LTL, CTL, etc. The paper doesn't discuss whether they are subsumed but CMDP's constraint form even theoretically. In the meantime, even the fragments of these constraint languages that can be compiled into CMDPs' constraints can cause exponential state space size explosion -- consider, e.g., constraints that specify that every trajectory must visit state A before state B before state C, etc. So in practice the paper's claim can't be true.-- The paper's title, "CONSTRAINED REINFORCEMENT LEARNING WITH LEARNED CONSTRAINTS" isn't descriptive of the proposed technique. The paper assumes that the entire nominal MDP (the MDP without constraints) is given, along with a set of demonstration trajectories, and the agent tries to learn *only* the constraints. This means that once the constraints are learned, there is no more learning going on, only optimization. Thus, a more appropriate title for the paper would be something like "constraint learning via maxent inverse RL".-- The paper's assumption of the nominal MDP being fully known, including the transition and reward function, is steep, stronger than IRL's. It also seems to require that the environment should be able to indicate constraint violations during training. All of this raises the question: why not solve the constrained MDP using one of existing forward methods for doing so (the paper mentions a few in the related work) without explicitly learning the constraints? Note that the paper's experiments don't just evaluate constraint learning, they evaluate solutions to the MDPs at hand, so a comparison to forward CMDP RL solvers is natural to ask for.-- One answer to the above question can be that the goal of learning constraints is transferring them to other settings. But if so, the paper ought to analyze the effect of expert quality on the quality of constraints. Note that this effect can be drastic: if the expert is suboptimal, then its trajectories don't necessarily imply constraints, which means that the proposed methods might learn fictitious constraints that may make other environments where they are applied unsolvable. -- In general, the paper doesn't compare its approach even to alternatives that also try to learn constraints explicitly. They may try to learn more constrained constraint types, but it would still be very useful to see a comparison to one of them (see the "constraint inference" part of the related work section).-- The paper proposes to use a sigmoid at the output. I'm wondering whether this will lead to vanishing gradient for DNNs.-- Related to the above point, note that while the paper claims to list all relevant hyperparameters for the experiments, it doesn't describe the network architecture (and the number of experts used in some of the experiments, as mentioned above) either in the main body or in the appendix, which makes its results unreproducible.-- The use of J(.), with or without superscripts, for a value function is non-standard and confusing. The RL and controls convention is to use J(.) for a cost function to be minimized and V(.) for a value function to be maximized. The paper uses J both for a value function (the paragraph right before section 2.2) and, with a superscript, for a cost function (the first paragraph of 2.2). I strongly suggest bringing the paper in agreement with the standard convention, which will also remove the need to use J with a superscript.-- In the related work is reasonably complete. A notable omission is Le, Voloshin, Yue, "Batch Policy Learning under Constraints", ICML-2019). Also, note that Tessler et al's and Subramani et al's papers' venue is listed in arXiv, but both have actually been published in refereed venues (ICLR-2019 for the former, CoRL-2018 for the latter). Please cite them as such. -- Figure 5's legends are barely legible, please increase their font. This paper extends a tabular method for constraint inference to work in high-dimensional environments, and demonstrates it on a few environments.Overall I liked the derivation of the algorithm (modulo some nitpicks described later). However, the experimental evaluation is sorely lacking: there are no baselines compared against, even though the obvious candidate to compare against is preference learning algorithms, e.g. imitation learning (such as GAIL [1]) and inverse reinforcement learning (such as AIRL [2]).Indeed, it is not clear even theoretically what benefit constraint inference gives over the preference inference framework. While it is not common for preference inference to learn what _not_ to do (as this paper highlights), it is possible -- [3] is an example that creates some toy environments where the agent must learn what not to do, and then solves it using a single-state version of Maximum Causal Entropy Inverse Reinforcement Learning. I would expect that high-dimensional preference learning algorithms like GAIL and AIRL would also be able to do this, and thus are important to compare against.In fact, it seems to me that the proposed algorithm can be cast as a special case of preference inference. This is most easily seen from the gradient expression in [4], where the MaxEnt IRL gradient in high-dimensional environments is written as$$\frac{\delta r_{\theta}}{\delta \theta} [\mathbb{E} [ \mu ] - \mu_{D_{\tau}}].$$This is the negative of what you might expect because they are defining a loss function to be minimized while you are defining a log likelihood to be maximized. (Note that $\mu$ is the state-occupancy measure.) If you set$$r_{\theta}(\tau) = \log \zeta_{\theta}(\tau)$$I believe you recover the proposed algorithm. So it seems that the main contribution relative to existing algorithms is to propose this particular structure on the reward function (which corresponds to a hard constraint because when $\zeta$ is zero the reward is negative infinity). So it becomes even more important to show why this particular structure is an improvement over e.g. the GAN-like structure in AIRL.(Other minor contributions include the use of importance sampling and the regularization of $\zeta$, though it is also hard to tell how important these contributions are.)Quality: As explained above, I have serious concerns about the experimental section.Clarity: The paper was quite clear. Ive listed a few minor cases below for improvement.Originality: To my knowledge no other paper has inferred constraints based on the maximum entropy assumption in high-dimensional environments. However, there are both preference learning algorithms (discussed above) and algorithms that learn logic specifications [5], which seem similar in spirit to constraints (though are not the same).Significance: Hard to judge without better experiments.Nitpicks / typos:> Note that this is not particularly restrictive since, for example, safety constraints are often hard constraints as well are constraints imposed by physical laws.Shouldnt physical laws (things like F = ma) already be encoded in the transition dynamics T? Why do we need constraints for this?Equation (5) seems to have dropped a $\beta$ compared to Equation (4).Should Equation (8) have an absolute value? Otherwise I dont see how it incentivizes the classifier to predict values close to 1. Actually, I see, the network is constrained by the sigmoid to output values in [0, 1]. It could be worth clarifying this.When going from Equation (5) to Equation (6), a policy À has suddenly appeared -- please explain what the policy is; the reader should not have to look at the appendix to understand the notation. (I assume it is a Boltzmann rational policy since you are using the MaxEnt framework.)First line of Section 4: TwoBrdiges -> TwoBridgesReferences:[1] Generative Adversarial Imitation Learning, https://arxiv.org/abs/1606.03476[2] Learning Robust Rewards with Adversarial Inverse Reinforcement Learning, https://arxiv.org/abs/1710.11248[3] Preferences Implicit in the State of the World, https://arxiv.org/abs/1902.04198[4] Learning a Prior over Intent via Meta-Inverse Reinforcement Learning, https://arxiv.org/abs/1805.12573[5] Maximum Causal Entropy Specification Inference from Demonstrations, https://arxiv.org/abs/1907.11792 This paper comes up with a novel scenario where the unlabled data are available as well as labeled data in the continual learning scenario.### Overall- Based on my understanding, the major contribution is the proposal of a task scenario, aka, experimental setting. The novelty of DistillMatch is an incremental modification of previous work. - The task setting sidesteps the learning with non-stationarity problem than solving it.- Further, this setting potentially makes the task easier for the proposed method. To verify whether this is true, more information are needed.- The presentation of the paper needs polishing, I listed a few points below.### Pros- The novel scenario of semisupervised continual learning is proposed. The argument is that in several realistic scenarios old data are often re-observed without label (The funiture labeling example). Therefore instead of storing a coreset, one may make use of the unlabeled data for pseudo-rehearsal/distillation. It is reasonable to make use of it when this assumption is true.- With the setting the author proposed, the DistillMatch method is able to perform better than previous methods. ### Cons1. The novelty mostly comes from the task scenario, the DistillMatch method is incremental.2. Although SSCL is a new scenario, and the author argues it is more realistic. IMO taking this assumption sidesteps the problem of continual learning rather than solving it. The central problem of continual learning IMO is to learn under non-stationary distribution, the assumption made in this submission makes the distribution more stationary.3. It is true that this assumption should be utilized when available. However, the only dataset used is manually constructed from CIFAR100, contradicting the initial motivation to move towards a more realistic scenario.4. There's a lack of information on how the compared methods are adapted to the new scenario. I searched the supplementary but failed to find a detailed documentation. With the given information, it is hard to tell whether the comparison is fair. My concerns are following,   - In the RandomClasses setting, it is stated that no coreset is used, if the compared methods depends on coreset to replay, it would be unfair. If that's the case, the only conclusion we can draw is that replay is better than no replay, which seems trivial to me.  - GD depends on internet crawled data, is it replaced with the unlabeled data since it is available in the experiment setting? If not, then I think it is just the setting that favors DistillMatch.  - With the above said, I suggest the author to list clearly the objectives, replay buffer sizes or even pseudo code for each of the compared method and their own method in a table, which will help the reader identify what major component in the proposed method is making the contribution.Regarding the quality and clarity,I found myself confused and making guesses sometimes while reading it.To list a few:- introduction paragraph 2, ... to determine which unlabeled data is relevant to the incremental task ..., I guess the incremental task means learning the newly observed data, but then for rehearsal we'll pick the unlabeled data which is from the distribution of past tasks.- section 1, ... save up to 0.23 stored images per processed image over naive rehearsal (compared to Lee) ..., here seems Lee et al is the naive rehearsal. But then "which only saved 0.08" confuses me, seems to be saying Lee saves 0.08 compared to naive rehearsal.- section 3, ... where data distributions reflect object class correlations between, and among, the labeled and unlabeled data distributions ... not enough information to infer what "reflect" and "object class correlation" means here.- section 4, ... Let S_{n-1} denote the score of our OoD detector for valid classes of our pseudo-label model ... what is the "valid classes" needs to be clarified. As I understand it, S_{n-1} measures how likely the unlabeled data is in the distribution of past tasks.- Super class / Parent class are not defined clear enough. This paper investigates how the inverse temperature parameter $\beta$ in the softmax-cross-entropy loss impacts the learning and the generalization. In the theory part, this paper introduces the concepts of early learning timescale and nonlinear timescale, and shows how the learning dynamics depend on the parameter $\beta$. The empirical investigations are carried out with wide Resnets on CIFAR10, Resnet50 on ImageNet, and GRUs on IMDB. The results suggest that the optimal $\beta$ is architecture sensitive. The theory developed in this paper is rudimentary. I don't see how the theory in Section 2 is crucial for the understanding of the impact of $\beta$. The results basically say proper normalization on the timescale is needed for a fair comparison among different $\beta$. There are neither formal statements nor proper discussions on the significance of the results. From the experiment results, the performance is relatively insensitive to the parameter $\beta$ with batch normalization. I don't see a clear motivation for dropping batch normalization or tuning $\beta$ with batch normalization. It is apparent that tuning for the best $\beta$ is always no worse than setting $\beta=1$. But the impact of $\beta$ seems to be insignificant, and the so-called optimal $\beta$ varies wildly across different experiments. The performance with different $\beta$ is not presented in Sections 3.2.2 and 3.2.3. Finally, the manuscript is poorly written and needs to be largely reworked to be considered for publication. None of the key concepts identified in the paper are formally defined. For example, it is unclear what is the mathematical meaning of $\ll$ and $\sim$ in Section 2.3, and what is the precise meaning of the phrase "no longer be neglected" in Section 2.4. Abbreviations like NTK are not introduced. Curves in the plots like Figure 6(c) dashed orange are not labeled. The paper proposes a hierarchical multi-agent reinforcement learning method for the restricted communication setting and verifies the algorithm performance in a number of useful applications. The hierarchical approach to the networked MARL problem proves novel, effective, and interesting.+ Strengths:+ The work targets an arguably less explored area by focusing on the restrictions on inter-agent communication that may be present in realistic scenarios.+ Evaluation setup is varied, explained in detail and visualized in an intuitive manner.+ Niche is well-identified, and the contribution is clear.- Major Concerns:- The reviewer had issues positioning the paper among the different lines of research. Although the research gap itself is clear (scalable MARL methods in a restricted communication setting), it isn't obvious why and how relevant the cited works are. For example, mentioning VDN, QMIX, and QTRAN (which together are some of the latest works in the factorization methods) does not seem to serve any further purpose, as they are no longer compared quantitatively or qualitatively to LToS. The authors' claim that they are not scalable leads the reviewer to anticipate that LToS naturally is scalable, but there appears to be no evidence whatsoever presented in the latter sections of the paper to show, let alone prove, the superior scalability of LToS, with, for example, growing numbers of agents and training times.-  Furthermore, some of the cited works have been left out at the evaluation stage, which leaves the reviewer puzzled as to which baselines LToS really hopes to outshine. The work needs some justification over why the following studies have not been compared to in the evaluation:If the main strength of LToS lies in its capability to function effectively and efficiently in restricted communications setting, comparison to one or more of the following works should be of great advantage in illustrating that edge:DIAL/RIAL by  Foerster 2016 - Learning to Communicate with Deep Multi-Agent Reinforcement LearningBiCNet by Peng 2017 arXiv - Multiagent Bidirectionally-Coordinated NetsCommNet by Sukhbaatar 2016 NeurIPS - Learning Multiagent Communication with BackpropagationIC3Net by Singh 2019 ICLR - Learning When to Communicate at Scale in Multiagent Cooperative and Competitive TasksSchedNet by Kim 2019 ICLR - Learning to Schedule Communication in Multi-agent Reinforcement LearningIf the main strength of LToS lies in its capability to resolve selfishness and assign credits appropriately to bring about a harmonious cooperation in social dilemmas, analysis with respect to the this work should be helpful:Eccles 2019 CoRR - Learning Reciprocity in Complex Sequential Social DilemmasIt would be interesting to draw some parallels between LToS and BAD, as both draw inspiration from a hierarchical decomposition:BAD by Foerster 2019 ICML - Bayesian Action Decoder for Deep Multi-Agent Reinforcement LearningThis recent AAMAS paper is based on peer evaluation and exchanging evaluation messages computed from recently obtained rewards:PED-DQN by Hostallero 2020 AAMAS - Inducing Cooperation through Reward Reshaping based on Peer Evaluations in Deep Multi-Agent Reinforcement LearningSome of the potential issues to discuss are: bandwidth usage of message exchange, message overhead in sharing the neighbors' rewards.Using neighbors' information to achieve scalability in MARL most likely requires discussion of mean-field methods, such as:Yang 2018 ICML - Mean Field Multi-Agent Reinforcement Learning.- Going through the Appendices spurred a great deal of curiosity, as the authors mention that all agents share the same, synchronized random number generator with the same seed across all the agents. This leads me to believe that the philosophy of decentralized learning is lost in LToS. Synchronization is definitely not cost-free; all the more so if the synchronized RNG is used to sample an experience from the agents' replay buffers. How do the agents synchronize their RNG in a decentralized manner?- In the Routing evaluation. has overhead been taken into account? How does LToS fare with respect to varied communications channel? What if the network were sparser? Do you observe any trends as you vary the extent of network connectivity? SUMMARY:The paper introduces MDP Playground, a parameterized suite of MDPs with low computational requirements that nonetheless present significant challenges for existing RL algorithms. The authors argue that MDP Playground is a valuable testbed for developing and evaluating ne RL algorithms. The paper also assess how the identified dimensions of hardness that can be exercised in MDP Playground transfer to more complex benchmarks. The paper also uses MDP Playground to evaluate several rllib algorithms.HIGH-LEVEL COMMENTS:This work has a lot of potential. It targets an important problem in RL research: studying the behavior of RL algorithms as an environment changes along various dimensions of hardness. It is valuable to have a benchmark suite of the sort this work aims to deliver, one that allows varying these dimensions in a controlled way and consists of problems that are simple enough for debugging. However, in its current form this work has non-trivial weaknesses in contribution and presentation that make publication at ICLR or similar conferences premature:1) MDP Playground's problems are completely unintuitive. They are randomly generated and aren't inspired by any real scenarios. This raises the question of how meaningful they are. In ML, it is understood from results such as the no-free-lunch theorem that no learner can do well across all possible tasks, and the sensible approach is to make ML algorithms work well on meaningful data distributions that are encountered in reality. Unfortunately, RL as a field hasn't been good at sticking to this principle: many of its existing benchmarks, such as videogames, look "interesting" and difficult, but in a very different way that real decision-making scenarios are. MDP Playground exacerbates this issue -- since its MDPs are randomly generated and don't have a natural interpretation, it's difficult to get even an intuition for a good behavior in them, and difficult to understand how to generate problems that that have a combination of hardness along different dimensions that is representative of realistic scenarios.A case in point is the paper's omission of MDP non-ergodicity (and presence of constraints on the desired policy as a common real factor that causes non-ergodicity) from its list of hardness dimensions. In reality, and even in some existing benchmarks, the learner can reach irrecoverable (absorbing) failure states, such as robots damaging themselves or objects they interact with, unless they are very careful. There are goal-directed MDP models with such states -- see, e.g., reference [a] at the end of the review -- and learning in their presence in realistic scenarios requires costly resets -- see, e.g., reference [b]. MDP Playground doesn't help with researching this aspect, e.g., by having knobs for probabilities of entering such states, penalties for doing so and the cost of recovery, despite it being ubiquitous in reality.Another major omission is the fact that in reality agent actions have durations that can make it very hard to learn a policy within a given time budget. The existing benchmark doesn't allow studying the effects of action duration on learning time. Given these gaps, I doubt that MDP Playground in its current form adds much value over the existing RL benchmarks, which have some drawbacks but have interpretability as a big asset for debugging.2) The paper's analysis of dimensions of hardness is quite unprincipled, contrary to the paper's claims. What makes many decision-making problems (both existing ones and those introduced in MDP Playground) hard is partial observability, but the paper never formally states what POMDPs are, nor even mentions this term. If it defined POMDPs formally, the incorrectness of some of the statements the paper makes about problems with partial observability (see the detailed comments below) would become obvious, and the connections between partial observability and hardness would become much clearer mathematically. See, e.g., reference [c] for a formal but accessible treatment of POMDPs.TECHNICAL ISSUES/RELATED WORK:-- In the intro, the paper claims that "partial observability [is] when the underlying environment is assumed to be an MDP, however, the state formulation, i.e., the observation used by the agent is not Markovian". This is an extremely inaccurate statement at best. In POMDPs, observations are Markovian -- their probability depends only on the current (hidden) state. "State formulation" is also Markovian, as it is fully observable MDPs. So are the belief states. What is non-Markovian in POMDPs is the optimal policy w.r.t. the observations: an optimal policy can depend on the entire observation history. However, again, each observation history maps to a belief state, and in the belief state space the optimal policies are Markovian.-- The same goes for several other statements, e.g. "performance degrades in environments where the delayed reward induces partial observability and hence makes the state used by the algorithm non-Markovian". How can delayed reward introduce partial observability? How can partial observability make "the state used by the algorithm" (an imprecise term in its own right) non-Markovian?-- Out of the dimensions of hardness the paper does identify, why is "sequence length" a distinct dimension, rather than being an instance of reward delay or reward sparsity?-- In the related work, a notable omission is reference [d], which looks at aspects that MDP hard and analyzes existing benchmarks w.r.t. those aspects.-- I wasn't sure what the benefit of the "varying reprsentations" experiment that tries to enforce various kinds of invariances was. First of all, the experiment is about applying various types of data augmentation to images, not about varying representations. Second, the conclusion that "this indicates that shift and other types of invariance do not come for free and that one needs to have sufficient amount of samples for the algorithm to become invariant to the transforms we desire." is rather obvious and well-known.-- A similar remark goes for several other experiments: varying time units, target variance, irrelevant features. Their results are completely expected, and while they might be a useful sanity check, the results description can be condensed to 1 line each and placed in the figure captions.-- A similar comment goes for the entire content of Section 4.3 as well-- it is very verbose and can be greatly condensed and structured into a short list with bullet points summraizing the findings. Typos:"Very low cost execution" --> "Very low cost of execution"[a] Kolobov, Mausam, Weld, "A Theory of Goal-Oriented MDPs with Dead Ends", UAI-2012[b] Eysenbach, Gu, Ibarz, Levine,  "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning", ICLR-2018[c] Kochenderfer, "Decision Making Under Uncertainty: Theory and Application", MIT Press, 2015[d] Maillard, Mann, Mannor "How hard is my MDP? The distribution-norm to the rescue", NIPS-2014 The paper presents a framework to learn depth prediction and semantic segmantation jointly; the key idea lies in making use of the pseudo depth label from stereo to provide supervision and as a means to enforce cycle consistency between the left and right views of the stereo. Reasons for scores: overall the paper is rather incremental and the idea is neither novel nor significant in my opiniont. I do not see interesting or deep insight from the paper towards the depth and semantic segmantation tasks.  Pros:+ First, the paper is clearly written and easy to follow. The proposed framework is pretty straightforward. + The idea of joint learning depth and semantic segmentation is good considering their tightly coupled nature. + The use of cross-view consistency as a constraint is good.Cons:- Overall, the paper does not have much novelty in my opinion. Joint learning of depth and semantic segmentation is clearly not new, and the paper does not provide new or particular insight towards this combined learning.- The use of pseudo label itself is nowadays quite common in the vision community.  And, the pseudo labels are used in the paper in a pretty trivial way in my opinion. -  The cross-view consistency across two views in a stereo setup is not new neither. It has been intensively used in the monocular depth estimation. In addition, this constraint is applicable to any individual task and does not seem to fit into the multi-task learning context, which is the main focus of this paper. I would expect specific insights in making use of pesudo labels towards solving the depth and semantics predictions; otherwise, any other tasks such as moving objects segmentation - The current title is too general, so much so that the main arguments made by the paper are not reflected in the title; I believe that the left-right consistency brought about by the pseudo ground truth depth is the main claim of the paper.  The paper aims to improve compositional generalization of grounded language learning methods. As far as I understood, this is achieved by (a) crafting a task-specific architecture with noise addition in certain spots and (b) changing the output format. Unfortunately, I did not understand many details that would be crucial to properly review this paper. I found the paper to be very unclearly written, see details below.1. As far as I understand the key contribution of the paper is supposed to be using interactions between the agent and the environment to find components in the output. The place in the paper that seems to explain what this is about is Section 3. But since this section does not explain what the original action space in the task is and does not show example trajectories, I found it impossible to understand what the "components in the output" and "interactions between the agent and the environment" are. 2. The entropy regularization is also explained poorly. I understand what EntReg is. But to explain ERL the paper refers to nodes x_i. I do not think the paper clearly explains what these nodes are, and how this is related to the possibility that a representation can be fed to multiple networks.3. I struggled to understand the specific architecture that is explained in Algorithm 1, but one thing that struck me is that unlike the baselines that the paper compares to, it uses an object-centric representation, whereas the baselines are taking images (albeit symbolic) as inputs. This distinction alone in principle might explain the difference in the results. 4. To make Algorithm 1 more understandable it could be helpful to visualize what the N command modules are and what they are supposed to do. A figure with an example that explains the motivation for the architecture would be of great help. Unfortunately, this concludes my review: I was not able to understand enough in order to make more substantial comments. I would encourage the authors to improve the paper in the following aspects: - clear explanation of the task, including input and output format- clear presentation of the model, including a visual motivating example- clear comparison to the baselines that takes into account the difference in input representations.  **Main Claim:**In this work, the authors propose to use the Feature Statistics Alignment paradigm to enrich the learning signal from the discriminator in a sentence generation GAN. The proposed model can generate sentences with better likelihood and BLEU on one synthetic and two real datasets.**Contributions:**This work introduces an novel and interesting idea of Feature Statistics Alignment in training GANs. The authors follow the convention in this domain, and evaluate the model on three datasets.The experiment results show that the proposed model outperforms existing models. However, the authors need to clarify some details to make the results trustworthy (see weakness). **Strong points:**The idea is novel and interesting. The model and training procedure is clearly explained. Related works are cited well. **Weak points:**In Table 2: - The LSTM model gets NLL lower than the real data. This is a clear evidence of overfitting. - In SAL (Zhou et. al, 2020), NLL_{gen} is used to evaluate the diversity of the generator. But this metric is missing here without explanation. In Table 3:- The BLEU metric in this paper is the BLEU(F) metric in SAL (Zhou et. al, 2020). This metric evaluates the generated sentences using the test set as a reference. Thus the BLEU(F) metric cannot show the diversity of examples. - The BLEU(B) (Zhou et. al, 2020) metric is missing. BLEU (B) metric evaluates the test set using the generated sentences as a reference, so it can detect mode collapse of a generative model. In section 4, although authors clearly cite previous works for experiment settings, I think its worthwhile to repeat the definition of each metric, and some other key points in the paper, so that readers can easily understand the notations and jargons in this section. **Recommendation:**Reject. Theres a major flaw in the evaluation metrics. On both synthetic and real datasets, the evaluation metrics prefers overfitted models, i.e. if the model can remember one example from the training set, and repeat that sentence, it can get a very high score. I will reconsider my recommendation if (1) I miss interpret the metrics or (2) the authors provide more evidence on the diversity of the generated sentences, for example showing the NLL_{gen} metric on the synthetic dataset, and BLEU(B) metric on real datasets. **Questions:**How is NLL_gen computed? Summary:This paper proposed a new approach for the video prediction task. The proposed model is built upon VQ-VAE, which has shown promising image generation qualities. The authors proposed to use VQ-VAE to perform compression (dimensionality reduction), which in return allows them to perform video prediction for high-resolution videos -- 256*256.Pros:1. The proposed framework is valid, and the equations in the paper describing the model are correct. There is no fundamental error. 2. The paper is well-written and easy to understand. 3. Qualitative results are promising.Cons:1. The biggest issue of this paper is the lack of novelty. In general, this paper is an incremental work based on VQ-VAE. It is a novel application, but the theoretical novelty is minimal. 2. The second major issue of this paper is that the quality of the evaluation is quite weak. Overall, there are two tables (table 1 and table 2) and one figure. Table 1 provides a human evaluation of the proposed Video VQ-VAE with baseline models. However, only one baseline model is compared. 3. The caption of table 5 and 7 are given as table 6 and table 8, which makes it very confusing to understand the content.4. In section 5.2, the author mentioned that FVD results for full-resolution as a baseline are given in Table 7. I do not think this table is included in the paper.5. Since the proposed model is built upon autoregressive models and VAE models, one important evaluation metric would be likelihood (or ELBO). There have been quite a few video prediction papers based on VAE models (such as "Probabilistic Video Generation Using Holistic Attribute Control" from ECCV2018), a thorough comparison with these model would provide more insight in the effectiveness of the proposed model.6. Compressing data with latents is listed as an important novelty of the model. However, I am not convinced this is the case. Any embedding, whether it's a probabilistic encoder in VAE or deterministic encoder, will have the compressing effect. Actually, almost all of the previous time-series papers rely on the dimensionality reduction step.7. In section 3.2, the authors claim that "the power-of-two design of our architecture leads us to condition on 4 and predict 12," while previous models condition on 5 and predict 11. This is listed as one of the benefits of the proposed model. However, most of the time-series models, no matter whether they are sequential latent variable models, or autoregressive models, or even simply an LSTM model, it is not a hard constrain on how many steps they can predict into the future. Theoretically, they can all predict infinite future steps. It is true that the quality of the prediction will decrease over time. But if the authors want to show that the Video VQ-VAE is superior from this perspective, experimental comparisons need to be provided to back up this statement. The goal of this paper is to learn cross-modal associations between a persons face and a voice. The authors use a standard three stream network trained with a triplet loss, and evaluate on the VoxCeleb-VGGFace datasets.   Strengths: - The authors train on VoxCeleb-VGGFace2, and evaluate on VoxCeleb1, which is a larger set of identities, and perform a t-test to show statistical significance. - The ablation study showing that performance saturates with more training data is interesting. Weaknesses: - There is limited novelty in the method. The authors use the triplet loss which has been widely used for this problem before (Kim et al. 2018, Cheng et al. 20- https://dl.acm.org/doi/pdf/10.1145/3394171.3413710). The authors also claim that anchoring the voice subnetwork with frozen weights is a novel contribution but dont discuss that this teacher-student style model was already tried in the Learnable PINs paper (where the face subnetwork is frozen). L2 normalisation of embeddings has also been used widely with the triplet loss, and has also been used in Learnable PINs (and hence been applied to this problem as well). - Table 2 is not a fair comparison. The authors have compared the performance of different models on completely different test sets, with a different number of speakers! To assess the performance of TriNet, it must be compared to SOTA methods on the same test set of 189 speakers, or if the authors prefer - on all of the 1,251 speakers in VoxCeleb1. In this case the other SOTA methods must be re-evaluated on this new test set.   This work focuses on the problem of cross-modal matching and retrieval for face and voice modalities. The paper suggests a new benchmark for the evaluation of both matching and retrieval tasks. It also proposes a confidence margin computation to verify the statistical significance of the results. The results reported on Vox (voice) and VGG (face) dataset using the suggested benchmark are encouraging. I have summarized my comments below which will help in improving the quality of this manuscript:  1. I believe the technical contribution of this paper is very limited for ICLR. This work builds upon the previous works in the area of cross-modal retrieval. The contribution is very incremental and most of the conclusions are very well known. The multi-stream N/W architecture is very similar to the previous works and triplet loss has been used extensively for face and voice tasks in the past. The suggested protocol just uses a different combination of training and evaluation set from Vox and VGG sets.2. In section 3.2, the authors mentioned "So the results of all related works that used VoxVGG-1 for training and testing are unreliable." I feel this is a very strong statement and may not always be true. First, the Vox dataset is collected from YouTube videos and represents a wide variety of conditions and results on the careful partition (no overlapping identities and conditions) of training and test may be very helpful. Second, in cases where evaluation conditions or use cases are known and similar to the VoxVGG-1 this may not hold true. 3. In speaker recognition, data augmentation plays a huge role in learning a noise-robust representation. It is not clear if the authors applied data augmentation to the Vox2 training set. These works highlight the importance of data augmentation:Snyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S., 2018, April. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5329-5333). IEEE.Zeinali, H., Wang, S., Silnova, A., Matjka, P. and Plchot, O., 2019. But system description to voxceleb speaker recognition challenge 2019. arXiv preprint arXiv:1910.12592.4. Did the authors use any voice activity detection system to remove silence in the audio? 5. In section 5.4, the authors claim "Therefore, voice anchored embedding learning outperforms face-anchored embedding learning." It would be better to clarify these results in Table 5.6. The current state-of-the-art in speaker recognition is TDNN based x-vector. I would request authors to add a reference to the following paper in the introduction:Snyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S., 2018, April. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5329-5333). IEEE. The paper proposes a method to avoid collision for the latent space based Bayesian optimization method. The main idea is to add a regularization term into the training process. Theoretical analysis is also conducted to understand the performance of the proposed method. Although the idea of the proposed method is somewhat interesting, I do have many concerns for the paper. 1) The writing is not good, so it makes it hard to understand the work. In particular, the English of the paper is frequently bad (wrong grammar, typos, unfinished sentences). The maths notations are occasionally not consistent. For example sometimes, the penalty is defined as p_[i,j}, sometimes it is denoted as p_{ij}.2) Section 4.2 is too ambiguous. What are z_i, z_j in the equation in Section 4.2? Based on the notation of the latent space Z, I can guess z_i, z_j are the values in the latent space, but this should be clearly mentioned in the paper. Also, what does \lambda represent? And how to set it in practice? I went through the 2nd paragraph in Section 6.2 and still feel unclear how to set this hyperparameter in practice.3) Section 4.3 is also not clear. What is the intuition behind the weight \omega_{ij}? What do \gamma and \rho represent? How to set them? And what does GP_{Kt}(M_t(x_i)) (in Eq. (1)) denote? 4) Regarding the theoretical analysis, unless I miss something, it is just the standard theorem as in Srinivas et al. (2010), but replace the assumption of the objective function f being a sample path from the GP, by the assumption of the latent space function h being a sample path from the GP? In which cases this assumption is satisfied? And what does it mean by comparing to Theorem 2 in Srinivas et al. (2010), the second part of the regret bound doesnt rely on \delta"? As much as I understand, the regret bound in Theorem 1 is the same as the one in Theorem 2 in Srinivas et al. (2010).5) Regarding the experiments, the experiments are only conducted on low-dimensional problems (2D, 6D, 3D, &), which is contradict with the motivation of the work (BO for high dimensional inputs). Besides, what does it mean when the neural networks are pretrained on a number of data points? Do we know the corresponding function values of these data points in advance? If yes, for the baseline methods the paper compares with, are these data points employed in these baseline optimization procedures? The submission proposed to use Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Basically, it makes use of bi-level optimization to find a subgraph that well encode the information for graph classification task. The major claim is that the resulting subgraph is more robust for the learning. 1). My major concern is on the novelty part. The submission is lack of novelty. First, the Graph Information Bottleneck (GIB) is used to learn a robust GNN against adverbial attack in the paper accepted in this year NeurIPS.Graph Information BottleneckTailin Wu · Hongyu Ren · Pan Li · Jure LeskovecNeurIPS 2020.The only difference is the submission uses the GIB principal to learn a subgraph. As for the subgraph selection, much work can be found. Basically, the edge dropping or node dropping  is popularly studied recently. By simple search, we can find DropEdge in ICLR20, NeuralSparse in ICML20, DropNode in NeurIPS20. The only difference is they are modeling general GNN, while this submission is only focusing on graph classification. As for the subgraph selection, is there any superiority of the proposed methods over NeuralSparse which uses gumbel-softmax?DropEdge: DropEdge: Towards Deep Graph Convolutional Networks on Node Classification ICLR20.NeuralSparse: Robust Graph Representation Learning via Neural Sparsification.ICML20.DropNode: A Flexible Generative Framework for Graph-based Semi-supervised LearningNeurIPS20.2). The submission is not well presented. Many notations are not defined before use. For example, in 4.2,´yi (y)´(Gsub,i), what is meaning of ´? What is the meaning of G_sub, i ?3). The experiments are not sufficient. The list methods are most famous GNNs but not SOTAs for graph classification task. Graph neural tangent kernel (GNTK), End-to-end graph classification (DCGNN) , Convolutional network for graphs (PATCHY-SAN). Besides, the graph kernel methods, like Graphlet kernel (GK), Weisfeiler-Lehman Graph Kernels (WLGK), and Propagation kernel (PK) are not compared.DCGNN: Zhang, Muhan, Cui, Zhicheng, Neumann, Marion, & Chen, Yixin. 2018. An End-to-End Deep Learning Architecture for Graph Classification. In: The Thirty-Second AAAI Conference on Artificial Intelligence4). The results reported seems much worse than results reported in other paper. For example, in the paper below. We can easily see the best result in MUTAG is 93.28±3.36, while the submission gives only 0.844 ± 0.141, in Proteins, best result is 77.47±4.34 while the submission gives only 0.749 ± 0.051. Similar case shows in DD dataset.Structural Landmarking and Interaction Modelling: on Resolution Dilemmas in Graph Classification,https://arxiv.org/pdf/2006.15763.pdf5). Typos:In 4.1, a informative representation==>an informative representationIn 4.3, S is 2-dimensional vector==>S is a 2-dimensional vector6). The authors does not report or discuss the running time complexity of the algorithm. Since the framework needs bi-level optimization, it is supposed to discuss how fast the algorithm will converge. This paper is basically unreadable. The sentence structure / grammar is strange, and if that was the only issue it could be overlooked. The paper also does not describe or explain the motivation and interpretation of anything, but instead just lists equations. For example, eta is the parameter that projects a spherical geodesic onto an the ellipsoid one, and an ellipsoid geodesic prevents updates of the core-set towards the boundary regions where the characteristics of the distribution cannot be captured. However, what are these characteristics, and how can they motivate how to choose eta? This paper mainly deals with the theoretical support for the loss function meta-learning and focuses on illustrating the generalization superiority of the TaylorGLO method [1]. Although the generalization performance is the core aspect of machine learning, I have several concerns as follows.1. Since the TaylorGLO method [1] is also under review in ICLR 2021, I can not judge the value of this paper.2. This paper claims that it theoretically analyzes the generalization superiority of the TaylorGLO method. But I cannot get this point from this paper. In my opinion, when we consider the generalization performance, the generalization error bound is preferred to answer this. Here the authors try to analyze the training dynamics of SGD for deep models since previous work has shown the implicit regularization effect of the gradient-based optimization algorithms. But, what's the connection between the training dynamics and generalization, or what's the hypothesis between these two in this paper? Do I miss something? 3. what do the theorems provided in this paper want to tell? I don't get the points that the authors want to tell. More intuitive explanations should be given followed by the theorem.[1] OPTIMIZING LOSS FUNCTIONS THROUGH MULTI-VARIATE TAYLOR POLYNOMIAL PARAMETERIZATION, https://openreview.net/pdf?id=bJLHjvYV1Cu 1. The premise of the paper is that the adversary can perturb the *test* set so that the model is shown to perform better that it really is capable of. And in Section 7 (Conclusion) the paper claims that it exposes this new risk. However, remember that this risk is already mitigated in practice by keeping the test data *independent* of the model/classifier (e.g., see Kaggle competitions where the test set is hidden). Therefore, the perceived risk is not even present. In the context that the technique has been introduced, it seems like the [malicious] actor would only be fooling him/her self rather than fooling the model/classifier.It is hard to come up with an application for manipulating the *test* data in the manner proposed and I am curious to hear what the authors feel.2. What if instead of manipulating the images to conform to the classifier's predictions, we manipulate the labels, i.e., relabel the test data as per the classifier's predictions? Would that result in the same final objective being achieved?3. Equations 1 and 2: The technique is based on generating adversarial training examples and does not make any fundamental technical contributions.4. Abstract: "Extensive experiments verify the theoretical results and the effectiveness of our proposed methods." -- Just a couple of standard simple image datasets (MNIST, CIFAR-10) have been employed. Harder and more number of datasets should be employed in order to justify the statement in the Abstract.5. Section 6 Discussion: "What we can do at present is using a relatively more robust model as a surrogate of the true friend to improve the robustness of a weak model." -- It is not clear how that would help. The attacker might mislead the 'relatively more robust model' as well. Moreover, why not use the 'relatively more robust model' instead of the weaker model?One simple mitigation strategy could be to digitally sign the test set to check for tampering. Why has that not been suggested? In the recent literature there has been a rise in the number of papers which attempt to verify neural networks. The specification of the verification problems often gets adapted according to the application in mind. More specifically, for image classification networks, the problem is to prove that the output of the neural network does not flip for small perturbations to the pixel values. For a robotic setting, the problem is often safety and convergence to some goal state. Where the neural network operates in closed loop with the system dynamics. The authors in this paper present an adversarial attack model on neural networks, which is deemed correct by some verifier. More specifically , given a neural network which can be shown to be robust to adversarial perturbations around some input, the authors exploit numerical errors in the computations to attack the network. Demonstrating the presence of loop holes in the proving engines itself. This is due to the approximation errors introduced by using floating point numbers.In my opinion, the notion of input sets in the space of images, is not a very useful one. Mainly because the interval valued sets representing  perturbations of the input image, is far removed from the intended specification.  It's a step in the right direction, if the verification of computer vision task was a well defined problem. Since it's not clear what to verify in the first place, the use case of this paper  is not a very convincing one in my opinion. The problem of verifying neural networks in a robotic setting has a more meaningful specification.  Hence, i don't think that this paper in itself will be interesting to the general theme of the conference.  This paper analyzes the convergence of SGD with a biased gradient, where the bias comes from label smoothing. The paper positions itself as a theorectical work towards understanding the success of such smoothing trick, but I feel the analysis does not add too much to the literature and its main results are somewhat trivial/misleading.In particular, given that the analysis hinges on a specific loss function, i.e. Eq. (2), which is linear in terms of $y_i$, label smoothing quickly translates to additive terms that can be well-controlled separately from the unbiased gradient. It is thus not surprising that standard analysis of non-convex SGD goes through here with a new additive term appearing in the final gradient upper bound, as shown in the paper. For this reason, I do not see quite technical novelty in this paper.My another concern is that the role of $\delta$ is unclear. By Eq. (4), this quantity is data-independent and should be treated as only depending on the distribution of $(x, \hat{y})$. Now if we take a close look at the main result, i.e. Theorem 3, it is fairly a weak result saying that SGD converges to a point with constant gradient. This is because the first case in Theorem 3 is essentially ensuring convergence to $\epsilon$-stationary point only when $\epsilon \geq \Omega(\sqrt{\delta}) \geq \Omega(1)$, i.e. a point with constant gradient. Likewise, the second case also boils down to the same guarantee.As such, the comparison in Table 1 seems problematic since $\epsilon$ should be treated as a variable arbitrarily close to 0. In words, only the first row of Table 1 will make sense. Yet, there are still two issues here.- I believe that the infinite iteration complexity of LSR is due to the drawback of Theorem 3 as I just pointed out. Namely, running LSR only gives you a point with constant gradient. Although TSLA does converge to $\epsilon$-stationary point, such performance guarantee is *not* due to your design of TSLA, but follows from standard SGD. In fact, simply running SGD from the very beginning already gives such guarantee. Thus, the first row to compare TSLA and LSR is somewhat misleading.- Now the question boils down to why not simply running SGD. In the first row of the table (and main text), it is argued that TSLA (i.e. LSR + SGD) has improved iteration complexity of $O(\delta / \epsilon^4)$ which is better than the $O(1/\epsilon^4)$ of vanilla SGD. I do not really agree with the conclusion because $\delta$ is independent of $\epsilon$. In fact, it can be a dimension-dependent quantity, and e.g. even blow up to $O(d)$ or so, making the iteration complexity of TSLA worse than SGD by a dimension-dependent factor.To make the work qualify for a top-tier venue, authors need to either present a new algorithm/analysis with vanishing gradient, i.e. an upper bound of the form $O(\delta \cdot \epsilon)$, or show hardness result, say any algorithm that takes the label smoothing must incur a stationary point with gradient $\geq \Omega(\epsilon + \delta)$. This paper proposes a reinforcement-learning-based model for aspect-based sentiment analysis. The RL model trains an agent that tries to walk through the most effective path from the aspect target to determine the final sentiment towards this target. In general, the RL model based on the dependency structure could be deemed as a pioneer implementation on aspect-based sentiment analysis. And this approach is able to remove the negative effect brought by irrelevant context. However, there are a few limitations that need to be addressed:1. The writing needs to be improved. There are many grammatical mistakes that affect the readability, e.g., "To effectively contain the impact from task-irrelevant information..." from the last paragraph in page 1: the misusing of word "contain". "The goal of aspect-based sentiment classification is to predict sentiment polarities (i.e., positive, neutral, and negative) for each given aspect.": "sentiment polarities"->"sentiment polarity".2. The proposed model is missing many details. For example, when will the policy terminate? It seems from (3) and (4) there could be infinite loop of candidate actions. And what is exactly the policy network in (3)?3. The RL model in this case is very limited in terms of exploration. From the construction, the candidate actions only contain connected nodes at each timestamp. This may miss important information, e.g., the second aspect in figure 1 could not traverse to "do not". How do you solve this issue? 4. I am not convinced by the statement that the proposed model is more generalizable compared to baseline models. Where does the generalization ability come from? And I conjecture RL needs even more training data to perform well.5. As mentioned in the main text, the reward function (6) could be any form, but no experiments are conducted on other forms. The experimental result in Table 2 misses other recent baselines, e.g., "Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification" that actually outperforms this method. ## SummaryThis paper provides a novel algorithm to estimate the optimal value of $n$ for $n$-step temporal difference methods. The paper derives an optimal value of $n$ based on minimizing a bias term, then utilizes intuition to derive an online approximation algorithm. The paper compares its adaptive $n$-step algorithm against fixed $n$-step values with both DQN and SAC empirically on several domains.## Review### SummaryI am recommending a reject for this paper. The paper is built from the assumption that "the underlying mechanism of why $n=3$ performs better than one-step temporal difference is still unclear." However, the paper misses a discussion on the well-understood bias-variance tradeoff present with $n$-step methods. I do not find that the paper provides any clarity or novelty in the _understanding_ of $n$-step methods. Although the proposed algorithm is novel, I remain unconvinced by the empirical demonstration and the lack of theoretical support for the algorithm.To increase my rating of the paper, I would have liked to have seen at least some of: * A discussion and analysis of the bias/variance properties of the proposed algorithm. * A proof that the proposed algorithm has better bias/variance properties than a naive fixed $n$ approach. * A statistically sound empirical investigation of the proposed algorithm (even if on smaller domains, like tabular gridworlds). * Fewer meta-parameters for the proposed approach. * A sensitivity analysis of the meta-parameters for the proposed approach.### TheoryThe theoretical components of the paper were largely poorly written, involving typos, inconsistencies, poorly motivated definitions, several forward references, etc. which made reading and reviewing quite challenging.* The terms "off-policyness" and "policy age" are used throughout the paper; however, they are only defined intuitively (i.e. not precisely) until section 5 where they are given an unsatisfactory definition. The paper defines "off-policyness" as the number of steps between when the behavior policy was recorded and the current timestep. This, however, does not take into account any aspect of the dynamical system defining the change in policies. For instance, imagine Q-learning with a stepsize of 0. The policy would never change, yet the "off-policyness" by definition would grow linearly according to this definition. The lack of solid definition here requires an unfortunate environment/algorithm specific meta-parameter later in the analysis to account for this fact (more discussion on this later in the review).* Throughout the paper, the term "error" is used but the formula given is $\mathbb{E}\left[ \hat{G} - v_\pi \right]$ which is the definition of bias. While a minor nitpick in terminology, this leads towards a lot of confusion throughout where some quantities are defined as "errors" while other as "biases", but what is ultimately meant is "bias". The term "approximation error" means something specific generally, and is not used correctly in the paper (for instance). Because "bias" is the only portion of the error that is investigated throughout the paper, then the most important portion of the discussion around $n$-step methods is ignored (namely, the variance). Even simplifying the setting investigated by this paper to being fixed-policy (e.g. policy evaluation) with target and behavior policies the same (i.e. on-policy), then still there is a trade-off between $n=1$ and $n=\infty$. This trade-off results from balancing between bias and variance of the estimated returns. While adding shifting policies and considering the off-policy setting does yield an interesting conversation on bias, the variance simply cannot be ignored.* The error decomposition is incorrect. It does not account for all of the error in the estimate, resulting in a "residual" unaccounted for error. We do not know what causes this error, we do not know if it implies that the defined accounted for errors are incorrect due to the residual error, or if there is simply some additional term not being accounted for, but we do know that the magnitude of the residual error is non-negligible due to the empirical investigation in Figure 1. Further, because we are actually looking at biases not errors, we cannot make use of the non-negativeness of these quantities to assert their independence. The residual error appears positive. What if the residual error results from not accounting for all of the error due to "off-policyness" (which is always negative) and thus cancels with the off-policy error?* Where did Equation 7 come from? This appears to be entirely intuited from a couple of empirical plots? Does the equation even make sense? I see $n^* = \text{round}\left( \frac{n_\text{max}}{n} e^{\min(1, \frac{p}{d})} \right)$ which results from an $\text{argmin}_n$. How do we solve this equation with an $n$ on both sides wrapped in a non-linear function? Is this based on Bhandari et al. (2018) where this paper incorrectly reads that gradient descent has an **exponential** convergence rate? Where does gradient descent come into play (TD is not the gradient of any function)? Why limit the exponent to be at least 1? Doesn't this lead to an algorithm where $n=2$ is the smallest n-step method we can consider?### Empirical StudyI highly appreciated section 4.2. I liked the investigation of the quantities defined in the theoretical section and asserting that assumptions hold. I however do not agree that the empirical results here imply the conclusion drawn. In what way do we see that "N-step returns works because a suitable selection of $n$ makes the overestimation and underestimation cancel each other"? This conclusion is not clear from the results. It also lacks the appropriate nuance given the highly limited study and lack of statistical evidence to support the claim.How did you choose the meta-parameter value of $d = 122952$? This seems....arbitrary at best. What does the sensitivity of $d$ look like? $d$ appears to result from the fact that you did not account for the rate that the policy is changing, only the number of timesteps that have passed. This makes the fraction $\frac{p}{d}$ become an approximation for policy change rate, where the onus is entirely on the algorithm user to estimate and tune this quantity for their given algorithm, other meta-parameters, and environment.The rest of the plots in section 6 onwards have a fundamentally broken empirical methodology that renders the results largely uninterpretable. Comparing the mean and standard error (I believe this is what the shaded region shows?) of the top 4 out of 8 runs will not yield statistically valid results. The differences between the algorithms will be lost to variance and noise, making it impossible to know which algorithm is actually best. More runs would be needed and likely a different validation metric as well. Note that using the 50th percentile disproportionately favors high-variance methods as it cuts off their failing runs. This implies to me that the proposed algorithm has much higher variance (as I would guess from the algorithm itself) and only works a small percentage of the time. Perhaps this is still a desirable trait, but this should be stated and investigated explicitly and not hidden behind strange empirical decisions.I will refer to Henderson et al. 2015 for a discussion on why showing the mean and standard error of the top 4 out of 8 runs is insufficient for a statistically valid methodology. I will also attach a short python script which demonstrates a simplified version of Figure 5 from Henderson et al. 2015, demonstrating very clearly that no conclusions can be drawn from this methodology. Even ignoring my complaint at the unusability of the empirical results in section 6, still the difference in performance of the proposed algorithm and a 2-step baseline appear negligible; especially once one considers the introduction of two new meta-parameters to replace the original one meta-parameter, and that these two meta-parameters appear difficult to tune.#### A demonstration of methodology```pythonimport numpy as npnp.random.seed(0)N = 8TOP_N = 4# hidden underlying process# let's see which of these two "algorithms" is better# Note, however, that they are in fact the samealg1_performance = np.random.normal(0, 0.1, size=N)alg2_performance = np.random.normal(0, 0.1, size=N)# only look at top n pointsalg1_performance.sort()alg2_performance.sort()alg1_top = alg1_performance[-TOP_N:]alg2_top = alg2_performance[-TOP_N:]# which is better? (note both should be approximately 0)print("mean 1:", alg1_top.mean()) # => 0.17print("mean 2:", alg2_top.mean()) # => 0.08# can we estimate variance? (Nope)print("std 1:", alg1_top.std()) # => 0.046print("std 2:", alg2_top.std()) # => 0.042# what about confidence intervals? (Nope)alg1_stderr = alg1_top.std() / np.sqrt(TOP_N)alg2_stderr = alg2_top.std() / np.sqrt(TOP_N)print("CI 1:", (alg1_top.mean() - alg1_stderr, alg1_top.mean() + alg1_stderr))print("CI 2:", (alg2_top.mean() - alg2_stderr, alg2_top.mean() + alg2_stderr))# yields CI 1: (0.15, 0.19)# yields CI 2: (0.06, 0.10)# these confidence intervals don't overlap, therefore# conclusion: they must be different algorithms# and alg1 has better average performance, so alg1 must be better``` This paper presents weakly supervised framework for image segmentation tasks with limited annotated data. It first builds several labeling functions with limited annotation and then uses probability graph to fuse the labels.  Last, the final output will be generated via CNN network.  There are two key challenges in such setting. First, how to build the labeling functions. Second, how to measure the accuracies from the labeling functions. However, these two challenges are not well described in this paper. First, the labeling functions are built on prior knowledges such as physical models, clustering and pretrained network.  Especially , for the pretrained network, it is not fair in few shot learning setting.  Second, the author claims she/he uses one image to train the LFs, since applying one image to train the network is difficult, it is very confusing on how to train the network. Third,  the author uses 60% data for training, it is not clear how to differentiate the proposed weakly supervised training with traditional supervised training (it is not uses true label?but the paper not describe it).  Morever, since FS-5 has reasonable good performance, the author should also compares with FS-60%.  Since current paper has lots parts need to be cleared, the current draft can not be accepted.  Summary:In this paper, the authors design a method to evaluate the gender bias for natural language inference tasks. They construct an evaluation dataset that consists of (premise, female hypothesis, male hypothesis) and design three scores (inconsistent predictions, probability gap, and dominant probability) to measure the gender bias for BERT, RoBERTa, and BART. The experimental results show that those models indeed have a gender bias. They also show that the bias can be reduced by data augmentation.Gender bias is an interesting and important topic in the NLP domain. However, I have some concern about this paper:- When constructing the evaluation dataset, the authors replace the occupation word in the premise with other occupation words. However, this can lead to inconsistent semantics. For example, "the doctor is operating" becomes "the teacher is operating", which may not fit the realistic situation.- The constructed dataset contains only entailment pairs. How about analyzing the contradiction cases as well?- When analyzing the results, the authors disregard the neutral case. I am wondering if they train the models in the same way. If not, it seems that there is a domain mismatch.- This point is my primary concern. In the constructed dataset, the hypothesis is generated by templates and looks like the context is not very related to the premise. However, in most of NLI datasets, the premise and hypothesis are usually related. So the domain of training set and their constructed evaluation set are different. It can be reasonable for models to perform not well and have the bias on the evaluation set, since the domain changes a lot. Why not consider the existing hypothesis? For instance, replace "he" or "his" in the existing hypothesis with "she" or "her" so you can have female hypothesis and male hypothesis.- I don't quite understand the definition of B for evaluation. Is that probability for some predefined gender-specific occupations? If that is the case, how to define those words?- What is the definition of "bias" in Figure 1?- In Figure 6, how to calculate delta P for female and male respectively? In my understanding, delta P is the probability gap between female and male hypothesis.I  suggest that the authors use one table to show the difference before and after the data augmentation to compare the numbers more easily. Some typos- In Table 2, teacher => guard. This paper claims to present an algorithm which enables a population of (two) agentsto learn to communicate and coordinate to solve a task, and thus positions itselfin the field of multi-agent Deep RL. After a long but rather vague and unspecific introductionand related work (see below), it describes the algorithm, then presents experiments wherethe introduced algorithm is compared with model-free MARL baselines.While the algorithm presented is interesting and has potentially some novelties compared tothe state-of-the-art (e.g. differentiability of message passing in model-based MARL), it hasalso a number of weaknesses:1) Globally, I had a lot of difficulty understanding clearly what are the aims of this paper:What are the problems it aims to solve? What are the scientific questions adressed?Neither the abstract nor the text provide sharp explanations of these aims.2) The paper uses very loaded but undefined vocabulary like "imagination", "language" and "communication".While in general I think it can be sometimes useful to use concepts and terms from human cognitive sciencesto describe AI systems, in this particular case I found it very far fetched to speak of "imagination" and "language",even "communication". It seems in practice authors might simply mean something like "prediction of future states"when they use the term "imagination". "Language" and "communication" are also far-fetched because in cognitivescience and linguistics it refers to systems that enable different individuals, with different world views, tocommunicate an intent to each other. Here, the "agents" share the same world model, so they are not reallydifferent individuals with their own world representations, and their communication is rather like message passing in GNNs, which is pretty far from "language" or "human-like communication".3) It is not even clear whether it is meaningful to call the presented system as "multi-agent", since in additionto a centralized shared reward, there is also a shared world model. To me, the system looks rather like an RLsystem that controls a multi-component body with local controllers that synchronize through message passing,quite similary to graph neural network controllers (also including message passing) used for e.g. in Pathak et al. 2019.A discussion of the similarities and differences with work such as Pathak et al. is needed.4) the authors are right to say that there is little research on model-based MARL, and cite one exception:Krupnik et al. However, it is not justified why this closely related work is not included in the baselines,or at least compared in discussion more thoroughly. Authors might also want to discuss another model-based MARLpaper: Zhang et al. 2020.5) A large part of the related work section is not relevant to this paper, in particular about Deep RL and model-based RL,which are much broader topics than the one addressed in this paper6) The description of the method lacks sufficient technical details for reproducibility, in particular it lacks detailedpseudo-code (some refs are said to be in an appendix, but I did not find an appendix), and no links to code is provided.Furthermore, there is no sufficient information on how hyperparameters selection for baselines was made.7) The two environments in the experiments are not sufficiently well motivated: why did you need to introduce them ratherthan reuse existing test environments? E.g. which particular problems did you want to address that was not possible withexisting environments ?8) Since the claimed topic of the paper is about the emergence of a "communication system", one would expect a detailedanalysis of the emergent communication code (currently only figure 5 gives a quite superficial qualitative analysis).9) The quantitative comparison of algorithms is not made using a sufficiently strong statistical method (only 5 seeds,no tests such as Welch t-tests)For these reasons, while the particular algorithms studied is in itself interesting, I think the paper would need a majorconceptual reframing and a better experimental methodology and justification before publication.References:Pathak et al. (2019) Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularityhttps://arxiv.org/pdf/1902.05546.pdfZhang, K., Kakade, S. M., Ba_ar, T., & Yang, L. F. (2020). Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461. The paper talks about developing a model-based method for cooperative multi-agent reinforcement learning. The proposed approach utilizes communication as a tool for mitigating the partial observability induced by the non-stationary task while also helping agents reason about other agents' behaviors. The authors present their motivation for using language as a medium in model-based RL stemming from early literature in psychology and linguistics.The setup consists of decentralized agents each of which is equipped with a world model similar to Ha et al. 2018. Further, each agent also has a separate message input that is received from the other players. Each agent does a form of decision-time planning where it produces rollouts for K steps before taking a real action. The message is then the encoding produced by the concatenation of the observations, rewards, and the actions taken during the rollouts.The approach is novel and one of the first works that combine model-based RL in a dec-POMDP. The paper does a good job of explaining prior work in related domains. The schematic diagram also depicts the setup in an efficient and standalone manner.Still, I have some qualms related to the experimental setup that arguably makes the contribution of the proposed imagination framework inconclusive.- In the digits game, the agents need to produce actions that represent the next observation of the other agent. The transition dynamics are defined in a way such that the next observation for an agent i is independent of the action taken at the current timestep. I find this formulation to be incoherent with the way MACI works. Specifically,      a) The AgentController that produces the action doesn't need to depend on the current observation since it has no effect on the action.     b) The WorldModel produces the next observations, next hidden states, and the rewards given the current observation, current action, and current hidden state. Similar to the above, the information about the current action is not needed to produce the next observation. Moreover, the rewards, in this case, are only tied to the action. So it would make sense to produce it along with the action in the AgentController with a recurrent network.Overall I believe this game is not aligned with the objectives of MACI, although I would love to have the authors clarify this.- There is no information about the objective functions used for optimization or any detail about the learning process without which it makes it hard to reproduce.- The choice of baselines doesn't seem to be appropriate for the task. Since all the baseline methods used do not use explicit communication in their original forms, the comparison thus becomes unfair. I would like the authors to reference if the baselines were modified in a way to accommodate this. This is important specifically in the two tasks chosen since I believe just adding communication should yield sufficient improvement.- The current approach is only applicable for a two-agent cooperative game narrowing down the scalability of the method. I believe the approach has the potential to extend to multiple agents either by having a confluence of messages or explicit grouping of agents. - An important missing ablation experiment is comparing comm+world model with only world model. This is crucial since it will determine whether the performance gain is due to the abstract planning or the communication.- The overall compute required is more than running a real-time experiment since the planning uses K-step rollouts. Some ablation of the choice of K would be interesting to look at especially in terms of wall time.typo: Fig 6-A title This paper aimed to understand the self-supervised learning algorithm under a teacher-student network setting. The authors argued that the gradient are specified by a covariance operator, that can amplify extracting the intrinsic features that are invariant to the data augmentation.I have several concerns on this paper, mainly on the over-simplified setting, unrealistic assumption, and inaccurate claim. Also, the presentation is weird.The detailed comments:1. I feel most of the theorem are just simple calculus. Its inappropriate to claim such calculation results as theorem. At least one theorem should contain some information on the property of the terms of interest.2. As far as I know, theres no existing empirical work used the ell_2 loss on the feature as the dissimilarity, though when the features have norm 1 (which cannot simply hold in practice) there are some connections between the ell_2 loss and inner product. I personally would argue that, the authors should first make an empirical justification on this simplification, say such dissimilarity measure can empirically work. I dont think this is a valid simplification. The authors may argue that methods like simCLR have normalization before computing pairwise similarity, but this will dramatically influence the gradient update. Maybe the remaining conclusion still holds, but at least the authors should include such part.3. I would like to say that the assumption for Theorem 3, i.e. the gradient of the negative samples are identical, are too strong and unrealistic. Definitely this will not happen, even though the negative samples are from the same distribution, there should be some variance. I cannot accept such assumption. Even take H=1 is better than this assumption.4. Beta is used without any introduction.5. As the authors say, the covariance operator are changing over time, its hard to claim that this covariance operator let the parameter align with some specific direction that is good to learn a good representation. Take the NTK limit may be one possible way to get some more interested result, however, the authors havent done that.6. And I would argue that, with only analysis on gradient, its rather hard to make some strong claim. There are several existing work on homogenous neural network that characterize the optimization and solution with e.g. separable data, which is much more convincing.7. I feel the description starting from Section 4 on two groups of latent variables are so intuitive and also unrealistic. How can we identify such groups of latent variables and how can we make sure in practice we dont perturb the class/sample-specific latent? On the other hand, what if we perturb the class/sample-specific latent? Imagine we have a Gaussian mixture of two isotropic components, if we augment the data with the isotropic Gaussian, can we make an informative projection onto low dimension with self-supervised learning? I think the current intuitive and unrealistic setting restricted the potential application in practice.8. Whats the definition of bar{K}_l(z_0) as the input of this term should be x? Whats the form of the term after integrated out z^\prime? I feel there are so many ambiguities here and Im afraid I cannot accept such claim.9. I think the arguments in Section 4.1 need to be justified more formally, if this is a `theoretical paper. I wonder what does the author want to say in Section 4.1, the importance of non-linear activation? 10. In Section 4.2, why constrained to 1 output? I would like to ask, as in the derivation in Section 3, there should be no straightforward barrier on considering this more general case?11. In Theorem 4, is the A matrix fixed during the dynamics? Im quite skeptical on that, as the w in the definition of u are changed over time, which can influence the indicator. Also, why we have the covariance is equal to zero? Is that the normal case? Also, there are so many ambiguities here, see 9, can the authors give all the formal description of the neural network we considered, the input data distribution, etc. in a clear way? To be honest, I cannot understand the proof of Theorem 4 as well, due to these ambiguities.12. For the description in the last of Page 7, I totally get confused. What does the authors want to say on that? Even the parameter converge to some point, we cannot say during the optimization it keeps the indicator 1? Can the authors stop using such ambiguous description and give some formal description and claim?13. Whats the meaning of considering such HLTM? Can it represent the general case of learning? I need to say the authors does not convince me that such setting is general and its necessary to consider the multi-layer network with such setting. I would like to say, its better first considering the fully-connect network rather than considering the network with local receptive field.14. I would like to say the organization of Section 4.3 is really weird. Can the authors give a formal description in the neural network and generative model in Section 4.3, even in the appendix with simpler but clear model? I dont even know the data generating process of x given z, thus does not the meaning of given a sample x how to resampling z^\prime. As the notation are only described in Table 1, I cannot get the meaning of each term in the theorem. Give some description on each of term with a formalized mathematical description on the generative model and network, please.15. Also, I does not feel the theorem in Section 4.3 give some strong arguments. What we really care in the self-supervised learning is the quality of representation, the sample complexity, the convergence analysis, none of them have been addressed by the author formally. Instead, I feel Theorem 6 presented here is not of particular interest. Also, the discussion is quite intuitive.16. Whats the sym subscript in Equation 10? Please introduce the notation before used it. Also, whats the term deltaW_l^{BN} in Equation 12? Please be self-contained.17. The observation of Theorem 7 can be interesting, however, in practice, what we really do during BN is the latter one, i.e. x - x.mean().detach()? And in fact, the mean we subtract in BN is a moving average statistics. I feel it can explain something, but not why BYOL works. The claim that such observation give an analysis of why BYOL work is not accurate. In the contrast, I think it even predict that BYOL will not work or at least need to be fixed.18. Overall, what the authors have proposed are better understood as some intuition or observation, not some rigorous theory.To sum up, I feel the presentation is weird. The settings are over-simplified and the authors even didnt introduce the setting with formal description. The assumption can be too strong while some explanations are too intuitive. The authors want to argue several points, however, none of them are strong enough and can be claimed as 'theorem' (I feel they are only 'observations'). Meanwhile, I feel the authors include such different points in the main text with the expense of readability.If this is an empirical paper introduced some observation on the self-supervised learning with detailed ablation study and some kinds of theoretical characterization, Im happy to accept this paper. However, as I dont feel the authors provide either strong theoretical results or detailed ablation study, meanwhile, the presentation is totally a disaster, I think this paper is not suitable for publishing right now. Summary: this paper generalizes existing decision trees to some neural-style model. The most critical argument is that the model generalizes decision tree while maintaining interpretability. Since this is a new model, interpretability should at least be justified with the visualizations that can be judged as interpretable (let alone an objective measurement or human experiments). However, the demonstration of interpretability is far from satisfaction, so I recommend a clear rejection. See below for further details. 1. One of the major motivation is using a linear number of parameters w.r.t. tree depth to construct a decision tree-style model. However, this problem has been investigated in literature but not compared theoretically or empirically at all in this paper. See the classic paper [1] and the recent paper [2]. 2. The current proposition 1 does not seem useful. Any classifier $p(y | x)$ can be written as a decision transformer $F \pi_0$ by letting $\pi_0 = [1]$ and $F = p(y | x)$. The factorization only makes intuitive sense when each $T_i$ is restricted / interpretable. 3. Interpreting a stochastically routing decision tree would inevitably involve inspecting the whole tree, since every prediction involves the whole tree. Hence, visualizing the whole tree is necessary to claim interpretability, especially for a new architecture. The only visualization in Fig. 3 is far from visualizing an interpretable model. 4. The theoretical statement is not clear and rigorous. E.g., what do the authors mean by "explainable"?5. the proposed architectures seems to highly relevant to hierarchical mixture of experts [3], which can be trained via EM algorithms efficiently. Can the authors show similar things here?[1] Langley, Pat, and Stephanie Sage. "Oblivious decision trees and abstract cases." Working notes of the AAAI-94 workshop on case-based reasoning. 1994.[2] Lee, Guang-He, and Tommi S. Jaakkola. "Oblique Decision Trees from Derivatives of ReLU Networks." International Conference on Learning Representations. 2019.[3] Jordan, Michael I., and Robert A. Jacobs. "Hierarchical mixtures of experts and the EM algorithm." Neural computation 6.2 (1994): 181-214. Short summary----------------------------The authors investigate the relationship between model generalization under distribution shifts and attribution techniques. They hypothesize that imposing better attributions in a model (which they define as being more aligned to a mask selected by domain experts) would increase model generalizability. However, they observe that such constraints hurt the performance under no shift, and do not necessarily lead to increased performance or better feature attribution maps under shift.Strengths-----------------------------This work investigates 3 datasets: one synthetic dataset where the effects are well-controlled for, as well as 2 manipulations of real-world data in medical imaging. It includes different and recent techniques to constraint model learning by masked representations, and does not make bold claims about the results.Weaknesses----------------------------There are however a few weaknesses that represent major concerns to me:1) The main assumption underlying this work is that improving feature attributions will help generalizability. I however do not think that this is the message that was relayed in past publications on the topic (contrarily to what is mentioned in the introduction) and do not think that better attributions is sufficient for better generalizability. My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts. Making this a sufficiency condition is a step that is not well-motivated to me.2) The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent models decisions and to be sensitive to different factors (including shift-variance, Kindermans et al., 2017, or being susceptible to adversarial perturbations). In addition, different attribution methods will likely display different patterns.3) Which is why I am wondering whether other attributions, which are more theoretically grounded (e.g. integrated gradients [Sundararajan et al., 2017] or more recent gradient-based techniques like DeepLIFT, or from non-gradients based techniques like SHAP or occlusion) were investigated.4) The authors mention that masks represent a good attribution map. However, there is no guarantee that features in those masks are not affected by distribution shifts. This should be discussed as a limitation of mask-based regularizers.5) The authors seem to have missed that the masked trained classifier also highlights the confounder on the synthetic data, despite it correctly predicting the outputs. I believe this is related to the choice of saliency maps, as raw gradients do not explain the decision of the model, i.e. it does not display the effect the features have between a bad or neutral decision and the current prediction. For this reason, I would suggest using integrated gradients or another technique.6) The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion.Novelty:------------------------The authors propose two novel methods to constrain models by using masked representations. While in theory they seem interesting (e.g. activations have been successfully used for OOD detection), the results obtained vary per dataset and there is no clear advantage in terms of model generalization to be using their technique over other methods.I feel that the main assumption is not well-grounded, and hence, to me, novelty does not reside in the field tackled.Clarity: ----------------------Mainly, I found the paper well written and the experiments clear. I would suggest some proof-reading (see minor comments, some repeated or missing words). Mostly I would suggest to revise the introduction as it wasnt clear to me what the purpose of the study was until well into the experiments. I also found that the introduction is not well matched to the main message of the paper (e.g. it does not mention mask-related constraints).Rigor:------------------------I found that the comparison between the proposed technique and the literature was well explained and sufficient. Hyper-parameters were described and confidence intervals were provided on all results. I enjoyed the fact that three datasets were included. Overall, I think the experiments were well executed.Detailed comments:------------------------------ A critical assumption underlying these aforementioned works is that properties of the saliency map are indicative of generalization performance. I do not agree with that statement given my major concerns (1).- Intro: masking is mentioned in the key contributions but not before and no justification is used. Such masking also assumes that the shifts across train and test distributions do not impact features in this mask. This seems like a strong hypothesis to me, especially when considering e.g. different imaging sites, or image resolutions.- It is unclear whether the goal is to obtain models that generalize better or whether it is to obtain feature attribution maps that are consistent with expert input. These two goals can be misaligned, as displayed in the results and should not be conflated.- actdiff method: can lambda be mentioned in the equation? How would such a regularizer perform in a high dimensional layer (curse of dimensionality)? Why use pre-activation outputs? Were any other versions of this formulation tried?- How were the masks defined? Is there variability per sample (e.g. different experts) in their definition?- The authors refer to the synthetic dataset as the changes in the chest x-ray but then have a separate synthetic dataset. The language is confusing and datasets should be defined before being referenced to.- Synthetic data results: the masked model performs best, ignoring the confounder. However, the saliency map reflects that the attributions are high for the confounder. Therefore, I do not see the same high correlation between IoU and AUC that the authors mention. In addition, this, to me, reflects the main limitation of attribution maps: they do not reflect the models decision. They rather reflect the local effect of a feature on the label (Lipton, 2016, Ancona et al., 2018). I am wondering why the authors select saliency maps compared to e.g. integrated gradients (Sundararajan et al., 2017) or other gradient-based but more mathematically grounded techniques. Given my own experience of attribution techniques, it is likely that using different attribution methods will lead to different conclusions with respect to the correlation between IoU and AUC.- I understand the choice in the setup of confounders for both datasets. However I am unsure how this represents real-world settings. For example, training sets might indeed be site-specific, but it would be surprising to me that the test presents the inverse of the confounder. We could for example expect the absence of that confounder (which could be simulated by removing the confounder in the test set for synthetic data), or lower correlations between a feature and the label.- ActDiff substantially decreases model performance in the absence of confounders- If the goal is to obtain more generalizable models, other techniques could be envisaged when the relationship between label and confounder is not deterministic, like resampling or reweighting. Were these considered?Minor:------------- Intro: PAC undefined- intro: the authors conflate the behavior of a model and which inductive biases it relies on, with the obtained saliency maps. This is however a complex and, in my opinion, unanswered question.- Figure 1: NIH, PC, PA and AP not defined. SPC and VPC are presented succinctly without intuition and we only understand them much later. Maybe this figure should be moved in the results.- contributions: point 1 needs proof-reading- out of distribution feature attribution phenomena: I searched DeGrave et al for this term but could not find it. If not used elsewhere, I would rephrase as this is a confusing formulation: samples/images can be OOD, and these samples can provide feature attributions, but the feature attributions themselves are not OOD.- proof-reading of the text is required. Some explanations are poorly framed and can be rephrased (e.g. related works, paragraph 2).- Zeiler & Fergus, 2013 (published at ECCV 2014) refers to the work on occlusion, where inputs are masked by a baseline value and the changes in predicted risk is defined as their attribution. While this technique provides one attribution per feature, it is not based on the gradients of the network. This paper also does not refer to the term saliency.- Formulas 4 and 5, consistency in the formulations is desirable: if showing for binary classification in most cases, it is better to keep this set up for other formulations (even if they could be extended to multiclass), especially as the experiments are run on binary classification. The limitation of GradMask to binary cases could however be mentioned.- Figure 5 can be difficult to investigate without colormaps. For instance, it looks to me like the masked training model does highlight the confounding factor. The author proposed a input transformation method for countering adversarial attacks. The proposed method can be applied to any pretrained network and improve the adversarial robustness.However, I have some strong concerns regarding the evaluation method of this paper:- The proposed method is based on the principle of obfuscated gradient, which has shown to be vulnerable against adaptive attacks [1]. The authors did not mention this at all in their paper, nor evaluate their method against adaptive attacks.They only evaluate the adversarial accuracy under PGD attack. I believe this is based on the assumption that the attacker does not known the defense algorithm. However, this is not a comfortable assumption in the cryptography point of view when building a defense algorithm. The author should evaluate their method against some adaptive attacks before making the conclusion of whether the method is robust or not.- As pointed out in [1], defenses based on obfuscated gradient may still suffer from gradient-free attacks. The author should also evaluate their method on gradient-free attacks like [2] or [3].Other minor comments:- Some baseline methods used by the author (e.g., Guo et al. and Xie et al.) have already been shown that are not effective [1]. It is not necessary to include them as baselines.- This paper [4] also utilize the first-order gradient information at inference time. I'm not sure how similar is [4] to the proposed method but the author should discuss the difference in the related work.[1] Anish Athalye, Nicholas Carlini, and David Wagner. "Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples." International Conference on MachineLearning, 2018.[2] Chen, Jianbo, Michael I. Jordan, and Martin J. Wainwright. "Hopskipjumpattack: A query-efficient decision-based attack." 2020 ieee symposium on security and privacy (sp). IEEE, 2020.[3] Ilyas, Andrew, Logan Engstrom, Anish Athalye, and Jessy Lin. "Black-box Adversarial Attacks with Limited Queries and Information."  International Conference on Machine Learning, 2018.[4] Xiao, Chang, and Changxi Zheng. "One Man's Trash Is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. The work utilizes the SIS (a local feature-importance method) to empirically prove that on existing benchmark datasets, the trained convnets are capable of making decisions based on a very small subset of pixels that are meaningless to the human observer but are nonetheless strong signals. Interestingly, unlike common belief, the same phenomenon is observed in adversarially trained models. The main problem with the work is the discrepancy between claims and results. Pros:* The observed phenomenon is interesting and the work is novel* The experimental setup is comprehensive.* The writing is decent.Drawbacks* The main drawback with the work is its claim: "that the highly sparse subsets found via backward selection offer a valid predictive signal in the CIFAR-10 benchmark". "these sparse pixel-subsets are underlying statistical signals that suffice to accurately generalize from the benchmark training data to the benchmark test data".I'm not so sure: 1- First, the fact that the observed phenomenon is very model-dependent (other models get low accuracy). This is against the assumption that there generally there exist sparse features that correlate with the class label. It might simply be a nuance of each architecture. One important modification that seems necessary is to create the training set using one architecture and then test the hypothesis using a different one. Just think about a simple scenario: In each architecture, the location of the interpretation mask tends to correlate with the class labels (important features of boat appear at bottom and dog appear at the top). In this case, the observed phenomenon is totally expected but not surprising. It seems like all the experiments on the sparse features are trained "and" tested with the "corresponding" dataset. I personally cannot be sure that the observed results are not simply indicative of the correlation between class labels and the shape of the SIS masks. One simple way of answering this question is how good a model trained on the sparse features is on clean images. If the answer is yes, then one can claim that there is enough signal that the chosen sparse subsets highly correlate with the label. Note that the reverse experiment (high accuracy of models trained on clean data when predicting sparse images) is not enough as in this experiment, the model itself is used to generate the sparse subset of features (which again means that it's a model-specific mask).* The tone of the work suggests that this behavior means that existing models will be fragile for out of distribution data. While that might be true, this paper's observations as mentioned above, do not provide enough evidence. The work should either show such OOD samples or create a set of sparse images of CIFAR10 that are classified with high accuracy using any CNN architecture trained on clean images. Questions and notes:* It seems like choosing 5% of images randomly captures a large amount of signal. This is a very interesting observation.* The interpretation being 5-10% of the images is not necessarily an indicator of poor behavior. Although the shown examples are indicative, they are hand-selected. I would like to see the average total variation of masks reported in order to have an idea about how scattered the important pixels are one average* The work seems very related to adversarial examples are features, not bugs work, the towards automatic concept-explanations work, and ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. It would be good to explain the relation of this work to each in detail. Summary: This paper focuses on model-based black box optimization problems in the offline setting. These are settings where access to ground truth is expensive, and instead the optimizer has access only to a trained model of the ground truth based on limited data. While optimizing on this surrogate space, a good optimizer often needs to account for model uncertainty and accuracy degradation. The main aim of the paper is to provide a test bed for algorithms that try to solve this challenge. Positives: This is a well-motivated line of work because there is a large interest in these problems across fields. There is indeed a need for better benchmarks and better libraries to make it easy to compare methods. I think the paper is executed cleanly and the it's well-written. I also think the work, when completed, has potential to be very useful. Areas for improvement: In my view, the paper has shortcomings in it's design, development and scope. A paper like this is helpful when it can: (i) Establish good practices across the board by streamlining workflow, and ensure the interface is used the same way when comparing methods.(ii) Contribute code that make it easy and fast to use and develop on.(iii)  Make it easy to collect and report relevant performance statistics the same way across algorithms, helping the field by making it easy to benchmark.(iv) Includes challenges that are diverse but relevant to the use case of the algorithm. (v) Synthesize a suite of methods from the literature that are distinct from each other and of interest to the community as strong benchmarks. I don't think the paper is addressing these aspects sufficiently. Some detailed comments below. (1) The categorical choice of benchmark tasks is not clearly justified (e.g. Why should anyone care if a protein design algorithm is poor at designing a robot controller?) At the end of the day, these problems share little structure, and "no free lunch" arguments (Wolpert 95) would suggest that there is no one good algorithm for every challenge. The real world settings for these problems don't map well to each other. This paper as designed, could result in follow up work with meaningless comparisons between algorithms that have no business being compared (unless there is a meaningful connection between the challenges). If the authors feel like they can justify this particular set of challenges, I'm open to be convinced. E.g. if the argument is that there will be a "master algorithm" that is just good at designing everything, and this benchmarking set is designed to enable that, I could drop this point, but then other issues will be relevant. (2) The particular choices within each class of tasks is insufficiently justified. Why is GFP a good design challenge given that the ground truth is also necessarily a trained oracle with likely poor performance outside the training data? Just because some previous papers chose this as the design task doesn't make it a good benchmark. No statistics is provided for how good the GP model for GFP is.  As for other proteins, GB1 for instance is far more completely surveyed than GFP.  There are also better published models of GFP available (e.g. TAPE, UniRep). All of these would of course struggle with out-of-domain samples. Why not use a physical simulator like Rosetta that wouldn't have this issue? I believe when designing a benchmarking suite, these decisions should be considered more carefully, as it would quickly become the test bed for follow up work and a flawed design choice here amplifies in future work. (3) A body of literature for algorithms that can readily perform MBO has been neglected.  It is trivial to run a regular optimization algorithm with the model (instead of ground truth) and compare the proposed solutions to ground truth.  Quality-Diversity/EDA algorithms (e.g. genetic algorithms,  simulated annealing,  CMA-ES, or even pure CEM (rather than DbAS)...) consistently perform "well" in these high-dimensional optimization settings.  The success of the gradient-based method gives more reason to believe representatives of each of these classical approaches should be included and suggests that the claim that climbing proxy model will necessarily result in bad "ground truth" outcomes is a weak one. (4) For a benchmarking library like this, there needs to be mature code available for review (not submitted). I've checked the provided website multiple times, and while it is under active development, the code is not accessible. From what I gather the current code interface simply gives access to some data points and a ground truth. This is too little API. A good benchmarking tool would let the user abstract away the modeling part easily, and be able to readily port and run their algorithm against benchmarks, producing the results in the same way. It should also take care of running sanity checks/tests for the user and generating the same plots as those in the paper. (5) The fact that gradient-based methods outperform other methods presented here is only surprising in the sense that they were not included in the original papers (i.e. why weren't gradient-based methods benchmarked there? not this paper's fault of course).  The authors express  a general conviction where gradient based methods have done very poorly in other attempts for MBO, but provide no references, it would be great to cite relevant references for this claim.  (6) As is, the paper/library only compares CbAS/DbAS with MINs and hill-climbing methods. I think it is not sufficient breadth of methods to make a "benchmarking" suite. As far as I can tell, CbAS/DbAS and MINs are not the best published algorithms in any of the domains suggested, so the authors should justify why they are the algorithms to benchmark against?For instance, Angermueller et al 2020 ICLR, have an offline RL algorithm that can in principle solve all of these problems. In fact DynaPPO, PPO, and Ensemble-based Bayesian optimization all outperform CbAS in that study. Some sequence design challenges used there seem to be better benchmarks than GFP.  There is substantive work in molecular design on MBO,  but none of the SOTA algorithms are included (e.g the now-classic Gomez-Bombarelli 2016, or  perhaps adaptation of Zhou et al 2019 Scientific Reports). A good rule of thumb in my view is to include the SOTA or well-established algorithm for each task category. (6) I suggest the authors think carefully about what the evaluation criteria are. While optimization itself is a good metric, other factors, such as providing a way of evaluating diversity of solutions, or sensitivity to dataset size (e.g. by subsampling), are good to consider.==========I would like to encourage the authors to continue the pursuit of this work because it is relevant and well-motivated, and has great potential, in my view it is simply not ready. I think this work needs to be reviewed again when it is more mature,  with wider range of algorithms, better justified challenges, a larger set of metrics that can be easily collected, and available code such that the reviewer can vet the benchmarks and code properly. Right now, it's a comparative review of a small set of methods, not a good benchmarking suite.If done well, it can be a very useful suite that can help researchers develop better algorithms. The danger of accepting it prematurely is that it will be a basis for future work that "game-ify" studies of algorithms against irrelevant/misleading set of benchmarks. That is only damaging to the development of good algorithms and could misguide research. As it stands,  I find the latter risk higher than it's contribution, and hence I believe it should be rejected and reviewed once more of these structural issues are addressed (and code is available to review). ### SummaryThe paper presents a new approach for building group-equivariant neural networks. The authors propose *L-conv*, a layer which is equivariant to transformations from a group $G$ in the neighborhood of identity. They show that a network with a sufficient number of such layers is $G$-equivariant as a whole. Additionally, the authors propose to learn the structure of the group directly from the training data instead of incorporating it in the network *a priori*.### Strengths:1. The paper is interesting and is easy to follow. The problem the authors are solving is clearly stated from the very beginning and is described in the title. 2. Proposition 1 is insightful. Equations 4 and 5 contain a novel parametrization of convolutional kernels. It also allows considering standard convolution from a different perspective.3. Proposition 2 reveals a useful connection between the  Lie algebra-Lie group correspondence and a network of *L-conv* layers.### Weaknesses:1. **Experiments**.  The conducted experiments do not demonstrate the advantage of the proposed models over other models that use the power of data symmetry. The advantage of the proposed models over conventional CNN is not properly demonstrated.    * The decision of using very shallow networks is not clear. An experiment with a 1-layer CNN is not directly generalizable to deeper networks.     * Paragraph **Learning $L_i$ during Training** does not contain enough information for a clear understanding of the chosen models. A diagram of the chosen models or a table could make it more clear.    * Figure 2, MNIST. The *L-conv* model contains 2 times more parameters than the standard CNN. It demonstrates $\approx 95$ \% while the standard CNN demonstrates $\approx 93$ \%. It is not clear whether the improvement is caused by the increased number of trainable parameters or by the proposed layer. The same argument is applicable to the CIFAR10/100 and the FashionMNIST experiments.    * There are no demonstrations of the learned generators $L$. A demonstration of it will make it possible to understand whether the learned structure matches the known structure of the data. For example the rotation operator for Rotation MNIST.     * It is worth understanding whether a network of *L-conv* layers converges to a group equivariant network or proposition 2 is purely theoretical. 2. Paragraph **Notation** and the proposed notation itself do not seem useful.     * The Einstein summation rule is used only in Equation 4 which can be easily rewritten with $\sum$. The rewritten version will be more readable.     * Equation 2 is not used in the paper at all.      * Some letters are used multiple times while meaning different things. For example $f$ is a structure constraint in Equation 2, it is a neural network in paragraph 1 of Section 2, and it is a point-wise activation function later in the same Section.### DecisionTo sum up, the paper proposes an interesting method for building group equivariant neural networks which allows for estimating symmetries from the data instead of incorporating it in the network *a priori*. However, the experimental results are not sufficient for proving the advantage of the proposed approach. The theoretical contribution of the paper is not sufficient for considering it as pure-theoretical. The paper seems raw for acceptance at the current stage. **Summarize what the paper claims to contribute.**The authors claims to show that disentanglement into subspaces by a continuous encoder is impossible for any finite group acting on Euclidean spaceThe authors claim to introduce an alternative definition of disentanglement that is more flexible and leads to a **Strengths:**The authors consider the problem of disentangled representation learning which is of considerable interest to the communityThe authors approach the problem by imposing structure through their disentangled operators**Weaknesses:**The reliance of the impossibility of disentanglement proof seems to rely heavily on the example of the perturbed triangle. The example and its assumptions seem fairly rigid and unnatural and I am unconvinced this captures the reality of disentangled representation learning with auto-encoding networks.The approach of adding structure by means of a transformation operator was also used in [1,2] which are cited but not compared against. Instead the authors compare against various VAEs which do not impose any external structure which does not seem particularly appropriate.If I understand correctly, the paper seems to be based on a mischaracterization of the arguments in [3]**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.**Reject. See weaknesses**Supporting arguments for your recommendation.**While the authors tackle an interesting problem and propose an interesting solution, the arguments on which the paper is based seem flawed.**Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.**As I understand, the argument is against the utility of the *linear* disentangled representation in [3]. The more flexible definition the authors propose seems quite close to *disentangled representation* in [3], please clarify the difference.Moreover, it seems the authors suggest the definition of disentangled representations proposed in [3] requires that subspaces corresponding to factors of variation are single dimensional (section 2) which is not the case, please clarify.How does the approach compare against other methods, namely [1,2,4] that use structure to encourage disentangling of the representation?Section 2 asserts that the VAE and its variants do not learn disentangled representations and uses PCA to show this is true. I expect that if this same analysis were used in the structured case, a similar result would be found, in particular, since the rotation matrix interacts with multiple dimensions of the latent code. Perhaps my intuition is incorrect, please clarify.**Provide additional feedback with the aim to improve the paper.**Perhaps a rewording could clarify: (Supervised Disentanglement) is composed of a 2x2 diagonal block...   is a block diagonal matrix with a 2x2 rotation matrix in the upper left block and 1s on the remaining diagonals(just after 11) The authors state that most deep networks are differentiable, my understanding is that the common ReLU networks are not differentiable but subdifferentiable**Possible typos:**(VAE, beta-VAE and CCI-VAE) the 4s  the ``4s(Dfn of a group; identity element) g_k e_G = e_G g_k = e_G  g_k e_G = e_G g_k = g_k[1] Falorsi, Luca, et al. "Explorations in homeomorphic variational auto-encoding." arXiv preprint arXiv:1807.04689 (2018).[2] Connor, Marissa, and Christopher Rozell. "Representing Closed Transformation Paths in Encoded Network Latent Space." AAAI. 2020.[3] Higgins, Irina, et al. "Towards a definition of disentangled representations." arXiv preprint arXiv:1812.02230 (2018).[4] Cohen, Taco, and Max Welling. "Learning the irreducible representations of commutative lie groups." International Conference on Machine Learning. 2014. The authors study how to improve the prediction and pruning performance with additional information generated by labels in the shared-label classification problem. As a starting point, the authors consider a simple scenario where side information can be extracted from the same labeled batch. To train the neural network, the authors use a balanced loss consisting of a weighted sum of general cross-entropy and cross-entropy of average batch prediction. The authors also suggest a new CNN-LSTM architecture to improve predictive performance to exploit the side information. The experiments section shows the proposed method performs well and achieves a high compression rate. The data model used in this study is different from the common classification problem. This paper assumes that n-data points give side information with the shared-label batch, referred to as "n-tuple." In general, classification problems have labels independent of other data points. [Strength] The authors study a general relationship between pruning and additional side information for the shared label problem. The authors define "relative information" to measure the appropriate compression rate for various prediction performance and pruning levels. Using the relative information, we can dramatically reduce the number of parameters while maintaining prediction performance. Besides, the proposed CNN-LSTM architecture improves the prediction performance with the shared-label training scheme.[Weakness]This paper considers a very different data model which never been studied before. The shared-label model should be motivated very well. I'm not convinced why we have to study this model.The explanation of which benefits the network from the side information is ambiguous.  There is no theoretical and empirical explanation of how the side information and balanced loss can discriminate ineffective parameters. It requires providing more clear evidence, such as the statistics of network parameters before and after the pruning algorithm.The existence of optimal compression rate \rho^* should be discussed more rigorously if possible with a theoretical proof based on relative information.  In the experiment section, only the 5 Conv net was investigated to check the effectiveness of pruning. It would be more convincing if the authors can add results for the proposed CNN-LSTM network.[Minor Issue]It is challenging to read Fig1-9 because of their interpretation. I suggest the authors use other colors with a bigger font size.It would be better to use tables rather than graphs to present many experimental results. Graphs are too small to understand  The paper proposes replacing each linear layer with a linear ensemble (average) of m (m>1) linear layers.  Authors argue that this helps to reduce network internal variance and achieve better accuracy.I think it is possible to see that the proposed model is equivalent to the baseline model with a different hyper parameter setting (specifically the random initialization and learning rate), and therefore the observed variance reduction and improved performance are a result of better hyper-parameter choice in the proposed model.  The equivalence between the baseline model (with single linear transformation in every layer) and proposed model (m linear transforms in every layer whose output is averaged) can be seen by observing that:Baseline:  y = w.xProposed: y = (1/m) sum_j w_j x = (sum_j (1/m)w_j) xWe see the following relation between initial weights of baseline and proposed model: w = \sum_j (1/m)w_j.   Furthermore, the gradients of objective w.r.t. w and w_j also follow a linear relationship: grad_{w_j} = (1/m)*grad_w, and due to this gradient scaling, the effective learning rate of the proposed model is smaller by a factor (1/m) as compared to the learning rate of the baseline model. Gradient of objective w.r.t. layer input x is same in the two models as long as the gradients w.r.t. output y are same and weights follow the relationship w = \sum_j (1/m)w_j.Thus I would think that if the baseline model follows hyper-parameter settings that are equivalent to the proposed model  i.e. weight initialization such that w = \sum_j w_j (leading to reduced variance of initial weights), and a learning rate that is smaller by factor (1/m)  then it should see variances and performance that is akin to that of the proposed model.I'd like to hear author's response and discuss if my reasoning is flawed. SUMMARYThe paper is heavily based on [Zhang et al., 2020], where the approach of building a "truly non-local gradient" is brought forward. That idea appears in many variants in the literature. As far as I can see, it simply amounts to estimating the gradient from samples in a deliberately chosen distance from the current reference solution. The scientific and practical value of the baseline method [Zhang et al., 2020] is all but clear. The current paper improves upon that baseline by getting rid of two tuning parameters, which is a nice contribution.In other words: The contribution of the paper in itself is valuable, provided that the baseline method is. I find the second part near-impossible to judge from the provided material. Why should a bunch of 1D estimators be vastly superior to a vector estimator? Looking for an answer I checked [Zhang et al., 2020]. However, there is no unbiased comparison with competitors, since all results are subject to extensive problem-specific hyperparameter tuning.THE PROPOSED METHODSection 2 provides a motivation of the method of [Zhang et al., 2020]. There is not much detail, in particular no pseudo code, and the various tuning parameters are missing.The proposed method is based on line search. The search itself replace the learning rate parameter, which is then used with with an exponentially fading record technique to adapt the sampling radius.When setting the algorithm parameters, it is assumed that there is a bounded domain inside of which optimization is performed. However, at least conceptually, all test functions are defined on unbounded domains (real vector spaces). In my opinion, a solid optimization algorithm should not depend (critically) on the presence of bounds. Gradient descent does not, so why should a line search method?Also, the default target precision is quite bad: L_{min} = L_{max} / 200, or 0.5% of the diameter of the space. In a 1000D unit hypercube that's roughly 0.158. I don't think that's a satisfactory precision. An evolution strategy will happily identify the optimum to arbitrary precision, and close to numerical precision in practice. Instead of limiting the algorithm in such a way it should feature a truly adaptive mechanism.I simply cannot believe the condition $|F(x_t) - F(x_{t-1})| / |F(x_{t-1})| < \gamma$. What prevents us from dividing by zero? The condition seems to assume that the optimal value is zero. That's a no-go.Under "reset the smoothing radius" it is claimed that the method "converges" in less than 10 optimization steps. How comes? Did I overlook a second order method? No first order method ever converges that fast, unless the problem is completely trivial. Anyway, the very need for a reset mechanism and the absence of a detection/trigger mechanism is a bad sign.Random generation of the rotation matrix: It should be said that sampling a random orthogonal matrix from the uniform distribution is a cubic time procedure, which is surely undesirable. On the other hand, generating a fresh matrix from time to time is obviously a good idea.AN IMPORTANT (SIDE) NOTE ON EVOLUTION STRATEGIESThe paper includes many weak and even clearly wrong statements about ES. I see lots of misconceptions, not only in this paper. Somehow, when it comes to ES, the ML community seems to live in a parallel universe; it really started to invent its own reality. ES research does not (primarily) take place at ML conferences, but at GECCO, FOGA, PPSN and a few others. However, nobody in ML seems to read any further than Salimans plus a few (good) papers published in JMLR. When talking about ES, ignoring these conferences simply does not work. Nearly none of the algorithms discussed in ML actually qualify as ES, according to the very definition of the method. More often than not, there is no ranking (and selection) of solutions, and no step size control, which was at the core of ES since their inception nearly 50 years ago. The NES family of algorithm was exploring an interesting connection to the gradient-based world. However, concluding that ES are gradient estimators is just wrong.Here is a concrete point how this attitude affects the current paper. Appendix B.2 states verbatim: "ASEBO refers to Adaptive ES-Active Subspaces for Blackbox Optimization proposed in (Choromanski et al., 2019). This is the state-of-the-art method in the family of ES."Wow...what? I must have lived behind the moon, I missed the new SOTA. That statement is shared by exactly no single lead figure in evolution strategies research. Maybe it was made up by the authors, or claimed by the cited paper or github repository -- I don't know, and it is anyway useless to track down its origin. In my surprise, I checked the ASEBO repository and the paper cited therein. There the ASEBO algorithm was found to be superior to several variants of CMA-ES. Knowing the compared algorithms well, none of the experimental results on analytic benchmarks looked even remotely plausible to me. I quickly fired up a control experiment on the 1000-dimensional sphere, where ASEBO was reported to reduce the function value by about a factor of two in 20K function evaluations. With identical setup, the three ES variants I tested reduce it by a factor between 30 and 100, with zero tuning, including an 11-lines (1+1)-ES. My test also included LM-MA-ES, which was included in that benchmark, where it did not do anything.Is it just me? This reminds me of "alternative facts". It is no wonder that ML research is chasing its own tail.Citing again from the paper: "A exemplary type of these methods is Evolution Strategy (ES) based on the traditional GS, first introduced by (Salimans et al., 2017)."The term "evolution strategy" goes back to Ingo Rechenberg's PhD thesis, published in the 1970es. The state-of-the-art is marked by CMA-ES and its variants. For a very brief history check wikipedia: https://en.wikipedia.org/wiki/Evolution_strategy). NES plays only a very minor side role in the development.In 2017, Salimans was standing on the shoulders of giants. There exists a vast (and in parts old) literature on evolution strategies, including very relevant but rarely cited work on applications in RL. The OpenAI paper simply leverages that literature, in poor a way, and without citing it properly (maybe that's where the problem started). The algorithm described therein has the same extremely slow convergence speed as pure random search (due to a lack of step size control), unless it is combined with ADAM, which is apparently done in the experiments but not even stated.A different point -- the paperOllivier, Yann, et al. "Information-geometric optimization algorithms: A unifying picture via invariance principles." The Journal of Machine Learning Research 18.1 (2017): 564-628.describes the gradient flow of proper ES, which are based on ranking of solutions. This flow is defined on the statistical manifold of search distributions, not directly on the search space. There is really a lot more behind evolution strategies than what was put forward in the Salimans paper. In particular I argue that ES should not be listed under the heading "Zeroth order methods based on local gradient surrogate". The gradients estimated by ES have never been local.A final point, highlighting the often shallow understanding of methods like CMA-ES: The covariance matrix has a quadratic number of coefficients in the problem dimension. For successful adaptation, it requires roughly a quadratic number of data samples -- that's how its learning rates are tuned. Anyway, they are not well tuned for problem dimensions much larger than 100. When running an experiment on a 10^3-dimensional problem this means that we need a function evaluation budget in the order of 10^6 for the technique to have a meaningful effect. When running for 20K evaluations, CMA does not make sense. Just drop the CMA technique and save 99% of the computation time. Or better, use a low-rank approach like LM-MA-ES. The method is even cited [Loshchilov et al., 2019], but then ignored.EXPERIMENTSIt is quite clear that the experiments are not designed to answer any specific research question. Instead, there is a battery of generic performance tests, with the single focus of being best in class. That's unreasonable. For example, it cannot be expected that the new method outperforms the same method with optimal learning rate schedule. It may still be considered superior if it comes close, removes two tuning parameters, and the adaptation mechanism is robust.I appreciate the code supplement. However, it does not allow me to reproduce the experiments since it does not include the competitor methods. Indeed, I quickly implemented a CMA-ES control experiment straight into the provided code, using the ellipsoid benchmark. The performance curve I get looks VERY different from what is reported. It is initially MUCH faster and then slows down when it needs to adapt the covariance matrix.I also ran a few control experiments, with my own code. I picked the Rosenbrock function as an example. Critical information is missing in the paper, like the initial step size. No matter what I do, I fail to reproduce the plots, even qualitatively. The only way to make CMA-ES stall completely for 5000 evaluations is to start with a too small step size.Line search is not a new technique, and it was applied to ES for step size control. The resulting "Two-point adaptation" technique is the maybe second-best step size adaptation rule for CMA-ES. Here is a link to the original technique, which was refined over the years:https://arxiv.org/abs/0805.0231With the small function evaluation budget (compared to the problem dimension) used in all experiments, results are dominated by initialization and step size control. Therefore please compare to this method.This paper claims to provide a well working algorithm, however, it provides very few experiments. The ES community has an established standard benchmark suite: https://github.com/numbbo/coco. It includes a "high-dimensional" problems track, and it delivers far more insights into strengths and limitations of algorithms than the presented material.A simple but very important test problem is missing: the sphere function. I would really like to see the performance on this problem, since it is directly related to the quality of the gradient estimator. With a perfect gradient, an exact line search solves the problem in a single step. However, does that happen? If not, what is the limiting factor -- the gradient or the line search? The sphere would be a good opportunity for a deeper investigation and real insights.The most important experiments are missing! The contribution of the paper is to remove tuning parameters from Zhang's method. Therefore I'd like to see the effect of the new automatic choice, with manually tuned (optimal) and naive parameter choices as baselines. Does the new mechanism work close to optimal or not? That's the core question that needs to be answered.The way the numerical results are presented is highly problematic. On smooth unimodal problems, any well-designed ES based on ranking of solutions exhibits convergence at a linear rate. The better the rate, the faster does the algorithm converge asymptotically. This rate is the slope of the graph of $\log(f(x_t) - f(x^*))$ as a function of $t$ (time). Therefore, in all serious ES research, performance plots ALWAYS use a log-scale for the vertical axis. I would go so far to say that the presented plots are close to meaningless.Table 1: Please plot progress curves instead of an arbitrary single point in time (1500 function evaluations).Section 4.3 reads as if super mario level generation would be a widely used benchmark. Then why does the (actually very nice) paper by Volz et al. have only 11 citations on Google scholar?Also, citing a youtube video for the A-star Mario agent is unhelpful. I'd like to see the method or the code or both, but not a demo. This is supposed to be research, not entertainment. Also, that agent is a hopelessly over-optimistic definition of a "playable" level.There is problem-specific hyperparameter tuning for this task, and only a single baseline method tested, for a shady reason. Please provide a complete comparison to all baselines and avoid problem-specific tuning. This is black-box optimization. If you tune then it counts towards your function evaluation budget.Appendix A, description of the Rosenbrock problem: "The ridge changes its orientation d - 1 times."The function is a 4th order polynomial. The ridge is parabola-shaped. There are no discrete changes whatsoever. Anyway, the function evaluation budget is so low that no algorithm even gets into the mode of really following the ridge, and approaching the optimum is completely impossible. In my opinion, even using the benchmark in this regime is misleading.CONCLUSIONThe proposed method may or may not be of value, I cannot really tell. The most relevant comparison experiments are missing. Furthermore, frankly, after my own experimentation, I do not have trust in the results. Also, I need to see BBOB/COCO benchmark results, including baselines.And finally, please correct various statements on ES.RECOMMENDATIONThe paper has many weaknesses. Most prominently, the experimental evaluation is problematic at least, and the most important experiments are missing. Therefore I recommend to reject the paper. Summary: A model that appears to very effectively predict SpO2 from respiration signals. I find the model and experiments well-designed, but have serious concerns about the motivation and prediction task itself.Objective: Predict blood oxygen sequence information from respiration sequence information, leverage auxiliary variables and demonstrate feasibility of RF-based contactless oxygen prediction. I view the first as the primary goal, bolstered by the auxiliary variables, which enables the RF-based prediction.Strengths:Overall, I think the model design and evaluation is very good! I like the paper a lot overall and think the authors did a good job addressing most methodological and experimental concerns.* Impressive, accurate predictions. * Experiments are well-designed. In particular, analysis w.r.t. race is well-motivated and well conducted. * RF experiment demonstrates both RF feasibility and ability of the model to generalize across respiration measurement devices/methodologies.Weaknesses:Major weaknesses (only 1):My most important concern is the choice of problem and prediction task. There is a well-discussed medical distinction between ventilation and respiration -- simply moving air vs actually adding oxygen to the blood stream. There is a good reason why respiratory rate and pulse oximetry are measured separately: resp. rate measures ventilation while oximetry measures respiration. Many conditions, i.e. pulmonary edema or embolism, affect the lungs' ability to exchange oxygen without affecting the ability to move air, and many conditions, i.e. traumatic or neurological, affect the ability to move air without affecting the ability to exchange oxygen in the lungs. Because of this, I see SpO2 as a variable that contains *independent* information not in the ventilation signal, and believe it is dangerous to display a patient SpO2 signal that is entirely *dependent* on ventilation - containing no respiration signal. Such a signal could look plausible enough but fail in the most important clinical cases.I understand there is low error on the tasks shown, but how does the model work in cases where we'd expect respiratory problems but no ventilation problems -- or vice versa? Beyond sleep studies or ambulatory settings, is it likely to work in cases of pulmonary embolism or ARDS in an acutely ill COVID-19 patient? Some of the problems discussed in the paper (e.g., sleep-time drops in oxygen saturation) seem like they could plausibly be predicted from ventilation signal, but for these cases, why not predict a binary target instead of the full SpO2 signal? The only reason for predicting the SpO2 signal would seem to be if you think it's a true inference of the physiology that will hold true even in cases you didn't examine in the paper -- and I think there are a lot of prior physiological reasons to believe that's not the case. My prior is that there are at least 2 causal paths where you can predict SpO2 from ventilation signal: (1) hypoventilation-->hypoxemia and (2) hypoxemia-->hyperventilation. I'm concerned that the model may be able to pick up SpO2 signals that follow these patterns (and are accurate in the datasets used) without learning a general, "true" relationship between breathing and blood oxygen. This is important because the paper is primarily clinical: the value is the ability to predict a new clinical outcome. Thus, I think the paper should only be accepted if it gives a good idea of in what cases the model would be clinically useful at predicting SpO2, and I don't see much of this analysis in the paper. If these issues were discussed at all (and ideally in detail), I'd be more open to seeing clinical value in the work. Minor points:* Methodologically, it seems the auxiliary variable strategy works well but I'm not convinced it's the only way or the best way to solve the problem -- both multitask and multi-headed seem good at capturing the shape of the signal, with multitask often off by a constant. This is what we'd expect from an MSE model trained on a large dataset -- the overall signal shape will be pulled towards the mean. Gating into separate models for groups with different baselines would reduce this problem (because there will be less variance within each group). This is not a huge issue (and I know L1 loss is used here rather than MSE), but there are other ways to handle such problems -- for example using shape and time distortion losses like DILATE.* I did not find the COVID analysis particularly compelling because there's only a single patient and mostly qualitative analysis -- it's hard to draw any clear conclusions form the example about the method's value overall in such cases. I think I'd prefer a more rigorous evaluation on a non-COVID task to something that's a bit speculative but COVID related.* Some writing could be smoother and more terse in the Method section, i.e. " In this paragraph, we answerthe critical question..." ##########################################################################Summary:The paper considers the reconstruction of the last layer for NLP data processing models. This problem is equivalent to the parameter estimation for logistic regression in the first of the paper and quite close to it in the second part when we purposely change the encoder via transfer learning.No surprise, that the reconstruction in this setting works well. This is what we already know from linear algebra and Gauss-Markov [1], Bernstein-von-Mises like theorems in statistics [2, chapter 10].More interesting is the part about what is happening, when we deal with reconstruction under a transfer learning setting. In this case, we observe a predictable degradation of the quality of the models, but nothing more specific##########################################################################Reasons for score: I vote for rejection, as this paper doesn't contribute to our understanding of what is happening in real-world NLP models with many layers, rather focusing on the last layer fine-tuning.##########################################################################More detailed review:################Theoretical resultsAll proposition in the paper are obvious and also equivalent to the recovery procedure for the coefficients of a multiclass logistic regression:1. Proposition 1 is obvious2. Proposition 2 is obvious3. Proposition 3 is obvious4. Proposition 3 is obviousThe general statement that concludes this section and leads to further experiments should be compared to theoretical results for softmax (or multinominal) regression, see e.g. [3] for some details on the quality of the estimates in this setting. Also, see similar results for logistic regression in [4]. Both these papers present result on the quality of parameters' estimates in a more advanced subsampling setting, and even in this case, they provide the speed of converges for the error of parameter estimates. So for the benefit of the quality of the paper, I suggest dropping all theoretical results as they are not new.################Practical results1. Due to the reasons similar to that mentioned above the experiments for $\eta = 0$ can be dropped to avoid confusion from the reader2. For the setting with the fine-tuning of the models, we can see from experiments that after learning emerges a disagreement between the parameters estimates via the proposed procedure and the initial values of parameters. In particular, how can we measure the distance between two models even if they are one-layer logistic regression models, and can we do something if there is one layer in a setting closer to the white box problem. [1] Henderson, C. R. (1975). Best linear unbiased estimation and prediction under a selection model. Biometrics, 423-447.[2] Van der Vaart, A. W. (2000). Asymptotic statistics (Vol. 3). Cambridge university press.[3] Yao, Y., & Wang, H. (2019). Optimal subsampling for softmax regression. Statistical Papers, 60(2), 235-249.[4] Wang H, Zhu R, Ma P (2018b) Optimal subsampling for large sample logistic regression. J Am Stat Assoc 113(522):829844 The draft proposes to bound the model generalization by controlling the adversarial perturbation of the model weights. If the weights in the neural network are bounded and the activation function is Lipschitz, the change of output of the network, as well as the Rademacher complexity of the hypothesis class, can be easily controlled. Connecting perturbation with the generalization is not something new no matter in theory or in practice. This is another draft formulating the network perturbation and generalization so that a norm product bound is derived. There are plenty of previous works on this already, e.g., the work by Neyshabur et. al. (perturb the parameters), and Bartlett et. al. (norm product bound). The generalization bound proposed in this work is a trivial application of Bartletts norm product bound. Perturbing the weights and directly applying norm product leads to a bound not as tight, to some extent, it is mostly vacuous. Frankly, the method gives a pessimistic norm product bound and ignores all the effects caused by the alignment between the internal coefficient matrix and the input vectors, which is crucial in terms of understanding how input signals are handled throughout the network. I would encourage the authors to read some recent work by Barron et.al. which reduces the generalization bound from norm product to product norm.  ** OverviewThis paper is generally well written and well organized. It is easy to read. It strives to disentangle the two possible effects of batchnorm in neural network in order to study its main contribution to network regularization. The proclaimed two possible effects are1. standardizing the intermediate activations2. regularizing against explosive growth in the final layer.By dropping the batchnorm layer and replace it with one of the two penalty functions --  penalty on normalization or penalty on the l2 norm of the final layer -- the authors claimed that much of the performance gain from batchnorm is recovered from the norm regularization of the final layer. Although the paper is interesting, I do not find the results to be very convincing (please see "questions" section for clarification)** Pros1 Well written and well organized.2 Experiments are conducted to explain the disentangled effect of batchnorm in regularization.** Cons and Questions.1. Even though the batch norm layer has the effect of standardizing intermediate layers and limiting the size of the final layers, it is not necessarily true that their effects can be replicated by simply putting a penalty on the network loss during training. In particular, In table 1 and table 2, the authors use very small penalty coefficient lambda to replicate the "standardizing effect", and claimed that "we found that higher coefficients led to divergence at high learning rate." This leads me to think that the results from directly penalizing the effects of batchnorm are unconvincing. 2. One of the benefit of batchnorm is that it allows for a significantly larger learning rate. After dropping the batchnorm and putting the penalty on, does such benefit still exist? I would very much prefer to see that the maximal "allowable" learning rate does not decrease after dropping the batchnorm layer.3. The authors claimed a connection between dropout and penalization of the output feature in equation (5). However, this plausible connection is only valid when features are decorrelated -- which seems to be a very strong assumption.4. The authors also make a difference between "feature embedding L2" and "Functional L2", whose difference seems unnecessary when weight decay penalty is used because of equation (5). Is that correct?5. If I understand correctly, it is the explosion of gradient instead of the output feature that makes training method hard to converge. Can the authors elaborate on the merit of limiting the feature size? After all, even if the feature is large, a rescaled weight matrix in the final layer can easily bring everything back to normal size. ##########################################################################Summary:- This work proposed a framework to optimize and personalize deal-based promotion strategy. The proposed framework is composed of a Temporal Convolutional Network to predict purchase probability, a function to approximate price elasticity, a constraint based utility function to optimize promotion strategy. The proposed method is tested on a public dataset.##########################################################################Strength:- This work is trying to address a real-world problem and has posed a reasonable framing- Indeed all three components of this framework are important research areas and the overall problem is also a critical application scenario- The overall presentation is easy to follow##########################################################################Weakness:- Limited technical novelty of the proposed methods- The overall execution needs significant improvements- A lot of related work is missing##########################################################################Detailed Comments:Despite the importance of the problem, unfortunately the overall technical novelty and execution of this work does not reach the standard of an ICLR research paper - therefore I vote for a clear *rejection*.- From the methodology perspective, as mentioned in the previous comments, all three components in this problem framing are important research areas - while the proposed framework in this paper is more like a combination of three existing methods. In this regard, I didn't find significant technical contributions out of this.- Following this, a lot of previous work in the abovementioned three areas, especially the first two areas need to be acknowledged in the related work. To name a few (some work I'm aware of but not limited to) - a) Modeling purchase probability with price sensitivity [1] "Modeling Consumer Preferences and Price Sensitivities from Large-Scale Grocery Shopping Transaction Logs", WWW'17 [2] "Shopper: A probabilistic model of consumer choice with substitutes and complements", Annals of Applied Statistics (2017) [3] "Price-aware Recommendation with Graph Convolutional Networks", ICDE'20and potentially many papers in the recommender system area leveraging the Temporal Convolutional Network - b) Causal inference for price elasitisty (unbiased characterization between demand and price) [4] "Estimation and Inference about Heterogeneous Treatment Effects in High-Dimensional Dynamic Panels" Semenova, Goldman, Chernozhukov, Taddy (2017)Additional work from economics and marketing community can also be considered.- Unfortunately the execution of this work needs many further improvements. The most critical problem is the lack of rigorous, quantitative evaluation of the proposed method. Current experiment execution is more like a data analysis on a public dataset - without evaluating against baselines (for example, some of the previous mentioned papers), it is very difficult to justify the effectiveness of the proposed method.Overall I'd suggest the authors carefully conduct literature reviews in this domain, improve the execution and evaluation, and possibly consider an application-driven venue (e.g. an ecommerce workshop etc.) for this work.##########################################################################Typos:- Period is missing at the end of Related work- Left quotation marks are not correctly rendered (e.g. in Section 3.1, consider using ` in latex instead of ') Summary: The authors attempt to investigate to what extent languages are hard to conditionally language-model. They do this by using some information theoretic measures. Claims:- There are no statistically significant differences between source language representations, but there are significant difference between pairs of target language representations.- There is no complexity that intrinsic to a language except its statistical properties concerning sequence length and vocabulary (unless word-based methods are used).- They also observe phenomena such as Double Descent and erraticity.----Strengths:- The Experiments are extensive.- The relative similarity of source language representations is interesting and worth exploring further.Weaknesses: - The diagrams are difficult to read- The paper is hard to follow and would benefit from a clearer focus rather than the broad range of topics covered here. For example:    - It is difficult to understand what the methods/terms (the information theoretic measure used, double descent) are - little time is spent explaining these.    - Double descent is discussed in the paper but it is still made not clear why this is relevant in the paper.    - Several portions of text are repeated - with some editing, space can be made to discuss concepts important to the paper- The authors make recommendations for modeling (Eg. using char level or byte level models for certain models - which have been extensively studied for this): this is not followed up with any concrete results on translation/downstream tasks or pointing out relevant work. Summary:The paper proposes to use LSTM to learn partial differential equations. Experiments include Wave equation, Heat equation, Burgers' equation, and Navier-Stokes equation. They compare with PINN on Allen-Cahn and Burgers equation.The paper is nicely written and easy to understand. But I have several concerns:Novelty:Using the RNN/LSTM type of networks for time series / time-dependent PDEs doesn't seem to be novel. Convolution-LSTM has been widely used in these tasks e.g. (https://arxiv.org/abs/2002.03014). Instead of using Convolution, the author uses the seq-to-seq structure. Unfortunately, there is no sufficient justification or any empirical comparison. Indeed, the seq-to-seq loses the location information compared to convolution-LSTM. I doubt if it can outperform the convolution. Experiments:The work compares again PINN. In my opinion, it is not a fair comparison. If I understand correctly, the Neural-PDE proposed in this paper uses the ground truth data $u$ for training, but PINN doesn't have the access to these ground truth data. To have a fair comparison, I suggest comparing with LSTM type solvers, for example, Conv-LSTM, Unet-RNN, TF-net (https://arxiv.org/abs/1911.08655)Error Metrics:The paper uses MSE as the loss metric. It's no problem to train with MSE, but when reporting the error rate, it's better to use the relative-L2 error, or at least some relative error. The absolute MSE error looks very encouraging, but it has no physical meaning, in my opinion.Therefore, I am sorry to suggest rejection. This paper proposes an algorithm to learn dynamics on graphs: node scores are computed by a GNN and then used to predict the modification of the graph (node/edge insertion/deletion). The training of the GNN is unclear to me. In particular, the authors do not describe how they obtain the 'teaching signal'. Theorem 1 is of little help as 'graph mapping' are not defined.I have a fundamental problem with this paper: it looks like the GNN is trained to mimic the teaching signal but as claimed by the authors on page 5 '...the key theoretical result of our paper yields a simple way to construct teaching signals...'. Hence what is the point of using a GNN if a simple algorithm gives a satisfactory answer?I do not agree with the authors about the expressive power of GEN: for simplicity, consider a cylce graph, then for a GNN as the one described in (2), the node representation will all be the same. Hence the node scores will be the same for all nodes and the algorithm 1 will have the same output for all nodes and edges. What am I missing? This paper does achieve good performance but its method is quite about engineering using very intuitive training tricks that everybody could be able to use given a lot of GPU machines. I would not like to encourage such work to be published as a research paper.Pros:1. The proposed framework achieves a good performance compared to its related works.2. It is a good organization of a lot of training techniques, and a good reference for engineering.Cons:1. No technique contribution. The main framework of this submission is very similar to the existing work [Scalable Transfer Learning with Expert Models] which has not been officially published but only on arXiv. Besides the common methods of pre-training and ensembling, it involves three "new" methods in its main framework: the first one is kNN selection on pre-trained models (referred to the same technique in the work [Scalable Transfer Learning with Expert Models]); the second is the hyperensembles by fine-tuning multiple diverse copies of the models (referred to the hyperparameter sets used in another related work [Big transfer (BiT): General visual representation learning]); and the last is greedy ensemble (referred to the third related word [Ensemble selection from libraries of models]). Not sure what is the contribution of this submission.2. The paper is quite about engineering tricks or combinations of tricks. In addition, in terms of engineering, it is not fair to compare to related methods under the condition of using the same numbers of pre-trained models. A better way may be based on the total computational COSTS such as the max running epochs, the network architectures, the total training time under the same usage of GPU machines. Summary:Paper proposed an ensemble learning approach for the low-data regime. Paper uses various sources of diversity - pre-training, fine-tuning and combined to create ensembles. It then uses nearest-neighbor accuracy to rank pre-trained models, fine-tune the best ones with a small hyper-parameter sweep, and greedily construct an ensemble to minimize validation cross-entropy. Paper claims to achieve state-of-the art performance with much lower inference budget. Recommendation: Based on my understanding of the paper I recommend a clear rejection. Please look at the details below: Strength: 1) Authors have tried to lot of experiments and give summary of conclusion/results in section 4. 2) Experimental setup is clear and the motivation is valid. Weakness/Questions: 1) Paper was very hard to read. I had to go back and forth between pages to make sense of whats defined and make my own definitions in many cases. In some cases, terms are defined but never used and in other cases terns are never defined. For example, a) AugEnsembles: Where is this used?b) ExpertEnsembles: Where is this defined? c) HyperExperts: Where is this defined?d) AugExperts: Where is this defined? 2) In figure 2, Single-model SOTA has only one model. Do you have a graph for total cost (training + inference) vs VTAB_{1K} performance for all the models that are shown in figure 2? Only showing an inference budget may not tell the entire picture here. 3) In Table 2, how is computational cost different for different sources of diversity (D, U and C)? If C needs more computational cost than U and D then is the comparison fair? 4) Appendix A.2 mentions the hyper parameters used when using hyper ensembles and then there is a default hyper parameter sweep - Default Hyper Parameter Sweep in appendix A.1. Did you find any pattern in the hyperparameters with the best model?  How were the hyperparameters chosen for baselines in table 1? minor: 1) VTAB should have been defined just before listing contributions - new form of diversity improves on the Visual Task Adaptation Benchmark (VTAB) SOTA by 1.8% (Zhai et al., 2019).2) Paper repeatedly cites Puigcerver et al 2020 [1] to justify experimental framework or as a follow up paper which is also very similar to the current paper in terms of motivation. [1] Puigcerver, Joan, Carlos Riquelme, Basil Mustafa, Cedric Renggli, André Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. "Scalable transfer learning with expert models." arXiv preprint arXiv:2009.13239 (2020). The paper presents a method for statically learning code invariants from source code using a variant of transformers.Strengths----- The paper demonstrates that on the synthetic dataset the proposed approach can infer many invariants. Weaknesses----- The evaluation with a synthetic dataset seems very weak. If checks are not good proxies for useful invariants as in most programs there are many if checks that are simply unreachable or redundant. In practice,  not all invariants are useful for the downstream tasks (code fixing, bug finding, etc.) mentioned by the authors. Without a more direct evaluation, it is very hard to tell how useful the learned invariants actually are for these tasks. The paper will be significantly stronger if the authors can evaluate their tool against existing loop invariant inference datasets with ground truth data like those used in Si et al. (NeuRIPS 2018). - The transformer-based model seems to be directly reused from Hallendoorn et al. (ICLR 2020). Thus the contribution in terms of model design is limited. - The authors also did not cite/compare against the current state-of-the-art loop invariant learning work: CLN2INV: Learning Loop Invariants with Continuous Logic Networks. Ryan et al. ICLR 2020 This manuscript studies the problem of robust and constrained reinforcement learning and proposes two new objectives for incorporating constraints and robustness to misspecified models into RL training. The advantage of these objectives is that their associated Bellman operators are contractive, which enables the use of value-function based methods. Overall, I vote to reject this manuscript for the reasons detailed below. Pros: - Constraints and misspecified models (or models that change over time) are real barriers to deploying RL in many applications. Therefore, the authors study an important problem that has garnered a lot of attention. - The manuscript offers some theoretical footing and some empirical evidence for the proposed methods. Cons:- The discounted penalized cost on the constraints does not enforce the constraints. In my opinion, having a discount factor does not make sense in this case since constraints violations in the future would be discounted. Why wouldn't it be a problem to violate a constraints in the future? I understand that this approach was considered before, but that does not make it a good idea. - The theoretical results seem to be immediate consequences of the work by Tamar et al, 2014. For example, to show that the sup Bellman operator is a contraction we only need to note that T_sup = - T_inf when we consider the reward -c. Similarly, T_R3C = T_inf when we consider the reward for the T_inf operator to be r - lambda * c. Therefore, all the results follow immediately from Tamar et al. 2014. Given these observations, why is necessary to prove the results in the appendix? - I do not find the empirical results compelling. I present my concerns in the next bullet points. - Only three random seeds were used for evaluation which is inadequate for capturing the variance of RL algorithms (see [Henderson et al. "Deep reinforcement learning that matters", 2017] and [Islam et al., "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control," 2017]). Therefore, comparisons such those presented in Table 2 are not meaningful. Even looking at the values in Table 2 one can see that the mean reward for R3C-D4PG is within a standard deviation of the mean reward of other methods. Similar complaint for the plots shown in Figure 2. - Although I saw in the appendix the constraint set for each of the tasks considered, I am not sure what the constraint violation costs are. Could the authors clarify this? In particular, I do not know how to interpret the costs shown in the last column of Table 2. How serious is a constraint violation with a cost of 0.113? Also, what are the standard deviations for the constraint costs? Is the difference between 0.113 and 0.128 meaningful? - Why does it make sense to average performance metrics across tasks in Table 2? Doesn't each task have it's own scale of rewards and costs? I appreciate that there is a breakdown of performance per task in Figure 2. - Finally, what was the motivation for considering only the MPO and D4PG methods? Would it be possible to try a larger collection of methods on the proposed objectives? Minor issues:- All plots are difficult to read. - Curly R in the first paragraph of section 2.1 is not defined. - Second line of the second paragraph in section 2.1 states that "C: S x A -> R^K is a K dimensional vector." However, "C: S x A -> R^K" is a map. Also, are multidimensional costs used in the rest of the manuscript? It seemed like all costs were scalars. - In Definition 1, the values functions V and V_C are never defined (although clear from context). - The notation \bf{V} (s) = r(s, \pi(s)) + \gamma \bf{V}(s') throughout the paper does not make sense. s' is a random variable whereas \bf{V} (s) and r(s, \pi(s)) are deterministic.- In the Metrics paragraph, below Table 2, "different" -> "difference"- Above equation 10 in appendix A.3, the sentence "The full derivation ..." is redundant. The paper suggests two approaches to combine the concepts of robust Markov decision processes (MDPs) with that of constrained MDPs. In the first approach, called R3C, a worst-case setting is used for both the expected total discounted rewards criterion and the constraints on the state-action pairs. The robustness is defined with respect to all possible choices (from an uncertainty set) of transition-probability functions. In the second approach, called RC, only the constraints should be robust against all possible transition probabilities. The paper studies the value functions and the corresponding Bellman operators of these problems and argues that, in both cases, these operators are contractions in the supremum norm. Finally, numerical experiments are presented on RWRL problems, such as the cart-pole and the walker, showing the effect of using the redefined operators.The general problem that the paper studies (namely, robust constrained MDPs) is nice and worth investigation, but the offered combination is straightforward, the theoretical results are weak, and the paper is poorly written (see below). Therefore, the current form of the paper is substandard and needs major improvements. Several notations and concepts are not specified, starting from the state and action spaces of the MDP. For example, it is not clear whether these spaces are finite, or otherwise, what structure is assumed about them (the minimal assumption that one needs is that they are measurable spaces). The uncertainty set itself is not defined, just the notation is used. The  constraint function $C$ is defined as $S \times A \to \mathbb{R}^K$, but then few lines later in the definition of $J_C^{\pi}$ we simply have $\sum_{t=0}^{\infty} \gamma^t c_t \leq \beta$, without actually stating what $c_t$ is. If $c_t$ is defined as $c_t := C(s_t, a_t)$, then $\beta$ should be a vector to make the above inequality meaningful, with the notation that $\leq$ means coordinate-wise less than or equal. However, things like these should be guessed by the reader as the paper lacks proper definitions.In Section 2.3.1, about the R3C part, in the second equation after (1), the obtained results are dubious, as ${\bf V}(s')$ should not depend on $s'$, as $s'$ is just a random variable with respect to an expectation is taken (see the definition of the classical Bellman operator). Also function ${\bf V}$ is not defined in the paper, the reader should guess its meaning from the appendix. In section 2.3.1, in equation (2), it is not clear with respect to what probability distribution the expectation is taken in $V(s)$. Is there a special element of the uncertainty set, a "nominal" model?The structure of the paper is also a bit chaotic. For example, there is an "Experiments" and also an "Experimental Results" part, both containing results of various experiments. Moreover, Sections 6 and 7 should be subsections of Section 5, etc. [Summary]This paper studies the optimization landscape of one-hidden-layer neural networks in the teacher-student setting, where the ground truth teacher network is one relu and the learner network is m>=2 relus. The paper proves that (1) when m=2, any stationary point (of the student network) has to be aligned with the teacher; (2) when m>=3, stationary points satisfying certain additional conditions have to be aligned with the teacher. Above, alignment means that the student network is perfectly equal to the teacher. [Pros]The one-hidden-layer teacher-student setting studied in this paper is an important and difficult theoretical problem. The reason why GD on an over-parameterized student could perfectly recover the teacher has been a somewhat long-standing open problem. This paper makes progress on this problem by proving the alignment property at m=2, and showing that the determinant condition implies alignment at m>=3.[Cons]I feel like the direction of studying alignment in this specific one-neuron teacher model may be a bit problematic, and may not be that significant or generalizable.---First, results in this paper seem to be about the *stationary points*, not the *local minima* (which at least additionally requires the Hessian to be PSD). This can be seen from Eq (1) and (2) which is only the stationarity condition (gradient equals zero) and does not consider the Hessian. This means that all the subsequent claims are about stationary points and must also contain the saddle points. It seems like the authors may be unaware of this. (The paper talks about local minima where it really means gradient equals zero.)---Second, the fact that the theory contains saddle points makes it somewhat confusing --- If a saddle point does exist, then it has to be non-aligned (because a saddle point must have non-zero loss). So the present result seems to imply that there is no saddle point at m=2 and (likely) no saddle point at m>=3, when the teacher is one relu. (Could the authors confirm or comment on this?)---Further, even if we restrict attention to local minima, (Safran and Shamir 2018) already showed that when the teacher contains multiple neurons, the student network could have spurious local minima with non-zero risk (i.e. non-aligned minima). Therefore we could not hope for alignment for all local minima when the teacher has multiple neurons, which implies that the studying the alignment in the one-neuron teacher model may not be generalizable. This paper proposes a new and simple way to determine word importance for black box adversarial attacks on text classification models. Instead of using example-specific measures of importance like recent work (typically expensive to compute), the authors propose to feed individual words from the vocabulary to a trained model and use the model confidences to get global, class-specific importance scores.While being an interesting paper, at a high level I am concerned about several points:- The paper is unpolished and at times hard to follow. I do not consider it ready for publication at this stage.- There are many easy to implement baselines (feature selection has a rich history) that would have been very interesting to study. Are the WW scores capturing anything that simpler statistical methods do not?- Many implementation details are lacking, which could be an issue for reproducibility. For example, the CNN model details are unclear (number of layers, filter sizes, embedding initialization, etc.). Learning rates and other hyperparameters are not mentioned.- Why is the vocabulary size only 5000? Why are experiments run on only 500 examples, and are these examples selected?- The claim of comparable performance seems slightly exaggerated, as the WW scores perform consistently worse than the baselines. In some cases, the difference seems around 0.1 / 0.15 absolute AUC. Also, providing raw scores (either all scores in Appendix or a subset of the most interesting case in the text) would help readers to quantify these differences.- Some of the claims seem overly broad. For determining, say, grammaticality, I would expect greedy to vastly outperform greedy_ww.- It is unclear to me how WW scores help with network interpretation. Again I would expect WW scores to correlate with a number of statistical correlation measures.There are many formatting issues, with figure numbers, references, equations, etc. Summary:This paper proposes WordsWorth score (WW score), a score to represent the importance of the word obtained from the trained model. Then, the score is applied to the greedy attack proposed by (Yang et al., 2018). In detail, the greedy attack first tries to search for the most important $k$ words in a text, and then it searches for values to replace the selected $k$ words. This paper uses the WW score to select the $k$ words in the first step.Strong points+ A simple but effective approach to utilize for the greedy attackConcerns: - The main concern of this paper is that a minor contribution to the current knowledge. Despite the paper stating that this paper is based on the greedy attack (Yang et al., 2018), the contribution of this paper is limited to calculate the word score from the trained classifier and applied it to the greedy attack.- Another concern about the paper is lack of rigorous experimentation to study the usefulness of the proposed method. This paper does not compare with other score-based approaches. That is, it was not even compared to the tf-idf based score approach.- The writing should be largely improved. Section 4 is the main part of this paper. In Step 1, this paper represents a word as $d$-dimensional vector. Why does this paper append the zeros in front of the word representation? Does it mean the one-hot vector? If not, some studies or discussions about this representation should be included. In Step 2, Equations are hard to follow, and some are incorrectly written (e.g. case equation and definition of the D).- On the same note, readability and completeness of this paper do not meet the standard of conference. The reviewer suggests the authors to review the paper several times before submission.- This paper states that $F$ is the trained classifier. However, there are no explanations on how to train or what kinds of classifiers were used.- In Section 5.1.2, this paper states that it covers 5000 vocabularies freely after 25 reviews are processed because reviews are written in 200 words on average. It is definitely incorrect. Because all words in the review are not unique, it takes a longer timer to cache all words in the 5000 vocabularies. Although the proposed method can speed up by looking up the cached score, the performance of the proposed method is lower than the one of the original greedy approach. Some ablation studies or discussions about the relationship between speed and performance would have been useful to understand this. - Some parameters or data are heuristically selected such as selecting 10 nearest neighbors in step 2, picking 300 examples from test data in IMDB review experiments, and so on. Some form of ablation studies about the parameters would provide appropriateness to readers.- Furthermore, it would be better to show examples of successful attacks with WW scores.- In experiments, the AUC score is used for IMDB evaluation and the accuracy is used for Yelp and AG news. Are there any reasons to use different evaluation measures?Minor comments: 1. It would be better to write some constants such as $k$ in Section 3 as an italic character.2. It will help readers if the authors can explicitly specify which side of the figure, left or right, is explained when the authors are referring to the figure in text.Some typos: 1. In Section 5.1.1, greedyww -> greedy\_ww2. In Section 5.4.2, figure ?? -> Figure 13. In Section 5.4.1, $greedy_ww$ -> greedy\_ww4. In Section 5.4.3, figure ??5. In Section 6.1, CNN 4 -> CNN at Figure 4... Brief Summary: The authors describe a method of generating sequences from neural networks conditioned on sequence annotations using novel statistics about the distribution of sequences. However, the evaluation criteria are lacking, requiring additional work.Pros: - I think using MMD to determine the similarity between the true and generated distributions was very interesting. I am also a big fan of the kmer kernel for the MMD test statistic.-I also think understanding mode collapse with the duality gap was a good idea.- I appreciated the hyperparameter optimization studies to understand which parameters were important to tune to get working.Cons:- I am concerned by the repeated usage of "valid protein sequences" throughout the paper. I'm not sure if the authors intended to use the word "functional". The real test of "valid" would be experimental validation in the lab to determine if the sequences still performed the functions of the GO categories, but I understand that this is not a particularly viable option. However, to make such a claim, the authors could calculate various energy statistics from Rosetta, or other mutation effect predictors to determine if the sequences are likely to be functional.- I am particularly troubled by the statement in the paper: With respect to general sequence quality, ProteoGAN reaches MMD values of 0.0463, which corresponds to roughly 20% of random mutations in a set of natural sequences (compare supplementary Table A5). In comparison, previous in vitro biological experiments showed that proteins with 34% (resp. 59%) of mutated positions compared to the wild-type were viable&This, in no way, proves that sequences generated by this method are valid or functional. A single mutation can abrogate protein function, let alone twenty.-I am also concerned about the train-test-validation splits in this paper: It seems like this was done randomly. This is not how to prove the method can generalize to new sequences. I worry that generated sequences in the training set can have homologues in the test set. Train/test splits using CATH or SCOP would be more appropriate.-Since there is no control for homology in this work, it would be interesting to just use HMMER generated sequences of a family in a given GO category to compare against (simplest baseline model).-In Figure 5, I don't understand where the "Random" values came from. Are you sampling from sequences of the same length or kmer composition? Truly random amino acids is not a useful comparison.- MRR is an odd statistic. Why not just BLAST, or an alignment-based approach? How does MRR compare to e-value, or closest in sequence ID?Neutral:- I am surprised the authors are only using manually annotated sequences for this task, or not leveraging some sort of pretraining. Homologous sequences could improve model performance.- As someone working in biotechnology, I think there is great interest in the refinement of molecular properties to highly-specific functions, and little interest in generating sequences generally from GO categories, which are much too broad. In this manuscript, the authors present a conditional GAN for generating protein sequences given specified GO terms. They argue that this approach to conditional protein generation is more appropriate than sequence-based generation, because it gets directly at functional specification. At a high level, this is an interesting idea though it has already started to be explored by other works. The authors are correct that these works focus primarily on optimize a single function of interest. However, there doesnt seem to be any specific reason that guided design approaches could not generalize to multiple criteria. Regardless, controlled generation of proteins with pre specified functions is certainly interesting.That said, the work presented here is too preliminary with too many missing baselines, missing or poorly/confusingly described experiments, and the dataset is not fully described. No comparisons are made against HMMs or other typical autoregressive generative models. Furthermore, there appear to be critical problems with the experiments. Specific comments follow below (in no particular order).1.If I understand correctly, the MMD method used by the authors compares k-mer distributions between generated and real sequences. They set the k-mer size to 3 (The size of the k-mers was set to 3). Therefore, MMD is only measuring whether the sampled 3-mer frequency matches the observed 3-mer frequency. This metric seems too simple and cant capture complex dependencies that exist in protein sequences. In fact, a simple 3-gram sequence model can perfectly optimize this metric. A simple baseline model in which sequences are generated from a 3-gram model conditioned on the function labels would be informative here. No evidence is otherwise provided that MMD is a good measure of the quality of a generative model.2.The GAN seems like overkill for this problem. Have the authors considered any autoregressive generative models that can be trained by maximum likelihood?3.GANs also have the problem that likelihoods cannot be computed which makes it difficult, maybe even impossible, to use them to rank candidates or otherwise as priors over sequence space. If one wanted to use this model in practice, how would I choose a set of sequences to synthesize and experimentally validate? Strictly by random sampling? Is there a way to choose the top-k most likely sequences? In practice, experimental throughput is not large enough to explore many random samples, which is why focused design approaches have received so much attention. Is there a way to resolve this with GAN-based models?4.Train/val/test splits: the val and test splits are very small. Much smaller than typically used for evaluating ML models (only 2% for val and test). Also, there is no analysis of sequence complexity within these groups. Typically, models are only of interest if they can generalize to relatively distant sequence. How similar are the train/val/test distributions? 5.Table 1 reports standard deviations (I assume. It isnt stated what the error values are in the caption.) for random samples from the latent variables, but this doesnt take into account variability due to model training or the small test set. I encourage the authors to report standard errors over multiple data splits instead.6.I propose a simple generative model: given some set of GO terms, retrieve proteins with those terms from the training set. Then, to generate new proteins I simply propose those sequences. As a second approach, I fit an HMM to those sequences and sample new sequences from the HMM. How does this compare to the proposed approach? These baselines would also reveal how easy the val/test sets are due to similarity to the training set. 7.In Table 1, Positive Control is described confusingly: The reference for MRR was again the test set and the evaluated sample an equally structured set (Positive Control) on p.23 is the only definition for positive control I can find. What is the positive control? 8.The primary use case for a model like this seems to be the ability to generate proteins with combinations of functions not found naturally. Otherwise, choosing a protein to start from is trivial: retrieve it from the database. Have the authors considered that use case?9.Sequences lengths for the baseline models were limited to 32 amino acids whereas the proposed model has a maximum sequence length of 2048. Not only that, but only the first 32 amino acids were used? This does not seem like a fair comparison at all. Typical proteins are much longer than 32 amino acids, so it isnt surprising that the baselines perform poorly. Furthermore, this limitation makes no sense. RNNs can easily be applied to protein sequences of length 2048.10.Proteins can have multiple GO terms associated with them. How was that dealt with? Do these proteins occur multiple times in the dataset for each term? Are proteins generated conditioned only a single term or multiple terms simultaneously? 11.As far as controlled sequence generation is concerned, Generative Models for Graph-Based Protein Design - Ingraham 2019 seems highly relevant but is not mentioned.Things that would improve my rating:1.Fully describe the train/val/test splits with analysis of sequence similarity between splits within functional categories. Increasing the size of the val and test splits is also a good idea.2.Add reasonable baselines to the model evaluation and provide proof that MMD with k-mer size of 3 is a good measure of generative model quality. Baselines must be trained to generate complete sequences, rather than only the N-terminal 32 amino acids.3.Provide a more rigorous description of the model and experiments. A reader should, ideally, be able to understand the conditioning structure from the main text. The paper at question tackles the well-known problem of differentially private (DP) deep learning:  already for moderate privacy guarantees, the model performance suffers greatly.The paper proposes a particular SDE based  method for obtaining privacy for ResNets. DP and stochastic differential equations have been considered in conjunction before e.g. inWang, Y.X., Fienberg, S. and Smola, A., 2015, June. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In International Conference on Machine Learning (pp. 2493-2502).Li, B., Chen, C., Liu, H. and Carin, L., 2019, April. On connecting stochastic gradient MCMC and differential privacy. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 557-566). PMLR.(you might consider adding these references).Although providing an interesting approach by combining DP deep learning and SDEs, I think the paper has some major deficits.One of them is certain sloppiness with the presentation. I do not understand how the forward and backward Euler discretisations correspond to backward and forward propagation of ResNet layers. The main results are regarding the DP-privacy guarantees of the method (Thm. 1 and Thm. 2).However these DP-guarantees are not used anywhere in the experiments, and it remains a mystery to me what is actually DP protected.Another criticism is regarding the experiments: when comparing to DP-SGD, there are no eps,delta-values given, it remains mystery how private the method is and how private is DP-SGD for the choice of hyperparameters listed.The paper would require a major revision and therefore I cannot recommend it for publication in ICLR. Summary: The authors proposed to randomly wire the GNN layers. They claim it can not only resolve over-smoothing problem but also enable the varied size of receptive fields.Pros:1.Really great looking graphics.Cons: 1.The motivation of why should we use Erdos-Renyi (ER) graph to generate DAG for the random rewiring is not clear.2.How can we learn from the results of the DAG statistics (such as averaged path length) is unclear. I dont understand how this can help us design or improve the proposed architecture.3.The experimental results seem inconsistent to [1], where the authors claim to follow their experiment setting.Detailed comments:The main weakness of this paper is its motivation. I do not see any theoretical reasoning that we should use ER graph instead of the other choice of random graph generator. The authors mention that small world and scale-free networks have been studied and use in [2], but I dont see any detailed comparison why the ER graph is a more preferred choice. Even in the experiment section, I do not aware of any comparison which is unsatisfactory. The other weakness is that the theoretical analysis mentioned in this paper neither leads to any reasoning in model design nor guarantee in performance. What can we learn from knowing Lemma 3.1 and 3.3? How do they explain why using ER graph for random wiring is favorable? I do not even see it helps on choosing the hyperparameter $p$.The final but most questionable part is the experiment section. The authors claim that they adopt a recently proposed GNN benchmarking framework [1]. However, the reported results are significantly different and much worse than those reported in [1]. For example, GCN ($L=16$) has 68.5% of accuracy in the experiment on CLUSTER dataset in [1] (Table 2, Node classification). In contrast, the authors report GCN accuracy for only 48.57%. Of course, the hyperparameters might be chosen differently from [1], but then the question would be why not optimize it according to [1]? Also, why not report the performance of more shallow $L$ (say $L=2$ or $4$) which is a more common choice? All these inconsistency makes me hard to believe the proposed methodology   would work.Reference:[1] Benchmarking Graph Neural Networks, Dwivedi et al., arXiv preprint arXiv:2003.00982, 2020.[2] Exploring randomly wired neural networks for image recognition., Xie et al., In Proceedings of the IEEE International Conference on Computer Vision, pp. 12841293, 2019. This paper proposes a new MCMC transition kernel. This kernel is parameterized by neural networks and is optimized through an objective maximizing the proposal entropy. Specifically, the authors use a combination of a flow model and non-volume preserving flow in [Dinh et al., 2016] as the neural network parameterized kernel. Then they use the objective in [Titsias & Dellaportas, 2019] which maximizes the proposal entropy to optimize the kernel. The proposed method is tested on synthetic datasets, Bayesian logistic regression and a deep energy-based model.The problem of improving the exploration efficiency of MCMC kernel is important. The proposed method is well-motivated. As far as I understand, the proposed method appears to be technically sound. However, I have the following concerns about the paper.-The connection and the difference to previous work are not very clear. If I understand it correctly, the proposed method seems a combination of L2HMC and [Titsias & Dellaportas, 2019] with some slight modification (a flow model) since the naïve combination did not work well (as stated in Section 4). I think it would improve the clarity a lot if the authors explain more clearly how the proposed method differs from previous work. -Since the use of a flow model is the main difference compared to the naïve combination of two previous methods, the authors should explain more about this choice. Currently, it is not clear why this helps and there is no explanation on why the naïve combination fails. -The proposed method seems to use more neural networks (e.g. an additional network R to handle gradient) than previous neural network MCMC. I wonder how the method performs if considering the cost. For example, the authors may instead show ESS per second on the experiments in Section 5.1. Though the authors mentioned the difficulty of computing ESS per second in the paper, Im not entirely convinced. As the experiments in Section 5.1 are all very small-scale, is it really necessary to use GPUs? -The baselines vary from experiments to experiments for no reason. For example, the authors compare their method to L2HMC on ill-conditioned Gaussian and strongly correlated Gaussian, to Neutra on Funnel distribution, and to MALA on EBM. I think this experiment design needs explanation. Also, there is no empirical comparison to [Titsias & Dellaportas, 2019] which is closely related to the proposed method.Some minor comments:-A.1 intends to show the benefit of using gradient information. But Variant 2 also uses gradient. What is the point of showing it? -It is not clear to me how to interpret the empirical results in Section 5.2. For example, how does Figure 3 show that the proposed method needs less sampling steps? -The color of points in figure 1 is hard to read. The paper is fairly easy to follow and does not have serious presentation problems. Especially, the discussion on the background (WAE) is clear and concise.None the less, the two weaknesses are1. The idea of having a hyperspherical latent space [1] is already been studied (and is a famous work), yet the author does not include any discussion with it. Even the author has discussed it, I can't see any difference between using the methodology presented in prior work and the submission. 2. There is nothing new I can learn from this submission. Precisely, I can't find the presented approach has any benefits over existing variational auto-encoder variants. Suggestion:1. I can't agree that momentum encoder makes contrastive learning "more computational efficient". It requires extra handing on the momentum. I think what the author likes to claim is that the momentum encoder leverages an asynchronous update and hence the effective batch size is larger. 2. In section 5.4, I can't agree with the statement from the author "for larger temperature, the estimated distribution becomes smoother and the entropy estimation becomes poor, which should result in a poor quality of generated samples." This statement is very vague and is incorrect. Typo:1. Theorem 1: should be when Z~P_Z not when g ~ P_Z[1] Hyperspherical Variational Auto-Encoders, Davidson et al., UAI 2018. The paper proposes an approach to improve masked based prediction explanation. They train an auxiliary model which predicts a mask that must satisfy two terms 1) maximize classification accuracy when applied to the image and 2) maximize entropy over softmax when the inverse mask is applied to the image (they also experiment with minimizing the classification accuracy instead here).The paper also positions their work to be classifier agnostic in the text which is not clear to me. I think this aspect is a negative because it is not in the list of contributions and seems like a distraction of a remnant of an old direction of the paper:> "Whereas a FIX approach seeks a saliency map that explains what regions are most salient to a given classifier, a CA approach tries to identify all possible salient regions for any hypothetical classifier (hence, classifier-agnostic). In other words, a CA approach may be inadequate for interpreting a specific classifier and is better suited for identifying salient regions for a class of image classification models." I'll focus now on studying how this paper addresses the 4 contributions they claim in the intro:> (1) We find that incorporating both masked-in classification maximization and masked-out entropy maximization objectives leads to the best saliency maps, and continually training the classifier improves the quality of generated maps. There are no standard deviations reported from multiple runs so nothing can be statistically claimed. Also, looking at table 1 this is not clear. It seems MinClass works the best compared to almost all other methods in terms of PxAP. In terms of other metrics it seems like no best can be determined.> (2) We find that the masking model requires only the top layers of the classifier to effectively generate saliency maps. This is reported in the table. It is not clear if this is a strong contribution as it would just be specific to this method and without a standard deviation we cannot conclude anything.> (3) Our final model outperforms other masking-based methods on WSOL and PxAP metrics. The reported difference between the methods is 48.6 vs 48.4.> (4) We find that a small number of examplesas few as ten per classis sufficient to train a masker to within the ballpark of our best performing model.This is reported but the paper doesn't have a section detailing the experiments or showing how this number is derived.The paper could be improved by refining the contributions and detailing what evidence should be observed to support these claims and then providing a significant amount of evidence. Right now the paper is not focused in general and does not focus on supporting the claims made in the introduction and therefore is not ready for publication. Federated learning is a distributed learning paradigm where models are learned from decentralized data sources. The paper proposes to train machine learning models with communication-efficient differentially private approaches.I have several concerns about the work including the experimental setup.Experimental setup:Experiments use a privacy budget of epsilon = 400 for LeNet Models and 2000 for ResNet models. Note that differential privacy algorithms ensure that the probabilities differ by at most e^{epsilon} with high probability. This means that the probabilities can be off by  e^{400} ~ 10^170, which is higher than the number of particles in the observed universe.The paper is very hard to accept with the current set of experimental results.Other:a. R_s is not defined in Section 1.2.b. PrivQuant seems to be a very interesting algorithm, however very little intuition is provided. It would be good to describe why this algorithm is preferred over others. c. In Section 2.2, authors argue that one can get better results if the gradients are sparse. However note that for this to work, even the non-zero coordinates have to be relayed with differential privacy, which can add to the total privacy cost.d. Step 14 in the algorithm: the role of r is not clear and needs to be explained more. This paper claims that decentralized parallel SGD (DPSGD) performs better than synchronous SGD (SSGD) and noisy version of synchronous SGD (SSGD*) in large batch setting. Theoretically, it shows that the noise in DPSGD is landscape-dependent, which may help generalization. Experimental results on CV and ASR tasks show that DPSGD can outperform baselines when batch size is very large. Meanwhile, DPSGD is observed to adaptively adjust the effective learning rate and converge to flatter minima.Although the idea in the paper is insightful, most of the claims are lack of sufficient supports and in-depth discussion. For the theoretical part, Eq(4) only shows the differences of noise between SSGD and DPSGD. The explicit relation between Delta^(2) and generalization is not provided. After reading Eq.(4), I still do not know why Delta^(2) can help generalization. In my mind, Delta_S is also landscape-dependent because the covariance matrix of \Delta_S aligns well with Hessian in some cases [1][2].The explicit relation between decreasing effective learning rate and convergence rate of an optimization algorithm is not provided too. The convergence not only depends on the tendency of the learning rate but its decreasing rate. For an extreme instance, if the effective learning rate decreases rapidly, it is hard to imagine that it will help convergence. Besides, there are many writing issues in the paper. Please see the details comments below.Detailed comments:(1) For a scientific paper, every claim should be objective. There are many subjective claims in this paper. For example, "Recently, ASGD has lost popularity due to....."(in Intro), "One possible reason is that the complexity of escaping a saddle point....."(in Intro), "The poor performance is likely due to......"(in Sec2.1), etc. These claims are lack of sufficient supports  and are not convincing.(2) There are redundant notations which make the paper hard to follow. For example, the "SSGD+noise" in Figure 1 and "SSGD*" in Figure 2; the subscript of a letter sometimes denotes the learner, sometimes denotes the average or subset average.(3) Some claims are ambiguous. For example, "It is clear that \Delta^(2)" depends on the loss landscape-it is larger in rough landscapes and smaller in flat landscapes". What is the measure of the loss landscape and the flatness? Both landscape and flatness are descriptive terms and their measures should be clearly introduced. Why does the noise in SSGD not depend on landscape?(4) The term "Multiple learners" is misleading, especially for SSGD. It is better to name them local workers because they only calculated gradients and SSGD outputs a unified model. (5) I suggest to use the original format because this version looks too crowded. For example, the space between sections is too small and the paragraphs in the introduction are merged.In summary, the current version is not ready to be published.[1] Wen, et al., An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise[2] Zhu, et al,. The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects. This paper gives some empirical comparison of centralized sgd and decentralized sgd for relatively large batch size. The authors claim that decentralized sgd performs better than centralized version and that this is because the noise introduced by the decentralized version helps escape the local minima.While it is claimed that "We show, both theoretically and empirically, that the intrinsic noise in DPSGD can...", it is hardly possible to find any theoretical insight. The theoretical analysis part, Section 2, contains a figure (Figure 2) with an entire subsection (Section 2.2) discussing it, but include no theorem, lemma, or even proposition! There is clearly stated noise comparison between centralized sgd and decentralized sgd and thus it is not clear why one can claim decentralized sgd does better becomes of noise.Second, as a paper studying the large batch setting, it is necessary to define what large batch means and how batch size relates to the convergence, none of which is included in the paper. In fact, in the theoretical analysis part (Section 2), batch size does not appear at all. Why is large batch size important, or is decentralized sgd always better than centralized one? The entire Section 2 is far away from understandable.The experiment section also fails to justify the claim. The authors compared decentralized and centralized sgd for batch size = 1024, 2048, 4096, 8192. Are those all large batch, or is 1024 small and the other large? From Table 3 I guess it is the former case. But then a nature question is: does decentralized version perform poorly for small batch size? The experiment section  fails to show a "phase transition", which cannot support the claim. Furthermore, for vision tasks, evaluation on a single dataset (CIFAR10) is certainly not enough. Same thing for ASR task. Most importantly, there is no empirical evidence supporting the claim that decentralized sgd " 1) [it] automatically adjusts the learning rate to improve convergence; 2) [it] enhances weight space search by escaping local traps (e.g., saddle points) to find flat minima with better generalization".  Figure 3 and Figure 4 simply show properties of the neural network and the datasets and has nothing to do with optimization methods. Table 2,3,5,6 only shows final accuracy produced by the optimizers, but say nothing about how the optimizers reach the solutions.The paper writing is problematic too. A single section (Section 3) talking about methodology with less than 1/3 page is awkward. Discussing empirical results in the analysis part (Section 2) is misleading. It is also not acceptable to have "Please refer to Appendix F." for a whole subsection (4.4). Many sentences do not make sense. For example,  "In a large batch setting, the learning rate must be increased to compensate for the reduced number of parameter updates". Why is number of parameters updates reduced? "While there was anecdotal evidence that DPSGD outperforms SSGD in the large-batch setting". What is the evidence? Which paper claims this? There are numerous typos too.In short, this paper is sloppy and far away from publishable in its current stage.  The authors propose an extension to label smoothing, where the smoothing parameter is determined based on the miscalibration of the validation set. They then compare the performance of their method in terms of calibration under domain shift to a small set of baselines. I have 3 main concerns regarding missing baselines, missing experiments and only marginal benefits over state-of-the-art.My first main concern is that the authors limit their comparison to rather dated baselines (2018 and older), when there has been a lot of active research in this field in the past year. In particular, I am disappointed that while the authors cite recent work on MixUp showing its benefits for calibration (Thulasidasan et al., NeurIPS 2019) they do not include MixUp as baseline (where also inputs are smoothed rather than labels only). Other notable recent work that should be included as baseline is Verified Uncertainty Calibration, Kumar et al., NeurIPS 2019, which has been shown to be superior to Temperature Scaling.  In addition, a baseline which also exploits links between calibration and adversarial, Stutz et al., ICML 2020, should be included as prior work.Another aspect of the presented work is the exploration of constructing an ensemble of neural nets trained with label smoothing. In light of this, it is crucial to also compare the presented approach to Mix-n-Match, Zhang et al., ICML 2020, who present work on ensemble methods for uncertainty calibration using label smoothing.  My other main concern is a whole set of missing experiments investigating calibration in truly OOD scenarios. Snook et al, on whose work this paper heavily builds, point out that in addition to the domain drift scenarios explored by the authors, it is crucial to investigate performance in truly OOD scenarios, where the test data is drawn from a distribution far a way from the training distribution. Since the model is per definition not able to make a correct prediction, in this scenario entropy is used to quantify model the quality of the predictive uncertainty. The authors should add these experiments for all datasets. In this context, it would also interesting to investigate the quality of predictive uncertainty of OOD detection methods. While such comparisons are not always meaningful, a model strongly related to label smoothing/MixUp is Hendrycks et al., ICLR 2019, where a GAN is trained to learn OOD samples, where in MixUp inputs are smoothed to generate OOD samples. A comparison to this approach would also be interesting. Finally, even though important baselines as well as experiments for truly OOD scenarios are missing, benefits over a simple ensemble of vanillas remain unclear. For large-scale data (i.e. Imagenet), ensemble of Vanilla perform consistently better than the proposed method in terms of ECE under domain shift. For CIFAR-10, performance is comparable to deep ensembles and only for AR-AdaLS of Ensemble for CIFAR-10 there is a marginal improvement for a subset of perturbation strengths. I would have liked to see also evaluation of ECE under domain shift for CIFAR-100 and SVHN. Also missing is an evaluation of AR-AdaLS of Ensemble for Imagenet. Finally, results for non-image data, e.g. using a recurrent architecture (as in Snoek et al) are missing. Other concerns and open questions include: Although the authors explain the hyperparameters they use in the appendix, a detailed sensitivity analysis is missing to understand how sensitive the method is with respect to alpha and R. The authors use Ensemble of Vanilla as baseline - however, Snoek et al. have shown that it is actually deep ensembles that perform best; in addition to the ensemble effect they are also trained using adversarials. What is the performance of actual deep ensembles rather than ensemble of vanilla?A conceptual concern is that in the proposed approach the validation set becomes part of the training set since it is used during training to update epsilon and thus is not anymore the independent set that may be necessary to tune other hyperparameters/for early stopping; I wonder whether this data leak may lead to problems related to overfitting?Please report a proper scoring rule in addition ECE (e.g. Brier score).A minor point ist that different colors are used for AR-AdaLS  and Ensemble of Vanilla in figure 3 and figure 4.Unfortunately the authors do not provide any code, which make reproducibility difficult.  ### Paper SummaryIn this paper, the authors proposed to train high quality classifiers from datasets with some mislabels.For this purpose, the authors considered adjusting the softmax prediction using an additional term $\alpha$ as follows:$$P^C(i | x; \theta) = \frac{\exp(m_i(x; \theta) + \alpha_i(x))}{\sum_j \exp(m_j(x; \theta) + \alpha_j(x))}$$where $m(\cdot; \theta)$ is a model and $\theta$ is its parameter.The authors claimed that, by adjusting $\alpha$ through training, the trained model $m(\cdot; \hat{\theta})$ with an optimal parameter $\hat{\theta}$ is asymptotically consistent with the model trained on a dataset with clean labels, i.e.,  the trained model without $\alpha$ performs well on clean test data$$P(i | x; \theta) =\frac{\exp(m_i(x; \theta))}{\sum_j \exp(m_j(x; \theta))}$$In the proposed method, for the training set $D = \\{x_n, y_n \\}$, we first train the model by minimizing the following loss function:$$\hat{\theta}, \hat{\alpha} = \arg\min_{\theta \in \Theta, \alpha \in \mathbb{R}^{N \times K}} -\frac{1}{N} \sum_{n=1}^N \sum_{i=1}^K 1[y_n = i] \log \frac{\exp(m_i(x_n; \theta) + \alpha_{ni})}{\sum_j \exp(m_j(x_n; \theta) + \alpha_{nj})}$$where $K$ is the number of classes. We then classify the new instance $x$ by $\hat{y} =\arg\max_i m_i(x; \hat{\theta})$.In Theorem 1, the authors claimed that the above estimator $\hat{\theta}$ converges to the *true* parameter $\theta^*$.### Pros & Cons[Pros]The experimental results indicate that the proposed method is effective on several datasets.[Cons]The paper contains a serious flaw in its problem formulation and the subsequent theorems. The proposed problem formulation has a trivial solution which is completely useless. The effectiveness reported in the experiments seems to be just an artifact caused by the tunings of hyperparameters. See my comments in "Quality" below for the detail.### QualityThe paper contains a serious flaw in its problem formulation.Recall the training problem:$$\hat{\theta}, \hat{\alpha} = \arg\min_{\theta \in \Theta, \alpha \in \mathbb{R}^{N \times K}} -\frac{1}{N} \sum_{n=1}^N \sum_{i=1}^K 1[y_n = i] \log \frac{\exp(m_i(x_n; \theta) + \alpha_{ni})}{\sum_j \exp(m_j(x_n; \theta) + \alpha_{nj})}$$This problem has a trivial solution that $\alpha_{n y_n} \to +\infty$ for $\forall n$, which leads to$$\frac{\exp(m_i(x_n; \theta) + \alpha_{ni})}{\sum_j \exp(m_j(x_n; \theta) + \alpha_{nj})} \to\delta(y_n = i)$$where $\delta(y_n = i) = 1$ if $y_n=i$ and 0 otherwise.Note that this trivial solution does not depend on the model $m(\cdot; \theta)$. Thus, any parameter $\theta$ can be an optimal solution $\hat{\theta}$ as long as $m(\cdot ;\theta)$ is finite.The above observation indicates that the proposed method do not work as expected if the training problem is solved appropriately. Thus, I conjecture that the good performances reported in the experiments are the artifact caused by the tuning of hyperparameters, e.g., the training converged to local optima that occasionally performed well.Note that the above observation on the training problem also suggests that the claim of Theorem 1 (the estimator $\hat{\theta}$ converges to the *true* parameter $\theta^*$) is not correct.In the proof, the authors considered the following objective function:$$L^C(\theta, \alpha) = - \mathbb{E} \sum_{i=1}^K \left[ \sum_{k=1}^K \pi(i | k, x) P(k | x, \theta^*) \log P^C(i | x, \theta) \right]$$Let $U(i | x, \theta^*) = \sum_{k=1}^K \pi(i | k, x) P(k | x, \theta^*)$. We then have$$L^C(\theta, \alpha) = - \mathbb{E} \sum_{i=1}^K U(i | x, \theta^*) \log \frac{\exp(m_i(x; \theta) + \alpha_i(x))}{\sum_j \exp(m_j(x; \theta) + \alpha_j(x))}$$By taking the derivative with respect to $\omega \in \\{\theta, \alpha\\}$, we have$$\frac{\partial L^C(\theta, \alpha)}{\partial \omega} = - \mathbb{E} \sum_{i=1}^K U(i | x, \theta^*)\left( \frac{\partial (m_i(x; \theta) + \alpha_i(x))}{\partial \omega} - \sum_{k=1}^K \frac{\exp(m_k(x; \theta) + \alpha_k(x))}{\sum_j \exp(m_j(x; \theta) + \alpha_j(x))} \frac{\partial (m_k(x; \theta) + \alpha_k(x))}{\partial \omega} \right) \\= - \mathbb{E} \sum_{i=1}^K \left( U(i | x, \theta^*) - \frac{\exp(m_i(x; \theta) + \alpha_i(x))}{\sum_j \exp(m_j(x; \theta) + \alpha_j(x))} \right) \frac{\partial (m_i(x; \theta) + \alpha_i(x))}{\partial \omega}$$Thus, any $\theta, \alpha$ that satisfy $U(i | x, \theta^*) = \frac{\exp(m_i(x; \theta) + \alpha_i(x))}{\sum_j \exp(m_j(x; \theta) + \alpha_j(x))}$ are optimal.In the proof of Theorem 1, the authors only considered a specific $\alpha$, and overlooked the existence of other $\alpha$ that are equally optimal, which led to the wrong claim that $\hat{\theta}$ converges to $\theta^*$.### ClarityApart from the serious flaw above, I think the paper is clearly written and the main claim of the paper is easy to follow.### OriginalityThe use of the adjustable parameters for fitting noisy data is studied in the literature of robust learning. I would like to suggest the authors to see [Ref1] and references therein. In [Ref1], an additional penalty is imposed on the adjustable parameter to avoid the trivial solution I raised above.[Ref1] Consistent Robust Regression, NeurIPS17.### SignificanceBecause of the flaw I raised above, I think the contribution of this paper is not significant. The paper analyzes the theoretical properties of noise injection in StyleGAN-like networks, and proposes an extension to in particular to StyleGAN2 that results in somewhat improved metric scores. Unfortunately the paper is rather confusingly written and hard to follow. To highlight what I mean, I will try to paraphrase my understanding of the paper in the following.The theoretical treatment begins by framing the problem around optimal transport, but later seems to mostly drop this viewpoint. While the cited OT/GAN work presents interesting and relevant viewpoints about the difficulties in GAN training, I am not sure if it is particularly more relevant here than any number of other theoretical works. It may be noted that StyleGAN and DCGAN are not even formulated as to minimize a Wasserstein divergence.The paper then coins a term "adversarial dimension trap", which I am not exactly sure why this terminology was chosen. The gist of the observation seems to be mostly well known, i.e. the generator can only cover a zero-measure region of the data space whereas the data is more spread out. That said, I am not thoroughly familiar with previous theoretical work on GANs and the particular formulation here may be novel. The paper then introduces a fairly general form of stochastic noise injection into the network layers and calls this fuzzy reparametrization. Here some connections to are drawn to "fuzzy equivalence relations" which (apparently?) are an existing concept, however as far as I see there is no citation to discuss these and little insight is given about why this is relevant.Then, the key theory is developed. If I understand correctly, the key idea here is that the zero-measure manifold is "puffed up" with random distributions centered around points on it, with spread that depends on the point. This makes sense as a principle but overall I am confused about whether something fundamental was discrovered or proved, or whether this is just an introduction to the reasoning behind the practical algorithms.This then leads to a proposal of a practical algorithm. If I understand correctly, it basically generalizes the StyleGAN noise injection in such a way that not only the mean, but also the stdev of each feature is predicted by the network. This plausibly allows for more flexibility. Unfortunately the description here is again rather confusing. Apparently formulas 6-8 are not really equations, but rather some kind of imperative pseudocode with variable assignments. It would be better to spell this out as an algorithm listing. As for the content of these formulas, I am not sure if I understand what the operations or the reasoning behind them is. Why the pixsum here? Apparently it produces a single value per feature map? After this there seems that these numbers are transformed by some global matrix(?) A and bias b, however I'm not sure what the convex combination with a matrix(?) I means here given that the first half of the formula is a vector (?). Then the result seems to be normalized again (why?) And finally the means are transformed by this standard deviation? There may well be good reasons to use these steps, but they are not explained so it ends up looking like an arbitrary heuristic. Here it would be important to make a strong connection to the insights derived from theory.The presentation is further made confusing by the language. I understand that the authors may not be native speakers, but the readability is much below the usual standard of ICLR papers and the paper would benefit from improving this.As for the results, it does appear that there is some improvement in some of the metrics, and the proposed method may in principle be useful. It is not hard to believe that adding some extra flexibility to the noise injection might improve the results, at least in a limited number of scenarios. In this sense the paper may be on to something. What is the meaning of using PageRank to reduce the number of LSUN-Cat images? How is PageRank related to choosing images and what's the difference between that and just taking the first 100k pictures in the set? And for that matter, I am not sure if we learn anything from randomly limiting the set to 100k images, when we don't know how it worked for the full set. For the inversion experiments, the table in the appendix does show improvement and this may well be the case. In figure 9, though, it's hard to see much of a difference between any of the methods, perhaps in part because the images shown are very low resolution and do not correspond to anywhere near the SG2 output image size -- any differences in details are completely hidden by this.The architecture figures 6-7 are unnecessarily low detail. They contain a black box "FR" node precisely at the place where you'd want to know more. Perhaps this node could be expanded into its own architecture diagram as well, given that there is no shortage of space in the appendix.In summary the paper might contain useful bits -- this is somewhat hard to judge -- but whether or not that is the case, it is not in an acceptable condition without some significant rewriting, and I would recommend rejection at this time. The authors tackle the problem of class imbalance in supervised learning. They propose an oversampling algorithm that is based on a genetic algorithm. Samples are iteratively generated and filtered to maintain informative samples. The authors demonstrate the efficacy of the approach using several examples.Although the problem is important and the solution might be interesting, I feel that the paper is not scientifically sound and the English level is not satisfactory. Below I will detail some of the problems with the current version of the manuscript:-First paragraph: only can estimated - this is is not in English.-First paragraph: Ensure that the sample does not make& - again this sentence does not make sense.-Authors did not use the correct citation format, please see instructions by ICLR.-P2: SMOTEChawla - here the citation is mixed with the method name.-P3: Holland - the year is mentioned twice.-2.3 has the title Genetic Algorithm and 3.1 has the title GA, arent these the same???-3.1 the paper does not generate anything, perhaps you meant the method?-Equation (2)  x_ik is not defined, what are the two indexes for? Sample and feature? Need to define.-Equation (2) you take the square of the square root, these cancel each other!-The authors mostly cite SMOTE based method for class imbalance, but there are other methods such as:[1] Lindenbaum, Ofir, Jay Stanley, Guy Wolf, and Smita Krishnaswamy. "Geometry based data generation." In Advances in Neural Information Processing Systems, pp. 1400-1411. 2018.-Perhaps you could use bar plots for figure 1 instead of the scatter plots which are not that informative.There are many other mistakes, so I feel that at this stage the paper can not be published at this venue. In this work, the authors propose an oversampling technique which creates a population of synthetic samples for an imbalanced dataset using genetic algorithms. Each individual in the GA population corresponds to a synthetic sample, and the fitness function is based on the similarity (in feature space) of the synthetic sample compared to nearby samples of the same and different classes; standard crossover and mutation operators are used. In a limited set of experiments, the proposed approach sometimes outperforms competing approaches.I think the basic idea of using a GA to avoid local minima in the feature space is reasonable, and the proposed fitness function makes sense, though it is perhaps a bit obvious. However, I believe the paper has several key limitations that need to be addressed.The discussion on related work omits a large number of highly related references. In particular, a large number of existing papers propose to use genetic algorithms for oversampling, including: [Saladi and Dash, Soft Computing for Problem Solving, 2019], [Tallo and Musdholifah, ICST, 2018], [Ha and Lee, IMCOM 2016], and [Qiong et al., JDIM 2016]. While the proposed approach likely differs from these in some respects, the complete lack of context with respect to existing GA approaches for addressing class imbalance makes it difficult to judge the methodological novelty.Second, the set of baselines considered in the work is rather limited. Thus, it is not clear if the paper meaningfully improves empirical performance compared to state of the art. In addition to lacking any other GA baselines, the experiments also lack a representative generative adversarial network-based approach. Considering the prevalence of such approaches, at least a standard implementation should be included in the comparison.Third, the experimental design itself could be significantly improved. The current design does not allow for any estimate of variance in performance of the methods on each dataset. Thus, it is not possible to tell if any of the results are significant. Indeed, additional experiments suggest that the proposed approach is not stable, so that further calls into question the empirical advantage of this approach. A simple way to estimate such variances could be to perform cross-validation by bootstrapping the dataset (e.g., k-fold cross-validation). It is also not clear how the hyperparameters for the other baseline methods are selected.It is also not clear how the approach may work for datasets with non-numeric features, such as text or images. Presumably, some standard embedding method (word2vec, CNNs, etc.) could be used to first embed the data to a vector space, but it is then not obvious to me how well the proposed GA approach would be in exploring that space.Minor comments---------------------The paper has numerous small grammatical errors. I do not believe these affect understanding the paper, but it requires another round of proofreading.There is some issue with the formatting of the references in the text; they are all followed by two closing parentheses.For the crossover operation, is \alpha drawn uniformly from [0,1], or from a truncated Gaussian with a mean around 0.5? or something else?In Algorithm 1, \alpha is used for the Gaussian kernel variance, while \sigma is used in the text.For Figure 1, something like a box plot or violin plot is probably more appropriate for presenting that data.The references are not consistently formatted. The paper proposes a new metric for measuring example difficulty: variance of gradients. Given a neural network for image classification, it measures the variability in the gradient of the output of the penultimate layer of the network with respect to each pixel, as observed during the training process. This quantity is averaged over all pixels to measure the difficulty of an image. Illustrative examples, observed correlation with error rate on test data, and successful identification of out-of-distribution examples indicate that the proposed metric is useful.The paper is very nicely written and a pleasure to read.The primary shortcoming of the paper is that it does not compare to any other measure of example difficulty. An obvious metric is the entropy of the estimated class probability distribution for an example (computed either with or without some form of calibration of the probability estimates). It is unclear whether the proposed metric provides a significant amount of additional information.Although intuitively plausible, and backed up by experimental results, the metric is not justified by providing some form of theory.Data augmentation is used in the experiments but the paper does not explain how this is accounted for in the calculation of the metric. Is the metric computed for the unmodified training examples only?Figure 6 (left): Why not compute the consistency of the rankings directly? The difference in observed error is an indirect measure.Typos:"p is the index of either the true or the predicted class probability.""ImaageNet-O" The paper proposes to use high-dimensional sparse representation as opposed to low-dimensional dense representation for representing input text. This can be useful in information retrieval applications. The proposed model is evaluated on a "product to product recommendation" dataset and shows superior performance compared to the prior work.The paper's strengths:1. Studying an important and interesting research problem (i.e., learning sparse representation for information retrieval)2. The paper is well-written and easy to follow. It is also well-structured. The results are well presented.The paper's weaknesses:1. In several places there are some overclaims that should be fixed. For example, in abstract and introduction (when describing the paper's contributions) it is mentioned that "In this paper, we make a novel and unique argument that sparse high dimensional embeddings are superior to their dense low dimensional counterparts." This is simply not true. As we go forward in the related work, we can see that the authors mention that SNRM [40] has exactly the same idea with empirical evidences. I think the authors should be more careful in claiming what is novel in their paper and what is borrowed from prior work.2. I think the weakest part of the paper is its evaluation. Here is the detailed comments:2.1. First of all, it has been only evaluated on a single dataset. It would be nice to see how the results can be generalized across datasets. Second, the task they used for evaluating their model is pretty artificial. There is no such real-world task as "product to product recommendation" and the experimental setting is not realistic at all. I believe using such synthetic datasets for evaluating models is acceptable when there is no good alternative publicly available. However, in this case, there are numerous large-scale datasets that can be used to evaluate the model. For example, MS MARCO dataset from Microsoft has been widely adopted by the research community and can be used for evaluating the model. A lot of Open-Domain QA datasets are available too. 2.2. As mentioned multiple times in the paper, SNRM is the only paper with the same idea of learning high-dimensional sparse representation. However, the results reported in the table are strangely low. Looking at the SNRM paper (which was published in 2018, so it's not an old paper) we can see that it outperformed a lot of strong baselines on a number of benchmark. I understand that sometimes it is difficult to reproduce the results from some papers. However, the SNRM paper seems to be cited by over 60 papers and some of them reported its results on different datasets and they all reported a reasonable performance for the model. I personally think that the authors did not carefully select the sparsity hyper-parameter in the SNRM model. I also recommend the authors to include some non-neural baselines, such as BM25 and RM3 as they sometimes outperform neural network models on information retrieval related tasks.3. It is not clear how scalable the model is and actually I have some doubt about its scalability. What will happen when the number of items goes beyond tens of millions or even billions? These questions are important to be answered when a model is proposed for information retrieval or recommender systems.All in all, I believe the authors are working on an interesting problem but the experimental results are not convincing and some claims should be changed. Some questions about scalability remain unanswered. This paper connected contrastive-learning and supervised learning from the perspective of the energy-based models. Then, the authors combine both objectives and evaluate the presented method on various datasets and tasks.Strengths: The paper attempts to connect supervised and contrastive learning. I like the attempt. But unfortunately, I don't think it is valid. See explanations as follows.Weakness:1. I feel the claim in the paper is too strong. The approximation from equation 12 to 13 is very crude. Specifically, the approximation states that the infinite integral (for the normalization constant) can be replaced by a finite sum, which is generally not true. 2. Even if we assume the above approximation is fine, the connection with contrastive learning is very unclear. Precisely, the approximation is for modeling p(x|y), yet the contrastive learning is modeling p(x_1|x_2) with  x_1 and x_2 being the outcomes from correlated data. The authors do not discuss or compare between p(x|y) and p(x_1|x_2), and hence it makes the connection very vague. 3. The resulting objective (eq. 15) is a combination of the discriminative and generative modeling, which has already been studied. 4. On page 4, "the representation captures important information between similar data points, and therefore might improve performance on downstream tasks." This sentence is super vague, and I can't understand what the "important information" is and why if we capture "this important information", we "may improve performance on downstream tasks." The author should spend time polishing the presentation.5. The main complaint of the presentation is the overclaim for the experimental section. I understand the contents are too much, and hence the author must move some experimental sections into Appendix. The author claims that the proposed method is performed on adversarial modeling and generative modeling, while these two sections only appear in the Appendix. In the last few lines of page 5, the author seems to rush the remaining experimental sections into the Appendix and asks the reviewer/reader to read themselves. The author should spend time arranging the contents and make sure the paper is self-contained. ==================================Summary of the reasons why I vote for rejection:1. The main contribution of the paper by connecting supervised learning and contrastive learning is overclaimed. The approximation of the intractable normalization term is not appropriate. The connection with contrastive learning is not solid.2. The paper doesn't seem to be ready for submission. The content is not organized well and some ambiguous wordings should be avoided.[1] Representation Learning with Contrastive Predictive Coding by Oord et al.[2] On Variational Bounds of Mutual Information by Poole et al. In this paper, the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives. They also show that the residue-residue contacts can be extracted by sparse logistic regression to learn coefficients on the attention heads. One of the advantages of using transformers models is that they do not require an alignment step nor the use of specialized bioinformatics tools (which are computationally expensive). When compared to a method based on multiple sequence alignment, the transformers models can obtain a similar or higher precision.Contributions of this paper are:- showing that the attention maps built in Transformer-based protein languages learn protein contacts, and when extracted, they perform competitively for protein contact prediction;- a method for extracting attention maps from Transformer models;- a comparison between a recent protein transformer protein language model (which does dot require sequence alignment), and a pseudo-likelihood-based optimization method that uses multiple sequence alignment;- an analysis of how much the supervised learning (logistic regression) contributes to the results.The paper covers a relevant topic and it is easy to read. However, I have a number of concerns. The main contribution of the paper is that attention maps built in Transformer-based protein languages learn protein contacts and can be used for protein contact prediction. However, this was reported before in Rives et al.(2019) (doi: 10.1101/622803). Also, several methods have been developed for this problem, but are not included in the comparisons. Finally, the provided implementation details are not sufficient to reproduce the results of the paper. I detail some of these concerns below, together with questions/suggestions for improvements:1) I would recommend comparing transformers to other methods besides Gremlin, or justify why other methods were not included. This review can be helpful:(Adhikari B, Cheng J., 2016.. doi: 10.1007/978-1-4939-3572-7_24)Also, more recent methods that were published after the review are:(Badri Adhikari, 2020. https://doi.org/10.1093/bioinformatics/btz593)(Luttrell  et al., 2019. https://doi.org/10.1186/s12859-019-2627-6)(Gao et al.,2019. https://doi.org/10.1038/s41598-019-40314-1)(Ji S et al., 2019. https://doi.org/10.1371/journal.pone.0205214)2) On page 7, the authors state that "We find that the logistic regression probabilities are reasonably well calibrated estimators of true contact probability and can be used directly as a measure of the model's confidence (Figure 10a)". However, from the plot in Figure 10a, it is not totally clear that the probabilities are well calibrated. Could the authors add more justifications of why they consider it well calibrated? Could they also show a comparison of the calibration of the other transformer models, perhaps using MSE as a calibration metric?3) To understand the occurence of false positives, the authors analyze the Manhattan distance between the predicted contact and the true contact, which is between 1 and 4 for most false positives. They also show an example of a homodimer, for which predictions were far from the true contacts, and explain that the model is picking up inter-chain interactions. Could the authors report how many predictions have a Manhattan distance larger than 4? Is this one example representative of the group of false positives far from the true contact? Maybe the authors could analyse whether this happens in most of the cases.4) While ESM-1 is open-source and publicly available, this is not the case for ESM-1b. In section A.5, the authors provide implementation details as differences between ESM-1 and ESM-1b, stating Compared to ESM-1, the main changes in ESM-1b are: higher learning rate; dropout after word embedding; learned positional embeddings; final layer norm before the output; and tied input/output word embeddings. The weights of all ESM models throughout the training process were provided to us by the authors.. In my opinion, this is not enough to reproduce the results in this paper. To make it reproducible, the authors need to provide a detailed enough description of the differences to make the reader able to implement ESM-1b, or provide the weights and hyperparameters required to reproduce their results. This paper considers the problem of personalization in federated learning. The proposed approach consists of allowing each client to have a local model for personalization and taking a convex combination of global and local models as personalized preditors. Authors also provide a theoretical analysis of the generalization and convergence guarantees and empirical evaluation.The proposed method is equivalent to the Mapper algorithm from [1]. Generalization analysis is also very similar.The empirical study is not conducted carefully:- Allowing for a separate local model for personalization increases the number of parameters which is not accounted for- Reported performance of the global model trained with FedAvg on MNIST and CIFAR10 is overly pessimistic. Prior works (e.g. [2]) report much better performance for the non-personalized model in similar heterogeneous data splitting scenarios.- Splitting MNIST and CIFAR10 datasets by classes is not a realistic way of simulating heterogeneous distributions in my opinion (see [3,4] for more recent strategies). It also does not seem appropriate for studying personalization as we know that there is a single global model with good performance (i.e. trained on the full dataset) and the key challenge is the optimization over heterogeneous datasets rather than the need for personalization. I recommend datasets such as FEMNIST [5] for studying personalization.[1] Three Approaches for Personalization with Applications to Federated Learning[2] Communication-Efficient Learning of Deep Networks from Decentralized Data[3] Bayesian Nonparametric Federated Learning of Neural Networks[4] Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification[5] https://leaf.cmu.edu/ This paper proposes to augment transformer architectures with memory components. The high-level idea is to use multiple special mem tokens as additional inputs. Depending how the mem tokens representations interact with the true input sequence, three variants are studied: (a) in MemTransformer [mem, input] attends to [mem, input]; (b) MemCtrl is similar, but uses a separately parameterized module to calculate the mem representations; (c) in MemBottleneck, mem attends to [mem, input], and [input] attends only to mem. Experiments with machine translation, language modeling, and fine-tuning on the GLUE benchmark are conducted.Overall the presentation of the paper could be significantly improved. For example, it lays out the technical sections in a way that assumes the readers have hands-on expertise with transformer architectures. Further, the motivation is not clear to me. It seems from the experiments that little practical gains can be expected, since MemTransformer never outperforms the transformer baseline in terms of accuracy or efficiency. This is probably fine if the paper studies a set of clear research problems with well-designed experiments and answers some interesting questions. But it is not the case. Last but not least, the experimental setting is flawed: e.g., the MT experiment cuts training at a suspiciously early stage, presumably contributing to the bad performance. In sum I do not think the paper is ready for ICLR.Pros:- Studying the transformers through the memory perspective is interesting.Con:- Writing can be significantly improved.- Motivation is not clear: the proposed approach doesnt seem to bring any interesting practical gain, nor does it answer any interesting research question.- Experimental design is flawed, making the conclusions less convincing.Detailed comments:- Im not against discussing related works in the introduction. But the current version is probably trying to scramble too much stuff into intro, making it tedious to read.- I suggest walking through the transformer architecture in a more self-consistent way. I would be very surprised if one without much hands-on experience of transformers can understand whats going on in section 2.- The baselines performance in the MT experiments is far worse than what we usually see. Im guessing this is because the training stops too early. Can the authors explain why they choose to do so? If limited computation is the concern, I recommend the authors to work with smaller datasets instead but use a more convincing settings. Same for other experiments.- The experiments do not show any practical gain from MemTransformer. The paper talked about its linear complexity in input sequence length. This paper would be stronger if it can present convincing experiments and show that MemTransformer can have a better efficiency in practice.- Section 3.2 would be much more clear and interesting if it opens with the research questions its trying to answer. #### Summary:The paper brought up an interesting limitation of a Transformer network, that information about the context is stored mostly in the same element-wise representation, which might limit the processing of properties related to global information. This work proposed adding memory tokens to store non-local representations and creating memory bottleneck for the global information. The evaluation shows positive results adding memory to a Transformer network.#### Weakness:The idea is not entirely new; Memory augmented networks have been proposed in previous work and the paper does not differentiate the method from related work very well. -The paper is not clear about how to obtain the memory input token. According to the paper, the memory input token seems like a static vector; however, a memory augmented network can retrieve a dynamic vector for global information, with the ability to read and write to the memory. -The naive Mem Transformer Layer is counter intuitive, as there is little differentiation between the memory layer and the sequence layers. How the memory layer contains additional global information is not explained. -Figure 1c is confusing, are the arrows crossed or not?-The formulation of memory representation and sequence representation are exactly the same for the MemCtrl Transformer, which again is very counter intuitive. It is intuitive that we should augment the sequence transformer layer with additional global information, however, we might not want the other way. The memory layers should be updated at coarser granularity. -The paper is poorly written with many errors in the paper. Please proof read the paper before submission. #### Detailed feedbacks:The reviewer finds this paper hard to read as there is not a flow of story in the paper. The layout of the paper and the design of experiments seems very arbitrary. -The motivation for a memory network like proposed in the paper is not clearly stated. How the memory network can capture long-term dependencies and information is not clearly explained. The design of variants of Memory Transformers seems very arbitrary. -The paper also lacks a more detailed comparison with related work, like Transformer-XL, Star-Transformer, Longformer, ETC, etc.-"global"->''global'' (make sure it looks correct in latex). -".Surprisingly"->". Surprisingly"-Please improve the quality of writing and ask native speakers to proof read the paper. This paper studies the implicit regularization effect that arises from using stochastic gradient descent with label noise and squared loss. They derive the expression for the implicit regularization term and show that it favors solutions which are stable against perturbations of the parameters. This paper validates their empirical findings using SGD on linear regression task with label noise. This paper also uses their results to study the self distillation technique.The main concern with this paper is that I think that the authors claim that they are the first ones to characterize the implicit regularization arising from SGD with label noise but this result is already known [1]. I believe that [1] works in the same setting and their implicit regularization for SGD exactly matches as derived in this paper.I would recommend rejecting this paper because I believe that the main result presented in this paper is already known [1] and this paper has not cited or compared with that paper.Other questions:1) Why do the authors assume that the noise terms in equation 10 are independent when step size is small as mentioned on page 4?2) I am not convinced by the self distillation experiments. Why do the authors consider self distillation with label noise added to the logits? Is this form of distillation considered in previous works before? Did the authors consider adding label noise to just the original labels? 3) Moreover, in the plots in figure 2, why do the curves follow this zig zag pattern with increasing noise?  Moreover, many points in the plots are actually below the red dotted line which is the accuracy without label noise. So, I am not fully convinced that label noise is actually leading to better performance. 4) It is unclear to me what is the motivation for considering these two different types of noises - gaussian and symmetric and what is the take away for these types of noises?Some typos1. Page 2 - extent -> extended2. Page 2 - not clear what new convergence exactly means3. Page 4 -  brownie -> brownian4. Page 6 - increases -> increase1) Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process, COLT 2020 This paper studies the implicit regularization effect from random label noise in L2 regression, and argues that it can be captured by the average squared gradient norm over samples, $\frac1N\sum_{i=1}^N \|\| \nabla_\theta f(x_i, \theta) \|\|^2$. This conclusion is achieved via analyzing the additional noise term in the update caused by the label noise, which in the SDE approximation has expected squared L2 norm proportional to $\frac1N\sum_{i=1}^N \|\| \nabla_\theta f(x_i, \theta) \|\|^2$.The paper does not cite a very related paper:Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process. Guy Blanc, Neha Gupta, Gregory Valiant, Paul Valiant. COLT 2020The above paper studies the same problem and obtains a similar conclusion that the implicit regularization of label noise can be captured by $\frac1N\sum_{i=1}^N \|\| \nabla_\theta f(x_i, \theta) \|\|^2$. It is proved that a solution $\theta$ which achieves zero training error is an attractive fixed point of the dynamics if and only if it is a local minimizer of the said implicit regularizer.The high-level conclusion in the current submission is the same as Blanc et al. In addition, the current submission has a weaker theoretical argument, since it is based on the SDE approximation which may not capture SGD as the noise distribution may not be Gaussian. On the other hand, Blanc et al. rigorously prove that any attractive interpolating solution must also locally minimize the implicit regularizer. Given the overlap, the submission does not meet the bar of publication. 1. Strength:Targeting an important problem of FL: reducing the communication cost.2. Weakness:This work simply applies the meta-learning method into the federated learning setting. I cant see any technical contribution, either in the meta-learning perspective or the federated perspective. The experimental results are not convincing because the data partition is not for federated learning. Reusing data partition in a meta-learning context is unrealistic for a federated learning setting. The title is misleading or over-claimed. Only the adaptation phase costs a few rounds, but the communication cost of the meta-training phase is still high.The non-IID partition is unrealistic. The authors simply reuse the dataset partitions used in the meta-learning context, which is not a real federated setting. Or in other words, the proposed method can only work in the distribution which is similar to the meta-learning setting.Some meta earning-related benefits are intertwined with reducing communication costs. For example, the author claimed the proposed method has better generalization ability, however, this is from the contribution of the meta-learning. More importantly, this property can only be obvious when the data distribution cross-clients meet the assumption in the context of meta-learning.The comparison is unfair to FedAvg. At least, we should let FedAvg use the same clients and dataset resources as those used in Meta-Training and Few-Rounds adaptation.Episodic training is a term from meta-learning. I suggest the authors introduce meta-learning and its advantage first in the Introduction.Few-shot FL-related works are not fully covered. Several recent published knowledge distillation-based few-shot FL should be discussed. 3. Overall RatingI tend to clearly reject this paper because: 1) the proposed framework is a simple combination of meta-learning and federated learning. I cannot see any technical contribution. 2) Claiming the few round adaptations can reduce communication costs for federated learning is misleading, since the meta-training phase is also expensive. 3) the data partition is directly borrowed from meta-learning, which is unrealistic in federated learning. The paper is to train a meta-model in a small number of selected nodes in a federated learning environment, and then use the meta-model to assist the federated learning in reducing the communication rounds. It is basically a federated version of a prototypical network.The proposed method relies on a strong assumption that there is a meta-training environment in federated learning. It is not a standard FL setting. Moreover, given the assistance of the meta-model, there is no guarantee that the federated learning environment will converge in a few-round.The major technique contribution of the proposed method is how to meta-train a global model in a federated setting. In particular, it adapts the prototypical network to fit the federated setting. It is unclear how the proposed method provides any theoretical contribution rather than applied research. In the experiment, one dataset is not enough to support the effectiveness of the proposed method. More federated learning-related benchmark datasets should be discussed, e.g., FeMNIST, Shakespeare texts, CIFAR, and FeCelebA. In particular, the proposed two-stage procedure is equivalent to: learn a global model in a standard FL setting, and then conduct personalized deployment for each device or a specific group of devices. Therefore, in the experiment part, the authors need to add more baseline methods, for example, some personalized federated learning method should be selected as baseline methods.THE MAJOR CONCERN: In Algorithm 1, lines 16 and 18 are a federated aggregation-based updating, and line 24 is a prototypical-based meta learner updating. These two updating methods are inconsistent which are to optimize different objectives, and the authors should give an overall loss to unify the updating steps rather than force two kinds of updating into one framework.Typo: Metra-training in Figure 1. The paper proposes a method for unsupervised image translation between unpaired domains of images. The main idea is to develop an iterative transformation module that operates in the embedding space.Overall I have the following concerns about the paper:The motivation for this architecture is unclear. The introduction motivates this model with fractals and iterated function spaces, but that seems to have nothing to do with the types of applications shown here. What do IFSs have to do with denoising and pictures of zebras? This iterated refinement strategy seems more similar to iterative refinement/projection algorithms like conjugate gradient and Richardson-Lucy deconvolution, which are relevant to low-level signal processing operations like denoising/deblurring, but not zebra synthesis.  The paper includes as motivation the idea that applying that different levels of transformation can be achieved by choosing different numbers of iterations, but the application of this is shown only for denoising (Table 1). No comparison is provided to the state-of-the-art in unpaired image translation: Contrastive Learning for Unpaired Image-to-Image TranslationTaesung Park, Alexei Efros, Richard Zhang, Jun-Yan ZhuECCV 2020Visually, the results are not convincing. Not many results are shown, and most do not look better than those from CycleGAN.  The results may be cherry-picked, since there was no statement as to how these results were chosen.  There is simply not enough visual evidence that the method has evidence over previous work. Additionally, quantitatve comparisons do not give a compelling outcome, but I would put more weight on visual comparisons anyway.For the task of denoising, it is unclear why one would want to use a general-purpose unpaired translation method; supervised methods ought to be much effective here, and there is an enormous literature of related work that is not cited or compared with here. If one is to use denoising as a motivation application (rather than a toy example), then much more rigor is required.  This paper presents an analysis on the neural processes in the signal processing point of view and gives a bound on the highest frequency of the function that a neural process can represent.I recommend to reject this manuscript. My comments are below.The key point of this work is Theorem 3.1. However the theorem itself is just a direct outcome of the NyquistShannon sampling theorem, and it is generally true to not only neural processes but also to all the other approaches. Meanwhile, the authors did not talk about the relationship quantitatively between the representability and the error tolerance in Definition 3.1. In addition, the analysis is limited to only scalar-valued function on a 1D interval. The writing could also be improved.Concerns:- The definition of neural processes in the background section is confusing. Despite the way of defining a map, P is a mathematical object defined by a set of tuples and a map,  meaning that the neural processes are also defined by data. In the original paper, the neural processes were however defined as random functions.- In the background section, the words say 'some sources define ...'. Could the authors give the sources?- In Def 3.1, what do the authors mean by 'discrete measurements'?- In the experiment section, do the authors mean sampling from a Gaussian process by saying GP prior? I don't see a GP plays the role of prior in terms of Bayesian inference.- The examples given in the experiment section lack quantitative results. It is better for evaluating the reconstruction by showing the posterior or predictive distribution instead of single reconstructions.- In Sec. 4.2. how did the authors sample regular grid on the 2D plane as y is determined by x. - Eq.11 is defined in the appendix. Better to use separate numbering. SummaryThe paper proposes to predict the required weights of a Neural ODE function from the input data. The paper has two parts - supervised classification, and unsupervised image reconstruction and generation. In the supervised part, has good visualizations of the decision boundaries induced. The unsupervised part shows good comparisons with previous methods, and good visualizations of the reconstructed and generated images.StrengthsThe paper is well organized. The images and figures are explained well. The graphs showing the trajectories of the Neural CODE are great for visualization. The paper provides enough details about training its networks.CommentsThe paper is split into two parts - supervised classification, and unsupervised modeling. The supervised classification part is highly related to the Data Controlled Neural ODEs section in Massaroli et al (2020b), a paper that has been cited in the related work section but not addressed in the main content. In fact, the problems tackled (-x, concentric annuli) and the results are highly related to those of Massaroli et al (2020b). This issue needs to be addressed sufficiently.The unsupervised section needs a lot more work. The experiments and tables can be described more effectively. For example, it would be preferable to clearly explain which row section 5.3 refers to in Figure 7, and which row corresponds to section 5.4.The fact that the replacement of a linear layer with a Neural CODE improves image reconstruction quality should mean that all layers in the encoder can be replaced to give better encoding, taking care of dimensionality (such as in Normalizing Flows). In fact, generative modelling papers such as FFJORD, or Normalizing Flows, do replace all layers with a Neural ODE and map to a latent space of noise. However, this paper uses a typical neural network for image reconstruction and generation. This perhaps means that the majority of heavy-lifting is done by the decoder, hence the Neural CODE is more amenable to warping the latent space suitable for the decoder. For image generation as well, the latent space has been designed so that the decoder can produce nice images, this is not necessarily a win for the Neural CODE.To make the case for Neural CODE, especially for images, higher resolution images need to be tackled, since it is in higher dimensions that the success of the advancements in the methods listed in Figure 7 lies.Massaroli et al. (2020b) : Dissecting Neural ODEs; NeurIPS 2020;  https://arxiv.org/abs/2002.08071Grathwohl et al. : FFJORD : FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative ModelsMinorMany variables needs to be bold at multiple places.Figure 4 needs to say Third for the closed loop figure.Zhang et al., 2019b is cited in related work, but needs to be cited along with the respective experiment for Figure 5. Summary: The paper proposes a method to avoid the overestimation bias. They modify the TD3 algorithm such that, instead of regressing the two Q-values towards the minimum of both Q-value functions, the regression target is a convex combination of both, determined by a parameter lambda which is updated on a slower time-scale.I have three concerns due to which I am currently recommending rejection: First, the smaller concern is that the paper is currently difficult to understand, second, my major concern is that I am not sure about it's correctness, due to equation (9). Lastly, the experimental evaluation is insufficient. Regarding understanding: The paper currently has a significant number of grammatical mistakes and unusual or wrong word choices. While this often is without consequence, unfortunately, at times it does impact the clarity and creates uncertainty about the argument the authors are trying to make. However, overall I believe I was able to understand the algorithm the authors are proposing, so this is not my major concern.Regarding correctness of the algorithm: My main issue is with equation (9), i.e. with how lambda (the trade-off parameter in the convex combination between the two q-value functions) is updated. I believe equation (9) should almost always be negative because the minimum of two values is always smaller or equal than any convex combination of both, since lambda is restricted to 0 d lambda d 1. Which means that based on equation (11), lambda is actually optimized to *maximize* the value, i.e. it encourages overestimation instead of preventing it. One caveat might be that equation (9) actually compares Q-values from before and after each update. It is not explained why this is done, but I believe that should  change the sign of (9) in only very rare cases as I'd expect Q to move only very little in each update.Lastly, lambda is a global variable, whereas the minimum operation of TD3 is local, i.e. performed independently in each state. I believe this is a big difference in the proposed algorithms which is not discussed at all.  Regarding experimental evaluation: As the authors propose a novel, general purpose algorithm which aims to improve on TD3, it is important to evaluate it on a wide range of benchmarks, including the typical, widely used mujoco environments on which most current continuous control algorithms have been evaluated and compared. Other, minor, points:- In equation (5): What is the min over? What is the max over? Also, the Q functions have the wrong argument signature- Below equation (5): I don't understand what xi and xi' are- Very minor point for Equation (6) & (7): Personally I don't like the introduction of the \bar{} version of lambda and would find (1-lambda) clearer in the equation The authors propose a deterministic policy-gradient algorithm that extends the TD3 algorithm (Fujimoto 2018). The main claim is that it reduces overestimation issues in a more effective way. Two Q-critics are maintained with separate parameters, but updated using the same transitions. Then a convex combination of these critics is used in the deterministic policy gradient update. The mixture parameter is learned on a slower time-scale to minimize this convex combination over states (instead of taking the minimum of the 2 critics per batch as in TD3). Another contribution in the paper is the Unbiased variant of the algorithm (UAD3), which addresses the off-policy nature of the replay mechanism of the AD3 algorithm described above. My understanding is that this is simply a version of the algorithm that does not use any replay mechanism and samples the state iid from the on-policy distribution, so it isnt a novel idea in itself.There are two theorems given to justify the algorithm choices, but I want to question their validity. The first one says that AD3 converges asymptotically, but no formal statement of what this means is given and the proof for it in the appendix only states broad facts about stochastic approximation, but nothing specific that applies to the AD3 algorithm. Theorem 2 is misleading in another way, it says that AD3 has the property of asymptotical expected policy improvement, but it only really says that the critic value will be increasing, not that the actual policy value is increasing (and so an actual policy improvement step). Moreover, the proof contains some approximation steps which are not justified.The approach is tested in two simple continuous control environments (maze + reacher task). (Are these using the full state as input?). There the proposed approaches perform better it seems than the baselines (TD3 and DDPG), but there isnt any analysis to understand whether that was due to better critics - why not plot the estimated and true returns during learning to see whether AD3 indeed does better than the other critic update strategies? The experimental section is missing details to make these results reproducible and interpretable, for example what network architecture was used for the policy and critic?  All learning curves have rather strange oscillation patterns. Is that an artifact of the smoothing used? How many seeds were used to obtain each learning curve?At the moment, the advantages of the proposed approach are neither demonstrated theoretically or empirically in a satisfactory way (see comments above). At least one of these aspects need to be improved significantly before the paper is ready for acceptance.Other comments:The objective to decide how to mix the critics could be better motivated. Making sure that the critic increase may not be the best criterion to ensure stability and reduce over/underestimation for example. Minor things:* In actor-critic methods, the policy is deterministic& Actor-critic methods are actually more commonly described in a stochastic policy setting I believe. This work seems to be based on the deterministic policy gradient paper which is not cited.* Citation style should use parentheses around names in most cases in the text.* Gattami 2019 is not the right reference for the Bellman equations and derived RL updates.* Lots of equations are redundant and add unnecessary complexity. For example Eq 3 and Eq 4 are the same equations with different parameters, so why not call the two parameters w_1 and w_2 and write the equation once with w_i  for i \in {1,2}* The hat on the \lambda in Eq 6,7 are hard to see because they are attached to the left bracket.* adopts -> adopt* Transition slots -> tuples?* Minus distance/reward -> negative The paper presents an approach to mitigate the overestimation issue, which is quite common in RL algorithms whenever computing boostrap target is needed. The key idea is to introduce a weight parameter between two Q values and adopt a dual problem formulation to learn this weight and the policy parameters. The authors propose a two-step method to estimate these parameters and present some experiments to show the proposed algorithm's performance.Overall Quality/Clarity is poor. The reasons are as follows. First, the theorems are incorrect or meaningless. Theorem 1 says the proposed algorithm AD3 (i.e. adaptive delayed deep deterministic policy gradient) converges. But the proof is for the tabular case and it becomes trivial. Furthermore, the proof itself seems to be incorrect. There should be restrictions on the function \mu as it would affect what type of operator it is. Theorem 2 is simply incorrect. The inequality (13) does not imply policy improvement at all. I guess the authors want to say the policy under the policy parameter of the i+1th step has a higher expected return. But the notations are not saying that. The proof itself uses approximation signs arbitrarily, and it is unclear what the meaning is. Second, experiments lack details and it is unclear what message they are trying to convey (e.x. Fig 2). How were those hyperparameters chosen? How many runs? What is the standard deviation across runs?  Third, many of the statements and notations are not rigorous. The authors never clearly state the objective function they are using. Is it the same as DDPG? And, it never makes a clear distinction between the objective used for updating the critic parameter and the actor parameter. The definition of J(\theta) is confusing, what is the underlying distribution of s (first paragraph on page 3)? In eq (6) and (7), the definition of J(\theta) changes. In actor-critic methods, the policy is deterministic and commonly parameterized as the actor network. This is inaccurate. The policy does not have to be deterministic in actor-critic methods. However, updating two Q networks according to the same target estimate will make them less independent, which will further negatively affect the training efficiency. Why? Although the underestimation bias accompanying the minimization operator is far preferable ..., it indeed negatively affect the policy improvement at every iteration and further brings fluctuation on algorithm convergence. Why does it "indeed affect the policy improvement"?Important detail missing in the background. The authors should cite the DDPG or the DPG papers at least when describing the updating rule (2). In the original work, there is also a slower moving network for the actor used for computing boostrap target to update the critic. For a paper concerning removing overestimation bias, it is a very important detail and should be clearly stated.  Last, those important derivations (i.e. those from eq (8))  do not have justification. Originality/Significance. I did not see exactly the same work before. But there are similar works and many papers in the same research line are not appropriately discussed. Please see the citation part below. Lack of citation & unrigorous citation. "The foundation for updates of network parameters is the Bellman equation Gattami (2019)." The cited paper is about multi-objective and constraint MDP; the credit of the Bellman equation should not go to this work. There is no need to cite anything here. Rigorously, Q-learning is based on the Bellman optimality equation. Maxmin Q-learning: Controlling the Estimation Bias of Q-learning by Lan et al. This work introduces a method that reduces bias to almost zero while also reducing variance. They also provide convergence proof under the tabular case. Since their approach does not have the issues as the author mentioned for DDQN, the authors should include such a citation and further discuss why their proposed approach is still interesting. The authors should also search for average Q-learning, ensemble Q-learning, etc. The earliest work discussing the source overestimation should be Issues in Using Function Approximation for Reinforcement Learning by Sebastian Thrun and Anton Schwartz. 1993. The following nice papers also discuss the harm of overestimation bias. Istva n Szita and Andra s Lo rincz. The Many Faces of Optimism: A Unifying Approach. In International Conference on Machine learning, 2008. Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement Learning in Finite MDPs: PAC Analysis. Journal of Machine Learning Research, 2009.  - Summary:This paper aims to tackle the out-of-distribution generalization problem where a model needs to generalize to new distributions at test time. The authors propose to utilize some extra information like the additional annotations as the conditional input and output the affine transformation parameters for the batch normalization stage. This extra information helps the backbone network get a more general representation from the training set thus the model is robust to the distribution shift when testing. Experiments are conducted on the Aerial Image Labeling and the Tumor-Infiltrating Lymphocytes datasets which correspond to the image segmentation and classification task respectively.Comments:1. The Conditional Batch Normalization (CBN) mechanism has been used in some generalization relevant tasks like domain adaptation detection [1] and few-shot learning [2]. The paper misses the insightful ideas, and only simply combines the CBN into different backbones and lacks theoretical analysis or empirical evidence about how the additional information and the conditional network can help to improve the models generalization ability. I think more ablation studies to demonstrate the effectiveness of the proposed conditional network and choices of the metadata or additional information in a dataset are needed.[1] Adapting Object Detectors with Conditional Domain Normalization[2] Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation2. The paper demonstrates the experiments on two tasks (i.e. semantic segmentation, image classification), but the proposed conditional network is different and does not have a unified architecture. So the concept of the conditional networks is over-packaged.3. The motivation proposed framework is not clear. The conditional learning assumption is a straight-forward idea for machine learning or deep learning, so what is the main novel mechanism proposed in the paper is not clear.Questions:1. Is there now an existing standard benchmark of the out-of-distribution (OOD) generalization task? I noticed that this paper does not compare with other OOD generalization methods.2. What is the meaning of Figure 4? It seems that the explanation is not clear.3. From this paper, the additional information of the Inria Aerial Image Labeling dataset is the geographical coordinates. But what is the additional information or metadata of the Tumor-Infiltrating Lymphocytes dataset? I think it is an important issue but I did not find an answer from this submission. The authors proposed that cerebellum in the brain computes synthetic gradients, as used in decoupled neural interfaces (DNI), to enable learning in neural circuits without waiting for gradients to propagate backwards. The authors incorporated several architectural properties of biological cerebellum into their cerebellar model. They showed that a LSTM trained with such synthetic gradients can learn a variety of tasks, from motor reaching to caption generation. The paper is clearly written, and the link between DNI and cerebellum is a novel idea. However, the authors made little attempt to actually compare their model with experimental findings from cerebellum (except for the cerebellar properties built into the network), limiting its scientific impact. Meanwhile, it is not clear whether the cerebellum-inspired DNI provides concrete advantages over DNI proposed in Jaderberg 2017.Major comments:(1) The authors made little attempt to actually compare their model with experimental findings, or at least make concrete testable predictions.Main results (Fig. 2-4) are mostly showing that their core models, CC-DNI and sCC-DNI, can successfully reduce losses across a variety of tasks, and do so better than heavily truncated BPTT gradients.Except for the architectural features incorporated into the model, and some loose arguments that cerebellum is involved in sensory, motor, and cognitive tasks, the link between the model and biological cerebellum appears somewhat weak.The authors claimed that our work makes numerous predictions and opens the exciting door to explicit comparison& without actually spelling out any key prediction. One prediction of the model is that when a task is very well-learned, the gradients (both real and predicted) should be close to 0, and cerebellum output neurons in the deep cerebellar nuclei should be somehow silent. Another prediction is that cerebellum lesioning should only impact learning, but not performance of well-learned tasks. Im not sure these predictions are supported by empirical evidences. I would feel much more comfortable supporting this manuscript if the authors provide more comparisons with experimental data, and make more concrete testable predictions.(2) Critical questions about how real gradients are computed and transmitted to inferior olive is not answered.For the brain, the backward lock may not be the most acute issue when searching for approximated gradient descent in brains. DNI relies on computing the real gradients, and using it to train the generation of synthetic gradients. Both computations are challenging in the brain. The authors completely circumvent this problem by saying it is outside of the scope of the current paper. However, I think this issue is critical for considering the feasibility of the proposed mechanism. For example, how can cerebellum learn to predict the gradient of individual cortical neurons when there are many fold less Purkinje cells than cortical neurons? There are of course more granule cells than there are cortical neurons, but in the model, granule cells are not the ones representing the synthetic gradient, \hat{g}_M, right?Minor(1) The references are at a number of places somewhere between inaccurate and incorrect. Here are a few that I noticed.In the introduction, the authors wrote These observations suggest that the cerebellum implements a universal function across the brain (Diedrichsen et al., 2019). However, if Im not mistaken, the Diedrichsen review is arguing the exact opposite that cerebellum is not implementing a universal function.In section 2.1.1, the authors wrote Here we use LSTMs (Hochreiter and Schmidhuber, 1997) as a model of cortical networks, which have recently been mapped onto cortical microcircuit (Costa et al., 2017) . Costa 2017 provided a potential way to link cortical microcircuit to a LSTM-like structure. I dont think its fair to say that LSTMs are "mapped" onto cortical microcircuits.In section 2.1.2, the authors wrote On the other hand Bellec et al. (2019) showed that temporal gradients as used in BPTT are equivalent to using eligibility traces that transmit gradient information forward in time. The eligibility-trace-based algorithm proposed by Bellec 2019, namely e-prop, is not equivalent to back-prop. It is an approximation that works well empirically in the cases studied in that paper.(2) Fig. 2 panels are mislabeled. The authors propose a loss function that is robust to the adversarial samples and claims the training with this loss function makes the model achieve better generalization ability.However, there are a number of problems in this claim.The method does not ground on a solid theoretical analysis\I tried to interpret the analysis, but failed.1. The right hand side of Eq(1) does not satisfy the rule of the probability. i.e., it does not satisfy $\sum_y P(y|x) = 1$ when $\sum_y P(y|x) = 1$ and $m$ is a non-zero constant.Thus the difference between two KLs around Eq.(5) does not make sense because $KL[P|Q]$ works as a valid divergence only when $P$ and $Q$ are probability distributions.2. We cannot derive Eq.(3) from Eq.(1)3.  If we add $m$ inside the exponential at the numerator for all the class $i$, it is equivalent not to add anything because it vanishes after normalization.4. There is a lack of the explanation why the adversarial training is not sufficient. It seems for me adversarial training will be a natural choice if we want to make the model robust to the adversarial training. Strange statement\The authors wrote Since the ground truth distribution of training data is **known**, $H(P(y))$ is a constant and hence,Usually we assume the ground truth distribution is **unknown**, if we know the ground truth distribution, there is no need to train the model. We can get immediately the minimizer of the loss. This may be a typo, but even if it is the mistake of unknown, still the statement is strange because it does not relate to whether $H(P(y))$ is a constant or not.Experiment is not convincing\The adversarial training works well for this experiment. Although the authors claim the adversarial training has a noticeable sacrifice of accuracy on clean examples. The mechanism is not well studied.Also the authors tested their method only on MNIST and CIFAR10. It is not so appealing.Reference\The intuition of the authors claim has a relationship with the distributional robust learning. The authors should discuss the relationship with these methods.\A. Sinha, et al., Certifiable distributional robustness with principled adversarial training, in ICLR 2018\Amir et al., Robustness to Adversarial Perturbations in Learning from Incomplete Data in NeurIPS 2019. This manuscript presents ADIS-GAN (affine disentangled GAN), a method for learning disentangled affine transformations of images such as rotation, zoom, and translation. The authors enforce this disentanglement using a regularizer that penalizes the difference between affine transformations on latent representations. The method is difficult to follow with many critical details left out. Without a complete description, its impossible to fully understand and evaluate this work. Some specific comments and questions follow below.1.How is the latent vector, C, used? How does it interact with W_basis and W_real? Must it be 5-d to match with rotation, zoom, and translation? How are additional factors of variability encoded?2.How is W_basis learned?3.What is Norm_AL4.What are the encoder/generator/adversary model architectures and how were they trained? For that matter, what is the encoder listed in algorithm 1? I cant find any reference to it in the text.5.How do you ensure that affine transformations of the latent vector actual produce affine transformations of the image? In Figure 5, it looks like the faces arent simply being transformed by zoom, because other attributes clear change. In column 5 from the right, the top face has short hair, an exposed ear, and visible teeth. The bottom face, in contrast, has long hair, covered ear, and no visible teeth. This does not appear to be a simple zoom transformation.Things that would improve my score:1.Fully explain the method, neural network architectures, and training parameters.2.Explain how affine transformations of the latent vector are forced to produce matching affine transformations of the image. This paper considers the setting of extreme multi-label classification, where labels typically follow a power-law distribution with many infrequently-observed labels (so-called tail labels). In this setting it often happens that multi-label classifiers more often predict frequent labels as positive than infrequent labels. In practical applications this is not always wanted, and the authors present a new algorithm that favors tail labels over frequent labels. To this end, a specific ranking-based loss function that consists of two parts is minimized. The first part of the loss ranks positive tail labels higher than positive frequent labels. The second part is more standard, and ranks positive labels higher than negative labels. Improving predictions for tail labels is an interesting research goal that has not been thoroughly addressed in the literature, but I am not convinced of the theoretical results and the introduced algorithm. Theorem 1 does not hold because an important condition is missing. The theorem would only hold if w_j^T x > 0 for all x. However, in practice, such a condition cannot be guaranteed. The formulation of the theorem is more difficult than needed, but what the authors want to say is the following: "P(y_j|x) is a monotonically increasing function of the norm of w_j". The proof that is found in the appendix cannot be correct because one can easily construct a counterexample when w_j^T x < 0, and the proof is also more complicated than needed. In fact P(y_j|x) is just a transformation of w_j^T x via a monotone function g with [0,1] as codomain. Useful choices for g are the logit or probit link, but not an exponential function (as stated in the proof). With this insight one can easily see that, when w_j^T x < 0, the probability P(y_j|x) will decrease when for example all coefficient in w are multiplied with a factor two. In that case the norm of w_j all increases, and we have a counterexample for the theorem. To my opinion, the proof makes a few very strange constructions, but I cannot immediately see where the mistake is. I also do not understand why the link function is only introduced in the appendix, because it is a key concept to link w_j^T x and P(y_j|x). To increase readability, I would advise to discuss this early in Section 2. I also do not understand what w_j represents in the case of tree-based models. More discussion is needed. For tree-based models, one doesn't have a weight vector per class, isn't it? I am also not convinced of the algorithm that is introduced in Section 3.2. The method is very ad-hoc, without any theoretical justification. As a result of pairwise terms, it might also be computationally challenging to optimize the proposed loss for extreme multi-label datasets. Isn't there a much simpler solution? Using the terminology of Section 2.1, one could simply improve the performance for tail labels by adjusting the threshold t for such labels only. Has such a simple solution been considered in literature? In that way one could fit standard probabilistic classifiers during training, following by a reasoning on probabilities in a post-training procedure. Similar to the approach of the authors, one could take label frequencies into account during this post-training procedure, resulting in a threshold t that depends on label frequency.  In the experiments it is not clear to me why only four XML datasets are used. Why were the other datasets in the XML repository not analyzed? Please provide a good motivation or analyze all datasets.  In this paper, the authors proposed a novel reparameterization framework of the last network layer that takes semantic hierarchy into account. Specifically, the authors assume a predefined hierarchy graph, and model the classifier of child classes as a parent classifier plus offsets $\delta$ recursively. The authors show that such hierarchy can be parameterized a matrix multiplication $\Delta \mathbf{H}$ where $\mathbf{H}$ is predefined by the graph. In addition, the authors further propose to fix the norm of $\delta$ in a decaying manner with respect to path length. The resulting spherical objective is optimized via Riemannian gradient descent.The strengths and weaknesses are very obvious in this paper.On the strength side:+ The paper itself is very well written. The notations are well defined and the methods are very clearly explained. The presentation is fluent.+ The proposed method seems novel and interesting. The derivations are technically correct.+ Experiments show performance improvement over baselines, especially on CUB200/Dogs/Cars.On the weakness side:- I think experiment presents the most significant weakness of this paper: 1) The comparison is rather weak without any reference to existing prior arts such as [1]. A simple search with respect to the 5 experiment datasets also show significant performance gaps between the proposed method and latest methods. 2) I remain skeptical about the solidness of the baseline performance as they show considerable gaps to standard baseline training without bells and whistles (https://github.com/weiaicunzai/pytorch-cifar100). 3) The performance gain diminishes very quickly on bigger dataset such as Tiny ImageNet. What about the results on ImageNet?- The proposed method depends on a pre-defined semantic hierarchical graph rather than a learned one, which potentially limits the technical value of this work. In certain cases, semantic hierarchy may not always be a reasonable choice to guide the learning of visual embedding.- I have some concern about the selection of initial radius $R_0$ and its decay policy. I think this parameter should be dataset dependent due to different numbers of categories and the densities of class distributions. As a result, how such parameter and policy can be optimally determined becomes a question.- Finally, forcing a fixed radius does not sound as reasonable as allowing a learnable radius with soft regularization.[1] Chen et al., Fine-grained representation learning and recognition by exploiting hierarchical semantic embedding, ACM-MM 2018 This paper studies a method to give upper bounds on the total number of linear regions in a deep ReLU network. These bounds are tighter than those previously obtained and they apply not only to fully connected networks but also to networks with skip connections.Strong Points: 1. Understanding the complexity of the function computed by a deep ReLU networks is of significant interest. 2. The authors state sharper bounds than previously available for this complexity, as measured by the number of linear regions. Weak Points:Many parts of the paper are not clear or imprecise:1. Definition 1 is incompatible with the definition P(f) of the linear regions of f. For example, consider a ReLU networks with a single neuron. The partition R^{n_0} into linear regions simply divides the input space into two half-spaces. Thus, the regions D of this function are the closures of these half-spaces and hence are not disjoint (they share the same hyperplane as their boundary). 2. The definition of RL(n,n) just before Definition 9 is not clear. Is there a bound on the number of neurons in the hidden layer? If one interprets containing & one ReLU activation function as having a single neuron, then it appears that (11) is a trivial histogram. If there can be any number of neurons, then in the definition 10 of when\gamma_{n,n} satisfies the bound condition, the max of H_a(S_h) over h in RL(n,n) appears to be a histogram whose entries are not well-defined since they are infinite. I am probably missing something but really didnt understand what is meant here.3. For a given ReLU network architecture, bounding the total number of linear regions in the worst case over all configurations of weights/biases is typically of no practical value (thought it is certainly an interesting problem in extremal combinatorics). The reason is that this number, even on average, is unbelievably large. This is true even in extremely simple networks. Take for instance a single hidden layer ReLU network in input dimension n_0, one hidden layer of size n_1 and output dimension 1. For generic configurations of the weights and biases, the number of linear regions grows like the sum of the binomial coefficients (n_1, j) as j goes from 0 to n_0 by Zaslavskys Theorem. So in the case when n_1 > n_0, for example, we get 2^{n_1} regions. This in no way reflects the fact that single layer ReLU networks often learn very simple functions. This is exemplified by the shocking numbers in Table 1 (where we see numbers like 10^{2956}).Recommendation: I found that this paper was difficult to read. The results could be interesting from the point of view of a theoretical analysis of deep ReLU networks, so I encourage the authors to rewrite the body of the paper to increase the clarity and precision around the definitions of linear regions. Ultimately, I dont think this paper is ready for publication.  This paper studied heterogeneous graph embedding with graph neural networks. The authors proposed an approach which samples multiple different subgraphs containing a subset of relations and then aggregate the node representations from different subgraphs with attentions. Experimental results on some big data sets prove the effectiveness and efficiency of the proposed approach. Overall, heterogenous graph embedding with graph neural network is a very important and interesting problem with a variety of applications. However, the novelty of the proposed approach seems to be quite marginal, and I am not quite convinced by the proposed approach. Why are we able to get better results by first sampling a subset of relations to get the subgraph and then aggregating the node representations from different subgraphs? It seems weird to be as a randomly selected subset of relations do not convey any specific semantic meanings as traditional metapath based approaches. I am also surprised by the very small standard deviation in Table 4, which is not consistent with results reported in existing literature on the tasks of node classification.  Summary: This paper talks about online continual learning in scenarios where there might be domain-shift during test time. Though I find the problem to be important, I believe that the solution proposed in this paper is straightforward (which is fine) and imposes a new set of constraints (knowing a priori the domain id during training) making the problem quite impractical for scenarios where continual learning might be useful. Please find below my comments in detail:1. Problem formulation: I do agree with the authors that task-free set-up (also known as single-head) is something we should focus more on, and like the fact that they remove this constraint of knowing task id during train/test. However, while relaxing this constraint, they added a new assumption of knowing the domain id during training. I find this rather a more strict requirement. The right problem formulation would be where there is a stream of samples coming (online) with a relatively blurry task boundary, and these samples might belong to different domains. Knowing domains a priori is very impractical in continual learning set-up. Could you please scenarios where its feasible to know domain id a priori? 2. Approach: While I understand the ER part, I do not understand clearly section 4.1. What is the need of all GAN literature here? Why do you call D_j a domain critic? Am I wrong in saying that the only role of D_j for class y_j is to apply cross-entropy loss over domains (clipping if needed)? Please correct me if I am wrong. The section was a bit unclear to follow. 3. How do you make sure that the assumption of data balance is valid given that the memory is very small compared to the current dataset. Also, it seems like that the memory sizes of 9k, 10k, and 34k are too big. Please comment. 4. Training time: Since the set-up is online, training efficiently is extremely crucial. In Algorithm 1, every parameter update requires |Y| extra backprops for the domain critics SGD step. Could you please comment on the training time?5. References: I would suggest the authors to correct their citations a bit. For example, when talking about task-free, I would also cite iCarl, synaptic intelligence (SI), and RWalk. Similarly, the forgetting measure used in this paper was not proposed in Chaudhry 2019a, it was proposed in RWalk. Overall, even though the problem is important, I find the final experimental set-up to be impractical. There already are too many impractical formulations for continual learning, I would rather refrain myself from encouraging a new one. On top of this, there are a few technical aspects (training time, memory budget etc) I do not completely agree with, therefore, would request authors to answer above questions for clarity. In this work,  the authors addressed the problem of Matrix Completion on Non-Euclidean domains.  They mainly adopt the method Matrix Data Deep Decoder, inspired by the Deep Decoder method for Image Completion. Here are my main concerns:1. The technical contributions of this paper is limited since it mainly adopts existing methods, like Matrix Completion + Deep Decoder + multi-graph convolution. It seems for me this paper is more like an engineering paper consist of lots of stuff. It is better to release the source code to understand this kind of composition stuffs.2. The motivation of this work is also not very sound. The authors spend lots of content on the related work. However, the main idea is only presented in a presuppose-code style in Page 6, which is not very clear.3. In the experimental parts. Why use different methods for different dataset in Table 2(a), (b), (c)?  Is it possible to conduct all of the baselines for all datasets? Also,  the matrix completion has been studied for many years with many solid theory results,  the baselines in the experiments are not SOAT.Typos:In page 4: stracture -------> structureIn page 7:  can be found in the work of [30] -----> the cite form is not correct.In summary, the paper show that a GCN network is a good prior for the Matrix completion problem. However,  the technical contributions of this paper is limited and the overall presentation can be further improved. Topic: Matrix Completion for Recommender Systemsmain contributionusing the idea of `deep prior for matrix completion.Strength-  performance: It seems that the method outperforms a number of baselines on different recommender system tasks. This suggests the usefulness of the proposed method.Weakness- Contribution Limited: The problem is formulated as graph Laplacian regularized matrix completion problem. This has been a long-existing technique. The contribution of this work lies in modeling the complete matrix within the range of a deep generative model and learns the deep generative model on the fly. GNN based link prediction is not a new technique. The work puts GNN based link prediction together with low-rank regularization, which is perhaps the reason why some practical benefits were observed, but the contribution is limited.In fact, it is very hard to see how much new contribution is there on top of the main reference Federico Monti, Michael M. Bronstein, and Xavier Bresson. Geometric matrix completion withrecurrent multi-graph neural networks. CoRR, abs/1704.06803, 2017. URL http://arxiv.org/abs/1704.06803.There may have been some regularization and network modifications, but the new contributions seem to be marginal.Is it possible to clarify what are the new contributions on top of the framework from the above reference?- Clarity and Soundness: -- Most of the effort was invested onto constructing the graph CNN that is claimed tailored for non-Euclidean matrices. But this part looks a bit theoretically (or even intuitively) ungrounded --- particularly, it is not easy to follow the rationale behind sec. 2.4. It is also unclear how is this construction suitable for handling non-Euclidean cases. This part may need some more explanations. -- the writing of the work is a bit convoluted or at least misleading. It tries to connect with deep prior based approaches, but the actual method is not really related. In a nutshell, the approach is a regression approach that maps embeddings of the row and column entities of the matrix to be completed to the ratings. The embeddings are constructed from graph Laplacians, and the mapping (regression model) is a particularly constructed neural network. It is suggested that these points to be clearly articulated.- Technical Depth: The regression formulation with the regularization and constraints are not outright unreasonable, but it seems that there is no further justification of the approach beyond intuition. The reason why the regression model/network should be constructed this way other than other ways is unclear and unjustified, even on an intuition level. The technical depth of this work may have large room to improve.- Simulation validation: It is unclear if the proposed algorithm converges or not, even on simple simulated data. It is suggested to show the algorithms numerical behaviors so that the readers have some sense about its convergence characterizations. The current version does not have such information.- Questionable Mathematical Representation: The last equation in page 3 looks a bit strange: if X is approximated by WH, then it is automatically rank limited (by the number of columns of W,H). Adding a nuclear norm on WH looks unnecessarily complicated.- writing: The writing seems to have been done in a hastily manner, with many typos (see Minor). Minor:page 2: ``non-convex (but still very well-behaved): what does it mean?page 2: two equations, min should be arg min?page 4  sec 2.4 stracturesec 2.3.2 better then most of the tables and figures seem to have very low resolutionsec 3.1 is very hard to read through  This paper introduced a log-barrier based regularization method to reduce the dynamic range of data types (activation, weight, error, gradient, and input) in neural networks. The authors claim that such regularization is important to avoid overflow or swamping in the accumulation of matrix multiplication. However, the reviewer is afraid to think that there are serious technical issues with this claim. Here are a few details regarding this concern.- The main claim of this paper seems to be that by reducing the dynamic range of the neural network data, they can avoid overflow. However, it does not seem to be (always) true; what matters more would be the resolution of information you need (or want) to keep, and the number of data elements you accumulate. E.g., assume that we use 8-bit -- even if the magnitude of data is constrained to be around 2^-3, if the resolution of info is 2^-8, there will be an overflow after 8 times of accumulation. Also, note that the magnitude of activation or weight of a layer is often not important due to scale-invariance of the normalization techniques (like batchnorm). Thus it is not clear if constraining the magnitude of data is always beneficial.  - The authors claim that previous reduced-precision training techniques (like [Sun et al., NeurIPS19]) have dynamic quantization range across the layers. This is NOT true, since the reduced-precision floating point format has the fixed dynamic range; e.g., [Sun et al., NeurIPS] employs (1-5-2) format for representing back-prop error, implying the fixed dynamic range of 2^(-15) -- 2^(16). In fact, one very compelling benefit of using the reduced-precision floating-point over the fixed-point format is that the floating-point format does not need scaling of the magnitude across the layers. - It is not clear if the authors' understanding of "overflow" or "swamping" is the same as what is reported in prior work (e.g., [Sun et al., NeurIPS19]).  First of all, overflow and swamping issues are different - the former issue is for the fixed-point accumulation where the accumulated value wraps around to the small value when it exceeds the largest value representable by the given accumulation precision. Whereas, the latter is for the accumulation of floating-point numbers, where the small magnitude value is ignored (or truncated) when it is added to the large magnitude sum. Thus, it is very confusing when the authors use these two different terms interchangeably in the text (e.g., the 3rd paragraph of Intro). - There are many confusing points in the analysis of the overflow in Sec 3. First of all, the authors seem to claim that Prob(z > 2) is the overflow condition... but why? I couldn't find the notion of bit-width in this analysis... but then how can we check the overflow?- The authors mention that they employ STE, which does not make much sense in case of the quantized training; what does it mean to take STE for the quantization of back-prop error?- It is not clear how the proposed method is evaluated. The computational graph in Fig 2 does not seem to include the accumulation quantization. Also, more detailed information should be provided about the following; how the MU8 format is implemented to use 8-bit hardware? how the proposed method is implemented in the deep learning framework?To sum up, this paper seems to have several technical flaws, which should be carefully addressed.  This paper describes a new method for learning factored value functions in cooperative multi-agent reinforcement learning. The approach uses energy-based policies to generate this factorization. The method is presented and experiments are given for smaller domains as well as starcraft. The idea of learning factored value functions is promising for learning separate value functions for each agent that allow them to learn in a centralized manner and execute in a decentralized manner (centralized training and decentralized execution). Several methods have been proposed along these lines, but as the paper points out, they have limitations that makes them perform poorly in some problems. The proposed approach in this paper has some promising experimental results, but there are questions about the novelty and significance of the method. Furthermore, evaluating these contributions is difficult due to the lack of clear details in the paper. In particular, the details of the approach itself in 3 are not clear. Starting with Definition 1, it seems like IGO is using an optimal *centralized* policy. Is this what is meant? If so, why is this needed (as opposed to an optimal decentralized policy). It will typically be impossible to achieve a centralized policy with decentralized information. Furthermore, the energy-based policies are defined in 3.2, but 'key' ideas such as approximating the weight vector aren't fully explained making the exact approach hard to determine. Also, it is beneficial that the current theorems and proofs are included, but the lack of sufficient detail makes it hard to parse and evaluate them. There are also similar max entropy approaches, such as the paper below. Iqbal, S. & Sha, F.. (2019). Actor-Attention-Critic for Multi-Agent Reinforcement Learning. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:2961-2970As well as other factorized methods, such as the papers below (which are admittedly new). Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. Tabish Rashid, Gregory Farquhar, Bei Peng, Shimon Whiteson. NeurIPS 2020.de Witt, Christian Schroeder, et al. "Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control." arXiv preprint arXiv:2003.06709 (2020).The paper should discuss how the proposed method is an improvement over this other work and have a more comprehensive related work section. The experiments are promising, but the relevant related work is not included and there isn't sufficient detail describing how the methods were run and discussing the results. In terms of comparisons, the paper should also need to compare with non-factored state-of-the-art methods. It is, of course, natural to compare with other factored methods, but what matters is general state-of-the-art performance of the domains. As noted, the clarity and writing of the paper should be improved. Beyond the examples above, some other instances are below. - If the reader doesn't already understand the relative overgeneralization problem, Section 2.3 probably isn't sufficient. Figure 1 is helpful, but it should be described in the text to make the issue clear. - The connection between the overgeneralization problem and factored representations isn't completely clear. Factored representations have problems because they typically cannot represent the optimal value function (or policy). That is a separate issue than getting stuck in a local optimum (which can happen with any type of method). 1. SummaryThe paper addresses the region proposal network (RPN) as a key component in few-shot object detection. Their analysis is centered around the concept of "proposal neglect": objects in the meta training set are ignored because no proposals are generated for them. They propose CoRPN, an assemble of RPN classifiers, to tackle the problem and present results on the original few-shot COCO and Pascal VOC benchmarks as well as ablation results.2. Strengths + I really like the general motivation and the RPN is for sure a key component for good few-shot object detection.+ The proposed model and especially the diversity loss seem well chosen to address "proposal neglect".+ The method brings a small but consistent performance improvement over TFA.3. Weaknesses- I think the main flaw of the paper is that there is no quantitative analysis of "proposal neglect", only some qualitative examples in the appendix. I don't want to say that it does not exist it makes only sense that such an effect messes up meta training. But I am missing experiments demonstrating the effect and it's impact on performance. - If proposal neglect is such a big issue for meta-training, why not simply add the ground truth boxes as proposals? Many object detection frameworks have that option built in. Comparison to this baseline seems crucial to evaluate the value of the proposed method.- The new hyperparameters (N-rpn, phi, lambda_c, lambda_d) are apparently not fixed but chosen in a per-experiment basis as only ranges are provided in the appendix, not exact values. This makes it hard to judge the actual value of the method as hyperparameters have to be chosen in advance in real-world applications.- I find the ablations lacking for a number of reasons which is critical as this should be the section that allows assessing the impact of "proposal neglect" and the proposed method. Specifically I have three issues: -- I am not sure the proposed experiments are suited to grasp the effect of the original problem and the effect of the method. How does the method impact RPN recall? What about classification accuracy, false negatives etc.? Table 6 does attempt to provide this information but I find it hard to understand what exactly is reported. Probably what I am interested in is already there it's just not that clear yet?-- Ablations have only been performed on Pascal VOC and not on the more interesting and more challenging COCO dataset. Especially Table 6 would be interesting for COCO as well.-- Ablation results appear to be extremely noisy. Compare the differences between configurations in Figure 4, adding or removing a classifier should not lead to a 5% performance change especially if this effect is not consistent across splits and changes again when another classifier is added or removed. To be clear this is not the authors fault but a problem of the setup introduced in Kang et. al. 2018 using a fixed set of images for meta-training. This approach makes overfitting to the specific chosen examples a big concern. Now I think it is good the authors follow the established evaluation protocol but this design makes it rally hard to examine if results are valid. In the case of the presented ablation studies this effect definitely casts a large shadow of doubt on the significance of these ablations. 4. RecommendationI think in it's current form this paper has to be rejected. At first I was excited to see someone addressing proposals in few-shot object detection but the paper sadly only addresses "proposal neglect" without any quantitative analysis or consideration of other potential problems. Furthermore due to the potentially large variability of results due to the fixed meta-training set (see variability in Figure 4) and potential hyperparameter variations between experiments it is hard to judge if the reported performance gains are robust and could be transferred to real-world applications. I do feel an investigation of the RPNs role in few-shot object detection is very worthwhile and the proposed method may in fact be valuable, however to make either of these two contributions a deeper analysis is needed.5. Questions/Suggestions- Provide quantitative analysis of the "proposal neglect" problem. Even better: Do a throughout assessment of the RPNs role and problems in few-shot object detection.- Compare to a simple baseline: Adding the gt bounding boxes to the proposals during meta-training- Please clarify if hyperparameters were the same for all experiments and report the chosen values in the main paper, not the appendix.- Run ablation experiments on COCO as well- Better explain Table 66. Additional feedbackNone, the paper is overall well written, figures are clear and understandable.  Paper Summary:This paper aims at discovering transferability of perturbations across different environments in RL. The authors propose some different types of advasaries and tested those adversaries on 6 atari games.Review Summary:The idea of analyzing perturbations' transferability is interesting, but it is hard to say that the approaches are satisfactory. The authors did not propose any novel algorithms or modifications based on existing algorithms, and provide only preliminary experiment results. As such, I suggest a clear rejection.Detailed Comments:Section 3.1, paragraph 2. "Such a model... of the deep reinforcement learning agent". I am hardly convinced by this statement and led by this statement, the proposed approach in this work. When we consider transferability in machine learning, we assume that there are something common to learn between the two domains. My first thought when I read the title is that it would be a transfer learning method for adversarial attacks. But the authors seem to prefer a universal offset on the raw pixel input. In this case, the only insights we might possibly learn is the input similarity between different Atari games, which is hardly a contribution to any field. Moreover, since adversarial attack is already used in this work, I don't see the reason why none of the referred methods for adversarial attacks in RL are tested.Besides, all experiments are done only on DDQN, hence the claims are hardly validated. What I expect is either a new algorithm that is more resilient or a comparison on robustness between different RL algorithms. It is also not explained how the environments are chosen. The paper proposed a method to address the distribution mismatch problem in off-policy learning. Its method is to estimate the density ratio between the stationary distribution under the target policy and the data distribution in the replay buffer, and then correct the state distribution with the ratio so that it can use experience from the replay as if they were generated from the target policy.Although I have many questions about the paper, the following two are most important currently and are hoped to be addressed first.It seems not clear to me whether this paper considers the tabular case or the function approximation case. It wrote in section 5 that "our goal is to improve performance of TD learning with function approximation", but its preliminaries (Section 2), its example (Figure 1), and its main theoretical result (Theorem 1) are all for the tabular case. In fact, the state distribution mismatch only matters when multiple states share the same parameter. For the tabular case, as each state has its own parameter, there is no interference between them, and thus there is no need to make the state distribution on-policy.Empirically, all the experiments are designed to solve continuous control problems, where both state and action are continuous. However, the density ratio d^\pi(s, a) / d^D(s, a) is only defined under the discrete setting. In fact, because both state and action spaces are continuous, all elements in the replay buffers are different from others and this ratio makes no sense. Therefore this algorithm doesn't seem appropriate to me for continuous control tasks.  **Summary of work**They propose an epistemic uncertainty estimation method (though itis not really estimating the uncertainty, but just whether a datapointexisted in the dataset or not). The proposed method samples newpoints $x_{epi}$ in the vicinity of the training data points $x_{tr}$from a Gaussian distribution (they set the covariance to $I$).Then it creates a dataset where the targetis 1 for $x_{epi}$ and 0 for $x_{tr}$ (with some tweaks on removing Ndatapoints from $x_{epi}$, which were the closest to training data points,but the main principle is roughly the same). Then they train a network topredict this target, and this estimate is called $\eta$.They compare their new model against Gaussian processes, Bayesian neuralnetworks, Dropout uncertainty estimation on some datasets. To evaluate,they propose a new weighted mean squared error metric, where the mean squarederror is weighted by $1-\eta$ and the mean is taken w.r.t. the sum of the$1-\eta$ terms. For the other methods, $\eta$ is computed by dividing theuncertainty for the prediction with the largest uncertainty in the data.Their method outperformed the other methods using this metric, except forGPs on some datasets, but they said that GPs do not scale, and are notsuitable for real-time applications.They use these networks in a quadcopter simulation control task. Their epistemic uncertainty estimation was used to decide whether to add adata point to their data set (if the epistemic uncertainty is high) orto discard it. The performance of their method including a disturbanceestimation model that uses their EpiOut approach improved the trackingperformance compared to not using any disturbance estimation model at all.**Strengths and weaknesses**Strengths:- The exposition was good.Weaknesses:- It is not clear to me how to pick the covariance for the sampling distributionof $x_{epi}$.- It is not clear to me that there is a good justification for the method.- It is not clear to me that using the largest uncertainty in the datasetfor computing the $\eta$ values for the other methods is appropriate. What ifthe other method just predicts an infinite uncertainty somewhere? It seemsarbitrary to use the maximum, and it may be biasing the results to favorthe newly proposed approach. I have not seen this metric used anywhere else.- The implementations use different frameworks, e.g. the GPs are based onGPy using numpy (where there are other frameworks e.g. GPyTorch, Pyro,or GPFlow that allow using GPUs), the BNN, dropout and EpiOut use tensorflow.- The use in the quadcopter task was unclear to me. Why not just store allof the data, or use each data point for a gradient update of the model, thendiscard it?- As far as I understood, in the experiment on the quadcopter the comparisonwas between not using a disturbance model at all, and using a disturbancemodel with EpiOut. It would be better to remove only the EpiOut componentto show that the uncertainty estimation was necessary, rather than justany arbitrary disturbance estimation model.- It seemed the number of data points gathered was around 150, which shouldbe easily handled with GPs, so the task does not seem to suggest that GPswill not scale to it.- The paper made up a new set of benchmarks for everything, and never testedon an existing benchmark to prior works.**Recommendation**I recommend rejecting the paper with the main reason that I did not seeany principle behind the newly proposed method, and how the $x_{epi}$ datasetcould be constructed in a sensible way. Currently the covariance of theGaussians seemed to be set arbitrarily, but there should be a principledway to set this covariance. Unless I have greatly misunderstood something,I do not see how the paper could be modified for me to recommend it foracceptance. I was also not convinced by the proposed quadcopter application forthe binary uncertainty estimation, and the other weaknesses listed abovealso make me tend towards suggesting to reject.**Questions**Please clarify if there were misunderstandings.**Additional feedback**Below are some raw notes and additional thoughts from when I was reviewingthe paper."However, sampling the network during inference is a high computationalburden and is therefore not suitable in real-time critical controltasks."Really? With parallel computing it may not be that heavy. Do you have anyevidence for your claims? I see that in the appendix you have somecomputational times, but it would have been good to point to this in themain text."An extension to heteroscedastic GP regression is presented in(Lazaro-Gredilla & Titsias, 2011), how-ever, it is a variationalapproximation and further increases the computational complexity ofGPs,which is prohibitive when employing them for large data sets."As far as I know, it is basically training two GPs, and the computationalcomplexity is the same, and differs only by a constant factor. Do youhave more evidence for your claim?"preformed" -> performed"considering a new data point takes $O(N_{tr}^3)$ computations"No, I believe it takes $O(n^2)$ computation to add one new data pointusing block matrix inversion.Why was GPy used, and not some other framework with GPU support such asGPyTorch, Pyro or GPFlow.The Bayesian neural network was not clear, because it was a link to ablogpost with many codes including numpy and tensorflow. (the codecleared this up though)"The measure in equation 6 relies on epistemic uncertainty predictionin the interval[0, 1].  This is only ensured for theproposed EpiOut approach and therefore the uncertainty measures providedbythe GP, Dropout and BNN are scaled to the unit interval based on theevaluation on all test points."Is this a fair evaluation method? If one of the other methods gives aninfinite uncertainty on some point, this will negatively affect its entirescore, even though the uncertainty predictions for the other points maybe fine. The evaluation method does not appear sound to me, and noreferences were provided for other works using such an evaluation method.Being only confident close to the data trajectory may not be the bestapproach. The model should be confident when the prediction is likely tobe accurate. For example if the model is smooth, then it should bepossible to interpolate reasonably accurately, and it should be OKfor the model to be confident as long as it has been able to learnthe structure sufficiently well. The authors show that adversarially trained models have higher shape and simplicity biases compared to their vanilla counterparts.The paper is well written and easy to follow. I think I understood most parts of the paper. The authors were explicit in stating which methods they used which is good.My major concerns are:1.Lacking novelty/ comparison to previous work: The authors did not compare to previous work properly. In my general comments, I suggest adding and discussing 11 additional publications. One of their major findings that adversarially trained models are invariant to additive high frequency noise has been shown and discussed in [8] which they did not cite. Other cited papers are cited in a wrong way, e.g. Geirhos et al 2018b did not perform experiments on adversarial examples; there are other examples in the General comments section below.2.Not relevant/ not surprising results: I am not sure how useful and interesting the finding is that adversarially robust models are more shape biased. It seems intuitive that given that adversarial perturbations are high-frequency, becoming invariant to them should make a model more attentive to low-frequency features such as shape.3.Lacking discussion/ analysis of the results: In the first experiments section, the authors found that adversarially robust models are shape biased. With the NetDissect analysis, they showed that there are more filters in adversarially robust models compared to vanilla models that detect textures. These two results seem contradictory to me and should be discussed. If they are not contradictory, this should be explained.4.The number of PGD steps used for the adversarial attack is 7 which is very low. I am not sure whether the results in this paper are meaningful in the sense that the models might not be adversarially robust at all if a higher number of PGD steps/random restarts were used. The authors should test bigger numbers of steps and make the step size dependent on the number of steps.General comments:Page 2 In contrast, there was also evidence that classifiers trained on one type of adversarial example often do not generalize well to other image types [Geirhos et al, 2018b, Nguyen et al., 2015]. What is image types here? Both references do not provide any support for the claim made in this sentence. In Gheiros et al, the authors train on different types of image corruptions such as additive uniform noise or using high/loss pass filters. They do not show results for adversarial examples. Nguyen et al was published two years before adversarial training was proposed by Madry et al. There are other references that would be good to cite here though: [1] show that increasing robustness against adversarial attacks does not increase robustness against translations and rotations. [2] show that adversarial robustness does not transfer easily between attack classes. [3] also show that there is a performance trade-off between robustness types.Page 3: Adversarial examples were generated using Projected Gradient Descent (PGD) (Madry et al., 2017) with an L2 norm constraint of 3, a step size of 0.5, and 7 PGD-attack steps. A number of 7 PGD steps seems very very low. A more sensible number might be e.g. 1000 [4] or at least 200-400 steps. In [5], the authors urge to check the convergence of the attack and that white-box attacks generally converge in under 100-1000 steps. Additionally, Carlini et al [5] write that the number of iterations necessary is generally inversely proportional to step size and proportional to distortion allowed. -> Here, a step-size of 0.5 is chosen without providing any reasoning and the choice seems arbitrary.Table 1: On the torchvision website, the top1 accuracies for the S models are given as: AlexNet: 56.55%, GoogleNet: 69.78%, ResNet50: 76.15%. These numbers differ from what the authors show in Table 1. Working with the pretrained torchvision models myself every day, I know that a ResNet50 from torchvision will get a top1 accuracy of 76.13% (same to what is stated on the used robustness package repo of Engstrom et al). Where does the difference to the numbers in Table 1 come from?Page 4: R models show no generalization boost on ImageNet-C i.e. they performed on-par or worse than the S counterparts (Table 3c). This is consistent with the findings in Table 4 & 5 in Geirhos et al. (2018a) that a stronger shape bias does not necessarily imply better generalizability. -> This is completely opposite to what we see in Table 5 in Geirhos et al. 2018a. There, the authors attribute the higher robustness to common corruptions to the higher shape bias of their SIN model: Again, none of these corruption types were explicitly part of the training data, reinforcing that incorporating SIN in the training regime improves model robustness in a very general way. Due to the contradictory results, it rather seems that shape bias and corruption robustness might be more disconnected than previously thought. Also, the result that adversarially trained networks perform badly on ImageNet-C has been observed in [6] which might be good to cite here. Page 4: However, this bias is orthogonal to the performance when only either texture or shape cues are present. I dont understand why it would be orthogonal? I would expect a model with a strong shape bias to care less if I remove texture cues from an image compared to a model with a strong texture bias. What exactly is new in this test compared to the cue conflict task?Page 6: To confirm this hypothesis, please write to test this hypothesisPage 6: That is, the smooth filters in R models indeed can filter out pixel-wise Gaussian noise despite that R models were not explicitly trained on this image type! The finding here seems very related to [7] and [8]. In [7], the authors show that adversarial training improves robustness to Gaussian noise and in [8], the authors study the Fourier properties of common corruptions and show that adversarial training improves accuracy for corruptions in the higher frequencies.Page 6: Our result suggests that most of the de-noising effects take place at lower layers (which contain generic features) instead of higher layers. I do not understand how this conclusion was made given the results, please explain.Page 6: We found a consistent trendadversarial training resulted in substantially more filters that detect colors and textures (i.e. in R models) in exchange for fewer object and part detectors. This result seems surprising and not consistent with the finding that adversarially robust models are more shape biased. If shape bias is indeed correlated to the filter responses (which is not necessarily clear), I would expect filters of a shape-biased model to rather detect objects and parts instead of textures and colors. Please discuss this. It would also be nice to test the SIN trained model from Geirhos et al 2018a, since it has a high shape bias.Page 7: As R hidden units fire for fewer concepts, i.e. significantly fewer inputs, the space for adversarial inputs to cause R models to misbehave is strictly smaller. I believe this is meant to be an explanation for why adversarial training works. I dont understand the argument well, but in [9], the authors show that adversarially robust models are better for transfer learning than their vanilla counterparts. Intuitively, a broad set of (unique) features must have been learned by the adversarially robust model for this task. I would like to see a discussion comparing these results to [9]. I also think that the analysis is not sufficient to claim that the space of adversarial examples is smaller for R models.Page 8: For example, our preliminary results showed that encouraging S networks to have smoother kernels in early layers improves CIFAR-10 ResNet-18 network robustness to adversarial and noisy images. This has not been shown in this paper/ reference missing.Suggestion: It would make the work stronger if the authors considered more adversarially robust models, e.g. [10], [11].Please fix the references: I noticed that the authors cite the arxiv version for [Geirhos et al 2018a]. This paper has been presented at ICLR 2019 and this should be reflected in the references. Please check and fix all other references as well where this critique applies.References:[1] Kang et al: Transfer of adversarial robustness between perturbation types[2] Jordan et al: Quantifying perceptual distortion of adversarial examples[3] Tramèr et al: Adversarial training and robustness for multiple perturbations.[4] Tramèr et al: On Adaptive Attacks to Adversarial Example Defenses. [5] Carlini et al: On Evaluating Adversarial Robustness[6] Rusak et al. A simple way to make neural networks robust against diverse image corruptions.[7] Ford et al: Adversarial Examples Are a Natural Consequence of Test Error in Noise[8] Yin et al: A Fourier Perspective on Model Robustness in Computer Vision[9] Salman et al: Do Adversarially Robust ImageNet Models Transfer Better?[10] Xie et al: Feature denoising for improving adversarial robustness[11] Shafahi et al: Adversarial training for free! SummaryThe author presented Linguine as a subgraph GCN training framework to train GCNs on large graphs. It includes bootstrapping and smart pruning algorithm to improve the quality of GCN models. In each forward pass, smart pruning algorithm prunes nodes to extract a concentrated smaller subgraph from the large subgraph randomly sampled previously. Strengths+ It is good to visualize the Pruning example.+ It is good to include rich baseline methods.+ well organizationWeaknesses- I am not convinced regarding the nodes with high training loss as redundant nodes. From my understanding, redundant nodes mean the nodes carry redundant information. Apparently, nodes that are hard to learn by the existing models bring richer information rather than bring redundant information.- GCN prone to overfit the error (error with low degree can easily get low training loss), the proposed methods will aggravate the negative impact brought by the error.- From my point of view, the nodes with the highest degree are the most import node which affects message passing among nodes in the graph the most. However, it seems that smart pruning tends to prune these important nodes instead.Questions:Since vanilla GCN is non-convex, I dont see why we need the proof of the theorem on the convergence on joint training scenario in the convex scenario. This work proposes a new framework, called LINGUINE, to produce high-quality sub-graphs that can assist in effective training of graph convolutional networks (GCNs) with a lower computational cost. The framework uses two consecutive components - (1) Bootstrapping, which learns a meta-model that can assign weights to nodes by being jointly trained with a proxy light GCN model (2) Smart pruning, which uses the learned meta-model to perform pruning on the large graph as the GCN model trains on it using the student-teacher learning concept. The paper presents results that the bootstrapping method improves the accuracy of smaller GCN models on large graph classification tasks. The smart pruning method removes nodes that are hard to learn or have a high degree of neighboring nodes in the subgraph, thus reducing error. Strengths: + The idea of performing smarter subgraph selection via meta-learning is very interesting. Sub-graph selection is an important problem in the domain for training GCN models on large graphs + The paper presents a theorem to show that under a convex setting, the proposed join training during bootstrapping is guaranteed to converge+ The benchmarking results illustrate that the bootstrapping step with a 2 layer SageNet architecture performs similar to the much bigger DeepGCN model on the OGBN-Product (Amazon product co-purchasing graph) dataset+ On the OBGN-protein dataset, the results demonstrate that the smart pruning step can improve model performance by selecting sub-graphs without nodes  that are hard to learn or have a high degree of neighboring nodesThe paper also presents visualizations and results on batch size effect and pruning ratios on the model performanceWeaknesses:  + My understanding from the method section is that LINGUINE is a framework that combines the meta-model learning and smart pruning to produce high-quality subgraphs that can be used to train a GCN effectively. However, the benchmarking results have been performed for the bootstrapping step only, instead of the whole framework. Since the output of this step (according to Algorithm 1) is meta-model, how are the F1-scores being reported in Table 1? + What is the architecture of the light-weight GCN? How is it used in reporting the bootstrapping results?+ The paper states that the sampling algorithm is user-defined. Is this suggesting only a node sampling algorithm? Also, while it is mentioned that a random node algorithm is chosen, the results also have a random-walk algorithm. How do the two compare in terms of cost-effectiveness? Since one gives better performance of the other. + The reasoning for missing DeepGCN F1-scores for 4 datasets is not provided.+ The paper describes results on the OBGN-protein dataset, for smart pruning. This is a different dataset from the ones for which the F1-score has been reported. The justification for this choice is missing. Similarly, the visualization results have been presented for Yelp data, and pruning ration graphs in the appendix for Flickr. These results should be presented consistently for one/multiple datasets. + For a paper trying to solve the computational cost issue of training on large graphs, there are no results comparing the numbers related to memory or time for LINGUINE with other baselines. The discussion on such a tradeoff would be very useful to the end-user. +There are multiple typos and grammatical errors in the paper that would require thorough reading and revisions. Minor comments: - Figure 1 is not intuitive in demonstrating different sampling strategies- In Figure 2, the role of the light-weight Graph is missing- Is there a reason to choose accuracy instead of F1-score for Figure 3(a) This paper tries to interpret PER from the lens of replaying experience that has the most "surprise" and shows how it connects to some notions of value such as the expected value of the backup and policy improvement value and evaluation improvement value and argue. The authors also derive a max-ent version of this and show that this can improve performance on some Atari games (though this is not that convincing).My score for this paper is based on these points: Motivation: I do not see the motivation for introducing these metrics and why that explains PER in the first place. Agreed that PER is a reasonable choice, and it can upper bound the EIB and EVB metrics (i have issues with this too, more on this next), it just seems to me that the paper doesn't make any convincing claim for why this helps us understand why PER works. If the focus of the paper is on understanding PER, then the paper does not do a good job of it. If it is to introduce these prioritization based on these metrics -- and the paper focuses entirely on them -- then I then have several concerns next.- Definition of the value metrics: The cited definition of these metrics requires using the true Q-function or the true value function of the resulting policy. If we end up approximating this using the learned model, what is the guarantee that these metrics are indeed useful? Also theorem 1 should be restated to say that they care about the "empirical" EIB and EVB, that is computed using the learned Q-function, else it doesn't make sense to me. Moreover, if the TD error is a bound (which I think isn't with neural networks as I discuss in the next point) on the empirical EVB, can't I just drastically overestimate Q-values and get a larger empirical EVB value to be super high and prioritize on those examples? Why is that good? Why won't that promote overestimation? - Why is the update on the Q-value assumed to be tabular if the experiments are with a deep network on Atari? In a non-tabular setting Theorem 1 does not hold so either that should be rederived for the case of DQN or the experiments should be adjusted to do it on tabular settings.   In any case, now it is not clear to me why the method works with DQN, since the update in this setting isn't equal to $Q(s, a) \leftarrow Q(s, a) + \alpha TD(s, a)$. In general, the solution isn't known with neural networks, so the upper bound story doesn't hold there. With the NTK (Jacot et al.) assumption, I can obtain a somewhat similar update but pre-conditioned with the kernel Gram matrix (see Achiam et al. Towards characterizing divergence in deep Q-learning). However, Theorem 1 doesn't hold anymore now. So, it is unclear why the method works.- Even if I were to look at the experiments only, the results are not that impressive. The method is generally close to PER, and maybe a little better, but no comparison is made on a more efficient method such as Rainbow, and there are only 9 Atari games, which is too little. So, that is not super convincing yet.I would suggest the authors make some of the changes above. #### DESCRIPTIONThis paper considers the representational ability of normalizing flows in terms of their overall size (depth, no. of parameters etc) and how they choose a partition for coupling layer transformations.#### DISCUSSION  While I think exploring explicit limitations of the representational ability of normalizing flows imposed by the invertible constraints and bipartite partitioning of coupling layers, I'm not sure that (i) the specifics discussed in this paper get to the heart of the matter, and (ii) the points that *are* made are hard to understand and stand behind. Specifically, I have the following main concerns:"Empirically, these models seem to require a much larger size than other generative models (e.g. GANs) and most notably, a much larger depth." -- what does it mean for one generative model to be "bigger" than another, or to have "larger depth"? Is it number of parameters, or memory cost? The number of transformations in a flow and the number of layers in a GAN are not really comparable. You could also argue that normalizing flows could be adapted to have a memory cost constant in their depth, since their activations can be reconstructed on the fly for backprop. "Question 1" -- the answer to this is yes, example: in R^2, the function f(x1, x2) = (x1, x1) (where x = (x1, x2) ~ N(O, I)) induces a distribution on the line x2 = x1 which cannot be represented by an invertible mapping. What you really mean to discuss is *to what extent* enforcing invertibility impacts representational ability, not *whether* enforcing invertibility impacts representational ability, because it does. I am finding it very difficult to understand what Theorem 1 is saying, other than that it's in some sense trying to formalize the fact the invertible functions impose representational constraints. Specifically: What does it mean for a network to have size O(k)? What is k? It seems to be defined as k = o(exp(d))? What does that mean? What does it mean for the depth to be k / p = o(exp(d)) / no. of parameters? Why is the number of parameters per layer p the same for every layer? It seems to me as if the statements here are vague and not well-defined."Question 2" -- I don't understand the point of this question. Why is it worthwhile to have a result about how many affine coupling layers with a fixed partition are needed to compensate for the removal of a 1 x 1 convolution? From the Glow paper, the 1 x 1 convolution added increased the parameter count of their model by 0.2%, and the wallclock time for training increased by approximately 7%. Neither of these are prohibitive costs which would warrant careful examination of the necessity of the 1 x 1 convolution. Like Theorem 1, I'm also not sure of the meaning/point of Theorem 2. In particular, "any applications of permutations to achieve a different partition of the inputs can in principle be represented as a composition of not-too-many affine coupling layers." How many is not-too-many? Even if we could be specific, why does it matter that we can compensate for the removal of 1 x 1 convolutions with more affine coupling layers? Sections 4 & 5 & 6: I'm finding it very difficult to parse what's going on here. The sections are technically dense, draw on a wide range of formal results in passing, and it's hard for me to understand how the material is relevant or necessary. Moreover, why are there extensive proof 'sketches' in the main text at all? Where are the actual proofs?The experimental results are toy, and I'm not entirely sure what they're trying to show. What exactly is happening with the padding? As in Huang et al (2020), are the authors now doing variational inference in an extended space? None of this is clear. It would be remiss of me not to mention the fact that the paper has altered the margins of the ICLR template to increase the amount of material. For better or worse, an 8-page conference paper is the widely used format for machine learning publications. While I'm sympathetic that sometimes this can be an undue constraint, it provides a level playing field, and sharpening a message to fit within imposed limits is part of research. Deliberately altering the margins (i) makes the job of a reviewer more difficult because while reviewing they have to reason about which parts of the paper will be removed from the main text to fit in the final template, and (ii) removes the need for the authors to reflect on whether their message is focused enough. Content aside, the structure and layout of this paper could be significantly improved -- the abstract does not need to be multi-paragraph, the sweeping technical details are introduced quickly and poorly motivated, and I found the overall narrative confusing and difficult to follow.  #### EXTRA NOTES"we can efficiently evaluate the likelihood of a data point" -- the likelihood is a function of the parameters, not data."which model distributions as pushforwards of a standard Gaussian" -- the base distribution in a normalizing flow is not necessarily Gaussian. In section 2.2, computational constraints are not necessarily the reason for not using invertible weight matrices and invertible pointwise nonlinearities -- the O(d^3) determinant calculation can be sidestepped by parameterizing the weight matrix using e.g. an LU decomposition. #### CONCLUSION As I mentioned at the beginning, I think an examination of the representational limitations of normalizing flows in terms of their overall size and how they are affected by a chosen partitioning may be worthwhile, but I feel as if this paper doesn't adequately address these questions, and what it does say is difficult to understand.  The paper attempts to analyze the softmax cross entropy loss and discuss useful properties it has in an intuitive and accessible way. The paper mainly presents comparisons between this loss function and two other (very standard as well) loss functions that are the "sigmoid cross entropy" and the "squared error" loss functions. It is very interesting to read the paper and realize that the topics discussed here have been written in well cited and well established statistics and ML books for decades. The softmax cross entropy loss is the multinomial regression loss while the sigmoid cross entropy loss is no other than the standard binary  logistic regression loss and finally the "squared error" is the least squared loss. Authors compare the performance of these three on a few benchmarks. I think it's good to read the "Elements of Statistical Learning" and get a clear exposition to what science has been up to for decades before getting too deep into deep learning.  This paper proposes a method, Differentially Private Graph Generative Nets (DPGGAN), to release graph in way the preserves utility while preserving privacy. This method trains a deep graph generation model in a differentially private manner, by injecting Gaussian noise to the gradients of link reconstruction module, thereby claiming to guarantee edge privacy, while ensuring utility via a structure learning component based on a variational generative adversarial network (GAN) architecture, to enable structure-oriented graph comparison to the original.The motivation of the work is sound, yet the work does not deliver on its main promise: while the claim is made that link privacy is protected, there is no experiment to justify this claim. While an experiment is mentioned that purportedly does this, the mentioned results are not shown, neither in the main paper, nor in the appendix. The closest the work comes to reporting such results is this statement:  "... links predicted on the networks generated by DPGGAN are much less accurate than those predicted on the original networks (26%-35% and 15%-20% AUC drops on DBLP and IMDB, respectively) as well as the networks generated by all baselines."These results are not presented, and the privacy parameters µ that lead to them are not discussed. Thus, it is hard to know how much utility has to be sacrificed for the sake of the privacy gains mentioned. Further, a drop of 15% does not corroborate the claim that the released data useless for link prediction.Overall, while the paper claims to offer robust privacy guarantees towards various graph attacks, such guarantees are not explicitly spelled out. It seems to be taken for granted that a differentially private link reconstruction provides the intended guarantees, yet that begs the question of what protection is achieved in practice, under a learning-based attack. The fact that learning under differentially privacy can be surprising successful, and thus constitutes an attack on differential privacy, has been established in previous work [1]. This paper should reflect more thoroughly on what privacy means in the proposed setting, how it is shown, and what utility it corresponds to. On the other hand, there have been efforts to define explicit guarantees regarding link reconstruction [2, 3], which this work does not take in consideration.Incidentally, the paper claims that graphs lack efficient universal representations, citing [Dong et al., 2019]. It is not clear how the cited paper, which studies a form of universal graph representation, supports this statement. Besides, there is work explicitly dealing with efficient universal graph representations [4], based on similar principles to those studied in the cited work.Last, previous work [5] has already established, to a higher degree that it is done here, that any kind of structural identification attack can effectively be prevented using random edge perturbation, even while important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated on the perturbed data. Given these results, it is unclear what this work adds to what has been already established.References:[1] Personal privacy vs population privacy: learning to attack anonymization. KDD 2011.[2] k-isomorphism: Privacy-preserving network publication against structural attacks. SIGMOD 2010.[3] L-opacity: Linkage-Aware Graph Anonymization. EDBT 2014.[4] NetLSD: Hearing the Shape of a Graph. KDD 2018.[5] Delineating social network data anonymization via random edge perturbation. CIKM 2012. This paper proposes to learn a Gaussian Mixture Model of the distribution of returns and use it as the critic in an actor-critic RL agent. From what I can tell the principal novel contribution of this work is the Sample-Replacement method, in particular the observation that when paired with a GMM the replacement can be done at the level of modes of the mixture instead of individual samples. Another potential contribution is showing that the GMM can be optimized using the Cramer metric, although obviously this metric has been fairly widely studied previously.However, this work has several problems that make it unpublishable at the moment. I'll begin with the least severe (lack of clarity and poor attribution) and then move to the much more problematic (misleading statements and factual errors).Unclear:Section 4.3, assumptions:"1) the reward given a state-action pair R(x, a) follows a single distribution with finite variance"What does this mean? Finite variance is clear, what do you mean "a single distribution"."3) the policy follows a distribution which can be approximated by Dirac mixtures"What does this mean? Approximated how well? Under what metric?Equation 18, second line, \mu and \sigma seem like they should both be functions of (x, a).Poor attribution:Existing work has used Gaussian Mixture Models for distributional RL, as well as for the actor-critic setting (D4PG, among others).Existing work has considered multi-step returns in distributional RL (Rainbow, Reactor, as well as almost all methods that use AC with Dist. RL). However, the Sample-Replacement method is an interesting contribution that is novel compared with this existing work."The distributional Bellman operator is a [...] contraction mapping in the Cramer metric space, whose proof can be found in Appendix C."This has previously been proven in the Rowland et al. (2019) paper the authors cite, but do not attribute such a result to.Misleading statements:"Third, the Wasserstein distance that is commonly used in DRL does not guarantee unbiasedness in sample gradients"While this is true for direct minimization, the quantile regression work cited in this paper does guarantee unbiased sample gradients."The instability issue is not present under the stochastic policy... Combining these solutions, we arrive at a distributional actor-critic..." (Much later) "One way to overcome this issue is learning value distributions under the stochastic policy and finding an optimal policy under principles of conservative policy iteration..."The instability issue the authors reference here is that the distribution of returns, though not its mean, can be an expansion under any probability metric when applying the optimality operator. While this is an interesting topic, the authors do not actually address it or contribute towards its understanding or solution in any way. The evaluation operator was already known to be a contraction in Wasserstein (as well as for Cramer), which is the relevant operator when considering an actor-critic framework. Unlike the authors' claim that this is due to using a stochastic policy, it is in fact due to performing evaluation as opposed to optimality operators."Barth-Maron et al (2018) expanded DDPG by training a distributional critic through quantile regression."This is completely incorrect, as they considered categorical distributions and Gaussian mixtures, but not quantile regression."The actor-critic method is a specific case of temporal-difference (TD) learning method in w hich the value function, the critic, is learned through the TD error defined by the difference..."Actor-critic uses TD to learn the critic, but it is not a specific case of TD."However, the Wasserstein distance minimized in the implicit quantile network cannot guarantee the unbiasedness of the sample gradient, meaning it may not be suitable for empirical distributions like equation 13."This is 100% false and shows a lack of understanding of multiple papers being cited in this work.Figure 2 and "Wasserstein distance (labeled as IQ) converges to a local minimum which does not correctly capture the locations of the modes"This seemed off to me so I went ahead and reimplemented this experiment myself. This has nothing to do with the Wasserstein distance and is exceedingly misleading to the reader. Suggestion to read the Rowland et al. (2019) paper that the authors cite for better understanding. Huber-quantiles are not quantiles. The authors learn Huber-quantiles and then treat them as quantiles and observe they look wrong. If you run IQ with the Huber parameter at 0 (corresponding to quantiles) then you get the correct (unbiased) distribution. If you instead learn Huber-quantiles and use the imputation in the Rowland you again get the right distribution.The experimental results in the main text look promising for GMAC, but looking at the full set of results in the appendix paints a much more mixed picture. SummaryThe paper suggests an improvement over double-Q learning by applying the control variates technique to the target Q, in the form of $(q1 - \beta (q2 - E(q2))$ (eqn (8)). To minimize the variance, it suggests minimizing the correlation between $q1$ and $q2$. In addition, it applies the TD3 trick. The resulting algorithm, D2Q, outperforms DDPG and competes with TD3.RecommendationI hope I havent misunderstood this paper, but Ive found neither the theory nor the experiment convincing. Therefore I recommend a rejection.StrengthsThe proposed algorithm is simple and straightforward to use.Weaknesses1. Theory(a) Minimizing the variance of eqn (8) requires maximizing the correlation between q1 and q2. If they are independent, whats the point of including q2? Check out https://en.wikipedia.org/wiki/Control_variates(b) $E(q2))$ is the average over all possible runs. Its unclear how its calculated. Maybe run a few identical RL experiments with different random seeds, just to get $E(q2)$? Feels wasteful to me.(c) Why would minimizing the squared cosine between last layer feature vectors lead to minimum correlation? If the feature for q2 is obtained from that of q1 through a deterministic 90º rotation, wouldnt that result in a zero cosine but really strong correlation?(d) Why is it ok to ignore $var(q1)$ while computing $\beta$? No theory is given here.2. ExperimentsIn Fig 1 - 3, D2Q sometimes outperforms and sometimes underperforms TD3. Because these two algorithms are so similar, I cant tell whether the comparison is statistically significant.Other feedbacksPlease address questions raised above. Perform additional experiments to make the paper more convincing.  The paper proposes a method that modifies double Q-learning by eliminating a linearly correlated part of one Q. I am not familiar with the proof of Double Q-learning and TD3, and thus find the proof of this paper hard to read as it omits the majority of proof by claiming it is similar to the proof of the aforementioned two algorithms. To name some of the part that confused me while reading: what is the definition of F^Q_t and c_t in (14)? Why does a small delta_2 exist in (16)? Why does Delta_t converge to zero as claimed in the line after (16)? Why is the randomness of s_{t+1} not mentioned in the subscripts of E's in (5) and (9)? etc. Therefore, I suggest the author(s) write a thorough proof and put it in the appendix to make the convergence analysis readable. I am also curious how the de-correlation term helps to improve the convergence in the analysis as it is the main contribution of this paper. Besides,  double Q-learning and TD are mostly used in function approximations. I wonder if the analysis can extend to some simple case of parameterized Q functions, e.g. linear approximations. The experiment part looks good to me as it compares D2Q with several sota algorithms and get satisfying results. **SUMMARY**This work proposes a novel neuro-algorithmic policy architecture for solving discrete planning tasks. It takes a high-dimensional image input and processes it through modified ResNet encoders to obtain a graph cost map and a start/goal heatmap. This is fed into a differentiable Dijkstra algorithm to obtain the shortest trajectory prediction which is trained using an expert-annotated trajectory via a Hamming distance loss. This module is evaluated in two dynamic game environments demonstrating generalization to unseen scenes.**STRENGTHS**- The general idea of integrating BlackBox combinatoric shortest path algorithms in a differentiable planning module is interesting and has a lot of potential to be useful. **WEAKNESSES**- The novelty compared to Vlastelica et al. (2020) is not clear.- The design choices are not clearly justified and the considered use-cases for this particular architecture are limited.- Important information is missing to make this work reproducible (see reproducibility section)- The evaluation considers only shortest-path planning scenarios that are amenable to the proposed architecture (see evaluation section). The authors should either provide a clear motivation of the considered scenario types or evaluation on scenarios learning more complex representations.**CLARITY**The general idea of the work is clearly written although important information for reproducibility is missing (see below). I also felt that the authors did not make particularly good use of space. For example, Sec. 3 and Sec 4.1 could be condensed into a joint background section, leaving more space for more detailed experiments. The information in Sec 4.3 seems to be a better fit for either the related work or the conclusion section. Smaller clarification questions:- What is the y axis in fig 5 (a).- In the conclusion: What exactly is meant by knowing the topological structure of the latent planning graph a priori? How is this incorporated as an inductive bias into the neural network? **REPRODUCIBILITY**The results in this work are not reproducible. Relevant information on training (e.g. which optimizer was used? what were the learning rates? ...), hyperparameters (which parameters were tuned? which range was considered? how were they tuned?), baseline method training (e.g., how long was PPO trained, how exactly were rewards defined, ...) and environment generation settings are missing.**EVALUATION**The evaluation seems one of the weak points of this work. The problems here are threefold:1. the number of considered tasks is very limited and their type is very limited to scenarios where one image input provides enough information to generate a full cost map. This does not hold in most planning tasks. 2. Because the architecture itself is a claimed contribution, this work would require a much more thorough evaluation of architecture design decisions such as which underlying CNN is used, what is the 3. Simply using the PPO baseline is insufficient. First, there is no discussion on how and why this algorithm was chosen as a baseline. Second, more recent or closer related baselines are missing. Some of those are mentioned in related work and they seem to be more fair/useful comparison methods. **NOVELTY / IMPACT**The work is not sufficiently motivated. While planning, in general, is an important problem and differentiable planners are an important research topic, the motivation of this work is not clear. The authors should not only name the use-cases where an intermediate planning module might be beneficial but also discuss what the main insights of this work are. As the authors write themselves, a differentiable implementation of TDSP in a neural network can simply be achieved by applying theory from Vlastelica et al. (2020). In fact, that paper already demonstrates the use of Dijkstra in a neural network computation graph. This opens up the question of what is the key idea and value of the present work? Furthermore, the authors did not sufficiently consider/discuss existing related work. The related work does not sufficiently cover state-of-the-art. Combining planning modules with deep learning pipelines (although in slightly different ways) has been considered in several works. More importantly, the related work should discuss why the proposed approach is beneficial compared to approaches containing differentiable planners. The related work is also missing numerous relevant papers. Some potential examples involve:- Kuo et al., Deep sequential models for sampling-based planning, IROS 2018- Kumar et al., LEGO: Leveraging Experience in Roadmap Generation for Sampling-Based Planning, IROS 2019- Gupta et al., Cognitive mapping and planning for visual navigation, CVPR 2017- Savinov et al., Semi-Parametric Topological Memory for Navigation, ICLR 2018- Tamar et al., Value Iteration Networks, NeurIPS 2016Finally, the related work section should not only provide a broader discussion of related works but also more clearly emphasize the relationship between these works and the present work (e.g. how and in what context is the present work better more useful than related works? Which ideas are used from related work and what is new to this work? ...).**REVIEW SUMMARY**Overall, I recommend rejection of this work because the issues raised in this review cannot be resolved without a significant amount of work. While this may be discouraging, I believe that the authors are considering an interesting topic that has a lot of potential and, after resolving the issues raised in this review, may result in a successful submission. Summary: The paper studies the problem of image-based planning in discrete state/action spaces. The paper proposes a neuro-algorithmic policy that can be trained end-to-end by differentiation and used together with a shortest path solver.Strengths:i) The motivation, organization and the overall writing of the paper are clear.Weakness:i) The manuscript is missing very relevant pieces of works that should have been discussed in detail and included as baselines in the experimental results section. Namely, the previous work (i.e., Latplan) [1,2] that performs (classical) planning from images in latent spaces using variational autoencoders in combination with off-the-shelf automated planners. Similarly, experimental comparison to work [3] is missing.ii) How does the proposed approach (i.e., NAP) reason about the value of the planning horizon T which is an important aspect of automated planning? Note that Latplan would again provide useful insights here (since it leverages off-the-shelf automated planners).iii) The benchmark domains seem too simple. For a more realistic image-based task, see Meta-World [4].Additional comments:Page 1: In the third paragraph, can you please ground the claim for the statement transition model is often harder than learning a policy? Moreover, instead of just saying there exists several approaches&, you should include previous works [5,6,7] as references that learn such transition models for planning.Page 2: Second item in the list on page 2: generalizing policies -> generalized policiesReferences:[1] Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary, Asai and Fukunaga AAAI-18.[2] Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS), Asai and Muise IJCAI-20.[3] Learning latent dynamics for planning from pixels, Hafner et al. ICML-19.[4] A benchmark and evaluation for multi-task and meta reinforcement learning, You et al. CoRL 2019.[5] Nonlinear Hybrid Planning with Deep Net Learned Transition Models and Mixed-Integer Linear Programming, Say et al., IJCAI-17.[6] Scalable Planning with Deep Neural Network Learned Transition Models, Wu et al. JAIR.[7] Optimal Control Via Neural Networks: A Convex Approach, Chen et al., ICLR 2019. The reviewed paper presents a completely unsupervised framework Meta-K for predicting the number of clusters. The approach advocated in the paper comprises two main parts: autoencoder for feature extraction and multilayer perceptron (MLP) for predicting the number of clusters. Autoencoder is used if necessary to decrease the dimensionality of the input data. The MLP is trained using policy gradient optimization schema to predict the best (according to silhouette score) number of clusters k in the given dataset. Overall, the authors show that their approach achieves near-optimum results on both a number of synthetic datasets as well as on two well-known computer vision datasets: MNIST and FMNIST.Strong points:* Overall, the paper is well written. The reading flow is smooth and clear without major disturbances. Alike text, figures (especially Figure 1), greatly facilitates the understanding of the paper.  * The review of the related literature seems to be very thorough and lists most of the relevant papers in the field. This section helps to put the current work into context and explain the limitations of approaches proposed by past research.Weak points as well as reasons for the score:* The main problem with the paper seems to be the apparent discrepancy between the performance and complexity of the proposed approach (Meta-k), and the baseline method (silhouette score). While both methods are completely unsupervised, the former is a complex composite pipeline, which includes two neural networks, one of which is trained using a reinforcement learning framework, the latter is a simple formula that can be calculated for as many clusters as necessary in a relatively short time. Both methods, according to the authors, perform on par, with the baseline being marginally better on synthetic data. Moreover, according to the paper, the Meta-k framework is trained to find the best number of clusters by maximizing the silhouette score in the first place. Unfortunately, the authors largely omitted a question of why their approach should be preferred over the baseline. Only once, when commenting on results of the comparison with baseline, the authors noted: "although the baseline approach achieves similar or better results, it is not feasible to use this method when the range of possible k's is broad", not disclosing details as to what exactly about a broad range of k's makes the baseline method infeasible, and why Meta-k must be preferred in such circumstances. Overall, the benefit of using a more complex method, such as Meta-k should be clearly stated in the paper and also extensively experimentally verified.* The lack of experimentation is the second most important concern and also the reason for the current score. Meta-k has been used on several datasets, including a number of synthetic datasets generated using sklearn package and also well-known MNIST and FMNIST. While experiments on synthetic data is a great way to confirm the initial hypothesis about the model, superior performance on real-world data is what can be of interest to the community. Although, MNIST and FMNIST are good starting points, a lot more datasets exist where the number of clusters (i.e. classes) is known: e.g. CIFAR-10, CIFAR-100, ImageNet, etc. The same can be said about the competitors, as currently only one approach (except baseline) is compared to Meta-k. * Also, it is not made clear to the readers why policy gradient optimization was used for training the controller network, rather than for example common SGD with custom loss based on silhouette score. The RL training in such circumstances and without proper explanations seems to be an excessive measure.* The paper can be better structured. Section 3, might have been called "Methods", where the authors could have described not only their proposed solution but also the competitor approache(s) that they decided to compare with. It might be a good idea to move some part of the policy gradient optimization discussion to supplementary materials, leaving more room for experiments and results. Section "Experiments" might have been split into "Experiments" and "Results", with "Experiments" focusing on performed experiments and their setup, while "Results" remaining more focused on obtained outcomes and relevant interpretation.* Large parts of the introduction are overlapping or repeated in related work, it is not necessary to discuss relevant literature in the introduction, this part should be moved to related work completely.  * Over a number of occasions (in the abstract, introduction, and related work) the authors point out a close relationship between their approach and meta-learning or 'learning to learn' concept, hence the name of the approach (Meta-k). However, nowhere in the paper, the authors seem to clarify which part of their method performs meta-learning. While it might be understood that the authors refer to the fact that their approach trains the controller network using the unsupervised signal from the silhouette score. It is however might be a good idea to make this connection explicit.Minor comments:* Training curves for the policy gradient optimization presented in Section 4.4 are left with no interpretation, and to the reader it remains unclear why they were presented in the final section in the first place.* Caption of Figure 1 should ideally also explain what x, x', z, and other variables mean.* The third item on the list of contributions, interprets experimental results and can hardly be considered as a contribution of the paper.* Some of the terms related to reinforcement learning, such as environment transitions or finite-horizon undiscounted return must be explained before use.* Figure 2, could have been made more clear by adding vertical lines that would correspond to the true value of the number of clusters.     * All acronyms, such as MLP should be defined before being used. 0) I reviewed an earlier version of this paper. The ICLR submission is similar to the earlier one, hence the similar (but updated) review. Generally, the paper is hard to follow and can only be understood by someone well-versed in both source separation and information geometry.1) Presentation of the ideas is lacking (insufficient explanations, motivations) and I failed to understand the main contribution of the paper, i.e., how the concept of information geometric log-linear model can be applied to BSS. The log-linear model models a discrete probability distribution of ordered events. As such the authors introduce a specific ordering for BSS that I failed to understand. Eq. (2) plays a central role but details and motivation are lacking and its hard to understand why this ordering is interesting and why a log-linear model makes sense.- Exemples would be welcome in Section 2.1. What is Omega ? what is an s ? what is S ? what is the meaning of Psi ? what is the intuition behind (1) ? 2) I would encourage the authors to clarify the settings in which their methods is applicable. It seems to be able to tackle undetermined methods and some forms of nonlinearity, though it's not clear why. - What are the assumptions about the sources and mixtures ?- How come your method can identify the sign of the sources ? What sort of prior information is available ?- The preprocessing of real-valued data (basically applying exp or some form of normalisation) is disturbing.3) The experiments consider artificial mixture of images and artificial mixtures of synthetical signals with little practical significance.- Can you explain more clearly why ICA or NMF fail on the simple 3x3 image separation problem ? - Do you have any idea of an application that could benefit from the nonlinear model in Section 3.2 ?4) The introduction of the paper could be improved as it seems to reveal some misconceptions about blind source separation (BSS). For example PCA is not a method dedicated to the separation of orthogonal sources (such a problem is actually not identifiable). It is instead often used as a pre-processing in ICA (whitening). The introduction also sustain a confusion between feature extraction and source separation. These two different tasks can be handled by similar methods (ICA, NMF) but in very different settings. In source separation, the data X contains signal in rows (e.g., the cocktail party problem). In feature extraction, the focus is on the columns of X which contains data samples (such as short-time frames of a single-channel mixture).  This paper presents a Robust Collaborative Autoencoder (RCA) for unsupervised anomaly detection. The authors focused on the overparameterization of existing NN-based unsupervised anomaly detection methods, and the proposed method aims to overcome the overparameterization problem. The main contibutinos of the proposed method are that (1) it uses two autoencoders, each of which is trained using only selected data points and (2) monte carlo (MC) dropout was used for inference.Although this paper has an interesting idea, i have doubt about the contributions. My comments are as below.1) First of all, to me it was very difficult to read this paper. The notations are very confusing.2) In Introduction section, it is confusing what the main focus of this paper is. They mentioned like "unlike previous studies, our goal is tho learn the weights in an unsupervised learning fashion". But because it seems the topic of this paper belongs to "unsupervised anomaly detection" (the labels indicating whether anomaly or not are assumed available in the training data), the point that your method is in an unsupervised learning fashion is pretty obvious.  You don't need to discuss about "supervised approachs" throughout the paper, but please clearly mention that at the beginning of the introduction section, and only discuss your method and other "unsupervised" anomaly detection methods.3) If the overparameterization is the problem when we build a NN for unsupervised anomaly segmentation (e.g. autoencoder-AE), we can simply think about various well-known NN regulaization techniques for the AE as remedy. I also think the two parts of the proposed method (corresponding to the contrbutions (1) and (2) )work as regularization for the AE. I'm curious if there's any reason to prefer the proposed method to other regularization techniques?4) The proposed RCA method involves an ensemble prediction by using MC-dropout (described in section 3.2). The authors metioned this is one of their research contribution, but the use of MC-dropout is quite general in neural network research. Also, while the proposed method definitely benifits from the use of MC-dropout, other unsupervised anomaly detections based on neural networks (e.g. AE, VAE,...) can also improve by employing MC-dropout. The ablation study in Table 1 showed that RCA significantly outperforms RCA-E (RCA without ensembling).  The authors can implement MC-dropout-based ensemble versions of AE, VAE, and other nn-based methods like Deep SVDD, and check whether the proposed only benefits from MC-dropout or RCA just outperforms others regardless of MC-dropout. The proposed approach differs from autoencoder based anomaly detection approach in the following ways(a) Autoencoders are trained using only selected data points with small reconstruction errors. These are selected using a sampling scheme with theoretical guarantees on convergence.  The selected points are then shuffled between two autoencoders. (b) During the testing phase, each autoencoder applies dropout to generate multiple predictions. The averaged ensemble output is used as the final anomaly score.Some of the issues with this paper(a) One key issue is why just two autoencoders (which the authors delegate for future work). However, it is key to understanding utility of such an ensemble based shuffling framework.(b) Poor presentation of results    1) Figure 2 legend issue    2) Figure 3 (a) is better presented as a table (Table 3 in Appendix should be here instead). Very hard to interpret it in the current form. Similar comments for Figure 3(b) and Table 1. Also for anomaly detection benchmarking AUC is not sufficient and the authors have to present AUPR or F-1 scores also. I suggest looking at these recent papers for presentation of experimental results http://proceedings.mlr.press/v108/kim20c/kim20c.pdfhttps://proceedings.icml.cc/paper/2020/file/0d59701b3474225fca5563e015965886-Paper.pdf (Goyal et al. ICML 2020)(c) Theorem 3 might have a connection with the notion of r-robustness presented in https://arxiv.org/abs/2007.07365      so authors would want to make it clear how they differ. Although the paper is covering an interesting topic, much of what's in the paper can be found in other works, and there's not a lot of novelty to the insights, nor a large breadth of experiments to justify it as a survey paper.- The linear and quadratic loss functions are not new.- The enforcing locality part is essentially why the pruning strength annealing schemes exist, this insight is not new and can be found in Zhu&Gupta, or in Bayesian settings like in the Molchanov paper, they suggest annealing from a Bayesian statistical perspective- The fact that this leads to multiple stages of pruning to be a good idea, is also known in the literature this paper cites.- The most meat of the paper is in the 'survey' part of it, investigating the results... but this section feels lacking since there are not a lot of experiments, and the insights of e.g. post-finetuning can be largely found in e.g. Blalock et al. I'm missing deeper insights/analysis here. What is the reason for this? How do we remedy this? What are the characteristics of networks that lead to this behavior? - It would have been great to see a lot more insight/experiments on this topic. The authors throw up a lot of hypothesis and suggestions/ideas throughout the paper, but don't back them up. E.g. If the ||Theta|| term is better of to be constant throughout the pruning procedure... can we somehow make an annealing scheme that keeps ||Theta|| small and constant throughout the pruning process, and show that that works well? This can be proven/shown somehow. I do think the paper is well written; and I encourage the authors to look further into this topic and come up with more novel insights/results and methods to improve pruningOther things/questions/suggestions:- In formulations (1), (2) and (5), (6). Why are the absolute brackets necessary? Especially for models that have not converged, why would you want to stay close to the original loss, as opposed to just decreasing the overall loss of the model? - For most of the discussion in section 3.2, the authors talk about the norm of the delta theta squared being large. But this largeness is relative to the 0th and first order term which the authors glance over. Under 'other considerations' for example, if the weights theta are large, the gradients likely follow suit. Thus the absolute magnitude of the weights might not matter, as it's the relative size of this to the gradient terms that should be considered. - Constraining the step size in section 3.2. Interestingly, if you take the assumptions that for each layer, the hessian of the loss w.r.t. the output is sqrt(lambda/2), and your input distributions to a layer are normalized, you end up with the weaker norm penalty. This is a crude approximation of the second order term, which would give this method a bit more of a theoretical foundation than just a regularization term.- 5.1 Convergence assumption. I don't get this part, both the OBD method, and the linear and quadratic loss terms depend on the locality, so all will also depend on the amount of steps taken. For OBD, as long as you recalculate the Gauss-Newton matrix, I don't see why this method is different when not doing fine-tuning. - 5.1 Convergence assumption. The result cited in appendix A is a very well-know result. How could this link explain the OBD performance on the VGG network? 'Could' is not strong enough to make it into a paper. Small tidbits:3. Do pruning criteria better at preserving the loss lead to better fine-tuned networks? <- this sentence doesn't flow nicely. I would add a 'that' so you have do pruning criteria that are better ... Strengths:- The authors address a well formulated and an important problem for lots of practical scenarios.- The paper is well written.- The experiments demonstrate that the proposed method outperforms several baselines.Weaknesses:- I personally found the proposed approach to be very cumbersome and complicated. It consists of many different components that are not conceptually intuitive. Contrary to some of the recent approaches for modeling visual relations (i.e. Non-Local Networks or Space-Time Video Graphs), the proposed model is much more difficult to understand. In my opinion, this is a big disadvantage because the models that are most useful to the community are usually conceptually (and technically) simple, yet effective.- The authors assume that the input features to the TOQ net are hand-engineered, and thus, are not learnable. This is very different to most modern approaches, which typically try to learn all the features from raw pixels end-to-end. Very few recent methods (to the best of my knowledge) rely on hand-engineered features for video modeling/action recognition. This begs the question how applicable the proposed approach is to modern computer vision community.- The biggest weakness of the paper is its experimental evaluation. The authors evaluate their approach on 2 small-scale artificial video datasets. Thus, it is not clear whether the proposed approach would generalize to real datasets such as Kinetics, Something-Something, EPIC-Kitchens, etc. Most current action recognition methods evaluate their model on these large-scale real-world datasets. Thus, I think it is imperative that the authors would conduct thorough experiments not only on their small artificial datasets but also on the datasets that are most often used by the action recognition community. In particular, datasets like Something-Something, EPIC-Kitchens, or Charades require spatiotemporal relation modeling as demonstrated by prior work (i.e. Space-Time Video Graphs). - The comparison with the pixel-level baselines (i.e. Non-Local Networks, or Space-Time Video Graphs) might not be exactly fair. The authors adapt these baselines to the dataset/task specific scenarios. Compared to their own model, the authors don't have as many incentives to tune these baseline models for their specific tasks. Thus, I believe that the experiments on the real-world large scale datasets such as Kinetics, Something-Something and Charades are essential for validating that the proposed approach is better than these prior methods.- Missing relevant work: Wang et al., "Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks." (CVPR 2020).Rebuttal Requests:- The authors should include thorough experiments on the real-world large-scale datasets such as Kinetics, Something-Something, and Charades. The currently used datasets are not sufficient to verify the generalizability and usefulness of the proposed approach. This article bases hyperparameter optimization over multi-objective by aggregating them with weights. An illustration is provided on a grid search to select Pareto optimal solutions.Aggregation of objectives to get scalar values has been studied at length in multi-objective optimization, so it is hardly novel. See, e.g.,  Miettinen, K., Nonlinear multiobjective optimization,  Springer, 1999, 12.Besides , choosing weights is known to be quite difficult in practice.Concerning optimization of hyperparameters, there are many works applicable that would be much more efficient than a crude grid search. In the case of Bayesian optimization, there are works like, e.g.,:-Swersky, K.; Snoek, J. & Adams, R. P., Multi-task bayesian optimization, Advances in neural information processing systems, 2013, 2004-2012- Paria, B., Kandasamy, K., & Póczos, B. (2020, August). A flexible framework for multi-objective Bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence (pp. 766-776). PMLR.- Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).Since no comparison is provided with methods from the literature, that the proposed method is not novel, I thus recommend rejection.Typos:Eq. 4: sise  sizeP2: the nearest Pareto front to the origin? This paper proposes MTMC, a method that solves the pareto optimization problem for multiple tasks and multiple criteria.Pro:1, concise.2, solution seems interesting from a technical perspective.Con:1, No explanation on why this proposed method should work well.2, No comparison to state-of-the-art method (or any baseline methods). Several related methods were discussed in related work, but why not compare to them, e.g. (Igel, 2005)?3, Experiments are not convincing. Please include descriptions of this problem in (Akhmetzyanov & Yuzhakov, 2019) and add more realistic real-world problems. Please include specific description of the metric to evaluate different methods including the proposed one.Some minor points:1, It's unclear what N_combination is exactly and why it's different from N_parameter.2, Typo: in Eq (4), x_sise -> x_sizeOverall I think this paper is half developed and can benefit from some intuitive explanations, theoretical analysis of the proposed method and more convincing experiment. Summary:This paper studies the detection and recovery problem in spiked tensor models in the form T = \beta v0^\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise.  The authors claim that they propose a new framework to solve the problem, by looking at the trace invariants of tensors. The authors provide a detection algorithm (Algorithm 1) and a recovery algorithm (Algorithm 2), as well as the corresponding phases. The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. The authors furthermore discuss the asymmetric case and the multiple spike case.Recommendation:At the current stage I vote for rejection. I am not able to follow the proofs in this paper due to missing definitions of terms and notations. Also some claims are not proved. See below for details.Pros:- The methods used in the paper seem new for spiked tensor models.- Some experimental results are provided. Cons: - The readability of this paper severely suffers from its writing. At the current stage, filled with undefined or inconsistent notations and terms, this paper is not self-contained and hard to follow. This becomes worse considering the fact that this paper studies tensor problems -- many tensor-related terms have multiple definitions (e.g., eigenvalues, ranks). It will be very hard to follow the proofs if the definitions are unclear. Here is an incomprehensive list:- Middle of Page 3: what is the *formal* definition of contracting (instead of saying "equivalent to a matrix multiplication")? Also, trace invariants are never formally defined in this paper.- eq.(2),(3): what is O(n) here? Also, what does the bold O refer to? Right before eq.(4) the authors use another notation \mathcal{O}(n). Is this the same as the first O(n)? In the abstract the authors use \mathcal{O}(1) to refer to the constant order. Why the inconsistency?- End of Page 3: how is \mathcal{G} related to trace invariants formally?- Section 2.2: this is not clear. What are the matrices here? What is the definition of M_{G,e}?- Section 2.3: what is the definition of I_G(T)?- Theorem 3: what is the Loss function here?- Top of Page 5: what is the exact definition of "dominating" here?- It should be noted that, without clear definitions of I_G(T) and M_{G,e}(T), there is no way to verify Algorithm 1 and 2.- The authors claim "polynomial complexity" at the beginning of the paper, but it is never proved. Theorem 7 claims that Algorithm 1 and 2 run in linear time. I cannot find that in the proof.   - It is unclear why the algorithms "are very suitable for parallel architectures", as the authors have claimed. Have the authors tried running the experiments in parallel?- Theorem 4, 5, 9, 10 do not have complete proofs. Minor comments:- Page 2 Notations: typeface of v is not consistent. - Page 8: "eg" should be "e.g." The authors propose the use of the Hellinger distance instead of KL divergence to constrain new policies to remain close to the behavior policy. The technical aspects are straightforward, noting that Hellinger provides tighter bounds on total variation than KL, and can straightforwardly be plugged into the CPI/TRPO bounds for policy improvement. They also propose an offline reinforcement learning algorithm based on enforcing a Hellinger constraint to the data policy, deriving iterative optimization procedure, and evaluate it on offlineI find the experimental evaluation highly lacking. It seems with the datasets and envs evaluated, policy performance actually *drops* as policy optimization is conducted, so it is not clear to me that these evaluations actually provide meaningful information towards which methods perform better in scenarious where we would want to use offline RL. I would like to see much more extensive evaluation of this method compared to other offline RL algorithms like BCQ https://arxiv.org/abs/1812.02900, BRAC https://arxiv.org/abs/1911.11361, or CQL https://arxiv.org/abs/2006.04779, over a much wider variety of datasets. In general, I'm not convinced that simply using the Hellinger distance instead of KL will lead to significant improvements on its own, given that in the BRAC paper, the authors experimented with different trust regions including Wasserstein, MMD, and KL and didn't find huge differences in the tested domains. Overall, the contribution does not seem significant enough to warrant publication without strong experimental results, which this paper lacks. ############################################################################# SummaryThe paper proposes a Bayesian formulation of multi-task learning in the classification scenario.  Specifically, the problem is cast at a variational Bayesian inference problem. The inter-dependency of the tasks is enforced through Gumbbel-softmax priors, with the weights learned from the data. The Bayesian formulation leads to better performance when the amount of training data is low, as shown on three benchmark datasets for image classification.############################################################################# Reasons for score Overall, I vote for reject. The methodology is not formulated in a proper Bayesian way as claimed.  The experiments, while covering a wide range of datasets and scenarios, do not have any error bars  and hence have very low statistical significance.############################################################################# Pros 1) The use of the Bayesian approach is well motivated for the case of limited data.2) The explicit inter-task dependency of the classifier weights is introduced, in addition to the mode standard approach of interdependent features.3) The related work section is quite thorough.4) The experiments cover a wide range of datasets and analyse different aspects of the model: effect of variational approximations on latent representations and the classifier weights, particular choice of the prior, different amounts of training data available.############################################################################# Cons1) While other Bayesian multi-task models are mentioned on the related work, their applicability for the task at hand is not addressed and the need for this particular approach is not motivated.2) The model is not formulated in a standard Bayesian way. There is no clear separation between the model and the inference scheme. Specifically, the prior for one task is dependent on the variational posterior for the other tasks and is learned, hence it is not a proper prior. 3) The experiments do not have any error bars, only one value (i.e. one run). Thus the experiments only weakly support the claims and are not statistically significant.############################################################################# Some typos/ minor commentsEq. (2): Use a different letter in the product, not tFig. 1: What do the dashed lines show? Is this an illustration of the model or the inference?Sec. 4.2 Implementation: You say about using VGGnet as a  "feature extractor in your architecture". The phrasing suggests you use it to extract z. I assume you pre-extract x. Clarify, please.Supplement: Eq. (15): The very last term should have expectation over both q(w) and q(z|x)Supplement B.3: "the prior of current takes is " -> "the prior of current tasks is" ##########################################################################Summary: The paper empirically analyzes the evaluation framework of the current OOD detection systems for the image recognition task, specifically the evaluation described in [1] using Max-softmax and calibrated confidence. They motivate the paper by the necessity of having better evaluation for OOD detection to be reflective of real world scenarios. The addressed problem is interesting and valuable for the field as many of the defined OOD datasets, and evaluation metrics may not cover many real-world scenarios. They specifically addressed three scenarios, inputs that i) are irrelevant to the task ii) are from novel classes and iii) are from another domain (domain shift), which for the first 2 scenarios, they only evaluate them as unseen classes and not distinguish between them. Based on my understanding of the paper, they compare 5 OOD detection methods from the literature, suggest a few test datasets/scenarios and conclude using cosine similarity is consistently favorable for evaluation, and the choice of using confidence-based methods in case of domain shift detection scenarios.##########################################################################[Q]: questions, and comments on the paper. [S] strength [W] weaknessQ1: Questions about the choices of the datasets:1.1. What are the reasoning for the choices of datasets? 1.2. Why are these datasets considered reflective of real world scenarios? For example, why didnt you analyze the models on some OOD datasets like Imagenet-A, -O, -P.Q2: Could you elaborate more on the specific contributions of the paper?Q3: What does genuine OOD samples mean in section 2.2?Q4: There is no related work part In the paper.Q5: Many recent and relevant papers are not cited or compared against. As an example, it is mentioned in the Domain shift detection [2] study in NLP, that is not the focus of the paper. However, there are multiple recent works in Computer Vision also addressing the domain or distributional shift in vision scenarios. In particular, for the example surveillance system scenarios mentioned in page 2, there are datasets and papers addressing the domain shift that should be cited such as [3 (a review paper), 4] or [5]. Q6: The following statement from the paper is inaccurate as a general statement. Which kind of pre-training contributes to performance improvement? In addition, there are many scenarios where pre-training or fine-tuning lead to reduced performance on seen classes or on the original data distribution. (i) Using a pre-trained network always contributes to performance improvements, confirming the study of Hendrycks et al [1]. (2019). Q7: For the evaluation using fine-tuning, the impact of the results on the seen classes (the current classes in the dataset), are not re-evaluated. In many cases, fine-tuning in a new distribution, contributes to reduced performance in seen classes or former distribution. I recommend checking the literature of generalized zero-shot learning and specifically the use of harmonic mean as a measurement.##########################################################################Reason for the score: S1: The paper is well-motivated and addresses an interesting problem. S2: I believe there is room for improvement.W1: The contributions of the paper are not clear. The ablation studies to compare the approaches are not sufficient. W2: I am not convinced the current set up can be sufficient for generalizable decisions on choice of evaluations.W3: Choices of the datasets for comprehensive guidance is unclear.W4: There is no related work part  in the paper and multiple recent and relevant papers are not being compared against or cited.##########################################################################   References:[1] Hendrycks and Kevin Gimpel, A baseline for detecting misclassified and out-of-distribution examples in neural networks, ICLR 2017 [2] Elsahar et al, To annotate or not? predicting performance drop under domain shift, IJCNLP 2019.[3] Toldo et al, Unsupervised Domain Adaptation in Semantic Segmentation: a Review, CVPR 2020 [4] Zhang et al, Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes, CVPR 2017.[5] Ganin et al, Unsupervised Domain Adaptation by Backpropagation, ECCV 2016. This paper proposes to integrate word alignment obtained from SMT into an NMT system. This is an exciting topic not only because it can help interpretability, but also because the same mechanism could be used e.g. for imposing a specific terminology in translations, something that was relatively easy to do with SMT but is much harder to achieve with NMT. The proposed method involves computing word-word alignment using existing SMT models (GIZA and FastAlign in the experiments) and integrating that information in the decoder of a transformer-based NMT model. Experiments on English-Romanian and English Korean show small improvement over a standard baseline.Modeling is described at a high level and would require significant guesswork to re-implement. The core of the method is in the word substitution model (Sec 3.2) and how it is integrated in the decoder (Sec 3.3). Unfortunately neither is described with much detail. For example, there are a dozen operators involved in describing the model in Eqs. 1-17, some of them defined, some of them relatively easily guessable, other entirely unclear.It would greatly help to clearly define the input and outputs of the model (e.g. vectors of context, matrices of probabilities or activation, appropriate dimensions, etc.)Regarding evaluation, it is a positive to have results on two language pairs, in both directions. In the English-Romanian pair, GIZA seems to produce slightly better results, although the difference is unlikely to be significant. Still, it is odd that only  FastAlign was used for Korean. Although gains seem fairly consistent, they are also very limited and unlikely to be significant (no significance test seems to have been performed). Also the variability of performance (e.g. due to sampling) is not assessed. Finally, the only comparison is to a straight transformer baseline. None of the methods mentioned in the prior art have been tested.The analysis of the impact of the alignment on the attention weights in Sec 5.2 is interesting. What is the motivation for using different sentences and epoch #, rather than show how alignment is modeled as training progresses (as claimed in the text) on the same sentence?Misc:[Sec 5.1]: why is the sampling rate suddenly called the « compression ratio »? The paper presents a strategy to integrate prior word alignments into NMT models. It is not clear the motivation for this in the NMT context, especially why the prior alignments are crucial information that is necessary to be given a-priori to the Transformer. Besides this, the description of the method and the discussion of related work is given, SMT methods are briefly mentioned but the usage of the idea in previous work, also SMT literature is necessary.The applicability of the method is discussed very generally for NMT, however most languages do not have a one-to-one mapping in words and even in extremely low-resource cases it is difficult to see how the alignment information can be useful except for translating for instance foreign words and named entities (in languages where transliteration is not necessary).The description of the method seems sound and the experiments are performed for two languages with improvements in BLEU scores. Have the authors analyzed what is exactly being improved in the translations?The paper is mostly clear but has too many grammatical errors and can benefit from revisions and more editing. Summary: The paper studies the problem of planning in domains with sparse rewards where observations are in the form of images. It focuses on solving this problem using model-based RL with emphasis on better trajectory optimization. The proposed solution uses latent models to extract latent representations of the planning problem that is optimized using the Levenberg-Marquardt algorithm (over a horizon). The experimental results show improvements over a) zeroth-order CEM optimization, b)  PlaNet (Hafner et al., 2019) and c)  gradient-based method that optimizes the objective in Eq. 1.Strengths:i) The motivation, organization and the overall writing of the paper are clear.ii) The tested experimental domains are good representatives of the realistic planning setting identified in the paper.Weaknesses:i) Discussion of literature on planning in latent spaces [1,2,3,4,5] is left out and should be included. Namely, [1,2] performs (classical) planning from images, and [3,4,5] perform planning with learned neural models. Here, space can be saved by removing Figure 4 since all of its subfigures look identical given their (visual) quality.ii) Have you tried solving Eq. 2. directly similar to [4]? It seems more appropriate baseline compared to c) (i.e., as labeled above).iii) How do you reason about the length of the horizon T? For example [1,2] use heuristic search.iv) There does not seem to be any presentation of hyperparameter selection/optimization, runtime results or quality of solutions. Table 1 is too high-level to provide any meaningful insight into understanding how each method compares. Similarly, Figure 5 is very hard to read and not clear what each axis represents. Overall, I would say this is the weakest part of the paper.References:[1] Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary, Asai and Fukunaga AAAI-18.[2] Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS), Asai and Muise IJCAI-20.[3] Nonlinear Hybrid Planning with Deep Net Learned Transition Models and Mixed-Integer Linear Programming, Say et al., IJCAI-17.[4] Scalable Planning with Deep Neural Network Learned Transition Models, Wu et al. JAIR.[5] Optimal Control Via Neural Networks: A Convex Approach, Chen et al., ICLR 2019. This paper considers federated learning for edge devices with multiple wireless edge servers. The paper proposes FedMes to leverage devices in overlapping areas covered by multiple edge servers. In particular, in FedMes, if a device is in the coverage area of multiple edge servers, the device receives current models from all the edge servers covering it. Each device uses a (weighted) average of the models it receives as a starting point, and performs local updates (using SGD). A device broadcasts the updated model to multiple edge servers that cover the device. The key idea is that these devices in the overlapping coverage area act as bridges, and communication between edge servers is not required (until the final averaging step). The authors carry out experiments to evaluate FedMes, and compare against hierarchical federated learning of (Liu et al., 2019).Strong points:1. How the broadcast wireless nature can be leveraged in federated learning is a practically motivated question, and the proposed algorithm is quite simple. 2. The paper is well-written and easy to follow.Major concerns:1. The main premise of the paper is that communication with the central cloud server located at the higher tier of edge servers is costly and incurs significant delay. (One of the selling points of FedMes is that it does not require any backhaul traffic.) However, the authors do not provide much evidence for this premise. In fact, the bottleneck in wireless networks is typically the communication between edge devices and edge servers, and the backhaul communication is usually cheap and fast. It is not even clear why a central cloud server is required since edge servers can talk with each other periodically, and simply average the models. It will be important to give an evidence that communicating with a cloud server or between edge servers incurs large delay. 2. The paper only presents experimental evaluation, and the experiments make several simplifying assumptions. For instance, the latency with the cloud is modeled with simple one-round delay parameter t_c, and the one-round delay between a device and edge server is assumed to be 1. Further, the devices in overlapping coverage areas are assumed to be very symmetric. These assumptions do not seem to be practical, and it is difficult to assess the usefulness of FedMes over hierarchical FL. FedMes may perform better in these somewhat ideal conditions, but it would be important to consider practical aspects.Specifically, (Liu et al., 2019) compute latency using a wireless model. Also, they consider various values of E and T_cloud. It will be useful to consider various values in the experiments for the fairness of comparisons. (Abad et al., 2020) also analyze end-to-end latency by considering the properties of the wireless nature. Overall, it will be important to consider similar models for latency other than taking a simplistic approach of one-round delay t_c.Considering the above points, the contribution and novelty seem to be fairly limited. It will be helpful if authors can consider a more practical latency model, and/or provide more evidence for the values and assumptions used in experimental evaluations.Other suggestions:1. The authors cite latency sensitive applications, e.g., smart cars, to motivate faster training time. It will be helpful to clearly distinguish between training time and inference time. For instance, the 100 milliseconds latency considered in (Mao et al., 2017) is for training or inference? 2. In Fig. 2 and 3, what are the units of $t_c$ and the running time? When $t_c$ (delay between cloud and device) is increased, clearly FL and hierarchical FL will be slower. It will be important to give evidence for how these values are chosen. Also, what about the case when ES can talk with each other over backhaul; does it incur similar delay as talking to a cloud server? 3. In experiments, it is assumed that adjacent edge servers select the same set of devices from the overlapping area by cooperating with each other. It will be helpful to comment on the impact of this cooperation on latency as compared to sharing the (possibly compressed) models between edge servers. 4. Considering the same devices in the coverage area of an edge server throughout the training process does not seem to be practical. Would it be possible to extend the experiments to consider the case when the devices in the coverage area of an edge server follow some practically motivated distribution?   The authors proposed a neighborhood to sequence construction & pre-training approach to handling graph representation learning on large graphs.The merits of this work include:1. provide a decent attempt toward solving the computation bottleneck for representation learning on large graphs;2. discusses and shows the benefits of the pre-training based on (unsupervised) sequence learning.The limitations, on the other hand, is obvious:1. Pre-training is a relatively standard technique for representation learning on large graphs, and the schema proposed in this paper has very limited novelty;2. The application of attention mechanism for sequence learning is also a standard practice that has been widely adopted in this domain;3. The only part where this paper may distinguish itself from the previous work is how the sequence is constructed from the neighborhood. However, minimal theoretical discussion and empirical ablation study are provided to reveal the guarantees & benefits of the proposed method.Therefore, the limited novelty and contribution of this paper clearly outweigh its merits. The authors propose to solve underspecified IRL problem  using an MCEM approach. They claim it is the first succint, robust and transferrable solution and have some results on a Gridworld-like environment. Assessment: The MCEM framework actually is a good fit to the IRL problem and I dont recall if this has been explicitly called out in the literature before. But to me, the connection and resulting algorithm don't seem enough it terms of innovation and usefulness. Arguably, both BIRL and MaxentIRL do essentially (sampled) EM with different parametric forms. The experiments are unacceptably small scale and inconclusive.There were confusing parts in the exposition that I will outline below.Going by section:Intro: A good overview of the IRL literature. sec 2: In the problem statement itself you need to say something about how the expert demonstrations are connected to R. Otherwise they seem like there is no relation at all.2.2:I'm trying to understand the motivation behind the definition of C^E_\epsilon. In the objective formulation you are setting \Theta to maximize the likelihood of a randomly choose subset of expert trajectories. I have never seen such a thing done before and it is interesting and possibly a route to robustness, but I would have liked more explanation and discussion. In particular, the choice of a uniform measure across subsets of different cardinalities seems bad. Would it be simpler to just introduce a hidden variable for each trajectory, stating whether it is valid or not (i.e.  drawn from the true reward function and should therefore be used in likelihood computation). I agree then with the 3rd point in 2.2.1, this could be a different way to model expert sub-optimality than BIRL. 3.1.2:    It took me a while to read this and I dont think I understand. You seem to set \Theta_2 = W by the end. Then this seems to be very complicated and redundant. In particular, it is very confusing how the dimensionality of the parameter space can depend on the number of monte carlo samples and changes at each iteration ! Maybe I misunderstood something.3.1.3:   This seems like a straightforward EM derivation.3.2: What is the difference between the 2 stopping criteria? Just the fact that the 2nd has a patience of 3? if so, seems trivial.3.3: what is being formally claimed here? That given the condition on N_t, convergence will hold and proof is in one of the references?4 (Experiments):  Experiments are done on exactly 2 small instances of a variant of Gridworld. This is much smaller scale than the experiments you would expect to see in an ICLR paper (even granted that ICLR is very theory-focused). Previous papers have used IRL for real world problems like robotics, vehicle routing etc. Summation: A couple interesting starting point ideas in the theory part, but not completely fleshed out and ultimately the development of the EM approach is partly straightforward and part of it is very confusing to me (sec 3.1.2).  Experiments are very unconvincing in terms of scope.   The paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. Prior work has either learned a point-estimate, notably maximum entropy IRL, or used Bayesian methods to learn a probability distribution over reward functions. Maximum entropy IRL has scaled to complex environments with unknown dynamics and non-linear rewards (with methods such as AIRL), but do not learn a probability distribution. By contrast, Bayesian IRL is more theoretically principled, but has not scaled to complex environments or non-linear rewards. This paper performs maximum likelihood estimation of a parameter for a *generative model* over probability distributions, using a Monte-Carlo expectation-maximization (MCEM) method. It therefore still outputs a probability distribution like Bayesian IRL, but is able to learn non-linear rewards unlike prior Bayesian methods.Strengths:  - The method is novel.  - The motivation of the work is good: you identify important problems on IRL in the first introductory paragraph. (It would benefit from more follow-through -- which of these problems have you solved?)  - Reporting examples of learned reward functions (or mean of the distribution learned) as well as summary statistics like EVD gives a more thorough understanding of the method.Weaknesses:  - The paper was difficult to understand.  - The contribution seems limited. The method still seems challenging to scale (and no attempt is made to evaluate this), which seems to be it's main advantage relative to Bayesian IRL. Moreover, it is unclear if the probability distribution learned is well-calibrated, unlike Bayesian IRL.  - Weak baselines. DeepMaxEnt is a fairly old approach -- why not try e.g. AIRL? There's also no information as to how you trained the baselines, so it's difficult to know whether e.g. hyperparameter tuning was performed appropriately (noting that this environment is fairly different to what DeepMaxEnt was originally trained in). You should report the results of GPIRL given that this algorithm was developed for exactly this environment.I find the approach intriguing and would encourage you to continue developing it, but the submission is too preliminary to accept at this stage. In particular, I would suggest the following modifications:  + Significant edits to improve clarity. For example, the abstract is quite terse, especially the second sentence. I understood it since I am very familiar with this area, but most readers would be lost. I had to read section 2.2 several times to understand what you were proposing -- here (and elsewhere) you would benefit from giving the reader some intuition before diving into the math. The basic idea is relatively simple: you are performing maximum likelihood estimation on parameters $\Theta$ that define a probability distribution over weights. Then there are some details: how you sample the data when you perform MLE (effectively from a power set of the demonstrations, thresholded by some minimum size),   + Clarify what the benefits of your method relative to prior work are, and then rigorously justify this (whether theoretically or empirically). For example, is having a probability distribution over rewards actually a desiderata (in which case you should evaluate if they're well-calibrated), or is it an artifact of the method? Likewise, being candid about it's limitations would help the reader evaluate whether it is appropriate for their application or what novel research directions exist to improve it.  + Improved evaluation. Stronger baselines as discussed in "Weakness" above. More environments. Perhaps report runtimes -- this would help assess scalability. I found this line uncompelling: "Note that since almost only objectworld provides a tool that allows analysis and display the evolution procedure of the SIRL problem in a 2D heat map, we skip the typical invisible physics-based control tasks for the evaluation of our approach". First, this is wrong: objectworld is not that unique, you could visualize reward over e.g. a gridworld or tabular MDP, and many IRL papers have done so. Second, providing this visualization does not preclude also evaluating in more complex environments -- as the AIRL paper did, for example. I strongly suspect your algorithm simply won't scale to such environments (at least without considerable hand-designed feature engineering) -- if this is true then admit it and discuss how you can address it in future work, and if it's false then show that I'm wrong with results.  + You should discuss limitations of your work somewhere, e.g. conclusion. For example the convergence guarantee only holds in the limit of infinite data if I understand -- there is no finite-time bound? This is common for MCMC methods which your approach is related to, so not too surprising, but it's important to make the reader aware of.Some questions I would appreciate clarification on:  - How is $f_M$ actually defined? In particular, what do the weights $\mathcal{W}$ really do? A naive reading would suggest that you take a linear combination of the "feature basis functions" -- but I think you must do something more complex since you evaluate in an environment with a highly non-linear reward?  - What is Algorithm 1: Generative Algorithm actually meant to be doing? It seems one could not use this in practice, since it requires computing the EVD, which is only computable if one knows the ground-truth reward -- in which case no need for IRL. So I assume it is meant solely for evaluation -- but I am unsure what this is evaluation. If this is meant to be focusing on the most robust samples, then you should report what probability mass these samples have. Also, is "any W'" meant to be a universal or existential quantifier?Some points to improve clarity. At a high-level:  1. Paper might benefit from separate related work section. Intro is currently serving as this. But it detracts from the story. You could just summarize the IRL problem, what the key deficiency in existing work is that you're trying to solve, and then dive into describing your method and contributions.  2. Many parentheses should be parenthetical, e.g. are known Russel (1998) -> are known (Russel, 1998)  change \cite to \citep in the source.  3. As mentioned before, would benefit from proofreading for grammar. Examples: problem is ill-posed that the -> problem is ill posed: the, an inverse problem that a -> an inverse problem where a; is in a lack of an explanation -> does not explain; variability set -> variable set.Some specific points:  - Abstract, considerable performance  weaselword, how good is it relative to expert, to a baseline, ...?  - Preliminary, description of transition dynamics: being current state s, taking action a and yielding next state s'  does not make the conditionality clear. Perhaps transitioning to next state s' when taking action a in state s.  - Preliminary, MDP\R: I have normally seen this include the discount factor, which is important to understand expert behavior. I think you assume this too in your algorithm? If not, flag this, and describe how you recover rewards without knowing the discount.  - Typo: "experimental trail"->"experimental trial". Before getting into the details of this work, I note that in my opinion it should have been desk rejected for violating the page limit base on the way it is written. The main 8 pages of the paper are far from being self contained, and regularly reference materials from the appendix as integral parts that are crucial for the presentation and understanding here. These include not only methodological illustrations, but also all results establishing the method. In fact, the main paper here does not show ANY result - it only describes the setting for getting them. ALL the figures and tables showing results appear solely in the appendix. If we are to ignore the appendices and only judge the paper based on these main eight pages, then there is no support, no results, and very little in the way of presenting the method here. If, on the other hand, we include the result figures as integral parts of the main paper (as they should be), then it clearly has significantly more than eight pages. Considering most papers submitted to this conference do  try to provide a coherent and relatively self-contained presentation of their work within the page limit, according to the guidelines of the conference, while only leaving technical and supporting details to the appendix, I believe it would be inappropriate to consider this work as meeting the conference page limit.As for the work itself, this paper presents a rather naïve attempt to combine together the UMAP visualization with deep learning. It essentially proposes to consider the coordinates optimized by UMAP as resulting from a neural network applied to input data. Then, instead of adjusting directly these coordinates via the UMAP optimization, the method here continues to backpropagate the coordinate optimization through the network to provide a parametric model, via a feed forward neural network, that embeds the data in low dimensions while preserving the local neighborhood structure in the same sense that UMAP, tSNE and LargeVis do with their nonparametric approach. This neural network formulation can also naturally be extended to consider other loss terms, such as reconstruction loss of autoencoders or any predictive loss (classification, regression, etc.) enabling supervised visualization. From a methodological perspective, this is a pretty straight-forward extension of the UMAP optimization, and does not indicate a clear advantage over it for the main task of unsupervised visualization or dimensionality reduction, neither in embedding quality or scalability. The authors show some interesting results (albeit only in the appendix and not in the main paper) on supervised visualization and out of sample extension speed, but these are not compared to relevant baselines that directly aim to address these tasks. Moreover, there is significant related work that is either ignored by the authors, or just mentioned in passing in the appendix without providing proper discussion and comparison with the proposed method. For example, in A.4, the authors mention topological autoencoders, connectivity-optimized representation learning, SCVIS, VAE-SNE, geometry regularized autoencoders, IVIS, and Differential Embedding Networks, but they do not compare their work to any of these, even though such comparison seems highly relevant here. More work that is completely ignored here includes, for example, Diffusion Nets (Mishne et al., 2015), Laplacian Autoencoders (Jia et al., 2015), DIMAL (Pai et al., 2019). Finally, briefly looking at Duque et al. (2020) cited here, while the main method there uses PHATE coordinates to regularize autoencoders, it seem they have also proposed the incorporation of UMAP loss terms in autoencoders, albeit only mentioned as somewhat of a sidenote together with tSNE regularization in their appendix. A discussion about the difference between these two approaches should be added to the main paper here, and it seems some comparison between them should also be presented to establish the advantages of the proposed approach here. Hence, even without the page limit argument, it does not seem the work presented here reaches the ICLR acceptance threshold without major revision to its presentation, discussion, and results. I must therefore recommend its rejection at this stage. Significance:The paper brings very little novelty or insight. It is unclear that the introduced architecture complexity worth marginal improvements (given high variance and only 5 random seeds) on 2 out of 11 tasks (5 from the main paper and 6 from appendix). This might be a good workshop paper but it clearly does not meet the high acceptance threshold of ICLR.Pros:-A simple architecture modification that might be beneficial to impose a stronger inductive bias on temporal dynamics.Cons:-The proposed algorithm is a trivial architecture change to the conv encoder, the introduced novelty is limited. Injecting the temporal difference inductive bias obviously will be beneficial, given that the sole reason for frame stacking is to infer velocity and acceleration. -In Fig 3 the authors chose to only use 2 consecutive frames for State SAC, while a common practice is to use 3 frames. Using only 2 consecutive frames is not enough to infer acceleration and thus it is not a realistic setup, which makes this comparison meaningless. Also given that the variance is pretty high here, comparing performance over just 3 random seeds is not statistically conclusive. -In Fig 6 the method is only evaluated only on 5 seeds and given that it demonstrates very high variance on the 3 tasks (out of 5) where it outperforms RAD it makes me think that the performance improvements are marginal and not worth introducing complexity. -The ablation study is not very illuminating, this partially comes from the fact that the results are inconclusive (due to the high error bars), and partially because the experiments themselves are not very interesting. Quality:While the paper is well executed and made it significantly lacks on novelty and significance fronts.Clarity:The paper in general is clearly written and well organized. **Correctness Issue**: The loss in equation 2 is the sum of the regular actor loss using the critic + an n-step return version of policy value, which would be reasonable for a policy update. However, the expression provided for the gradient (and the calculations in the code) are incorrect in that they are not the gradient of equation 2 with respect to the policy. In particular, it is not reflective of policy performance, as it fails to account for the fact that the future states in the imagined rollout are also functions of the policy. The resulting policy update in FORK actually consists of:1. Regular actor update using the critic at the current state2. Updating to greedily maximize the reward at each intermediate timestep (which does not reflect the policy performance)3. Another regular actor update from the last state in the rollout.The straightforward way of correcting this to make it optimize the loss would be to simply differentiate through the model, which may not work well as differentiating through learned models often leads to instability (though it may be fine for such short rollouts). Alternatively, a REINFORCE estimator can instead be used for the gradient of the n-step term for stochastic policies. We note that the loss in eq. 6 (if we keep the dependence of the future state on the policy) is also not reflective of policy returns (since it overweights future returns over the present), but can provide reasonable interpretation of the FORK update if we drop the dependence of the future state on the policy as the authors do. The gradient of the FORK-Q variant the authors take from eq.6 is the actually the update we would obtain by using the regular actor update (the $\nabla_{\theta} \pi_{\theta}(s) Q(s, \pi(s))$ term for deterministic policies) but under a different state distribution. Instead of using the state distribution in the replay buffer, the update takes into account the distribution obtained by running the policy for a few steps starting from the buffer distribution. As running the policy from the buffer distribution would result in a distribution closer to the on-policy state distribution, one explanation for why the FORK-Q update would improve is that the gradient is closer to the on-policy policy gradients by changing the state distribution. The same reasoning can be applied to primary FORK update presented, as it includes an actor update on the value in future states (3rd term in my previous list). The issue is with the greedy reward maximization in the inner steps (2nd term), but maybe it simply doesn't hurt on the environments tested or it perhaps takes advantage of a bias-variance tradeoff in greedily maximizing immediate reward for a few intermediate timesteps. I would like the authors to explicitly address these issues in the paper and present a clear explanation of why their FORK should be a better policy update.**Relation with Model Based RL Methods**: Overall, I also disagree with the authors' claims that FORK is very different and much simpler than other model based algorithms. With regards to their comment on how their method is somehow simpler? than rollout based methods, their policy update is using a short Monte-Carlo rollout for estimating the policy gradients; the difference being that the rollouts are being used to update a policy rather than to explicitly plan at test time. The authors' claim that FORK does not require high-fidelity simulation seems unsupported to me, and the claim that model-based RL algorithms use the model in a sophisticated way is vague. In particular, Dyna-style algorithms (like STEVE https://arxiv.org/abs/1807.01675, MBPO https://arxiv.org/abs/1906.08253) which use the model to generate experience to help learn the critic, seem to be using the model in the same way as FORK (in the sense that they only use the model to generate samples with short rollouts). There is also no discussion of methods like ME-TRPO https://arxiv.org/abs/1802.10592, SLBO https://arxiv.org/abs/1807.03858, or the algorithms in https://arxiv.org/abs/2004.07804, all of which only the model to generate trajectories to use with a policy gradient algorithm. Overall, I would appreciate much more discussion about how FORK relates to and compares empirically against past RL algorithms that only use the model to generate samples, as well as clarifying the statements about FORK uses the model in a less sophisticated way.On a separate note, using the model to generate n-step-return estimates of the policy value has been previously done in https://arxiv.org/abs/1807.01675 for example. The key difference is that prior work used it to generate target values for learning the Q-function, while here it is used purely for policy updates. Given how similarly the models are used however, I would recommend discussing this line of work explicitly in the related work, even though they are complementary. **Experimental Evaluation**: Despite the aforementioned correctness issue, the method seems to provide improvements when applied to TD3, and seemingly smaller improvements on top of SAC. However, I find it extremely strange that the authors chose in Figure 4 to plot returns against the number of training steps instead of the number of samples. As acknowledged by the authors, this makes the SAC vs TD3 comparisons incomparable, with TD3 and TD3-FORk enjoying the advantage of having seen twice as many samples. Moreover, comparing only on the number of actor/critic updates isn't even a fair comparison between FORK and baselines, as FORK additionally has to train a dynamics model and reward predictor. I would highly recommend simply showing learning curves with respect to the actual number of environment samples, rather than arbitrarily using the number of actor updates.I would also like to see comparisons against model based RL baselines, particularly MBPO, which uses a Dyna-style update with SAC to compare which method of utilizing the model is better. The authors could also see if the FORK actor update further improves upon MBPO or other model based RL methods.**Summary**:  As it stands, I believe the paper should be rejected due to the correctness issues (and resulting lack of justification for why FORK should give better actor updates), and insufficient discussion of how it relates to prior in model based RL. To consider accepting the paper, I would at least need to see these issues addressed by the authors.Regarding novelty and significance, using a model to predict n-step value estimates (as the paper claims to be doing) or using the model to explicitly adjust the state distribution of the policy update (as I suspect this might be doing instead) for the policy update in an off-policy actor critic algorithm has not been done before as far as I know. However, this change in how the model is used seems fairly minor, and to be convinced it were useful, I would like to see evidence of how it compares against the other model based RL algorithms. In particular, the benefit I imagine it might have over other model based RL methods is in being more robust to poorly fit models, but I would need to see empirical evidence supporting this. ### SummaryThis paper focuses on the field of off-policy reinforcement learning. Specifically, the authors propose a model-based reinforcement learning method on top of actor-critic methods. The proposed method trains a dynamics model and a reward function on the off-policy data with supervised learning, and then uses the trained model to generate synthetic future states and rewards during the actor update. During policy update for a given state, the method computes the sum of the Q value estimate of the state and the Q value expansion for a few steps using the learned dynamics model and reward function.The authors implement the proposed method on top of SAC, and TD3, and evaluate their performances on several MuJoCo and Box2D environments. The experiment results show that the proposed method outperforms the model-free baseline in terms of sample efficiency.### CommentsThe paper is well written and the idea proposed in this paper is really easy to understand. The authors also include a wide suite of experiments to demonstrate the sample efficiency of the proposed method. Despite these advantages, I cannot recommend acceptance of this paper due to the lack of novelty and absence of fair baseline comparison, which I will elaborate on next.First of all, despite the title of the paper, the proposed method is really a model-based reinforcement learning method since the system network and reward network are just dynamics model and reward model. The proposed objective for the policy (eqn 2), is merely a sum of current Q value estimate and Q value expansion for a few steps using the learned model, which has been proposed before in various papers such as [1] and [2]. The only difference is that when computing the gradient with respect to the policy, the authors leave out the gradient that passes through the learned model, which results in biased estimate of the policy gradient. Therefore, Im not convinced about the novelty of the proposed method.Moreover, while proposing a model-based method. The authors do not include baseline comparisons with other model-based RL methods. It is widely known that on low-dimensional control tasks, model-based method outperforms model-free methods ([3]), and therefore merely comparing to model-free baselines is unfair. It would be important to include comparisons to model based methods ([3]).Due to the lack of novelty and fair comparison to existing model-based methods, I cannot recommend acceptance for this paper.References[1] Heess, Nicolas, et al. "Learning continuous control policies by stochastic value gradients." Advances in Neural Information Processing Systems. 2015.[2] Clavera, Ignasi, Yao Fu, and Pieter Abbeel. "Model-Augmented Actor-Critic: Backpropagating through Paths." International Conference on Learning Representations. 2019.[3] Langlois, Eric, et al. "Benchmarking model-based reinforcement learning." arXiv preprint arXiv:1907.02057 (2019). Overview:Overall I believe the comparisons to baselines seem too problematic to understand the value of the proposed method. Regarding the reduced training budget results (Knee schedule can achieve the same accuracy as the baseline with a much reduced training budget), were the baseline schedules also retuned for the reduced training budget? If not, this seems like an unfair advantage to the proposed method. For example, in the MLPerf competition (https://arxiv.org/abs/1910.01500) for ImageNet there have been schedules (consisting of a linear warmup followed by quadratic decay) that have been tuned to reach 75.9% in only 64 epochs, even at massive batch sizes, implying that the baseline schedule in Table 3 could likely do much better than what is reported if it was retuned with the same number of trials as the proposed method, or if a more competitive baseline schedule was used. Some of the results seem misleading as well; in the training curve figures 6, 7, 8, 9, 10 in the appendix, it seems odd that the proposed method only catches up to the (untuned) baselines towards the very end of training, and that this was not mentioned in the main text. For example:-on CIFAR10 the baseline beats the proposed method until the final *5 out of 200* epochs of training-on BERT_LARGE pretraining it is unclear from the plots when the proposed method beats the baseline as the curves are so similar-on WMT14 (EN-DE) the baseline beats the proposed method until the final 54 out of 70 epochs of training-on IWSLT14 (DE-EN) the baseline and proposed method cross each other a few times, the final time being at epoch 41 of 50-on IWSLT14 (DE-EN) with the MAT network, the baseline and proposed method cross each other a few times, the final time being at epoch 330 of 400While it is not invalid for a proposed method to overtake a baseline towards the end of training, these results indicate that perhaps if the baselines were retuned, they could maintain their better performance for the last few epochs of training. Using the same initial LR for the proposed and baseline methods is useful, however it is insufficient to demonstrate that the proposed method could still perform well under different initial conditions. I have additional concerns about the significance of the proposed method over the baselines which I describe below.Regarding comparing to the sharpness of the baseline LR schedules: With fewer explore epochs, a large learning rate might still get lucky occasionally in finding a wide minima but invariably finds only a narrower minima due to their higher density. it would help to show curvature metrics at frequent intervals during training to confirm this hypothesis, and to also show these for the other learning rate schedules compared to, so that you can demonstrate that the proposed schedule achieves something the baselines cannot. The sharpness values in Figure 2 are interesting, but I am unable to determine how impressive they are given that they are not compared to sharpness values for any other schedules, so I dont know what the baseline numbers should be.Finally, it is unclear that the proposed method is novel enough to warrant a standalone paper, without more rigorous theoretical explanations to support the claimed reasons behind its performance.Pros:-It is useful to note that definitions of curvature can be problematic, which the authors do discuss (citing https://arxiv.org/abs/1703.04933)-The breadth of experiments is genuinely impressive, but unfortunately would be more impressive if the breadth was smaller and more careful tuning was done for the proposed method and baselinesConcerns:-In Appendix C when describing your curvature metric, you say The maximization problem is solved by applying 1000 iterations of projected gradient ascent. How was 1000 chosen? Did the sharpness metric stop changing if more steps were used?-What are the stddevs of the results in Tables 6, 7, 18, 19, 20, 21, 26? The proposed results seem very close to the (untuned) baselines, and so it would be useful to understand how statistically significant they are.-Toy problems can be extremely useful to empirically demonstrate this wide vs sharp minima selection phenomena would be useful, such as in Wu et al. 2018 (https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective), or the noisy quadratic model in https://arxiv.org/abs/1907.04164-The curves in Figure 7 seem extremely similar, it would help to plot the loss on a log scale-In Section 4.1, In all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set., is early stopping used in all experiments? If not, why?-Thus, in the presence of several flatter minimas, GD with lower learning rates does not find them, leading to the conjecture that density of sharper minima is perhaps larger than density of wider minima. it is unclear to me how their previous results support this hypothesis; couldnt one retune the learning rate of SGD to find sharper/flatter minima, independently of how many sharp/flat minima exist?Writing:-The experiment details in the intro could be moved to later in the paper (it seems to be repeated in section 2)-Overall the paper length seems like it could be drastically reduced by removing repeated statements-Figures 6, 7, 8, 9 would be much clearer to read if it was a single plot per row, possibly on a log scale on the vertical axis when applicable-For consistency, it would be useful to have Baseline (short budget) also be reported in Table 5Prior work:There are many previous works on explaining the benefits of large learning rates, the most relevant being https://arxiv.org/abs/1907.04595 which seems to make the same case as this paper, but is not cited. Additionally, https://arxiv.org/abs/2003.02218 has more theoretically explanations for this, using the Neural Tangent Kernel literature, and the authors could likely derive similar explanations. In fact, they use a similar schedule as the proposed method, but do not give it a name: The network is trained with different initial learning rates, followed by a decay at a fixed physical time t · · to the same final learning rate. This schedule is introduced in order to ensure that all experiments have the same level of SGD noise toward the end of training. Finally, there are other works that describe how low curvature directions of the loss landscape will be learned first, benefiting from a higher LR, followed by high curvature/high noise directions, which benefits from a smaller LR, described in https://arxiv.org/abs/1907.04164. I believe that a more formal explanation and analysis of the claims on solution curvature density should be provided.Additional feedback, comments, suggestions for improvement and questions for the authors:I believe that fairer experimental setup would be similar to the following:-pick several competitive LR schedules for each problem (not just the standard ones)-identify a similar number of hyperparamters for each*, such as number of warmup steps, decay values, decay curve shapes, etc.-retune each schedule and the proposed method for the same number of trials, using similarly sized search spaces for each (ideally one would also retune the initial/final learning rates, momentum, and other hyperparameters for each, but this may be too expensive)-select the best performing hyperparameter setting for each schedule, and rerun it over multiple seeds to check for stability*it can be problematic to make comparisons across methods with different numbers of hyperparameters even with the same tuning budget, because it is impossible to construct the same volume hyperparameter spaces with different numbers of hyperparameters, see https://arxiv.org/abs/2007.01547 for a more thorough treatment This paper proposes 3TConv for interpreting 3D CNN when operating video tasks i.e., video action recognition task discussed in this work. The motivation of this paper is good but lacks extensive experiments to validate the hypothesis.Concerns:1. The authors only conduct experiments using two datasets: UCF101 and Jester, which are not sufficient. UCF101 is more scene-related dataset but is kind of small that may easily cause overfitting when training from scratch on this dataset. Jester contains sufficient samples to train deep neural nets but is more temporal-related. So a direct comparison on these two datasets is not representative. I think larger scene-related datasets e.g., Kinetics-400 or even mini-kinetics should be used in the experiment. I also suggest adding more experiments using other common large-scale termporal-related datasets e.g., Somethig to something and Egogesture to validate the performance.2. Section 4.3 on page 6, the authors draw the conclusion ''This is likely due to 3DConv being a more complex and exible model and better at exploiting the availability of more data'', which I think is not appropriate. In table 1 (1)  we can see that UCF101 has very low accuracy  (caused overfitting), which means the model has not been trained appropriately. How can authors compare results with a model that not trained properly? And even with pretrained model, I suspect results on UCF101 is still very low; (2) Regarding the Jester dataset, I do not think it would have such a big difference with training from scratch and pretrained as jester contains sufficient samples that allow you training from scratch (e.g., R(2+1)D performs well). The authors have not shown pretrained R(2+1)D and pretrained 3DConv that are all necessary in order to validate the performance.3. On page 7,  ''This discrepancy is explained by the fact that models with more parameters can learn from bigger datasets better than smaller or more restricted models.'' There is no actual experiment  to validate this conclusion. If authors would like to validate this result, I think extra experiments with deeper model using 3DConv and 3TConv should be added e.g., using ResNet-34 backbone.4. On page 7, ''We can only compare the model trained from scratch due to time limitations involving the download of the Kinetics dataset for a full comparison and we reserve this for future work.'' I am afraid this reason is not acceptable. If the authors have not prepared for this, this work is not ready for ICLR.5. Figure 3, as I mentioned before, I do not think training from scratch on Jester will be different from pretrained on Jester. I suggest authors to check if the experimental setting for training from scratch. I suspect the model is not fully converged.Questions:1. Figure 1 (1) what is the rule of picking a channel? why not do an average across all channels? Is this a cherry-pick? (2) I think it is doable for using CAM for visualization which might make more sense (3) Can authors explain that if compare saliency map on the first row, I think 3DCOnv visualization makes more sense than 3TConv.Summary:The motivation of this work is good (try to give an interpretation for 3D CNN). However current lacks enough experiments to validate the performance and I also suspect the experimental setting as the issue I mention when using the jester dataset. Strengths:- The paper addresses an interesting problem. 3D convolutional networks are complex, they require lots of computation, and they are also difficult to interpret- The proposed approach is easy to understand.Weaknesses:- The writing of the paper could be significantly improved. At the moment, many parts of the paper feels like a documentation to the code, which is not how a conference paper should look like. The formatting, and the notation could be improved, e.g. 3TConv is a cumbersome term that is used continuously throughout the paper. Many variables are bolded and they also contain overly many subscripts. Some of the paragraph "subheaders" are weirdly formatted. There are also many grammar mistakes, typos, incoherent sentences. Overall, the paper looks unprofessional.- The action recognition experiments are not convincing. It is not surprising that 3TConv based models perform better than the 3D CNNs in the low-data regime, i.e., due to their large number of parameters, 3D CNNs require lots of data to work well. Even the 2D CNNs would most likely outperform 3D CNNs in such cases. The more interesting experiment would be to show how the performance of 3TConv and 3D CNNs differ in the large-scale regime, i.e., on the commonly used datasets such as Kinetics or Something-Something. The lack of such large-scale experiments make the current draft look incomplete. Most modern action recognition models are evaluated on these standard benchmarks.- The complexity analysis should include not only the number of parameters but also the FLOP counts. Most current action recognition models use FLOPS to assess the efficiency of their model.- I also found the interpretability experiments in the current draft unconvincing. The authors claim that their proposed model is more interpretable than 3D CNNs. However, even after reading the draft, and studying the figures, I couldn't fully understand the basis of such claims. The authors present a few qualitative visualizations, which are very difficult to interpret objectively (as admitted by the authors in the paper). Furthermore, the figures are labeled and described so poorly, that it makes it even more difficult to understand what exactly they depict. Overall, I was confused by these experiments. This is unfortunate because the entire story of the paper is tied to these experiments.- Instead of comparing their approach to 3D CNNs, the authors should consider R(2+1)D as their main baseline. R(2+1)D models are simpler, more effective (in many cases), and much easier to interpret than 3D CNNs.Overall:- The paper is poorly written, and the current experimental section is inadequate. Therefore, I recommend rejecting the paper. The paper presents a score-based approach for causal graph discovery from heterogeneous data. The approach is claimed to be guaranteed to find the correct graph skeleton asymptotically, and can detect more causal directions than previous algorithms designed for single domain data.  Pros:-The paper addresses an important problem of causal discovery from heterogeneous data.Cons:I have doubts about the correctness of many conclusions in the paper. For example, it looks to me that for Theorem 1 to be true, the pooled data $D_C$ must be i.i.d. samples from distribution P(V,C). However, $C$ is defined as the domain index. The problem setup simply assumes we are given $n$ data sets from $n$ domains. Then its natural to assume the domain index C will stand for 1, 2,  &, n. Under this setting, the pooled data are not i.i.d. samples, Theorem 1 will not hold, and I believe some other conclusions in the paper will not hold either. For Theorem 1 (and other conclusions) to hold, I believe one has to assume the domain index C is a random variable, such that the data sets from different domains are drawn randomly based on some P(C) and P(V|C). However, this may not be a realistic setting, and is not how the problem is set up in the paper. I dont think the real data sets in the experiments satisfy this setting (not clear how the synthetic data sets are generated).Overall, I think the paper has mistakes and vote for reject.  This work studies the problem of online or incremental learning in temporal graphs (dynamic networks), and more precisely, whether past data can be discarded/ignored without losing predictive accuracy under the assumption that there is the presence of a distribution shift. This question has been essentially investigated over the years in various contexts, e.g., relational learning and classification in dynamic or time-evolving networks. It is also completely obvious that forgetting older data, especially under the assumption of a distribution shift, makes sense and is the correct thing to do. This is exactly what has been done in time-series forecasting for decades. The problem formulation is unclear and can be more precisely defined and motivated appropriately. This needs to be fixed. Are the class labels of a node changing over time, so if a node has label A at time t, then at time t+1 it could have label B, etc. This doesnt seem true, as it seems the class labels of the nodes are static, which is unrealistic in many cases. How are the graph snapshots created? How was the timespan selected? What does every time step represent (1 hour, 5 minutes, etc.)?  Also, are the node features changing over time? This doesnt seem true, but if this is the case, then it is unclear why this would be the case in practice (it would be great to provide some motivation for this, or an example application or problem where this may be true). There are many assumptions that make this problem unrealistic. Furthermore, there have even been works that study the dynamic node classification problem previously, see [1-2] below. The contribution and novelty of this work is unclear. Many important related works are missing. There have been countless works that have studied the impact of the temporal window and its size, as well as discarding past data, and using different amounts, as well as the representation of that past data (exponentially weighting links). This work also studies the impact of ignoring past data on node classification. Furthermore, many of the standard papers on this topic are seemingly missing such as CTDNE [10] and JODIE [6]. There are many other important works on incremental/online learning in dynamic and streaming graphs that are missing in the paper, see [4]-[13], which need to be referenced and appropriately discussed, mentioning the differences, and so on. The real contribution seems to be a new dataset with a controlled distribution shift. But putting this work into perspective with the related work, and explicitly stating the differences would help clarify the contribution and better position this work with respect to the existing literature.Pros  + Paper is well-written for the most part and easy to understand  + New dataset with controlled distribution shiftCons  - Limited technical novelty and contribution  - Important related work is missing and should be discussed appropriately to better position the work   - Problem formulation is unclear and can be more precisely defined, and motivated.   - Previous work has studied essentially the same research question and findings are obviousThe results and findings are in terms of time steps, however, the notion of a time step is not the same for every graph, nor is it ever discussed how the time steps are actually derived. Does every time step represent 30 seconds, 5 minutes, 1 hour, 1 day, etc. Furthermore, the results only make sense for the specific time step chosen for each graph. For instance, it is mentioned that GNNs achieve 95% accuracy with a small window size of 3 or 4 time steps. However, if the time step is extremely large then the result/findings change. And so all the findings in this paper and the discussion depend precisely on the data and the authors choice of how to create the time steps, and what granularity to use, which isn't discussed. This issue was discussed extensively in previous work. Minor comment: the labels in nearly all the figures are too small to read.1. Time-evolving relational classification and ensemble methods2. Deep dynamic relational classifiers: Exploiting dynamic neighborhoods in complex networks3. A task-driven approach to time scale detection in dynamic networks4. Dynamic Node Embeddings From Edge Streams5. Afraid: fraud detection via active inference in time-evolving social networks6. Learning Dynamic Embeddings from Temporal Interactions7. Node Embedding over Temporal Graphs8. Representation Learning in Continuous Entity-Set Associations9. Efficient representation learning using random walks for dynamic graphs10. Continuous-Time Dynamic Network Embeddings11. Dyn2Vec: Exploiting dynamic behavior using difference networks-based node embeddings for classification12. Real-Time Streaming Graph Embedding Through Local Actions13. Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph Representation Learning This paper proposes a curriculum learning method to handle noisily labeled data. The idea is to introduce a consistency measure instead of directly apply a loss function for the typical supervised learning, where the specific consistency is measure for both temporal dimension along neighboring steps and spatial dimension over different data augmentation samples for a given real sample. The consistency measure is applied for self-supervision while the loss function is applied for supervised learning. The final optimization is managed between the two components through weighting parameters, such that the training is made through a migration from a supervised learning to self-supervision by gradually adapting the weighting parameters. Evaluations are reported on Cifar10/100, WebVision, and ILSVRC2012.  The motivation and rationale of this work makes sense, and the proposed method appears to be correct, at least at the conceptual level though I did not check in detail. However, the paper has several issues with which I dont think that the paper is ready for publication. Let me elaborate my comments in detail.Ill begin with relatively minor ones. The paper appears very rough, with a lot of grammatical errors, typos, or misleading/incorrect statements. The caption in Fig. 1 is not consistent with the description in text. The the experiment section, it is stated that that three datasets were used for evaluations but in Table 1 it appears there is the fourth dataset ILSVRC2012 used, which never mentioned in the text, nor is mentioned in the paper on how that dataset is used (a portion like WebVision or the full). The whole paper ended up with no conclusion or discussion.Now let me move on with more major comments. First, the title of the paper is misleading. The title reads: ROBUST CURRICULUM LEARNING: FROM CLEAN LAEL DETECTION TO NOISY LABEL SELF-CORRECTION. However, the proposed method, together with all the reported evaluations, focuses on learning the noise (in the labels); neither clean label detection nor noisy label correction is addressed.Second, the datasets used is small in scale and simple in complexity. Basically the paper uses Cifar10/100 and the first 50 classes of WebVision (but not the full WebVision dataset). For ILSVRC2012, since there is no documentation at all in the paper, I have no idea whether the full set is used or only part of it, but from the reported data in Table 1, I believe that only the first 50 classes were used also. That is not the state-of-the-art.Third and more importantly, the reported evaluation comparison is NOT fair. In the comparison studies reported in Table 1, some of the baselines (e.g., MentorMix) actually used the full sets of WebVision and ILSVRC2012 with 1000 classes and the proposed method only used the first 50 classes! Thats the huge difference! This is like comparing apple and orange. In fact, the performance results of MentorMix reported in Table 1 were actually not for ImageNet, but for TinyImageNet and MentorMix did have performance results for ImageNet but the authors of this paper referenced the wrong results. I am not sure whether the authors actually were aware of this by having read these papers when they referenced these baselines data. If they were not, that is an unfair comparison the least. If they were, thats more serious, ethical issue. Existence of much stronger results:I don't get why majority voting is claimed to be the "state-of-the-art" technique. If I'm understanding it correctly, the majority vote technique can only handle a number of corruption points up to O(K), K being the number of voters. Furthermore, since the voters more or less split the dataset, in order to maintain the accuracy of each voter, the number of voters can't be too large, and is usually O(1). Therefore, this majority vote approach can only handle O(1) number of corruption. More specifically, for the case of kNN, the certified accuracy (Theorem 2, ) becomes vacuous as soon as the number of corrupted points $e$ becomes greater than $k$.On the other hand, there are techniques developed from the robust statistics community that can be robust against an $\epsilon$-fraction of corruption points, that is, if there are a total of $N$ training points, it allows $\epsilon N$ number of points to be corrupted. For example, Sever [1], a recently developed robust supervised learning algorithm, guarantees $O(\sqrt{\epsilon})$ generalization error under $\epsilon$-fraction of arbitrary corruption. Such a guarantee is much stronger than the ones majority voting approaches are able to achieve. Thus, I'm having trouble appreciating the contribution of this paper given the existence of much stronger results.Relevance to the field:While prior approaches like DPA also suffers from the same weak/trivial guarantee, they are at least meta-algorithms that allows one to plug in any base learners depending on the application. The method developed in this paper, however, only works on kNN. And let's be honest, not many modern ML applications use kNN with the slightest chance. So I don't see much empirical value nor any significant theoretical contribution. [1] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, Alistair Stewart. Sever: A Robust Meta-Algorithm for Stochastic Optimization.  The authors introduce a new algorithm AdaLead to solve the problem of efficient design of biological sequences and FLEXS, an open-source simulation environment for sequence design. The structure of this paper is somewhat unconventional, the paper would benefit from more rigor and structure. There are 4 sections, the method section which is the section that contains the novel part (section 2) is a fairly short section (less than 2 pages). The experimental setting is crammed together with the results. So, the details of the outcome of the experiments are lost in the various implementation details. The experimental setting can be moved in a section where the authors talk about the benchmarks alone. Or perhaps in the appendix.  The authors talk about the contributions at the very end of the paper. A reader wants to know what the contributions of the paper are in the introduction section. It wasn't very clear for example that FLEXS was the main contribution of this paper until the very last section. FLEXS was barely mentioned at the beginning of the paper. The second contribution of the paper according to the authors is the novel algorithm AdaLead. This evolutionary algorithm is poorly explained. How are the recombine and mutate operators defined for example? In addition, this algorithm seems simplistic and I'm not sure we can call this algorithm a novel contribution. The authors should make more explicit why is this evolutionary algorithm a novelty, how does that improve wrt EAs? The authors dismiss the use of Bayesian optimization by saying that BO doesn't perform well on high-dimensional space and citing a tutorial by Frazier 2018 that says that usually BO is used on problems that have less than 20 dimensions. I'm not entirely sure why this is a problem for this work where Table 1 shows that the applications considered have a few variables only, up to 14 if I understand correctly. In addition, some BO frameworks such as SMAC (https://www.automl.org/automated-algorithm-design/algorithm-configuration/smac/)  were used on problems with more than 100 variables. The authors compare against BO by implementing a BO algorithm based on EI and an evolutionary sequence generator. While it is not clear what this means, this is surely an uncommon BO algorithm. Why didn't the authors use one of the standard BO packages available? The experimental results are unsatisfactory without a proper BO comparison. Finally, I question the interest of the ICLR audience for this type of study. While the paper would follow in the track: "applications in audio, speech, robotics, neuroscience, computational biology, or any other field" I found the algorithm being a specific application of a simplistic search algorithm to the domain of sequence design. What are the lessons learned? Is there any general insight that became available from this study? The improvement wrt to SOTA is shaky. Minor remarks: - Figure 2 is not cited in the text. - CMA-ES is not introduced - even if the CMA-ES is a popular algorithm it should be properly introduced.- Model-free evolution in Figure 2 is not introduced. What is that algorithm about? - Figure 2, I'm not sure what is the y-axis in the performance plot. In these types of papers, we usually show simple regret or similar performance quantities. It seems that the y-axis is the higher the better and that 1 is the max achievable? - Figure 2 should give a better understanding of what the figure means and what are the methods that perform best by writing about that in the text of the paper. - Not clear to me why the queries to the surrogate model are sample-restricted. This is an uncommon setting in Bayesian optimization where usually the budget is restricted for the evaluation of the latent black-box function.  - Not entirely clear what the role of the cardinality |S| in the optimization is. Is this an optimization with unknown feasibility constraints problem formulation?  This paper claims to make three contributions:  (i) a theoretical and experimental demonstration that the driving force behind performance in certifiable training methods is loss landscape smoothness (ii) a new training method and domain designed to have a smooth loss landscape (iii) an evaluation showing their new training method performs comparably to the state of the art.Pros:* The paper provides some experimental analysis support a hypothesis that has been conjectured since Mirman et al. (2018), that smooth loss landscapes produced by smoother transformers (zSmooth/hSmooth) would improve training.  * The paper does show experimentally that certain methods tend to have less smooth loss landscapes.  * The theoretical result connects the smoothness of the relaxed gradient approximations to the smoothness of the loss landscape, which may be useful for designing future certifiable training methods.Cons:* In terms of accuracy and verified error, the proposed system performs comparably to some pre-existing systems.  The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work.* The authors do not compare it to the state of the art, COLT [1], which achieves for example 21.6/39.5 (standard, verified error) compared to this papers 31.49/49.42 results on CIFAR10 2/255.  This is a much more significant difference than for any of the comparisons made to other work in this paper.* It is unclear how fast the proposed method is.  No comparison appears to be made on training speed or memory usage compared with other systems.  Given comparable accuracy and verified error results, I would not see a reason to use this system if this one is slower.* I am not sure about the utility of the experiment on tightness and Figure 3.  Different methods could have different scales of loss while producing similarly verifiable networks (as this paper in fact demonstrates).  Further, even the worst case margin for a class could be fooled by a network with entirely  zero weights.  * Loss value is also compared between methods in Figure 1.* The paper claims that state of the art methods such as CROWN-IBP and CAP are held back by their non-smooth loss landscapes while failing to meaningfully outperform them.  5 out of 10 of the standard accuracies are worse than CROWN-IBPs for example.* Section 4.1 claims that the experiments show that the non-smoothness of the relaxed gradient approximation of linear relaxations negatively affects their performance yet the evidence presented is circumstantial, and not capable of indicating causality.* Section 4.2 claims that the theoretical analysis states that some landscapes are more favourable  than others, whereas in fact it only demonstrates the smoothness of some landscapes not their favourability. * The paper leaves out additional related work.  In particular, I would like to see a discussion of the difference between the analysis used by Fastened Crown [2].While this paper does provide a new technique with some theoretical justification, unfortunately neither the theoretical justification or experimental results are significant enough to recommend acceptance.  Furthermore, because the papers central assumption, that a smooth loss landscape leads to better results, is unsupported by their own experimental results, I must rate the paper a rejection.[1] Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap. In International Conference on Learning Representations, 2020.[2] Lyu, Z., Ko, C.-Y., Kong, Z., Wong, N., Lin, D., and Daniel, L. Fastened crown: Tightened neural network robustness certificates.  In AAAI, 2020. Summary: This paper does empirical analysis on an object detector, esp. how it fails due to slight shift of objects in videos. By using different padding schemes for conv filters, the authors find the padding scheme is the root for such failures.Reasons for score:1. This paper is not written professionally. The introduction is very short. The following sections are more like experimental notes than a technical paper. 2. The analysis is mostly based on a single use case, i.e. an SSD object detector for traffic lights. This study is highly insufficient. More use cases and application scenarios should be investigated.3. Experimental evaluation is also highly insufficient. Only changing the input image size by 1 pixel doesn't reveal much. A lot of ablation studies and changing of padding schemes should be evaluated, preferably on a few datasets of different tasks (classification, detection, segmentation, etc.).Other comments:I agree that padding could lead to biases when doing downsampling using stride > 1, as illustrated in Fig. 6. However, this seems to be easily fixed by doing random image flipping as one of the data augmentations? It would even this bias. So this issue may not be practically severe, although it is worth study per se.Minor issues:The PDF file of this paper seems to have issues. The printed characters are blurry, and acrobat reports errors when open it. But some other PDF readers can open it.  ## SummaryThe paper suggests a novel framework (coined L2E) to learn a policy that is optimized to adapt quickly (and exploit) a wide range of unknown opponents.To do so it trains a base policy that is optimized so that it can maximize its expected reward against a variety of opponents using only a few updates of its parameters (i.e. a few gradient steps of a straightforward optimization problem).The various opponents that are used for the training of this base policy are generated in two steps.First, given a base policy, an "hard-to-exploit" opponent is generated adversarially (in a procedure coined hard-OSG), to minimize the the reward that the base policy would get by adapting to it (using the few updates that are allowed to it in its adaptation step).Then, given a base policy and a "hard-to-exploit" opponent, more diverse opponents are sequentially generated (in a procedure coined diverse-OSG) by optimizing their expected reward against the base policy while maximizing their "diversity" with the "hard-to-exploit" opponent and the already generated diverse opponents.More formally, this diversity between two policies is defined (and optimized) as the MMD between the distributions over the trajectories that they generate (when the policies are seen as MDPs "playing" against the given base policy).The base policy is then trained iteratively (as described above) against the diverse opponents, that are themselves generated (as described above) with the current iterate of the base policy.After exposing this training procedure, the authors evaluate L2E on 3 toy games, showing that the trained policies are indeed able to benefit from little adaptations to a variety of heuristic opponents and perform better than some baseline methods.They also empirically confirm that their "diversity-regularized policy optimization" indeed generates diverse policies.Last, the authors empirically show the effect of their hard-OSG and diverse-OSG modules on the performance of L2E.## Pros- This paper is tackling a very relevant, interesting and difficult problem.- I find the general approach of optimizing a policy to be able to "rapidly" and exploit a broad range of opponents to be very exciting.- To the best of my (admittedly limited) knowledge, the suggested approach is significantly novel.- While maybe a bit "roughly used" the diversity-inducing regularization term, using the MMD over the distribution of trajectories induced by the policies is interesting and potentially has a broader applicability than only L2E.## Cons- After careful reading, several key points remain unclear to me. Most notably, after training of L2E and when facing opponents with unknown policies, how does the base policy adapts? Is it done using eq. 2? If yes, is the expectation over the trajectories approximated with the actual observations made during the observation? How many observations are being used? If my understanding is correct, clarifying those points would help put in perspective how fast it actually takes for L2E to adapt in practice.- It looks (to me, because no comment is made about it in the manuscript) like L2E must scale terribly with the size of the action space. First it must be extremely computationally intensive. While this remains feasible for the toy games that were used in the experiments, I am having very high doubts that this would scale well with larger games (even BigLeduc poker is ridiculously tiny compared to actual poker). At least, some comments about the computational aspects of L2E, or empirical evidence that L2E can handle larger games would be nice.- A point is made, several times in the manuscript, that the base policy becomes "more robust and eliminates its weaknesses by learning to exploit hard opponents". First, it is not really clear what is precisely meant by this. Without further assumptions on the class of games, I do not really see why the base policy would be having a good expected reward before adaptation (either in average over a broad class of opponents or against the optimal opponent), or even less why it would be hard to exploit (especially after adaptation). In fact the empirical results suggest that the base policy is breaking even against a random opponent (before adaptation) at Leduc poker, which seems rather weak to me.- The last point brings to a more general issue. I understand that the value of the contribution is more empirical than theoretical. Yet, it is absolutely not obvious to me whether L2E is supposed to converge at all (let alone having a clear idea about to what kind of solution it would converge). I am not a specialist of game theory, but I understand this is likely a difficult setting to analyze. At least, empirical evidence on the convergence of L2E would, in my opinion, strengthen greatly the manuscript.- I find that the writing could also be improved. While the algorithm is admittedly hard to fully describe in a very succinct way, the amount of repetions or redundancies in the first 6 pages suggests that L2E could be more concisely and sharply introduced. The split between the main text and the appendices seems a bit arbitrary to me as I definitely think more content about related work should be exposed in the main document. At the very least, appendices A and B should be referenced in the main text. As it stands, there is literally no indication in the paper that the related work section and the algorithms can be found in the supplementary materials. There are also a lot of imprecisions in the form of somewhat vague claims or missing important details. In addition to the ones already mentioned, I would for instance take the example of section 3.2.2. where it is not clear to me how the first policy of diverse-OSG is generated in the absence of the hard-OSG module. And in that same paragraph the bold statement that hard-OSG helps enhance the stability (what is meant by that exactly?) is not clear at all to me. It is also claimed in 3.2. that positive returns are guaranteed against opponent without a clear style (whatever that precisely means). I see rather mild empirical envidence of this but certainly not guarantees. Another minor point is that I find the Theorem 1 to be a bit weirdly formatted. I imagine the theorem is supposed to be the statement that MMD equals 0 iff the two distributions are equal, but then, the following sentence shuld be more clearly separated (as not part of the theorem) and it should be more clearly stated that the result is not a contribution by adding the reference where this result first appeared (Gretton et al. '07, I assume). If not, the derivation of the gradient computation does not really constitute a "theorem". Last, there are a number of typos, or verbs missing throughout the manuscript, that should be easy to fix (sorry, it's really not convenient for me to list them without line numbers...).- I'm a bit puzzled by some implementation details of the diverse-OSG. Notably, there seems to be no weighing on the MMD term in eq. 11. That seems pretty arbitrary to me. Could you elaborate on that? More specifically, I would imagine that if the MMD ways too little, the generated policies will be roughly identical while they will be diverse but potentially arbitrarily bad (in terms of expected reward). Also, while I don't have an issue with the somewhat arbitrary choice of an RBF kernel, I am a bit more puzzled by the choice of a width of 1. But I could imagine I'm missing an argument as to why this is a good choice.- In Table 1, L2E is reported to have a positive average return against the oracle, which is defined as "making decisions based on perfect information". It's not clearly described what those decisions are but unless they are pretty bad, there is no way L2E or any policy can win against it. (And it should be pretty easy to find and implement the optimal strategy for the perfect information game.)## Reasons for scoreWhile I really want to emphasize that the problem is very interesting and that I like the premise of L2E, I think the paper, in its current form, is missing the target.The main reasons can already be found in the "cons" that I listed.To elaborate a bit further, I think that either the selected games for the experiments are too small and toy-like for a purely empirical paper (in contrast with AlphaStar or Liberatus achieving superhuman performance at games like Starcraft 2 or heads-up no limit hold'em, although they definitely tackle a different, and probably simpler problem).In this current form, I consider the experiments as a crude proof-of-concept, which could be totally fine if there were more theoretical analysis to support the suggested approach.## Questions during rebuttal periodI think several questions have already been raised in the rest of my review.Most importantly, I would really love to understand how the base policy is updated in a "real setting", after training (see my cons #1). ##########################################################################Summary:This paper proposes to use deep learning neural network to model eigenfunction while solving a generic eigenvalue problems. Since such problems is very generic this application of such framework are huge and the experiments show some promising results. ##########################################################################Reasons for score:  Overall, I vote for rejecting (see cons for more details). The idea is very good, but many important information are missing. For example it is impossible to reproduce the results since we don't have any clue on the used networks.##########################################################################Pros:  1. This paper proposes a new framework for generic eigenvalue problem which seems promising.2. The framework is able to deal with several eigenfunction and almost assure the orthogonality of the function.3. This paper provides comprehensive experiments, that shows that such methods cam catch complex eigenfunction. ##########################################################################Cons: 1. The notation is confusing on some parts. First please use <,> for scalar product instead of (,) since you have functions with two arguments. Second I don't w_u is a good notation for the weights , for example in equation 6 we don't know which weights it is. Perhaps using a function which gives the associated weights could help.2.  When dealing with multiple eigenfunction, there is en additional term is the equation 6 for the orthogonality constraints. However it is not clear on how it is approximate, I assume that it is something similar to equation (3). Please clarify.3. Again on the orthogonality constraint, the complexity of such constraint is huge. It is impossible to deal with many eigenfunction. Such limitation can drastically reduce the interest of the framework, I think there should be some discussion on this point.4. On the implementation details, we don't have any clue on which kind of neural network are used. Are they just dense network?5. Again on the implementation details, the loss function of equation 6 is highly non-convex and I assume that there is several way to try solving it. I don't think it is possible to deal with all the networks (the u) at once, so I figure there is a solving scheme (most likely alternate minimization) behind. Please clarify as it may answer to one of my previous remarks. ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above #########################################################################Some typos: all pages: be careful with the citation, when citing a paper please add parenthesispage 2: There is an extra parenthesis in equation 2 The paper argues that there are some challenges in learning text representations with vanilla contrastive learning techniques, due to the metric-aware property of  KL divergence as well as the difficulties in selecting good negative examples. It then presents two remedies: employ a Wasserstein constraint to the critic function and a simple active sampling strategy for negative examples.I think the paper is not strong enough to get into ICLR. Firstly, the Wasserstein constraint has been proposed to improve general representation learning in Ozair et al. (2020). The novelty of this work is very limited compared to Ozair et al. (2020). The active sampling contribution is a simple heuristic. Secondly, the authors only compare the proposed method with the non-CL baseline. A vanilla CL baseline should be included in all the experiments. Finally, the improvement margins are also pretty small.The writing can be much better if the authors can use examples for concrete tasks. For instance, I am not sure the distinction between u and v representations after reading sec. 2.2. **Summary:**The paper focuses on the behavior of gradient descent in one-hidden-layer neural networks (with fixed second layer weights), in a teacher-student setup. In this setup, the labels are generated by an unknown teacher network with the same architecture, and the student aims at recovering the teacher weights. The main ambition of the paper is to provide guarantees for the convergence of gradient descent on the cross-entropy loss, for an input data which comes from a non-trivial model, here a mixture of Gaussians. The algorithm is moreover initialized with a tensor initialization method, which will be crucial to assess its performance. The paper provides a precise theorem to guarantee convergence of this algorithm, and studies how the learning process can depend on the parameters of the mixture of Gaussians. Importantly, the theoretical analysis also relies on the knowledge of these parameters. In particular, they derive limit regimes in which learning should be very hard, e.g. when the variances of the mixture are either very small or very large. They finally provide numerical evidence to support their claims. Given the length and available time, I did not check the calculations given in the supplementary material.**Overall decision:**Considering all my criticisms and comments, I cannot recommend publication of this paper at ICLR 2021 as it is. For me to reconsider this decision, the authors would have to slightly improve the quality of the writing (see the remarks and typos), to improve the numerical analysis, and to provide a more convincing presentation of the impact and novelty of their theoretical results with respect to the previous literature. **Strengths of the paper:**- The authors provide an involved tensor initialization method to start the gradient descent algorithm, which provably reaches a basin of attraction of a local minimum very close to the true weights.- The bounds derived in Theorem 1, both on the convergence of the algorithm and on the distance between the local minimum and the true weights are quite explicit, in particular as a function of the number n of samples, the dimension d of the data, and the number K of hidden neurons. Having these two bounds together is important and interesting, as it allows not only to probe the optimization but also the generalization properties. Moreover, the assumptions needed on the activation function (Assumption 1) are quite generic, and allow for a large class of activation functions. - They show that the sample complexity needed for precise estimation with this algorithm is in the scale $d \ (\log d)^2$. I wonder if such a bound is sharp? Also perhaps the authors could comment on the work of [Mei, Bai & Montanari, 18], which showed that in this scaling, the landscape of some estimation problems is already trivialized (i.e. close to the population loss landscape). They showed it in particular for a single-hidden-node model: it could be interesting to compare their result with the present paper in the limit $K = 1$.- Corollary 1 allows to study the impact of the parameters of the mixture of Gaussians (i.e. the structure of the data) on the learning procedure, with the mentioned algorithm. In particular, they show that either too large or too small variances can be detrimental to optimization. While it is intuitive that large variances would harm optimization, the finding on small variances is particularly interesting, as one would expect that small variances in the Gaussian mixture would imply an easier optimization problem. On this point, Fig. 2 is very qualitative, but shows the dependency of the sample complexity on the parameters of the mixture of Gaussians. In particular, Fig.(2b) manages to show the interesting divergence of the sample complexity when the variance goes either to $0$ or $\infty$. - Figures 3 and 4 do a decent job at showing the dependency of the final convergence time on the parameters of the mixture, of the convergence rate on the size of the hidden layer, and of the distance of the true weights to the critical point achieved by gradient descent on the number of samples. The results are quite consistent with the theoretical analysis. **Concerns and remarks:** - In the introduction, the authors explain that they consider a setup in which the labels are generated by a ground truth network, and provide some recent literature on this hypothesis. They however do not mention that this assumption is known as the teacher-student setup, and it has been studied for a long time in the statistical learning community (in particular from a statistical physics point of view in which such a setup is very natural). The paper should correct this point by providing a much more exhaustive view of the literature on this topic. For instance, they can refer to [Seung, Sompolinsky & Tishby 92], [Engel & Van den Broeck 01]. See also [Goldt & al, NeurIPS19] for recent applications to neural networks. - Fig 1 (in the numerical experiments) is somehow unclear. While the authors pretend that the sample complexity needed to recover is indeed almost linear in d, this is not obvious from the picture. Moreover, the difference in orders of magnitudes between n (from 6000-60 000) and d (from 1 to 30) indicate that, even with a quasi-linear dependency, the prefactor would be huge, and the authors should discuss this point. My two main concerns are the following:- First, as emphasized in the beginning of Section 4, the tensor initialization is crucial, as it actually returns an estimate already in the basin of attraction of a critical point very close to the ground truth.  However, in Section 5 (on the numerics), the authors precise We use random initialization rather than tensor initialization to reduce the computational time, without any justification of how this could impact the results. This reduces greatly the relevance of these numerical results, and their relation with the theoretical findings. This also raises the question: is the tensor initialization numerically tractable? If the tensor initialization provably returns an estimate in the correct basin of attraction, this algorithm is actually doing the most important part of the estimation, and replacing it with a random initialization close to the ground truth removes a lot of the relevance of the theoretical findings (as obviously, when starting in a convex region, gradient descent will work). - Secondly, the following bold claims can be found in the abstract and the main text, and are very emphasized:1) Instead of following the conventional and restrictive assumption in the literature that the input features follow the standard Gaussian distribution, this paper, for the first time, analyzes a more general and practical scenario that the input features follow a Gaussian mixture model of a finite number of Gaussian distributions of various mean and variance.2) This paper provides the first theoretical analysis of learning one-hidden-layer neural networks when the input distribution follows a Gaussian mixture model containing an arbitrary number of Gaussian distributions with arbitrary mean and variance.3) This is the first theoretical characterization about how the input distribution affects the learning performance.4) Theorem 1 provides the first theoretical guarantee of learning one-hidden-layer neural networks with the input following the Gaussian mixture model.    In my point of view, these are exaggerated statements. While it is true that there is ample room for new studies of non-trivial input distributions, several previous and impacting papers have followed very similar approaches, beyond [Du&al 17] which is the only paper cited by the authors in this context. For instance, the following (very incomplete) list of papers all either consider training a one-hidden-layer neural net on a dataset with a non-trivial covariance, or a mixture of Gaussians data model:    1. Mei, Montanari & Nguyen [PNAS 2018] study one-hidden-layer networks in the mean-field limit trained on a large class of distributions, including a mixture of Gaussians with the same mean (see Fig 1 of the paper).     2. Li & Liang [NeurIPS 2018] studied over-parameterized one-hidden-layer nets "when the data comes from mixtures of well-separated distributions".    3. Yoshida & Okada [NeurIPS 2019] study one-hidden-layer neural networks with inputs drawn from a single Gaussian with arbitrary covariance.    4. Goldt et al. (arXiv:1909.11500 and arXiv:2006.14709) study one-hidden-layer networks with inputs drawn from a wide class of generative models.     5. Mignacco et al. (arXiv:2006.06098) give exact equations for the size-one minibatch SGD evolution in a perceptron (i.e. a single-node network) trained on a mixture of Gaussians.     6. Ghorbani et al. (arXiv:2006.13409) consider labels which depend on low-dimensional projections of the inputs, which, I believe, is very related to a mixture of Gaussians.Some of the papers in this list (e.g. [1,2,4,6]) also provide a rigorous analysis of some of their results.While, to the best of my knowledge, the theoretical results of the present paper (i.e. global convergence guarantees for gradient descent with tensor initialization, for data coming from a mixture of Gaussians) are indeed new, their impact and novelty is, I believe, exaggerated. In particular, similar results already exist in the literature cited above, and the new results of this paper should be discussed in comparison to them. **Minor points and questions:**- The notations section is very long. While some precisions are useful, many are very standard (N, Z, R, or the transpose, or the L2 norm) and, I believe, do not have to be reminded. Similarly, the footnote at the end of page 5 is not necessary. - The paper studies one-hidden-layer neural networks with fixed second layer weights. The authors should mention that this setup is known as the committee machine, and they should refer to some literature on this (besides some references given in the concerns section, one can for instance refer to [Aubin&al NeurIPS 2018, Schwarze&al 92,93, Monasson&Zecchina 95] and many others).- Is it possible to add a noise in the gradient descent algorithm without affecting the theoretical findings? Even an uncorrelated noise (i.e. Langevin dynamics), as I expect that the noise in plain SGD will not be easily tractable.- Algorithm 1 requires a constant learning rate: I wonder if the authors tried (even just numerically) to see if the bounds could be improved by considering an adaptive learning rate (for instance with a linear decrease)?**Typos:**- At the beginning of the introduction: Neural  neural.- Just after, I believe: theoretical underpin of leaning neural networks  theoretical underpin of learning in neural networks.- Again, just after: lack of the theoretical generalization guarantee  lack of theoretical generalization guarantees.- In the Contributions paragraph,  the etc when listing the applications of the Gaussian mixture model does not read well. - In the Contributions: One interesting finding is the One interesting finding is that.- Just after: all the variance approaches  all the variances approach.- In Section 2: Let kappa denote the number that kappa =...  Let kappa = &- Below eq.(6): sigma_l \in R  sigma_l \in R_+.- End of page 4: more details of  more details on The authors tackle the important problem of feature selection. They propose to use differentiable gates with an RNN architecture to select different subsets of features for each time point. I think the idea and method are interesting, and the method could be useful. However, I have crucial problems with the way the paper is presented. Most importantly, the authors describe the l_0 relaxation of Bernoulli random variables as if it is their own contribution. They describe existing known results under a section titles Methodology as if they are the first to present Bernoulli random variables to feature selection or that they are the first to relax them using the Gumbel Softmax trick. They also use the word: we derive (p.3). This is wrong! And misleading! The same relaxation appears in [1] and used for model sparsification, the descriptions are almost identical to what appears in [1] with almost zero credit to the authors in [1] (a citation appears in related work in a different context). Bernoulli relaxation was already used for feature selection, in [2], and [3], these papers were not even mentioned. The reader can think the authors are the first to introduce such relaxation into the problem of feature selection, while this is again, clearly wrong.The authors are well aware of that this relaxation was presented in [1], and in the experiment section they describe the baseline which solves (4) by citing [1] (citation [18] in their paper), this is again in contradiction to the way they describe the relaxation as if it is their own contribution. Putting these CRITICAL comments aside, I think the results are misleading. Specifically, comparing the average number of selected features to the (constant) number of selected features of the non-adaptive method is misleading. You need to compare the union of selected features by your method to the constant number, otherwise, there is no way to infer if this feature selection method can result in any compression of the model or could lead to training or inference speed up.  Given that this is what you measure since you still need all the features to use your model, what are the advantages of the method? Only interpretability?The authors do not explain how the method is used in the testing phase, is the randomness removed? How exactly?The authors do not explain how training/ testing is performed, this appears in the appendix but should be moved to the main texts.The authors should compare the method to the distribution suggested in [1], which seems more suitable for feature selection than the Concrete distribution (used by the authors).Citations are not in the correct ICLR format.Some pros: I like the examples used in the paper as well as the comparison to ARM, ST, ST-ARM.To conclude, I am voting to reject the paper, based on all the reasons mentioned above.[1] Louizos, Christos, Max Welling, and Diederik P. Kingma. "Learning Sparse Neural Networks through $ L_0 $ Regularization." ICLR, 2018.[2] Yamada, Y., Lindenbaum, O., Negahban, S., & Kluger, Y.  Feature selection using stochastic gates. ICML, 2020.[3] Bal1n, Muhammed Fatih, Abubakar Abid, and James Zou. "Concrete autoencoders: Differentiable feature selection and reconstruction." ICML. 2019. In this paper, the authors compare several kinds of MBRL models, including observation prediction, to show which approach is better. In conclusion, The models that contain prediction on both a reward and an observation outperform the ones with only reward prediction. [Quality]This paper requires a revision for enhancing clarity. For example, Section 4 seems exceedingly long, and it contains too many redundant details, which should not have been emphasized.[Originality & Significance]This paper provides neither a new method nor a different perspective.[Strengths]+ The authors showed the effect of using visual observation prediction for MBRL models. It might show the way for further MBRL models.+ It was mentioned on the paper that prediction accuracy of observations and rewards and exploration performance is on the trade-off relationship.[Weaknesses]- The number of parameters on the model that predicts only rewards is much (about a hundred times) smaller than other models. Additionally, the authors used SV2P and PlaNet to implement each experiment; it is doubtable that comparisons are fair enough.- There are no clear and consistent differences between models with an observation prediction, which reduces this work's novelty. It is better to use four different SOTA models or four modified models from one backbone model instead of modifying two models.- The trade-off relationship is mentioned, but there is not enough quantitative and theoretical analysis to show this point. [Comment]- There is no bolded item in Table 2; it seems to be omitted, or the result is not strong enough. Summary: The paper studies the problem of safety verification of model-based RL controllers in continuous action&state spaces. The paper proposes a verification method based on reachable set analysis. The proposed method either verifies the controller to be safe or identifies the subset of initial states that are safe.Strengths:i) The motivation, organization and the overall writing of the paper are clear.Weakness:i) The problem of verifying properties of NN based controllers is studied in the literature [1,2,3,4,5,6,7,8,9,10,11,12,&], which is not discussed in detail in the paper (i.e., there is a paragraph present in the supplementary material). Moreover, there are no experimental comparisons made to any of these previous works.ii) The tested experimental domains are not the best representatives of the control problems motivated in the paper. They are extremely small in size and do not really require NN approximation. As such, the resulting NN architectures are very small (as seen in the supplementary materials).References:[1] Verisig: verifying safety properties of hybrid systems with neural network controllers, Ivanov et al. HSCC-19.[2] Learning and Verification of Feedback Control Systems using Feedforward Neural Networks, Dutta et al., 2018.[3] Reachability Analysis and Safety Verification for Neural Network Control Systems, Xiang and Johnson, arvix 2018.[4] Efficient Verification of Control Systems with Neural Network Controllers, Yang et al., ICVISP-19.[5] Verifying Deep-RL-Driven Systems, Kazak et al. NetAI-19.[6] Guarded Deep Learning using Scenario-based Modeling, Katz MODELSWARD 2020.[7] Reachability Analysis for Neural Agent-Environment Systems, Akintunde et al, KR-18.[8] Verification of RNN-Based Neural Agent-Environment Systems, Akintunde et al, AAAI-19.[9] Reachability Analysis for Neural Feedback Systems using Regressive Polynomial Rule Inference, Dutta et al., HSCC-19.[10] ReachNN: Reachability Analysis of Neural-Network Controlled Systems, Huang et al., TECS-19.[11] Reachable Set Estimation and Safety Verification for Piecewise Linear Systems with Neural Network Controllers, Xiang et al., ACC-18.[12] A Reachability Method for Verifying Dynamical Systems with Deep Neural Network Controllers, Julian et al., arvix, 2019. This paper focuses on the problem of augmenting the user-provided knowledge into Neural networks. The core premise of the work is that the existing approaches tend to replace the True and False with values in [0,1] and then use maximum/minimum operators which have vanishing gradients. The key idea is to use logit representation and therefore, the authors hope to avoid the vanishing gradients problem. The paper presents empirical results to argue that embedding knowledge helps. I think its an interesting paper but somehow the comparison with prior work is not properly contrasted, which makes me believe that the paper has probably reinvented quite a bit of aspects. An important missing work is [1], which in fact uses product and addition instead of minimum/maximum.  (It is well known that exists and forall operators can be expressed using disjunction and conjunction)Also, Xie et al (NeurIPS 2020) build on [1], and  do perform empirical on the same VRD dataset and achieve significant improvements; their numbers seem better than what is reported in Table 1.In light of the above remarks, it is hard to understand the new contribution of the paper. [1] https://arxiv.org/abs/1711.11157 (Published at ICML 2018) This paper introduces a network architecture search (NAS) suitable for a feature pyramid network (FPN) that provides notable detection accuracy for objects at every scale. Based on the decomposition of FPN structure as (multi-scale) feature generation and feature utilization, the proposed NAS offers a new design strategies for both components. For feature generation, NAS super-net is trained to find the optimal selection of whether to reduce, maintain, or extend feature resolution after each module. For feature utilitization, it defines conditions for efficiently selecting the optimal FPN architecture. The proposed MSNAS yields the better accuracy than its backbone FPN and other NAS methods on the COCO object detection benchmark dataset.I have a concern in technical novelty.The only novel element considered in the proposed NAS method is to learn the stride value that controls the feature resolution at each module. The most contribution of this paper is very similar to FPN-NAS in that it re-designs FPN by choosing the optimal connections between all the modules. Furthermore, this search method does not retain the main claim of FPN, in which features at every level can be trained to have the same level of semantics.The method used for Feature utilization is a very simple rule not relying on training to select the final feature layer to be connected to object detection heads. This simple rule can speed up training at the expense of marginal accuracy. This can be seen as part of engineering rather than technical contribution.I also have a concern in its presentation. Algorithm 1 is very important for understanding how to implement the proposed method. However, it is very difficult to understand this algorithm due to missing definition of some terms such as CrossoverEncoder, MutationEncoder, CrossoverFeatureStride, MutationFeatureStride etc. Summary:This paper uses deep neural networks into the ecological inference problem and shows its effectiveness by using large-scale datasets of the Maryland 2018 midterm elections. Some experimental results have been shown.Pros.1. Ecological inference is an important problem in political science for modeling individual-level voting behavior given only aggregate-level data.2. The authors attempt to apply the recently advanced technique (I.e., DNN) to the ecological inference problem.3. This paper provides a case study that analyzes real-world voting behavior by using various kinds of datasets.Cons:1. The proposed approach is not new.2. Related work is not adequately cited.3. This paper is not well-written, especially the Experiment section is not readable.Reasons for score: I think the task that the authors address is interesting and important. However, this paper does not present new technical contributions and related works are not listed adequately. The authors attempt to provide the case study of the Maryland 2018 midterm elections via deep learning approaches in ecological settings. But, the Experiment section is not well-organized, and I did not find insightful results from this manuscript. Accordingly, my opinion is that this paper is not ready for publication. Detailed comments:1. Important references are missing. So, It is not clear how far the proposed method has pushed the boundary of existing technology. Firstly, as the authors mentioned, the ecological inference is strongly related to the problem learning with label proportions (LLP), the authors should add more discussion.  For example, in [R1], the ecological inference problem has been formulated as LLP. Also, ecological inference relates to other problems discussed in the machine learning community. The common aim is to learn the individual-level models from only aggregate-level data; for example, distribution regression [R2], multiple-instance learning [R3, R4], and collective graphical models [R5, R6]. It would be a great idea to survey and discuss the relationships with these methods.2. In the approximation procedure for the loss function, the assumption is not clear. Is that each individual adopts one of the candidates based on the shared probabilities that is the average over $N$ individuals? If that is the case, the loss is the logarithm of multinomial distribution (i.e., Eq. (4)); this is equivalent to that has been used in the Collective Graphical Models (e.g., [R6]). 3. Section 3.2 is not readable. Does each paragraph in Section 3.2 correspond to which Figure? What is Figure 3.2? 4. In experiments, the authors should specify the experimental settings. For example, what are few-shot settings? Also, the authors should clarify some hyper-parameters such as optimizer selection and learning rate.     [R1] Tao Sun, Dan Sheldon, Brendan OConnor, A probabilistic approach for learning with label proportions applied to the US presidential election, in ICDM, pp. 445-454, 2017.     [R2] S. R. Flaxman, Y.-X. Wang, and A. J. Smola, Who supported Obama in 2012?: Ecological inference through distribution regression, in Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 289298.     [R3] H. Hajimirsadeghi and G. Mori, Multi-instance classification by max-margin training of cardinality-based Markov networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.     [R4] H. C. L. Law, D. Sejdinovic, E. Cameron, T. C. D. Lucas, S. Flaxman, K. Battle, and K. Fukumizu. Variational learning on aggregate outputs with Gaussian processes. In NeurIPS, pages 60846094, 2018.     [R5] D. R. Sheldon and T. G. Dietterich, Collective graphical models, in Advances in Neural Information Processing Systems, 2011, pp. 1161 1169.     [R6] D. Sheldon, T. Sun, A. Kumar, and T. G. Dietterich, Approximate inference in collective graphical models, in International Conference on Machine Learning (ICML), vol. 28, no. 3, 2013, pp. 10041012.Minor comments:- Modify the mathematical symbols to the italic font;  for example, unit $i$ and features $X$ in the sentences of Section 2.2.1.- Letters in the figures are small and hard to see. #### SummaryThe paper discusses an interesting direction to efficiently approximate the loss function in the ecological inference problem, which enables extensions using linear models, deep neural networks, and Bayesian neural networks. The proposed approach was evaluated using Maryland 2018 midterm elections data on a range of tasks.#### Strengths- The paper tackles one important and practical problem of ecological inference: inferring labels from label proportions, which is applicable to a lot of settings, one of which is "voting" as studied in the paper- The paper discusses an interesting direction to approximate the loss function of the ecological inference problem in an efficient manner, which enables different extensions, especially using Bayesian neural networks.- The paper evaluates the proposed model using real Maryland 2018 midterm election data and produces interesting insights#### Weaknesses- The paper is not easy to follow. Apart from various typos (see details below), I think the structure of the paper could be improved significantly to make it more accessible. For example:  - (1) Poisson binomial/multinomial losses were not introduced early in Section 1, which makes it hard to follow and understand the "Contributions" described at the end of Section 1  - (2) Although described in text in Section 1, it's still pretty unclear what the input data are. I'd suggest discussing the input data formally at the beginning of Section 2 before describing the techniques in details  - (3) It's very unclear what the evaluation tasks are (especially for people who are not familiar with the data and/or domain) and the intuitions behind why the tasks are suitable to evaluate the effectiveness of the proposed methods- The paper lacks details on how the proposed methods (and baselines) are implemented. In addition, there are various baselines/methods included in the "results" section but it's unclear what they are in details.- In addition, there are various typos / minor writing problems. Here are some of them:  - Sec 3: "This gives us a useful test case for examining not correlations in voting patterns between races." "not correlations"?  - Sec 3: "We demonstrate two practical use cases for out methods. " -> "We demonstrate two practical use cases for our methods. "  - References in many places are without parentheses   - poisson -> Poisson This paper presents a GCN-based solution for multi-step spatio-temporal data forecasting. The main contributions are 1) a spatio-temporal joint graph convolution network to simultaneously capture spatial and temporal correlations; 2) a combination of the sequence decoder and short-term decoder to alleviate the error accumulation when modeling the temporal dependencies. I agree with the incremental contribution of this paper. To verify the effectiveness of their approach, the authors conduct experiments on three real-world datasets.However, I have the following concerns:Presentation: The major concern is the writing of this paper. There are many parts (e.g., notations, statements) that are unreadable or hard to understand. For example, 1) What is ST in GCM in Figure 2(a)?2) In page 4, x \in \mathbb{R}^n. What is n here? It seems to be inconsistent with the notation N in Section 3.1.3) This paper introduces a learnable spatial mask matrix W_{mask}, but how to use it is not clear.4) Does Figure2(c) in page 5 mean Figure 2(b)?5) In the last equation of page 5 (for computing Y_{s_out}), what is t? Moreover, since W_F is of shape F by 1 and W_t is t by M, how can we multiply W_F by W_t?6) How to obtain Y_{m_out} in equation 7?7) Many typos and grammatical errors. I only point out several of them. Spatial-temporal data is -> are, correlations STG2Seq -> correlations. STG2Seq, Laplqaacian->Laplacian, which needed -> which is needed, we conjecture the reason is-> we conjecture that&.Technical:1) Novelty of the proposed model is limited. The proposed spatio-temporal joint graph convolution is very similar to the concept of 3D GCN [1]. However, in this paper, I could not see any discussion about the difference and comparison between them.2) In section 3.1, this paper assumes that the graph is undirected. However, the propagation of traffic in spatio-temporal domains are certainly directed (e.g., downstream, upstream). 3) What is the difference between the temporal attention in Section 3.4 and an FFN (two fully-connected (FC) layers)? In my view, this paper only uses two matrices for the dimension transformation, which is the same as two FC layers.Experiments:1) It is more reasonable to compare the proposed model with CNN-based solutions in the Mobile traffic dataset, as there is no explicit graph structure (only Euclidean structure exists). Baselines like ST-ResNet, ConvLSTM, DeepSTN+ or DeepLGR should be included for comparison.2) According to the paper, I cannot see any detail of the multi-run experiments (e.g., in Table 1 and Figure 3). How about the stability (i.e., variance) of the proposed method?3) The experiment shows little improvement to the existing approaches in the first two datasets. There should be significant test (e.g., student t-test) conducted to make sure the experimental result is reliable. Considering the limited improvement, it is also questionable whether adding such complexity to the model is worthy. It may not make sense to improve the MAE by 0.02 in the first dataset by increasing the runtime a lot.4) No experiments to show the effects of the spatio-temporal kernels. Why should we set it 3x1, 1x3 and so on?5) It seems that the MAPE in Table 1 largely exceeds 100%, which is unusual to see. What is the unit of MAPE here?Minor issues:1) All acronyms should be expanded for the first time in text (e.g., HA is not expanded. What is HA? Historical average?).2) There should be one space before each citation. For example, DCRNN(Li et al., 2017) -> DCRNN (Li et al., 2017).3) In the first paragraph of Section 1, the paper claims that Typical applications include &, traffic road condition forecast (&, Liang et al., 2018), & and geo-sensory time series prediction (Li et al., 2017). However, Liang et al., 2018 is for geo-sensory time series modeling while Li et al., 2017 is for traffic prediction. Please carefully read these papers before citing them.4) It is difficult for those who are not familiar with the Inception Network to understand how your model draws insights from it. In addition, the paper proposing the Inception Network should be cited.5) No future work discussed in this paper.In summary, I recommend not to accept this paper in its present form.Reference:[1] Yu et al., 3D Graph Convolutional Networks with Temporal Graphs: A Spatial InformationFree Framework For Traffic Forecasting, Arxiv 2019. This paper proposes a spatial-temporal graph neural network, which is designed to adaptively capture the complex spatial-temporal dependency. Further, the authors design a spatial-temporal attention module, which aims to capture multi-scale correlations. For multi-step prediction instead of one-step prediction, they further propose the sequence transform block to solve the problem of error accumulations. The authors conducted experiments on three real-world datasets (traffic on highways and mobile traffic), which shows their method achieves the best performance.Overall, I think the problem of capturing spatial-temporal dependency shown in Figure 1 is interesting. But the description of the method is unclear and the writing of this paper still needs improvements. Furthermore, I didn't find enough novelty that meets the standard of ICLR. Concerns: Methods: I don't understand why the authors introduce spectral graph convolution networks while it seems that the authors only use GCN in the spatial domain. The equation (4) is confusing: $X$ in the left changes to be $T_K(L)X$ without more explanations. It's unclear about how to use the mask matrix $W_{mask}$ in the proposed model.The equation $C_{gl} = \sum_{i=1}^T \sum_{j=1}^K C_{out}$ is confusing. There are many method descriptions like the above examples, which the authors should describe more clearly.  Typos: Page 3 Last Line: LaplqaacianPage 5 Paragraph 2 Line 12: node $v_i$asPage 8 Line 4: performed respectivelywithoutPage 8 Line 17: both of them..  Summary:The authors propose ASTI-GCN to solve the multi-step spatial-temporal data forecasting problem. The model uses a convolutional block to model the spatial-temporal correlations and an inception attention based module to capture the graph heterogeneity. They evaluate the proposed method on three different traffic prediction datasets.Pros:1. The problem of traffic prediction is important.Cons:1. The contributions are limited in this paper. The ideas of jointly model spatial-temporal information via convolutional layer (see 3D GNN to model irregular regions [1] and 3D CNN to model regular regions [2]), and multi-scale spatial-temporal modeling (see [3, 4]) are not new. These papers have been released for more than one year. Especially, the multi-scale motivations in [3] [4] are almost the same in the paper. The only difference is that this paper involves attention to weight different scales, which is also a very common practice. Thus, I think the contributions are not enough to be accepted by top machine learning conference.2. The experiments could be improved.- Besides the ablation studies in this paper (add modules to the base model), it would be more convincing to add the ablation studies by removing some components. Since only combining one module (results in Table 2) performs worse than AGCRN, removing some components and keep the rest could provide deeper analysis. In addition, it would be more convincing to conduct all the ablation studies on all three datasets.- To support the claim of region heterogeneity, it would be more interesting to show some case studies to verify the motivation and see the reasons for the improvement. Otherwise, the improvement may come from the increasing of the number of parameters.- It would be better to show the error bar for each result since the improvement in some datasets is limited (e.g., PeMS04). 3. Some figures could be improved. For example, some arrows in Figure 2(b) are broken.[1] Yu, Bing, et al. "3d graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting." arXiv preprint arXiv:1903.00919 (2019).[2] Chen, Cen, et al. "Exploiting spatio-temporal correlations with multiple 3d convolutional neural networks for citywide vehicle flow prediction." 2018 IEEE international conference on data mining (ICDM). IEEE, 2018.[3] Geng, Xu, et al. "Spatiotemporal multi-graph convolution network for ride-hailing demand forecasting." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.[4] Cui, Zhiyong, et al. "Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting." IEEE Transactions on Intelligent Transportation Systems (2019). The paper proposes a method to infer goal distance maps for use in path planning using transformer networks. These 2D distance maps are evaluated on both planning and combined mapping and planning experiments on synthetic data.While the proposed method seems technically sound and based on the experiments works as intended, it is not clear how the proposed system would be applicable in practice. Additionally, the motivations for the proposed approach over classical planning methods, such as run-time and incomplete environments, were not explored.From a machine learning point of view, there is nothing particularly novel about the proposed method which uses well known and existing methods to build the planning framework. This is not a negative point for methods which develop interesting algorithms for applications. However, in the case of this paper, there are many questions regarding the applicability of the resulting system.The proposed method produces a distance to goal map based on an obstacle representation. While such a distance map can be used with search methods to find a path, it does not constitute a path planning method itself. Furthermore, the proposed method is by design limited to 2D spaces. While this to some extent might be justifiable for ground-based robots, it precludes the use of the approach in challenging outdoor environments or on flying robots. More importantly, this reduces the applicability this method has in manipulation severely as the vast majority of manipulators used these days have six or seven degrees of freedom. These limitations are furthermore exaggerated by the very coarse environment discretization used in the examples of 0.25 cm or 10 degrees. While 0.25 cm grid sizes for navigation can be acceptable in large outdoor environments, the 10 degree discretization used would be unsuitable for most manipulation tasks.One of the motivations given by the paper for the development of an end to end learning framework is the computational efficiency of having to only perform a single forward pass over the input. This, in theory, enables fast recomputation of a solution should the environment change. Given the 2D grid structure of the input and outputs, one would expect a comparison to widely used search methods such as A* and R*. These methods would be very fast to compute on the problem setups used in the experiments while being provably optimal. Providing a run-time comparison against such methods would have been good to see. Similarly, comparisons to RRT based methods, which in the limit are also provably optimal, would be appreciated.While the combination of planning and mapping was interesting, it is not clear why this was needed, as methods capable of accurately mapping large scale environments exist without relying on learning systems. As the focus of the paper was on planning, this also caused some confusion on their relation.Overall, while the proposed method works, it is not clear what practical benefit it has. As a deep learning approach, it requires a significant amount of training and has no guarantees to provide predictable results at all times. This is in stark contrast with decade-old methods such as A*, RRT*, and trajectory optimization which have such guarantees. The major limitation of the proposed method to 2D spaces severely limits its applicability with questions regarding scalability given the sizes used in the experimental section. The paper is concentrated at dealing with the data missing problem of MTPP and applies an AAE for the incomplete multi-categorical MTPPs.  First, the problem description appears questionable. Point processes are a class of stochastic processes for modelling discrete event sequences in a continuous time domain. They are statistical models and have a well-defined mathematical meaning. The expression of incomplete point process is quite confusing. One possible reason is that the authors fail to distinguish between the model and the data. One can say incomplete data or incomplete observations of a model, but incomplete model is not acceptable unless properly defined. Therefore, the proposed method seems not specifically for point processes but for the sequential data. The authors seem to misunderstand the difference between empirical distribution and probabilistic distribution. The probabilities, e.g., $p_d(\mathcal{H})$, are actually empirical distributions of the time. Here the authors see the arrival time of the events as a random variable, and $t_{ij}$s are independent samples of the random variable, which is unrelated to point processes. The percentiles used in the algorithm are essentially the cumulative empirical distributions of the arrival time. If point processes need to be considered here, the authors should define their probabilistic structures as they are probabilistic models, where the intensity function is often inevitable unless otherwise defined. I believe the confusing mathematical formulation should be considered as a fatal flaw. This work studies the generation technique for multi-category MTPP using adversarial autoencoders for sparse and incomplete datasets. To address the sparsity and incompleteness, some pre-processing and post-processing methods are utilized to adapt the data to AAE.Strengths: The model setup is reasonable, and the idea is straightforward to understand. I recommend rejection of the paper for the reasons below.Weakness: The major concern is the contribution of the work is very limited. The generation framework is a combination of adversarial auto-encoder and feature mapping encoder (decoder), where the AAE part is completely same as the original work in [Makhzani et al., 2015]. In my eyes, the only contribution is the addition of the feature encoder and decoder. Although the author named it like that, the so-called encoder and decoder are in fact some heuristic pre-processing and post-processing (shift and normalization) of the raw data. Some specific concerns: the notation in Eq.(2) is confused. If c_j is defined as the category, then c_j should \in [1,m] not j. I understand what the author means by the current notation. It is better to define e_i=(t_i,c_i) where i \in [1,n] and c_i \in [1,m]. In algorithm 1, the steps 1-3 are some heuristic pre-processing of the raw data. Why do that? Why change the timestamps to days and scale it to a probability value? Any intuition behind this operation? My understanding is the raw data is shifted, scaled and normalized to [0,1]. Add some theoretical analysis why performing that pre-processing will help AAE handle the sparse and incomplete data. In experiments, there is only one baseline model to compare with. Add more DNN-based generative baseline models will make the experiment more convincing. In experiments, although the author claimed 3 real datasets, I did not see any experimental results of the 3rd (stackoverflow) dataset. Typo: above Eq.(5), significance-->significant Clarity:---------Major:******It is not clear what the actor critic algorithm is. What is presented in Fig 1 seems to be policy iteration, and Theorem 1 seems to be stating policy iteration (yet $\pi^{n+1}$ has not been defined). Minor:*****1) What is $V^{\pi^n}$? Is it the value function of the policy $\pi^n$.2) What is the difference between $\pi^{*,n}$ and $\pi^{n+1}$.3) What is $\pi$ is eq(3)?Quality:-----------1) The argument following eq (15) is very informal: "the number of samples in $\mathcal{D}^{n+1}$ is only a little more than in $\mathcal{D}^n$".2) For the contraction property of the Bellman operator, the author can consider citing standard text books instead of "https://towardsdatascience.com/ mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f"Significance:------------------1) The  authors claim "We present a convergence analysis for actor-critic methods by Banachs Fixed Point Theorem" to be one of the contributions. The fixed point theorem here is to show that Bellman operator is a contraction operator (a well known fact).2) The experiments are on only 5 domains. Overall Feedback: Adding the KL based penalty is perhaps a useful idea (this may also not be an entirely new one). It will be great if a) the algorithm can clearly stated,  b) the novelty of the method pointed out clearly by comparing it with related works c) the ideas can be supported by formal theoretical statements. Before beginning with the review, I would strongly request the authors to correct grammar errors, spelling errors, and notation inconsistencies which seriously hinder understanding in some cases.This paper explores error accumulation in actor-critic methods. The authors claim to present an analysis of approximation error in actor-critic methods (though I couldn't find any concrete analysis of this fact) and then suggest that bounding the KL-divergence of the policy against the previous policy aids actor-critic algorithms by deriving an upper bound on the change in TD error due to faster changes in the policy.My overall opinion is that this paper restates results that are already well known and the general theme of the method ECAC is already explored in RL literature (see KL-control literature). So, I don't really think there is any novel concept presented in the paper. I move to specific points next:- Theorem 1: I don't see what's novel here. Such results have been shown in the absence of function approximation for off-policy actor-critic (Degris et al. 2012). What's new here?- Analysis of approximation error: In the abstract, it is claimed that the paper performs an *analysis* of this error. I could not find this anywhere in this paper. If this refers to Section 3.2 (Hidden dangers in AC methods), I would not call that "analysis". It states some known facts without actually discussing anything formally there. So, either this claim should be removed or a valid analysis needs to be presented.- The reason for why KL-constraints work: I can see what the derivation does and how the authors arrive at the conclusion of KL-constraint I do not buy the argument that KL-constraints help learning because they control TD error -- they do stabilize the critic, as would a smaller learning rate for the policy would and lead to lower accumulation of error (for e.g., see A theory of Regularized MDPs, Geist et al. or Leverage The Average: An Analysis of KL regularization in RL) and these analyses are far more sophisticated that the argument presented in this paper. - The empirical results in the paper are OK, but the baselines utilized are a bit suboptimal, for e.g. SAC and TD3 in HalfCheetah -- having used these myself, it seems like official codebases of these papers do get better performance than what is reported.  I think the numbers are this unreliable.I think overall the algorithm with the KL-constraint is a good idea -- as has been explored several times in literature -- but I don't think this paper cites that work, nor compares to them along with not a rigorous enough evaluation and poor writing quality. Summary:The paper revisits previously known definitions of "adversarial accuracy" (and its complement: adversarial risk) which captures the accuracy (and risk) of learning models under adversarial perturbations of the test instances. The paper argues that the established (called standard) definition of adversarial accuracy has issues that need to be resolved, and then this work proposes a new definition (called genuine adversarial accuracy). The main issue they mention with previous definition is that they come at odds with accuracy. Their definition aims to fix this by giving an alternative way of defining accuracy. They then study the robustness of 1-NN under the newly proposed definition and claim that this algorithm is the most robust classifier according to this new definition. Proofs are differed to supplemental material. ######################################################Summary of reasons for the score: Unfortunately, the papers treatment of the subject is not formal enough. The new definition also does not seem to be an improvement in any clear way. There is no given clear intuition for the new definition (which is quite complex to state) and what is exactly achieved. The theorem statements that are based on the definition are not formal either.  ########################################################## Pros: Understanding the adversarial robustness (e.g., generalization of adversarially trained models) from a theoretic perspective is a very important and nontrivial problem. So, alternative (new) definitions that might allow us to understand the picture better are potentially highly valuable. This paper aims to improve the state of the art along this direction.  #############################################################Cons:  1.Unfortunately, the paper does not clearly set objective goals to be achieved. The "issues" with previous definitions are not clearly discussed.2.There are issues with the level of formality of the stated results, and the proposed definition is too complex to even read. No clear evidence is presented to justify the usefulness of the new definition in resolving challenges that previous definitions do not. A work aiming to settle definitional issues needs to be a lot more formal and precise.3.Previous work has addressed some of the issues that the paper seems to be aiming to address. The papers [1,2] below (not cited) already point out a simple variation of the definition of adversarial risk under which there is no trade-off between accuracy and robustness. A simple way to prevent the definition of robustness to be at odds with accuracy is to require the adversarial sample (x^*) to be a misclassification. Also note that all these variants of definitions are equivalent to the main-stream definition (Def 1 in the paper), if one assumes that the underlying concept function (i.e., the ground truth) remains robust under the allowed amount of perturbation. So, in contexts such as image classification in which human is the ground truth and is already assumed to be robust to the amount of allowed noise/perturbation, the mainstream definition that is used has no issues. Issues arise when such definition are applied to a *different* context in which the ground truth might *not* be robust under the amount of allowed perturbation. [1] Suggala et al. "Revisiting adversarial risk." aistats 2019.[2] Diochnos et al. "Adversarial risk and robustness: General definitions and implications for the uniform distribution." NeurIPS 2018. #######################################################Main comments and questions:The abstract states that:"Based on this result, we suggest that using poor distance metrics might be one factor for the tradeoff between test accuracy and l_p norm-based test adversarial robustness."I am not sure how this conclusion was obtained. On a related note, I do not follow how Section 4 is expanding on this thesis.Note that l_p norm-based attacks are used since bounded l_p norms of perturbation does not seem to change human's judgement (there might be other perturbations that have this property too) so an *attack* under such limited perturbations is already an issue that needs to be addressed (but using such metrics for positive results could be questioned).In problem setting page 1: why do you pick and fix the cross-entropy loss? It seems you want a general treatment of the definition of adversarial examples, and so other losses could be used as well?I think definition 2 is confusing two different notions: adversarial loss and surrogate losses used for training for computational reasons. Once a candidate classifier f is trained, to know its risk/error under adversarial attacks, one shall *not* pick x* based on a loss such as cross-entropy loss. x* is simply a point in the Ball of allowed perturbations around x such that f(x*) is different from c(x) (assuming that c(x)=c(x*)  this means x* is also misclassified). A different question is how to train a model with minimum empirical adversarial loss. To do so, one needs to use computationally efficient approaches that might lead to using some surrogate loss instead of the 0/1 loss. top of page 2:"f2 and f3 are equally robust according to this measure. Thus, the maximum norm-based standard adversarial accuracy function cannot tell which classifier is better."I cannot follow the argument here. In Def 3:Consider a case where X contains a ball of non-zero volume. Then, *every* point in R^d will belong to VB(X), as for every x' there will be two different points x1, x2 in X that have the same distance to x'. Then VB(X)^c will be empty, which makes your X_\eps (allowed perturbation sets) to be empty. Is that so?In your notation in the first bullet of page 5, what is the distribution over S_{exact(\eps))? Note that stating the set alone is not enough to figure out what the expected value means, and since we are dealing with continuous settings the exact distribution is important to be state.Theorem 1 is not formally stated. "adversarial accuracy only use each point only once" is not a mathematically well-defined statement (especially in the continuous-domain regime). If you desire concrete properties that your definition implies, state those properties mathematically (e.g., if you are talking about a probability here, write it down formally).Theorem 2 is not formally stated. It is not clear what "it is the almost everywhere unique1 classifier that satisfies" means. What is the probability measure space here? You seem to say that "with measure one" in some probability space "(1-NN) classifier maximizes genuine adversarial accuracy..." if this is correct, the theorem needs to be restated differently.#########################################################################More minor comments and typos:The notation a_{std; max}(\epsilon) is missing (and shall somehow denote) the function (f1 or f2) as well, as it depends on the function.page 3"adversarial training try to" -> "tries to"pag3 4:"dataset is consists" -> remove "is""The other class (class B) is consist of" -> consists ofpage 4: the points on the red circle are denoted as red, but you said the points "points inside the circle classified as class A". I guess you mean the points on and inside the circle are in class A? This paper proposes a new measure called "genuine adversarial accuracy" for adversarial robustness of a classifier. The key idea is that 1) they calculate robust accuracy when the perturbation norm is exactly $\epsilon$, not smaller than $\epsilon$ like standard robust measures, and 2) each point will not be used multiple times in calculating the robust measure. Their theoretical results say that 2) is really the case, and an 1-NN classifier is most robust w.r.t. the proposed measure.Strength of the paper1. It provides a new robust measure, which potentially provides no trade-off solution for the adversarial robustness, if a distance leading to good generalization is properly chosen.2. The information in Table 4 gives good insight of the benchimarking datasets, which can potentially benefit researchers of this literature. 3. Many interesting insights, such as regular adversarial training may push the model to behave similar to 1-NN classifiers on the training set, if $\epsilon$ is set small enough. Some crucial technical flaws are out standing, and the paper is generally hard to read.  The key concern about the paper is the lack of clear definitions on the important concepts, and therefore, on the proposed robust measure. The authors may have a clear measure in their mind, but based on the provided definitions, it does not seem clear. 1. The provided definition of Voronoi boundary $$VB(\mathcal{X})=\\{x'\in \mathbb{R}^d|\exists x_1,x_2\in \bar{\mathcal{X}}:x_1\neq x_2, \|x'-x_1\|=\|x'-x_2\|\\}$$ seems incorrect, or some examples on which their measure cannot be applied are not properly addressed. At least, I can find an example on which the proposed measure is not applied. For example, let's take a unit sphere as $\mathcal{X}.$ Consider $\ell_2$ norm. Then, for any combination of $x'\in \mathbb{R}^d$ and $x_1\in \bar{\mathcal{X}}$, we can find $x_2\in\bar{\mathcal{X}}$ s.t. $\|x'-x_1\|=\|x'-x_2\|$ and $x_2\neq x_1$ as long as $x'\neq x_1$. That means, for a unit sphere $\mathcal{X},$ the Voronoi boundary is the entire input space $\mathbb{R}^d$. Now, $VB(\mathcal{X})^c=\phi$, and therefore $S_{exact}(\epsilon)$ is not defined or empty in this case. Is this correct? Since having a sphere as a distribution support is not really pathological, e.g., C.I. for Gaussian distribution, I think this is a serious problem.   Because of this, \textbf{their key example in Figure 6, the illustration of genuine adversarial accuracy (GAA) seems to be incorrect. } I checked the proof for Figure 6 in Appendix A. In the provided calculation of the GAA in Figure 6, they say that the Voronoi boundary is $VB(\mathcal{X})=\{0\}.$ However, based on the their presented definition of Voronoi boundary, it seems to be incorrect. For example, let $x_1-=-1\in\bar{\mathcal{X}}.$ Then, for any point $x'\in(0,0.5),$ $\|x'-x_1\|=(x'+1)$. Define $x_2 =2x'+1$. Now we know that $x_2\in\bar{\mathcal{X}}$ and $\|x'-x_2\|=(x'+1)$. Therefore, $(0,0.5)$ must be a subset of $VB(\mathcal{X})$, and like wise $(-0.5,0)$ also must be. Therefore, at least $VB(\mathcal{X})\supset (-0.5,0.5).$ However, they caculate their results in Figure 6 by setting $VB(\mathcal{X})=\{0\}.$ Therefore, the presented entire results in Figure 6 seem to contain errors.        Likewise, for this reason, the theoretical results seem dubious. 2. Assuming somehow $VB(\mathcal{X})$ is well defined, e.g., by a re-definition, the set $$S_{exact}(\epsilon) = \\{x\in\bar{\mathcal{X}}|\exists x'\in \mathcal{X}_{\epsilon}^c\cap VB(\mathcal{X})^c:\|x'-x\|=\epsilon\\} $$  over which the expectation is taken seems not representing the data very well. The perturbations set is not defined or empty for so many data points. To see this, bet an $\epsilon$-ball neighbor $B(\mathcal{X},\epsilon)=\\{x'|\|x-x'\|<\epsilon, x\in\bar{\mathcal{X}}\\}$ Now, your definition of $\mathcal{X}_{\epsilon}$ in page 5 is identical to $\mathcal{X}_{\epsilon} = VB(\mathcal{X})^c\cap B(\mathcal{X},\epsilon)$, which means $\mathcal{X}_{\epsilon}^c = VB(\mathcal{X})\cup B(\mathcal{X},\epsilon)^c$.Therefore, $$\mathcal{X}_{\epsilon}^c\cap  VB(\mathcal{X})^c = (VB(\mathcal{X})\cap VB(\mathcal{X})^c)\cup (B(\mathcal{X},\epsilon)^c\cap VB(\mathcal{X})^c) = B(\mathcal{X},\epsilon)^c\cap VB(\mathcal{X})^c\subset B(\mathcal{X},\epsilon)^c$$ Now, in the above definition of $S_{exact}(\epsilon)$, $x'$  must be reached by $x\in\bar{\mathcal{X}}$ by distance $\epsilon$. In other words, any element in $S_{exact}(\epsilon)$ must be able to reach to $B(\mathcal{X},\epsilon)^c$ by $\epsilon$ distance. For example, if $\mathcal{X}$ is again a unit sphere, then $S_{exact(\epsilon)}$ is a subset of the surface of the sphere. Now, no matter how the classes are distributed \emph{inside} the unit sphere, the genuine robust measure only consider the \emph{surface} of the sphere to measure the robustness. The second major concern is that the experiment section (section 4) seems not ready yet. It contains unsupported/self-contradicting arguments. Also, some additional information about the experiment conducted seems necessary.1. \textit{This result gives us an insight that properly applied adversarial training(adversarial training (Goodfellow et al., 2014) with no conflicting regions originating from overlapping regions) will train models to mimic 1-NN classifiers as their training loss try to enforce them to make the same prediction as 1-NN classifiers}: The authors argue that this training's loss forces to predict on the training set as like 1-NN classifiers, which is optimal w.r.t. the genuine robust accuracy. The author may need to provide that when $\epsilon$ is small, minimizing the standard adversarial loss is identical to maximize genuine robust accuracy. Otherwise, this argument does not make sense.2.  \textit{If robustness to perturbations on the distance measure is not much related to generalization, it is possible that there exists a tradeoff between test accuracy and test adversarial robustness}: Why is that? This argument is confusing because so far the generalization is w.r.t. robustness, but all of sudden the authors are talking about tradeoff between natural accuracy and robustness. So far, you said that for a good distance metric, 1-NN classifiers will be robust also on the test dataset. Now, what are the missed sentences to get your conclusion on the trade-off?3. How do you define and measure proportions of agreements with 1-NN classifiers for Table 1 and 2? Do you test a point from the training set after adding some perturbations or not? If you add some perturbations, then how do you generate them?4. The result in Table 2 (among different predictions) is very different from Table 6 (on whole predictions). What is the definition of and difference between "among different predictions" and "on whole predictions"? Why does one show significantly different agreements, whereas the other show no almost no difference? 5. Along the same line, the numbers in Table 2 are way larger than the numbers in Table 5. Why is that?6. In Table 1 and 2, I see that the agreement proportion values for non-adversarially trained models are different. For example, in table 2, 0.1529 for PGD-AT $\epsilon=0.25$ and 0.1996 for PGD-AT $\epsilon=1.0$. I guess that for PGD-AT $\epsilon=0.25$ and $\epsilon=1.0$, the non-adversarially trained model would be $\epsilon=0$ for both models. Why do we see this difference? If they were not adversarially trained, and if they all have the same model architecture, these observed differences may challenge to the solidity of the experiment part.   Other concerns: 1. The main contribution of this paper is unclear. First, the usefulness of this measure is unclear. We can not use this measure to measure the robustness of models as the authors do not provide a way to measure this genuine adversarial accuracy. They only compare the prediction of classifiers to that of 1-NN classifiers, which is by Theorem 2 optimal on the training set. Then, if it is difficult to measure genuine robust accuracy, then do you think the major usage of your measure is to show the lack of model capacity used for robust learning? Or, what is a main benefit of this proposed measure even in the absence of the specific algorithm to actually measure it?2. I do not see the theory and experimental results are coherent to their final conclusion. If I understand correctly, originally, the first experiment was intended to show the high agreement between 1-NN classifiers and adversarially robust models. By the argument of the authors, since the $\epsilon$'s used for PGD-AT or TRADES are small enough based on Table 4, these trainings can be seen properly applied adversarial training. Then, the adversarially trained models on the training set should behave similarly to 1-NN classifiers. However, not as expected, in Table 1,2,5, and 6 the portion of agreement is at best about 0.6 for MNIST and 0.36 for CIFAR-10. Since those numbers are not close to 1, i.e., lower than as expected by Theorem 2, they are speculating these low numbers are due to the limited network capacities. However, in Conclusion, they say that \textit{" can mimic 1-NN classifiers. That was confirmed on analysis on CIFAR-10 data"}, which is perplexing.  Suggestions and Questions:1. The definition of $VB(\mathcal{X})$ could have been refined and some intuition about this definition could have been provided. Also, could you give a justification of your robust measure in my unit sphere example in 2?2. W.r.t. Table 4, I think it will be even more beneficial if the authors can provide the maximum of the minimum distance within the same class, across or within the training and test set. It will help to understand the robustness of 1-NN classifiers and the performance gab between 1-NN classifiers and other network based classifiers, either robust or not.3. The main body can be revised in a way that the major contribution is clear and extra information is located in a proper location. For example, too many new/necessary information is at the conclusion even in a parenthesis.4. In conclusion, for the sentence \textit{"That provides one possible factor ...why a tradeoff between test accuracy and test adversarial robustness might exist.", }can you explain more about it? In your paper, I personally did not get any possible factor why a tradeoff happens. The authors study the question of adversarially robust classification---classifying inputs under worst-case norm-bounded corruptions. The authors argue that the standard notion of robust accuracy is inadequate for properly evaluating model performance and propose an alternative. Then, they show that for any fixed set of data points, the 1-Nearest-Neighbor classifier is the optimally robust classifier according to their proposed measure. Finally they measure empirically how similar existing robust classifiers are to the 1NN classifier.While the paper studies an interesting problem, the impact of its contributions is unclear:- **Inadequacy of robust accuracy.** The authors provide a number of toy examples that are unfortunately not convincing. In these examples, the notion of ground-truth label is undefined on most of the input space and the authors resort to arguments about an "oracle classified" and "invariance-based adversarial examples" which are not rigorously defined here. For instance, in the one dimensional examples, why is f2(x) not a good solution? It is perfectly accurate and robust up to eps=1. Similarly, in the sunset example, why is the top point classified as blue? Based on my understanding, in this dataset, the maximally robust classifier is exactly the max margin classifier and there is no trade-off between robustness and accuracy.- **Alternative notion of robust accuracy.** I was unable to understand the proposed definition at an intuitive level. How does it defer from standard accuracy? Why is it better? What does it mean to "use a point more than once when calculating robustness"?- **The 1NN classifier is maximally robust.** Since this statement if proved for a fixed set of datapoints, I do not understand its significance. Clearly, if we are given a set of points with their labels and we are asked to predict these labels *on the exact same set of points*, then using a 1NN classifier makes sense. But clearly, for many realistic machine learning problems (e.g., images) NN classifiers will inevitably perform poorly due to their inability to account for natural data invariances (e.g., image crops).- **Similarity of adv. trained models with the 1NN classifier.** I do not see the significance of this experiment. For instance, on CIFAR10, the 1NN classifier performs quite poorly so I'm not sure what this comparison is meant to convey.Overall, the paper need to significantly improve in focus, stating concrete and relevant research questions that its contributions are meant to answer.Other comments (not affecting score):- [This paper by Suggala et al.](https://arxiv.org/abs/1806.02924) also explores alternative notions of robust accuracy and are quite relevant.- There are several grammar issues  throughout the manuscript. The paper introduces a novel metric for measuring adversarial accuracy of machine learning models, dubbed genuine adversarial accuracy. This is motivated by standard adversarial accuracy favouring in some instances classifiers with higher vulnerability to invariance-based adversarial examples.One of my main reservation about this paper is the lack of significance. The genuine adversarial accuracy metric seems to be motivated by corner cases; the paper does not demonstrate the importance of this metric neither for practical evaluations of adversarial robustness nor for significant theoretical purposes.Further observations:- In Definition 2: I dont think it is common to define adversarial accuracy w.r.t. a specific loss function.- References are repeated numerous times, e.g. any mentioning of adversarial training is followed by the (Goodfellow 2014) reference, which is redundant and affects readability. - In 1.2.1: I dont see why the oracle classifier will be f_3 or why it should even be uniquely determined.- In 1.3: The objective of adversarial training is not necessarily to improve accuracy on clean samples.- In 1.3.1: is consists of -> consists of- In Definition 3: What does it mean to use exact perturbation norm? The type setting could be improved by not putting equations under bullet points.- Avoiding conflicts of using points twice may not be required if those points have measure zero.- Theorem 1 immediately follows by definition - this is more of a proposition the proof of which is obvious.- Also Theorem 2 is obvious, and Section 3 consists of only one paragraph.- Section 4 is pretty vague and inconclusive. The argumentation relating adversarial training to generalisation of 1-KNN classifiers isnt convincing.- The conclusions make several references to different parts of the Appendix which havent been discussed in the main body of the paper.  This paper proposes to use a deterministic classifier to replace the samplingprocess in randomized smoothing based certifiably robust models.  The goal oftraining a deterministic robust classifier to avoid the high cost of randomizedsmoothing is a right direction to look at. The reason that we can get certified robustness with Gaussian smoothing is thatthe smoothed classifier becomes Lipschitz (see Salman et al.). Unfortunately,it is a stochastic classifier and typically it is impossible to access thesmoothed classifier directly, and that's why in randomized smoothing (e.g.,Cohen's PREDICT procedure), sampling is necessary.The approach in this work is to train a smoothed classifier that essentiallyreturns the same prediction as the mean of the originally stochasticclassifier. The authors pointed out the connection between Gaussian smoothedclassifier and gradient regularization, so they use gradient regularization toobtain the desired deterministic classifier.Unfortunately, it seems to me that the proposed approach is not sound. Withgradient regularization, we try to make the learned classifier to be smooth,however there is no guarantee that such a learned classifier will indeed besmooth and produce the same outcome as the original Gaussian smoothedclassifier, so we cannot use the outcome of this classifier to replace Cohen'sSampleUnderNoise procedure.  More precisely, optimizing the proposed lossfunction (3) does not guarantee the learned classifier f^smooth to be Lipschitz,where the original Gaussian smoothed classifier is guaranteed to be Lipschitz.Although the loss attempts to do so with gradient regularization, there is noguarantee here. So the entire procedure is not certified anymore.On the positive side, the proposed method may work as a good empirical defense,since the smoothed classifier can be learned quickly and can be more robustthan the original classifier. This may be advantageous for certain applicationswhere adversarial training is too slow or we don't want to retrain the originalclassifier.Because of the fundamental problem mentioned above, I cannot recommendacceptance of this paper. I am willing to discuss with the authors further incase I misunderstand some parts of the paper. The paper claims that a (computationally intractable) randomized smoothing of any classifier can be distilled into the (deterministic) classifier itself via fine-tuning it with gradient penalty. This is motivated by a theoretical result that Gaussian smoothing of a classifier is equivalent to solving a certain heat equation, which can be approximated by a regularized loss training. Experimental results use the resulting deterministic classifier to compute the certified radius compared to (stochastic) smoothed classifiers, arguing its efficiency and higher certified radius of the proposed method. The relation between randomized smoothing and PDE seems to be an interesting direction to explore. My biggest concern is that, however, whether the proposed deterministic smooth classifier is indeed *certifiably* robust in practice. Apart from its theoretical motivation, I could not find any statistical guarantee that the deterministic smooth classifier is provably close to its stochastic counterpart for every input x, which is essential to claim the certifiable robustness of the proposed model. Otherwise, how one could prove that the deterministic smooth classifier is indeed robust to adversarial examples? - although randomized smoothing may require much more computation time, it at least provides such a statistical yet practical guarantee, namely as the CERTIFY algorithm [1].Another possible way to go is to show that the *empirical* robustness of the deterministic smooth classifier is non-trivial. In this context, actually, the message of the paper can be "very" surprising (and very unlikely, at the same time): one can robustify any classifier by a single pass of fine-tuning with gradient penalty, even without adversarial training. The paper indeed provide a related result, i.e., empirical test accuracy on adversarial attacks, but the current evaluation is too weak to support the claim: considering a bunch of defense papers that are broken after published [2, 3], it is too hard for me to believe the results as is even it could bear from a specific configuration of PGD and DDN. I would recommend the paper to follow the standard guidelines suggested by [4]. The paper could explore more possible configurations of PGD attacks, or other types of attacks (e.g., gradient-free attacks [5], or black-box attacks), just to name a few.[1] Cohen et al., Certified Adversarial Robustness via Randomized Smoothing, ICML 2019.[2] Athalye et al., Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, ICML 2018.[3] Tramer et al., On Adaptive Attacks to Adversarial Example Defenses, 2020.[4] Carlini et al., On Evaluating Adversarial Robustness, 2019.[5] Uesato et al., Adversarial Risk and the Dangers of Evaluating Against Weak Attacks, ICML 2018. **Summary**This paper presents a new type of brain-inspired dual-pathway DNN model where the coarse (faster, less accurate) and fine (slower, more accurate) visual pathways augment each other during training and inference (via imitation and feedback) to boost the network's robustness to various noises.**Pros**(1) The topic is interesting and important as the P and M pathways are both crucial elements of the efficient and robust human visual system but their computational models are much less studied.(2) The proposed model is new and overall simple to implement.**Cons**(1) The model's design decisions are arbitrary and poorly justified.(Imitation learning) It's unclear why CoarseNet activations must mimic FineNet activations, since only FineNet activations are ultimately used for inference (not CoarseNet activations, which are further transformed and used by the FineNet). To justify the necessity of imitation learning, the authors should present full FineNet+CoarseNet results (not just Fig 3) using all 3 losses versus using only FineNet's classification loss.(Binary masks) The setting that the CoarseNet can use clean binary masks of objects as inputs doesn't make sense at all in my opinion. Segmentation itself is a complex task that often requires high-level vision so it's unclear to me why the authors assume that PRGCs can provide such information.(SMA) It's unclear why the SMA's u and v are implemented as memory buffers (that are updated per 2 epochs, which seem arbitrary and not biologically plausible either) following [Orhan, 2018], instead of end-to-end learned parameters (that are consistently updated every iteration with the rest of the network using backprop gradients). More generally, the authors should present solid reasons why association should also produce results that mimic FineNet activations.(DMA and feedback) It's unclear why an RBM is required as an additional component to introduce dynamics, since extending the feedback loop dynamically (i.e. more loops) should also be able to achieve similar results using existing components (which seems more biologically plausible in terms of resources).(2) The experimental results are weak and incomplete, and comparisons against related work are missing.(Noise robustness) As the key benefit claimed in this paper, the noise robustness of the proposed model is however weak (still roughly 20% accuracy drops for all types of noises) and only better than simplified or similar variants of the model. In addition, using only FGSM (targeting the FineNet) to generate adversarial noises doesn't fully test the robustness of the model, since more recent techniques [1, 2] can easily generate smooth adversarial examples that will likely severely affect the CoarseNet (unlike FGSM).(Related work) Although the authors argued that existing models are conceptually different, it doesn't mean that architecturally similar models [3, Hou et al.], SOTA in adversarial defense [4, 5], and most importantly other brain-inspired models, targeting robustness [6, 7] or not [8, Tang et al.], shouldn't be compared against to properly prove the value of this work. Also, as the field has started to more directly use neural data to guide better network design [8], it's unclear why the authors seem to have completely omitted this approach.(Rough-to-fine processing) The value of this approach is unclear since the accuracies of training both networks using the subclass labels (CIFAR-100) are missing.(Backward masking) Visual results alone (Fig 5 and 12) don't properly support the claim that this model "can explain visual cognitive behaviors that involve the interplay between two pathways". Please consider providing more detailed statistical analyses (e.g. R^2) if the authors want to make this claim.(3) The clarity needs improvement.The clarity of this paper is substandard as many key details are ambiguous or completely missing. For example, how does the SMA memory buffer store features? Random sampling over the training set? Is the model also trained on noisy data? How does a CN+SMA model (Fig 4) even work? What exactly are the backward masking stimuli used in the experiments (frame by frame)?**Recommendation**I recommend rejection of the paper given the following two major cons (see details above).(1) The model's design decisions are arbitrary and poorly justified.(2) The experimental results are weak and incomplete, and comparisons against related work are missing.**Questions**Please address the cons listed above.**Additional Feedback**(1) Although using an L2 loss for imitation learning is straightforward mathematically, the authors' arguments regarding how the brain may implement imitation learning aren't very convincing (Sec 3.2). For example, is there direct evidence of synchronized oscillation supporting the transfer of neural representations and thus imitation learning?(2) Speed seems to be a major potential benefit of the proposed model, which however was not clearly discussed or benchmarked. Please consider adding speed comparisons against SOTA networks in terms of inference speed.**References**[1] Low Frequency Adversarial Perturbation, UAI, 2019[2] SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations, WACV, 2020[3] U-Net: Convolutional Networks for Biomedical Image Segmentation, MICCAI, 2015[4] Feature Denoising for Improving Adversarial Robustness, CVPR, 2019[5] Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks, ICCV, 2019[6] Brain-inspired Robust Vision using Convolutional Neural Networks with Feedback, NuerIPS-W, 2019[7] Biologically Inspired Mechanisms for Adversarial Robustness, arXiv, 2020[8] Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs, NeurIPS, 2019 This paper proposed a two-pathway neural network to mimic the interplay between the parvocellular (slow and fine-grained) and magnocellular (fast and course) pathways in neural systems. The two pathways are named as FineNet and CourseNet. During inference, the FineNet received recurrent feedback signals from the CoarseNet via an attention layer and memory. During training, cross-entropy loss are used for both pathways, and an "imitation" loss is used to encourage the CoarseNet pathway to mimic the FineNet First, a coarse-fine two-pathway network is not novel, as coarse-fine or multi-scale pathways is a well-known design used in computer vision applications. Using an attention module for the coarse-to-fine interaction recurrently might be new, but the paper does not show if it could outperform simpler interactions.Second, the "imitation" loss is essentially model distillation. It is well-known that learn a weaker network by distilling a stronger network can result in stronger results for the weaker network. Third, the accuracy on Cifar-10/100 are low, far away from the well-established SOTA. Though the paper provides a few ablations, observations made on a too9 weak model are not conclusive. Finally, the paper might aim to be explanatory, but it falls short in clarity. The model description in Sec 2.1 is elusive and not sufficiently detailed. The choice of the number of feedback steps is ad hoc. The outreach to backward masking oversold a computational analogy as a neural computational model while providing a vague explanation of the background and results of the experiments.  The work proposes a new algorithm that can be used for the complete verification of neural networks (NNs). Unfortunately, the authors do not define the verification problem they study: Based on the second paragraph of the introduction, one is given a neural network (NN) on the input, and the task is to determine whether the NN has a specific formally defined property - but which kind of properties are verified is never explained. Intuitively, one would expect verification to focus on determining whether the NN gives a "correct" output for certain inputs, but that does not really match the general description given in the paper, and I did not find a place where the verification problem is formalized further. Without knowing what "verification" means in the context of this paper, it's difficult to follow the reasoning provided in the paper without having some rough idea of what kind of properties one wishes to verify (for instance, the discussion about using LP bounds assumes that the property that is being verified can be expressed using LP). I believe this issue could have been avoided by formalizing the precise task of verification (the verification problem).On that note, many parts of the paper seemed rather confusing and hard to follow, either due to inconsistencies or due to language issues. For instance, the sentence "Input domain split is shown effective in verifying the properties with low input dimensions while performs as poorly as incomplete verifiers on higher dimension properties" on page 2 seems to contradict the definitions given for complete and incomplete verification. How can a complete verifier perform "as poorly as incomplete verifiers" if, by the definition given in the paper, complete verifiers must always correctly determine whether the NN has the given "property" or not? (Section 2: "Complete verifiers guarantee to terminate either the property is proved or a violation is located.")In terms of presentation, the submission contains an incredibly large number of minor language issues (roughly 1 per 2-3 lines on average, ranging from minor article issues to malformed sentences; see also the quote in the previous paragraph), and I strongly encourage authors to fix these as they have a rather disruptive effect when trying to read and understand the paper. A very small number of examples is provided below:Page 1-"cause the changes of NN predictions" -> "cause changes of NN predictions"-"Recently, a framework of Branch and Bound (BaB) (Bunel et al., 2018) is widely used for efficiently verifying NNs" - cannot combine "recently" and "is".-"adopts Linear Program (LP) bounding procedure" -> "adopts a Linear Program (LP) bounding procedure"Page 2-"for construct LPs" -> "for constructing LPs"The main contribution of the paper is the use of incomplete verifiers for complete verification, and the authors propose an algorithm for doing that using LIRPA bounds. However, I found no proof (or anything resembling a proof) showing that the resulting algorithm is correct, i.e., that it performs complete verification for neural networks. In fact, the problem is not even properly and formally defined in the paper. Hence, regardless of the experimental results, I do not think that the submission is ready for publication at this stage. Unfortunately the authors link directly to the code, and the code is not anonymous. This might be a desk-reject as this is not a double blind review.This work is a description of a library for developing variational inference algorithms using the ELBO-within-Stein framework developed in Nalisnick et al. (2017). The library is evaluated on on Neal's funnel and two moons, and on a polyphonic music dataset.Comments- Nalisnick et al was published in 2017. I assume this was a typo on the authors' part.- Table A in the Appendix, describing different kernels, should include a column with computational and memory requirements for each kernel if they differ. This can affect the scalability.- The work describes LDA but does not evaluate it. It would be helpful to include held-out log likelihood numbers on a standard topic modeling dataset such as 20 newsgroups. This would help people compare to prior work. - Similarly, the library is evaluated by fitting to a standard polyphonic music dataset. Please report these numbers in a table, alongside a reasonable approach using standard variational inference and Stein VI (using the library) side-by-side. For example, the numbers here are much better, and use standard variational inference with the KL divergence: https://papers.nips.cc/paper/6039-sequential-neural-models-with-stochastic-layers.pdf (Stein Variational Inference can be difficult to understand, as can be Pyro, which is built on jax/pytorch, and the library developed here is built on top of all of these moving parts. Before embarking on using the library, a machine learning researcher should be very convinced that all this additional effort is worth it. Benchmarking this new library against existing work is important and will go a long way toward justifying its existence.)- The references are very poorly formatted. Please clean up. ##########################################################################Summary:The paper studies policy optimization in multidimensional action spaces. They consider atomic factorization of the action space (i.e. action space is factored into sub-action spaces, one per action dimension). In this setting, the authors consider two well-known policy representation techniques: 1) independent sub-policies (diagonal-covariance policies over the sub-action spaces) and 2) sequential/autoregressive policies (an ordering of the sub-action spaces is assumed a priori and the sub-policies receive as input the state and the selected sub-actions for the preceding sub-action spaces). The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks.  ##########################################################################Reasons for score:I believe this paper fails to position itself appropriately with respect to the literature. This makes it difficult to assess what the main contributions are and whether sufficient new methodology is proposed. Additionally, the paper misses on several fundamental experiments for the results to be convincing. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? Is it the improvements of the discrete versions over the continuous versions of the methods that are interesting (in which case, the results for the continuous versions on the MuJoCo tasks should be reported)?  ##########################################################################Pros:- Factorizing action spaces is an interesting approach for scaling to high-dimensional action spaces. Also, they have been shown in several recent studies to enable discrete-action methods to outperform numerous continuous-control methods. So studying them more extensively is useful. - The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. ##########################################################################Cons:1) The paper generally does not position itself appropriately with respect to the literature. Below is my overview of the paper with additional related works (some are suggestions that would enhance the paper):  - Branching Q-learning (BQL) in Ref. [I] is similar (if not the same) to the independent critic of FSAC. Using this approach they scale to domains with 33^17 discrete actions.  - Autoregressive critic of FSAC is the same as that in Metz et al. (2017) (also stated in the paper I believe).- Independent versions of numerous PG methods have been previously developed and studied extensively by Tang and Agrawal (2020). Also, the original TRPO paper was used with independent discrete sub-action policies in Atari tasks (if I remember correctly). Therefore, there is nothing new about FPPO-independent (also stated in the paper I believe).  - Other methods for learning the independent critic in FSAC are also possible but are not discussed in the paper (see e.g. Ref. [II]). Moreover, Ref. [II] uses independent policies with A2C.- When independent policies are used in PG, a better baseline can be used which provably reduces variance [III]. Discussion of such a baseline would be very useful in this paper which somewhat serves to summarize deep RL in factored action spaces.- Ref. [IV] combines autoregressive and independent proposal policies for approximate Q-maximization in Q-learning. In this way, they scale to very high-dimensional action spaces. This work is able to handle hybrid action spaces without discretization. A discussion and positioning with respect to this paper could be valuable.  2) Experiments fall very short in my opinion. Below are some of the experiments that I think are necessary to include: - Figure 1a,b: Why not run FSAC?- Figure 1c: I believe the autoregressive FPPO and FSAC are reported in Platform. But why not independent? Why not report the standard (non-factored) baselines of PPO and SAC?- Figure 2: Compare against the continuous versions of SAC and PPO on MuJoCo.- Figure 2: Why not evaluate autoregressive policies in MuJoCo? Metz et al. did that with DQN.  3) No new factorization schemes are explored in this paper. For instance, Ref. [V] explores a mixture of independent and sequential action-value function representations. A discussion of the spectrum of other possible factorizations would be interesting. [I] Action Branching Architectures for Deep Reinforcement Learning, AAAI 2018.[II] Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement Learning, arXiv 2017. [III] Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines, ICLR 2018.[IV] Q-Learning in enormous action spaces via amortized approximate maximization, arXiv 2020. [V] Inferring DQN structure for high-dimensional continuous control, ICML 2020. ##########################################################################Questions during the rebuttal period:1) In light of new related works together with those included in the paper, I believe the novelty of the paper currently is in developing the policy optimization updates for autoregressive policies. The rest (independent policy optimization updates, independent critic, and autoregressive critic) are not novel as far as my assessment goes at this point. Is this correct?   2) Does "without autoregressive" (in Figure 1) imply "independent"? Or does it refer to the standard (non-factored) versions of PPO and SAC? If not, I need clarification on what it exactly means.3) This paper considered stochastic policies. I'm curious about any potential use for extending it to deterministic policies; e.g. could it be useful to use an autoregressive policy network with DDPG?4) I think that the manual ordering of the sub-action spaces in an auto-regressive policy could introduce bias (i.e. I think there could be a setting that some orderings would make it impossible to learn the optimal policy). Can you comment on this?##########################################################################Minor comments:- The title is too broad in my opinion. Unless the paper also incorporates analysis and results for purely action-value methods (e.g. DQN-based agents such as Sequential and Branching Q-learning), the paper should specify that policy optimization methods are of focus. Also, stating atomic factorization in multidimensional action spaces instead of only "factored" could clarify the kind of factorization that is the subject of this paper.   - "FPPO was trained only for 2 days on 4 CPU cores while IMPALA was trained with 150 CPU cores.":This is not very useful without providing training-time of IMPALA.- The word action is frequently used instead of action dimension and sub-action.- "With this transformation, both factored agents reach completion in a few time steps, see Figure 1c": Saying "a few time steps" for a plot that goes to 90k time steps is somewhat strange.  - This work claims to develop techniques for dealing with hybrid action spaces. But in the experiments, this is not demonstrated. In fact, the action space in Gym Platform features discrete and continuous sub-action spaces, but they are discretized. As such, I think this paper should state dealing with discrete action spaces or continuous ones via discretization.  SummaryThis paper proposed an attention module in which each element of features is scaled based on the sign of its value.  The authors argue that the combination of this attention module and ReLU can be considered as an activation function.This novel activation function amplifies its gradient comparing with ReLU.The experimental results show the proposed method outperforms conventional activation functions in few-shot settings of transfer/meta learning.Strong points* The proposed method consistently improves the accuracy of transfer/meta learning tasks.Weak points* The technical novelty is limited since the derived activation function is an expansion of PReLU:  * PReLU: min(0, a * x_i) + max(0, x_i)  * Proposed: min(0, alpha * x_i) +  max(0, (1+sigmoid(beta))*x_i) where 0.01 < alpha < 0.99.* Although the very specific form of ELSA is used in Section 3.2, there is no explanation of why C and sigma are introduced for ELSA.The learning rate is not tuned for each activation function in Section 4.4 and 4.5.Decision reasonSince the learning rates of the baseline methods seem to be not optimized, the experimental results are not convincing.I recommend resubmitting the paper after the tuning of baseline methods.Questions* Why did you use momentum SGD for alpha and beta instead of using Adam or SGD as other parameters?  Their learning rates are also different from these of other parameters?* How is the learning rate determined for the experiments in Section 4.4 and 4.5?   Even if the learning rate is tuned for baseline methods, the proposed method outperforms them?Additional Feedback* Although it is interesting to formulate activation functions with an element-wise attention mechanism, the merit of this new perspective is not clear for me.* It might be easier to understand the proposed method as an expansion of PReLU since the proposed attention mechanism is much different from the attention mechanism in the community. ## SummaryThis work claims to address the problem of learning the structure of interactions between multiple point processes. This can, e.g., be used to predict co-occurrences between different types of events. The authors develop a method that (a) is theoretically principled and (b) scales to large datasets.## Reasons for scoreThe overall problem is clearly relevant, and some aspects of the contribution appear to be promising (connection to GAMs, approach to inference). However, in its current form, the paper is excessively unclear, to the point where I am not sure that I understand the precise problem the authors are trying to address (despite my familiarity with the area).## Pros- if the problem is that of understanding correlations between multiple point processes, then that is an important problem that is clearly relevant to the ML community.- The approach builds on generalized additive models, which gives their approach a solid theoretical foundation- The inference algorithm based on minimizing KL-divergence appears to be interesting.## Cons- the paper lacks clarity throughout, making it difficult to understand the problem, let alone the contribution itself (examples below).- the experiments are not explained in sufficient details (NYC taxi dataset for example), and as such they are not supporting the contribution.## CommentsReferring to "stochastic processes" is too generalmaybe talking about "point processes" would be better. For example, a Gaussian process is a stochastic process, but doesn't have anything to do with the processes considered in this work.Section 2, "Formulation" is confusing. Usually, a multi-dimensional / spatial poisson process is a *single* point process defined on $\mathbf{R}^D$, and a realization of that process is a collection of points in D-dimensional space. This is what, e.g., Flaxman et al. (2017) consider.In this paper, the authors seem to assume that every dimension is a different process, and somehow events in each dimension are grouped together based on their arrival order ("Each $t_i$ is the time of occurrence for the $i$-th event across $D$ processes and $T$ is the observation duration"). I am not even sure this is always possible, since it seems to assume that every process / dimension has exactly $N$ events during the interval $[0, T]$. Thus, I fail to understand even the basic foundation on which their work builds.It seems to me that the authors are really considering a *multivariate* Poisson process, where the intensity function of each process are dependent. In that case, the likelihood would be different from (1). Can the authors comment on this?Experiments:- "the sample size is randomly chosen by the mixture of Gaussians" - what does this mean? a GMM does not define a distribution over number of samples.- where is the notion of timestamps in the synthetic GMM experiments?- Figure 3 does not seem to be referenced in the text. How can the intensity function be 1D for if the experiments are with 2D Gaussians?- NYC taxi dataset: how is space taken into account? Defining a "common pick-up" as one that happened within at 1 min interval but at opposite ends of the city seems pointless.Other comments / questions:- How does your work compare / contrast to multivariate Hawkes processes?- In what way is Flaxman et al. 2017 non-convex?- " ...where the different events are marked so that a particular subset of events can be used to generate the intensity function" what does this mean? This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. The main contribution is to consider the partial order of high-order interactions when parameterizing the model.The proposed method is a reasonable solution, but there are several points in the paper that are particularly weak:First, the paper does not find a proper scenario to apply the additive Poisson process model. The experimental part lacks real data analysis. In Section 3.3, they only show a quantitative result regarding two Poisson processes [T, W]. It is hard to summarize that the performance gain comes from high-order modeling without explaining it in detail. For example:i) Why are KDE and RKHS predictions inconsistent across January through March?ii) How do you define M?iii) How sparse is the data?Second, jumping from section 2.1 to 2.2 is confusing, since section 2.2 seems to have nothing to do with the Poisson process. So my first question is:i) Can you define the likelihood in a single Poisson process model in order to consider all the interactions you mentioned in section 2.2?After carefully reading the setup in section 2.2, I feel that the entire method is based on a crude approximation of Poisson processes. For example, Im curious about:ii) Does your method really depend on discretizing the time window? What if you let M goes to infinity? Im particularly interested in this question because the original Poisson process likelihood model does not require discretization.To conclude, this paper proposes a reasonable method to model high-order correlations. However, its experimental result is weak and the model construction is of a question to me. This work aims to study so called comprehensive robustness by considering natural accuracy, sensitivity-based robustness and spatial robustness simultaneously. The authors claim that they propose a Pareto Adversarial Training strategy to balance the mutual impact within different robustness and the resulting solutions provide the set of optimal balance among accuracy and different adversarial robustness.Pros:- The paper investigates comprehensive robustness. For me, the problem itself is interesting.- This paper provides empirical evidence to show that there exists representation discrepancy for different notions of adversarial robustness. This shape-bias representation viewpoint is appealing.- The motivation for their training strategy incorporating Pareto criteria is good, despite that their method has a few questionable inaccuracies summarized below. Cons:- The proposed Pareto Adversarial Training strategy is not well justified. First, the authors misuse Pareto. Pareto optimality (or front) is introduced to describe the trade-off between conflicting objectives from the multi-objective optimization. However, the proposed method is actually solving a series of bi-level optimization problems by varying the hyperparameter $r$. Since each problem is solved independently, it is not a multi-objective optimization, and the solutions obtained are also not Pareto optimal (or front). Second, in the lower-level formulation (12), the authors propose to use weighted quadratic difference to measure the trade-off. However, it is not clear why this makes sense. Is it because the two-moment objective function results in a quadratic programming problem which is easy to solve. In Section 3.2, the authors claim that there exists representation discrepancy for different adversarial robustness. Isnt it more reasonable to consider the representation discrepancy as a measure? - To integrate flow-based and RT-based attacks, the authors propose to use equation (2) instead of the original Flow-based attack equation (1). To justify this, the authors then provide Proposition 1 which proves that the smooth approximation of max operation in equation (1) has a parallel updating direction with cross entropy loss in equation (2). However, equation (1) is to minimize the max operation over $\omega_F$, whereas equation (2) maximizes the cross entropy loss over $\omega_F$. Therefore, equation (2) is different from equation (1). Moreover, the integrated spatial attack proposed in Section 2 seems not to appear anymore in their next analysis and training strategy. The readers might think that Section 2 is a little unnecessary as it is unrelated to other parts of the paper.- In section 4.1, the authors claim that Max AT is closely linked with specific weights of Ave AT by providing Proposition 2. However, I have some concerns about the proof of Proposition 2. First, it assumes that KKT conditions hold. This assumption is strong in the sense that KKT conditions imply zero duality gap which usually does not hold for nonconvex problems. Second, even if KKT conditions hold, we can not derive that $R^*_{max}$ is a first-order stationary point of $\sum \lambda_iR^{S_i}$ because the Lagrangian function might be non-convex with resepct to $f$.Minor comments:- It would be better if the authors can give the definition of $x_{\omega_F}$ in (1).- In equation (9), $clip_\epsilon$ should be used after the update of $w_F^t$.- In equation (10), what does $n$ represent? The number of training data or the dimension of input? SummaryThis paper proposed to explore a given disentangled latent code and explore it through random interpolations. The idea is to ensure these interpolated codes rely inside data convex hull by enforcing latent code reconstruction by first decoding and then reencoding this explored codes. The authors proposed a way to manipulate the final latent space by using Unit Direction Vector (UDV) to generate unseen attribute values. The results are illustrated on 2 datasets (Fonts and RaFD) both qualitatively and quantitatively.Reason for scoreOverall I vote for reject, this paper has limited contribution and lack of consistency (disconnected ideas: DEAE and UDV), clarity (respective contributions of GSL-AE and DEAE) and justification (spurious and not convincing experimentations).Pros1. The general topic of having more controlable synthesis by having more disentangled representation if of clear interest for the field. 2. The idea of reconstructing random interpolations of valid latent codes by decoding/reencoding is interesting. I don't think this is novel, but not widspread enough in the field so far, so the interest remains. Cons1. While the topic is of interest for the field, the interest of the proposed approach is clearly limited since it relies on a preprocessing step (GSL-AE) of which it is already the role. A clearer distinction of the contributions of each step would help to illustrate the additional interest of DEAE. Unfortunatly the paper relies too much on GSL-AE which limits the potential scope of their main contribution which could be wider.2. The title is misleading since it presents the approach to proposed a "disentangled exploration" while it is essentially provided by GSL-AE used as a preprocessing step. Thus, it would also be good to mention that the approach is supervised (like GSL-AE).3. The paper lacks of clarity: UDV or "dataset bias elimination" seems to be  spurious addition and are not really well connected in the paper. It's hard to understand how it contributes to illustrate the interest of DEAE.4. Results are not convincing.- MSE loss between latent spaces from different methods cannot be compared at all. We can imagine spurious differences only due to scale differences for instance.- Fig 2. is mentioned p.5 to illustrate the qualitative superiority of DEAE over GSL-AE: not really obvious of this state.- Section 3.3 + fig 7.: interpolation is not smooth which contradicts the quality of the representation and the interest of UDV.- Section 3.4: the idea of evaluating the performance of generative methods through their capacity of improving a classification task by augmenting the initial dataset, is very indirect and not really convincing.- Gradcam results (Fig. 8) does not illustrate any improvement in favor of background information for the "unbiased model"5. "Dataset bias elimination"- This section seems spurious. Nothing in the proposed approach is really specific to such an application appart the global context to which it is attached. I don't see what this section brings to the paper.- The way the unbias model is obtained is not realistic since you need to know what information has to be dropped, whereas in real applications you generally don't know the source of bias.6. While having thinner way of exploring the latent space is interesting, the proposed UDV approach sounds spurious and ad hoc and does not seem to be connected to DEAE.Questions1. What is the context of application of your approaches since it relies on a supervised approach to create the initial disentangled embedding ? Coutrolable synthesis is generally understood in an unsupervised setting.2. Is there a link between UDV exploration and the way DEAE is learnt ?3. Why using "random interpolation" for inference and illustrating performance (in Fig. 5) ? It would be clearer with standard interpolation between left and right bounds. I also would expect that interpolations would reach (or at least get closer to) the targeted right bound.4. How do you ensure that MSE (from section 3.2) can be compared between totally different latent spaces ?5. Section 3.4: Why D_S is used rather than D_L for further analyses ? (to get augmented datasets)Minor comments1. Intro: Flows could be mentionned on top of VAE and GANs2. Intro on GAN & VAE limits are too much like a caricature regarding recent results. VAE can be used to generate HD images now, and GAN are much easier to train these days with GP strategy or different learning rates between generator and discriminator for instance. Approaches like InfoGAN could be mentioned to illustrate standard strategies to control GANs.3. Fig 2. Not clear which kind of interpolation has been used for GSL-AE and DEAE since neither font, font size, font color, letter nor background color are interpolated in the figures. This figure would be more powerful to stick to one (or a few) letter(s) and change only the considered attribute (background or font color). Here the comparison and the illustration of the effect is not easy.4. Section 2. "whic h maps" à "which maps"5. notation "*" has not been introduced (like in f_theta^* and g_phi^*)6. L_reg has not been introduced or referred in the paper.7. Fig 7. Choosing a fg color different from the targeted bg color would be better This paper proposes to use uncertainty estimates from an ensemble of action-values, to provide a weighting on the updates in Q-learning. The main idea is to use the sigmoid of the negative of this uncertainty in the next state, to produce a weighting between 0.5 and 1 to downweight updates with high uncertainty targets. This uncertainty estimate from the ensemble is also used to improve exploration, in a combined algorithm called Sunrise that leverages learning an ensemble in these two ways. The idea of using weighted Bellman updates is, as far as I know, novel. The evidence for the idea, however, needs more work. First, the weighted update in Eq (4) is not motivated from first principles. Second, the empirical evidence is weak because the experiments highlighting the role of the weighting do not demonstrate significant differences.  The first issue is the justification for the approach. The ensemble of Q-learning agents is trained using the weighting, derived from that ensemble. There are natural questions as to the interaction between the ensemble uncertainty estimates and the ensemble estimates. Does it result in any instability? What is the final point of convergence? Does it change the solution?But, one could argue that that is not much of a problem, since the weighting w(s,a) is always between 0.5 and 1, so it is not that skewed. Then the question arises how much it is helping, and why this small reduction in weight helps. This is particularly important to ask, considering the algorithm requires an ensemble to be learned, with subsets of data used for each action-value. There is a lot of effort expended for that weighting.   The experiments then do include ablations, to examine the effect of these weightings. Unfortunately, the results are inconclusive. The experimental time spent must have been large to get all the results in this paper, across so many environments and algorithms. But, the ablations themselves are not sufficiently in-depth to provide insight into the idea and algorithm. The results in Figure 2 are key, since that figure examines Sunrise with and without the weighting. Due to the variance across runs, with only 4 runs, there are large standard errors (and so even larger 95% confidence intervals); it is hard to conclude that weighting is helping. The additional results in Figure 5 in the appendix have a similar issue.The results in Figure 3, which motivate the exploration utility, are more clear in Cartpole. This provides some motivation for learning ensembles, so they can be used for exploration. But, this exploration approach with ensembles is an existing method. The main novelty in this work is the weighting. I highly recommend taking a few domains and carefully studying the impact of the weight. More runs would help for significance, as well as parameter sensitivity analysis to gain insight into the generality of the improvement. Sometimes performance gains are from hyperparameter tuning, rather than from the utility of an idea; here, you really want to know if and why this weighting improves performance. As a more minor comment, Sunrise is pitched as combining three ideas for using ensembles: your weighting, bootstrapping and UCB exploration. However, I see Sunrise as combining two ideas: weighting and UCB exploration. The Bootstrap DQN approach gives you a way to learn your ensemble of bootstrap models, so that it provides a useful uncertainty estimate. Given that ensemble, you can then use it to compute a weighting and optimistic action. It would be more clear to separate it out that way, rather than saying "Furthermore, since our weighted Bellman backups rely on maintaining an ensemble, we investigate how weighted Bellman backups interact with other benefits previously derived from ensembles: (a) Bootstrap; (b) UCB Exploration." The bootstrap is arguably not a benefit, but an approach to obtain confidence (uncertainty estimates).  Minor comments:1. Bootstrap DQN is listed under "Ensemble Methods in RL", rather than under "Exploration in RL", but is it an exploration approach.2. "Recently, Kumar et al. (2020) showed that this error propagation can cause inconsistency and unstable convergence." The terms inconsistency and unstable convergence should be explained, since they seem like technical terms. 3. Bellman backup seems to be used to describe the squared error to the expectation over next action, in Equation (2), and then to a stochastic sample of the action in (4). Which is it?4. What is meant by the signal-to-noise in Q-updates? 5. A natural baseline to include is to tune an agent that uses random weights between 0.5 and 1 in the update, but keeping other parts of Sunrise the same. The ablation removes the weighting all together, which is also important to include. But, it's worthwhile observing if random weights performs similarly, especially if that agent is tuned.  This paper studies visual question-answering (VQA). The authors proposed an end-to-end differentiable framework that performs "soft" logical reasoning based on object-centric scene representations and attention-based language parsing. The authors claimed that the new model is more data-efficient.While I think this direction is interesting and useful, I think the submission really needs more work before it may be published at a top-tier conference. The current manuscript is hard to understand and the results are not convincing.First, the method is not well-motivated. The intro mostly discussed related papers and their differences, so I was expecting an analysis paper, but it turns out the rest of the paper is about a new model. There is no justification for why we need a new model, what is new about it, and why we expect it to do better. Apart from this, the writing is in general unclear. For example, the model has an acronym DePe, but the full name was never fully told. The experiment section is also poorly written. In 4.2, it's unclear what "Train Ratio" means in Table 4 or 5, and these tables were never referred to in the main text.  I also don't understand the setup in Section 4.3, either, as essential descriptions are completely missing.This makes it hard to assess the results.  In addition, all results are on the CLEVR dataset. One selling point of methods such as MAC, NS-CL, and Stack-NMN is that they can naturally be applied to real images such as the GQA dataset. Without experiments on real images, the analyses presented in this paper are unlikely to convince researchers in this area. Quality: Pros:- The role of spuriously correlated features in causing poor generalization in the target domain is well described.Cons:- The derived bound consider unrealistically simple setup and makes strong assumptions that makes the derived bound practically useless.- Even if all the assumptions are right the suggested solutions for training a "robust" model is very ad-hoc and unlikely to be robust.Clarity:The paper is poorly written and most parts of the paper are difficult to follow.Originality/Significance:Although deriving a generalization bound  as a function of spurious features is original, due to strong assumptions, the derived bound is unlikely to have much practical value and  make a significant impact in the field.Detailed Comments:This work approaches cross-domain generalization from a relatively easier and somewhat unrealistic direction and does not consider the more challenging setup of emerging classes/subclasses in the target domain nor it discusses the effect of class imbalance on generalization. It was not clear how invariance to spurious function can be imposed in ERM when the spurious function is not known during training.A3 is a very strong assumption. Two labeling functions both predicting same labels on the training set yet not sharing even a single feature is not a very practical assumption.Section 3.2 is hastily written and does not read well.It is not clear what was so interesting about this finding "This argument may also align with recent discussions suggesting that reducing the model complexity can improve cross-domain generalization (Chuang et al., 2020)." In the context of structural/empirical risk minimization Vapnik has demonstrated (more than 40 years ago) that model complexity can be reduced by reducing VC dimension, which in turn will tighten the VC bound and improve generalizability on test data. This  result is distribution independent and thus would hold nicely for cross-domain generalization setting  considered in this paper as well (no emerging classes in the test data). It is not immediately clear what the benefit of the added c(\theta) term would be. Comparing equation (6) with (2) we now have the test error upper bounded by a term that is larger by c(\theta), which makes the bound loose compared to equation (2). This is a standard problem for domain-adaptation. So, I don't consider this as a flaw but some insight would have been useful. A5 (Sufficient of training samples) is also a very strong assumption that can not be easily justified in real-world domain adaptation problems.  Although theorem 3.2 shows that the extra term c(\theta) is less than or equal to the empirical term in the standard domain adaption error bound, the difference may not be practically significant.It is not clear under what conditions r(\theta;A(fp; x))=1 suggest an adversarial sample. Possibly only when an adversarial feature is generated by perturbing spurious features that don't overlap with nonspurious features? However, an adversarial sample can also be generated by perturbing only a small fraction of non-spurious features. "In short, in practice, while we do not know fp, we usually know A(fp; x) through intuition or common sense, such as texture or background of images. Thus, the estimation is to test the whether the model switches its correct prediction when these features are perturbed over the possible space."For modern machine learning problems that use semantically rich and high-dimensional features (i.e., ResNet) such a task can quickly become impractical. One cannot easily and uniquely identify the subset of features to perturb as there will be an impractically large number of combinations to try.In section 4.2 the solution that was suggested involves learning only with the top \rho fraction of features ranked according to the gradient. This is somewhat counter-intuitive as this approach will inevitably create sudden spikes in the gradient between different epochs which is expected to make the gradient less useful for feature selection. The loss function being minimized is not the same between different epochs. This paper proposes an adaptive neighbor clustering method by estimating normal distribution on the representation space. The proposed neighbor clustering can utilize the acceptable range for each dimension of each instance from the estimated variance, which leads to different size of neighbors for each instance, and improved the neighbor clustering performances. In addition, the proposed neighbor clustering method can replace the KNN-based neighbor clustering in the previous SCAN (Semantic Clustering by Adopting Nearest neighbors) framework for image semantic clustering.Overall, the paper is hard to follow and understand with many typos, incorrect notations, and especially the main term, VAE. Why can it be called VAE without a decoder which decodes some original input from a compact representation? I think the proposed network contains only the encoder (transforming z to r is also encoding), and it applies the stochastic process in the middle of encoder (512-d z instead of 64-d r). Therefore, it seems that the proposed training can be considered as the previous contrastive learning with stochastic regularization. And, it naturally raises the question that why not imposing the normal distribution on the final representation r? In addition, I do not understand why the sum of the sampled z and \mu is fed into for obtaining r. What if z is solely fed into? What is v in Eq. (1)? Is it d \circ e? How to explicitly connect Eq. (1) and (2)?In Table 3, The clustering performance drop of SCAN with AC-VAE is not marginal compared to the previous SCAN with KNN, even though the proposed AC-VAE seems to produce significantly improved neighbor clustering results in Table 1. However, there is no analysis on this.The amortized inference of normal distribution on the (middle of) representation space, and the use of its estimated variance in obtaining reliable neighbors for each instance seems to be make sense, however the motivation, insight, and contribution of the derived framework seem to be very limited.Typos: SLT -> STL, VEA -> VAE, argumentations -> augmentations, d(x_i, x_i) -> d(x_j, x_i) Summary =====This paper proposes and evaluates different normalization techniques for graph neural networks. Also, the authors argue that the best normalization technique is task dependent, so they propose to use a weighted average of different normalizations that is learned during training, called AGN. In the paper they propose 4 different normalizations some of which are structure-dependent, and compare the performance of GCN, GAT and GatedGCN with and without these normalizations, and the learned combination of all of them.===== Pros =====The paper explores normalization for Graph Neural Networks, which is a relevant topic.In the experiments section the authors follow the benchmark proposed by Dwivedi et al. 2020, which uses standard tasks so that comparison with other methods is easier.===== Cons =====The main concern about this paper is that the authors provide no justification about why the proposed normalizations are needed, or an explanation about why any of the proposed normalization techniques should perform better than the other techniques presented in this or other papers. Moreover, the authors only evaluate their proposed normalization methods without experimenting with other methods such as PairNorm or MsgNorm [2] which is also not cited.Node-wise normalization is normalizing the feature vector of each node in each graph independently of all the other nodes in the same graph, therefore it is not equivalent to Layer Normalization as the authors say, because Layer Normalization would normalize all nodes in the same graph together.Additionally, the Node-wise normalization has already been proposed as NodeNorm in [1]The authors claim that adjacency-wise normalization and graph-wise normalization are both structure aware. While that is true for the former, since it normalizes nodes according to its neighbours, it isnt true for the graph-wise normalization. The reason is that as explained by the authors in the beginning of section 2 each sample in the batch is a single graph, so normalizing across the graph is the same as normalizing across the whole sample, which doesnt use the graph structure. It looks like this case is equivalent to Layer Normalization.==== Questions =====I would like the authors to explain the motivation for their proposed normalization techniques beyond the empirical results. Why should they improve the vanilla GCN, GAT and GatedGCN models?Specifically since it is a novel normalization technique presented in this paper, the authors should explain what is the motivation for the adjacency-wise normalization. Making an analogy to images, the proposed normalization would be the same as normalizing one unit in a convolutional layer by the value of its direct neighbours. Why is that useful for graphs?===== Minor comments =====Some sections of the paper are not very well written. For example the following sentence On other hand, while multiple normalization methods are available for training GNNs and it is still hard to know in advance which normalization method is the most suitable to a specific task. is not very clear.It isnt clear if AGN stands for Automatic Graph Normalization or Attentive Graph Normalization as both are used across the paper.===== Reasons for score =====Because of the lack of motivation and explanation of the proposed normalization methods I vote for rejection. In my opinion the paper lacks justification and explanations of the proposed methods, and it isnt clear why they are better than other normalization approaches.===== References =====[1] Zhou, K., Dong, Y., Lee, W. S., Hooi, B., Xu, H., & Feng, J. (2020). Effective training strategies for deep graph neural networks. arXiv preprint arXiv:2006.07107. [2] Li, G., Xiong, C., Thabet, A., & Ghanem, B. (2020). Deepergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739. #### Summary- In the context of gradient-based meta-learning for few-shot learning, the authors propose TreeMAML, an algorithm that leverages the existence of a tree structure in a task distribution in order to pool inner-loop gradients between tasks. More specifically, the inner-loop corresponds to a level-by-level traversal from root to leaves, where at each step, task-leaves that are children to a common node in the current level average their gradients for this step's update. - The authors consider two cases: one in which the ground-truth tree structure is known (Fixed TreeMAML) and one in which it is unknown and must be discovered online (Learned TreeMAML). The authors extend a hierarchical clustering algorithm from prior work for this purpose. - The authors evaluate TreeMAML in three scenarios: sinusoid regression (Finn et al. 2017), linear regression, and mixed regression (Yao et al. 2019). TreeMAML compares favorably against MAML and a naive multi-task learning baseline. - Interestingly, Learned TreeMAML also consistently outperforms Fixed TreeMAML.#### Strengths- The authors investigate an important and under-considered problem in meta-learning: how to leverage structure within a task distribution. - The proposed TreeMAML algorithm is conceptually simple.#### Weaknesses- The motivation for the TreeMAML algorithm is very weak. Yes, averaging gradients across tasks might decrease variance, but presumably always increases bias. This crucial trade-off is not even mentioned.- The fact that Learned outperforms Fixed is troubling. It suggests that TreeMAML is not properly leveraging the ground-truth task hierarchy. Dissecting Learned to look at the tree structure it proposes would help diagnose this issue.- This is a purely empirical paper which only presents results in toy regression settings. More comprehensive empirical evaluation is needed, e.g. the image classification benchmark proposed in Yao et al. (2019), which is perfectly suitable for this paper (and indeed the authors took Experiment 3 from this work). - The comparison between TreeMAML and MAML might not be very fair. MAML is artificially constrained to use the same number of inner-loop steps as the depth of the task tree. Since MAML makes no assumptions about the task tree, this should be a tunable hyperparameter.#### Recommendation- I currently recommend a clear reject (3). Given the weaknesses outlined above, this submission does not meet the bar for acceptance.#### Questions- How is the Baseline model trained? Its MSE in Table 2 is uncommonly high.#### Minor suggestions- Please fix the numerous typos throughout the submission. Just in the first paragraph: inappropriate capitalization of multi-task and meta-learning; "The field of of".- K is used to denote the number of inner gradient steps in Alg. 1, but the number of datapoints per task in Sec. 4.1.- The num_shots=3 case in Fig. 3b directly contradicts the caption. The submission proposes a meta-learning algorithm attuned to the hierarchical structure of a dataset of tasks. Hierarchy is enforced in a set of synthetically-generated regression tasks via the data-sampling procedure, which is modified from the task-sampling procedure of [1] to include an additional source of randomness corresponding to which of a set of cluster components task parameters are generated from. The authors propose to adapt the model-agnostic meta-learning algorithm (MAML) of [1] to reflect this hierarchical structure by either observing (Section 4.1, FixedTree MAML) or inferring (Section 4.2, LearnedTree MAML) an assignment of tasks to clusters at each step of the inner loop (task-specific adaptation phase) of MAML; if tasks belong to the same cluster, the correspond task-parameters receive the same update at that step (in particular, the update direction is averaged). It is assumed that there are increasingly many clusters at each step, so that task-specific parameter updates are increasingly granular.##### Strengths:1) **Clarity**: The experimental setting and exactly how the data-generating process relates to the proposed algorithms are clearly described.2) **Significance**: Results on the hierarchically structured synthetic regression task datasets demonstrate that {Fixed|Learned}Tree MAML: is at least as good as MAML, and often outperforms MAML; that it learns more efficiently than a MAML in terms of the cumulative number of datapoints observed; and that both MAML and {Fixed|Learned}Tree MAML outperform a naive baseline.##### Weaknesses:1) **Significance**: Since the evidence provided in favor of the proposed algorithm is in the form of an empirical evaluation on a synthetically generated dataset, the present impact of the algorithm is limited. In particular, there is no evidence that (i) the algorithm works for larger and/or more complex datasets; and (ii) that natural datasets of interest to the community exhibit a hierarchical structure analogous to the synthetic datasets presented in the submission.2) **Novelty**: The algorithm modifies and combines previously introduced components: the MAML algorithm of [1]; the online top-down clustering algorithm of [2], and the task-similarity-as-gradient-similarity approach of [3].3) **Clarity**: Specific details surrounding the relationship between Algorithm  1 and Algorithm 2 are insufficiently discussed:   i) Algorithm 2 as it appears in the text is very similar to Algorithm 1 (The OTD algorithm) in [2] with the exception of the new hyperparameter $\xi$, and introduces new symbols that do not appear elsewhere in the text. It is therefore not sufficiently adapted for clarity in the context of this work.  ii) Whether Algorithm 2 acts as a strict subroutine of Algorithm 2 is not stated. I believe it is not because the clustering decision for a new task relies on tree structures that are "generated for a training batch," although what a "training batch" refers to is not clear. Similarly, how the "online"/"offline" distinction in the context of the clustering algorithm fits into the training/evaluating setup borrowed from [1] is not made clear.  iii) How exactly the task-similarity approach of [3] is employed in Algorithm 2 is not made clear. The only mention of the use of [3] is briefly around Eq. (8) before the main algorithm (Algorithm 1) is introduced, and Algorithm 2 only refers to a generic "similarity metric" (as in the original work, [1]).##### References[1] [Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." arXiv preprint arXiv:1703.03400 (2017).](https://arxiv.org/abs/1703.03400)[2] [Menon, Aditya Krishna, Anand Rajagopalan, Baris Sumengen, Gui Citovsky, Qin Cao, and Sanjiv Kumar. "Online Hierarchical Clustering Approximations." arXiv preprint arXiv:1909.09667 (2019).](https://arxiv.org/abs/1909.09667)[3] [Achille, Alessandro, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. "Task2vec: Task embedding for meta-learning." In Proceedings of the IEEE International Conference on Computer Vision, pp. 6430-6439. 2019.](https://arxiv.org/abs/1902.03545) In this paper, the authors aim to solve the problem of one class classification using self-supervision. While, this method has been adopted previously, in this paper, the authors aim to make the one-class classification robust to rotations. The main idea in the method is based on the idea of using anchor transformations instead of augmenting the dataset using transformed examples. The method is compared against other self-supervised one class classification methods on CIFAR-10/CIFAR-100 and SVHN datasetsThe novelty in the proposed work is limited as it specifically addresses the issue of geometric transformations for one-class classification and it is a very specific case that is addressed. The techniques adopted are also fairly straightforward. In the first technique, K anchor transformations are used to obtain the prediction with the learned self-supervised method. The second technique obtains the conditional likelihood conditioned on each transformation. These techniques are not particularly novel.The evaluation is limited as the paper argues that data-augmentation would not be applicable as it would result in inconsistent supervision. However, no actual evaluation is provided for the same. Methods like SIM-CLR advocate the use of contrastive learning with many classes. It is not evident that using the same approach for one-class classification can be done using just a single class. It would be interesting to consider additional evaluation where SIM-CLR is evaluated only with the additional one-class that is of concern.  *Summary*: This paper proposes a new dataset based on textbook / classroom chemistry questions for complex knowledge retrieval and aggregation. The authors scrape several thousands questions from online repositories and add additional natural language annotations signifying the quantities to be solved for in each question, as well as the declarative knowledge. Two baselines, one end-to-end neural and another symbolic, both fail at this dataset.*Strengths*: The dataset targets the important question of how to build models that can retrieve knowledge while performing complex reasoning.*Weaknesses*: As-is, the dataset fails to target the knowledge retrieval component---models are either expected to magically know how to calculate the answer, or use hard-coded functions that complete a graph of values. The neural baseline also seems a bit non-standard, raising questions of how well modern systems can actually do on the task; furthermore, the end-to-end neural system is disadvantaged in that it likely has not seen much chemistry-related content during fine-tuning, whereas the symbolic baseline has access to a host of human-defined functions. Furthermore, dataset quality is a bit difficult to assess without more samples.*Recommendation*: 3 . This benchmark is motivated by the lofty goals of encouraging the development of models that can combine knowledge retrieval, complex reasoning, and language understanding. However, its unclear to this reviewer whether it will prove useful in making progress towards such goals---theyre too conflated to be meaningfully evaluated within this context. To improve the benchmark and make it more amenable toward advancing those research goals (versus just being a difficult datasets that current models cannot handle), Id recommend explicitly targeting and evaluating this knowledge retrieval component as well. For instance, given a specific knowledge-base thats guaranteed to span the facts necessary to answer the questions, how well can a model (1) retrieve relevant information and (2) use such relevant information to answer questions?Questions:Chemical Calculation Problems cannot be solved by end-to-end neural networks since complex symbolic calculations are required: this is a hypothesis---there are many tasks where complex symbolic calculations are required, but end-to-end networks excel.What extent of knowledge is required to solve this task? For instance, many old semantic parsing datasets came with databases, and it was guaranteed that within the database, an answer would occur. What would a corresponding knowledge graph for this case look like, and how complex would it be?Unlike similar tasks annotation, we cannot collect all the atomic operations needed before starting annotation, since the set of chemical operators is not closed. The set of mathematical operators is also not closed (e.g., in math word problems). Why is this approach better than collecting all the operations represented in the dataset (even if it doesnt cover all of the operations that one could conceivably see)?The annotation interface / process looks quite regular---you arent expecting too much variation from the templates given. Given that you can help crowdworkers with these templates, why not just use these templates as the baseline for a formal meaning representation that would encompass the knowledge needed for the task?Can you give more details about the annotation process, beyond the short paragraph near the end of section 2.2? (We employed crowdsourcing for this annotation work...around 336 hours). Id be surprised if any crowdworker could label this sort of data well. What quality control filters did you put in place? Can we see more (random) samples of the dataset, so we can better assess its quality?End-To-End Solver: where did you get this model architecture from, such that This method represents a class of powerful neural networks, which achieved state-of-the-art performance on many QA tasks.? Ive never seen BERT used in a seq2seq setting like this (instead, people tend to use models trained on input/output pairs, like BART or T5). Id like to see how this compares to using BART or T5, since its not clear that the BERT initialization would be good for generation.Graph-Search based Solver: the need to implement specific functions (78, in this case) is significant, and undermines the point of this dataset, in my opinion. Theres no inherent value in learning to solve chemical equations well---the hope is that, in the process of doing so, well get modeling insights into what methods work well and can be generally applied to other knowledge-intensive tasks. This graph-search based solver seems narrowly scoped to ChemistryQA and difficult to adapt to other tasks, and its not entirely clear why we should value its results.Token-level accuracy: Is it guaranteed that the output of the graph-search based solver will be the same length as the gold output? How? Else, how is token-level accuracy computed? This paper aims to combine the label propagation and graph convolutional network with the modeling of their latent relationships. In the developed model, the node label is utilized to infer the edge weights between different nodes. From the evaluation results in Section 4, the performance improvement between the proposed method GCN-LPA and GDC is marginal, which can hardly demonstrate the advantage of the unified model (with GCN and LPA) over the graph diffusion network (without the restriction of information aggregation over neighboring nodes).Furthermore, this work generates another baseline with the combination of prediction GCN and LPA methods. From the evaluation results, this baseline performs much worse than GCN and GAT, which may indicate that the predict combination involves some noise. It is better to conduct further experiments to show the effectiveness of the proposed GCN and LPA integration mechanism over simplified combination.A minor note would be the lack of detailed hyperparameter tuning strategies of compared baselines. Different parameter settings may offer different performances; thus, it would be better to report how to perform the parameter tuning over various compared methods (such as GAT, GCN and GDC), to achieve good model performance and ensure a fair performance comparison.The proposed method incorporates the label propagation to calculate edge weights, which share similar paradigm for learning node correlations with graph attention network and its extensions. In addition to the performance gap between the GCN-LPA and GAT, more clarifications about the model difference could be added, to have a better understanding of the new combined GCN and LPA framework.It would be better to show the performance as the training/test ratio varies. It will be interesting to see is more data helpful to capture graph structural information better.In the experiments, only model scalability comparison between the new GCN-LPA method and GCN, is studied. Is the GCN-LPA more efficient than other baselines, and which component of the new framework is more computationally expensive? This work considers the graph deconvolution networks. It proposed a graph deconvolutional networks to reconstruct the graph signals. The concerns for this work are as below:1.The contributions in this work are incremental. It seems most parts are based on the previous works. The deconvolution part follows the scheme defined in (Bianchi et al. 2020). The overall graph autoencoder also follows a very common settings by involving pooling and unpooling operations. In pooling part, two pooling methods are employed that are hard pooling and soft pooling. However, it is not clear to me how to combine these two methods since they are very different. The equation (3) shows the combination of these two methods but fails to make sense to me. I would like the authors to clarify this and provide detailed explanations for this part.2.In both Section 4.1 inverse of GCN and Section 4.2 wavelet de-noising, the authors talked about two methods but didnt explain why they are related to the proposed graph deconvolution networks. Also, most of the content in these two parts are following previous works like (Li et al. 2019b) and (Xu et al. 2019). I would recommend the authors to clearly point out what is the contribution and how these are related to the proposed methods.3.The visualization section in 4.3 is confusion. Firstly, why the authors believe this is a contribution to the graph deconvolutional networks. To my understanding, there is no technical contribution for this paper. It would be better to show this in the experimental parts. Secondly, there is no description on the settings for the illustrations. I didnt understand how the image are converted into graph and how the inverse GCN and the proposed models are used to them. Thus, the authors need to clarify this and point out the technical contributions for this part.4.The experimental results are very limited. The four datasets used in table 1 are very small in terms of the number of graphs, which cannot provide comprehensive evaluations to the proposed results. The baseline methods are not well established. There are far better results on these datasets such as GIN. The authors claim that the unsupervised settings are used. However, it is weird to me why the unsupervised settings do not help the  overall model performances. ## SummaryThis paper attempts to unify the three most prominent regularization-based continual learning methods: EWC, MAS, and SI.While EWC has a solid theoretical justification under certain assumptions, the other two are based on intuition and heuristics.The authors show that the importance weights of MAS and SI are similar to the diagonal absolute Fisher.---## Pros- Connecting the path integral of SI to the Absolute Fisher is interesting.- The paper is well-organized and easy to follow.---## Cons### Lack of justification for the Absolute FisherThe Absolute Fisher seemingly appears from nowhere. In contrast to the diagonal Fisher, it does not seem to have any other interpretation. The authors merely show that MAS and SI are similar to using Absolute Fisher as importance weights. The similarity alone is not enough to be a theoretical explanation for effectiveness. I think the authors should justify that the Absolute Fisher is an optimal regularization under certain assumptions. ### Weak connection between the diagonal Fisher Information and the Absolute FisherDespite searching the web, I could not find any proper material on the Absolute Fisher that explains its connection to the diagonal Fisher.The only similarity that I find between the Absolute Fisher and the original Fisher is that they can be computed with the gradient. I do not see any reason to call it the Absolute *Fisher*.Therefore, I cannot agree with the claim that this paper presents a unified framework, just because it fits several methods into distinct concepts that have "Fisher" common in their names.### Doubts about the value of a unified frameworkI ask the authors a more fundamental question: do we really need a unified framework? Although the derivation of EWC is mathematically grounded, it heavily relies on certain assumptions about the shape of the loss surface. The assumptions make the use of the diagonal Fisher information an optimal choice. However, considering its poor CL performance, those assumptions seemingly do not hold in deep neural networks.Similarly, I think other methods, such as MAS and SI, can also be optimal under different assumptions. Therefore, I argue that it is more meaningful to investigate which condition/assumption makes a certain algorithm optimal, rather than framing all algorithms into one unified framework. I suspect that the latter case is not even possible.The experiments also support my claim: AF does not necessarily outperform MAS or SI. Instead, the focus of this paper is the correlation among methods. I think the best regularization method varies depending on the specific model architecture and task design.### Minor issues- Assumption 1 in Section 5.2 depends heavily on the batch size. The gradient noise will quickly diminish as the batch size grows.---## Overall evaluationIt is hard for me to agree with this paper's fundamental motivation: a unified framework for regularization methods. Also, the overall logic of this paper is too weak. Since I could not see other utility in this paper, I recommend rejection. Summary: The author proposes a method to train ensembles to have different prediction by using a correlation loss between the model's predictions. The authors show that their loss decreases the relative prediction difference between the models in the ensemble.Strong points:The correlation loss is a great idea to make model predictions as different as possible.Weak points:- The paper is difficult to read, and the line of reasoning is difficult to follow. It might be that I am just unfamiliar with the details of literature, but for me, it seems the authors are jumping back and forth between different interpretations and motivations.- It is not entirely clear why the authors work on an advertisement dataset and why consistent predictions between ensembles are relevant.- The experimental compare only against a baseline model, but the results are not grounded in the literature by comparing it to other methods.- It is not entirely clear why de-correlation within an ensemble can be seen as "anti-distillation."Recommendation (short):This work's motivation seems to be misaligned with the content of the paper, and the experimental setup offers no comparison with similar work. I do not think this work reaches the standard of ICLR and would like to see it rejected.Recommendation (long):Please see the weak points.Comments for authors:Do not be discouraged by my recommendation. This work has some interesting results, but it currently just a little bit below scientific standards. The major flaws are the experiments are difficult to compare on their own without any comparison with other methods. The private(?) data does not help make it easy to understand the results, and the MNIST results are a bit toy-ish. I see related work does use the Criteo Display Ad Challenge dataset, which is also an advertisement dataset which you can compare with other methods. Beyond this, your motivation is initially about false positive/false negatives in the medical realm and their impact on people as a motivation for more robust methods. For me, it is not entirely clear why de-correlation helps and how the work can be seen as "anti-distillation." I can see the analogy to co-distillation, so for me, it looks like co-distillation with de-correlation loss, but why is this interesting? Furthermore, the writing is difficult to follow, and you do not seem to use a latex library for citations/figures, which makes it difficult to navigate the citations. I also see some obvious spelling and grammatical errors  please run your paper through a spellchecker like Grammarly. Some of the related work is strange: Why is your work related to the model pruning literature?Overall, I would recommend working more on this paper, cleaning up all the major issues, and resubmitting it to another conference for more feedback and learning more from the process. If possible, it might also be useful for you to look for more experienced collaborators to improve writing and the overall experimental design process and the laying out of an argument based on those experiments. This paper proposes the Anti-Distillation method to encourage prediction diversity in an ensemble model, in order to improve reproducibility. As I understand, the reproducibility defined in this paper refers to the prediction variance w.r.t. the random factors during training, e.g., SGD.However, a trivial but complete reproducibility can be achieved by simply fixing the random seeds during training (without affecting model performance). Then why we would prefer the (incomplete) reproducibility induced by Anti-Distillation? If the reproducibility is the metric, then a single model or an ensemble model with fixed random seeds would trivially be the best model.Besides, the experiments are done on MNIST and a private dataset. I suggest the authors evaluating their method on public datasets like CIFAR or ImageNet, where there are many existing baselines to compare the model performance. ## SummaryThe paper describes the problem of "model irreproducibility" - the fact that two models (with same architecture and initial weight values) trained on the same data (in different order) do not give the same prediction on the same data point at test time, while they do have the same accuracy on the test set.The authors propose a method to quantify this effect using by looking at the absolute distance between the average prediction of an ensemble and predictions of elements of the ensemble.The authors also propose a loss penalty to combat this effect called "anti-distillation". The penalty computes the correlation matrix between the average prediction of an ensemble and the prediction of an element of the ensemble and is trained to reduce the sum of squares of the off diagonal elements.## ReviewOne of the motivations of the authors to quantify and attempt to reduce the effect of "model irreproducibility" is medical imaging, where one model would predict the current image contains a pathology but another one wouldn't. This is indeed a worrisome prospect of the use of ML in diagnostics.This problem has however been widely studied in the Bayesian literature, and is an instance of model uncertainty. If we take for example a Gaussian Process and sample a function from its predictive posterior, then that function has some predictions for certain inputs, however if we sample another function it might be different. In fact, whenever we find a lot of variation in the predictions for the same input then we consider to have large uncertainty on that input.Ensembles of neural networks allow for an effective way of quantifying uncertainty by looking at the entropy of the average prediction. If this entropy is high (meaning different classes where predicted by different ensemble elements, or the ensemble elements predict with low confidence) then the predictive uncertainty is high.Using the concept of uncertainty, it's possible to signal to a medical practitioner that the model should not be trusted on this particular input. This approach is much preferable than if we attempt to hide this diversity, and instead focus all models on a single prediction. In fact: the diversity is important information that we can use! Avoiding it using anti-distillation, might give a false sense of security for the user of the system, but the underlying uncertainty is still there.If the authors are interested in distilling an ensemble of models into a single model (for improved accuracy/uncertainty), which would be the outcome if the AD regularizer goes to zero, then additional comparisons need to be made for example with for example Malinin et al, 2019.In conclusion, I think the authors look at the important problem applying ML in automated decision making, but the proposed approach should be reworked to consider the concept of uncertainty and is currently not a viable solution to the problem set out.### References:Gal, Yarin. "Uncertainty in deep learning." University of Cambridge 1.3 (2016).Gal, Yarin, and Zoubin Ghahramani. "Dropout as a bayesian approximation: Representing model uncertainty in deep learning." international conference on machine learning. 2016.Kendall, Alex, and Yarin Gal. "What uncertainties do we need in bayesian deep learning for computer vision?." Advances in neural information processing systems. 2017.Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. "Simple and scalable predictive uncertainty estimation using deep ensembles." Advances in neural information processing systems. 2017.Malinin, Andrey, Bruno Mlodozeniec, and Mark Gales. "Ensemble distribution distillation." arXiv preprint arXiv:1905.00076 (2019).Example medical applications:Filos, Angelos, et al. "A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks." arXiv preprint arXiv:1912.10481 (2019).## NotesTable 1: "orrelation" -> "correlation"MNIST results: difficult to interpret the deltas. You improved on reducing prediction diversity, it's not unreasonable that this should work given your objective, but why is it useful? Why doesn't an ensemble of models improve the accuracy? On other datasets (such as cifar-10) that does happen and you might be able to measure more PD difference there.Difficult to interpret "Real Data", not enough details on the dataset. "Training was applied on billions of examples in a single pass over mini-batches of data" - does this mean you trained using one epoch? Why does it matter for the CTR if models disagree on their prediction? Isn't the original data very sparse/noisy anyway? It's unclear to me why this dataset was chosen given the motivation in the introduction.The paper is written with many tangents, where alternatives are suggested but never analysed and limited motivation is given for the current approach. For example, why don't you use an entropy penalty on the average prediction of the ensemble? The paper can benefit from increasing the clarity, motivation and additional discussion of the results. This `work applies the batch decorrelation loss proposed in  Reducing Overfitting in Deep Networks By Decorrelating Representations [Cogswell '15] to the final predictions an ensemble of neural networks in order to produce more diverse ensembles.The Anti-Distillation approach proposed is a method for learning diverse models which may reduce overfitting and improve performance in real-world task. Outside of applying this loss to the logits or predictions from various networks it is unclear what is the main contribution of this work relative to related works. There are mentions of various potential types of AD (Anti-distillation loss) but no ablative studies and the loss used seems identical to prior work. The tasks which this approach is tested on, MNIST and private CTR data, show negligible performance change and the results aren't clearly explained. Please provide clear captions on the charts in future and perhaps test on a tasks where ensemble methods show significant performance improvements.An approach of this style may very much be beneficial and useful for the community in setting where overfitting significantly impacts model performance. I'd encourage the authors to investigate more challenging domains, ColoredMNIST may be an appropriate alternative. As is the work should be arranged to more clearly present the contributions of the authors and the effects of the proposed method should be more clearly illustrated. The authors present an approach to pre-training of an ANN which utilizes a purportedly novel approach.  This approach aims to be "data aware" by incorporating the individual data points in the orthogonality constraint in the loss function.The paper is well-written and his mathematically rigorous.  The mathematics appears to be correct.  It is a concept paper, which I appreciate.The paper puts the proposed approach in juxtaposition with the greedy, layer-by-layer approach.The authors show improved performance on a handful of tasks against some baseline tasks.  The baselines were, mostly, well selected with a major exception (see below).The authors provide some experimental data analysis showing that the features learned by the hidden layers in the proposed approach are more easily interpretable by humans.  Furthermore, the features learned by the baseline features appear to be noisy whereas the features learned by the proposed approach capture the structure seen in the data.  Finally, the authors have some interesting results indicating that the features learned by this approach lead to greater generalization or transferability between tasks/data sets.My largest critique of this work is the lack of discussion/comparison with the long-standing auto-encoder approaches which can already compute PCA/SVD/orthogonal basis vectors utilizing a single hidden layer.  Are these methods unrelated in some way?  If so, why?  This work should be tied back to these approaches.Another critique of this work is the lack of future work directions.  The authors offered no questions about the proposed approach which were generated during their line of inquiry.  No analysis of where the proposed approach might be inferior to existing approaches was provided or discussed. Summary:This paper presents a graph pooling operator by first predicting scores on edges, then performing min-cut to separate subgraphs, and finally construct coarsened graphs. Authors perform experiments on several small datasets to verify their claims.Pros:1, The problem of defining pooling operators on graphs is important. The proposed idea is straightforward and easy to follow.2, The writing of the paper is smooth.Cons & Questions & Suggestions:1, The idea is not novel. Normalized cut type of methods for graph coarsening have been investigated by quite a few works, e.g., [2]. The proposed pooling operator is not fully differentiable since the gradient through the min-cut optimization is not exploited. Therefore, it is less appealing compared to methods like diff-pool. Moreover, the empirical performance is only comparable to diff-pool on few datasets. I am further concerned about the efficiency of the proposed method since min-cut on large-scale graphs is slow which would significantly slow down the inference of GNNs. However, this part is not discussed or empirically investigated.2, The experiment section is not that convincing. First, the performances are worse compared to other baselines. For example, the numbers in Table 1 are worse than g-U-nets consistently (I do not know why authors bold their results which are clearly not the best). Second, authors should include more graph pooling baselines, e.g., the ones with similar min-cut objectives like [2]. At last, it would be great to extensively test the proposed method on a wider range of datasets since the ones used for now are small-scale and many baselines achieve similar performances anyway.3, I think it is necessary to discuss or at least mention the original GNN paper [1] in the literature review. [1] Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M. and Monfardini, G., 2008. The graph neural network model. IEEE Transactions on Neural Networks, 20(1), pp.61-80.[2] Defferrard, M., Bresson, X. and Vandergheynst, P., 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NeurIPS.Conclusion: Overall, I do not think the paper is ready for being published. This manuscript proposes a new pooling layer in Graph Neural Networks (GNN). By computing certain scores on edges which indicate the importance of edges in the process of information propagation, top r% edges are selected and a pooled graph is constructed by considering the connected components to be super nodes. The authors tried to explain some connection between their pooled graph and the normalized cut problem, which is not clearly stated in the manuscript. Even though the manuscript explores an interesting and timely topic, their approach is not technically appealing and the explanations are not enough to thoroughly understand the authors' ideas. My main concerns and major questions are as follows:#1. In Section 3.1 (page 4), how $W_{pool}$ and $a$ can be considered as trainable variables and how these variables are actually trained are not clear. #2. Even though the authors argue that the number of clusters needs not be specified in advance, one should determine $r$ in their method instead.  In the experiments, the authors just tried several values for this $r$ value. There should be some rules to appropriately set this $r$ value. I'm wondering if it is possible to look at the distributions of edge scores and determine an appropriate $r$ value.#3. The descriptions about the relationship between their clustering approach (i.e., forming super nodes by taking the connected components) described in Section 3.1 and the graph normalized cut problem described in Section 3.2 are not clear. Where and how $L_{reg}$ is used in their proposed method?#4. Experimental results do not support that the proposed method is better than existing methods. Given this limited empirical contributions, I'm wondering if the proposed method has any theoretical benefits over existing methods. This paper studies the position embeddings of transform-based models, and proposed a unified position embedding evaluation method in three aspects, i.e., Monotonicity, Translation invariance and Symmetry, which can well summarise the properties of the existing position embedding methods.### Strengths of the paper1. It is good that the authors summarise three property for position embedding models, and discuss four related position embedding models under the three properties.2. The three properties proposed by this paper is suitable for most position embedding models.3. The experimental details are complete and easy to reproduce. Extensive experiment details are provided in the appendix.### Weaknesses of the paper1. The presentation and organization of this paper should be improved. Typos and language issues can be easily found. The contributions are not well highlighted in both the abstract and introduction section. Based on the contributions/conclusions described in the introduction section,  this paper seems to be an empirical study paper.2. The analysis of the experimental results are too subjective, without a convincing quantitative analysis. there are only some result figures about the dot-product of embedding vectors, which are not very rigorous and convincing. For example,   1) From Fig (2) and their discussion, I could not see any quantitative evidence to draw this conclusion: Lastly, note that fully-learnable RPEs also do not significantly distinguish far-distant RPEs (from -64 to -20 and from 20 to 64), suggesting that truncating RPEs into a distance of 64, like (Shaw et al., 2018), is reasonable.   2) The same issue for the conclusion on Figure (3): This may be an advantage of RPEs over APEs to perceive forward and backward words, especially in span prediction tasks where capturing this matters.3. This paper lacks the discussion with the latest position embedding models such as,   [1] "Learning to Encode Position for Transformer with Continuous Dynamical Model". ICML. 2020   [2] "Encoding Word Order in Complex Embeddings",  ICLR. 2019, where the 'Translation' property is also discussed.4. As an empirical study paper, the downstream tasks and datasets of the experiment section are insufficient, for example, there is no evaluation for the text generation tasks.Other minor comments:1. In the introduction, "distances in $\mathbb{N}$ and $\mathbb{R}^{D}$ ", $\mathbb{N}$ and $\mathbb{R}^{D}$ should be explained when they appear in the first place.2. The references should be formatted in a unified manner.3. Footnote 6 is unnecessary.Overall, I think this paper indeed shows some interesting empirical results of position embedding models for BERT. But, the analysis is monotonous and too subjective, lacking the necessary mathematical quantitative indicators, which prevents it from being a general way to verify the conclusions in this paper. The paper proposes a method to learn concept classes along with its concept superclasses. The proposed method relies on an ontology which they heuristically re-organize by essentially pruning nodes that have few descendants and large semantic overlap. The network proposed to model the ontology essentially just consists of a learned multiplicative gate at each level of the ontology with a standard xent loss over concepts and regularizing term that indicates if the category is an ancestor concept. The experimental results claim gains over some baselines, e.g., combined acc. of 69.05 vs chosen baseline 66.15 on ImageNet12 for ResNet-50 features at a cost of between 18.2% increase in parameters.Overall, while some of the empirical results seem competitive, I am most concerned with the weak foundations of the motivation of the setup. The work reads like a proposed solution that's trying to look for a problem / motivation and as a result struggles to find its footing in explaining modeling choices & results. * The paper uses as motivation that many networks use a softmax head over semantic categories at the leaves of an ontology and claims this is therefore why models using such networks do not learn that say English Setter is a dog. This is a shallow argument for incorporating concept hierarchies since such models clearly would still not be learning deeply what the concept of a dog is, only encoding weak priors introduced by the ontology, an external knowledge base from the network. The connection to learning relationships like "is-a" relations don't ultimately fall out from the proposed method, instead you just get a list of likelihoods that correspond to superclasses that contributed to a concept predictionthe model is not learning the relation, just the co-presence of these superclasses.* The argument that works like Deng et al (2012, 2014), which for example propose label relation priors like HEX graphs, only either predict fine-grained labels or superclasses exclusively, but not both simultaneously, is another example of where this paper falls short in its problem setup. This work doesn't answer the question of _why_ one would even want to predict both simultaneously well. If we believe that a superclass is unlikely present, why would we still predict the child classes? Even if there are reasonable arguments for this, they are absent in this paper.* The approach to creating the "compressed concept hierarchy" largely felt like a description of what was done, again, rather than why. Unless I missed it, I also expected a baseline for ablation that doesn't use the "compressed" hierarchy, but just uses it as-is.* It's a bit strange to me why a MSE loss is used for the indicator of whether a concept is an ancestor. Why use an unbounded error in L2, even if (or especially if) you are squeezing through a sigmoid? What is the intuition?* I would be curious to know if the improvement in results that we see in Table 1 are just due to increased model capacity (params/compute), i.e. how does it compare to the Deng et al baselines. The comparison discussed are only made with respect to the ResNet-50 & Inception-V4 backbones.I will also note that the paper could improve on its clarity in writing. As an example, from Figure 2, it's unclear what exactly changed from the LHS and RHS, how, and why it's meaningful; and in Figure 3, it's not obvious without work how the input, output and z-term relate. ### Paper SummaryThe authors proposed Deep Cognitive Learning Model (DCLM) as a surrogate of "discrimination part of CNN" which is the fully connected layers after the convolution layers. The authors claimed that DCLM can explain the causal relationship between the features and the output result of the discrimination part of CNN.### Quality & ClarityI found the paper very hard to read. I tried my best to understand the paper. The major contributions of this paper are as follows.1. The authors proposed to use DCLM to approximates the fully connected layers (FC), i.e. $\mathrm{FC}(x) \approx \mathrm{DCLM}(x)$ for any $x$. Because DCLM is in a logical formula, we can interpret the "discrimination part of CNN" approximately by interpreting DCLM.2. The authors further proposed to use DCML to "regularize" FC. That is, train CNN so that its FC to be close to DCLM.The first contribution is described in Sec3, while the second contribution is described in Sec4 and Sec5.Below, I list the points that makes the paper hard to read.* The definition of the term "discrimination part of CNN" is described at the beginning of Sec3, although the term is frequently used in Sec1. I could not understand what the "discrimination part of CNN" means when I first read the paper. This problem is fatal because the readers cannot understand the problem the authors want to solve in this study.* The paper does not describe why interpreting "discrimination part of CNN" is important. Thus, the paper failed to motivate the problem. Although the authors cited some related studies, such as [Wan+20], the importance of the problem need to be described in the paper. This problem is fatal because the readers cannot understand why the problem is important.* Algorithm1 contains undefined notations. For example, the inputs and the outputs of $\mathrm{CN}$ and $LN$ are not defined. There are no descriptions what $FMs$ and $f_{nn}$ are. Moreover, the second $\mathrm{CN}$ in Algorithm1 outputs $FCMs$ in addition to $FMs$ and $f_{nn}$. This problem is fatal because the readers cannot understand how the proposed method operates.* The authors merely compared the accuracies of DCLM and its approximation performance. There is no demonstration nor discussion that DCLM can resolve the problem of interpretability of the "discrimination part of CNN", which should be the primal goal of this study. This problem is fatal because the readers cannot understand whether the problem considered in this study is solved.Overall, I think the paper needs major rewriting so that the main message of the paper to be clear: what the problem is; why it is important; how the authors solved the problem; and how the authors confirmed the problem is solved.In addition to the readability, I also found some technical errors.* In Eq.(5), the partition function $\log \Xi$ should appear because it is a function of $y_{dclm}$ and $a$. The authors somehow ignored it.* In Eq.(13) and Eq.(14), the partition functions $\log \Xi_1$ and $\log \Xi_2$ should appear because they are functions of $w$. The authors somehow ignored them.There should be some justifications why one can ignore the partition functions. Or, the experimental results need to be updated based on the objective functions with the partition functions.### Originality & SignificanceThe use of decision tree for approximating the "discrimination part of CNN" is considered by [Wan+20]. This paper proposes using a logical formula instead of the decision tree. The idea seems to be straightforward, and the innovation made in this paper is marginal.### Pros & Cons[Pros]* I could not find anything positive about this paper.[Cons]* The paper is very hard to read. The major rewriting is needed.* There are some technical errors.* The improvement over [Wan+20] seems to be marginal. I found this paper very difficult to follow as it has many grammatical and syntactic errors. I believe it needs a significant amount of editing in order for the paper to be published in english. In particular careful attention should be made to omitted particles and pluralization.  This alone is a barrier to publication. Setting aside the grammatical errors, I believe there are some issues in the content of the paper that would need to be addressed as well in order to make this paper ready for publication. If I understand the paper correctly, which I am not sure I do, I believe the authors propose a method for extracting an interpretable model which replaces the fully connected layers of a Convolutional Image Network. They present results on toy datasets, MNIST, Emnist, and FashionMNIST.   My primary issue with the paper is that it attempts to provide an 'explainable alternative' to a CNN but this explainable model still relies on the features extracted from the convolutional section of a CNN. The paper does not put forward a convincing argument to justify the focus on the fully connected layers. It is interesting to extract an interpretable model from a fully connected network, but if this is the goal of the paper, then the authors should focus on datasets in which a fully connected network outperforms standard explainable models such as logistic regression but interpretability would still be desirable, such as the MIMIC medical dataset. The paper would also be significantly improved if more realistic datasets would be explored. The only datasets used are variants of MNIST, in which good performance can be achieved with traditional explainable models. This paper proposes a method for estimating conditional and average treatment effects under unconfoundedness. There are two main ideas: (1) train a VAE with latent space aimed at the adjustment-relevant information, and (2) incorporate the targeted regularization of Shi et al into the training.The problem of estimating causal effects using deep learning is important, and I think paper has a promising direction. It is closely related to a growing literature on this subject (as noted in the paper itself), and does a reasonable job of explaining the innovations.However, there are some significant issues.The exposition is generally unclear. The paper needs a complete rewrite, with particular attention to clarity about the (salient parts of) TMLE, and the claimed advantages of this approach over closely related methods. There also needs to be substantial improvement around the VAE component of the model, and precisely what the identification assumptions are.The empirical results are inadequate. The IHDP and LaLonde datasets are not challenging enough to separate methods in the bakeoff, nor rich enough to give substantial insight into their performance. I suggest you use ACIC competition data. Further, the ablation study doesn't seem to test the right aspects of the model. E.g., there should be some experiment showing that not applying targeted regularization to the propensity score part of the model leads to better performance, since that's the core distinction with Shi et al.This paper isn't ready for publication in its current form. However, I reiterate that I think the basic ideas are interesting, and worth developing.Some further comments and questions:1. missing expectation symbol in 2nd paragraph of background (in def of \tau)2. equation 1 is meant to be describing an observational quantity; the do(T=t) should just be t3. the discussion about optimal epsilon=0 is confusing (I understand that you mean that running more than 1 round of the TMLE update doesn't make a difference, but this is not clear in your writing)4. indeed, all of the prose around equation 2 should be rewritten. It is not clear in the current version.5. under DAG (c), x doesn't block backdoor paths and strong ignorability is not satisfied6. the related work should be clearer that targeted regularization was proposed in Shi et al 7. what models are you using for Q and g for the TMLE reported in the table? In particular, are you comparing training with targeted regularization to training without targeted regularization and then plugging in to TMLE?8. when you compare to 'Dragonnet' are you comparing to Dragonnet + Targeted Regularization, or just vanilla dragonnet?9. what exactly is done to prevent the propensity score from being modified by the targeted regularization? I presume that at least the shared latent representation is still affected by the inclusion of the targeted regularization term? The authors study the problem of adversarial robustness, aiming to find regions of the input space for which a classifier is robust. Instead of the standard approach of defining a neighborhood around each data point based on some $\ell_p$-norm, they use PCA to identify directions along which the model is robust or brittle. They then use these methods to identify large regions of input space for which models are robust and, in a complementary direction, to craft imperceptible adversarial examples with few model queries.In general, understanding the set of perturbations that our models are robust to is an important research question. Unfortunately, the current paper does not go into significant depth. **Robustness neighborhoods.** ¤he input robustness regions computed using this approach are rather unintuitive. What insight is gained by reading Table 1? Do these robustness regions correspond to something concrete and meaningful (e.g., diagonal stripes, brightness) that we can convey to human users of the model? What fundamentally new understanding do we obtain via this analysis?**New adversarial attacks.**  The space of existing adversarial attacks is huge. By now, there exist so many different approaches for computing examples that fool models. From that perspective, it is not clear what the new attack proposed offers. Similarly to the point above, since these directions do not necessarily capture something human-understandable it is unclear how they reveal a fundamentally new model vulnerability.Overall, while the research direction is interesting, further exploration is needed to reach novel insights about these models.Other comments (not affecting score):- The large numbers representing "number of images" are somewhat misleading. How can we quantify whether these images are actually distinct. I do not think that these number convey significant information and I would thus recommend removing them.- https://arxiv.org/abs/1807.04200 and https://distill.pub/2019/advex-bugs-discussion/response-3/ also study model robustness to PCA-based perturbations and might thus be worth discussing. This work proposes a method to find weak and robust features via sampling, and utilises the method to (a) define feature-guided neighborhoods and (b) to improve score-based black-box adversarial attacks.I examine the two main contributions (a) and (b) separately. The feature-guided neighourhoods are basically defined as 1D linear subspaces around a given sample x. The subspaces are usually defined along PCA-projections of the data set, although the definitions would also allow non-linear subspaces. The work claims that along some of these directions a classifier D is robust, and that using verification techniques one can show that these robust subspaces can be much larger than simple epsilon-ball neighbourhoods. While this claim might be true, the significance of this insight is unclear to me. Of course, if carefully chosen, one can find large neighbourhoods in which a classifier does not change its decision - but I fail to see what insights can be taken away.In a second step, this work computes weak features, i.e. directions in the pixel space along which the classifier D is generally susceptible, and uses this to perform a score-based adversarial attack. The work claims higher robustness with fewer iterations than AutoZOOM and GenAttack. This is indeed an interesting direction, but the work misses a lot of relevant prior work on this area that should be discussed and compared against (in particular many other score-based but also decision-based attacks, one would need to take into account the number of queries needed to find the weak PCA directions to compare against other attacks, etc.).It might be beneficial to remove the discussion of the neighbourhoods and to concentrate on the improvement of black-box attacks (which would have be shown much more convincingly). This paper considers federated learning with straggling and adversarial devices. To tackle stragglers, the paper proposes semi-synchronous averaging wherein models with the same staleness are first averaged together, and then a weighted average of the results with different stateless is computed. To mitigate adversaries, the paper proposes to first perform entropy-based filtering to remove suspected outliers, and then compute loss-weighted average. The server is assumed to have some public data, which is used for entropy-based filtering. Together, the proposed algorithm is called semi-synchronous entropy and loss based filtering (Sself). Strong points:1. Mitigating adversarial attacks, especially model poisoning and backdoor attacks, is an important challenge in federated learning. 2. The proposed algorithm is simple. The paper is well-written, and easy to follow. Weak points:1. Theorem 1 does not seem to consider adversarial devices, and the proof seems to only the semi-synchronous part of Sself without entropy-based filtering and loss-weighted average. Intuitively, the convergence performance should degrade with the number of adversarial devices. However, the theorem statement does not seem to indicate so. If the theorem only analyzes the semi-synchronous part of Sself, then this should be explicitly mentioned. The theorem in its current form is a bit misleading.Further, even when there are no adversaries, what is the theoretical improvement over FedAsync? Specifically, what is the impact of staleness on convergence. It is important to add remarks to elaborate the gains qualitatively. 2. Experiments are performed for the simplistic case when each device has a delay of 0, 1, or 2 rounds (chosen uniformly at random and independently for each device). Further, experiments (in the main body of the paper and supplementary material) are for a limited number of adversarial attacks. The performance can be evaluated for practically motivated straggler models and more powerful known attacks. For instance, the attack from the following paper:G. Baruch, M. Baruch, Y. Goldberg, A Little Is Enough: Circumventing Defenses For Distributed Learning, NeurIPS 2019.While proposing entropy-based filtering, the authors hypothesize that if the local model is poisoned, e.g., by reverse sign attack, the model is more likely to predict randomly for all classes and thus has a high entropy. However, some attacks such as targeted backdoor attacks typically do not hurt overall accuracy. So, it is not clear why the poisoned model is likely to predict randomly for all classes. It will be helpful to add more evidence. 3. Evaluating the loss on the public data for each device may incur significant computational complexity. It is important to elaborate on how much the complexity at the server will increase by using entropy-based filtering on public data. Other suggestions:1. Fig. 2, should the x-axis label be round number than running time?2. In experiments, can you quantify the performance gain of Sself over FedAsync? For instance, in Fig. 2, FedAsnc seems to be almost on par with Sself. It will be helpful to quantify the performance improvement. 3. Zeno proposed in the following paper also uses public data at the server. If possible, it will be good to compare against Zeno for fairness. Cong Xie, Sanmi Koyejo, Indranil Gupta, Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance, ICML 2019.In summary, the theoretical result (Theorem 1) is weak as it does not seem to consider adversarial devices. In experiments, the stragglers are simulated in a simplistic manner, and the gains over FedAsync are not quantified. It will be good to consider the weak points and suggestions to improve the paper. Currently, the novelty seems to be fairly limited.  This work introduces a notion of "usable information" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are "minimal" (discard task-irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations. Pros:-- the definition of usable information is reasonable and likely useful for future analyses (though see below)-- the questions addressed by the paper are interesting / important and are in need of thorough empirical studyCons:-- the tasks used in this paper are very simple, with even/odd MNIST classification being the hardest task considered (and most analysis is conducted on an even less complex task, which is similar to a simple XOR).  It is very hard to know whether the paper's conclusions would generalize to tasks of interest to the machine learning community, or even to other simple tasks with different structure-- It is not clear from the pretraining experiments whether the negative results are due to pretraining or just the scale of the weights.  If networks were initialized randomly with mean / std taken from the pretrained network, would they also not learn minimal representations?-- It seems that the results of the paper must necessarily depend on several hyperparameters which were not explored.  For instance, if the learning rate in early layers is set sufficiently small, the network should learn these simple tasks without minimal representations.  -- The result about generalization correlating with minimality was not confirmed on MNIST.  It is not clear whether this is because the result does not hold on MNIST or simply because the authors did not test it.-- Transfer learning is known to be helpful in some practical settings.  Is this framework able to account for situations when transfer might be helpful, as well as harmful?  More discussion of this is neededOverall, given that this is an empirical paper (no new theory is provided), it is important for the experiments to be extensive and comprehensive.  The experiments in this paper, though they touch on interesting ideas, are not thorough enough to convince a reader of the authors' broader claims.   SUMMARYThe paper presents a method to decompose scenes into its constituent objects. This is done with a generative framework that generates both bounding boxes and segmentation masks for each object. It relies on several previously existing technologies. Its main contribution is enforcing consistency between bounding boxes and segmentation masks.PROS* Outperforms the baselines.CONS* The paper can be hard to read.* Contributions seem minor.* Good results, but on two toy datasets only.COMMENTSThe writing should be improved, as the paper can be hard to follow. One one hand, this includes broken sentences ("Inspired by the observation that foreground segmentation masks and bounding boxes both contain object geometric information and should be consistent with each other."), grammatical errors ("It proves that there are still many useful information can be discovered in those unlabeled data."), and sentences which are just hard to parse ("In the former type of models, the scene is encoded into the object-oriented disentangled spatial and appearance encoding explicitly."). On the other, the authors cite many concepts without introducing them in the paper (stick breaking, spatial broadcast decoder, multi-otsu thresholding, etc) which makes it non self-contained.The paper presents what seem like engineering improvements over previous works (e.g. combining bounding boxes and segmentation masks) by adding more components to the framework, which is quite convoluted (see Fig. 1: ResNet, FPN, RPN, segmentation, VAEs, etc). It is hard to know where performance comes from, despite the ablation tests.The experiments are limited to two toy datasets with a fixed number of simple objects (which must be known beforehand), which show no background interference and little occlusion.In all, I do not think it meets the ICLR bar.I am not an expert on the topic so I may have missed relevant datasets/baselines.Detail: "Region of Interest" introduced after ROI has been mentioned several times. This paper proposes to use a counterattack strategy to attack an input x, and calculate the distance between x and the crafted example x' as the detection metric. There are two main concerns about this paper:1. The authors claim that their detection method is attack-agnostic, but their motivation in Fig 1 highly depends on the assumption of the attacking mechanism. There is no guarantee on the effectiveness of AttackDist when the attacks do not follow assumed patterns.2. In experiments, there are only oblivious attacks, where the adversaries do not know the mechanism of AttackDist. For a defense method, it is necessary to carefully design a convinced adaptive attack and demonstrate the effectiveness of the defense under the adaptive attack.Minors:In the introduction section, the authors claim that "Existing adversarial defense techniques could be classified into two main categories: adversarial training and detection". Actually, there are many other types of defenses including input processing, robust architecture, random smoothing, certified defenses, etc. Besides, existing adversarial training methods like FastAT can easily scale to ImageNet, running for several hours on a single GPU. The authors should be more updated on these related progresses. This paper proposes a method to detect adversarial examples. The detectionscheme is based on the observations that typical adversarial attacks generateadversarial examples on the decision boundary, so if we use a "counter attack"on the adversarial example, it will be easy to change its label.A main weakness of this paper is that the proposed approach does not include anadaptive attack for evaluation.  If the proposed detection scheme is known tothe attacker, the attacker can still generate visually indistinguishableadversarial examples that the detector fails to detect. This can usually bedone by adding the detection objective to the loss function for attack.  Manyheuristic adversarial example detections and defense methods have been brokenby stronger and adaptive attacks [1,2], and the use of adaptive attacks iscrucial [3].Additionally, although the paper claims to detect zero-day, or unknown attacks,in evaluation the selection of attacks are quite limited. For example, it onlyincludes gradient based attacks, but not decision based attacks or evolutionaladversarial attacks.The paper attempts to make several theoretical justifications, but thesetheorems are too simple (e.g., based on direct application of triangleinequality) and do not significantly improve the contribution of this paper.As a conclusion, I cannot support the acceptance of this paper because thenovelty of the proposed method is limited and evaluation is insufficient.[1] Athalye, Anish, Nicholas Carlini, and David Wagner. "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples." arXiv preprint arXiv:1802.00420 (2018).[2] Carlini, Nicholas, and David Wagner. "Adversarial examples are not easily detected: Bypassing ten detection methods." Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 2017.[3] Carlini, Nicholas, et al. "On evaluating adversarial robustness." arXiv preprint arXiv:1902.06705 (2019).[4] Brendel, Wieland, Jonas Rauber, and Matthias Bethge. "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models." arXiv preprint arXiv:1712.04248 (2017).[5] Cheng, Minhao, et al. "Query-efficient hard-label black-box attack: An optimization-based approach." arXiv preprint arXiv:1807.04457 (2018).[6] Alzantot, Moustafa, et al. "Genattack: Practical black-box attacks with gradient-free optimization." Proceedings of the Genetic and Evolutionary Computation Conference. 2019. #### Summary:The paper proposes an active testing approach that actively selects test instances to estimate the performance of a (black box) machine learning model. The key idea is to train a Bayesian Neural Networks (BNN) with a small amount of labeled test data and evaluate how well the model-under-test agrees with the BNN on samples for which the BNN has a high confidence. More instances to be labeled by an oracle are selected with active learning, i.e. select the data point that minimizes the uncertainty of the metric prediction.#### Recommendation:I recommend to reject the paper, mostly because the experimental results do not support the claim that the proposed approach performs much better than traditional evaluation or prior works. Furthermore, appropriateness of the experimental setup and novelty of the BNN training are unclear and proper discussion of related works is missing.#### Strong Points:- Active testing is an important research direction since more and more pre-trained models are applied in practice without being fine-tuned.- The approach is clearly described. The paper is well-written and easy to understand.#### Weak Points:- The evaluation does not support the claim that ALT-MAS performs substantially better than traditional evaluation or prior works. Out of 30 plots, the proposed method performs best in approx. 3 cases, ties for the first place in approx. 8 cases, and is inferior (wrt. the traditional eval and BALD) in the remaining ~19 cases.- The title suggests that the paper addresses machine learning in general. However, only a simple deep neural network for MNIST and CIFAR10 is used in the evaluation. Hence, it remains unclear if the approach can successfully be applied to other machine learning methods and datasets. In general, it would be a strong point of this paper to include many different machine learning models since it is agnostic regarding the learning framework used.- I disagree with the general statement 'Better methods are those that converge faster to zero'. It depends a lot on the budget available for labeling new instances and the desired value (and confidence) of the target metrics. A clearly better method would have a lower error for any amount of labeling budged.- Active testing is appealing when it can be assumed that the test data has a different distribution than the training data. However, most of the experiments use same train/test distribution. Hence, the approach is not evaluated in an appropriate setting.- The paper claims to contribute 'novel approach to train the BNN so as to accurately estimate the metrics of interest'. However, it uses Monte Carlo dropout. It is unclear which part of the training is novel.- The main body of the paper does not contain a section that positions the work well in the available literature.#### Questions:1. Do I understand correctly that the 'Traditional' evaluation uses as many samples for evaluation as ALT-MAS and BALD? I am referring to the phrase 'the traditional method where the metrics are computed using their mathematical formula with all the labeled data *up to the current iteration*, and the labeled data is picked randomly from the whole test dataset'.2. In Section 4.2, you write that 'ALT-MAS performs well on all of the scenarios'. In Figure 2, first row, I'd say that ALT-MAS is inferior than the traditional evaluation in 4 out of 5 cases and ties for the first place for Recall_2. How do you come to the conclusion that ALT-MAS performs well on all of the scenarios? ##########################################################################Summary:This paper presents a simple meta-learning method for iteratively fine-tuningword embeddings. The method works by adding noise and bias terms to the learnedembeddings after each training session, and initializing the same model on thesame task with the resulting representations for further fine-tuning. Noise isadded to a subset of the vocabulary, while bias is added to every element of it.The process is repeated until noise has been added to all the elements of thevocabulary.Results show that the proposed method consistently improves results in the setof classification tasks in which it was tested.##########################################################################Reasons for score:However, the paper is not very thorough in its exposition and leaves too manyquestions without answer. In all, it is difficult to assess the realcontribution of the suggested method.##########################################################################Pros:- Method could be useful to the community working on non-contextualized word  representations.##########################################################################Cons:- The proposed method is unclear.- Analyses are lacking#########################################################################Comments, suggestions and questions for the authors:- p. 1, sec. 1, par. 1: I do not understand the difference between  "starting with better representations" and "building more sophisticated  architectures". For the former you cite ELMo as an example, but ELMo was both  a new architecture and training methodology, hence my confusion with  qualifying it as "starting with better representations". For the latter you  cite the Transformers, which makes more sense as a new architecture. But then,  how would you classify BERT, being both a new architecture sharing transformer  components, and a new training methodology? (This is more of a rhetorical  question; I suggest clarifying the points above directly in the paper.)- p. 1, sec. 1, par. 4: Did you take into consideration phenomena such as  catastrophic forgetting (https://arxiv.org/abs/1312.6211), when re-training?  Even though you are not changing the training task, I think it would be  valuable to at least mention this.- p. 2, sec. 2.1, par. 1: Mikolov et al. 2013b make no mention of the  distributional hypothesis, and even if they did, there is no implication  between this hypothesis and the fact that pretrained embedding vectors are  composed of the pairs you mention.- p. 2, sec. 2.1, par. 2: A reader not deeply familiarized with the differences  between contextualized word representations, and pre-trained word-vectors will  probably have a hard time understanding this paragraph. I suggest making it  clear that contextualized representations in the context of ELMo and BERT are  actually pre-trained models, and not only pre-trained word representations  such as GloVe, word2vec, and fasttext.- p. 2, sec. 2.2, par. 2: References to previous works doing "Word Dropping"  would be a welcome addition to your paper. Is this paragraph an allusion to  stopword removal, or something else?- p. 3, sec. 3.2, par. 2: `MaskingRate` is a hyperparameter of the noise  maskers, but in sec. 3.1, par. 2 it says that the bias is weighted by  1/[`MaskingRate`]. I suggest clarifying the differences between noises and  biases, and their relationship to maskers.- p. 3, sec. 3.2, par. 3: You mention you add a bias to the fine-tuned word  embeddings, in order to take them to the same embedding space. What do you  mean exactly with this? Addition does not change the dimensionality of the  summands, therefore adding something to W' will only shift its values and not  modify their embedding space.- p. 5, sec. 4.2, par. 3: How do you initialize word embeddings randomly? Is it  also by sampling from the uniform distribution between -1 and 1?- p. 5, sec. 5.1, par. 2: It is unclear what you mean with "since we train a  model from scratch except for word embeddings."- p. 6, sec. 5.1, par. 2: You mention that GraVeR makes substantial improvements  when using relatively poor embeddings, but I can see that GraVeR has a  significant impact on fasttext embeddings (Table 1), which I would not call  poor.- p. 6, Table 2: I suggest specifying what the values between parentheses and  after the $\pm$ are; standard deviation? confidence interval? if so, with  which confidence level?. Same suggestion for Table 3.- p. 7, sec 5.2: Why did you decide to show cherry-picked results for GloVe,  despite your method not working well with these pre-trained embeddings, as  shown in Table 1? Why not show results for fasttext embeddings instead?- p. 7, Table 4: If NoiseScale 1 means that the values from the masker were  sampled from Uniform(-1; 1), as explained in section 3.2, I assume that  NoiseScale 5 implies Uniform(-5; 5). If this is true then what does NoiseScale  0 imply?- p. 7, sec. 6.1, par. 1: How can you tell from Table 4 that the model  overfits?- p. 8, Table 5: As mentioned previously, I am confused by the choice of  parenthesis versus plus-minus symbols, and the meaning of the numbers they  refer to. Why do you use $\pm$ for every Gen variant, but parentheses for Sp?  why do you use $\pm$ for BERT (Sp), despite the previous? Also, why did you  decide to invert the Gen-Sp row order in the BERT entry?- I am a bit confused about the role Maskers play in your model. You mention  them for the first time in the caption of Figure 1, without much context. Then  you mention that you "add maskers filled with random values to a portion of  W'", which makes me think that maskers are nothing other than random vectors,  but I am not 100% sure. The paper goes on assuming that the reader has a clear  understanding of what maskers are, but I personally found this not to be the  case. Further, maskers are not mentioned in Algorithm 1, only `MaskedWords`  and `NextFrequentlyUsedWords`, which I guess are somehow related to maskers,  but again, I can't be sure exactly how just by reading the paper.- Another pain point related to the above is that you superficially mention (in  Algorithm 1, and the two paragraphs before it), that word frequency somehow  conditions which words are masked, but no details are given as to how this  happens.- I found the tables to be a bit overcrowded and difficult to parse in general.  I suggest trying to simplify them to better get your point across.- I think the Table 1 mentioned in the Appendix should be Table 6.#########################################################################Some typos:- Abstract: "some of noises" -> "some noises"- Abstract: What do you mean with the last sentence?- p. 1, sec. 1, par. 2: "the general and meaning of words" -> "the general  meaning"- p. 1, sec. 1, par. 4: "to the next re-training process" -> "in the next  re-training process"- p. 1, sec. 1, par. 4: What do you mean with "the model must be overfitted"?- p. 2, sec. 2.2, par. 3: "It can regularize the models,[...]re-usability." is  not a grammatically correct sentence.- p. 2, sec. 2.3, par. 2: In "Also, dropout discourses the weights" I think you  meant to write something other than "discourses".- p. 3, sec. 3.1, par. 2: "best-performed" -> "best-performing"- p. 4, sec. 3.2, par. 4: I think the sentence "Although this approach cannot  warrant moderate noise, we take a safe approach." is not clear enough.- p. 4, sec. 4.1, par. 1: "in Appendix" -> "in the Appendix"- p. 5, sec. 4.2, par. 1: "with the 32 channels" -> "with 32 channels"- p. 5, sec. 4.2, par. 1: "max-pooled" -> "max-pooling"- p. 5, sec. 4.2, par. 2: "small parameters" -> "few parameters"- p. 5, Table 1 caption: "vector" -> "vectors" Summary: this paper claims to show that the mathematical formulation of SDEs is directly comparable with the formulation of GANS. I found this paper to be poorly premised. At the outset, the authors state An SDE is a map from a noise distribution...to the solution of the SDE which is some other distribution on path space. This statement is incorrect. First of all an SDE is not a map on measure space. It defines the evolution of sample paths of stochastic processes that induce measures. This suggests to me that the authors conflate the measures on path space with paths themselves. Im also confused by the analogy between sampling SDEs and GANs  one might as well draw analogies with sampling Gaussian distributions. This is entirely confusing.There are further fundamental issues that crop up throughout the paper. For instance, in Section 2.2, the authors state that $Y\stackrel{d}{\approx} Z$; but what do they mean by this? That  the finite dimensional distributions are approximately equal?It appears that the point of the paper is that Wasserstein GANs can be applied to path measures induced by SDEs. This is not a novel insight, in my opinion. The paper "Simple deductive reasoning tests and data sets for exposing limitation of today's deep neural networks" describes several datasets for deep learning to test deductive reasoning abilities of neural networks. The paper tests several neural network architectures (as well as random forests) on these datasets and concludes that neural networks are generally not able to perform deductive reasoning.My main critique is that the authors do not seem to be aware of any of the research that is going on in the area. The opening statement of the paper is as follows: "Learning for Deductive Reasoning is an open problem not yet explicitly called out in the machine learning world today." I'm afraid such a statement is simply not true. Reasoning (including deductive reasoning) is a very active research area in the machine learning communities. See below for a very partial list of works.Second, the paper contains many assertions that neural networks are incapable of reasoning. For example: "The deep neural networks with todays notion of a neuron are not suitable for deductive reasoning" The main reason the authors give for this claim is that "neurons" perform only simple arithmetic operations. However, consider that computers only perform simple boolean operations and yet can perform the tasks described in this paper.These claims are also contradictory to some findings in the literature, which the authors do not seem to be familiar with. Here is a bunch related work that the authors might want to take a look at. (And there are plenty more papers in the area.)Datasets with similar objectives:    Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv preprint arXiv:1804.06028, 2018.    "ANALYSING MATHEMATICAL REASONING ABILITIES OF NEURAL MODELS", by Saxton, Grefenstette, Hill, Kohli, ICLR 2019    "INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving" Wu, Jian, Ba, Grosse, https://arxiv.org/abs/2007.02924Datasets and theorem proving with neural networks:    "DeepMath - Deep Sequence Models for Premise Selection" Alemi et al. https://arxiv.org/pdf/1606.04442.pdf    "Learning to Prove with Tactics" Gauthier et al. 2018    "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving", Bansal et al, ICML 2019    "Learning to Prove Theorems via Interacting with Proof Assistants" Yang, Deng, ICML 2019    "GamePad: A Learning Environment for Theorem Proving", Huang, Dhariwal, Song, Sutskever, ICLR 2018    "Generative Language Modeling for Automated Theorem Proving", Polu, Sutskever, arxiv 2020    "Can Neural Networks Learn Symbolic Rewriting?" Piotrowski et al., 2019Neural network architectures for reasoning, including (Tree)RNNs, GNNs:    "Can Neural Networks Understand Logical Entailment?", Evans et al. 2018 https://arxiv.org/abs/1802.08535    "Graph Representations for Higher-Order Logic and Theorem Proving" Paliwal et al, AAAI 2021    Also, plenty of pre-deep learning work by Joseph Urban on how to turn logical formulas into features.Recently, Transformers have been shown to be good at logical reasoning:    "Deep Learning for Symbolic Mathematics", Lample and Charton, ICML 2020.    "Transformers Generalize to the Semantics of Logics", Hahn et al, 2020. https://arxiv.org/abs/2003.04218    "Mathematical Reasoning via Self-supervised Skip-tree Training", Rabe et al, 2020, https://arxiv.org/abs/2006.04757My third point is that the paper does not specify the experiments precisely. What are the hyperparameters of the neural networks?Fourth, the paper claims to consider "today's deep neural networks" but does not consider modern neural architectures, such as GNNs and Transformers. These have been shown much better reasoning abilities than RNNs.In summary: The paper addresses an important question and I encourage the authors to continue to follow this path. But this work does not consider the existing literature at all and a does not make significant contributions beyond the state-of-the-art as far as I can see.Minor comments:"A majority of the machine learning models are inductive reasoning models"I believe by "inductive reasoning" the authors here refer to the learning process. I think the learning phase has to be contrasted with the inference phase."However for the sake of convenience and interpretation, a vector is typically represented as a tensor"The notion of tensor is a generalization of vector. This paper's contribution is introducing a set of tasks and datasets that require deductive approaches as opposed to common induction-based models. The paper tackles an important and interesting problem that helps to shape the future of the neuro-symbolic research area. My main concern however is, the paper ignores and does not cover the current state-of-the-art techniques and their corresponding datasets and by just introducing some datasets fail to give a correct image of the current efforts in this area. For example, the variation of Neural Turing Machine and Memory Networks has been successfully applied to the sorting problem (which has been proposed as one of the tasks of interest in deductive reasoning in this paper as well) [1], however, the authors have not discussed these class of networks at all. In fact, the authors mention the gap in the current models by talking about the need for models that can store the facts and the intermediate results for being able to conduct deductive reasoning but do not talk about the role and shortcomings of Memory Networks and Neural Turing based models or  NeuralStacks/Queues. Similarly, there are no arguments in the paper about why Neural Theorem Provers [2] cannot be used to emulate the deductive inference mechanism. In summary, the authors have initiated a good step toward defining the simple deductive reasoning tasks; However, the work has not placed well on the body of current neural and neuro-symbolic techniques, tasks, and datasets and therefore the contribution is not enough for the publication in ICLR.Minor comments:- 3rd sentence of the introduction needs rewriting.- Section 2.2: of of ---> of- Results: 2^5 0 ---> 2^501) Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. "Order matters: Sequence to sequence for sets." arXiv preprint arXiv:1511.06391 (2015).2) Rocktäschel, Tim, and Sebastian Riedel. "Learning knowledge base inference with neural theorem provers." Proceedings of the 5th Workshop on Automated Knowledge Base Construction. 2016. This paper studies the limitations of deep neural networks to model deduction based inferences. This is done by crafting simple datasets and experimentally showing that some (details are not provided) RF, NN and RNN models fail on these.The paper is hard to follow at places. The main contribution seems to be Algorithms 1-5, which can be used to generate 10 different dataset "benchmarks". The listings of the algorithms seem quite redundant considering the simple types of datasets one wishes to generate, when this would often be achievable using mathematical formula or code "one-liner" (the algorithms are also missing information what is returned and the fonts are used inconsistently). The experimental evaluation gives no details of the trained models.I agree with the authors, that feature engineering is very relevant when it comes to using ML models and in recent years there has been some tendency to consider neural networks as simple plug-in solutions to all scenarios. However, it seems hardly surprising that the crafted benchmarks proposed here are difficult or even impossible to learn for random forest or neural networks. I might be missing something crucial here, but the paper's contribution seems not really warrant publication.Pros:Raising awareness that deep learning is not a plug-in solution for every occasionCons:Significance and novelty seem questionableQuestions:Please address and clarify the con above Minor:This structure is repeated in several places and is hard to parse, consider clarifying it:" - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisability of two numbers and divisability by 3; (d) representation (2 data sets) - binary representation and parity."Regarding the statement "However to the best of our knowledge and exploration, today there is no RNN formulation which is meant to learn facts, unification and deductive inferences.", have the authors checked the recent approaches to use deep learning to learn to solve combinatorial problems (e.g., SAT, CSPs) and using GNNs. This is a currently very active area of research that might be interesting to the authors, see e.g., https://arxiv.org/abs/1905.13211https://arxiv.org/abs/1905.12149https://arxiv.org/abs/1904.01557https://link.springer.com/chapter/10.1007/978-3-319-98334-9_38https://openreview.net/forum?id=BJxgz2R9t7https://openreview.net/forum?id=HJMC_iA5tmfor some recent examples. ### SummaryThe authors propose a rule for neural network (NN) initialization, which takes into account input data. They suppose that weights and biases of a NN are randomly drawn resp. from $\mathcal{N}(0, \sigma_w^2 / N)$ and $\mathcal{N}(0, \sigma_b^2)$, where $N$ is the number of inputs. Then, they are able to compute *explicitly* the covariance matrix of the corresponding Gaussian Process. Since an explicit result is needed, they concentrate on one-layer NNs.They use their explicit formula for the covariance matrix to compute the likelihood of the data, given $\sigma_w^2$ and $\sigma_b^2$. Therefore, they are able to select the best pair $(\sigma_w^2, \sigma_b^2)$ according to data, i.e., maximizing the likelihood.### ClarityI did not understand the experimental setup presented in Section 2.5. For instance, the train/test location are supposed to lie in the interval $[0, 1]$, but the test location points lie in $[0, 2]$ in the graphs.Besides, all the computation of the covariance should be put in appendix.### SignificanceSince the paper relies on an explicit computation in one-layer NNs, the presented method has a very low significance. At least, the authors should propose an application in deeper NNs, even by making strong approximations. As such, no clue about any generalization is provided. The paper proposes a post-hoc uncertainty tuning pipeline for Bayesian neural networks. After getting the point estimate, it adds extra dimensions to the weight matrices and hidden layers, which has no effect on the network output, with the hope that it would influence the variance of the original network weights under the Laplacian approximation. More specifically, it tunes the extra weights by optimizing another objective borrowed from the non-Bayesian robust learning literature, which encourages low uncertainty over real (extra, validation) data, and high uncertainty over manually constructed, out-of-distribution data.Using Eq (7) and (8) to quantify posterior uncertainty doesn't seem correct to me. Instead of manually building some fake OOD data and forcing their posterior variance to be large, plus forcing all real data points' posterior variance to be small, one should focus more on the different posterior uncertainty *within* the real observed data. For example, data in highly uncertain areas should naturally have higher posterior variance. But in the proposed method, they are all forced to have small variances as long as they are real data.Another confusion I have is more related to the math behind. The extra network weights and hidden dimensions are designed not to have any connection to generate the output. If so, does it mean that no matter what values the extra network weights take, their curvature to the output prediction should be entirely flat? How would these values affect the uncertainty if their Hessian values are zero? It would be beneficial if the authors could explain more about why that's not the case. The paper proposes a standardized benchmark for offline RL research. The data collection is well motivated from real world scenarios covering many important design factors. I really appreciate that human demonstration and hand-crafted controllers are also included. The evaluation protocol and the API looks clear and easy to use. The benchmark of existing methods is thorough and provides many useful insights. I believe this work will have a high impact on the offline RL community. I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research.However, I notice that this dataset may not be accessible for underrepresented groups. I therefore vote to reject. As the authors note in the paper, each task consists of a dataset for training and a simulator for evaluation. In my understanding, half of the six tasks (Maze2D, AntMaze, Gym-mujoco) depend heavily on the MuJoCo simulator, which is a commercial software and is not free even for academic use. A personal MuJoCo license costs 500 USD per year. I am concerned that MuJoCo is not accessible for most underrepresented researchers. It is not clear when MuJoCo becomes a dominating benchmark for online RL research, though there are indeed free, open-sourced alternatives, e.g. PyBullet (https://github.com/bulletphysics/bullet3). In online RL, we need the simulator for training. One reason MuJoCo becomes popular may be because it's more stable and faster than PyBullet. However, in offline RL, a simulator is used only for evaluation not for training. So the high reliability of MuJoCo may no longer be so necessary. I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares, making RL research more accessible for underrepresented groups. If accepted, this paper will indeed greatly promote the use of MuJoCo given its potential high impact, making RL more privileged.  Overall, I really enjoy reading the paper and am glad to see a standardized benchmark for offline RL. I am happy to raise my score if the accessibility issue is addressed, e.g., by using PyBullet as the physical engine. This paper studies the dynamics of the parameters while training a neural network via SGD. SGD is not studied directly but three continuous time approximations of SGD are considered: the classical gradient flow, a stochastic differential equation of Langevin type, and a "modified" gradient flow whose derivation has roots in the literature. For each dynamics, the authors show that some invariant properties of the loss function (which are often satisfied in practice) imply some invariant quantities for the dynamics.  Overall, the paper is rather clear. It tries to provide a physical meaning behind the dynamics of learning, which is an interesting question.However, I do not see any significant theoretical contribution. For instance, all derivations are simple differential calculus applications. Moreover, several models for SGD are used, each of them have their own invariant properties (which look alike) and then what? The technical contribution is not clear to me. Finally, the experimental contribution is rather mild because the numerical experiments are not discussed in the main text (except marginally in the conclusion). There is a lot of room for improvement (see below) and I don't recommend the paper for publication in this form.Specific remarks:- The first paragraph of Section 3 is not necessary in my opinion (already explained in the intro). Moreover, the notation for the group action is a bit misleading, e.g. there is confusion between \psi, \psi(\theta) and \psi(\theta,\alpha)- Eq 2: the translation invariant suddenly applies only to a subset of the parameters.- Figure 1, 2 are not commented in the main text. Same for 3,4,5 (except in the conclusion)- The authors could recall Noether's theorem for comparison- The models used for SGD are not theoretically justified, except for the classical gradient flow, for which it is standard (Kushner & Yin 2003). For Eq (11) it starts to be sloppy (CLT + Forward Euler). For Eq (13), only intuition is provided.- Last paragraph of Page 5. It seems that the discussion applies for any \xi (not necessarily Gaussian). Does it help to remove the Gaussian noise assumption?To improve the paper, I suggest the authors to - clearly locate their work within the existing literature. This would help to understand why the questions answered in this paper are important, and to highlight their contribution.- Turn their paper into an experimental one. To this end, dedicate a whole section to **commented** numerical experiments. This does not mean adding more simulations, just explain them in the main text and how they contribute to the main message of the paper.Minor:Page 2: "nornalization"Between Eq 3 and 4: Notation not definedPage 4: ReLU not defined (give the formula)Section 5.1, 2nd line. \nabla is missing Page 6: graidentPage 7 (two times): previosly  The paper proposes an approach for self-supervised time series representation learning by using inter-sample and intra-temporal relational reasoning. The paper builds upon the existing ideas on relational reasoning [3] and self-supervised learning to train models from unlabeled data.Self-supervised learning for time series is still under-explored and this paper attempts to bridge this gap in time series literature.The key novelty of the paper as claimed by the authors is in the design of inter-sample and intra-temporal tasks and loss functions to learn a feature extractor from unlabeled data. This idea of designing pretext tasks using global-sample structure and local-temporal structure and adapting it for time series is interesting. However, the inter-sample loss function seems to be a straightforward application of ideas in self-supervised learning literature that rely on various augmentations to create positive and negative pairs of samples. The intra-temporal task is defined where the distance between subsequences (referred to as time pieces in the paper) of a time series is used to create a classification task. This is a potentially novel (albeit incremental) aspect of the approach but the empirical evaluations (as detailed below) fail to highlight the impact of the same on performance clearly.Furthermore, the idea of using subsequences to define self-supervised (intra-temporal) tasks for time series has been explored earlier, e.g. in [1,2]. The authors seem to be unaware of papers like [1,2], and Introduction and Related Work sections suggest that self-supervised learning for time series has not been attempted earlier.Apart from lack of clarity on novelty and contribution of the work, I have following concerns regarding empirical evaluation: 1. The ablation studies show the effect of number of classes and the length of subsequences (pieces) on linear evaluation accuracy (Linear ACC) for the downstream classification tasks. However, the sensitivity of results to the hyperparameters of intra-temporal task is reported only on CricketX. Do the same results hold on other datasets? Also, the results on downstream task seem to be very sensitive to the choice of parameters C and L/T. The authors state that "In the experiment, to select a moderately difficult pretext task for different datasets, we set {class number (C), piece size (L/T )} as {3, 0.2} for CricketX, {4, 0.2} for UWaveGestureLibraryAll, {5, 0.35} for DodgerLoopDay, {6, 0.4} for InsectWingbeatSound, {4, 0.2} for MFPT, and {4, 0.2} for XJTU" - It is not clear how these choices for hyperparameters are arrived at, or what "we set"  and "moderately difficult pretext task" mean. Also, the ablation study is more of a sensitivity analysis of the choice of values for C and L/T. It is not clear what happens if intra-temporal relation reasoning task or the inter-sample relation reasoning task is removed from SelfTime. Does that make SelfTime same as one of the other baselines, e.g. "Relation"? Since most of the baselines used are adaptations from image domain, it is difficult to gauge where the proposed approach stands w.r.t. time-series specific methods like [1,2].2. For CricketX, DLD, and XJTU datasets, SelfTime performs significantly better than the supervised learning model (Table 2). How does one explain this observation as supervised methods would typically perform better or at least as good as the self-supervised methods?3. The authors observe that "we find that the composition from a magnitude-based transformation (e.g. scaling, magnitude warping) and a time-based transformation (e.g. time warping, window slicing) facilitates the model to learn more useful representations." As per the description, it seems that this observation is based on analysis of just one dataset (CricketX) and on one algorithm (SelfTime). I think a more thorough description and evaluation across datasets and baselines could be useful.4. In the ablation results, it would help to see results across all datasets when using only inter-sample or intra-temporal losses. Also, the results in Fig. 5 and Fig. 6 are for only one dataset, and that too different ones: CricketX for Fig. 5 and UGLA for Fig. 6. Do the observations from Fig. 5 and 6 hold across datasets? Similarly, what motivates the particular pairing of source-->target for the Domain Transfer Evaluation? Given six datasets, several other combinations are possible - does this observation hold across all such combinations?5. The authors loosely mention that "more augmentation results in better performance" on the basis of references from existing literature. Though this might hold in other domains, an evaluation of the same for this setting would be useful to claim the same in the given context. Given the above, a more thorough and consistent description of the contribution in light of recent related work and empirical evaluation is needed. Given works like [1,2,3], the contribution of the paper is limited to defining the intra-temporal task, but the same has not been evaluated carefully enough.The write-up can be improved at several places, e.g.:Eqns. 1 and 2 need a minus sign to be called as losses? Is i=j valid for the positive inter-sample relation pair? If not, Eqn. 1 might need an update.fire --> fair?$z_n$ is bold at a few places and not at otherlimited amount training samples Or for video datathere is not enough featurewe generate from its transformation counterpart and another individual sample as the positive and negative samples respectivelyIn the experiment, we achieve new state-of-the-art results, and outperforms existing methods by a significant margin on multiple real-world time series datasets for classification taska intra-temporalFirstly, takes the original time series signals and their sampled time pieces as the inputsshow that both two kinds of relation reasoningwill drop the evaluation performancet-SNE visualization of the learnt feature.References:[1] Unsupervised scalable representation learning for multivariate time series. Franceschi et. al. NeurIPS'19[2] Multi-task Self-Supervised Learning for Human Activity Detection. Saeed et. al. PACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019.[3] Self-supervised relational reasoning for representation learning. Patacchiola and Storkey. NeurIPS'20 The authors proposed a distance measure to characterize the information space among neural networks. They derive this measure by empirically compute the expected code length. They applied the distance measure to neural network models in didfferent settings and compared with some baselines.Strength:+ the information theoretic perspective of the deep learning is a useful approach to compare neural networks.+ the spectrum of the empirical study is pretty wide, covering some interesting aspects of neural network behaviors.+ the connection with generalization of neural networks is an interesting observation.Weakness:- lack of important details on the experiments and related work. Despite briefly introduced the baselines in the Appendix, the main text should be self-contained. Why are these baselines chosen is also not clear. Without a clear description of the baselines, experiments, or codes greatly hinders the reading and evaluation of the results.- limiting novelty: the concept of computing code length is not news in deep learning (Blier et al., 2018; Kim et al., 2018), and the authors are only incrementally extending the complete formulation of information transfer in Zhang et al., 2020 into the expected codelength. It is not clear the contribution, both theoretically and empirically.- why is eq 10 a valid assumption? if not, the entire basis for this measure is not solid.- missing definition of math notations: e.g. what is f_square in eq 14? the technical sections are below standard to top-tier conferences.- didn't show a clear benefit over existing methods: section 3.2 explored the invariances of the measure in different parameterization settings of neural networks. however, we see that many baselines are equally invariant if not better than the proposed measure.  even in section 3.3, we see EMD performing just as well in capturing the change in training process.- lack of comparison with other methods in Fig 4. It is likely that other similarlity measures offers interesting visualizations like this in three-dimensional space as well. Why is this method better in any way?- several acronyms used without introducing their definition first.Suggestion:This submission appears to only preliminary work to a potentially interesting approach to understand the behaviors of deep learning. The writing is a little bit hard to follow, mostly due to a lack of details in the empirical evaluations from section 3 onward. I think it would require some significant rewriting before ready for publication.The contribution of the work is also a bit confusing. It feels like the heavy lifting of the idea actually comes from the formulation of information transfer in Zhang et al., 2020. The empirical study also didn't show any clear benefit of the proposed approach over existing ones.  #########################################################################Summary:This paper uses the existing tricks that can enhance the standard training, to show that combining some of those tricks (in this paper, labels/logits smoothing and weight averaging) can improve adversarial training. #########################################################################Pros: 1 Compared with existing weight manipulation AT methods, this paper first utilizes stochastic weight averaging (SWA) (averaging multiple checkpoints along the training trajectory) without incurring computational overhead. 2 This paper conducted experiments across four different datasets.#########################################################################Cons: 1 The papers novelty is marginal. Specifically, first, label/logit smoothing has been demonstrated effective in adversarial training due to the better separation of different classes. For example, to my knowledge, three papers got accepted with the shared philosophy but slightly different techniques/decorations [1, 2, 3] Second, as the authors mentioned, manipulating model weights is also shown effective [4].Therefore, this paper's conceptual improvements are marginal.  [1] Metric Learning for Adversarial Robustness, NeurIPS 2019\[2] Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness, ICLR 2020\[3] Boosting Adversarial Training with Hypersphere Embedding, NeurIPS 2020\[4] Revisiting loss landscape for adversarial robustness, NeurIPS 20202 This paper hypothesizes one source of robust overfitting might lie in that the model overfits the attacks generated in the early stage of AT and fails to generalize or adapt to the attacks in the late stage.  It is not clear to me why this hypothesis is valid. Would you explain more justifications?3. In Figure 3, are there any experimental results for SWA SA and SWA RA? Besides, more robustness evaluations are needed, e.g., CW attack, AA attack, Guided Adversarial Margin Attack. More adversarial training on different network structures are needed, e.g., Wide ResNet.  The paper analyzes joint scalings of the parameter initialization and the learning rate, with respect to the limit of infinite width, in the context of two-layer neural networks with stochastic gradient descent and binary logistic loss. It proposes some dynamically stable conditions and identifies a range of scalings that satisfy these conditions. This covers the neural tangent kernel (NTK) and mean field (MF) scalings, as well as others. The paper then proposes to add an extra correction function to the initialization of the MF limit and argues experimentally that this correction can give a proxy for standard neural networks.Overall there are interesting points in the paper, but I think the paper needs a lot more works to make the study rigorous, complete and easier to read. More analyses into why any point in the dynamically stable model evolution band leads to stable dynamics at all training time are needed.### Positive points:The recent trend in analysis of large-width neural networks has drawn a lot of attention, so the paper is timely. The idea of joint scalings between the initialization and the learning rate, w.r.t. the width d, has been explored before; but here the paper proposes to do so in relation with a new set of criteria. The results make an interesting view on the connection between two distinct regimes NTK and MF, and beyond.### Negative points:**Presentation:**There are many unnecessary notations, for example, \tilde{q}_a and \tilde{q}_w which are equal to each other in the main analysis. It is better to keep all discussions around just the two key scaling exponents. The main figure, Figure 1, is very hard to read, and I cannot see where the curve for MF is in this plot. The statements also seem to miss several assumptions e.g. assumptions on the data (will things hold if |x| ~ exp(d)??).Some notations are not explained, for example, \sigma^{(0)}(x) that appears in Eq (13).**Incomplete proofs:**Aside from many derivations which are heuristic (which are fine as long as they are not stated as propositions/theorems), the proofs for the stated lemmas / propositions are unrigorous and may entirely omit the more difficult technical points. For example, in the proof of Lemma 1.4 in Appendix B.1, why is it that the collection of the weights \hat{w}_r^{(k), r=1&,d, at any time k>1 allows one to apply the law of large numbers? The variables in this collection are not independent; if they are mildly dependent, in what sense are they so that one can apply the law of large numbers?In fact, there are simple mathematical mistakes throughout the proofs. For example, to prove Lemma 1.2 in Appendix B.1, the paper only proves the statement for a single fixed x; why does it hold for almost every x?**Abstract missing key information:**The abstract should be more precise: it should mention logistic loss for binary classification and that the initializations are zero-mean. These are important; without them, a number of key results will fail.**Dynamical stability criteria are insufficiently justified; most are limited to just initializations:**The paper does not give sufficient justification of why the proposed criteria (Conditions 1 and 2) should be considered. In particular, Condition 2 only concerns with what happens at initialization; does it guarantee stable dynamics well beyond initialization and in what sense? The paper does not discuss this for most points in the dynamically stable model evolution band. In fact, compared to the well-studied NTK and MF scalings, the newly identified scaling sym-default is said to have some divergence behavior (in which its prediction function is always infinite, for infinite d, at any positive training time). If anything, the paper should justify why this divergence behavior does not lead to instability; by looking at Figure 1, I think it suffers from numerical instability.In fact, scalings other than NTK and MF have been studied well beyond initialization in a rigorous fashion. In particular, [1] shows that with a wide range of different scalings, its entirely possible to have the same stable behavior: NTK behavior where weights do not move and the model tracks some random-feature models. Given that such rigorous and thorough study is possible, it is unclear how the proposal of Conditions 1 and 2 leads to novel and significant insights.**Condition 2.3 is unusual in terms of physical units and not motivated:**In particular, why should we compare the magnitude of the kernel K with the magnitude of the prediction function f, while they should have very different physical interpretation? Why is this condition interesting? The second paragraph in Section 3 seems to give some arguments for Condition 2.1 and 2.2, but none for Condition 2.3.Given that the role of Condition 2.3 is unclear, if one removes this condition from the diagram of Figure 1, the band picture disappears and we are effectively left with a half plane, separated by the evolving kernels boundary. To the left of this boundary, it is the NTK behavior as expected in [1]. So unless Condition 2.3 signifies some significant and meaningful change in network behaviors, the diagram would not show new significant insights compared to the literature.**The derivations for the dynamics in Section 3 are incomplete:**In the regime of small learning rate (\hat{\eta}* goes to 0), what we should obtain is a continuous time evolution with an expectation over data, not a discrete-time one with stochastically drawn data. Evidently Eq (14) as given in the paper completely contradicts all previous published reports on the MF limit.It does not make sense to claim, after Equation (19), that the prediction function diverges from iteration k=1 onwards. Firstly, as said, the analysis has to be done with respect to small learning rate and hence the iteration k has to be determined in relation with the learning rate. Secondly, that claim contradicts Figure 1: the prediction function does not seem to diverge at iteration 1.**The initialization-corrected mean field (IC-MF) limit lacks justification:**Its aim is to correct the MF limit w.r.t. Condition 2.1, but there are very simple alternatives to do this correction. The first way is to add any fixed function, whose magnitude are independent of d in suitable sense, to the MF limit. The second way is to initialize the MF limit with non-zero mean distributions; in fact, it is known that non-zero mean initializations are more typical in MF limit.The paper argues that IC-MF limit can be good proxy for standard networks, giving only one simple experiment shown in Figure 1. There is no proof or mathematical heuristics provided. I think this IC-MF idea does not have anything to do with the theory in the previous sections, nor the binary classification problem with logistic loss. As such, it should be at least further tested on more complex experimental tasks, e.g. CIFAR-10. From a mathematical standpoint, I do not foresee a simple argument to show why it can be a good proxy for standard networks.[1] A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics, E, Ma and Wu, 2019. Summary: The submission proposes a method to learn Kronecker factors of the covariance of a matrix-variate normal distribution over neural network weights. Given a setting of the neural network weights, the Kronecker factors can be found in closed form by solving a convex optimization problem. Strengths:+ The paper provides a thorough theoretical motivation for computing a positive definite factorization of the regularization term in the empirical Bayes setup.Weaknesses:- The novelty of the method is overstated.- The method is claimed to be efficient, but each iteration requires an inner loop of solving a MAP problem (via gradient descent on the negative log likelihood with an extra regularization term), which is at least as expensive as a standard training run.- The submission lacks precise technical writing, and many technical details appear in inappropriate places, such as the introduction.- The experimental evaluation is not strong.Major comments:- The use of empirical Bayes is not novel in the context of neural networks despite the submission's claim that "Existing studies onparametric empirical Bayes methods focus on the setting where the likelihood function and the prior are assumed to have specific forms" (pg. 1). In particular, see e.g., https://papers.nips.cc/paper/6864-an-empirical-bayes-approach-to-optimizing-machine-learning-algorithms.pdf, https://arxiv.org/abs/1801.08930, https://arxiv.org/abs/1807.01613 for the use of non-conjugate likelihoods in empirical Bayes.- The cited motivation for the use of a matrix-variate normal prior over the weights of a neural network is weak. In particular, one iteration of credit assignment via backpropagation in a one-layer neural network does not adequately describe the complex interactions between parameters of a nonlinear model over the course of an optimization procedure such as the one used in line 2 of Algorithm 1. In addition, a learned prior can be used to introduce additional correlations between parameters, so it is strange to describe the learned prior as "capturing correlations" resulting from a single weight update.- The submission lacks clarity on the assumptions entailed by using the proposed methodology, namely that the Kronecker factors are assumed positive definite. For instance, the logdet function is defined for positive definite (PD) matrices, the results in the paragraph titled "Approximate Volume Minimization" hold only for PD matrices, and the InvThresholding procedure is valid for only the same class. It should additionally be noted that this assumption is *not* required for a Kronecker factorization to be defined.- The submission makes heavy use of Kronecker factorization, but neglects to cite works that use a similar factorization of the covariance matrices for neural network applications (e.g., https://arxiv.org/abs/1503.05671, https://arxiv.org/abs/1712.02390). Furthermore, the method bears a strong similarity to https://arxiv.org/abs/1506.02117 in learning a Kronecker-factored covariance structure between parameters of a neural network. Can the authors comment on the similarities and differences?- Results are reported on a simple regression task (SARCOS) and multiclass classification problems (MNIST &amp; CIFAR10) using a neural network with a single hidden layer. Moreover, "in all the experiments, the AEB algorithm is performed on the softmax layer" (pg. 7) and the justification for this in the "Ablations" section is opaque to me. Was a similar restriction used for L2 weight decay regularization? I can't interpret how thorough the evaluation is without such details. It is also not clear that the approach is extensible to more complex architectures, or that there would be a significant empirical benefit if this is done. - Figure 5 does not really exhibit interesting learned structure in the correlation matrix. Why not plot a visualization of the learned prior, rather than the weights?Minor comments:- The submission needs to be checked for English grammar and style.- abstract: "Learning deep neural networks could be understood as the combination of representation learning and learning halfspaces." This is unclear.- pg. 2: "Empirically, we show that the proposed method helps the networkconverge to better local optima that also generalize better..."  What is a "better" local optimum?- Section 3.1 describes empirical Bayes with point estimates. Please make it clearer that this methodology is not itself a contribution of paper by citing prior work.- pg. 5: "Alg. 1 terminates when a stationary point is found." What exactly is the stopping criterion?- pg. 6: The labels in Figure 2 are extremely small. Moreover, please keep the y-axis range constant.- pg. 6, Figure 2 caption: "AEB improves generalization under both minibatch settings and is most beneficial when training set is small." Do the CIFAR10 results not show the opposite effect, that the regularization is most beneficial when the training set is large?- pg. 7: "Batch Normalization suffers from large batch size in CIFAR10" weird wording- pg. 8: "One future direction is to develop a better approximate solution to optimize the two covariance matrices from the marginal log-likelihood function." This is unclear. This paper proposes a neural architecture for segmental language modelingthat enables unsupervised word discoveries. The architecture employes a two-stage architecture that a word might be a type, or a sequence of charactersof its spellings. This idea is basically similar to Nested Pitman-Yor language models (Mochihashi et al. 2009) and two-stage language models (Goldwater et al. 2011),but the authors seem not to notice these previous work.Experimental results show some improvements on naive baselines, but clearly below the state-of-the-art in unsupervised word segmentation.As noted above, the crucial drawback of this paper is that the authors arecompletely unaware of latest achievements on unsupervised word segmentationand discovery, rather than old, simplistic baselines such as Goldwater+ (2009,idea is based on Goldwater+ ACL 2006) or Berg-Kirkpatrick (2010).The idea of using characters and words is already exploited in Mochihashi+(ACL 2009) in a nonparametric Bayesian framework; it has a better F1 than thiswork by a large margin. Moreover, it is recently extended (Uchiumi+ TACL 2015) to also include latent word categories as well as segmentations to yield the state-of-the-art accuracies on F1=81.6 on PKU corpus, as compared to 73.1 in this paper. Note that they employ a prior distribution on segment lengths as a (mixture of) Poisson distributions or negative binomials whose parameters are automatically learned during inference, as compared to a post-hoc regularizationused in this paper.In a Bayesian framework, interpolations between words and characters aretheoretically derived and quite carefully learned, and regularizations areautomatically adjusted. While neural architectures have some potentials to improve over them, current heuristic architectures that have lower performance does not have any advantage over these methods, both theoretically and empicially. Composing polyphonic music is a hard computational problem. This paper views the problem as modelling a probability distribution over musical scores that is parametrized using convolutional and recurrent networks. Emphasis is given to careful evaluation, both quantitatively and qualitatively. The technical parts are quite poorly written.The introduction is quite well written and it is easy to follow. It provides a good review that is nicely balanced between older and recent literature. Unfortunately, at the technical parts, the paper starts to suffer due to sloppy notation. The cross entropy definition is missing important details. What does S exactly denote? Are you referring to a binary piano roll or some abstract vector valued process? This leaves a lot of guess work to the reader. Even the footnote makes it evident that the authors may have a different mental picture -- I would argue that a piano roll does not need two bits. Take a binary matrix: Roll(note=n, time=t) = 1 (=0) when note n is present (absent) at time t. I also think the term factorization is sometimes used freely as a synonym for representation in last paragraphs of 4 and first two paragraphs of 5 -- I find this misleading without proper definitions.The models, which are central to the message of the paper, are not described clearly. Pleasedefine function a(\cdot) in (2), (3), (4), : this maybe possibly a typesetting issue (and a is highly likely a sigmoid) but what does x_p W_hp x x_pt etc stand for? Various contractions? You have only defined the tensor as x_tpn. Even there, the proposed encoding is difficult to follow -- using different names for different ranges of the same index (n and d) seems to be avoiding important details and calling for trouble. Why not just introduce an order 4 tensor and represent everything in the product space as every note must have a duration? While the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to ICLR and hard to judge/interpret the extensive simulation results.Minor:For tensors, 'rank-3' is not correct use, please use order-3 here if you are referring to the number of dimensions of the multiway array. What is a non-linear sampling scheme? Please be more precise.The Allan-Williams citation and year is broken:Moray Allan and Christopher K. I. Williams. Harmonising Chorales by Probabilistic Inference. Advances in Neural Information Processing Systems 17, 2005. Paper uses Generative Adversarial Networks' (GAN) paradigm to achieve Steganography/Steganalysis but  fails to dig deeper into obtained results to explain suitability of GAN for Steganography/Steganalysis or the types of embedded images (textures, patterns, complexity etc.) most suitable.- Figure 3/Page 7: Providing the residual differences between covers and stego-images would be appreciated. - Define Acronyms before using them: ISS-GAN for example (Integrated Steganography-Steganalysis (ISS)).- General concepts like State of the Art should not in our humble opinion be used through acronyms (SOTA). - Page 3/Paragraph (Deep Learning based Steganography):  Clean your references:     - Paper (Volkhonskiy et al.,2017) , Paper (Dong et al., 2018), Paper (Shi et al.,2017).In need of a serious rewriting: - No need for 1. Introduction's first paragraph. Good intention for a simplistic definition scenario rendered redundant by following paragraph. - Multiple needless repetitions especially for definitions such as those of Steganography/Steganalysis (Paragraphs 4 &amp; 5, page 4). - Ill-articulated phrases:    - Introduction: - 'Steganalysis as the counterpart, is an attack to the steganography'.                               - 'Since their birth, steganography and steganalysis promote the progress of each other.' Just say complementary processes.    - Paragraph 3/Page3: 'Their model is suitable for embedding secret with the random key.'.In need of more proofing (Phrasing errors):   - Abstract: 'Its application on data generation' instead of 'Its application on data generation'.  - Introduction: 'Imagine that you are' instead of 'Imaging that you are'.  - Multiple unjustified uses of  'the':       - we propose 'the' novel framework.       - You are on 'the' trip to New Orleans This paper considers arranging the examples into mini-batches so as to accelerate the training of metric embeddings. The- The paper doesn't have sufficient experimental evidence to convince me that the proposed method is useful. There is no comparison against baselines. The paper is not clearly written or well organized. Detailed comments below:- For example, when introducing focus and context entities, it would be helpful to give examples of this to make it clearer. - In section 3, please clarify that after drawing both positive and negative examples, what is the size of the minibatch for which the gradient is calculated? - How do you choose the size of the microbatches? If the microbatch size is too small, then the effect of associating examples is small. - In the line, "Instead, we use LSH modules that are available at the start of training and are only a coarse proxy of the target similarity" Why are you not iteratively refining the LSH modules as the training progresses? Won't this lead to an improvement in the performance? - In the line "The coarse embedding can come from a weaker (and cheaper to train) model or from a partially-trained model. In our experiments we use a lower dimension SGNS model." Could you please clarify what is the additioanal computational complexity of the method? This involves additional computational cost? It doesn't seem to me that the results justify this increased computation. Please justify this. - In Lemma 3.2, the term s_i is undefined- "In early training, basic coordinated microbatches with few or no LSH refinements may be most effective. As training progresses we may need to apply more LSH refinements. Eventually, we may hit a regime where IND arrangements dominate." This explanation is vague and has no theoretical or empirical evidence supporting it. Please clarify this. - Please fix the size of the axes and the legend in all the figures. - For figure 1, how is the step-size chosen? What is the dimensionality of the examples?- From figure 3, it is not clear that the proposed methods lead to significant gains over the independently sampling the examples? Are there any savings in the wall clock time for the proposed methods? Why is there no comparison against other methods that have proposed non-uniform sampling of examples for SGD (like Zhang, 2017)? Are the hyper-parameters chosen in a principled way for these experiments? This paper proposes using structured mini-batches to speed up learning of embeddings. However, it lacks sufficient context with previous work and bench-marking to prove the speed-up. Furthermore, it is difficult to read due to its lack of revision. Sentences are wordy and do not always have sufficient detail. Argument issues and questions:- Since the main claim is a speed-up in training, the authors should support with robust experimentation. Only a synthetic and small test are conducted. - Not being an expert in this subject, it was difficult to follow some of the ideas of the paper. They were presented without clear explanation of why they supported the conclusion. For example,  I do not understand Figure 4. It seems COO and IND change places. It is not always clear how the figures support the argument. - What impact does the size of the micro-batch have on the speed-up? - How does this approach compare to other embedding approaches in terms of speed? There are no benchmarks other than IND. Formatting issues:- On page one, the sentence "We make a novel case here for the antithesis of coordinated arrangements,where corresponding associations are much more likely to be included in the same minibatch" seems contradictory. It reads that you are arguing for "the antithesis of coordinated arrangements, namely independent arrangements" when you mean "the antithesis of independent arrangements, namely coordinated arrangements."- The figures in this paper are all very small with minuscule text and legends. Only after zooming in 200% were they legible. Figure 3, 4, 5, 6, and 7 have no axis labels. It is sometimes clear from the caption what the axes are, but it is hard to follow. - Often references are cited in the text without being set off with parentheses or grammatical support. For example at the top of page three: "One-sided updates were introduced with alternating minimization Csiszar &amp; Tusnády (1984) and for our purposes they facilitate coordinated arrangements and allow more precise matching of corresponding sets of negative examples to positive ones." This interrupts the sentence making it hard to read. This paper discusses the problem of evaluating and diagnosing the representations learnt using a generative model. This is a very important and necessary problem.However, this paper lacks in terms of experimental evaluation and has some technical flaws.1. Morphological properties deals with only the "shape" properties of the image object. However, when the entire image is subject to the generative model, it learns multiple properties from the image apart from shape too - such as texture and color. Additionally, there are lot of low level pixel relations that the model learns to fit the distribution of the given images. However, here the authors have assumed that the latent space of the generative models are influenced only by the morphological properties of the image - which is wrong. Latent space features could be affected by the color or texture of the image as well.2. Extracting morphological properties of the image is straight-foward for MNIST kind of objects. However, it becomes really difficult for other datasets such as CIFAR or some real world images. Studying the properties of a generative model on such datasets is very challenging and the authors have not added a discussion around that. 3. Now assuming that my GAN model has learnt good representation in Morpho-MNIST dataset, is it guaranteed to learn good representations in other datasets as well? There is no guarantee on generalizability or extensibility of the work. This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given.This paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong.Theorem 3:"Note that $\theta^i_t$ and $\hat \theta^i_t$ are initialized with the same initial distribution µ0 = ½0 and we can also set $\theta^i_0$ to be independent of $\hat \theta^i_0$, we can have $\gamma(0) = 0$. $\gamma(0) = E \|\theta^i_0 - \hat \theta^i_0 \|^2$." - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density."Then according to the Gronwall Lemma, we have" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and  $\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case."... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption "We can tune the bandwidth of the RBF kernel to make K d H_K, which is omitted in the Assumption due to the space limit." First, there is a missing norm, since K is a vector and H_K is I believe a scalar constant. Second, c1 = H_K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement).I find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations)."Note the first bullet indicates U to be a convex function and W to be ... " I think it should be K, not W.Theorems 3-6 could be lemmas, while there should be a unifying theorem for the bound.Finally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process.Other comments:Example in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative.It is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data. This paper presents a new adversarial defense method.  I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well.  The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value.  The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all).  They use both partially converged perceptron models as well as fully converged perceptron models. Pros:1. The defense technique does not require knowledge of the attack methodCons:1. The paper is incredibly difficult to understand due to the writing.2. The performed experiments are insufficient to determine  whether or not their technique works because:a. They don't compare against any other defense techniques.  In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect.b. They do not show the results of an attack by an adversary that is aware of their technique.  (i.e. F(PR(A(F,PR,x))))  Many alternative defense techniques will work much better if we assume the adversary does not know about the technique.c. Their comparison against random noise is not an apples-to-apples comparison.  Instead of perturbing uniformly within a range, they perturb according to a normal distribution.  The random noise perturbations also have a much larger L2 distance than the perturbations from their technique.  To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method.d. They only show results for one value of epsilon and one value of "# of colors" for their technique.  Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large "# of colors"/small range per color and the attacker chooses a large epsilon).3. Their use of machine learning models is quite ad-hoc.  In particular they use a perceptron trained on algorithmically generated data.  And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model.4. The authors partly deanonymize the paper through a github link5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length. Overall Score 3This paper introduces Cross domain schemas (CDS) for semantic parsing of utterances made to a virtual assistant. CDS captures similarities in requests according to the underlying actions or attributes being discussed, regardless of the users high-level intent. Also introduced is a model which leverages CDS to improve semantic parsing of utterances to a meaning representation language (MRL). This model first parses an utterance to CDS, then uses an encoding of the CDS jointly with the utterance encoding to decode a meaning representation. By treating different intents as separate domains, the authors construct a multi-task learning setup for CDS and MRL parsing. Results are provided for the Snips dataset of virtual assistant queries. Unfortunately, this paper fails to sufficiently explain its main proposal, the CDS. The stated goal is to explicitly define the cross-domain features that would otherwise be implicitly learned by the parameters of a neural network, yet no explicit definition is given. No rough quantification of how many or what percent of features appear across domains is provided. Rather, significant time and space is given to describing a fairly unsophisticated two-decoder model for inserting the mysterious CDS representation into the final decoding task. The paper ignores standard semantic parsing datasets (GeoQuery and ATIS) due to their size and scope. However, comparable models (Goo et al. 2018) are trained and tested on ATIS. Moreover, an evaluation on a small, unseen target domain would be the perfect justification for the kind of cross-domain learning proposed here. Instead, this paper opts only to evaluate on the recent Snips dataset. This dataset seems to be best suited to evaluating intent classification and slot filling (intent-slot), but the current work fails to improve over what Goo et al. 2018 report on this data. In the current work, the Snips dataset is used to evaluate MRL parsing, where the CDS model shows improvements over other seq2seq models. However, since MRL can be parsed from intent-slot format by predefined rules, it is uncertain whether the CDS model outperforms the Goo et al. model at even the task of MRL parsing (no such comparison is provided). Overall, the paper suffers from some clarity issues especially regarding the definition and value of CDS. The model provided may be slightly original but is quite similar to the model of Dong and Lapata 2018. The significance of this work is questionable due to the poor comparison with recently released baseline models for the more common intent-slot task. ProsIntroduces Cross Domain Schemas (CDS) for semantic parsing, which help improve robustness of semantic parsers by allowing models to learn patterns in one domain for use in another. Through the use of CDS, train semantic parsers in a multi-task learning setupConsCDS is not described in sufficient detail. In particular, the possible actions and attributes are not defined. The model is described as multi-task learning, however all tasks are parsing requests made of a virtual assistant. Results on standard data for semantic parsing such as GEO or ATIS are not reported.The model does not appear to improve the results on the Snips dataset compared to the paper that introduces this dataset. Thus, the value of CDS is difficult to judge. No per-domain analysis of the impact of CDS is provided. The paper discusses a method to increase accuracy of deep-nets on multi-class classification tasks by what seems to be a reduction of multi-class to binary classification following the classical one-vs-all mechanism. I fail to see any novelty in the paper. The problem of reducing multi-class to binary classification has been studied thoroughly with many classical papers like:1. Usual one-vs-all - this paper does the same thing as one vs all in my opinion even though this technique is known for a decade or so. 2. Weighted One-Against-All - http://hunch.net/~jl/projects/reductions/woa/woa.pdfand more sophisticated techniques like:3. Alina Beygelzimer, John Langford, Pradeep D. Ravikumar. Error-Correcting Tournaments. CoRR, abs/0902.3176, 2009.4. Erin L. Allwein, Robert E. Schapire, Yoram Singer, Pack Kaelbling. Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. Journal of Machine Learning Research, 113141, 2000.5. Thomas G. Dietterich, Ghulum Bakiri. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263286, 1995.In my opinion the methods in [1,2,3,5] above can be used with any binary learners and therefore deep-networks. This paper makes no effort in comparing with any of these well-known papers. Moreover the experiments do not show any gain in state of the art performances in the data-sets used, as experiments are done with toy-networks. Further some rules for selecting the class is discussed in Section 3. There are many known rules for generating probability scores in one-vs-all classification and the relations to these are not discussed. Therefore, I fail to see any novelty in this paper (theoretical or empirical). This paper proposes a special dropout procedure for densenet. The main argument is standard dropout strategy may impede the feature-reuse in Densenet, so the authors propose a pre-dropout technique, which implements the dropout before the nonlinear activation function so that it can be feeded to later layers. Also other tricks are discussed, for example, channel-wise dropout, and probability schedule that assigns different probabilities for different layers in a heuristic way. To me this is a mediocre paper. No theoretical justification is given on why their pre-dropout structure could benefit compared to the standard dropout. Why impeding the feature-reuse in the standard dropout strategy is bad? Actually I am not quite sure if reusing the features is the true reason densenet works well in applications.Heuristic is good if enough empirical evidence is shown, but I do not think the experiment part is solid either. The authors only report results on CIFAR-10 and CIFAR-100. Those are relatively small data sets. I would expect more results on larger sets such as image net.Cifar-10 is small, and most of the networks work fairly well on it. Showing a slight improvement on CIFAR-10 (less than 1 point) does not impress me at all, especially given the way more complicated way of the dropout procedure. The result of the pre-dropout on CIFAR-100 is actually worse than the original densenet paper using standard dropout. Densenet-BC (k=24) has an error rate of 19.64, while the pre-dropout is 19.75.Also, the result is NOT the-state-of-the-art. Wide-ResNet with standard dropout has better result on both CIFAR-10 and CIFAR-100, but the authors did not mention it. The authors compare a collection of machine learning models to predict the expected rental income from an investment property. The dataset they use to train their model is fairly small (around 4K transactions). In addition to using house specific features the authors use other macro features, such as, walk score etc. Using this the authors compare a set of machine learning models and report their findings. While the work presented in the paper is informative, I feel there are a number of issues with the paper, making it unsuitable for publication in ICLR. Some of them include: - There is no new novel model or technique proposed in the paper. It is essentially a collection of experiments run on some dataset with reported findings. - The dataset used is really small. Making the generalizability of the results somewhat questionable. - Lastly, even though there is nothing technically wrong with the presented work, I feel that ICLR is not the best venue for such works. Perhaps a data science conference, such as, KDD/WSDM might be much better suited. This paper aims at tackling the lack of interpretability of deep learning models, which is especially problematic in a healthcare setting --the focus of this research paper. Specifically, the authors propose Prototype lEArning via Rule Lists (PEARL), which combines rule learning and prototype learning to achieve more accurate classification and better predictive power than either method independently and which the authors claim makes the task of interpretability simpler.  The authors present an interesting and novel architecture in PEARL. Combining the two approaches of rule lists and prototype learning. However, my main concern with the paper and with the architecture in general is the lack of clarity upfront regarding what the authors perceive as the criteria for interpretability. This seems to be one of the chief aims of the paper, however, the authors dont reach this point until Section 4 of the paper. Given that this is one of the main strengths of the paper as proposed by the authors, this needs to be given more prominence and also needs to be made more explicit what the authors mean by this. The authors define interpretability as measured by the number of rules and number of protoypes identified by a particular model, without, providing an argument, justification, or a citation of previous work which justifies these criterion. Especially since this is one of the main points of the paper, this needs to be better argued and the authors should either elaborate on this point, or restrain on making claims that these models are more interpretable.The model architecture of Section 3.1 was quite obscure both from the intuitive and implementation level. Its not clear how the different modules (prototype learning, rule lists) link together in practice, nor how these come together to create an interpretable model.Generally, the paper is quite poorly structured and there were several grammatical errors which made the paper quite hard to follow. Although the problems articulated are important, the paper did not do sufficient justice to addressing these problems. The paper sets out to address a class of pure exploration problems. Of particular interest to the authors seem to be settings where the sample complexity requirements are very stringent. A good example of such a task would be that of tuning the hyperparameters of an expensive training algorithm like BERT pre-training or ResNet.Even though the problem domain is very interesting, the paper is just not fit for publication in this form for the following reasons:1- The write-up is just horrific:  a- The problem setting doesn't mention anything about intermediate rewards, which was very confusing because the reader is given the impression that learning happens based on just the total reward at the end of the trajectory.  b- There are some definitions that just don't make sense: for instance, Equation (1) is subtracting a number from a policy. My guess is that the equation is missing some parentheses, but what's the point of formulas if they're not precise. Also, a more minor point is that the is no such thing as arginf: the whole point of inf is to deal with situations where the function doesn't have a minimum, e.g. inf_{x>0} 1/x = 0. What you mean is argmin, in which case you need some sort of compactness assumption in the policy space.  c- It's completely unclear what the point of Prop 1 is.  d- More generally, the paper could really benefit from a table of notations so the reader doesn't have to keep jumping back and forth.2- It's really unclear what the contributions of the paper are: there are tons of papers on linear bandits in the pure exploration setting, so what does this paper add? There is no sample complexity analysis in there, which would be fine if the paper was a solid experimental paper, which it is not as discussed in the next point.3- I find the experiments very unconvincing: as mentioned above, a really good motivating use-case is hyperparameter tuning for a very complex models or at least a diverse set of examples. A good example of a paper that does a satisfactory job of providing convincing experimental results is the Hyperband paper [Li et al, 2017], which render the paper valuable despite its weak theoretical results.Please get one of your colleagues to read the paper before resubmitting it. The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:1) In model section, the task definition is not clear. It is expected to see what's the question generation task studied in this paper. An example or a model overview will definitly help.2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf "Seo, Minjoon, et al. "Bidirectional attention flow for machine comprehension." arXiv preprint arXiv:1611.01603 (2016)."  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?4) In the training, how to deal the ground truth that are not in the vocab? The authors stated "using a modified heuristic described below", but no follow-ups in the paper.5) The paper is not well written and organized. Small typos: in introduction, 'and and answer span', 'and output and output sequences'. In model, 'Glorot initialization', 'Bahdanau attention', it is not the common way to cite others' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder. Quality/clarity:- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve. Starting with S and i: I guess S and i are both simply varying-length sequences in U.- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).Originality/Significance:I have certainly never seen a ML-based paper on this topic. The idea of 'learning' prior information about the heavy hitters seems original.Pros:It seems like a creative and interesting place to use machine learning. the plots in Figure 5.2 seem promising.Cons:- The formalization in Paragraph 3 of the Intro is not very formal. I guess S and i are both simply varying-length sequences in U.- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).-In describing Eqn 3 there are some weird remarks, e.g. "N is the sum of all frequencies". Do you mean that N is the total number of available frequencies? i.e. should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.- Your F and \tilde{f} are introduced as infinite series. Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.- In general, you have to introduce the notation much more carefully. Your audience should not be expected to be experts in hashing for this venue!! 'C[1,...,B]' is informal abusive notation. You should clearly state using both mathematical notation AND using sentences what each symbol means. My understanding is that that h:U-&gt;b, is a function from universe U to natural number b, where b is an element from the discrete set {1,...,B}, to be used as an index for vector C. The algorithm maintains this vector C\in N^B (ie C is a B-length vector of natural numbers). In other words, h is mapping a varying-length sequence from U to an *index* of the vector C (a.k.a: a bin). Thus C[b] denotes the b-th element/bin of C, and C[h(i)] denotes the h(i)-th element. - Still it is unclear where 'fj' comes from. You need to state in words eg "C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum."- What I don't understand is how fj is dependent on h. When you say "at the end of the stream", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?- Sorry, it's just confusing and I didn't really understand "Single Hash Function" from Sec 3.2 until I started typing this out.- The term "sketch" is used in Algorithm1, like 10, before 'sketch' is defined!!-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).******Honestly, this paper is very difficult to follow. However to sum up the idea: you want to use deep learning techniques to learn some prior on the hash-estimation problem, in the form of a heavy-hitter oracle. It seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ML venue. This work proposes to defend against adversarial examples by denoising the input image through an autoencoder (a BiGAN trained similar to InfoGAN) before classifying it with a standard CNN. The robustness of the model is evaluated on the L_infinity metric against FGSM and PGD.My main criticism is as follows:* Novelty: several defences are based on a similar principle and the contributions of this paper are unclear.* DefenseGAN is broken: the most similar work, DefenseGAN, has already been broken by Athalye et al. 2018, which is not discussed. The attacks deployed in this paper do not break DefenseGAN.* Insufficient evidence: The evaluation is minimal (only FGSM and PGD, no decision-, transfer- or score-based attacks) and insufficient to support the claims.* Gradient masking: There is at least one clear sign of gradient masking in the results (FGSM performing better than PGD).### NoveltyThe only prior work against which the paper compares is DefenseGAN. The only advantage over DefenseGAN being stated is performance (because no intermediate optimisation step is used). However, besides DefenseGAN there are several other defences that project the input onto the learned manifold of natural inputs, including (see prior work section in [1] for an up-to-date list):* Adversarial Perturbation Elimination GAN* Robust Manifold Defense* PixelDefend (autoregressive probabilistic model)* MagNets### DefenseGAN is brokenThe work most similar to this, the DefenseGAN, has already been broken. Yet, this is not discussed in the paper and none of the attacks used can break DefenseGAN. This suggests that the attacks used are too weak (see also next point).### Insufficient evidenceThe only attacks employed are two gradient-based techniques (FGSM and PGD). It is known that gradient-based techniques may suffer from gradient-masking (see also next point) and that the effectiveness of different attacks various greatly (which is why one should use many different attacks). Hence, a full evaluation of the model should include score-based and decision-based attacks.### Gradient maskingIn Figure 5 (b) the FGSM attack performs better than PGD for epsilon = 0.05 (66.4% vs 71.5%). PGD, however, should be strictly more powerful than FGSM if the gradients are ok. This is a strong indicator of gradient masking.Gradient masking is the primary reason for why 95% of all proposed defences turned out to be ineffective, and there are good reasons to believe that the same might affect this defence. The robustness evaluation has to be much more thorough and convincing before any substantiated claims about the bidirectional architecture proposed here can be derived. In addition, the difference to prior work has to be made much clearer.[1] Schott et al. Towards the first adversarially robust neural network model on MNIST Summary:This paper gives a novel adversarial defense that consists of denoising images before classification. The denoising procedure consists of passing an image through a bidirectional GAN, which the authors use to map inputs to the latent space and then back to the original input space. Novelty:The exact mechanism through which this paper operates is novel, but many similar defenses have been proposed before that involve a latent space mapping followed by a mapping back to the original space; examples include DefenseGAN and PixelDefend. Concerns:- The evaluation is not thorough enough: Only two attacks are considered (FGSM and PGD, with the former being strictly weaker than the latter)- DefenseGAN is similar in defense mechanism but the authors do not attempt to use the attacks of Athalye et al 2018 (ICML 2018) in their evaluation. We thus do not have strong lower bounds on adversarial robustness.- In Figure 5b, the attack FGSM performs better than PGD, but FGSM is the single step case of PGD. This indicates that the attacks were not tuned properly, as you should always have PGD as a stronger attacker than FGSM- The method does not perform as well as adversarial training in standard defense tasks- Several writing/clarity errors (detailed below)Smaller edits:Page 2: paragraph 2: second last line: "feed" instead of "fed"Page 2: bullet 1: under our contribution: line 3: "which are unchanged" instead of "which is unchanged"Page 3: paragraph 3: second last line: "two distribution" missing an s (plural)Page 3: Section 2.2: paragraph 2: line 2: "here are two most famous attacks" missing "the" before "two most famous"Page 4: Section 3.2: first paragraph: line 4: "the latent codes is decomposed" should be "are" instead of "is"Page 5: Paragraph 1: line 9: "E are trained" should be "E is trained"Page 5: Section 4: Paragraph 1: last line: "are those have access " should be "are those which have access" missing which/thatPage 6: Last paragraph: Line 1: "the attacker can only access to the classifier" there is no need for "to" This paper presents a new adversarial defense based on "cleaning" images using a round trip through a bidirectional gan.  Specifically, an image is cleaned by mapping it to latent space and back to image space using a bidirectional gan.  To encourage the bidirectional gan to focus on the semantic properties, and ignore the noise, the gan is trained to maximize the mutual information between z and x, similar to the info gan.Pros:1. The paper presents a novel (as far as I am aware) way to defend against adversarial attacks by cleaning images using a round trip in a bidirectional ganCons:1. The method performs significantly worse than existing techniques, specifically adversarial training.a. The authors argue "Although better than FBGAN, adversarial training has its limitation: if the attack method is harder than the one used in training(PGD is harder than FGSM), or the perturbation is larger, then the defense may totally fail. FBGAN is effective and consistent for any given classifier, regardless of the attack method or perturbation."b. I do not buy their argument, however, because one can simply apply the strongest defense (PGD 0.3 in their results) and this outperforms their method in *all* attack scenarios.  And if someone comes out with a new stronger attack there's no guarantee their method will be strong defense against that method2. The paper is not written that well.  Even though the technique itself is very simple, I was unable to understand it from the introduction, and didn't really understand what they were doing until I reached the 4th page of the paper. Missing citation:PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples  (ICLR 2018) This paper describes an approach for automated curriculum learning in a deep learning classification setup. The main idea is to weigh data points according to the current value of the loss on these data points. A naive approach would prevent learning from data points that are hard to classify given parameters of the current mode, and so the authors propose to use an additional loss term for these hard data points, which encourages the hidden representation of these data points to be closer to representation of points that are close in the hidden space and yet are easier to classify (in the sense that the loss of easy samples is lower by some threshold value then the loss of hard samples). This last part is implemented by caching hidden representations and classification loss values during training and fetching nearest neighbours in the feature space whenever a hard data point is encountered. The final loss takes the form of a linear combination of the classification loss and the representation loss.The idea is interesting in the sense that it tries to use information about how difficult classification of a given data point is to improve learning. The proposed representation loss can lead to forming tight cluster of similar data point in the feature space and can make classification easier. It is related to student-teacher networks, where a student is trained to imitate the teacher in generated similar feature representations.The authors justify the method by introducing the notion of inverse internal covariate shift. However, it is not defined formally, nor is it supported empirically, and is based on the (often criticized [1]) notion of internal covariate shift. For this reason, it is hard to accept the presented argumentation in its current state.Moreover, there seems to be a mistake in equation (2) in §4.2. The equation defines the method of computing loss weighting for a given datapoint. The authors note that it converges to the value of one with increasing training iterations, but for correctness it should be \in [0, 1]. If it is &gt; 1, one of the losses in equation (3) is negated and is therefore maximised (instead of being minimised), which can lead to unexpected behaviour. Current parameterization allows it to be \in [0, + infinity].Experimental evaluation consists of quantitative evaluation of random sampling (usual SGD) and the proposed approach in training a classification model on MNSIT, CIFAR-10 and CIFAR-100. The proposed approach outperforms random sampling. This is encouraging, but the method should be compared to state of the art in curriculum learning in order to gauge how useful this approach is.The paper is poorly written, with many grammatical (lack of s at the end of verbs used in singular 3rd person, many places in the paper) and spelling mistakes (e.g. §3.2¶6 tough instead of through, I think). Some descriptions are unclear (e.g. §4.2¶2), while some parts of the paper seem to be irrelevant to the problem at hand (§3.1 describes training on a single minibatch for multiple iterations as if it were a separate task and motivates random sampling, which is just SGD).To summarize, the paper presents a very interesting idea. In its current state it is hard to read, however. It also contains a number of unsupported claims and can be misleading. It could also benefit from a more extensive evaluation. With this in mind, I suggest rejecting this paper.[1] Rahimi, A (2017). Test of Time Award Talk, NIPS. This paper reads like the first thoughts and experiments of a physicist or mathematician who has decided to look at word representations and hyponymy. I mean that in both the positive and negative ways that this remark could be construed. On the positive side, it provides an interesting read, with a fresh perspective, willing to poke holes in rather than accepting setups that several previous researchers have used.  On the negative side, though, this paper can have an undue, work-from-scratch mathiness that doesn't really contribute insight or understanding, and the current state of the work is too preliminary. I think another researcher interested in this area could benefit from reading this paper and hearing the perspective it presents. Nevertheless, there just isn't sufficient in the way of strong, non-trivial results in the current paper to justify conference acceptance. Quality: - Pro   o Everything is presented in a precise formalized fashion. The paper has interesting remarks and perspectives. I appreciate that the authors not only did find most existing work on modeling hyponymy but provide a detailed and quite insightful discussion of it.  (A related paper from overlapping authors to papers you do cite that maybe should have been included is Chang et al. https://arxiv.org/abs/1710.00880  which is a bit different in trying to learn hyponyms from text not WordNet, but still clearly related.)  -Con   o There just isn't enough here in the way of theoretical or experimental results. In the end, two "methods" of hyponymy modeling are presented: one is a simple logistic regression, which is estimated separately PER WORD for words with 10 or more hyponyms. This performs worse than the methods of several recent papers that the author cites. The other is a construction that shows that any tree can be embedded by representing nodes as ranges of the real line. This is true, but trivial. Why don't ML/NLP researchers do this? It's because they want a representation that doesn't only represent the ISA hierarchy but also other aspects of word meaning such as meaning similarity and dimensions of relatedness. Furthermore, in general they would like to learn these representations from data rather than hand-constructing it from an existing source like WordNet. For instance, simply doing that gives no clear way to add other words not in wordnet into the taxonomy. This representation mapping doesn't really give any clear advantage beyond just looking up hyponymy relationships in wordnet when you need them.Clarity: - Pro   o The paper is in most respects clearly written and enjoyable to read. - Con   o The mathematical style and precision has it's uses, but sometime it just seemed to make things harder to follow. Referring to things throughout as "Property k"  even though some of those properties were given names when first introduced  left me repeatedly flicking up and down through the PDF to refresh myself on what claim was being referred to without any apparent need....Originality: - Pro   o There is certainly originality of perspective. The authors make some cogent observations on how other prior work has been naive about adopted assumptions and as to what it has achieved (e.g., in the discussion at the start of section 5.1). - Con   o There is not really significant originality of method. The logistic regression model is nothing but straightforward. (It is also highly problematic in learning a separate model for each word with a bunch of hyponyms. This both doesn't give a model that would generalize to novel words or ones with few hyponyms.) Mapping a tree to an interval is fairly trivial, and besides this is just a mapping of representations, it isn't learning a good representation as ML people (or ICLR people) would like. The idea that you can improve recall by using a co-product (disjunction) of intervals is cute, though, I admit. Nice.Significance  - Con   o I think this work would clearly need more development, and more cognizance of the goals of generalizable representation learning before it would make a significant contribution to the literature. Other: - p.1: Saying about WordNet etc., "these resources have a fundamentally symbolic representation, which can not be readily used as input to neural NLP models" seems misplaced when there is now a lot of work on producing neural graph embeddings (including node2vec, skip-graphs, deepwalk, etc.). Fundamentally, it is just a bad argument: It is no different to saying that words have a fundamentally symbolic representation which cannot be readily used as input to neural NLP models, but the premise of the whole paper is already that we know how to do that and it isn't hard through the use of word embeddings. - p.2: The idea of words and phrases living in subset (and disjointness etc.) relationships according to denotation is the central idea of Natural Logic approaches, and these might be cited here. There are various works, some more philosophical. A good place to start might be: https://nlp.stanford.edu/pubs/natlog-iwcs09.pdf - p.2: The notions of Property 1 and 2 are just "precision" and "recall", terms the paper also uses. Do we gain from introducing the names "Property 1" and "Property 2" for them? I also felt that I wouldn't have lost anything if Property 3 was just the idea that hyponymy is represented as vector subspace inclusion. - p.2: fn.2: True, but it seems fair to more note that cosine similarity is very standard as a word/document similarity measure, not for modeling hyponymy, for this reason. - p.4: Below the equation, shouldn't it be Q(w', w) [not both w'] and then Q(w', w) and not the reverse? If not, I'm misunderstanding. * Summary of the paperThis paper studies how hyponymy between words can be mapped to feature representations. To this end, it lists out properties of such mappings and studies two methods from the perspective of how they address these properties.* ReviewThe goal of this paper: namely, formalizing the hypernymy relation over vector spaces is not only an interesting one, but also an important one -- being able to do so can help us understand and improve vector representations of words and reason about their quality.In its execution, however, the paper does not seem ready for publication at this point. Two major issues stand out.First, several things are unclear about the paper. Here's a partial list:1. Property 3 is presented as a given. But why is this property necessary or sufficient for defining hyponymy?2. It is not clear why the measure based definition is introduced. Furthermore, the expression above the statement of property 4 is stated as following from the definition. It may be worth stating why.3. Section 3.1 is entirely unclear. The plots in Fig 2 are empty. And in the definition of Q on page 4, the predicate that defines the set states P(w, f(x)). But if the range of P is [0, 1], what does it mean as a predicate? Does this mean we restrict it to cases where P(w, f(x)) = 1?Second, there are methodological concerns about the experiments.1. In essence, section 3 proposes to create a word-specific linear classifier that decides whether a new vector is a hypernym or not. But this classifier faces huge class imbalance issues, which suggests that simply training a classifier as described can not work (as the authors discovered). So it is not clear what we learn from this section? Especially because the paper says at the just before section 3.1 that "we are ultimately not interested in property 5".2. Perhaps most importantly, the method in section 4 basically represents a pruned version of WordNet as a collection of intervals. It is not surprising that this gets high recall because the data is explicitly stored in the form of intervals. Unfortunately, however, this means that there is no generalization and the proposed representation for hyponymy essentially remembers WordNet. If we are allowed to do that, then why not just define f(w) to be an indicator for w and P(w, f(w')) to be an indicator for whether the word w is a hyponym of w'. This would give us perfect precision and recall, at the cost of no generalization.* Minor points   1. The properties 1 and 2 are essentially saying that the precision and recall respectively are alpha. Is this correct?2. Should we interpret P as a probability? The paper doesn't explicitly say so, but why not?3. The paper is written in a somewhat informal style. Some examples:   - Before introducing property 3, the paper says that it is a "satisfying way". Why/for whom?   - The part about not admitting defeat (just above section 3.1)   While these are not bad by themselves, the style tends to be distracting from the point of the paper.* Missing referenceSee: Faruqui, Manaal, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. "Retrofitting Word Vectors to Semantic Lexicons." In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1606-1615. 2015.This paper and its followup work discusses relationships between word embeddings and relations defined by semantic lexicons, including hyponymy. This paper explores the notion of hyponymy in word vector representations. It tests the capacity of a logistic regression classifier to distinguish words that are and are not hyponyms using fastText embeddings, and it also describes a method of organizing WordNet relations into a tree structure and defining hyponymy based on this structure.The problem of capturing hyponymy relations within vector space models of word representation is an interesting and important one, but it is not clear to me that this paper has made a substantive contribution to it. The paper seems simply to 1) observe that fastText embeddings are imperfect for hyponymy detection with a linear classifier, and 2) reconstruct the fairly natural interpretation of WordNet relations as a hierarchical tree structure, and to re-extract hyponymy relations from that tree structure. As far as I can tell, the paper's supervised model does not use embeddings  (or learning) at all.Assessing the paper's contribution is made more difficult by an overall lack of clarity. The details of the experiments are not laid out with sufficient explicitness, and the reporting of results is also fairly confusing (I am not clear, for example, on what is depicted in Figure 2). The paper is not organized in a particularly intuitive way, nor has it made clear what the contributions might be. Overall, while I think that this is a worthy topic, I do not think that the contribution or the clarity of this paper are currently sufficient for publication. Additional comments:-The PCA plot is too dense to be a useful visual - it would be more useful to plot a smaller number of relevant points.-Results should be presented more clearly in table form - there seem to be a large number of results that are not reported in any table (for instance, the results described in Section 3). Summary: The paper proposes methods for identifying prototypes. Unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means. The experiments are not very convincing, and often present results that are either inconclusive or negative, i.e. seem to demonstrate that prototypes are not very useful. Pros:- The notion of prototypes is used in various papers, but a formal definition is lacking, and the usefulness of prototypes is not demonstrated. The fact that this paper sets out to do both is laudable, although the paper needs work before it can be accepted for publications.Detailed comments / cons:*Defining prototypes:   - The authors list desirable properties before defining (even informally) what a prototype is, and what its purposes are. Taking the first property as an example, is it reasonable to expect a metric for prototypes to be useful for image classification AND image generation? The answer completely depends on what one expects from a prototype, what its purpose is, etc.  - The second property seems to indicate that prototypes are model-independent, i.e. two models trained on the same dataset will have the same prototypes. This is confusing as the metrics proposed are clearly model-dependent (e.g. adv completely depends on the trained model's decision boundary, conf obviously depends on the model providing the confidence score)- The third and fourth property are poorly defined. Human intuition presupposes that humans agree on what a prototype means. Using 'modes of prototypical examples' in trying to define a metric for prototypes is circular, as a mode of prototypical example depends on a working notion of prototypical examples.- The last property is completely dependent on which models are trained, and how they are trained. If a model has high label complexity, maybe it does not achieve high accuracy even when trained on high quality prototypes. In any case, this property is at odds with the first two properties.In sum: it's not clear what prototypes are, so it becomes hard to judge if the list of desiderata is reasonable. The list is in any case ill-defined, and contains contradictions.* Metrics for prototypicality- The second paragraph in this section is unnecessary- All of the metrics proposed are heuristics with little to no justification. Specific comments below.- Adversarial robustness is a property of a trained model, not of prototypical examples, unless prototypes are supposed to be model dependent (contra property 1). In any case, it is not clear why examples that are robust to adversarial noise are good 'prototypes'.  Using facial recognition as an example, a 'mean face' may be very robust to adversarial noise but not prototypical at all under common definitions. A face with a particular type of facial hair (e.g. nose hair) may be very representative of a class of faces (i.e. a prototype), but very susceptible to adversarial noise. In fact, any examples in the boundary of the decision function will be more susceptible to adversaries, but that does not make them 'less prototypical'.- Holdout retraining is again completely model dependent. Why should we expect a model to treat a prototype the same regardless of whether or not it is trained on it? This basically means that we expect the model to always be accurate on prototypes.- Ensemble agreement proposes a notion of prototypes that is based on prediction 'hardness'. It is clear that such a notion depends completely on which models are being considered, which features are being used, and etc, much more than on notions of prototypicality inherent in the data. The same criticism applies to model confidence.- Privacy preserving training assumes prototypicality has to do with the model being able to learn with some robustness to noise (related to Adversarial Robustness, but different). This assumes a definition of prototypes that is not congruent with the other metrics.In sum: the proposed metrics are basically heuristics with little justification, and different metrics assume different notions of what a prototype is.* Evaluation- Section 3.1 claims that the metrics are strongly correlated, but that is not true for MNIST or CIFAR, and is somewhat true for fashion-mnist. In any case, since the metrics are so model-dependent, it is not clear if these results would hold if other models were used.- Section 3.2 - The question asked of turkers in the study is too vague, and borderline irrelevant for the task at hand - what does the 'best image' of an airplane mean, and how does this translate to it being a prototype? All that the study demonstrates is that the proposed metrics score malformed images with low score. The results in Table 1 are very spread out, and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users.- The results in Section 4 are very discouraging: sometimes it is better to train on most prototypical examples according to the metrics, sometimes it is worse, sometimes it's better to take examples in the middle. That is, prototypes don't seem to help at all. 'Prototype percentile' is uncorrelated with robustness for MNIST in Appendix E, while being correlated for other datasets. It is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected). As a whole, the results do not provide any help for a user who wants to produce a more robust model, other than 'ignore prototypes and use the whole dataset'. The paper proposes a setting for evaluation of a text infilling task, where a system needs to fill in the blanks in a provided incomplete sentences. The authors select sentences from three different sources, Yahoo Reviews, fairy tales, and NBA scripts, and blank out words with varying strategies, ranging from taking out prepositions and articles to removing all but two anchor words from a sentence. On this data, they compare the performances of a GAN model, Recurrent Seq2seq model with attention, and Transformer model in terms of BLEU, perplexity, and human evaluation.The setting is certainly interesting, and the various data creation strategies are reasonable, but the paper suffers from two main flaws. First, the size of the data set is far from sufficient. Unless the authors are trying to show that the transformer is more data-efficient (which is doubtful), the dataset needs to be much larger than the 1M token it appears to be now. The size of the vocabularies is also far from being representative of any real world setting.More important however is the fact that the authors fail to describe there baseline systems in any details. What are the discriminator and generator used in the GAN? What kind of RNN is used in Seq2seq? What size? Why not use a transformer seq2seq? How exactly is the data fed in / how does the model know which blank it's generating? It would be absolutely impossible for anyone to reproduce the results presented in the paper.There are some other problems with the presentation, including the fact that contrary to what is suggested in the introduction, the model seems to have access to the ground truth size of the blank (since positional encodings are given), making it all but useless in a real world application setting, but it is really difficult to evaluate the proposed task and the authors' conclusions without a much more detailed description of the experimental setting. This paper proposes to regularize the training of graph convolutional neural networks by adding a reconstruction loss to the supervised loss. Results are reported on citation benchmarks and compared for increasing number of labeled data.The presentation of the paper could be significantly improved. Details of the proposed model are missing and the effects of the proposed regularization w.r.t. other regularizations are not analyzed.My main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.Details / references of the transposed convolution operation are missing (see e.g. https://ieeexplore.ieee.org/document/7742951). It is not clear what the role of the transposed convolution is in that case. It seems that the encoder does not change the nodes nor the edges of the graph, only the features, and the filters of the transposed convolution are learnt. If the operation is analogous to the transposed convolution on images, then given that the number of nodes in the graph does not change in the encoder layers (no graph coarsening operations are applied), then learning an additional convolution should be analogous (see e.g. https://arxiv.org/pdf/1603.07285.pdf Figure 4.3.). Could the authors comment on that?Details on the pooling operation performed after the transposed convolution are missing (see e.g. https://arxiv.org/pdf/1805.00165.pdf, https://arxiv.org/pdf/1606.09375.pdf). Does the pooling operation coarsen the graph? if so, how is it then upsampled to match the input graph?Figure X in section 2.1. does no exist.Supervised loss in section 2.2.1 seems to disregard the sum over the nodes which have labels.\hat A is not defined when it is introduced (in section 2.2.2), it appears later in section 2.3.Section 2.2.2 suggests that additional regularization (such as L2) is still required (note that the introduction outlines the proposed loss as a combination of reconstruction loss and supervised loss). An ablation study using either one of both regularizers should be performed to better understand their impact. Note that hyper-parameters chosen give higher weight to L2 regularizer.Section 3 introduces a bunch of definitions to presumably compare GCN against SRGCN, but those measures of influence are not reported for any model.Experimental validation raises some concerns. It is not clear whether standard splits for the reported datasets are used. It is not clear whether hyper-parameter tuning has been performed for baselines. Authors state "the parameters of GCN and GAT and SRGCN are the same following (Kipf et al; Velickovic et al.)". Note that SRGCN probably has additional parameters, due to the decoder stacked on top of the GCN. Reporting the number of parameters that each model has would provide more insights. Results are not reported following standards of running the models N times and providing mean and std. Moreover, there are no results using the full training set. This paper presents a group-theoretic approach to compositionality, and attempts to provide necessary and sufficient conditions for the possibility of compositional representations. While this is an admirable goal, I am concerned that the paper does not live up to it. I recommend rejection at present, unless the definitions and presentation are substantially clarified.* The authors have assumed a particular definition of "compositional" for "compositional representations," but they have neither clearly stated a definition for compositional representations nor justified it. Section 4.2. does not clearly state a definition of compositionality.    * The authors cite Bengio (2013) as a reference when they introduce the idea of learning compositional representations, but this term is not defined there. In their mathematical definitions, they appear to assume compositional = componential.    * However, what it means for a representation to be compositional is actually quite difficult to assess, some of the papers that the authors cite (e.g. Andreas, 2019) discuss this. See also Zadrozny (1992).     * The authors definition of compositionality seems to my view to be closer to a notion of disentanglement. Indeed, their definition of compositionality itself seems to closely reflect the disentanglement definition given in Higgins et al. (2018). The authors note that representations could still be compositional even if the features are not statistically independent, which is valid, but does not contradict the point I am making here.    * My point is that, from Montague, we know we can define compositionality in terms of any composition operator in representation space. The authors do not justify that *any* composition operator could be specified in terms of their definition of compositionality. In fact, it seems clear that assuming a product of a finite number of subgroups could *not* capture the original linguistic goals of compositionality, which was *precisely* to allow *infinite recombination of elements to arbitrary depth*. For example, the grammar of arithmetic symbols allows arbitrarily long sequences of operations. How could this structure be experessed in this framework?     * It appears to me tha this article is therefore not about compositionality per se, but rather about disentanglement of a finite number of subgroups underlying the data. This is also a subject worth investigating, but the paper framing should reflect this, and the relation to prior work should clarify more fully how it differs from prior group-theoretic definitions of disentanglement (e.g. Higgins et al., 2018).* Other definitions are not clearly expressed either. For example, what are the "original" representations that the authors refer to, where do they come from? This entails certain assumptions about e.g. the mapping from possibly noisy data into the representational spaces that are essential to understanding whether these ideas have any meaning in practice.* In general, the authors focus too much on formal notation, without developing intuitions for what results mean and why they would be useful. The authors present a page of group theory definitions, but this seems almost certain to be unhelpful. Any reader who is not familiar with group theory will not be able to develop sufficient intuitions to understand the rest of the paper from reading these terse definitions without examples, and to those of us familiar with group and category theory this is space that could be much better spent on clarifying and elaborating the exposition of the contributions.In summary, I think that while this work could present a useful contribution to understanding disentanglement, it would need substantial rethinking of the definitions, more connection with the prior work, and clarification of the writing and exposition for me to see it as ready to present.References---------Zadrozny, Wlodek. "On compositional semantics." COLING 1992 Volume 1: The 15th International Conference on Computational Linguistics. 1992.(Other references cited above are referenced in the article.) ** Summary ** The paper attempts to formally explore the necessary and sufficient conditions for compositional representations, leveraging the formal tools from group theory. While the ideas look potentially promising, the presentation is fundamentally flawed, with certain key notions left without formal definitions. As a consequence, it becomes impossible to estimate the theoretical significance of the contribution or use the obtained results in practice.** Strengths **The paper addresses a highly relevant and important problem. In general, it would be extremely helpful for a broad range of ML and AI researchers and practitioners if we were to formally describe the conditions in which compositional representations can be obtained.** Weaknesses **Unfortunately, the proposed approach is not described clearly enough for it to be widely useful.In general, I believe that when formal tools (like group theory) are applied to prove anything outside of their original domain (i.e. when we are using group theory to reason about compositional representations in machine learning), it is crucial to 1) clearly define all involved notions (not only mathematical, but also the ones to which mathematical tools are applied) 2) clearly motivate the application.** Clarity **Unfortunately, the clarity of the contribution is not up to the standards of ICLR conference. In general, I believe that clarity concerns are secondary to other evaluation components (experimental support, novelty, etc.). In this case, however, it becomes impossible for me to evaluate other components because I can not fully understand the approach from its description.For example, while the paper is focused on compositional representations, the actual description/definition of what exactly authors mean by compositional representations comes only on the 4th page (after some formal results were already stated).The description is as follows: "Compositionality arises when we compare different samples, where some components are the same but others are not. This means compositionality is related to the changes between samples. These changes can be regarded as mappings, and since the changes are invertible, the mappings are bijective. To study compositionality we consider a set of all bijections from a set of possible representation values to the set itself, and construct a group with the following Proposition 4.1.". At the same time, there was no formal definition of "representation" before that paragraph. In the very next paragraph, however, the authors say "We consider two representations and corresponding sets. X is original entangled representation, and Y is compositional representation".The concerns I described above are related to the overall structure of the contribution. A separate and also a major concern is that the writing itself should be improved too. There are numerous confusingly phrased sentences which make reading difficult.For example, we can take a look at the very first sentence in the abstract: "Humans naturally use compositional representations for flexible recognition andexpression, but current machine learning lacks such ability". It's not clear what is meant by "recognition and expression", it also seems unnatural to say that "machine learning" lacks a certain ability,because machine learning is a field of study. It may be better to rephrase it to "machine learning methods". While these concerns are minor, they are ubiquitous thoughout the paper, which substantially hinders readability.** Suggestions **The direction may be promising, but unfortunately, the paper needs a thorough reorganization in order to be publishable.I would like to suggest moving the standard group theory definitions from the main text into appendix. Some of the proofs could be moved there too. The space obtained this way may be used to 1) formally define a) what a "representation" is b) what a "compositional representation" is c) the general problem setting 2) motivate the chosen definitions with some potential applications. I realize that the examples in the end of the article are intended to serve that goal, but in my opinion, neither of them is explored in enough detail.I understand that a lot of work went into this article, and I hope that the authors won't feel discouraged by the feedback, but use it as an opportunity to improve the paper. # Overall reviewThis paper applies concepts from group theory to help find necessary and sufficient representations on the presence of compositionality in representations and on mappings between them.  This topic is very important, as people tend to use "compositional" in many ways, often not explicitly defined.  The paper, however, is hard to follow, because the main concepts and theorems are not adequately illustrated with examples.  In the final section, when examples are provided, it's still not clear how they apply to representation learning, the topic of this conference.  Thus, while the topic is very relevant and the approach welcome, it is hard to know what lesson we have learned from the theorems in the paper.Pros:* Seeks to clarify and precisify the definitions of compositional representations, and find necessary and sufficient conditions on their existence.* Contributes theoretical results on compositionality, a timely topic.Cons:* The text is hard to follow without motivating examples of the definitions and theorems.* The applications in the discussion section are somewhat opaque.  The application to attention doesn't appear to make any explicit reference to the attention mechanism at all.* While theoretical understanding is extremely valuable, it's unclear how to use the results to detect or promote compositionality as usually understood.# Minor comments* "In language, a sentence is a combination of grammar and lexicon."  It might be more informative here to say that the meaning of a whole sentence is composed from the meanings of the parts and the grammatical structure of the sentence.* At the end of the introduction, the statements of Propositions 1.1 and 1.2 are hard to understand without later background from the paper.  Is it possible to provide a more intuitive statement (even if not fully precise) for these?* "We will provide examples and look into more details in discussion section."  For this reader at least, it would have helped to have examples more thoroughly laid out in Sections 3 and 4.  For example: what structure of groups is being assumed for X and Y in 4.1-4.2?  Y is assumed to be compositional representations, but we haven't been given a clear definition of that concept or an exact example.# Typographic comments:* I think e in definitions 3.8 and 3.9 refers to the identity element of the group, but this should be stated explicitly (e.g. in the definition of a group).* Defn 3.12: I'd use X instead of A here for consistency. The paper sets out to improve the evaluation of GAN models as e.g. the previously used BLEU score is not sensitive to semantic deterioration of generated texts. The paper claims to propose alternative metrics that bettercapture the quality and diversity of the generated samples.Strengths:-The paper has a valuable goal-Some of the evaluations are interesting.Weaknesses:1.The claim of the paper to propose alternative metrics that better capture the quality and diversity of the generated samples is not met in multiple ways:a.The paper seems not to propose any new metrics but evaluate existing ones.b.The metrics are not extensively compared to human judgments, e.g. by computing correlation. In fact, Figure 5 suggests that they are not very well correlated.c.The diversity is not explicitly studied on generated text samples.2.The paper concludes that the human eval assigns better scores to the Language Model, which is incorrect as Seq gan scores 3.49 vs. 3.37 for language model (even if the seq gan has higher variance).3.The metrics are not very well defined, e.g. with formulas, although this is one of the central points of the paper. e.g. what are the reference the blue score is computed against?  SummaryThe method extends [21], which proposes an unordered set prediction model for multi-class classification. For that problem, [21] can assume logistic outputs for all distinct classes. This work extends set prediction to the object detection task, where box identity is not distinct  this is handled by an additional model output that reasons about the most likely object permutations. The permutation predictions are used during training, but are not needed at inference time  as shown in Fig1 and Eq 7. Results are on detection of overlapping objects and a CAPTCHA toy summation example. Clarity The exposition is not particularly clear in several places:  - U^m in Eq 1 is undefined and un-discussed. What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.  - The term p(w) disappears on the left hand side of Eq 2.  - Notation in Sec. 3.2 is very cumbersome, making it hard to follow. Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5. Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3). If so, why does term f2 in Eq 5 contain the permutation head output O2 and how do the two relate?  - The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce. The dimensions of the convolutional feature map matter (probably need to be kept tractable).  SignificanceKey aspects of the model are not particularly clear, specifically about how the permutation prediction ( the key novelty here) is used to benefit training.  Term f2 in Eq5 uses w~ estimates, which appeared to be based on statistics from past SGD runs, yet also depends on the output of the permutation head O2. Am I misinterpreting the method? In the paragraph right after Eq5, its claimed that Empirically, in our applications, we found out that estimation of the permutations from just f1 [in Eq5] is sufficient to train properly & by using the Hungarian algorithm. So then f2 term is not even used in. Eq5? If so, what is the significance of the permutation head other than adding an auxiliary loss? Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above  if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?  While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm-set prediction handles them well is interesting and promising. Solving the general case with larger images and many instances would increase the impact significantly  and likely require a combination of perm-set prediction and image tiling, although this is just a hypothesis. The Captcha toy example also shows some interesting behavior emerging  without digit-specific annotations (otherwise it would be multi-class classification setup from [21]), the model can handle the majority of summations correctly.  Experimental resultsThe results are interesting proofs-of-concept but a few more experiments/answers would be helpful:- It still appears that PR curve in the high-precision regime (fig 3b) has lower precision than FRCNN/YOLO. Any idea as to why? - Ablation results on the effect of the permutation predictions vs Hungarian algorithm, etc would be helpful, as discussed above. - How sensitive is the method to seeing a certain cardinality? What if it never sees 3 pedestrians in an image, but only 1,2,4 will it fail to predict 3? Or alternatively, if we train a model that can handle up to 5-6 entities with examples than have &lt;=4? What is the right way of data augmentation for this model (was there any and should there be?)- Given that values for U differ across applications, how sensitive is the output / how much sweeping did you have to do?(-- Related workTo the best of my knowledge it's representative. It would help to cite more recent work that decreases detector dependence on NMS. For example, "Learning Non-Maximum Suppression", Hosang, Benenson, Schiele, CVPR 2017 or "Relation Networks for Object Detection", by Hu et al, CVPR 2018 and references therein. This paper looks to predict "unstructured" set output data. It extends Rezatofighi et al 2018 by modeling a latent permutation.Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers. 1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me. The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others. The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense. Conditioned on the permutation of the set, the points are exchangeable. Let's just consider a 2 element "set" at the moment Y = (y_1, y_2). Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word. we have:p_\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way. Essentially the paper has just written a mixture model for the output points where there are as many components as permutations. I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues. It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3. This needs to be justified.There are some stylistic shortcomings as well. For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs). Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1. But these and other points are minor.The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above. The paper adresses the problem of incremental learning when data from new classes are available as a stream and one wants to be able to update to learn new observed classes without forgetting the older ones. There is a budget issue here and one does not want to just keep the whole training set of all known previously observed classes but rather one wants to consider a maximum memory budget allowed to store what is necessary for an optimal incremental learning (typical examples, statistics etc). There is also a privacy issue preventing from storing original training samples.This is a relevant problem that is has gain interest in the last few years. It is related to topics such as few shot learning and meta few shot learning (with respect ti the number of examples per class that are kept, which is limited) and somehow to budget learning . Yet these topics and associated references are surprisingly not evoked in the text.The paper is rather well written but it strongly lacks precision about the proposed method. A description of the ICARL state of the art method is missing and would have been mandatory since the proposed work appears to build on iCARL method. Actually the description of the method is very short since the dedicated section (§4) is mainly used to describe a rather standard convolutional auto encoder architecture. At the end one tries to guess what the proposed method consists in. As far as i understand it is based on iCARL method where selected examples of past observed classes are not stored as is but in their encoded form (by the convolutional autoencoder). At the end my understanding of the proposed approach is that it consists in an incremental progress of a state of the art method, then an incremental work with limited innovation.By the way i am not sure of the meaning of pseudo exemplar as used in the proposed method. Are these drawn following a distribution computed on training samples ? Or are these pseudo exemplar because you use reconstructed samples from encodings (by the CAE).  When looking at experimental results the proposed method seem to bring some benefit but it does not look fully convincing. As written in the paper the proposed system outperforms iCARL in case the examples are encoded in the same dimension as original examples (hence no benefit on the storage side) but reaches similar performance when using less storage capacity. The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods? The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.The introduction is vague and involves undefined terms such as "useful habits". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines. General:In general, this looks like a technical report rather than a research paper to me. Most parts of the paper are about the empirical analysis of adaptive algorithms and hyper-gradient methods. The contribution of the paper itself is not sufficient to be accepted.Possible Improvements:1. The study of such optimization problem should consider incorporating mathematics analysis with necessary proof. e.g. show the convergence rate under specific constraints. Even the paper is based on others' work, the author(s) could have extended their work by giving stronger theory analysis or experiment results.2. Since this is an experimental-based paper, besides CIFAR10 and MNIST data sets, the result would be more convincing if the experiments were also done on ImageNet(probably should also try deeper neural networks).3. The sensitivity study is interesting but the experiment results are not very meaningful to me. It would be better if the author(s) gave a more detailed analysis.4. The paper could be more consistent. i.e. emphasize the contribution of your own work and be more logical. I might miss something, but I feel quite confused about what is the main idea after reading the paper. Conclusion:I believe the paper has not reached the standard of ICLR. Although we need such paper to provide analysis towards existing methods, the paper itself is not strong enough. Clarity: Below average- The introduction would be easier to follow if you named Baydin's approach and your own approach, because in the 2-4 bullet points you say "this online scheme", and "the learning rate schedule", without being perfectly clear what you are talking about- The last sentence of the introduction is meant to clearly state your hypothesis, so I was expecting "emphasize the value of *", i.e. either adaptive or non-adaptive methods, rather than just general 'tuning', which is self-apparently important.Quality: Below averageThis is a purely empirical study that does not go too deep. It is not quite a review paper, but only compares previous methods.Pros:I especially appreciate the sensitivity analysis, ie Fig 6. If only all ML papers had something like this to suggest the difficulty of setting hyperparameters for their proposed methods.Cons:- You should use mathematics to describe what you are talking about with adaptive stepsize in Sec 2.1. "these methods multiply the gradient with a matrix". Just giving one equation would be extremely helpful.- If I understand correctly, you are interpreting the inverse-Hessian as used in Newton's method and other non-diagonal 'gradient conditioners' as types of stepsize. This is definitely interesting, but again it would be very simple to see what you are saying with an equation instead of starting with the phrase "stepsize" which is generally understood to be a scalar multiple on the gradient.- I'm surprised you jump right into experiements after your background settings. It's apparent that this paper fundamentally relies on the Wilson (2017) hypergradient paper. Your paper should be more self-contained: 'hypergradient' is not even defined in this paper, is it?...Especially:How do you know that if you change the model architecture, data, and loss, that a similar result will occur? I imagine that it heavily relies on the data and model-- in other words, that the sensitivity is dependent on "how an algorithm reacts to a certain data/loss/model landscape". I'm trying to say that I'm not convinced these results generalize to any other situation than the one presented here (so does it really say anything about the different stepsize selection rules?)Random side note:Since your appendix is only a few lines, you could consider succinctly listing learning rates with set notation, for example {1e-n,5e-n : -5&lt;n&lt;1}. Pros: This seems like very competent and important work in an under-served area: Doing the mapping (or "entity linking") of chemical names to their standardized systematic forms. It's not my area, but I was frankly surprised when the paper said there was only one relevant prior piece of work, but having searched for a few minutes on Google Scholar, I'm at least inclined to believe that the authors are (approximately) right on that one. (This stands in stark contradistinction to the large quantity of biomedical entity recognition and linking work.) So, it's valuable to have work in this area, and the approach and application are sensible. In one sense, this gives the work significance and originality (as to domain). The paper is also clearly written, and certainly sufficiently accessible to an ML reader.Cons: Unfortunately, though, I just don't think this qualifies for acceptance at ICLR. It's application of known techniques, and lacks any ML novelty or sufficient ML interest. It would only be appropriate for an "ML applications" track, which ICLR does not have. And while its performance is _way_ better than that of the only previous work on the topic that they know, accuracy of mapping non-systematic chemical terms (54.04%) is still low enough that this technique doesn't seem ready for prime time. Other comments: In table 5, you show that a prior pipeline stage of spelling correction is definitely useful in your system (table 5). And yet, given the power of deep learning seq2seq transductions, and the potential to use them for spelling correction, one might wonder whether this prior step of spelling correction is really necessary. It might be interesting to explore further where it helps and whether the gains of spelling correction might be obtainable in other ways such as using data augmentation (such as spelling error insertion) in the seq2seq training data. The Golebiewski bib entry is lacking any information as to where it is published, which seems especially bad for the key citation to prior work of the whole paper. In general, the bibliography has issues: non-ASCII characters have been lost (you either need to LaTeX-escape them or to load a package like utf8, and capitalization of acronyms, etc. should be improved with curly braces. This paper focuses on deep reinforcement learning methods and discusses the presence of inductive biases in the existingRL algorithm. Specifically, they discuss biases that take the form of domain knowledge or hyper-parameter tuning. The authors state that such biases rise the tradeoff between generality and performance wherein strong biases can lead to efficient performance but deteriorate generalization across domains. Further, it motivates that most inductive biases has a cost associated to it and hence it is important to study and analyze the effect of such biases. To support their insights, the authors investigate the performance of well known actor-critic model in the Atari environment after replacing domain specific heuristics with the adaptive components. The author considers two ways of injecting biases: i) sculpting agents objective and ii) sculpting agent's environment. They show empirical evidence that replacing carefully designed heuristics to induce biases with more adaptive counterparts preserves performance and generalizes without additional fine tuning.The paper focuses on an important concept and problem of inductive biases in deep reinforcement learning techniques. Analysis of such biases and methods to use them judiciously is an interesting future direction. The paper covers a lot of related work in terms of various algorithms and corresponding biases.However, this paper only discusses such concepts at high level and provides short empirical evidences in a single environment to support their arguments. Further, both the heuristics used in practice and the adaptive counterparts that the paper uses to replace those heuristics are all available in existing approaches and there is no novel contribution in that direction too.Finally, the adaptive methods based on parallel environment and RNNs have several limitation, as per author's own admission.Overall, the paper does not have any novel technical contributions or theoretical analysis on the effect of such inductive biases which makes it very weak. Further, there is nothing surprising about the author's claims and many of the outcomes from the analysis are expected. The authors are recommended to consider this task more rigorously and provide stronger and concrete analysis on the effects of inductive biases on variety of algorithms and variety of environments. This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning. Those heuristics include reward clipping, discounting for effective learning, repeating actions, and different network structures. However, since the training algorithms also greatly affect the performance of RL agents, it seems hard to draw any quantitive conclusions from this paper.Detailed comments:1. It seems that actor-critic algorithms are defined for RL with function approximation. What is the tabular A2C algorithm? A reference in Section 3.1 would be better.2. This paper claims to study the "inductive biases", which is not clearly defined. How to quantify those biases and how to measure "generality"?3. Are there any quantitive conclusions that can be drawn from the experiments?4. Since the performance of RL agents also relies on initialization and the training algorithms. There are a lot of tricks of optimization for deep learning. How to measure the "inductive biases" by ruling out the effects of training algorithms? The authors propose to improve classification accuracy in a supervised learning framework, by providing richer ground truth in the form a distribution over labels, that is not a Dirac delta function of the label space. This idea is sound and should improve performance.Unfortunately this work lacks novelty and isn't clearly presented.(1) Throughout the paper, there are turns that used without definition prior to use, all table headers in table 1. (2) Results are hard to interpret in the tables, and there are limited details. Mixup for example, doesn't provide exact parameters, but only mentions that its a convex sum.(3) There is no theoretical justification for the approach.(4) This approach isn't scalable past small datasets, which the authors acknowledge. (6) This has been already done. In the discussion the authors bring up two potential directions of work:   (a) providing a distribution over classes by another model - &gt; this is distillation (https://arxiv.org/abs/1503.02531)   (b) adding a source of relationships between classes into the objective function -&gt; this is (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42854.pdf) The paper presents a new version of CIFAR10 that is labelled by multiple people (the test part of the data). They use it to improve the calibration of several image classifiers through fine-tuning and other techniquesThe title is too general, taking into account that this setting has appeared in classification in many domains, with different names (learning from class distributions, crowd labellers, learning from class scores, etc.). See for instance,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994863/http://www.cs.utexas.edu/~atn/nguyen-hcomp15.pdf Also, at the end of section 2 we simply reach logloss, which is a traditional way of evaluating the calibration of a classifier, but other options exist, such as the Brier score. At times, the authors mention the trade-off between classification accuracy and cross-entropy. This sounds very much the trade-off between refinement and calibration, as one of the possible decompositions of the Brier score.The authors highlight the limitations of this work, and they usually mention that the problem must be difficult (e.g., low resolution). Otherwise, humans are too good to be useful. I suggest the authors to compare with psychophysics and possible distortions of the images, or time limits for doing the classifications. Nevertheless, the paper is not well motivated, and the key procedures, such as fine-tuning lack detail, and comparison with other options.In section 2, which is generally good and straightforward, we find that p(x|c) being non-overlapping as a situation where uncertainty would be not justified. Overlap would simply say that it is a categorisation (multilabel classification) problem rather than a classification problem, but this is different from the situation where labels are soft or given by several users. In the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.Pros:-        More information about borderline cases may be useful for learning. This new dataset seems to capture this information.Cons:-        The extra labelling is very costly, as the authors recognise.-        The task is known in the classification literature, and a proper comparison with other approaches is required.-        Not compared with calibration approaches or other ways where boundaries can be softened with less information from human experts. For instance, a cost matrix about how critical a misclassification is considered by humans (cat &lt;-&gt; dog, versus cat &lt;-&gt; car) could also be very useful, and much easier to obtain. This paper proposes N-ball embedding for taxonomic data. An N-ball is a pair of a centroid vector and the radius from the center, which represents a word.Major comments:- The weakness of this paper is lack of experimental comparisons with other prominent studies. The Poincare embedding and the Lorentz model are recently proposed and show a good predictive performance in hypernymy embedding.- Related work is not sufficiently described.- It is not clear why N-ball embedding is suitable for hierarchical structures. This paper proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. For a 2D point set, the idea is to define a complex polynomial with the points (interpreted as complex numbers) as roots. To compare two point sets, the loss eventually boils down to evaluating the complex polynomial at all target points. The loss can be extended to high-dimensional point sets without the same theoretical framework. It is a sum over each target point the product of the distance of between the target point and all points of the source set. The claimed advantage of the proposed distance is it is more efficient compare to other metric such as Hausdorff distance or the sum of squared distance between matched point pairs (SMD). Experiments show that the proposed loss can be used in training.The idea is interesting and new. However, the paper has a lot of room to improve. Here are a few specific issues that can be addressed in future version:1) It is unclear what the advantage of the new idea is. I agree it leads to strong gradients compared to Hausdorff distance. However, this way of pulling all points to all points may also be very inefficient, as each point can get pulled toward all target points. These forces can easily cancel each other. 2) As already hinted in the paper, the gradient can be extremely unstable numerically. There is not any solution to address this issue.3) Experimental results do not show the proposed method has any advantage over others. Also important loss such as the Wasserstein distance should be considered and compared with. The paper presents an extension of relation networks (RNs) for natural language processing. RNs are designed to represent a set as a function of the representations of their elements. This paper treats a sentence as a set of words. Whereas regular RNs assume that the representation of a set is a uniform aggregation of the representation of the pairs of elements in the set, this paper proposes to weight the relevance of the pairs according to their tree-structured dependency relations between the words. The authors evaluate on a suite of NLP tasks, including SNLI, Quora duplicate question ID, and machine translation. They show marginal improvements over naive baselines, and no improvement over SOTA.I am concerned about both the motivation for and the novelty of this work. My reading of this work is that the authors try to reverse engineer a TreeRNN in terms of RNs, but I am not sure what the reason is for wanting to use the RN framework in order to derive an architecture that, IIUC, essentially already exists. I can't find any fundamentally meaningful differences between the proposed architecture and the existing work on TreeRNNs, and the results suggest that there is nothing groundbreaking being proposed here. It is possible I am missing some key insight, but I do believe the burden is on the authors to highlight where the novelty is. The intro *and* related work sections should both be rewritten to answer the question: what is the insufficiency with current sentence encoding models that is addressed by this architecture? Currently, the intro addresses the tangential question: what is the insufficiency with RNs for NLP that is addressed by this architecture? If the latter is the question the authors want to answer, they need to first answer: why should we want to cast sentence encoders as RNs as opposed to any of the (many) other available architectures? Without a firmer understanding of what this paper contributes and why, I can't recommend acceptance. More detailed comments for the authors below. - You introduce a few naive baselines, but none of these is a TreeRNN. TreeRNNs are the obvious baseline, and you should be comparing on each and every evaluation task, even if there is no previously published result for using tree RNNs on that task. For the one result (SNLI, table 1) on which there is previous work using TreeRNNs, the table confirms my intuition that the proposed model is no improvement over the TreeRNN architecture. It seems very important to address this comparison across all of the evaluation tasks.- I like the notion of marginalizing over latent tree structures, but the related work section needs to make clear what is being contributed here that is different from the cited past work on this problem- On the MT eval, why are you missing values for zh-en on the NMT models that are actually competitive? I think many of these models are open-source or easy to reimplement? Its hard to draw conclusions when from such a gappy table.- Only table 2 has significance values (over naive baseline that is) which implies that the other results are not significant? That is disconcerting. - I am disappointed in the analysis section. As is, you provide an ad-hoc inspection of some inferred trees. I find this odd since there is no evidence that the tree-ness of the architecture (as opposed to, e.g., recurrence or attention) is what leads to quantitative improvements (at least according to the experimental results in the tables), so there is no reason we should actually expect the trees to be good or interesting. My interpretation of these cherry-picked examples is that the learning is fighting the architecture a bit, basically "learning a tree" that reduces to being an attention mechanism that up-weights one or two salient words. - The analysis I *wanted* to see instead was why recursion helped for sentence classification, it did not for MT. You give an intuition for this result but no evidence. (That is assuming that, quantitatively, this trend actually holds. Which maybe is not the case if none of the results are significant.)- In general, regarding evaluation, SNLI is overfit. You should use MNLI at least. I have trouble interpreting progress on SNLI as "actual" progress on language representation.- The related work section as a whole is too short. If you need to cut space, move technical content to appendix, but don't compromise in related work. You listed many relevant citations, but you have no context to situate your contribution relative to this past work. What is the same/different about your method? You should provide an answer to that for each and every paper you cite. This paper investigates a meta-learning approach for the contextual bandit problem. The goal is to learn a generic exploration policy from datasets, and then to apply the exploration policy to contextual bandit tasks. The authors have adapted an algorithm proposed for imitation learning (Ross &amp; Bagnell 2014) to their setting. Some theoretical guarantees straightforwardly extracted from (Ross &amp; Bagnell 2014) and from (Kakade et al 2008) are presented. Experiments are done on 300 supervised datasets.Major concerns:1 This paper investigates a problem that does not correspond to the real problem: how to take advantage of a plenty of logs generated by a known stochastic policy (or worst unknown deterministic policy) for the same (or a close) contextual bandit task? Most of companies have this problem. I do not know a single use case, in which we have some full information datasets, which are representative of contextual bandit tasks to be performed. If the full information datasets does not correspond to the contextual bandit tasks, it is not possible to learn something useful for the contextual bandit task. 2 The experimental validation is not convincing.The experiments are done on datasets, which are mostly binary classification datasets. In this case, the exploration task is easy. May be it is the reason why the exploration parameter \mu or \epsilon = 0 provides the best results for MELEE or \epsilon-greedy?The baselines are not strong. The only tested contextual bandit algorithm is LinUCB. However a diagonal approximation of the covariance matrix is used when the dimension exceeds 150. In this case LinUCB is not efficient. There are a lot of contextual bandit algorithms that scale with the dimension.3 The theoretical guarantees are not convincing. The result of Theorem 1 is a weak result. A linear regret against the expected reward of the best policy is usually considered as a loosely result. Theorem 2 shows that there is no theoretical gain of the use of the proposed algorithm: the upper bound of the expected number of mistakes obtained when Banditron is used in MELEE is upper than the one of Banditron alone.Minor concerns:The algorithms are not well written. POLOPT function has sometimes one parameter, sometimes two and sometimes three parameters. The algorithm 1 is described in section 2, while one of the inputs of the algorithm 1 (feature extractor function) is described in section 3.1. The algorithm 1 seems to return all the N exploration policies. The choice of the returned policy has to be described.In contextual bandits, the exploration policy is not handcrafted. The contextual bandit algorithms are designed to be optimal or near optimal in worst case: they are generic algorithms. This paper provides a system to play CCP using some deep learning. The system consists of  three modules - the bid module, which is rule based, and the policy and kicker networks, which are simple convolutional neural networks. The authors use a dataset of 8 million game records consisting of 80 million state action pairs, and train the network in a supervised fashion. The resulting model is able beat MicroWe, the current state of the art in playing CCP, and even are able to beat a few "top amateur players"- Why is the bid module also not learned? It seems like the feature set for the bid module is fairly simple, and a linear or MLP can do fairly well compared to a rule based module.- It's not clear that separating the policy and kicker networks would be more advantageous than combining them. Thousands of actions is not a too large number - language modeling work routinely deals with outputting many more classes than that.- Were the convolutions chosen 1D, 2D, or 3D? The figure seems to imply that the convolutions were over the XZ dimensions, with Y as the channel dimension. If so, this doesn't make too much sense to be, since the Z dimension is not uniform - the last index is all unseen cards, which is significantly more than the middle indices of "what was played in this round". There shouldn't be a lot of translational invariance in the Z dimension. I'm also not convinced that translational invariance is helpful in the X dimension.- There were no comparisons with baseline models or different model architectures. I would like to see some results on the same structure, but with an Linear model, MLP or LSTM across the time dimension, or search through different types of convolutional networks.- What hyperparameters were searched through in the learning process?- Missing citations for MicroWe being the best CCP AI, and citations for the accomplishments of the top amateur players.- How far away are the top amateur players from professional players? Please provide some context on how far this system is from solving CCP.- Fig 3, 4 should just say #of games instead of "iteration"This paper shows that one choice for a supervised learning system on a CCP game database can achieve amateur level human play. It does not give insight to why the system was designed this way, why the model choices were made, and how good simpler baselines might be able to achieve. The paper is not clearly written enough, and does not provide enough scientific value to be accepted to the conference. The authors propose a new set-up for reinforcement learning which considers undiscounted episodic returns and introduces a loop-penalty to ensure that all episodes terminate and that the returns are bounded. In the tabular case, the loop-penalty zeros out the reward if a loop is detected in the current episode. For continuous state-spaces, the authors propose to detect loops using methods that estimate state-similarity.The proposed formulation is relevant as well as novel and creative. While discount factors have been questioned repeatedly in recent years, research looking into alternative formulations remains sparse. The most common formulation for non-discounted reinforcement learning, average-reward RL, suffers from the difficulty of estimating the average reward effectively and alternative formulations could be beneficial; however, the paper is lacking in details when it comes to its experimental set-up as well as the method used for non-tabular methods and the analysis is often trivial. I would encourage the authors to spend more time on the practical questions of how to estimate the loop-penalty in non-tabular domains. The positive results on vizdoom as well as a robotics task imply that detecting such loops is possible, but there is no discussion of how this is done, in which cases it can work well and in which cases it is still too difficult. It would also be good if the empirical evaluation would include higher, more realistic discount factors such as 0.9 for robot grasping or 0.99 for VizDoom.With regards the analysis, I believe that the following is problematic:* A large part of the analysis focuses on uniformity. The authors first show that the value-function has no unique solution on ergodic states with zero reward and then draw a connection between this and the phenomenon illustrated in Figure 2. My first issue here is that the result is well-known in the average reward scenario of which this is a special case. The claim immediately follows and the proof in the paper is unnecessary. My second issue is that it is unclear how the constant offset that can be applied to the value function relates to the flatness of the value function shown in Figure 2. The constant offset can be applied to non-discounted value functions even when the value function does not exhibit this issue.* Theorem 3 writes out the value function for RL with loop-penalty. The loop-penalty makes the reward non-markovian, therefore it is unclear what theoretical insights can be gained from this theorem.* Theorem 1 is proving that the episodic return corresponds to success probability if the reward is 1 for success and 0 otherwise. This is trivial and the proof is overly complicated. I dont believe a proof for this is necessary. This paper proposes a variant of AWR with an added Q-function. It is motivated by the *state-determines-action* assumption, which is used to argue that AWR should always return the data-collecting policy if optimized to convergence. Experimental evaluation of QWR focuses on online sample efficiency and offline performance.I have reviewed this paper for a previous conference. It seems that the issues pointed out during that review process have not been addressed. Most importantly, the main motivating theorem (Theorem 1) for QWR appears to be incorrect: it relies on $\log \pi(a \mid s)$ being nonpositive, but this is not true in general in continuous action spaces. Instead of fixing the theorem, it has been moved to the appendix.The algorithm presented here is largely the same as that in two recent papers: [advantage-weighted actor critic](https://arxiv.org/abs/2006.09359) and [critic regularized regression](https://arxiv.org/abs/2006.15134). It would be unfair to fault this paper for its similarity to others written in the same timeframe, but it does probably mean that the paper should be judged more on its experimental evaluation and analysis than on its novelty. By the metric of experimental evaluation, it falls short. The offline setting has recently seen more standardization of benchmarks (see, eg, [D4RL](https://arxiv.org/abs/2004.07219) and the [DQN Replay Dataset](https://offline-rl.github.io/)), and evaluating on these would allow for cleaner comparison to other works. (To the paper's credit, it does discuss these prior works and their relation to QWR.)It is possible this comparison was not performed because the offline setting studied is different than the usual offline setting: the data-collecting sampling policy $\pi(a \mid s)$ is required for the optimization in Equation 3, unlike most offline RL works which use only logged trajectories. It is also not necessarily an issue to study a different setting, but it requires some amount of justification for the modified problem setting. It also makes the evaluation more important, because it becomes unclear whether any gains are due to improving the algorithm or relaxing the problem statement.If I am mistaken about the main theorem being incorrect, I will happily increase my score. The motivation for the work is not very clear to me. The paper is difficult to follow especially the motivation, problem setup and architecture part. The dataset is not publicly available for everyone to use to the idea is not reproducible. Authors write in the paper that dataset is publicly available in their organization, I dont understand what do they mean by that. Comparison with state of the art neural symbolic machine/ function generators is missing. I didnt find the baselines to be strong enough. Overall:  I found this paper to be hard to follow. The motivation for the task of predicting formulas given table is not very clear. It would have been better if authors could explain a real world example application for their task. Figure-1 is not clear, it needs some more intuition and discussion. A running example would have really helped. I found introduction to be a bit disconnected.Question: why authors have not reported results for other related tasks like taskfill using their technique? Can you please explain motivation/real world application for the task and technique in details? How different their taks is from semantic parsing for table question answering? The paper proposes an objective function for learning representations, termed the conditional entropy bottleneck (CEB). Variational bounds on the objective function are derived and used to train classifiers according to the CEB and compare the results to those attained by competing methods. Robustness and adversarial examples detection of CEB are emphasized.My major comments are as follows:1) The authors base their 'information-theoretic' reasoning on the set-theoretic structure of Shannons information measures. It is noteworthy that when dealing with more than 2 random variables, e.g., when going from the twofold I(X;Y) to the threefold I(X;Y;Z), this theory has major issues. In particular, there are simple (and natural) examples for which I(X;Y;Z) is negative. The paper presents an information-theoretic heuristic/intuitive explanation for their CEB construction based on this framework. No proofs backing up any of the claims of performance/robustness in the paper are given. Unfortunately, with such counter-intuitive issues of the underlying theory, a heurisitc explanation that motivates the proposed construction is not convincing. Simulations are presented to justify the construction but whether the claimed properties hold for a wide variety of setups remain unclear.2) Appendix A is referred to early on for explaining the minimal necessary information (MNI), but it is very unclear. What is the claim of this Appendix? Is there a claim? It's just seems like a convoluted and long explanation of mutual information. Even more so, this explanation is inaccurate. For instance, the authors refer to the mutual information as a 'minimal sufficient statistic' but it is not. For a pair of random variables (X,Y), a sufficient statistic, say, for X given Y is a function f of Y such X-f(Y)-Y forms a Markov chain. Specifically, f(Y) is another random variable. The mutual information I(X;Y) is just a number. I have multiple guesses on what the authors' meaning could be here, but was unable to figure it out from the text. One option, which is a pretty standard way to define sufficient statistic though mutual information is as a function f such that I(X;Y|f(Y))=0. Such an f is a sufficient statistic since the zero mutual information term is equivalent to the Markov chain X-f(Y)-Y from before. Is that what the authors mean..?3) The Z_X variable introduced in Section 3 in inspired by the IB framework (footnote 2). If I understand correctly, this means that in many applications, Z_X is specified by a classifier of X wrt the label Y. My question is whether for a fixed set of system parameters, Z_X is a deterministic function of X? If this Z_X play the role of the sufficient statistics I've referred to in my previous comment, then it should be just a function of X. However, if Z_X=f(X) for a deterministic function f, then the CEB from Equation (3) is vacuous for many interesting cases of (X,Y). For instance, if X is a continuous random variable and Z_X=f(X) is continuous as well, then I(X;Z_X|Y)=h(Z_X|Y)-h(Z_X|X,Y)where h is the differential entropy and the subtracted terms equals -\infty by definition (see Section 8.3 of (Cover &amp; Thomas, 2006). Consequently, the mutual information and the CEB objective are infinite. If Z_X=f(X) is a mixed random variable (e.g., can be obtain from a ReLU neural network), then the same happens. Other cases of interest, such as discrete X and f being an injective mapping of the set of X values, are also problematic. For details of such problem associated with IB type terms see:[1] R. A. Amjad and B. C. Geiger 'Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle', 2018 (<a href="https://arxiv.org/abs/1802.09766)." target="_blank" rel="nofollow">https://arxiv.org/abs/1802.09766).</a>Can the authors account for that?4) The other two reviews addressed the missing accounts for past literature. I agree on this point and will keep track of the authors' responses. I will not comment on that again. Beyond these specific issue, they text is very wordy and confusing at times. If some mathematical justification/modeling was employed the proposed framework might have been easier to accept. The long heuristic explanations employed at the moment do not suffice for this reviewer. Unless the authors are able to provide clarification of all the above points and properly place their work in relation to past literature I cannot recommend acceptance. The authors propose a new adversarial technique to add fake nodes to fool a GCN-based classifier. The basic approach relies on a greedy heuristic to add edge/node features, and the authors also present a GAN-based approach, which allows the model to add fake nodes that are not easily distinguishable from regular nodes. The primary motivation behind the idea of adding fake is that it is unrealistic to change the features/edges of existing nodes. Experimental results show that adding a large number (20% in most cases) of fake nodes can significantly degrade accuracy of a GCN, and results show that the GAN-based approach is somewhat effective at making the fake nodes less distinguishable.  In terms of strengths, the GAN-based approach is well-motivated and it appears that the authors were thorough in their experiments on Cora/Citseer (e.g., with a number of ablation/sensitivity studies).However, while interesting, this paper has a number of areas where it could be substantially improved:1) With regards to the motivation: It is not clear what substantive technical novelty there is in the idea of adding fake nodes, compared to existing approaches that simply modify existing nodes in an adversarial way. Intuitively, the approach of Zugner et al can already handle this case of "adding new nodes". One just adds a set of nodes with random/null edges/features to the graph, treats this as their attacker node set and then runs Zugner et al's greedy algorithm. Some clarification on why this simple application of Zugner et al's approach does not work would be useful and/or empirical results using their method as a baseline would be useful. (Also, Zugner et al was published in KDD 2018, so the citation should be corrected). 2) In Zugner et al, they derive approximations and algorithms that allow them to compute the score of adding/removing an edge in constant time. The greedy approach in this work appears quite expensive as every greedy update requires an expensive gradient computation. Some discussion of computational complexity would improve the paper. 3) Results are only provided on two small datasets (presumably due to the large computational cost for the approach). These two very small datasets are not indicative of many real-world scenarios, and additional results on larger (and more diverse) datasets would greatly strengthen the paper. 4) Adding 20% fake nodes seems like a prohibitively large number. Even 5% fake nodes is extremely large. It is unclear what real-world applications could admit such drastic numbers of fake nodes, and some comments on this would greatly strengthen the paper. 5) The GAN method is interesting and well-motivated, but it is not clear if this method offers any utility beyond the distribution matching approach of Zugner et al (Section 4.1 of their paper). A comparison between these methods is necessary to justify the utility of the proposed GAN-greedy approach. The main idea of this paper is that a 'realistic' way to attack GCNs is by adding fake nodes. The authors go on to show that this is not just a realistic way of doing it but it can done in a straightforward way (both attacks to minimize classification accuracy and GAN-like attacks to make fake nodes look just like real ones). The idea is neat and the experiments suggests that it works, but what comes later in the paper is mostly rather straightforward so I doubt whether it is sufficient for ICLR. I write "mostly" because one crucial part is not straightforward but is on the contrary, incomprehensible to me.  In Eq (3) (and all later equations) , shouldn't X' rather than X be inside the formula on the right? Otherwise it seems that the right hand side doesn't even depend on X' (or X_{fake} ). But if I plug in X', then the dimensions for weight matrices  W^0 and W^1 (which actually are never properly introduced in the paper!) don't match any more. So what happens? To calculate J you really need some extra components in W0 and W1. Admittedly I am not an expert here, but I figure that with a bit more explanation I should have been able to understand this. Now it remains quite unclear...and I can't accept the paper like this.Relatedly, it is then also unclear what exactly happens in the experiments: do you *retrain* the network/weights or do you re-use the weights you already had learned for the 'clean' graph? All in all: PRO:- basic idea is neat CON:- development is partially straightforward, partially incomprehensible.(I might increase my score if you can explain how eq (3) and later really work, but the point that things remain rather straightforward remains). TargetPropThis paper addresses the problem of training neural networks with the sign activation function. A recent method for training such non-differentiable networks is target propagation: starting at the last layer, a target (-1 or +1) is assigned to each neuron in the layer; then, for each neuron in the layer, a separate optimization problem is solved, where the weights into that neuron are updated to achieve the target value. This procedure is iterated until convergence, as is typical for regular networks. Within the target propagation algorithm, the target assignment problem asks: how do we assign the targets at layer i, given fixed targets and weights at layer i+1? The FTPROP algorithm solves this problem by simply using sign of the corresponding gradient. Alternatively, this paper attempts to assign targets by solving a combinatorial problem. The authors propose a stochastic local search method which leverages gradient information for initialization and improvement steps, but is essentially combinatorial. Experimentally, the proposed algorithm, GRLS, is sometimes competitive with the original FTPROP, and is substantially better than the pure gradient approximation method that uses the straight-through estimator.Overall, I do like the paper and the general approach. However, I think the technical contribution is thin at the moment, and there is no dicussion or comparison with a number of methods from multiple papers. I look forward to discussing my concerns with the authors during the rebuttal period. However, I strongly believe that the authors should spend some time improving the method before submitting to the next major conference. I am confident they will have a strong paper if they do so.Strengths:- Clarity: a well-written paper, easy to read and clear w.r.t. the limitations of the proposed method.- Approach: I really like the combinatorial angle on this problem, and strongly believe this is the way forward for discrete neural nets.Weaknesses:- Algorithm: GRLS, in its current form, is quite basic. The Stochastic Local Search (SLS) literature (e.g. [1]) is quite rich and deep. Your algorithm can be seen as a first try, but it is really far from being a powerful, reliable algorithm for your problem. I do appreciate your analysis of the assignment rule in FTPROP, and how it is a very reasonable one. However, a proper combinatorial method should do better given a sufficient amount of time.- Related work: references [2-10] herein are all relevant to your work at different degrees. Overall, the FTPROP paper does not discuss or compare to any of these, which is really shocking. I urge the authors to implement some or all of these methods, and compare fairly against them. Even if your modified target assignment were to strictly improve over FTPROP, this would only be meaningful if the general target propagation procedure is actually better than [2-10] (or the most relevant subset).- Scalability: I realize that this is a huge challenge, but it is important to address it or at least show potential techniques for speeding up the algorithm. Please refer to classical SLS work [1] or other papers and try to get some guidance for the next iteration of your paper.Good luck![1] Hoos, Holger H., and Thomas Stützle. Stochastic local search: Foundations and applications. Elsevier, 2004.[2] Stochastic local search for direct training of threshold networks[3] Training Neural Nets with the Reactive Tabu Search[4] Using random weights to train multilayer networks of hard-limiting units[5] Can threshold networks be trained directly?[6] The geometrical learning of binary neural networks[7] An iterative method for training multilayer networks with threshold functions[8] Backpropagation Learning for Systems with Discrete-Valued Functions[9] Training Multilayer Networks with Discrete Activation Functions[10] A Max-Sum algorithm for training discrete neural networks This paper proposes an extension to hredGAN, which is an adversarial framework for multi-turn dialogue model, to simultaneously learns a set of attribute embeddings that represents the persona of each speaker and generate persona-based responses. The generator of the proposed system phredGAN is conditioned on both the history utterances and the speakers persona by concatenating the utterance encoding with attribute embeddings. For discriminator, the authors explore two versions: 1) phredGAN_a takes attributes as inputs; 2) phredGAN_d adds a dual discriminator that predicts the attribute(s) for each utterance. Strength: 1) to the best of my knowledge, adding persona information to an adversarial multi-turn dialogue model is novel; 2) the authors explore two different approaches to build the discriminator(s) and the idea of adding a second discriminator that predicts the attributes seems interesting.    Weakness:1) Novelty: The idea of learning speaker-specific attribute embeddings is very similar to the Speaker Model proposed by Li et al.(2016) http://www.aclweb.org/anthology/P16-1094 and the proposed system only makes minor changes to hredGAN https://arxiv.org/abs/1805.11752.  2) Presentation: The writing of this paper is a little hard to follow, for example, it presents the two discriminators after the objective function (Equation 2) and does not explain the intuition behind each model. In Equation 2, the objective function, why training the discriminator to minimize the attributes prediction probability Simply saying the attribute prediction loss is collaborative is not clear enough. Or is the min for the second term a typo? 3) Model:The idea of adding a discriminator that predicts the attributes seems interesting. However the loss is not adversarial for the second discriminator (Equation 6), you should not indicate L_att is GAN in your notation. Im also not convinced that this should be collaborative. Despite that the discriminator is trying to predict the correct attribute id, the input of the two terms in Equation 6 is different, one comes from the true data, the other comes from the generator. Shouldnt the discriminator try to differentiate these two cases? Otherwise, its not a discriminator (also raise the question for Equation 2, why argmin min(L_att)).4) EvaluationThe evaluation is not strong enough to demonstrate the benefit of the proposed model. a. It only compares against one previous work that takes speaker identity into account on one dataset. Despite that the authors apply several different metrics to evaluate the proposed model, they only compare with previous models by Li et al. (2016) on perplexity and BLEU. b. The perplexity scores of the proposed models are worse than SAM by Li et al. (2016). The authors explain this by stating that the entropy for a multi-turn model is supposed to be higher than the single-turn model. Its better to provide a more rigorous analysis. For a fair comparison, they could also train the proposed model using only one-turn history, which should be identical to Li et al.s setting (How many turns history are you using?). The improvements of the BLEU score might also be the consequence of substituting the past generated sequence in the generator with ground truth (since the model uses the same training algorithm as hredGAN https://arxiv.org/abs/1805.11752). Its unclear if this is the cause unless the authors provide the comparison among SAM, hredGAN, and phredGAN. c. Table 2 compares the non-persona hredGAN with phredGAN on UDC, but the authors do not provide a comparison between these two on the TV dataset in Table 1. d. The comparison between phredGAN_a and phredGAN_d is inconsistent for the two datasets (Table 1 and Table 2). Summary:The authors propose a framework for training an external observer that tries explain the behavior of a prediction function using the minimal description principle. They extend this idea by considering how a human with domain knowledge might have different expectations for the observer. The authors test this framework on a multi-variate time series medical data (MIMIC-III) to show that, under their formulation, the external observer can learn interpretable embeddings.Pros:- Interesting approach: Trying to develop an external observer based on information theoretic perspective. - Considering the domain knowledge of the human subject can potentially be an important element when we want to use interpretable models in practice.Issues:(1) In 2.4: So between M and M^O, which one is a member of M_reg? (2) On a related note to issue (1): In 2.4.1, "Clearly, for each i, (M(X))_i | M^O(X) follows also a Gaussian distribution: First of all, I'm not sure if that expression is supposed to be  p(M(X)_i | M^O(X)) or if that was intended. But either way, I don't understand why that would follow a normal distribution. Can you clarify this along with issue (1)?(3) In 2.4.2: The rationale behind using attention &amp; compactness to estimate the complexity of M^O is weak. Can you elaborate this in the future version?(4) What do each figure in Figure 4 represent?(5) More of a philosophical question: the authors train M and M^O together, but it seems more appropriate to train an external observer separately. If we are going to propose a framework to train an agent that tries to explain a black box function, then training the black-box function together with the observer can be seen as cheating. It can potentially make the job of the observer easier by training the black box function to be easily explainable. It would have been okay if this was discussed in the paper, but I can't find such discussion.(6) The experiments can be made much stronger by applying this approach to a specific prediction task such as mortality prediction. The current auto-encoding task doesn't seem very interesting to apply interpretation.(7) Most importantly: I like the idea very much, but the paper clearly needs more work. There are broken citations and typos everywhere. I strongly suggest polishing this paper as it could be an important work in the model interpretability field. In this work the authors propose an extension of mixture density networks to the continuous domain, named compound density networks. Specifically the paper builds on top of the idea of the ensemble neural networks (NNs) and introduces a stochastic neural network for handling the mixing components. The mixing distribution is also parameterised by a neural network. The authors claim that the proposed model can result in better uncertainty estimates and the experiments attempt to demonstrate the benefits of the approach, especially in cases of having to deal with adversarial attacks.The paper in general is well written and easy to follow. I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology. Let me elaborate. First of all, I dont understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?I also find weird the way that the authors arrive to their final objective in Equation (5). They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points. Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term. However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi). Is there a reason why the authors do not introduce their objective by following the variational framework?Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which maps x to a distribution over parameters instead of specific value \theta. How is this different from the case that we were considering so far? If we had a point estimate for \theta we would not require to take an expectation in Equation (3) in the first place.My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)). The Kronecker product between two diagonal matrices results in another diagonal matrix, i.e., diagonal covariance, which implies that the weights within a layer are given by an independent multivariate Gaussian. What is the purpose then for introducing the matrix variate Gaussian? I would expect that you would like to impose additional structure to the weights. I expect the authors to comment on that.Regarding the experimental evaluation of the model rather confusing. The authors have proposed a model that due to the mixing is better suited for predictions with heteroscedastic noise and can better quantify the aleatoric uncertainty. However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of systems noise, i.e. epistemic uncertainty. The generative process of the toy data clearly states that there is no heteroscedastic noise to handle. The same applies for the notMNIST data which belong to a completely different data set compared to MNIST and thus out of sample prediction cannot benefit from the mixing; i.e., variations have to be explained by systems noise. So overall I have the feeling that the authors have not succeeded to evaluate the models power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing. The plot by itself, as I understood, quantifies the models uncertainty in in- and out-of sample prediction. While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model. There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy. Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice. ummary: The paper considers finding the most adversarial random noise given multiple classifiers. They formulate the problem as the standard min-max game and propose a multiplicative update algorithm. The authors show experimental results.Comments: I am afraid that the main technical result is already known:Yoav Freund Robert E. Schapire: Adaptive game playing using multiplicative weights, Games and Economic Behavior, 29:79-103, 1999.The paper shows that a multiplicative update algorithm can approximately solve the min-max game. If you use the result, you can readily obtain the main results of the present paper. Overall, I could potentially be persuaded to accept this paper given a relatively favorable comparison to some other blackbox optimization algorithms, but I have some serious issues about clarity and some technical details that seem wrong to me (e.g., the inclusion of L-BFGS as a "derivative free" baseline, and the authors' method outperforming derivative based methods at optimizing convex loss functions).I'd like to start by focusing on a few of the results in section 5.2 specifically.In this section, you compare your method and several baselines on the task of training logistic regression and SVM models. Given that these models have convex loss functions, it is almost inconceivable to me that methods like L-BFGS and SGD (at least with decent learning rates) would perform worse than gradient free optimization algorithms,as both L-BFGS and SGD should clearly globally optimize a convex loss. I am also generally confused by the inclusionof L-BFGS as an example of a derivative free optimization problem. Are you using L-BFGS with search directionsother than the gradient or something as a baseline? I think the exact setup here may require substantially more explanation.The primary other issue I'd like to discuss is clarity. While I think the authors do a very good jobgiving formal definitions of their proposed methods, the paper would massively benefit from some additionaltime spent motivating the authors' approach. As a primary example, definition 2 is extremely confusing. I felt it wasn't as well motivated as it could have been given that it is  the central contribution of the paper. You reference an "exploration process" and an "exploitation process" that "were shown in Eq. 4," but equation four is the next equation that directly jumps in to using these two processes X(t) and Y(t). These two processes are very vaguelydefined in the definition. For example, I understand from that definition that Y(t) tracks the current min value, but even after reading the remainder of the paper I am still not entirely sure I understand the purpose of X(t). Perhaps the paper assumes a detailed understanding on the readers' part of the work in Su et al. (2014), which is cited repeatedly throughout the method section?To be concrete, my recommendation to the authors would be to substantially shorten the discussion in the paperbefore section 3, provide background information on Su et al., 2014 if necessary, and spend a substantially larger portion of the paper explaining the derivation of SHE2 rather than directly presenting it as an ODEthat immediately introduces its own notation. In the algorithm block, the underlying blackbox functionis only evaluated in the if statement on line 9 -- can the authors explain intuitively how their surrogate model evolves as a result of the Y_{t} update?In addition to these concerns, some of the claims made in the method section seem strange or even wrong to me,and I would definitely like to see these addressed in some way. Here is a list of a few concerns I havealong this line:- A few of the citations you've given as examples of derivative free optimization are confusing.You cite natural gradient methods and L-BFGS as two examples, but natural gradient descent involves preconditioningthe gradient with the inverse Fisher information matrix, and is therefore typically not derivativefree. You give Gaussian process surrogate models as an example of a convex surrogate, but GPsin general do not lead to convex surrogates save for with very specific kernels that are notoften used for Bayesian optimization.- In the background, it reads to me like you define GP based Bayesian optimization as a quadraticbased trust region method. This seems strange to me. Trust region methods do involve quadratic surrogates,but my understanding is that they are usually local optimization schemes where successive local quadratic approximations are made for each step. GP based Bayesian optimization, by contrast, maintains a globalsurrogate of the full loss surface, and seeks to perform global optimization.- Equation 3 defines the squared norm \frac{1}{2}||X-Y||^{2}_{2} as the "Euclid[ean] distance".Based on the following derivatives, I assume this is intended to be kept as the squared Euclidean distance (with the 1/2 term included for derivative simplicity). This paper suggests a continuous-time framework consisting of two coupled processes in order to perform derivative-free optimization. The first process optimizes a surrogate function, while the second process updates the surrogate function. This continuous-time process is then discretized in order to be run on various machine learning datasets. Overall, I think this is an interesting idea as competing methods do have high computational complexity costs. However, Im not satisfied with the current state of the paper that does not properly discuss notions of complexity of their own method compared to existing methods.1) The computational and storage complexity for (convex) surrogates is extremely high. The discussion in this paragraph is too superficial and not precise enough.a) First of all, the authors only discuss quadratic models but one can of course use linear models as well, see two references below (including work by Powell referenced there):Chapter 9 in Nocedal, J., &amp; Wright, S. J. (2006). Numerical optimization 2nd.Conn, A. R., Scheinberg, K., &amp; Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.I think this discussion should also be more precise, the authors claim the cost is extremely high but I would really expect a discussion comparing the complexity of this method with the complexity of their own approach. As discussed in Nocedal (reference above) the cost of each iteration with a linear model is O(n^3) instead of O(n^4) where n is the number of interpolation points. Perhaps this can also be improved with more recent developments, the authors should do a more thorough literature review.b) What is the complexity of the methods cited in the paper that rely on Gaussian processes?(including (Wu et al., 2017) and mini-batch (Lyu et al., 2018)).2) The convergence of trust region methods cannot be guaranteed for high-dimensional nonconvex DFOTwo remarks: a) This statement is incorrect as there are global convergence guarantees for derivative-free trust-region algorithms, see e.g.Conn, A. R., Scheinberg, K., &amp; Vicente, L. N. (2009). Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points. SIAM Journal on Optimization, 20(1), 387-415.In chapter 10, you will find global convergence guarantees for both first-order and second-order critical points.b) The authors seem to emphasize high-dimensional problems although the convergence guarantees above still apply. For high-order models, the dimension does have an effect, please elaborate on what specific comment you would like to make. Finally, can you comment on whether the lower bounds derived by Jamieson mentioned depend on the dimension.3) Quadratic loss functionThe method developed by the authors rely on the use of a quadratic loss function. Can you comment on generalizing the results derived in the paper to more general loss functions? It seems that the computational complexity wouldnt increase as much as existing DFO methods. Again, I think it would be interesting to give a more in-depth discussion of the complexity of your approach.4) Convergence rateThe authors used a perturbed variant of the second-order ODE defined in Su et al. 2014. The noise added to the ODE implies that the analysis derived in Su et al. 2014 does not apply as is. In order to deal with the noise the authors show that unbiased noise does not affect the asymptotic convergence. I think the authors could get strong non-asymptotic convergence results. In a nutshell, one could use tools from Ito calculus in order to bound the effect of the noise in the derivative of the Hamiltonian used in Lemma 1. See following references:Li, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.Krichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuousand discrete time. In Advances in neural information processing systems, pages 28452853.Of course, the above works rely on the use of derivatives but as mentioned earlier, one should be able to rely on existing DFO results to prove convergence. If you check Chapter 2 in the book of Conn et al. (see reference above), you will see that linear interpolation schemes already offer some simple bounds on the distance between the true gradient of the gradient of the model (assuming Lipschitz continuity and differentiability).5) NoiseThe noise would help the system escape from an unstable stationary point in even shorter timePlease add a relevant citation. For isotropic noise, seeGe, R., Huang, F., Jin, C., and Yuan, Y. Escaping from saddle points-online stochastic gradient for tensor decomposition.Jin, C., Netrapalli, P., and Jordan, M. I. Accelerated gradient descent escapes saddle points faster than gradient descent. arXiv preprint arXiv:1711.10456,6) Figure 2Instead of having 2 separate plots for iteration numbers and time per iteration, why dont you combine them to show the loss vs time. This would make it easier for the reader to see the combined effect.7) Empirical evaluationa) There are not enough details provided to be able to reproduce the experiments. Reporting the range of the hyperparameters (Table 2 in the appendix) is not enough. How did you select the hyperparameters for each method? Especially step-size and batch-size which are critical for the performance of most algorithms. b) I have to admit that I am not extremely familiar with common experimental evaluations used for derivative-free methods but the datasets used in the paper seem to be rather small. Can you please justify the choice of these datasets, perhaps citing other recent papers that use similar datasets?8) Connection to existing solutionsThe text is quite unclear but the authors seem to claim they establish a rigorous connection between their approach and particle swarm (In terms of contribution, our research made as yet an rigorous analysis for Particle Swarm). This however is not **rigorously** established and needs further explanation. The reference cited in the text (Kennedy 2011) does not appear to make any connection between particle swarm and accelerated gradient descent. Please elaborate.9) SGD resultsWhy are the results for SGD only reported in Table 1 and not in the figure? Some results for SGD are better than for P-SHE2 so why are you bolding the numbers for P-SHE2?It also seem surprising that SGD would achieve better results than the accelerated SGD method. What are the possible explanations?10) Minor comments- Corollaries 1 and 2 should probably be named as theorems. They are not derived from any other theorem in the paper. They are also not Corollaries in Su et al. 2014.- Corollary 2 uses both X and Z.- Equation 5, the last equation with \dot{V}(t): there is a dot missing on top of the first X(t)SHE2 should enjoy the same convergence rate &(1/T) without addressing any further assumptions =&gt; What do you mean by should?- There are **many** typos in the text!! e.g. the the, is to used, convergeable,... please have someone else proofread your submission. This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:- There are versions of attention mechanisms that are spare and differentiable. See here: From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label ClassificationAndré F. T. Martins, Ramón Fernandez Astudillo - Why is "rationalizing textual matching" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:D. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018http://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf- I don't agree that explanations are sufficient if removing them doesn't degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:Adversarial Examples for Evaluating Reading Comprehension Systems.Robin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.- What is the "certificate of exclusion of unselected parts" that the proposed method has?- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it. This paper introduces Geomstats, a geometric toolbox for machine learning on Riemannian manifolds. In comparison to previous packages such as manopt and pymanopt, the paper claims that the proposed software provide more efficient implementations, and is integrated with deep learning backends. Several potential applications settings for the software are explored, introducing some performance gains on specific problems when one resorts to the geometry of the space. An example setting for deep learning on SE(3) is also presented.Strengths: The use of such a toolbox could be a significant step to leveraging geometric models in deep learning.Weakness: 1. The paper is written as more of a software document than a scientific paper. Several well-known manifolds are presented in the setting of Geomstats, however what is lacking is a treatment of some of the internals of how backpropagation can be implemented effectively for such a toolbox. For example, many of the Riemannian geometric algorithms need to resort to numerical algorithms such as SVD and such; how effective or feasible is automatic differentiation in dealing with such cases? 2. The paper discusses computational advantages in the introduction -- however such advantages are not quantitatively analyzed across different platforms or prior softwares. Overall, it is not clear why this work needs to be treated as a scientific paper? It appears to be more of a tutorial on the use of the proposed software. Quality: A simple approach accompanied with a theoretical justification and large number of experimental results. The theoretical justification is spread out in the main body and appendices. The proof given in the appendix is overly short and not detailed enough. The large number of experiment although welcoming needs to be properly discussed and related to the state of the art numbers, including any work that the authors are referring themselves in this submission. The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model. Clarity: The simple approach is clearly described. However, the theoretical justification and experimental results are not.Originality: The work is moderately original.Significance: It is hard to assess given the current submission. In this paper, the authors study the problem if learning for observation, a reinforcement learning setting where an agent is given a data set of experiences from a potentially arbitrary number of demonstrators. The authors propose a method which deploys these experience to initialize a place. Then estimate the value of this policy in order to improve it.The paper is well written and it is easy to follow. Most of the theoretical results are interesting and the derivations are kinda straightforward but not fully matching the main claim in the paper. Mainly the contribution in this paper heavily depends on an assumption that Q^D and Q^\beta are close to each other. This assumption simplifies the many things resulting in a simple algorithm. But this assumption is too strong while the main challenge in the line of learning from observation comes from the fact that this assumption does not hold. Under this assumption and the similarity in distributions mentioned in proposition 4.2 make the contribution of this paper significantly weak.Please let me know if you do not actually use this assumption in your results and justification. This papers introduces a quantization scheme for the back-propagation algorithm to reduce the bit size in the target neural networks. While the paper introduces one way to bring the quantization inside the training procedure and shows the tradeoff between number of bits and the accuracy, the paper is poorly written so it is hard to understand the paper's main proposal.So I would recommend to re-organize the paper and introduce one toy example to illustrate how the proposed method works in the training time and the inference time.Currently the important part, the overall architecture, is explained in the appendix, not in the main paper.The main idea is rather simple, to introduce a quantizer in various components in the back-propagation algorithm.I think we need a clear explanation on "how to" quantize each tensor in each quantizer, instead of many obscure terms in the section 2 and 3. Also the important numbers are in the appendix C, but their meanings are hard to understand.Also in general, quantization is one way of reducing training and inference computational complexity. There are other ways of achieving the same purpose such as distillation to a smaller network (less parameters), etc, so in order to argue the computational gains over this obvious approach, we need a training time and inference time benchmark. This paper proposes a new approach to construct model-X knockoffs based on VAE, which can be used for controlling the false discovery rate. Both numerical simulations and real-data experiments are provided to corroborate the proposed method.  Although the problem of generating knockoffs based on VAE is novel, the paper presentation is not easy to follow and the notation seems confusing. Moreover, the main idea of this paper seems not entirely novel. The proposed method is based on combining the analysis in ''Robust inference with knockoffs'' by Barber et. al. and the VAE.  Detailed comments:1. The presentation of the main results is a bit short. Section 2, the proposed method, only takes 2 pages. It would be better to present the main results with more details. 2. The method works under the assumption that there exists a random variable $Z$ such that $X_j$'s are mutually independent conditioning on $Z$. Is this a strong assumption? It seems better to illustrate when this assumption holds and fails.3. The notation of this paper seems confusing. For example, the authors did not introduce what $(X_j, X_{-j}, \tilde X_j, \tilde X_{-j} )$ means. Moreover, in Algorithm 1, what is $\hat \theta$ and $\hat f$. 4. I think there might be a typo in the proof of Theorem 2.1. In the main equation, why $\tilde Z$ and $\tilde X$ did not appear? They should show up somewhere in the probabilities.5. In Theorem 2.2, how strong is the assumption that $\sup_{z,x} | log (density ratio)| $ is smaller than $\alpha_n$? Usually, we might only achieve nonparametric rate for estimating the likelihood ratios. But here you also take a supremum, which might sacrifice the rate. The paper suggested that $\alpha_n$ can be o( (n \log p)^{-1/2}). When can we achieve such a rate?6. Novelty. Theorem 2.2 seems to be an application of the result in Barber et. al. Compared with that work, this paper seems to use VAE to construct the distribution $ P_{\tilde X| X}$ and its analysis seems hinges on the assumptions in Theorem 2.2 that might be stringent.7. In Figure 1 and 2, what is the $x$-axis?8. A typo: Page 2, last paragraph. "In this paper, we relaxes the ..." # SummaryThis paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work.# StrengthsControlled toy experiments of Deep RL generalization issues:The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper.Investigating the impact of different GANs on the end task:The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below).# WeaknessesDiscrepancy between quantitative and qualitative results:The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the "translated" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games).Does not address the RL generalization issues:Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here).Experimental protocol:The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment.Data efficiency vs actual training efficiency:The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware?# First RecommendationUsing image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions.I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results. summary--The paper focuses on improving object localization, though the title highlights "interpreting deep neural network" which is another area. It analyzes the classifier weights for image classification, and compute the derivative of the feature maps from the network for a sensitivity map of the image. Then it learns a simple linear mapping over the sensitivity map for bounding box regression. Experiments report competitive performance.However, there are several major concerns.1) The paper appears misleading from multiple claims. For example, [abstract] "common approaches to this problem involve the use of a sliding window,... time consuming". However, current state-of-the-art methods accomplish detection in a fully convolutional manner using CNN, and real-time performance is achieved. the paper claims that "computer vision can be characterized as presenting three main tasks... (1) image classification, (2) image localization and (3) image detection". This appears quite misleading. There are way more topics, from low-level vision to mid-level to high-level, e.g., stereo, boundary detection, optical flow, tracking, grouping, etc. Moreover, just in "localization", this could be object localization, or camera localization. Such misleading claims do not help readers learn from the paper w.r.t related work in the community.2) The approach "is rooted in the assertion that any deep CNN for image classification must contain, implicit in its connection weights, knowledge about the location of recognized object". This assertion does not appear obvious -- an reference should be cited if it is from other work. Otherwise, recent work shows that deep CNN can overfit random training data, in which case it is hard to imagine why the object location can be implicitly captured by the CNN [R1]. Similarly, the paper claims that "once weights are found, the gradient... with regard to X would provide information about the sensitivity of the bounding box loss function with regard to the pixels in the images". This is not obvoius either as recent work show that, rather than the whole object, a part of it may be more discriminative and captured by the network. So at this point, why the gradient can be used for object location without worrying that the model merely captures a part? [R1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning requires rethinking generalization, ICLR 2017.[R2] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Learning deep features for discriminative localization, CVPR 2016.3) The paper admits in Section 2.2 that "we have not yet done a formal comparison of these two approaches to constructing the sensitivity map". As the two approaches are suggested by the authors, why not comparing in this paper. It makes the paper less self-contained and not ready to publish. A formal comparison in the rebuttal may improve the rating of the paper.4) In Equation 3, how to represent the bounding box coordinate? Are they any transforms? What does it mean by "bias weights"? Are they different from Cartesian coordinates, or the one used in Equation (2)?5) The experiments are not convincing by merely reporting the metric of IoU&gt;0.5 without any in-depth analysis. Perhaps some visualization and ablation study improve the quality of experiments.6) In Section 3.2, why using two different aggregation methods for producing the final sensitivity map -- max-pool along the channel for PACAL VOC 2017 dataset and avg-pool for ImageNet dataset, respectively? Are there some considerations?7) In Table 1, it shows the proposed method outperforms the other methods significantly, achieving 41% better than the second best method. However, there is no in-depth analysis explaining why the proposed method performs so well for this task. Moreover, from Figure 1 and Figure 3, it is straightforward to ask how a saliency detection model performs in object detection given that the images have clean background and objects are mostly centered in the image.8) What does it mean by "CorLoc (mAP)" in Table 2? As defined in Equation 4, CorLoc acounts the portion of detection whose IoU greater than 0.5 compared to the ground-truth. But mAP accumulates over a range of IoU threshold and precision across classes.9) As the proposed method is closely related to the CAM method, how does CAM perform on these datasets? This misses an important comparison in the paper.10) The readability of the paper should be improve. There are many typos, for example --1. What does "..." mean above and below Equation (2)?2. inconsistent notation, like $w_{ki}$ and ${\bf w}_{ki}$ in Equation (2).3. conflicted notation, w used in Equation 2 and Equation 3. Summary:This paper proposed a new approach for modeling multi-domain dialogue state tracking by incorporating domain-slot relationship using a pre-trained language encoder. The proposed approach are based on using special tokens to mode l such relationship. Two kinds of special tokens are proposed to represent domain-slot pair, DS_merge token for each specific pair, and tokens for every domain and slots separately Comments:1- what is the role of segment embedding (Fig3) for DST tank? Are two different segment used in the pretraining of model?2- Table 1:                  2-1 what type of label cleaning is used to compute joint goal accuracy? From SimpleTOD paper, each baseline has used different label cleaning                  2-2: DS-split is bolded, while it is lower than SimpleTOD3- Table 2: the results indicate that DST performance drops in total by blocking attention across different DS tokens. However, it is not clear how much of the performance drop belongs to turns with cross-domain related slots. The figure 4 only present one example of this case, which might not be correct for all wrong predictions. Also, it is helpful to report DST for single domain and to evaluate the importance of proposed approach. 4- Since the proposed approach can be used on any pretrained encoder, the evaluation on BERT and/or Roberta is helpful to understand the robustness of approach to the choice of pretrained encoder. This paper studies a straightforward generalization of v-optimality from linear regression to (kernel) ridge regression. A standard greedy algorithm is used to optimize the v-optimality criterion.  A simple experiment is conducted comparing the proposed method with random sampling on MNIST. I vote for rejection. This is a simple derivation of the v-optimality of ridge regression (or Bayesian linear regression). The novelty is somewhat limited. For a paper with such limited technical novelty, the empirical studies are too thin; only one experiment is presented, comparing with only a naive random baseline. The title single shot active learning seems a little inappropriate. As far as I understand, the word active in the context of active learning means the model actively query labels from some oracle in an __iterative__ fashion, so it already means sequential I think. Also, it's easy for people to confuse this with "one-shot learning".typo: K=VV^t \in R^{n,n} should be {m,m}?  This paper proposes to use lightFM to solve the matching of the agent and the ticket belonging to some categories.Using lightFM to solve this application is interesting. However, there is no modification on lightFM, except testing three loss fuction. The theoretical contribution and novelty are limited.The experiments are also not enough. They are lightFMs of three loss fuctions, without other compared algorithms. So the statements is better when compared to the traditional approach of agent routing and  outperforms the standard collaborative filtering model in the Conclusion section are not convincing. The main contribution of this paper is a new proposed score to evaluate models that yield uncertainty values for regression.As constituted, the paper can not be published into one of the better ML conferences. The novelty here is very limited. Furthermore there are other weaknesses to the study.First, the stated goal of the "metric" is that "reflects the correlation between the true error and the estimated uncertainty ... (and is) scale independent and robust to outliers." Given this goal (and the name of the paper) it is perplexing why the correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) of true error and predicted error (\sigma_i) was not tried as baseline score. The correlation would have some "scale independence" and I'm sure that there are robust estimates (a simple thing would be to consider the median instead of mean, but there are probably more sophisticated approaches). This just feels like an obvious omission. If one wants to mix both predictive quality and correctness of uncertainty assessments then one could just scale the mean absolute error by the correlation: MAE/Corr, which would lead to a direct comparison to the  proposed MeRCI.Second, the paper does a poor job of justifying MeRCI. On toy data MeRCI is justified by a confluence with existing scores. Then on the depth prediction task, where there are discrepancies among the scores, MeRCI is largely justified qualitatively  on a single image (Figure 4). A qualitative argument on a single instance in a single task can not cut it. The paper must put forth some systematic and more comprehensive comparison of scores.Even with the above issues resolved, the paper would have to do more for publication. I would want to see either some proof of a property of the proposed score(s), or to use the proposed score to inform training, etc. The paper proposes an augmentation of the DDPG algorithm with prioritized experience replay plus parameter noise. Empirical evaluations of the proposed algorithm are conducted on Mujoco benchmarks while the results are mixed.As far as I can see, the paper contains almost no novelty as it crudely puts together three existing algorithms without presenting enough motivation. This can be clearly seen even from the structuring of the paper, since before the experimental section, only a short two-paragraph subsection (4.1) and an algorithm chart are devoted to the description of the main ideas. Furthermore, the algorithm itself is a just simple addition of well-known techniques (DDPG + prioritized experience replay + parameter noise) none of which is proposed in the current paper. Finally, as shown in the experimental sections, I don't see a evidence that the proposed algorithm consistently outperform the baseline.To sum up, I believe the submission is below the novelty threshold for a publication at ICLR. The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST.I am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And its not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful.Moreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. Pros:+ Improving joint training of non-differentiable pipelines is a meaningful and relevant problem+ Using the stochastic computation graph structure to smooth a pipeline in a structured way is a plausible ideaCons:+ The main result of the paper concerning sufficient conditions for optimality of the method seems dubious+ It is not obvious why this method would outperform simple baselines, and baselines for joint training were tried+ The notation seems unnecessarily bloated and overly formal+ The exposition spends too much time on prior work, too little on the contribution, and the description of the contribution is confusingThe submission describes a method for smoothing a non-differentiable machine learning pipeline (such as the Faster-RCNN detector), so that gradient-based methods may be applied to jointly train all the parameters of the pipeline.  In particular, the proposal involves recasting the pipeline as a stochastic computation graph (SCG), adding stochastic nodes to this graph, and then using REINFORCE-style policy gradients to perform parameter learning on the SCG.  It is claimed that under certain conditions, the optimal parameters of the resulting SCG are also optimal for the original pipeline.  The method is applied to optimizing the parameters of Faster-RCNN.I think making non-differentiable pipelines differentiable is an intuitively appealing concept.  A lot of important, practical machine learning systems fall into this category, so devising a nice way to do global parameter optimization for such systems could potentially have significant impact.  In general, we cant hope to make much meaningful progress on the problem of optimizing general nonlinear, differentiable functions, but it is plausible that a method that targets key non-differentiable components for smoothingsuch as this papercould outperform a generic black-box optimizer.  So, I think the basic idea here is plausible and addresses an important problem.Unfortunately, I think this work loses sight of that high-level goal: to me, the key question is whether the proposed approach outperforms any other simple method for global parameter optimization in the presence of nonlinearities and nondifferentiability.  The paper fails to answer this question because no baselines for global parameter optimization were tried.  We can just treat the pipeline as a black box mapping parameters to training set performance, and so any black-box optimization method can be applied to this problem.  It is not clear that the proposed method would outperform an arbitrary black box optimization method such as simulated annealing, Nelder-Mead, cross-entropy method, etc.I think there are also much simpler methods in a similar vein to the proposed method that might also perform just as well as the proposal.  One key conceptual issue here is that reducing the problem to a reinforcement learning problem, as the submission does, is not much of a reduction at all.  First, if the goal is to do global parameter optimization, then we dont really have to smooth the pipeline itself: we can just smooth the black box mapping parameters to performance, and then optimize that with SGD.  There are many ways to do this--if we want to use policy gradient, we can just express the problem as something in this form:min_\phi E_{\theta ~ q_\phi} C(\theta)where C is the black-box mapping parameters \theta to a performance index (such as mean AP), q_\phi is a distribution over parameters (e.g., Gaussian), and \phi are the distribution parameters (e.g., mean, covariance of the Gaussian).  We can then optimize this using REINFORCE policy gradients.If we want to really smooth the pipeline itself, then it is also easy to do this by devising a suitable MDP and then applying REINFORCE with the usual MDP structure.  We simply identify the state s_t at time t with the output of the tth pipeline stage, introduce a new action variable a_t representing a stochastified output, and trivial dynamics (P(s_{t+1} | s_t, a_t) = \delta(s_{t+1} - a_t)).  If the policy is a Gaussian (P(a_t | s_t) = N(a_t; s_t, \Sigma)), then this is similar to relaxing the constraint that one stages output is equal to the input of the next stage, and somehow quadratically penalizing their difference.  In fact, there is a neural network training method based explicitly on this penalization view [A], and it would make yet another great baseline to try.In fact, the proposed method is essentially similar to what I have just described, but it is unfortunately described in an overcomplicated way that obscures the true nature of the method.  I think the whole SCG framework is overkill here.  Too much of the paper is spent just rehashing the SCG framework, and the very heavy notation again just obscures the essential character of the method.If there were, as the paper claims, some interesting condition under which the method produces solutions that are optimal under the original pipeline, that would be remarkable and interesting.  However, I have serious doubts about this part of the paper.  The key problem is the statement that It follows that c(k_c, DEPS_c - k_c) = c(&) + z_c.  The paper seems to be claiming that if E z = 0, then c(k + z) = c(k) + z, which cant possibly be true in general.  The heavy and opaque notation makes it very difficult to understand this section.  Perhaps it would help to consider a very simple example.  Suppose we want to minimize E_{x ~ q} c(y(x)) (where x ~ q means x is distributed as q).  We can introduce only one new stochastic node (k = y + z), between y and c.  Clearly c(y + z) is not generally equal to c(y) + z, even if E z = 0.In summary, I think the submission needs a lot of work on multiple axes before it can make a significant impact.  The most important issues are a complete lack of relevant baselines and the dubious claims about sufficient conditions for optimality.  The idea could have merit, but it needs to be carefully compared and motivated with respect to existing work (such as [A]) as well as the simple baselines I have mentioned.  The presentation also needs to be revised to find the simplest expression of the method and to focus on the interesting parts.[A] Taylor, Gavin, et al. "Training neural networks without gradients: A scalable admm approach." International Conference on Machine Learning. 2016. Focus on navigation problems, this paper proposes Q-map, a neural network that estimates the number of steps (in terms of the discount factor gamma) required to reach any position on the observable screen/window. Moreover, it is shown that Q-map can be applied for exploration, by trying to reach randomly selected goal.Pros1. Novel goal-based exploration schemeCons1. Similar idea has been proposed beforeFor example, Dayan (1993) estimates the number of steps to reach any position on the map using successor representations. Discussion about this field (successor representations/features) is completely missing in the paper.Ref:- Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613624, 1993.- Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In Advances in Neural Information Processing Systems, pp. 40584068, 2017.- Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features and generalised policy improvement. In International Conference on Machine Learning, pp. 510519, 2018.2. Comparison to existing methods is only vaguely discussedFor example, it is claimed multiple times that UVFA requires the goal coordinates, but Q-map also requires coordinates when doing the exploration.3. The network architecture is not clearly presentedFor example, the output of the network needs to be clipped, which suggests that there is no output transform. Since the predicted output is in [0,1], it would make sense to use Sigmoid transform for each pixel and use logistic loss.4. The proposed exploration scheme could be unnecessarily complicatedSec.3.1 provides lengthy discussion about the drawback of eps-greedy exploration. Then in Sec.3.2, \epsilon_r is basically the same as the eps-greedy algorithm, using to randomly select an action. Isn't this a "bad" thing as suggested in Sec.3.1? Moreover, the new exploration scheme requires two more hyper-parameters (min/max distance threshold), which will add more complication to the already very complicated deep RL learning procedure.5. Experiment results are limitedFor the toy experiment in Sec.2.3, the map are relatively simple. The example of Dayan (1993) with an agent surrounded by walls is an interesting scenario and should be included. The proposed Q-map (ConvNet) could fail because it is hard to learn geodesic distance with only local information. More importantly, there is no comparison to similar methods in Sec.3. UVFA can replace Q-map to do similar exploration.6. Writing can be greatly improvedThere are many grammar errors. To name a few, "agent capable to produce", "the gridworld consist of", "in the thrist level".Minors- UFV should be UVF in the introduction- Citation in Sec.3 is not consistent with the rest of the paper. Use \citep or \citet properly. Stating the observation that the RL agents with neural network policies are likely to be fooled by adversarial attacks the paper investigates a way to decrease this susceptibility.   Main assumption is that the environment is aware of the fact that the agent is using neural network policies and also has an access to those weights. The paper introduces a poisoning attack and a method to incorporate defense into an agent trained by DQN.  Main idea is to decouple the DQN Network into what they call a (Student) policy network and a Q network and use the policy network for exploration. This is the only novelty in the paper. The rest of the paper builds upon earlier ideas and incorporates different training techniques in order to include defense strategies to the DQN algorithm. This is summarized in Algorithm 1 called DadQN. Both proposed training methods; adversarial training and Provable robust training are well known techniques. The benefits of the proposed decoupling is evidenced by the experimental results. However, only three games from the Atari benchmark set is chosen, which impairs the quality of the evidence. In my opinion the work is very limited in originality with limited scope that it only applies to one type of RL algorithm combined with the very few set of experiments for supporting the claim fails to make the cut for publication.Below are my suggestions for improving the paper.1. Major improvement of the exposition  a. Section 2.2 Agent Aware Game notation is very cumbersome. Please clean up and give an intuitive example to demonstrate.  b. Section 3 title is Our Approach however mostly talks about the prior work. Either do a better compare contrast of the underlying method against the  previous work with clear distinction or move this entire discussion to related work section.2. Needs more explanation how training with a defending strategy can achieve better training rewards as opposed to epsilon greedy.3. Improve the exposition in Tables 1 and 2. It is hard to follow the explanations with the results in the table. User better titles and highlight the major results.4. Discuss the relationship of adversarial training vs the Safe RL literature.5. Provide discussions about how the technique can be extended into TRPO and A3C. This paper investigates the use of techniques for improving neural network training (regularization, normalization of covariance, sparsity) in terms of their generalization properties, empirically and analytically. The claim is that most of these tools do not help improve performance, with the exception of mutual information.Pros: It's interesting to investigate and compare these different "regularization" techniques and compare them on different tasks empirically.Cons:Many of the points made in the paper are not properly capturing the nuance in the "conventional wisdom", and although it's good to be reminded and the empirical results are interesting to look at, in fact these are not really new discoveries, and sometimes the conclusions are very misleading. 1) There is test loss, and there is generalization loss, and it isn't exactly the same thing. For a hypothesis class H, we havetest loss &lt;= train loss + generalization loss where train loss measures how well we've fit a particular sample, and generalization loss measures how well a model that is trained on one sample can fit a new sample. Note that if I apply L2 loss to my model and have a regularization parameter --&gt; infty, my train loss is huge but my generalization loss is 0. In other words, for a large enough regularization parameter, most of the methods experimented here WILL limit effective capacity and minimize generalization loss; it just will not give you the best test error performance. So the distinction here should be made much clearer--conventional wisdom for regularization limiting generalization error is not wrong.2) The point that is trying to be made in section 3.1 is somewhat well-known in the general optimization literature. Let's consider a much simpler example: linear regression min_x ||Ax-b||_2^2 + gamma ||x||_2^2Let's consider first no regularization, gamma = 0. Assume that A has a huge nullspace. Then technically there are an infinite number of globally optimal solutions x, although if we solve this problem using SGD starting with x = 0, it is known that the minimum norm solution is always picked. You can also think of this as whitening, since large lambda smooths the spectrum of the Hessian. Now add in regularization. Now the solution is unique, even if A is ill-conditioned. It's true that it isn't super necessary to add this regularization, since SGD can get you a good solution, but now we can GUARANTEE that the generalization error is 0. In practice, also, regularization adds stability to the numerics. In deep learning, the hidden layer z also acts as a coefficient matrix for determining y. I assume that is why people pursue low correlation, since it affects the conditioning of z. 3) Comments like  "for scaling and permutation, their influences are rather insignificant" seem a bit careless to me. In fact it is well known that scaling can affect training performance significantly. But of course, if I know the global solution for z which feeds into a softmax, then any scaling on z does not affect the output of the softmax. That, however, does not mean I don't care about the scaling of z when training. 4) Sparsity and rank BOTH limit the degrees of freedom. In fact, sparsity makes more sense when optimizing a nonlinear objective, which is always the case in deep learning. The reason to limit rank is when you wish to "learn your codewords", e.g. the eigenvectors, whereas in sparsity, the "codewords" are already learned, and you just learn the weighting. But if the codewords span the space, they both have equal representability. 4) It is not clear to me what the task is in 4.3. What is the "accuracy" in a data generation task? Is this the normal classification task? If so, is the accuracy reported train or test accuracy? How exactly is generalization error being measured? 5)I am not clear as to what conclusion is being drawn in section 5, with the last sentence "obviously, many of the statistical characteristics become meaningless for such a scalar representation, and it is high time to reconsider the so-called conventional wisdom on representation characteristics." why is this conclusion drawn based on the observation that, if a scalar z perfectly correlates with y, in fact this is the most generalization neural network? 6) Table 4: how did you choose your hyperparameters? (regularization performance is extremely sensitive to parameter choice.)7) A major concern is that basically very little training is done in these comparisons, except in the very last section. As I previously mentioned, many of these regularization / normalization techniques are also meant to better condition the optimization itself, and thus this advantage should not be discarded. minor comments: - page 5 last sentence "characteristics" should be singular - page 8 first sentence "to [a] deep network's performance" This paper introduces a new framework for learning an interpretable representation of images and their attributes. The authors suggest decomposing the representation into a set of 'template' latent features, and a set of attribute-based features. The attribute-based features can be either 'free', i.e. discovered from the data, or 'fixed', i.e. based on the ground truth attributes. The authors encourage the decomposition of the latent space into the 'template' and the 'attributes' features by training a discriminator network to predict whether the attributes and the template features come from the same image or not.While the idea is interesting, the paper is lacking an experimental section, so the methodology is impossible to evaluate. Furthermore, while the authors spend many pages describing their methodology, the writing is often hard to follow, so I am still confused about the exact implementation of the attribute features \phi(x, m) for example. The authors do point to the Appendix for their Experiments section, however this is not a good idea. The paper should be self-contained and the authors should not assume that their readers will read the information presented in the Appendix, which is always optional. Unfortunately, even the experimental section presented in the Appendix is not comprehensive enough to evaluate the proposed method. The authors train the model on a single dataset (MNIST), no baseline or ablation results are presented, and all the results are purely qualitative. Given that the ground truth attribute decomposition for MNIST is not known, even the qualitative results are impossible to evaluate. I recommend that the authors present quantitive results in the updated version of their paper (i.e. disentanglement metric scores, the log-likelihood of the reconstructions), including new experiments on a dataset like dSprites or CelebA, where the ground truth attributes are known. This paper aims at solving the problem of estimating sparse rewards in a high-dimensional input setting. The authors provide a simplified version by learning the states from demonstrations. This idea is simple and straightforward, but the evaluation is not convincing. I am wondering if this approach still works in more general applications, e.g., when state distributions vary dramatically or visual perturbations arise in the evaluation phase.  In addition, it is weird to use adversary scheme to estimate rewards. Namely, the agent is trying to maximize the rewards, but the discriminator is improved so as to reduce rewards. In section 3, the authors mention an early termination of the episode, this is quite strange in real applications, because even the discriminator score is low the robot still needs to accomplish the task.Finally, robots are subject to certain physical constraints, this issue can not be addressed by merely learning demonstrated states. &lt;Summary&gt;: This paper presented a method for incorporating binary relationship between objects (relative location, rotation and scale) into single object 3d prediction. It is built on top of previously published work of [a] and used same network architecture and loss as of [a] and only added the binary relations between objects for object 3d estimation. The results are shown on SUNCG synthetic dataset and only *4 image* instances of NYUv2 dataset which is very small for a computer vision task.[a] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2d image of a 3d scene. In CVPR, 2018.&lt;Pros&gt;: The paper tackles a problem of obvious interest to computer vision research community. It shows better results compared to previous similar work of [a] without considering binary relation between objects.&lt;Cons&gt;:*Technical details are missing:The set of known and unknown variables are not clear throughout the paper:-The extrinsic camera parameters are known or estimated by the method?-The intrinsic camera parameters are known or estimated by the method?-What are the properties of ground truth bounding boxes in 2D camera frame and 3D space?-What is the coordinate of translation? is it in camera coordinate or world coordinate?-What are the variations of camera poses in training and testing for synthetic dataset and how are the samples generated? Are the train/test images generated or are rendered images from previously published work of [b] used?[b] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In CVPR, 2017.*The proposed method is trained on synthetic dataset of SUNCG and their object relations have biases from scene creators. While using binary relation between objects increase the recall in prediction it can also make the predictions bias to the most dominant relations and decrease the precision of detection in rare cases in synthetic dataset. Also, such bias can decrease prediction precision in images of real scenes. *One of the main issues in this paper is that the result of fully automated pipeline versus having ground-truth annotation at test time are mixed up. For example, in the teaser figure (Figure 1-b), does the proposed method use ground truth bounding boxes or not? It is mentioned in figure caption: (b) Output: An example result of our method that takes as input the 2D image and generates the 3D layout.. Is the input only 2D image or 2D image + ground truth object bounding boxes?In order to make sure that reader understands each qualitative result, there should be a column showing the Input to the pipeline (Not Image). For example, in Figure 3 and Figure 4, the image overlaid with input ground-truth bounding boxes should be shown as input to the algorithm. *The experiments and results does not convey the effectiveness of the proposed approach. There are major issues with the quality of the experiments and results. Here are several examples:- Missing baseline: Comparison with the CRF-based baseline is missing. This statement is not convincing in the introduction: One classical approach is to use graphical models such as CRFs. However, these classical approaches have usually provided little improvements over object-based approaches. For a fair comparison with prior works, reporting results on a CRF-based baseline using similar unary predictions is necessary. -The experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted.-If the ground truth boxes are given at the test time, it means that the ground truth binary relations between objects are given and it makes the problem trivial.-It is not clear what is the ground truth box in experimental setup. Is it amodal object box or the ground truth box contains only the visible part of the object? -The qualitative results shown in Figure 4 have full objects in voxel space with predicted rotation, scale and translation. In the qualitative result of Figure 3 and Figure 5 the voxel prediction is shown as final output. Why the result of full object in voxel space with predicted (rotation, scale and translation) is not shown in Figure 3 and Figure 5 and why it is shown in Figure 4?*Very limited results on real images:-Quantitative result on a dataset of real images is missing. The results on synthetic datasets is not a good proxy for the actual performance of the algorithm in real use cases and applications.- The paper only shows few results of NYUv2 on known ground truth boxes. The errors in object detection can be propagated to the 3D estimation therefore these qualitative results are not representative of the actual qualitative performance of the proposed algorithm. Several randomly selected qualitative results on a dataset of real images without ground-truth boxes are needed for evaluating the performance of the proposed method on real images. -Reporting variation in all parameters of scale, rotation and translation is necessary in order to find the difficulty of the problem. For example, what is the distribution of object scale in different object categories. What is the error of scale prediction of we use mean object scale for each object category for all object instance at test set?*Unclear statements and presentation:- It is mentioned in the paper: While the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and non-linearity of the manifold.-Is it necessary for the relative rotation to be formulated to classification task? -If not the comparison of modeling relative rotation via linear constraints is missing.- In some of the tables and figures the know ground-truth boxes/detection setting are in bold face and in some cases are not. This should be consistent throughout the paper. This paper designed a GapNet-PL architecture and applied GapNet-PL, DenseNet, Multi-scale CNN etc. to the protein image (multi-labels) classification dataset.Pros:1. The proposed method has a good performance on the given task. Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance.Cons:1. The novelty of the proposed architecture is limited. The main contribution of this work is the application of CNN-based methods to the specific biological images.2. The existing technical challenge of this task is not significant and the motivation of the proposed method could be hardly found in this paper. 3. The baselines are not convincing enough. Since the performance of Liimatainen et al. is calculated on a different test dataset, the results here are not comparable. The prediction from a human expert, which may vary from individuals, fails to provide a confident performance comparison.4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited. This paper presents a new benchmark for studying generalization in deep RL along with a set of benchmark results. The benchmark consists of several standard RL tasks like Mountain Car along with several Mujoco continuous control tasks. Generalization is measured with respect to changes in environment parameters like force magnitude and pole length. Both interpolation and extrapolation are considered.The problem considered in this paper is important and I agree with the authors that a good set of benchmarks for studying generalization is needed. However, a paper proposing a new benchmark should have a good argument for why the set of problems considered is interesting. Similarly, the types of generalization considered should be well motivated. This paper doesnt do a good job of motivating these choices.For example, why is Mountain Car a good task for studying generalization in deep RL? Mountain Car is a classic problem with a two-dimensional state space. This is hardly the kind of problem where deep RL shines or is even needed at all. Similarly, why should we care whether an agent trained on the Cart Pole task can generalize to a pole length between 2x and 10x shorter than the one it was trained on without being allowed to update its policy? Both the set of tasks and the distributions of parameters over which generalization is measured seem somewhat arbitrary.Similarly, the restriction to methods that do not update its policy at test time also seems arbitrary since this is somewhat of a gray area. RL^2, which is one of the baselines in the paper, uses memory to adapt its policy to the current environment at test time. How different is this from an agent that updates its weights at test time? Why allow one but not the other?In addition to these issues with the proposed benchmark, the baseline results dont provide any new insights. The main conclusion is that extrapolation is more difficult than interpolation, which is in turn more difficult than training and testing on the same task. Beyond that, the results are very confusing. Two methods for improving generalization (EPOpt and RL^2) are evaluated and both of them seem to mostly decrease generalization performance. I find the poor performance of RL^2-A2C especially worrisome. Isnt it essentially recurrent A2C where the reward and action are fed in as inputs? Why should the performance drop by 20-40%?Overall, I dont see the proposed tasks becoming a widely used benchmark for evaluating generalization in deep RL. There are just too many seemingly arbitrary choices in the design of this benchmark and the lack of interesting findings in the baseline experiments highlights these issues.Other comments:- Massively Parallel Methods for Deep Reinforcement Learning by Nair et al. introduced the human starts evaluation condition for Atari games in order to measure generalization to potentially unseen states. This should probably be discussed in related work.- It would be good to include the exact architecture details since its not clear how rewards and actions are given to the RL^2 agents. This paper argues that analyzing loss surfaces in parameter space for the purposes of evaluating adversarial robustness and generalization is ineffective, while measuring input loss surfaces is more accurate. By converting loss surfaces to decision surfaces (which denote the difference between the max and second highest logit), the authors show that all adversarial attack methods appear similar wrt the decision surface. This result is then related to the statistics of the input Jacobian and Hessian, which are shown to differ across adversarially sensitive and robust models. Finally, a regularization method based on regularizing the input Jacobian is proposed and evaluated. All of these results are shown through experiments on MNIST and CIFAR-10.In general, the paper is clear, though there are a number of typos. With respect to novelty, some of the experiments are novel, but others, including the improved training method, have been explored before (see specific comments for references). Finally, regarding significance, many of the insights provided this paper are true by definition, and are therefore unlikely to have a significant impact. While I strongly believe that rigorous empirical studies of neural networks are essential, this paper is lacking in several key areas, including framing, experimental insights, and relation to prior work, and is therefore difficult to recommend. Please see the comments below for more detail. Major comments:1) In the beginning of the paper, adversarial robustness and generalization are equated. However, adversarial robustness and generalization are not necessarily equivalent, and in fact, several papers have provided evidence against this notion, showing that adversarial inputs are likely to be present even for very good models [5, 6] and that adversarially sensitive models can often generalize quite well [2]. Moreover, all the experiments within the paper only address adversarial robustness rather than generalization to unperturbed samples.2) One of the main results of this paper is that the loss surface wrt input space is more sensitive to adversarial perturbations than the loss surface wrt parameter space. Because adversarial inputs are defined in input space, by definition, the loss surface wrt to the input must be sensitive to adversarial examples. This result therefore appears true by definition. Moreover, [3] related the input Jacobian to generalization, finding a similar result, but is not discussed or cited.3) The main result of Section 3 is that all adversarial attacks utilize the decision surface geometry properties to cross the decision boundary within least distance. While to my knowledge the decision surface visualization is novel and might have important uses, this statement is again true by definition, given that adversarial attack methods try to find the smallest perturbation which changes the network decision. As a result, all methods must find directions which are short paths in the decision surface. It is therefore unclear what additional insight this analysis presents. 4) How does measuring the loss landscape as an indicator for adversarial robustness differ from simply trying to find adversarial examples as is common? If anything, it seems it should be more computationally expensive as points are sampled in a grid search vs optimized for. 5) The proposed regularizer for adversarial robustness, based on regularizing the input Jacobian, is very similar to what was proposed in [1], yet [1] is not discussed or cited. Minor comments:1) The papers first sentence states that It is commonly believed that a neural networks generalization is correlated to ...the flatness of the local minima in parameter space. However, [4] showed several years ago that the local minima flatness can be arbitrarily rescaled and has been fairly influential. While [4] is cited in the paper, it is only cited in the related work section as support for the statement that local minima flatness is related to generalization when this is precisely opposite the point this paper makes. [4] should be discussed in more detail, both in the introduction and the related work section.2) The paper is quite lengthy, going right up against the hard 10 page limit. While this may be acceptable for papers with large figures or which require the extra space, this paper does not currently meet that threshold. 3) Throughout the figures, axes should be labeled. 4) In section 2.2, it is stated that both networks achieve optimal accuracy of ~90% on CIFAR-10. This is not optimal accuracy and hasnt been for several years [7].5) Why is equation 2 calculated with respect to the logit layer vs the normalized softmax layer? Using the unnormalized logits may introduce noise due to scaling.6) In Figure 8, the scales of the Hessian are extremely different. Does this impact the measurement of sparseness? Typos:1) Introduction, second paragraph: For example, ResNet model usually converges to& should be For example, ResNet models usually converge to&2) Introduction, second paragraph: ...defected by the adversarial noises... should be ...defected by adversarial noise&3) Introduction, third paragraph: ...introduced by adversarial noises... should be ...introduced by adversarial noise&4) Section 3.1, first paragraph: cross entropy based loss surface is& should be cross entropy based loss surfaces is&[1] Jakubovitz, Daniel, and Raja Giryes. "Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization." arXiv preprint arXiv:1803.08680 (2018). ECCV 2018.[2] Zahavy, Tom, et al. "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms." arXiv preprint arXiv:1602.02389 (2016). ICLR Workshop 2018[3] Novak, Roman, et al. "Sensitivity and generalization in neural networks: an empirical study." arXiv preprint arXiv:1802.08760 (2018). ICLR 2018.[4] Dinh, Laurent, et al. "Sharp Minima Can Generalize For Deep Nets." International Conference on Machine Learning. 2017.[5] Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. "Adversarial vulnerability for any classifier." arXiv preprint arXiv:1802.08686 (2018). NIPS 2018.[6] Gilmer, Justin, et al. "Adversarial spheres." arXiv preprint arXiv:1801.02774 (2018). ICLR Workshop 2018.[7] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html This paper introduces a variation on temporal difference learning for the function approximation case that attempts to resolve the issue of over-generalization across temporally-successive states. The new approach is applied to both linear and non-linear function approximation, and for prediction and control problems. The algorithmic contribution is demonstrated with a suite of experiments in classic benchmark control domains (Mountain Car and Acrobot), and in Pong.This paper should be rejected because (1) the algorithm is not well justified either by theory or practice, (2) the paper never clearly demonstrates the existence of problem they are trying to solve (nor differentiates it from the usual problem of generalizing well), (3) the experiments are difficult to understand, missing many details, and generally do not support a significant contribution, and (4) the paper is imprecise and unpolished.Main argumentThe paper does not do a great job of demonstrating that the problem it is trying to solve is a real thing. There is no experiment in this paper that clearly shows how this temporal generalization problem is different from the need to generalize well with function approximation. The paper points to references to establish the existence of the problem, but for example the Durugkar and Stone paper is a workshop paper and the conference version of that paper was rejected from ICLR 2018 and the reviewers highlighted serious issues with the paperthat is not work to build upon. Further the paper under review here claims this problem is most pressing in the non-linear case, but the analysis in section 4.1 is for the linear case. The resultant algorithm does not seem well justified, and has a different fixed point than TD, but there is no discussion of this other than section 4.4, which does not make clear statements about the correctness of the algorithm or what it converges to. Can you provide a proof or any kind of evidence that the proposed approach is sound, or how its fixed point relates to TD?The experiments do not provide convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches. There are so many missing details it is difficult to draw many conclusions:1) What was the policy used in exp1 for policy evaluation in MC?2) Why Fourier basis features?3) In MC with DQN how did you adjust the parameters and architecture for the MC task?4) Was the reward in MC and Acrobot -1 per step or something else5) How did you tune the parameters in the MC and Acrobot experiments?6) Why so few runs in MC, none of the results presented are significant?7) Why is the performance so bad in MC?8) Did you evaluate online learning or do tests with the greedy policy?9) How did you initialize the value functions and weights?10) Why did you use experience replay for the linear experiments?11) IN MC and Acrobot why only a one layer MLP?Ignoring all that, the results are not convincing. Most of the results in the paper are not statistically significant. The policy evaluation results in MC show little difference to regular TD. The Pong results show DQN is actually better. This makes the reader wonder if the result with DQN on MC and Acrobot are only worse because you did not properly tune DQN for those domains, whereas the default DQN architecture is well tuned for Atari and that is why you method is competitive in the smaller domains. The differences in the average change in value plots are very small if the rewards are -1 per step. Can you provide some context to understand the significance of this difference? In the last experiment linear FA and MC, the step-size is set equal for all methodsthis is not a valid comparison. Your method may just work better with alpha = 0.1. The paper has many imprecise parts, here are a few:1) The definition of the value function would be approximate not equals unless you specify some properties of the function approximation architecture. Same for the Bellman equation2) equation 1 of section 2.1 is neither an algorithm or a loss function3) TD does not minimize the squared TD. Saying that is the objective function of TD learning in not true4) end of section 2.1 says It is computed as but the following equation just gives a form for the partial derivative5) equation 2, x is not bounded 6) You state TC-loss has an unclear solution property, I dont know what that means and I dont think your approach is well justified either7) Section 4.1 assumes linear FA, but its implied up until paragraph 2 that it has not assumed linear8) treatment of n_t in alg differs from appendix (t is no time episode number)9) Your method has a n_t parameter that is adapted according to a schedule seemingly giving it an unfair advantage over DQN.10) Over-claim not supported by the results: we see that HR-TD is able to find a representation that is better at keeping the target value separate than TC is . The results do not show this.11) Section 4.4 does not seem to go anywhere or produce and tangible conclusionsThings to improve the paper that did not impact the score:0) Its hard to follow how the prox operator is used in the development of the alg, this could use some higher level explaination1) Intro p2 is about bootstrapping, use that term and remove the equations2) Its not clear why you are talking about stochastic vs deterministic in P33) Perhaps you should compare against a MC method in the experiments to demonstrate the problem with TD methods and generalization4) Section 2: can often be a regularization term &gt;&gt; can or must be?5) update law is a odd term6) tends to alleviate &gt;&gt; odd phrase7) section 4 should come before section 38) Alg 1 in not helpful because it just references an equation9) section 4.4 is very confusing, I cannot follow the logic of the statements 10) Q learning &gt;&gt; Q-learning11) Not sure what you mean with the last sentence of p2 section 512) where are the results for Acrobot linear function approximation13) appendix Q-learning with linear FA is not DQN (table 2) This paper proposes a method for multi-document abstractive summarization. The model has two main components, one part is an autoencoder used to help learn encoded document representations which can be used to reconstruct the original documents, and a second component for the summarization step which also aims to ensure that the summary is similar to the original document. The biggest problem with this paper is in its evaluation methodology. I don't really know what any of the three evaluation measures are actually measuring, and there is no human subject evaluation back them up.- Rating Accuracy seems to depend on the choice of CLF used, and at best says whether the summary conveys the same average opinion as the original reviews. This captures a small amount about the actual contents of the reviews. For example, it does not capture the distribution of opinions, or the actual contents that are conveyed.- Word Overlap with the original documents does not seem to be a good measure of quality for abstractive systems, as there could easily be abstractive summaries with low overlap that are nevertheless very good exactly because they aggregate information and generalize. It is certainly not appropriate to use to compare between extractive and abstractive systems.-There are many well-known problems with using log likelihood as a measure of fluency and grammaticality, such as biases around length, and frequency of the words.It also seems that these evaluation measures would interact with the length of the summary being evaluated in ways which systems could game.Other points:- Multi-Lead-1: The lead baseline works very well in single-document news summarization. Since this model is being applied in a multi-document setting to something that is not news, it is hard to see how this baseline is justified.- Despite the fact that the model is only applied to product reviews, and there seem to be modelling decisions tailored to this domain, the paper title does not specify so, which in my opinion is a type of over-claiming.Having a paper with poor evaluation measure may set a precedent that causes damage to an entire line of research. For this reason, I am not comfortable with recommending an accept. The paper presents a new Generative Adversarial Network (GAN) for learning a  target distribution that is defined as the difference between two other distributions. Applications in semi-supervised learning and adversarial training are considered in the experimental evaluation and results are presented in computer vision tasks. The paper is not very well written and can be hard to follow. One very important issue for me was motivation for defining the target distribution as a difference between two other distributions. I am not familiar with this area, but reading through the introduction it was never clear to me why this is a useful scenario, in practice. Furthermore, some statements in the introduction felt quite arbitrary. For example, the authors state that PixelCNN "does not have a latent representation" in a manner that makes it sound as if that is a bad thing. If indeed it is, then why so? It would be very helpful to motivate the setting more and to provide a couple of examples of where this method would be useful, in the introduction. Also, regarding the MNIST example in the end of page 1, what is the "universal set"? This paragraph also felt a bit arbitrary and unclear.Some comments about the rest of the paper:  - The theoretical results of section 3 are just stated/listed, but are not     connected to algorithm 1. Please connect them to the different parts of the     algorithm and state in a couple sentences what they imply for the algorithm.  - Right after theorem 1, which assumption are you referring to when you say     "the assumption in Theorem 1"?  - The reformulation of section 3.1 is never justified. What led you to use     this reformulation and why do you think it is more stable in practice?  - You should mention in the caption of table 4, what quantity you are     computing.Note that my evaluation for this paper is based mainly on the way it is written as, in its current state, it is hard for me to judge what is novel and what is useful, and what readers are supposed to take in by reading this paper. The main question that the paper definitely needs to answer, but does not do so currently (in my opinion) is:  When is this method useful to readers? For solving which problems and under   what conditions? And also, when is this method bad and should not be used?== Experiments ==Section 5.1 is hard to follow and I don't quite get how it connects to the rest.Also, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your method does not need to rely on an additional density estimation network. Even if that is true, I cannot see how it is a useful remark given that the method of Dai et al. seems to always beat your method.== Style ==In figure 1, no labels or legends are provided making it hard to figure out what's going on at a glance. It would be very helpful to include labels and a legend.Equation 2 is not written correctly. The equals sign only refers to "V(G, D)" and not the min-max of that, right? Please make that explicit by first defining "V(G, D)" alone. This paper proposed to solve the instance-based transfer learning and feature-based transfer learning by stacking with a two-phase training strategy. The source data and target data are hybrid together first to train weak learners, and then the ensembled super learner is utilized to get the final prediction. Details for the stacking process are provided. Experimental results on MNIST-USPS, COIL, and Office-Caltech datasets show the proposed method can boost the performance, compared to TrAdaboost. Pros:The paper proposes to using stacking or ensembling to solve the domain adaptation problem, which shows some insight for further domain adaptation research.Cons:1. One of the main issues of this paper is the lack of novelty. The framework is incremented from the previous domain adaptation method such as TrAdaboost or BDA. For feature-based transfer learning, Equation (7)(8)(9) directly from the previous method. 2. Some arguments in this paper are not solid. For example, in the abstract,   the authors claim that under the two-stage training architecture, the fitting capability and generalization capability can be guaranteed at the same time. However, this is not well-justified in the following literature. Another example is "the settings of \lamda and N should be taken into consideration, if \lambda is too large, the performance of each learner can't be guaranteed, if \lambda is too small, training data can't be diversified enough" (page 7line 9~11)3. This paper is weakened by the experimental part. Firstly, only TraDaboost method is used as a baseline. The paper can be largely improved by comparing with the state-of-the-art ensembling method for domain adaptation, for example:Self-ensembling for visual domain adaptation, Geoff French,  ICLR 2018.Secondly, the datasets used in this paper is small-scale and biased. It would be exciting to see how the proposed method will perform on the state-of-the-art large-scale domain adaptation dataset, for example, Office-Home dataset, Syn2Real dataset.  Others:1. Some terminologies used in this paper are confusing: (1) the h_t and c are not defined in Equation (2). in Algorithm 2, how to construct kernel matrix K_t using k_t?   2. The written of this paper can be largely improved. Some sentences are grammarly mistaken. Typos examples: Abstract line 1: overtting -&gt; overfittingSection 2.1, we use TraAdaboost -&gt; We use TrAdaboost3. The citation style used in this paper is not correct.Problems:1. In section 2.2, what's the difference between the kernel matrix K with the unbiased estimate of MK-MMD (proposed by Gretton, NIPS 2012, also used in Deep adaptation network, Long, et al. ICML2015)? The paper describes a new loss function for training, that can beused as an alternative to maximum likelihood (cross entropy), oras a metric that is used to fine-tune a model that is initiallytrained using ML.Experiments are reported on the WMT 2014 English-German andEnglish-French test sets.I think this is an idea worth exploring but overall I would notrecommend acceptance. I have the following reservations:* I found much of the motivation/justification for the approachunconvincing - too heuristic and informal. What does it meanto "overgeneralize" or "plunge into local optima"? Can we sayanything semi-formal about this alternative objective? * The improvements over ML are marginal, and there are a lot of movingparts/experimental settings in these models, i.e., a lot oftweaking. The results in tables 2 and 3 show a 0.36/0.34 improvementover ML using DSD. (btw, what is meant by "DSD-deep" or "ML-deep"? I'mnot sure these terms are explained?)* The comparison to related work is really lacking. The "Attention isall you need" paper (Vaswani et al.) reports 28.4/41.0 BLEU for thesetest sets, respectively 3.4/5.96 BLEU points better than the resultsin this paper. That's a huge gap. It's not clear that the improvements(again, less than 0.5 BLEU points) will remain with a state-of-the-artsystem. And I think the paper is misleading in how it cites previousresults on these data sets - there is no indication in the paper thatthese better results are in the literature.Some small things:* unplausible -&gt; implausible* "Husz (2015) showed that D(P || Q) is not identical to its inverse formD(Q || P)" this is well known, predating 2015 for sure. The paper uses a number of deep learning approaches to analyse sets of Traffic data. However, as these sets of traffic data are never explained it is difficult to follow or understand what is going on here.Some major comments:1) Many of the key concepts in the paper are not discussed. The primary one would be that of what the two data sets contain. Without knowledge of this it is difficult to ascertain what is going on. 2) Many of the processes used are not described in enough detail to either understand what is going on or to re-produce the work. Without this it is difficult to make headway wit the work.3) It is not clearly articulated what the experiments performed are doing. For example, how have you applied the other techniques to this data?4) Key terms are not defined. Such as Traffic Flow.5) The English structure of the paper is poor with many mistakes. A thorough proof-reading is essential.Some more specific points:- "with the larger road network, the difficulty of flow forecasting grows." - This seems to be a consequence of the other ones not a challenge in it's own right.- What is "superiority"?- "Spatiotemporal traffic flow forecasting task is currently under a heated discussion and has attracted a large research population." - evidence to back up this statement.- Your contributions aren't contributions, but rather a list of what you have done.- How does your related work relate to what you have done?- Hard to parse "To extract temporal relationships within the history traffic flows, we model this process as a layering structure with autoencoder as cell"- Appendices B and C should be in the main paper.- What is in x^{(1)}?- "When take the sparsity constrains into consideration" - what are the sparsity constraints?- How do you obtain the weights?- Figure 2 should come much sooner as it relates a lot of the concepts together.- "On both datasets, we slice traffic flow information into 15 minutes windows, where 70% of data is for training, 10% for validation and remaining 20% for testing." - Is that each 15 mins is split 70:10:20?- Proof by example is not a proof. ICLR 2021 Review - Better Sampling in Explanation DieselgateSummary: The paper suggests to replace the perturbations part for the existing post-hoc explanation methods like LIME and SHAP with on-data manifold sampling methods. SHAP and LIME use perturbations or randomly generated points to explain the decision of the black-box models. These points are out-of-distribution data, that leads to a new avenue for adversarial behavior discussed in Slack et al. (2020). The authors use existing data generators to produce better perturbations. They further empirically evaluate the robustness of explanations generated after proposed changes on real-life datasets.Comments:1. It is not clear what exactly the contribution of the paper. The problem is identified by existing papers [slack et al (2020), https://arxiv.org/pdf/2007.09969.pdf], etc, and mentioned that such attacks fail trivially if perturbations are from data distributions. The data generators are used from the existing literature. A recent paper [https://arxiv.org/pdf/2006.01272.pdf] proposes more efficient and theoretically sound on-data manifold SHAP computations.  2. The definition of robustness is not formally stated in the paper. The usual robustness in explanations [https://arxiv.org/pdf/1806.08049.pdf] bounded/negligible change in the explanation if the point of interest it changed slightly. It is not clear how random perturbations around the point of interest affect robustness.3. The evaluations in the paper are weak, it is trivial that if perturbations are from data distributions the attack proposed in Slack et. al (2020) will fail (it is discussed in Slack et. al (2020) as well). Moreover, the paper does not evaluate the effects of used sampling methods in explanation. The data generating model itself a black-box model and involves more uncertainties in explanations. Minor: why one cant use the training dataset itself to generate model explanations rather than using black-box data generators?    ### SummaryThis paper proposes to apply uncertainty-based measures to guide the collection of training samples for reading comprehension. The paper describes a relatively simple metric to estimate model uncertainty of unlabeled examples, and develops an algorithm to sample examples that exhibit least model certainty. They describe a learning and regularization model for this scenario and evaluate their proposal on SQuAD and NewsQA datasets.### Strong and weak pointsStrong points:- The use of uncertainty-based measures for active learning seem intuitive and elegant,- The experimental evaluation contains a few interesting baselines and provide context to this work### Weak points:- The idea of using model uncertainty for sample efficiency is somewhat standard now. A few missing references on this are, for example [1], [2] and [3]. It is even described in blog articles online [4].  After reading the paper, it was not clear what part of the proposal is really novel, and which parts are well established in the literature.Part of the work described seems to be dedicated to performing learning while sampling in a loop (Algorithm 1). Unless Ive misunderstood the work, this assumption seems to be quite different than the premise of the work. After running an iteration of model training, new samples may be requested for annotation. In a real application, it would take hours, or most likely days, to receive new examples with labels. As such, Im not sure it makes sense to design an algorithm that expects the new samples on every batch/iteration.  Given the time budgets, I would expect that models can be easily trained from scratch after new examples are available.- I found the paper hard to follow at times. For example:    - In Section 2.1.1 there seems to be use of informal notation: We use a zero-one vector a to indicate the correct answer, . But at this point in the paper its unclear what the vector space represents (tokens in the input?). Later, in Section 3, a span-based representation is introduced.    - The thresholding logic was hard to follow and understand the rationale. Are you simply sampling amongst the most uncertain samples? Can you not specify a $k$ to sample from? Why have an adaptive threshold?    - The baselines models are not really explained clearly, with a single sentence per model,- Overall, the results are quite inconclusive. Although there is modest improvement in SQuAD benchmark, it does not seem to be the case for NewsQA (the NewsQA results are only available as a chart, which makes comparison difficult). Also, in the SQuAD setup, the Random sampling method seems to generally be as good or better than the other baselines. There are no explanations as to why this may happen. Are the results being averaged over several runs? Do they represent the performance of a single run? Could the variations be attributed to natural variation in model training? References used above:[1] Heterogeneous uncertainty sampling for supervised learningDavid D. Lewis and Jason Catletthttps://scholar.google.com/scholar?cluster=9211137857521772693&hl=en&as_sdt=0,33 [2] Deep Bayesian Active Learning for Multiple Correct Outputshttps://arxiv.org/abs/1912.01119 [3] You Need Only Uncertain Answers: Data Efficient Multilingual Question Answeringhttp://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-113.pdf [4] Active learning  Uncertainty Sampling (P3)https://medium.com/@duyanhnguyen_38925/active-learning-uncertainty-sampling-p3-edd1f5a655ac ### RecommendationI would recommend not to publish this work at ICLR at this time.  As described above, it is unclear what the novelty of this study is, besides applying an established technique (uncertainty-based sampling) to machine comprehension tasks. I have concerns regarding the premise of the learning setup (incremental learning assuming new examples can be sampled). Finally, the results seem inconclusive to me, and it is unclear whether statistical variations are dominating effects in SQuAD and NewsQA.### Questions for authorsPlease refer to the weak points described above.### Additional feedback- Besides the technical work, I think the paper could benefit from improved writing. Currently, some parts are a bit hard to read. - Please separate the notation, task definition and model design into separate (sub)sections, so readers can read the paper linearly.- Overall, the writing could use some editorial help.- Page 5, the text describes the best performance for 5k examples as 64.07%, however,  Table 1 states it is 64.03%. In this work the authors propose a two-stage, adversarial training technique to calibrate a semantic segmentation model when faced with conflicting labels in the train set.  Their approach is to first train a segmentation model. Then, it is used to feed a GAN model which itself is trained against a discriminator to produce diverse segmentations that reflect the diversity in the train set.  The authors compare their approach to similar methods on a synthetic data set and two semantic segmentation data sets. Pros:1) The technique is conceptually simple as training a segmentation model with cross entropy loss as well as training a GAN are both well-understood.2) The technique can really be used to calibrate any differentiable semantic segmentation model, regardless of who trained it or how.3) The experimental details are described in enough detail to likely be able to replicate most of the results4) Minus some organizational issues, the writing is good overall.Cons:1) I found the motivation in the introduction to be slightly difficult to follow.  More specifically, the concepts ambiguity in data labels leading to a multi-modal data distribution and the need to model uncertainty need to be more tightly discussed.  As it is written now, the authors first argue for modeling a noisy empirical distribution that captures the ambiguities, which seems counter intuitive since there assumedly exists a single true segmentation of the image then later discuss uncertainty.  I think discussing uncertainty modeling, and specifically calibration, first would alleviate this issue.2) "Isola et al. (2017) have demonstrated that it introduces only minor stochasticity in the output and returns inconsistent samples." (page 2) - In Isola et al. (2017)  dropout is used for image to image translation in a GAN.  I do not think this provides sufficient evidence that using dropout BNNs  such as in Kendal and Gal (2017) has these properties.3) The authors argue that combining the GAN loss and the pixel-wise cross entropy is not well-suited for noisy data because they are often at odds.  It's not clear to me that this is a problem in practice, as the cross entropy will spread probability mass across different conflicting labeled instances.  Assume a pixel in the data set appears twice with two different labels.  The sum of the cross entropy over these two examples is minimized by a model that puts probability 0.5 on each of the two classes.  This seems like the multi-modal behavior the authors argue for.  Perhaps there is something more subtle going on, but the lack of formal analysis makes it difficult to understand how much of an issue it is.  Further, the proposed method is adding a KL Divergence term to the generator loss that is at odds with the GAN loss, which was what the authors argue against.  In short, the argument against the most similar method and the proposed method is weak in my opinion.4) I think the name calibration network is a bit misleading. It seems the calibration network is the base segmentation model and the refinement network is calibrating the model.5) The paper would benefit from an algorithm sketch to explicitly show the two stages of training.6) In the experiments the authors switch to mean squared loss after arguing against cross-entropy.  It's not clear why this is done or why this is a proper baseline.7) Looking at figure 3, it would seem the refinement network does not generate outputs that closely match any of the ground truth annotations, but rather some combinations of them.  In practice, I would assume that someone would use the mean and standard deviation to understand the predictions and not samples, so this is less of an issue, but it highlights an issue with considering each pixel independently.8) While I think the GED metric make sense here, I think calibration (like expected calibration error) or uncertainty focused (like those proposed in (Mukhoti and Gal; 2019)) would be useful to tell a more complete story of the evaluation. 9) Figure 5 is unclear.  The text seems to imply that this shows that their technique does not calibrate their model well.  To me this is a strong argument against their approach.  I am not sure the value of a diverse set of segmentations if the model cannot accurately convey uncertainty in predictions. Summary: randomized smoothing is a method to construct provably robust classifiers via additive Gaussian noises on the input. The authors propose to learn score functions as a means to denoise the randomized image prior to a trained classification model. As the denoising + pre-trained classifier architecture is already proposed, the contribution is only limited to the choice of using a score function. The justification and realization of the method is limited for two main reasons. See below. 1. Efficiency: one of the most critical bottleneck of randomized smoothing methods is the slow prediction time. The score-function based generative / denoising models are known for their slow sampling time, so the proposed method undermines randomized smoothing in efficiency. 2. Many design choices in this paper is not well justified. 2-1) How good does the RHS of Eq. (12) approximates the gradient descent procedure? 2-2) Even if the true gradient descent can be executed, the bound in Eq. (13) seems very bad in high dimension, thus the smoothed classifier will not be accurate unless the pre-trained classifier is already robust in the local region. 2-3) Clearly using the same score function for multiple $\sigma$ is suboptimal. Although the authors mentioned this part as an advantage, but it is not clearly compared to existing methods. Would existing methods fail if they use the same denoising function for multiple $\sigma$? Overall, I vote for rejecting. While most of the related literature focuses on computing the hypergradient, this paper tackles the full optimization problem, which is much harder and more interesting. In particular I really like the idea of taking advantage of the (finite sum) structure of the bilevel optimization problem (algorithm 2). The problem tackled is very important but I have a lot of concerns regarding the novelty of the 'proposed' algorithm 1. Moreover I do not understand the focus on avoiding to compute the inverse of the Hessian, since in such application the Hessian is never inverted, only linear systems are solved, which is much cheaper. ##########################################################################Advantages of the paper:- The paper is overall well written.- The idea of algorithm 2 is elegant.##########################################################################Concerns 1- Authors may have missed an important reference [1], which is a seminal paper on bilevel optimization for hyperparameter optimization.2- Authors claim novelty on algorithm 1 'design a new bilevel optimization algorithm'. I may have missed something important, but it seems to me that algorithm 1 not new: it is exactly the algorithm in [1], Section 4.3- Moreover how do the authors chose the stepsize of the gradient descent $\beta$? IMO this is a strong bottelneck in practice, that is why [2] uses LBFGS once the hypergradient is computed, and [3] uses a line-search procedure.Do authors have heuristics to choose the stepsizes $\beta$ in practice?This is paramount in practice, and the value of the stepsize is hidden in appendix, set to '0.001'. This seems very custom, and not general. How do authors know that the proposed algorithms converged for this choice of stepsize?4- 'proposition shows that the differentiation involves computations of second order derivatives'I do not understand. To my knowledge, the hypergradient $\partial f$ can be computed in 3 differents ways: implicit[3], forward or backward [4].To my knowledge there is not Hessian inversion, neither in the implicit nor in the backward: algorithm 1 in [3] solves a linear system to compute the gradient, and no Hessian are inverted in [1], Section 4. I am very confused with this Hessian inversion problem. Since I did not understand it, I lost a lot of the motivation for the introduction of algorithm 2.5- At the beginning of section 2.1 it seems that the end of an old sentence remains in the text.6- Part 4 is very technical and felt to me like an arid succession of theoretical results. IMO more insights and comments on the theoretical results would be very helpful for the reader.7- Part 5 proposes experiments using algorithm 1, which is already known (unless I missed something). Experiments on algorithm 2 are only in appendix.A comparison with  the approximated procedure of [3] would strength the paper: from my experience, approximated gradient like in [3] drastically improves the convergence time.[1]  Domke, J. Generic methods for optimization-based modeling, AISTAST 2012[2] Deledalle, C. A., Vaiter, S., Fadili, J., & Peyré, G. (2014). Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM Journal on Imaging Sciences[3] Pedregosa, F. (2016). Hyperparameter optimization with approximate gradient, ICML2016[4] Franceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017). Forward and reverse gradient-based hyperparameter optimization. ICML2017 This paper proposes a sentence embedding called AriEL. Specifically, based on arithmetic coding and k-d trees, AriEL maps sequences of discrete data into volumes in the latent space, and can then retrieve sequences by random sampling. AriEL is compared to other standard techniques such as Transformer and Variational Autoencoders. Results show that it can generate more diverse and valid sentences.**Pros:**The idea of constructing sentence encoders by arithmetic coding is interesting and novel. AriEL takes the frequency of sentences into account, and the frequency implicitly models the statistics of human language. I have not seen prior work in this direction, and it is worth exploring.**Cons:**1. My major concern is the evaluation. The evaluation metrics in the paper are very limited and not convincing. They fail to assess the strengths of the proposed sentence embedding. Specific downstream applications are missing here. A solid evaluation should at least include either classification tasks, such as sentiment analysis, textual entailment; or generation tasks, such as simplification, machine translation.2. The paper is hard to follow and the writing has greatly impeded understanding. I will mention a few that confuse me most:\(1) In the second paragraph of Section 3.2, what does but the probabilities defined by the latter are used as a deterministic Russian roulette mean? Does that mean the argmax is one-hot?\(2) What is the purpose of defining biased and unbiased sentences? And in Table 1, what does comply with the bias mean?\(3) In Section 3.4.2, I can hardly believe the definition of grammar coverage. Why is the number of adjective able to represent grammar rules?3. I doubt only modeling the frequency of sentences is enough for a sentence encoder. There are many aspects of language (e.g. taxonomy, lexicon) that should be considered. But again, these are to be proved by downstream tasks.Overall, I suggest rejecting the paper in its current state. # Summary of ContributionsThe paper proposes a formalization for one aspect of what makes adversarial examples suspicious, positing that visible perturbations to the foreground arouse more suspicion. Based on this proposed threat model the paper proposes the dual-perturbation attack, which perturbs the image with small perturbations in the foreground and larger perturbations in the background, with the background and foreground distinguished either by a provided mask or the DeepGaze II model, while maintaining a low saliency score in the background. The authors successfully generate images with low background saliency, and show that adversarial training with the dual-perturbation attack generates a network that is robust to the dual-perturbation attacks.# Score RecommendationI am recommending a rejection for this paper. While the overarching idea of identifying a class of unsuspicious adversarial examples is interesting, the formalization itself (which relies on a computation model, DeepGaze II) is not robust to adversarial perturbations. Furthermore, the experiments conducted do not substantiate the claims made (in the last two sentences of the abstract) about the effectiveness against classifiers robust to conventional attacks and the robustness of models trained via adversarial training with dual-perturbation. Finally, the approach does not consistently succeed at generating adversarial examples that are visually unsuspicious.# StrengthsI appreciated that the paper attempts to introduce a formalization for a new class of attacks going beyond the existing $l_p$ norm-bounded attacks. In addition, the example provided in Figure 1 shows that increasing $\lambda$ _does_ result in perturbations to the background that make it less salient.# Weaknesses## Potential for Attacks on Saliency ModelsOne of my concerns is that the saliency model is designed only for unperturbed images, and thus can itself be attacked. This could lead to cases where the perturbed image is suspicious (specifically, the background is actually salient to humans) but the saliency model has a low score for the saliency of the background. This would compromise one of the key planks of the approach.## Unconnected Experiments### Evaluation of attack strength on robust classifiersThe paper demonstrates that the dual-perturbation attack is successful against robust models. However, the fact that the attacks succeed is not really meaningful, as the models attacked are _not_ trained to be robust against the magnitude of attacks applied.#### AT-PGD networkThe AT-PGD network is trained to be robust to attacks with $l_2$-norm up to $\epsilon = 2.0$ (see the fourth paragraph of page 6. As such, it is unsurprising that it would be vulnerable (as shown in the left and middle plots of Figure 4) to attacks where $\epsilon_B > 10$!#### Randomized Smoothing Networks- $l_\infty$ attacks: Randomized smoothing is intended to generate models robust only against attacks with bounded $l_2$ norm. The success of $l_\infty$ attacks in Table 5 of Appendix J.1 is thus not relevant.- $l_2$ attacks: The total attack budget for the smallest non-zero attack presented in Table 5 of in Appendix J.1 is $\sqrt{(\epsilon_F^2 + \epsilon_B^2)} \approx 1.27$, which is >2.5x the attack budget (127/255) designed for in [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918.pdf). It is thus unsurprising that the attack succeeds (merely because it is so large) even if the budget needs to be distributed between the foreground and the background.#### Suggestion for ImprovementIt would be far more interesting to constrain the _total_ budget for the dual-perturbation attack to be equal to that of the PGD attack. For example, in the $l_2$ case, we would have $\sqrt{(\epsilon_F^2 + \epsilon_B^2)} = \epsilon$. This would mean that the allowed perturbations for the dual-perturbation attack are a strict subset of that for the PGD attack. Having a significant proportion of samples vulnerable to PGD attack are also vulnerable to the dual-perturbation attack would be a convincing demonstration that the dual-perturbation attack is a strong attack (specifically, that perturbations primarily in the background are sufficient to change the classification of these images).### Evaluation of AT-Dual defense against additional attacksFigure 5 shows that AT-Dual is more robust as compared to AT-PGD on $l_\infty$ PGD attacks and $l_0$ JSMA attacks. This is unsurprising since AT-Dual is trained to be robust against perturbations of larger overall magnitudes (and thus should be expected to be more robust against all attacks in general), albeit at a cost to its clean accuracy (of ~5%; see the right plot in Figure 4 for example). #### Suggestion for ImprovementIt would be more interesting if it could be shown that AT-Dual has a clean and robust accuracy comparable to networks trained to be robust against $l_\infty$ and $l_0$ attacks. ## Failure to generate unsuspicious adversarial examplesOne of the focuses of the paper is generating unsuspicious adversarial examples. Unfortunately, limitations of the algorithms computing saliency mean that the proposed attack often fails to achieve this goal; instead adversarial examples generated by the described dual-perturbation attack are often _more_ suspicious. This is the case since the boundaries between the foreground and background regions are sharp, and is worst when the background is relatively uniform and the saliency algorithm incorrectly identifies part of the background as the foreground. Good examples of suspicious adversarial examples generated by dual-perturbation attack can be seen in the STL-10 and ImageNet-10 images in Appendix L. First off, I would like to thank the authors of "Guiding Representation Learning in Deep Generative Models with Policy Gradients" for their neat, well written submission.  Summary of the paper --------------------------------The paper discusses the problem of representation learning in deep RL, with a focus on the effects of pretraing the state embedding function on agent performance. More specifically, the authors consider the case where a variational autoencoder (VAE) is to be trained to embed the state, and three training regimes are compared:1. The VAE is pretrained on a dataset of episodes - collected separately with a pretrained agent. It weights are then frozen, and a policy is trained online.2.  The VAE is pretrained on a dataset of episodes - collected separately with a pretrained agent. Its weights are then refined durign the training a policy online (gradients of the policy are backpropped into the VAE encoder).3. The VAE and the policy are traiend end-to-end online jointly.The authors' main claim is not only possible, but that pretraining without refinemenet / joint optimization can potentially result in the VAE discarding essential details needed for policy/value optimization; the claim is supported by: 1. Comparing the three training regimes on Breakout using a PPO;2. Providing a visualization of the PPO value function highlighting how in the regimes in 2 and 3 details of the game that intuitively are relevant for good performance are indeed picked up during training.Assessment------------------- The positives --The paper is well structured and mostly well written - it was in fact a pleasure to read apart from a few confusing bits that I will point to later in the review. The results are a bit thin in quantity, but to the point! The goal and the description of the experiments are clear and detailed, the figures clear and easy to understand.-- The concerns --1. The broad topic addressed in the paper (model training in RL) is interesting, but the contributions are not novel enough for this venue.* First contrbution in Section 1: "we show that combined learning is possible and that it yields good performing policies"Training models (variational or not) alongside a policy (in this or similar settings) is commonplace, and we now they can be trained. Example of much richer (action-conditional) and larger scale models than those presented in this submission are for example MuZero (https://arxiv.org/abs/1911.08265), or belief state models (one example https://arxiv.org/pdf/1906.09237.pdf).* Second contribtion of Section 2: "we show that jointly trained representations incorporate additional, task-specific information which allows a RL agent to achieve higher rewards then if it was trained on a static representation" This statement is quite unsurprising, as we are literally training the encoder to retain that information and provide it to the policy/value functions. Furthermore, the authors do not actually strictly verify that the fully pretrained model doesn't encode the ball location - that is, is the VAE decoder failing to reconstruct or the encoder failing to capture the ball localtion? What happens if you apply the method in Wang et al to PPO^{fixed}?   2. Whilst running on 1 environment with 1 training setup helps making the point, I can't help wondering if the comparisons between PPO^{VAE} and PPO^{adapt} could have yielded different results if multiple training tricks had been compared, e.g. something like using target networks for the VAE, using replay - which would have also made the comparison of the two method more fair to PPO^{VAE} since PPO^{adapt} has access to more compute.  The problem of how to use a pretrained model is indeed very exciting and I wish they authors had expanded more this section of the paper.Suggestions -- To make the paper more interesting, the authors could consider framing the paper in the broader context of unsupervised pretraining in DL (e.g. https://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf) .Please consider clarifying: - Section 4.2 "This weight sharing method has proven to be more time efficient than to query the encoder n timesand concatenate the results afterwards." --> I find this a bit confusing - isn't the graph compiler going to optimized this out? Can you elaborate?- Section 4.3. "While for PPOadapt, the latent space can be changed without restrictions, the decoder of PPOVAE constantly produces gradients do not contain information about the ball. " --> Sentence is badly formed, please fix. - Section 4.3 "Therefore, PPOVAE has less incentive to achieve higher rewards, which is reflected by its performance." --> I do not see this, and would need to see an experiement to show/support this claim. Please point me to the relevant sections of the paper if I missed them.- Conclusion "Our work verifies that the fundamental issue with pre-trained representations still exists and shows possible solutions in RL scenarios."  --> Why should the fundamental issue should have gone away? Also, the paper doesn't seem to suggest any solutions to improving the bridge between unsupervised pretraining and online training. Please do correct me if I am mistaken.I look forward to the rebuttal, hoping the author will help me reassess the paper in a more positive light.  Summary:The authors propose a new algorithm for inducing sparsity in the weights of neural networks after training. The proposed algorithm exploits the properties of commonly used activation functions to cast the sparsification problem as the minimization of a sparsity measure subject to approximation accuracy constraints. The proposed problem can be solved using convex optimization.Pros:The authors insights about popular activation functions and the approach used to cast the sparsification problem as a convex optimization problem are clever and interesting. The authors presented experiments on a wide range of deep learning models and tasks.Cons:Some of the details of the authors experiments are not clear or potentially misleading:1. Some results presented for existing techniques are from those techniques original papers, but some results were re-run by the authors. For example, consider the ResNet-50 results on ImageNet (table 3, left). The RigL authors did not present results at 60% sparsity, and Appendix D does not include details on how this number was generated beyond the authors using the released code with RigL. The numbers at 80% and 90% sparsity are taken from the RigL paper. However, these numbers were achieved with 5x the number of training steps which was enabled by the reduced number of FLOPs used by RigL during training because it maintains a constant level of sparsity throughout the training process. They also use non-uniform distributions of sparsity across the layers of the network which affects the number of FLOPs in the resultant network. The authors of this paper report a lower top-1 accuracy at 60% sparsity than the RigL paper reports at 80% sparsity, which leads me to believe that the training conditions (time, sparsity distribution) are not the same. Similarly, all of the RigL results for MobileNet family models (table 3, right) appear to have been run by the authors and the training setup details are not clear. For these results generated by the authors of this paper, they should also detail the amount of hyperparameter tuning performed for these baseline, as this can make a large difference in accuracy. I focused here on RigL because it appears to be the most commonly used baseline by the authors of this paper, but it seems likely that these observations apply to other techniques as well.2. Using RigL as the state-of-the-art baseline for most comparisons is not entirely fair given it has additional capabilities (i.e., the ability to enable sparse training by maintaining a constant number of parameters across training) compared to the authors proposed post-training sparsification algorithm. Sparse training (i.e., sparse-to-sparse training) is known to be a more difficult problem than dense-to-sparse training [1] or post-training sparsification. It is good to include RigL for comparison, but this distinction should be made clear and other techniques that have comparable ability to the proposed technique should be included as well.My suggestion to the authors are the following:1. All results should be reported as accuracy with a given parameter count and accuracy with a given FLOP count. Ideally, these tradeoff curves should be plotted across a range of accuracies and FLOP counts. This helps to avoid many of the pitfalls in the comparisons of model compression approaches details by [2] and [3].2. Make it clear that some algorithms under comparison have additional capabilities compared to the proposed approach (e.g., RigL with sparse training).3. Add comparisons with other algorithms of similar capability to the proposed approach. The magnitude pruning approach of Zhu & Gupta [1] would be ideal for this I believe.Comments:The brackets are backwards in the last paragraph on page 4. I would encourage the authors to explain more of the background of their approach (proximal operators, convex optimization, etc.) in sections 3 and 4. Many of those working in model compression who would be interested in this work will not be familiar with these topics.References:1. https://arxiv.org/abs/1710.018782. https://arxiv.org/abs/1902.095743. https://arxiv.org/abs/2003.03033  Summarythis paper propose a constraint in GAN training, to improve generated samples fidelity and stabilize training. The proposed conditioning is based on limiting the generator from departing normality function of real samples, which is computed in the spectral domain of Schur decomposition. It is claimed that this conditioning will not limit the exploration of all modes of real data distributionMissing references: GAN for unsupervised and semi supervised speech classification1- Hosseini-Asl E, Zhou Y, Xiong C, Socher R. Augmented cyclic adversarial learning for low resource domain adaptation. ICLR 20182- Hosseini-Asl E, Zhou Y, Xiong C, Socher R. A multi-discriminator cyclegan for unsupervised non-parallel speech domain adaptation. INTERSPEECH 2018Comments:1- what is the relation of batch size to # iteration before collapse? in Table 1, on US8k, it shows larger batch using spectral normalization result in faster collapse, while this is reverse on ESC-50 dataset. Figure 1 is only shown for US8k dataset only.2- If using spectral normalization stabilize training, the results in Table 1 indicate this is not always true with different batch size and different dataset. 3- Table 2: there is no evaluation of SA-GAN and BigGAN on 256 resolution for MCV. Also no 128 resolution for all models on US8k dataset. what is the reason behind selecting some resolutions only. Is the proposed normalization only better on some selected resolutions?4- [1] and [2] in missing references proposed multi-discriminator to provide more informative gradient to generator on spectrogram space. It would be also interesting to explore this in combination with proposed normalization5- what is the impact of the proposed normalization on speech recognition performance? It would be very helpful to add classification metric, since the proposed approach is evaluated on class-conditional GAN. 6- The evaluation on image generation is also required for the proposed normalization approach.  Summary:This paper introduces an additional objective to GAN training called difference departure from normality that is meant to encourage correlation between real and generated samples projected into some subspace. This is intended to prevent instability during training.  The technique is applied to audio spectrogram generation.Reason for score:Honestly I didnt completely understand this paper, so Im marking low confidence.  However, I do feel like the paper was quite low on clarity.  The technique wasnt well motivated or concretely explained.  And I found it odd that it was applied to spectrogram generation exclusively rather than image generation which is the standard testbed for GANs algorithms.Comments:* This paper could really use some improved motivation (e.g, Why is DDFN a good metric? Why would it improve training stability?).* Some of the terms and symbols need to be defined for clarity, e.g., p_g, p_r.* Section 3 would be improved by the inclusion of intuitive explanations to help the reader (and me) understand why the DDFN metric makes sense and what it represents. * I would recommend including the steps of the overall learning algorithm for those who want to better understand the concrete steps involved in implementing the approach and how the DDFN objective is integrating into the standard GAN learning algorithm. * Theres some parameter matrices that show up as $`_g$ and $`_d$ (backticks) in Section 3. Are these latex typos?* I would recommend testing the approach on common image datasets, since that is the typical testbed for generic GAN improvements.  This paper compared three different Chinese language pretraining methods. The paper is not well organized. Although the paper mentioned the char and word are both important in the representation of the Chinese language, it is not clear why the author(s) used the current three pretraining methods.  More intuitive explanation of the design of the pretraining structure should be added. The most intuitive way of combining the pretrained word and character information is to pretrain them separately and concatenate them together, the proposed models should compare with this most intuitive method and also explain why the proposed models are better than this simply pretraining concatenation method. The experiments were not persuasive. Although it is not necessary to beat all the state-of-the-art models, the comparison with other models should be given. For example, the MSRA NER in this paper is only 82% which is largely behind the SOTA models (>93%).  Given the poor baseline performance, it is hard to persuade the readers the conclusions in this paper are still hold in the most recent/advanced models. Page 2, citation error "Albert ?"Page 5, format error, the sentence is over the page boundary. The authors use some of the theory from optimal transport to certify the degree of robustness of an image classifier to adversarial perturbation. The theoretical contributions of the work, however, are largely already present, or easily deduced from results that are either already published or available online in a pre-published format. For theorem I have given a brief overview.  My score reflects a strong concern over the degree of originality of the results presented, particularly considering the authors cite some of the papers in which these results can already be found. I welcome the authors and other reviewers to draw my attention to any contributions in the paper I may have overlooked by mistake.### Theorem 1In essence Theorem 1 corresponds to weak Kantorovich duality for the robust optimal transport problem. The best reference for strong Kantorovich duality in this setting is [Blanchet & Murthy (2019)](https://doi.org/10.1287/moor.2018.0936) and the result is easily deduced from Section 2.2.1. More closely the result can also be deduced from Proposition 1 of [Sinha et al. (2017)](https://arxiv.org/abs/1710.10571),  [Kuhn et al. (2019)](https://pubsonline.informs.org/doi/10.1287/educ.2019.0198) also have a variety of results on this topic. Theorem 1 of  [Cranko et al. (2020)](https://arxiv.org/pdf/2002.04197.pdf) additionally qualifies the tightness of the upper bound and relaxes a number of assumptions of some of the aforementioned papers. There are a number of other, related papers by some of the authors mentioned that I have left out. ### Theorem 2Theorem 2 is not particularly surprising. In the proof the authors implicitly assume $\ell(\theta,\cdot)$ is twice differentiable and bounded (for all $\theta$). The addition of Gaussian noise is an additional complication, but it does not change things too much. The theorem is unsurprising because if it were not true it would lead to an absurd contradiction with the boundedness assumption.  The strong concavity result in the following corollary is also observed by [Sinha et al. (2017)](https://arxiv.org/abs/1710.10571) (at the bottom of page 2).### Theorem 3Theorem 3 was already proven in a significantly more general setting by [Blanchet & Murthy (2019)](https://doi.org/10.1287/moor.2018.0936)  (Theorem 1). Furthermore I am not convinced of the correctness of the proof since there appears to be an assumption that $\gamma$ is sufficiently larger for $\ell(\theta; x) - \gamma c(x,x_0)$ to be strongly concave. And I cannot even find an assumption that at this point $c(\cdot, x_0)$  is assumed so much as convex for a choice of $x_0\in\mathcal{X}$.  This paper proposes smoothing the classifier in the distributional robust learning framework by adding random noise to the input. The smoothed distributional robust framework is used to gain robustness against adversarial perturbations in settings where the classifier is originally non-smooth and then smoothed via the additive noise. While the proposed idea can be potentially useful for training adversarially-robust classifiers over non-smooth function spaces, the paper's theoretical formulation seems to reduce to original non-smoothed distributional robust optimization. Theorem 2 also seems incorrect and its proof suffers from several mistakes.  To see the main issue with the paper's problem formulation, note that Theorem 1 results in the adversarial loss function \phi_{\gamma}= E_Z[ max_x{...} ] which first maximizes over x\in \mathcal{X} (inner operation) and then takes expectation over Z (outer operation). This sequence is also the case in Equation (6) and is the basis of the theoretical and numerical analysis throughout the paper. However, one can see that this objective E_Z[ max_x{...} ] simply reduces to max_x{...} with no expectation over Z. This is because as long as the support set \mathcal{X} is unconstrained, which is the case in Algorithm 1 applying no projections on X, the maximization solution for x+z will be the same given any outcome Z=z. This is a direct consequence of maximizing a strongly-concave objective in \phi_{\gamma} that has a unique maximizer. In fact, I think the opposite order max_x{ E_Z[...] } which hasn't been analyzed in the paper will lead to a properly smoothed optimization problem. I, therefore, suggest redoing the analysis for the properly-ordered max_x{ E_Z[...] }.Furthermore, Theorem 2 seems incorrect and the constant 2M in the theorem's upper-bound should be replaced with a function of both M and \sigma. In the theorem's proof, Equation (17) implicitly assumes that \sigma=1, whereas this assumption has not been mentioned in the theorem. Also, the step from Equation (22) to (23) mistakenly substitutes 1 with M and obtains a constant 2M instead of M+1, while there are no assumptions on M>1. The theorem needs to be revised since the current version is incorrect. Because of the above issues, I don't recommend this paper for acceptance in its current form. The paper should be revised to resolve these major issues.  The paper aims to benchmark a suite of Multi-Agent Deep Reinforcement Learning algorithms across different environments in the cooperative multi-agent setting. The paper compares standard algorithms alongside extensions of well-known policy gradient algorithms to the multi-agent setting, i.e. PPO (MAPPO), SAC (MASAC) and TD3 (MATD3). The paper investigates the ease of using an algorithm by doing a fair hyperparameter search for different algorithms. The paper concludes by saying that MAPPO is a promising choice for tackling a multi-agent problem. Such a benchmarking paper would absolutely be useful. In its current form, however, it is not yet ready for publication. There are three primary reasons. First, the scope is misplaced: the paper starts with the scope of benchmarking different algorithms but seems to shift focus primarily to MAPPOs benefits. Second, the experiments only include 3 runs, with large overlapping deviations, limiting the ability to make claims about any significant differences. This is particularly problematic for a paper where the main focus is on experiments to provide insight. Finally, it seems like Pop-art is only used with MAPPO (please clarify this if I am mistaken), calling into question if the main reason for improvements in MAPPO is this choice. Reward scaling, done by Pop-art, can significantly affect learning performance as pointed out in Henderson et al. (2017). Using Pop-art only for MAPPO makes the comparison unfair between different approaches and potentially invalidates the conclusions presented in the paper.As two other more minor points:1. Multi-agent methods are built around numerous agents learning separate policies. The choice of sharing the same weights (Page 4, Sec 3.3) among different policies doesnt seem to me like a fair choice. This also makes the algorithms different from their original intended use. 2. There are several inconsistencies or questionable choices in the paper: a. As per appendix, MADDP, MATD3, MASAC, QMix have gamma parameters, whereas there is no such parameter mentioned for MAPPO. Having a different gamma value for the methods essentially changes the problem.b. There is a difference between similar hyperparameters for different methods, i.e. MAPPO uses a gradient clipping of 20, whereas other methods use a gradient clipping of 10. How was this chosen and why is it different?c. Appendix (Hanabi), MAPPO uses a learning rate of 7e-4, whereas the learning rate is not mentioned as a value being tested in the search grid. d. MADDPG and MASAC are said to use a centralized critic, but according to Algorithm 1 and Algorithm 2, they seem to estimate separate critics. e. MASAC should have a tuple of three parameters (value functions, q function and policy), but Algorithm 2 talks about only two parameters (q function and policy). A few questions: 1. In SMAC environments, why are MASAC, MATD3 and MADDPG not run for other settings (Table 2)? 2. In MPE, I was not able to compare the results for MADDPG to the original paper. Can you point out the results (i.e. example the table number in Lowe et al. 2017)?  This also seems strange as Lowe et al. 2017 use a non-recurrent network with a different parameterization for each of the policies. In contrast, the current paper uses a recurrent network and shared policies, making the models quite different. Suggestions: 1. Although the paper compares all the well-known policy gradient methods, comparing against a simple Vanilla Actor-Critic network would provide useful insight by removing some of the complexity.2. It would be good to include hyperparameters of PPO like the \epsilon clipping. 3. The graphs in Figure 3 do not have a visible axis.4. Including parameter sensitivity would provide much more insight into the properties of the algorithms,  and how they perform for general configuration of parameters. 5. Section 3.1, P(s|s,u) : \mathcal{S} \times U \times \mathcal{S} \rightarrow [0,1], should have a U^n instead of U.  References1. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2019). Deep Reinforcement Learning that Matters. 2. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Mordatch, I. (2020). Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.  This paper investigates the SGD with constant step size (SGD-CS) on non-conex optimization problems. Theoretically, the paper shows the conditions under which a minimizer $x^*$ is a point of attraction in a local neighborhood under the algorithm SGD-CS with sufficiently small step-size. Furthermore, the paper experimentally shows that vanilla SGD-CS with relatively large step-size performs well, or even outperforms its momentum and/or adaptive counterparts, on several popular deep learning tasks.[Comments]:1: Claim and analysis are not consistent. The authors claim in the introduction that convergence of SGD-CS on non-convex functions is shown. However, the analysis only focuses on points of attraction and the stay within a local neighborhood. I would like to point out that the latter concepts are not equivalent to convergence. To show convergence of an algorithm, the missing piece of the paper is that, starting from the initialization point, the algorithm can be guaranteed to find such a local neighborhood around the point of attraction. Without this guarantee, the analysis of points of attraction is meaningless in the sense of algorithm convergence. Hence, I disagree that convergence of SGD-CS is theoretically shown in the paper.2: Considering the necessary condition for points of attraction, Theorem 1, the assumption A.3 barely holds true in practical cases. As stated in Theorem 1, $x^*$ being a point of attraction implies interpolation property. For most real world tasks, interpolation can be only achieved when the model is over-parameterized, i.e., number of parameters is greater than the number of data samples. (For example, consider solving a system of linear equations). As pointed out by the work [Liu et al. 2020], most of the minimizers are not isolated, instead they form a low-dimensional manifold. In this case, none of the minimizers satisfies Assumption A.3, because the Hessian matrice H at the minimizers always have zero-eigenvalues (flat directions).3: The main theoretical result, Theorem 2, relies on the fact that step-size is sufficiently small. However, one of the main claims of the paper is the convergence under large step size, as discussed in Section 5 and experimented in Section 6. I dont see the connection between the small step-size theoretical result and the large step-size experiments. The theory seems not to explain the experiments.4: The paper frequently talks about large learning rates. However, it is not clear to me what is the criteria to be large or small. Especially, in section 5, the paper provides a certain range of step size values (e.g., step size $t \in (0, 1/\lambda_{max}(H))$, within which the SGD-CS converges on a few simple examples. What are the reasons to claim these step-sizes are large?[About clarity]:1: It should be reader friendly to enlarge some of the figures.2: Providing an intuition of the error function, defined in Eq.(4), should be helpful.3: Notations can be improved.[References]:[Liu et al. 2020] Liu, Zhu, and Belkin. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. arXiv:2003.00307. This work extends the binary Hopfield network (Demircigil et al., 2017) to continuous patterns and states. Connections are drawn between the result model to the attention layers of the transformers, the pooling operation of LSTM, similarity search, and fully connected layers. Experimental results are briefly described for analyzing the attention of Bert models, multiple instance learning, and small UCI classification tasks.The proposed model seems very interesting, and the proposed applications seem reasonable at a very high level. However, there is just not enough detail in this paper for me to understand how the models are implemented or why the model works better than other approaches.For example, section 3 declared 3 types of Hopfield layers, but without any formal definitions to them, or how they are integrated to the proposed models. The experiment section compares performances with existing models, but lacks any analysis of why the proposed models work better. Similarly, there is a lack of motivation in the introduction section. 1. Summarize what the paper claims to contribute. Be positive andgenerous.They propose to include the policy parameters as an inputto the value function, so that the value function could generalizeacross different policies (there are 2 other concurrent works with a similaridea, one they have cited and discussed "Policy evaluation networks"https://arxiv.org/abs/2002.11833, another is submitted to ICLR2021 onopenreview https://openreview.net/forum?id=V4AVDoFtVM).They put the policy parameters theta as an input to the value functionin 3 cases $V(\theta)$ (PSSVF), $V(s, \theta)$ (PSVF), and $Q(s,a,\theta)$ (PAVF).They propose new policy gradient theorems for the $V(s,\theta)$ and$Q(s,a,\theta)$ cases (but I believe these to be theoretically flawed).They perform experiments testing $V(\theta)$ in 2 cases: 4.1) (sanity checkexperiment) visualizing and testing for correctness on an LQR task,4.3) zero-shot learning: after training a policy pi using the $V(\theta)$ method,a new policy is reinitialized pi_new and trained from scratch using only thetrained $V(\theta)$ without interacting with the environment. The interestingbit was that $\pi_{new}$ managed to outperform the learned policy duringdata collection $\pi$ (this implies that the $V(\theta)$ function managed togeneralize. It would have been nice to, in addition to $\pi_{new}$, alsosee whether $\pi$ could have been improved by just continuing to optimizeit without interacting with the environment, but this was not done).They tested $V(\theta)$, $V(s,\theta)$ and $Q(s,a,\theta)$ on MuJoCo taskscompared to augmented random search (this is similar to evolutionstrategies) and to deep deterministic policy gradients (DDPG). Andthe performance did not change much, and sometimes all the new methodsfailed when DDPG worked (on the reacher task).The final experiment 4.4 was for offline learning with fragmentedbehaviors, i.e. they do not observe full episode data for a fixedtheta, which makes it impossible to learn $V(\theta)$ directly, but$V(s,\theta)$ can be learned by TD methods (also note that the data iscollected from a different behavior policy). Then they test a similarzero-shot learning procedure as they did for $V(\theta)$ at differentstages of the learning (but as far as I understood, for $V(s,\theta)$ they sampleddata from the replay buffer when training the policy (thus not fully withoutinteracting with the data). Perhaps the authors can clarify this), andshow that the newly learned policy can outperform the behavior policy,thus demonstrating the generalizability of the method.2. List strong and weak points of the paper. Be as comprehensive as possible.\+ The experiment on zero-shot learning is nice to show that the $V(\theta)$function can generalize.\+ The paper is clearly written.\+ They discuss a lot of related work.\+ The experimental methodology seemed mostly good and honest, andwas explained in detail in the appendix (some nice points: They includea sensitivity analysis showing quantiles of the performance.Also the final best chosen hyperparameters were evaluated with20 new seeds, separate from the 5 seeds used during hyperparameter tuning).\- The new policy gradient theorems seemed flawed. Also some discussionaround off-policy learning seemed incomplete.\- The methods were not shown to experimentally lead to major gains.\- One of the difficulties with searching in parameter space is howto deal with large parameter spaces. The two concurrent works considering$V(\theta)$ proposed solutions to this issue by embedding the policy intoa smaller space. In the current work no solution is proposed. Theexperiments on zero-shot learning using $V(\theta)$ were only good withlow-dimensional linear policies.\- A sanity check experiment on LQR was performed for only $V(\theta)$ (which wasthe only one for which the gradient was theoretically sound); it wouldhave been good to do similar experiments for the other ones.\- I would expect $V(s,\theta)$ to outperform $V(\theta)$ due to using thestate information, but this did not appear to be the case.3. Clearly state your recommendation (accept or reject) with one or twokey reasons for this choice.I recommend rejecting the paper due to the theoretical flaws in the newlyproposed policy gradient theorems using $V(s,\theta)$ and $Q(s,a,\theta)$. Also,the practical advantages of using $V(s,\theta)$ and $Q(s,a,\theta)$ were not shown.4. Provide supporting arguments for your recommendation.The theoretical issues in this paper start in equation 1. They write:"... we can express the maximization of the expected cumulative rewardin terms of the state-value function:"$J(\pi_\theta) = \int d^{\pi}(s) V(s) ds,$  (in the paper)where $d(s)$ is the discounted state visitation distribution. However, thisis not the RL objective. The RL objective would be$J(\pi_\theta) = \int d^{\pi}(s) R(s) ds.$    (what it should actually be)The authors probably took their objective from the work byDegris et al (2012, https://arxiv.org/pdf/1205.4839.pdf);however, in Degris'12, $d(s)$ is _not_ the discountedstate visitation distribution. It is the limiting distribution as$t \to \infty$, which is a stationary distribution. When $d^{\pi}(s)$ is stationary,then the two objectives become equivalent: $d(s)$ does not changefrom one time step to the next, so the difference between the objectiveswill be just a $1/(1-\gamma)$ constant factor. Putting aside this issue,probably the limiting distribution formulation is not realistic as mostRL researchers consider the episodic setting, so using a discountedstate visitation distribution is probably better. However, the newlyproposed policy gradient theorems do not appear sound for the true RLobjective using $R(s)$.Next, they replace the distribution $d^{\pi}(s)$ with adistribution $d^{\pi_b}(s)$ gathered using a behavioral policy (so they areworking off-policy). However, they do not apply an importance weightingcorrection for the distribution shift, and just ignore the importanceweights (Note that this is also done by Silver et al (2014) in deterministicpolicy gradients, and by Lillicrap et al (2015) in DDPG, so it is not thatstrange per se, as long as it gives better practical performance. However,it should at least be acknowledged that the importance weights are beingignored). Note that they still apply an importance weight on the actions($\pi(a|s)/\pi_b(a|s)$) once the state is sampled from the data buffer, however,this does not correct for the distribution shift from $d^{\pi}$ to $d^{\pi_b}$,so the policy gradient computed using such a method will necessarily be biased.For example, see the following works for examples that try to deal with thedistribution shift problem:Munos et al (2016, https://arxiv.org/abs/1606.02647),Wang et al (2016, https://arxiv.org/abs/1611.01224),Gruslys et al (2017, https://arxiv.org/abs/1704.04651)Putting aside the issue of whether ignoring the distribution shift is OK,the main issues are the new policy gradient theorems derived from thisformulation. Both the $V(s,\theta)$ as well as $Q(a,s,\theta)$ formulations appearflawed:In the $V(s,\theta)$ case they propose the policy gradient:$\nabla_\theta J(\theta) = \int d^{\pi_b}(s) dV(s,\theta)/d\theta ~~ds$ in equation 8.However, the true policy gradient is:$\nabla_\theta J(\theta) = \int \mu(s) dV(s, \theta)/d\theta ~~ds,$where $\mu(s)$ is the start-state distribution. Actually they wrotethis also in equation 7, when they considered the $V(\theta)$ formulation,but for some reason sampled from $d(s)$ instead for $V(s, \theta)$ when computingthe policy gradient in the $V(s,\theta)$ formulation.In the $Q(a,s,\theta)$ formulation, they add an extra $dQ/d\theta$ term tothe policy gradient. Their motivation is the following:$\nabla_\theta J(\theta) = \int d^{\pi_b} (dQ(a=\pi(s,\theta),s,\theta)/d\theta) dads$  $   = \int d^{\pi_b} dQ(a,s,\theta)/da*da/d\theta + dQ(a,s,\theta)/d\theta~~ dads$However, this derivation stems from the flawed definition of J that isnot maximizing the sum of rewards over the trajectory distribution, butmaximizing some other objective that sums the value functions at all statesin the trajectory distribution. My strongest argument for why the originaloff-policy derivations by Degris et al and Silver et al are less flawed isthe following:If we are on-policy, i.e. $\pi_b = \pi$ and $d^{\pi_b} = d^{\pi}$ we would wantthe off-policy policy gradient theorem to be unbiased, hence it shouldrevert to the standard policy gradient theorem. In the formulationsof Degris and Silver, this is indeed the case, and these theorems wouldbe unbiased in the on-policy setting. The new theorem in the currentpaper, on the other hand, would have an extra $dQ/d\theta$ term, which wouldbias the gradient. Therefore, I do not see any good theoretical reason toadd this term. Moreover, the practical performance did not improve, sothere is little evidence to suggest it as a heuristic either.If someone would say that the original policy gradienttheorem requires the $dQ/d\theta$ term, I would urge them to look at the originalproofs---there is no approximation, these theorems are exact for the trueRL objective based on maximizing the rewards over the discounted trajectorydistribution. The intuition is that the remaining $dQ/d\theta$ term for theremainder of the trajectory from a time-step t is estimated by summingthe $dQ/da\*da/d\theta$ or $Q\*dlog/d\theta$ terms for all the future time-steps.Another more minor theoretical issue in the paper is that while thetheory considered the discounted state visitation distribution, thediscount factors are not added into the policy gradient in the algorithmicsections. This omission is common, and tends to work well as a heuristic(but it should at least be mentioned that such an approximation is made).See the following papers for more discussion on this:Nota and Thomas (2020, https://arxiv.org/abs/1906.07073)Thomas (2014, http://proceedings.mlr.press/v32/thomas14.html)5. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. How did the computational times compare? Was there much of an overhead tousing the more complicated critics including theta as an input?6. Provide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.For me to change my assessment, first the theoretical issues should befixed or cleared up.Next, I have some possible suggestions:1) Test also $V(s,\theta)$ on LQR as well as on zero-shot learning while samplings from the initial state distribution $\mu(s)$. This does not require interactingwith the environment (because you never apply any action), and I would considerit fair in terms of comparing to $V(\theta)$. If the learning from the TD erroris working well, I would expect it to outperform the $V(\theta)$ formulationin the zero-shot task. 2) Test the parameter value functions using the standard policy gradientswithout adding the $dQ/d\theta$ term. Because you are using $Q(a,s,\theta)$, theremay be some learning to generalize across different policies due to thetheta input, so it may outperform the original policy gradients withoutchanging the policy gradient theorem. Actually, it would have been better toperform such experiments as an ablation study from the beginning anyhow.3) Test $Q(a,s,\theta)$ also on the LQR task to show it's correctness(for example by sampling s from the initial state distribution and computingthe action). It may also be nice to test it in the zero-shot task as well.4) Perhaps test combinations of the various gradients, for example takingthe average of the $V(\theta)$ gradient with the policy gradient using $Q$(i.e. taking the average of two equivalent policy gradients).If the above points are convincingly done, I may increase to marginalaccept. The current contributions are not enough for me to go higher thanthat: taking away the proposed new policy gradients, the main contributionis to add $\theta$ as an input to $V$ and $Q$, which I think is not enough.Moreover, the advantage of adding $\theta$ as an input was not shown convincinglyusing compelling evidence. Currently the most compelling evidence is thezero-shot task, which shows that there is some generalization happening inthe $\theta$ space; however, what is missing to me, is a demonstration of howthis additional generalization helps in solving the original task in a moredata-efficient manner. Perhaps interleaving the policy search with longersessions of off-line learning (without any interaction) using $dV/d\theta$to take advantage of the generalization may improve the data-efficiencyand show the advantage of the new method (exaplaining good practices on howto do this may be a useful contribution). I think it would also be importantto show compelling evidence that including the s input helps in learningbetter $V$ and $Q$ functions. Perhaps there are also other ways to bettershow the advantage of the method.Another option may be to change the problem setup, so that the new policy gradient theorems would be more sound. For example, using the original formulation of Degris'12 where $d^{\pi_b}(s)$ is the limiting distribution as $t \to \infty$ would make the new policy gradients correct; however, the standard setup would not correspond to this. One setup that would correspond to this objective is the following: an infinite horizon continuing setting, where the agent is never reset into the initial distribution, but has to continually change the policy to improve. The learning would iterate between running one behavioral policy until it converges to its stationary distribution, then optimizing a new policy while in the off-policy setting, then switching the behavioral policy to this new policy, and repeating the process. In this situation, $d^{\pi_b}(s)$ can be seen as the initial distribution for the new policy, and in this case the new policy gradient theorems would make sense. My previous argument about wanting the policy gradient theorem to be unbiased in the on-policy case would also be satisfied, because if $d(s)$ is stationary then the $dQ/da\*da/d\theta$ and $dQ/d\theta$ gradients would differ by only a constant factor. Summary: Authors propose an augmentation technique for image classification. The augmented image is obtained by segmenting the salient object and masking the background. Therefore the technique gives one additional augmented image per each training image. The authors show an improved performance when using this augmentation on binary classification task (cats vs. dogs) using a dataset of 2000 images.I find the paper in its current form lacking in many aspects, mainly: questionable novelty, bad presentation, minimal experiments.Concerning novelty, the authors themselves remark: "It is a good practice to train a model using images that include the labeled object only". The section on related work is very superficial ignoring the achievements thanks to synthetic images. The approaches such as the following, used usually in the context of object detection, should be considered prior art and at least properly discussed, e.g.:- Gupta et al, "Synthetic Data for Text Localisation in Natural Images", CVPR, 2016.- Dwibedi et al, "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection", ICCV, 2017.- Georgakis et al, "Synthesizing Training Data for Object Detection in Indoor Scenes", RSS, 2017- Dvonik et al, "Modeling Visual Context is Key to Augmenting Object Detection Datasets", ECCV, 2018- Tripathi et al, "Learning to Generate Synthetic Data via Compositing", CVPR, 2019There are many places in the manuscript where the sense of the text is not clear or the text appears out of context, e.g.: "After ResNet was proposed (He et al., 2016), new network architectures have been proposed as well (Zagoruyko and Komodakis, 2016; Han et al., 2017). Pixel-dropping, however, injects noise into the image (Sietsma and Dow, 1991)." The text seems in general unfinished and outdated at the same time (no citations of prior art newer than 2017).Concerning experiments. The authors speak about lack of data but instead of evaluating on some few-shot datasets (e.g., mini-Imagenet or omniglot), they evaluate on a dataset with two classes and 1000 training images per class. The experiments are limited to evaluating a a model trained on the original unaugmented dataset, augmented using automatic segmentation, and augmented using manual segmentation. There is no comparison to other methods, not even the ones cited in the paper. Given obtaining segmentation can be costly I would expect the authors to test whether it is necessary at all and whether cutting the object to the bounding box is not enough. I would also expect a discussion of when can background removal actually hurt the performance (e.g., because it is predictive of the target class). The paper presents an image data set augmentation approach (4 variants+ original data set variant) in order to cater to the lack of sufficient data for learning purposes by increasing available image data sets through creation of  variations of existing images. The variations are all about extracting recognizable objects in images (Object-Focused Images or  OFIs), manually and automatically,  and combination with original data set images.Questions:- In fact, when the OFI technique is used, the number of the images supplied nearly doubles.- Does not this depend on the type of images in the data set: multi-object images, no discernible object sets (textures, backgrounds etc.)?        - Image given as example in Figure 2 is quite simplistic as the cat figure is clearly easily isolated. You Should state the type of application your model is intended for as you use for such a simplistic application.- In 3. THE OFI TECHNIQUE: - You state It is a good practice to try to train a model using images that include the labeled object only and nothing else..         - Does not this go against building models that will use real life images and therefore generalize well? Except may be in controlled setting like manufacturing lines.- In 4. THE EXPERIMENTS /4.1 USED MODELS:- You conduct 5 different tests (or models as you call them): Original images, Automatic OFIs, Manual OFIs, Original + Automatic OFIs and Original + Manual OFIs.  - Did not you try: Original + Automatic OFIs + Manual OFIs? Why not, you had the data.Comments:-In  1 INTRODUCTION, you state:  - " benefit further from using the mixed data set of old and new images. Rather original data set images and derived/augmented images. - "The automated method uses an Application Programming Interface (API) to remove the background of the image and leave only the labeled object (a cat or a dog) in the foreground. An API is an interface to remotely run an algorithm/call a method, not the actual method.  - For consistency, since you deemed each experiment as a model, asking Will the model have a better validation accuracy when only the original set of images is used? is not consistent. Rather will the model with only the original set of images & - This paragraph seems out of place  After ResNet was proposed (He et al., 2016), new network architectures have been proposed as well (Zagoruyko and Komodakis, 2016; Han et al., 2017)..- In 3, THE OFI TECHNIQUE: - Am perplexed by these questions: - Would our model get a little confused by those additional objects and try to be trained on?  - Would our model keep changing its parameters and weights based on the additional objects in the image? - In 4. THE EXPERIMENTS /4.1 USED MODELS:- When a human photo editor keeps one object in the image and removes all other objects, the image tends to be more accurate. Not sure, this phrasing isaccurate. Summary:The paper proposes a parameterized learnable aggregation function (LAF) that can aggregate a multi-set of numbers (i.e. map them to a single real-valued number). This is different to prior works such as Deep Sets that use fixed aggregation functions such as max, mean, etc.Strong Points:- While Deep Sets have shown that is theoretically sufficient to have a sum aggregation, it is still unclear which kind of aggregation functions work well in practice. Hence, the paper addresses an interesting research question.- The presented idea is rather simple, which I think is a strong point of the paper.- I like the analysis in Table 1 that shows how different parameterizations of the LAF correspond to different functions such as sum, min, means, etc.- I also like the evaluation presented in Figure 4.Weak Points:- The setup of the experiments with scalars is unclear. For example, a LAF is supposed to aggregate all scalars in the input. However, the paper states that the LAF model is comprised of 9 LAF functions. Why do we need 9 functions? And how are they composed into a single architecture? Similarly, the paper states that 'DeepSets contains three max units, three sum units, and three mean units'. However, a Deep Set should have one function mapping the input to an intermediate representation, a sum aggregation, and a function that maps the aggregation to the output. I don't see why the model needs three max, sum, and mean units.- In experiment 1, it is reported that the 'input mapping is performed by three layers with the hyperbolic tangent as non-linear activation'. However, the input is simply a multi-set of scalars. It remains unclear to me why it can make sense to map scalars with a 3-layer network. Furthermore, a sigmoid is applied in case of LAF. Hence, it is unclear if the observed performance is due to the Sigmoid or due to the LAF. Furthermore, it is unclear why the paper uses tanh as activation function while Deep Set implementations use ReLUs. Also, the architecture of the output mapping is not described.- Similar to experiment 1, the experimental setup in experiment 2 is unclear. Additionally, it is unclear how the aggregation of features (i.e. individual dimensions in MNIST images) is related to the aggregation function used to compute the target output of the multi-set.- While the problem investigated in the paper is permutation-invariant, several methods have been proposed to approximate permutation invariant problems with recurrent architectures such as LSTMs and GRUs. However, no comparison to these kinds of methods is performed. A comparison would be interesting since they also learn an aggregation function.- It would be interesting to see if the parameters of the LAF function are learned as expected, i.e. if they correlate with the expected values as listed in Table 1. An analysis of this question is missing.- The paper does not share code or data to improve the reproducibility of the experiments. ###############################################################################SummarySummarize what the paper claims to contribute. Be positive and generous.The authors propose an interpretation of feed-forward neural networks and recursive neural networks as chain graphs (CGs). They claim this new interpretation can provide novel theoretical support an insights to existing techniques, as well allow for the derivation of new approaches.###############################################################################Pros and consPros: - the text is well-written - the attempt at studying neural networks under a graphical probabilistic model perspective is praisableCons: - the chain graph interpretation put forward by the authors is superfluous, as in reality the definition found in the apper is that of DAGs with particular parametric constraints - most of the results presented by the authors do not rely on the CG interpretation, and can already be found in the litterature - the paper is missing a connexion to stochastic feedforward neural networks (SFNNs), to which the proposed interpretation is extremely similar###############################################################################RecommendationI recommend rejection of the paper, for two reasons. First, I believe most of the contributions proposed in the paper are not novel, as they can already be found in published form. See my detailed comments below. Second, the CG interpretation put forward by the authors is trivial and superfluous, since the authors in reality only consider CGs restricted to DAGs. As such, the whole claim of the paper that CGs can give a new, relevant perspective to NNs, is not credible.###############################################################################Questions to authorsI would like the authors to comment on the fact that their CG interpretation is indeed the LWF-CG interpretation, restricted to DAGs (singleton chain components). Furthermore, I would appreciate if the authors could relate their approach to the SFNN model, and what differs from that interpretation.###############################################################################Detailed comments:p.1 §3: an efficient approximate probabilistic inference -> What is meant here by efficient ? Is it an unbiased estimator ? What is meant by inference ? Computing $p(y|x)$ ? $\arg \max_y p(y|x)$ ?p.2 §2: The chain graph model -> There exists at least 4 chain graph interpretations. SeeDrton, Mathias. Discrete Chain Graph Models. In: Bernoulli 15 (Sept. 2009),pp. 736753.Among those, there are two which subsume UGs and DAGs, while the two others subsume BGs and DAGs. If you follow the CG interpretation from Koller and Friedman then you assume NNs are LWF-CGs.I strongly suggest that you make it explicit which chain graph interpretation you follow, for the sake of clarity. These interpretations are not equivalent.p.2 §2: there exists a series of works [...] -> I believe the list is much bigger than that. You forget very popular models which are direct instanciation of PGMs, such as VAEs, HMMs, LDAs, GMMs, CRFs, and all of their variants.p.2 §4: an approximate probabilistic inference procedure -> What is meant by that ?p.2 §4: provides additional insights -> such as ?p.3 §2: layered chain graph -> A chain graph is always layered into chain components. The name "layered chain graph" is confusing, as it seems to imply that some chain graphs may not be layered. Since you are using the LWF-CG interpretation, you might point to the relevant papers where the graphical model was introduced, and simply re-use the establihsed LWF-CG name from the litterature, instead of inventing a new one.Lauritzen, Steffen L. and Wermuth, N. Graphical Models for Associationsbetween Variables, some of which are Qualitative and some Quantitative. In:The Annals of Statistics 17.1 (Mar. 1989), pp. 3157Frydenberg, M. (1990). The chain graph Markov property. Scand. J. Statist. 17 333353. MR1096723p.3 eq.1: This factorization is not that of a CG, but that of a DAG. Since you assume no undirected connection between variables in the same layer, then all chain component in your chain graph actually contain a single variable. What is the point of introducing the conecpt of CGs then, if you only consider CGs which are restricted to DAGs ?p.3 eq.2: The LWF-CG interpretation, which you seem to follow since you refer to Koller and Friedman, does not imply this factorization for the graphs you consider. Since each chain component $K$ in your graph is a singleton, each $X_i$ and its parents form a clique in the closure graph $K$. As such, the CG structure does not imply any further factorization for p(x|pa(x))... It does seem your interpretation of NNs is not as general CGs, but as very specific DAGs with parametric constraints. Furthermore, you do not define what are $T$ and $f$ here.p.3 Section 2.2: I believe your CG interpretation is simply a reformulation of stochastic neural networks, for which there already exists a great body of work. See, eg:Eric Jang Shixiang Gu Ben Poole. Categorical Reparameterization with Gumbel-Softmax. ICLR (2017)Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.Yichuan Tang, Ruslan Salakhutdinov. Learning Stochastic Feedforward Neural Networks. NIPS 2013p.4 Proposition 2.1: This result seems very limited, since it assumes linear layers, and in the end a linear NN.p.4 Corollary 2: What are alpha and beta here ? How do they relate to $f$, $e$ or $T$ from Definition 1 ? What are $e_i^l$ and $s_i^l$ here ?p.4 §6: the modularity of chain components justifies transfer learning via partial reuse of pre-trained networks -> I do not understand this argument. What is meant here by "modularity" ?p.4 §7: However, feed-forward is no longer applicable through these intra-connected layers. -> You finally give an example here that corresponds to non-trivial chain graphs (with chain components greater than 1), and recognize that your theoretical results do not apply any more. This contradicts your claim that CGs offer a general interpretation of NNs with theoretical support. I believe the CG interpretation is not justified, nor required to derive the results you present in this paper.p.5 Definition 2: I fail to see the point of introducing this new concept, which is a re-definition of residual blocks.p.5 §4: While a vanilla layered [...] -> I fail to understand where the CG interpretation fits in this argument.p.5 Proposition 4: This result seems trivial to me, and does not require the concept of a CG. Once you show that a linear layer followed by a specific activation can be interpreted as a parametric model for $p(\textit{out}|\textit{in})$, then any NN that unrolls as an acyclic directed graph can be interpreted as a probabilistic model.p.6 §1: The simple recurrent layer, [...] -> Again, I do not see where the CG interpretation gives any insight here.p.6 Section 3.3: This result, again, does not require the CG interpretation. It is also already know. See, e.g., Pierre Baldi and Peter J. Sadowski. Understanding Dropout. NIPS 2013p.6 Section 4: It seems to me you are reinventing SFFNs. See, Yichuan Tang and Ruslan Salakhutdinov. Stochastic Feedforward Neural Network. NIPS 2013p.7 §2: the sampling operation is not differentiable -> This is not true. See, e.g.,  the reparameterization trick for VAEs. You mention this in the next sentence... **Summary**: the paper aims to explain the success of BYOL, a recently proposed contrastive method that mysteriously avoids the trivial constant solution without requiring negative samples. The paper proposes a new loss named RAFT. Compared to BYOL, RAFT is more general since it subsumes a variation of BYOL as its special case, and contains a cross-model term to be maximized which regularizes the alignment loss and encourages the online encoder to "run away" from the mean teacher.The paper claims this cross-model term encourages disparity, which could help demystify why BYOL does not collapse to a trivial solution. However, the cross-model term itself cannot prevent outputs from collapsing, as explained below.**Question 1**: my main concern is the effectiveness of the cross-model loss: I disagree that the cross-model loss prevents collapsing representations. I think the authors may be confusing contrasting two samples ("cross-sample") and contrasting two functions ("cross-model"):- The cross-model loss is essentially the L2 distance between two functions, which is the average squared error between two model outputs on the *same sample.*- The common contrastive loss, which contrasts outputs from the same model on *different samples*.For example, suppose MT is a constant function at the $t_{th}$ iteration (i.e. the function outputs some constant $c$ for all input), then the online encoder could be updated to be another constant as far away from $c$ as possible, i.e. the cross-model loss is maximized, however we still have the sample collapsing issue. As a side note, a constant function also achieves a perfect alignment loss.More concretely, consider $f(x) = Wx +b$ where $W$ is initialized to be the all-0 matrix, i.e. $f(x) = b$ is a constant function. Then for all future updates, learning $f$ only updates $b$ but not $W$ (since there's no gradient on $W$), and therefore $f$ will remain a constant function, i.e. it always collapses the points. One may argue that it is wrong to choose $W = 0$, but the point is, the success of BYOL needs more careful analysis of the optimization process, which cannot be addressed by the cross-model loss term itself.**Question 2**: section 3 phrases the need of a predictor as a disadvantage of BYOL, however RAFT also requires a predictor head to achieve good classification performance. Studying the effect of the predictor is an interesting direction and will make the paper much stronger, as the authors also point out in the conclusion.**Other comments**:- Table 3: why are there no results for BYOL'-MLP? Comparing RAFT-NP to BYOL'-NP, there doesn't seem to be a clear edge of RAFT, both in terms of the uniformity loss and the accuracy.It would also be better to highlight the key results in the table. The current table is quite dense; adding more highlights and comments will have the reader understand what to take away from these results.- Paper organization: a lot of material is deferred to the appendix, which makes the paper a bit hard to follow since the reader needs to jump back and forth. It would be better if results in the appendix are better summarized in the main text.- The term BYOL' first appears at the end of the first paragraph on page 2 without a definition.- Minor typo: there's an extra left parentheses in front of the second $f$ in equation (5); an extra comma after "distribution" in the first paragraph of section 3. This paper analyses the recently proposed Bootstrap Your Own Latent (BYOL) algorithm for self-supervised learning and image representation. The authors first derive an alternative training procedure called BYOL' by computing an upper bound of the BYOL objective function.After diverse analyses, the authors then introduce Run Away From Your Teacher (RAFT), where RAFT is another BYOL variant that resembles contrastive method by having an attractive and repealing term in the training objective. According to the authors, this decomposition allows for a better understanding of the training dynamics.Finally, the authors made the following transitivity reasoning: - BYOL and BYOL' are almost equivalent - RAFT and BYOL' are shown to be equivalent under some assumptions.Thus, conclusions that are drawn from analyzing RAFT should still hold while analyzing BYOL. They thus link the interest of BYOL's predictor and the EMA through the RAFT loss decomposition. I have multiple strong concerns regarding this paper. These concerns are both on the paper results, shortcuts in the analysis, and the writing style. Results:-------------- - In section 4, the authors introduce BYOL' as a variant of BYOL. To do so, they derive an upper bound on the BYOL loss, i.e. the L2 distance between the projection and the projector, and they try to minimize it. However, this approach disregards that BYOL does not minimize a loss (due to the stop gradient). In other words, the BYOL objective keeps evolving during training; the target distribution is non-stationary.  As mentioned in the BYOL paper: "Similar to GANs, where there is no loss that is jointly minimized w.r.t. both the discriminator and generator parameters; there is therefore no a priori reason why BYOLs parameters wouldconverge to a minimum of L_BYOL given the online and target parameters". Minimizing an upper-bound is at best insufficient, at worst a non-sense. The sentences, "minimizing L_{BYOL'}  would yield similar performance as minimizing L_{BYOL}" and "we conclude that optimizing L_{BYOL'} is almost equivalent to L_{BYOL}" are unfortunately wrong.  This is somewhat highlighted different qualitative results in Appendix F.1.b != F.1.d.A better approach would be to ensure that the *gradients* go in a similar direction (so the training dynamics are similar rather than the objective function). However, even such a demonstration could be insufficient due to compounding factors in the training dynamics.  - The 1-1 mapping between BYOL' and RAFT rely on three hypotheses. While (i) and (ii) are reasonable, hypothesis (iii) is quite strong, and more importantly, neither elaborated nor discussed. In other words, I am unable to validate/invalidate the interest of the theoretical results. Would it be possible to measure the normal gradient empirically? To bound it?  - In section 3, i would recommend the author to mention that multiple components were also in the BYOL paper; especially when writing "therefore, we conclude the predictor is essential to the collapse prevention of BYOL." - Although I acknowledge that self-supervised learning requires heavy computational requirement, and few teams may run experiments on ImageNet. Yet, I would recommend the authors to not use CIFAR10 as the dataset has multiple known issues (few classes, small images, few discriminative features). Other variants such at STL or ImageNete can be trained on a single GPU over a day, and are less prone to misinterpretation in the results. Besides, I want to point out that BYOL was not correctly tuned: the experiments are based on a different optimizer (Adam vs LARS) and no cosine decay were used for the EMA, while these two components seem to be critical, as mentioned in BYOL and arxiv:2010.1024. Overall, I have a serious concern about the paper's core contributions. However, there are still some good elements in the paper that I think are under-exploited: - RAFT is itself an original, new and interesting algorithm. The potential link to BYOL is indeed an interesting lead, but in its current state, I would make it a discussion more than a key contribution. - Table D.3 shows that RAFT/BYOL' does not collapse without predictors when \beta is high. Albeit providing low accuracy, a non-collapse is quite surprising. Unfortunately, the authors leave it for future workShortcuts:--------------I was surprised by multiple shortcuts in the reasoning process or undiscussed conclusions: - The authors mention that the predictor is a dissatisfactory property of BYOL. Could they elaborate? This is actual the key component of the method (if not the only one!), and such pro/cons could be detailed in light of other methods.  - In section 4.1, the authors mention that: similar accuracies and losses are sufficient somewhat confirm that BYOL and BYOL' are similar. Two completely different methods may have the same errors while being radically different...  - In Section 4.2, the authors mention that "Based on the form of BYOL, we conclude that MT is used to regularize the alignment loss". However, there is no experiments to try to contradict/validate this claim. Differently, the EMA may ease the optimization process or it may have different properties. Even if I understand the logic behind this statement, I regret that the authors do not try to confort it. - In section 4.2, the authors mention that there exist multiple works (while only citing one...) demonstrating that EMA is "roughly" equivalent to sample averaging and may encourage diversity. While this is sometimes true in specific settings (cf. markov game and fictitious play), this is also known to ease optimization (cf. target network in DQN). Stating that RAFT is better than BYOL because it better leverage the EMA target is tricky without proper analysis.- Albeit understandable, the transitivity between BYOL and RAFT is difficult to defend due to multiple approximations and hypothesis. Therefore, it is of paramount importance that the approximations and hypothesis are validated, which is not sufficiently done in the paper. Writing: -------------- - Although papers' writing quality remain subjective, I tend to expect a formal language. I kind of feel ill-at-ease when reading sentences including "BYOL works like a charm", "disclosing the mistery", "to go harsher", "bizarre phenomon". Other sentences also expresses judgement such as "inconsistent behavior", "dissatisfactory property of BYOL" or "has admirable property" without proper argumentation.  - It is non-trivial to follow the different version of the algorithms... which are defined in the appendix. Please consider renaming BYOL'.  - A related work section would have been useful to put in perspective BYOL that are theoretically motivated e.g. AMDIM, InfoMin, other self-supervised learning methods without negative example, e.g. DeepCluster, SwAV. Section 2 is more about the background, not related work. - there are a few confusions in the notation, \alpha \beta have different meaning across equations (Eq 7 vs 8) - In section 3, random is ill-defined. In Cifar10, random should be 10%, I assume that you refer to random projection. Please clarify.   - Figure 1 is clear, and I recommend to keep it as it is. - From my perspective, the mathematical explanation in Section 5 is quite obfuscated, and I would recommend a full rewriting. - Please avoid unnecessary taxonomy, e.g. uniformity optimizer, effective regulizers and others. - In conclusion, you mentioned some results about the projector. However, you never detail them in the paper. Please, do not discuss unpublished results.Overall, I had difficulties following the paper: I keep alternating between the appendix, previous sections, and the text. Again, the phrasing makes me ill-at-ease.Conclusion:--------------------I have some serious concerns about the core results of the paper. Importantly, Theorem 4.1 follows a misinterpretation of the BYOL training dynamics. From my perspective, there are too many unjustified claims, and I cannot recommend paper acceptance. However, there is some good idea in the paper, and I strongly encourage the authors to study RAFT independently of BYOL in the future. The paper discuss how to detect erroneous steps in gradient descent on a non-trusted GPU using a separate slower trusted execution environment,by randomly deciding in each step whether to check the values returned by the GPU, as well as using small learning rates and clipping the gradients to ensure all updates are small.The reason for checking this is based on the assumption that since GPU calculations are out sourced there may be trust issues and attackers with control of thevalues returned by the GPU can alter the final network in subtle ways. The paper includes experiments and shows that this approach is faster than just running everything on the trusted execution environment.The experiments test an attack approach where the attacker tries to inject some bad samples to get some success of making the network being trained output some particular class on a particular image kind.I have questions about the model. It is explained that the attacker is full control of CPU and GPU etc. but it is also assumed the attacker runs the code on GPU as expected. I would prefer a complete black box model, where the system can input model, batches, parameters etc. to the GPU and get whatever it wants back, i.e. gradients, activations, whatever you desire and given an input the attacker can decide to return whatever you want.It seems the attacker is seemingly completely oblivious to the actual network, and parameters being used, and only measures his current success rate, and seems very naive. Finally, the time is measure compared to a pure based TEE solution which is stated in the paper as completely unsatisfactory, and should instead be compared with training time without using any form of verification as this is the target.In my opinion this paper is an implementation of a straight forward idea and the theorems for setting the probability parameters  are basic probably computations.There is no theoretical contribution in the paper and all the arguments are heuristically based on some intuition about training dynamics.In short, in my opinion the paper is simply not relevant or strong enough to warrant acceptance at ICLR. ## Summary:---This paper raises and studies concerns about the generalization of 3D human motion prediction approaches across unseen motion categories. The authors address this problem by augmenting existing architectures with a VAE framework. More precisely, an encoder network that is responsible for summarizing the seed sequence is shared by two decoders for the reconstruction of the seed motion and prediction of the future motion. Hence, the encoder is trained by using both the ELBO of a VAE and the objective of the original motion prediction task. ## Pros:--- The paper has a novel and interesting direction as robustness to distribution shifts has not been studied before in 3D human motion modeling. It is implemented around one of the SoTA models based on Graph Convolutional Networks (GCN) using discrete cosine transformation (DCT) features extracted from the motion sequence. To simulate the out-of-distribution scenario, the baselines and the proposed extension are trained on a single action category such as walking and tested on the remaining actions such as eating and sitting. Experiment results on the H3.6M and CMU datasets show that the proposed approach is useful on out-of-distribution (OoD) test cases. ## Cons:--- I have two main concerns on the proposed benchmark and the models.  -- OoD Benchmark--- It looks like there is a significant underfitting problem. The performance of GCN on the walking category is 0.56 at 400 ms while the in-distribution (ID) performance with OoD training is 0.66 (Table 1). The training split for the OoD setup proposed by the authors is possibly too small. I also can not grasp the motivation for selecting a training set as small in quantity, and narrow in domain as possible (Section 3). While there is not enough or barely enough training samples, the comparisons might be misleading. We do not know how the proposed extension behaves on the standard task. The authors should compare their models on the main task as well. - Motion samples from different categories (i.e., walking, eating, etc.) can still be useful for the models in learning the 3D human motion prior. In fact, it has been shown by Martinez et al. (2017) [4] that training motion models with _all_ available actions improves the performance significantly compared to a single-action models as done in this paper. While the proposed approach outperforms the baselines in average performance, it is not always or substantially better on the fine-grained actions.  - It is a tedious setup, but a leave-one-action-out strategy can be more reliable. In my opinion a better option would be training on one dataset and testing on another one. This would allow for an evaluation of the existing (and even pre-trained) models directly where the proposed extension would remain as the only factor for evaluation. In the context of H3.6M and CMU datasets, this might not be straightforward due to different skeletal configurations. Yet there exists a much larger benchmark for 3D motion prediction: AMASS [1]. This would be a suitable candidate for this task as it is a collection of several diverse mocap datasets with different motion categories. It would be very easy to train on a subset of datasets and test on the remaining ones as all the datasets follow a unified skeletal configuration. Note that this is only a suggestion to improve the current work and I am not asking for running experiments on AMASS for the rebuttal as it would drastically change the submission. -- Quantifying the OoD--- The existing architectures are augmented with a VAE latent space and a decoder, which is not technically novel. However, regularization of the representation space and reconstruction of the inputs as auxiliary tasks seem like helpful to the motion prediction task and a good contribution under the limited training/evaluation protocol. At the same, time the proposed evaluation protocol is not orthogonal to the task but instead it just follows the existing task protocol. In other words, it is not an independent metric/method/framework for assessing the existing models OoD performance. I am asking the following questions as the proposed approach is presented as a framework:- The authors hypothesize that motion prediction in generative modeling frameworks can alleviate the OoD problems. I find it too broad as generative modelling can be applied in various frameworks. Although it is conceptually very different, [1] uses an auto-regressive model, which is a generative model by design, and trains the model by predicting both the seed (i.e., loosely reconstruction) and the future frames similar to the proposed approach. Can we say that they also deal with the OoD problems implicitly? How do the authors position their framework compared to this line of work? - The authors only focus on the Seq2seq-based methods for motion prediction and choose a baseline with an implicit temporal model (i.e., using DCT to encode the motion sequences). Hence, the proposed approach seems to be limited to this GCN-based architecture. How can the sequential deterministic models [1, 3, 4] be addressed?  -- Additional comments --- Figures should be improved. Especially the text is hardly readable. - I find it very hard to follow Section 3. It was clear only after I read the section A in the appendix. It would be clearer if some of the findings are discussed in Section 3 already. - The losses in the tables are too high compared to the actual task. I am not sure if there is a qualitative difference between the models as the authors did not present any qualitative results. - Missing related work on 3D motion modelling. I list a s,all collection of SoA representatives below: [1] Aksan, Emre, Manuel Kaufmann, and Otmar Hilliges. "Structured prediction helps 3d human motion modelling." Proceedings of the IEEE International Conference on Computer Vision. 2019.[2] Gui, L. Y., Wang, Y. X., Liang, X., & Moura, J. M. (2018). Adversarial geometry-aware human motion prediction. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 786-803).[3] Pavllo, D., Grangier, D., & Auli, M. (2018). Quaternet: A quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485.[4] Martinez, J., Black, M. J., & Romero, J. (2017). On human motion prediction using recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2891-2900). The authors design a distributional word embedding method inspired by Gibsonian theories of perception. They use matrix factorization techniques to derive low-rank object representations in what they call an "affordance space," linking each object to aspects of meaning shared among different types of physical actions. They argue that the learned representations are interpretable, and that this affordance space "underlies the mental representation of objects."  I unfortunately found the paper both conceptually and methodologically flawed. These criticisms fall mainly under the "Quality" and "Significance" categories, expanded below. First, a summary in pros/cons:  Pros: Designs cognitively-motivated knowledge representations; leverages a diverse set of experiments to better understand and defend these representations.Cons: Conceptual flaws about the content of the derived representations; evaluations are insufficient to support the claims of the paper.  QualityThis paper suffers from both conceptual and methodological issues.1. The claimed "affordance space" is not falsifiably \*about\* affordances in any deep sense. While the original data matrix linking words and their associated attested verb combinations clearly gets at possible event--object interactions, the factorized affordance space doesn't necessarily have this property. The lower-dimensional basis may span the space according to "modes of interaction" as claimed, but equally likely may describe coherent categories of contexts/places in which the actions occur, or categories of agents which perform the action, for example.I actually see three facts reported in the paper that make me think the derived data isn't about affordances per se. First, figure 2b actually shows that some of the dimensions of the affordance space best correlated with SPoSE dimensions are object-taxonomic properties. Second, the evaluation based on the raw affordance matrix (called "PPMI" in Table 1) underperforms the full model by a substantial amount, suggesting that the factorization introduces information not captured in the actual affordance data. Third and possibly most importantly, table 2 confirms that "structural" and "appearance" features are some of the best predicted features from the affordance space.The authors may argue that the set of English verbs used in the raw matrix are not the right basis for affordance knowledge, and that the factorization leads to a better abstract/conceptual affordance knowledge representation less tied to linguistic productions. But this claim about the content of the factorized representation needs to be articulated and substantiated with tests of alternative hypotheses.As a quick analogy in case my point isn't clear: you might learn word embeddings on a Wikipedia dump by factorizing a matrix of word--Wikipedia topic co-occurrence counts. The resulting low-dimensional representations aren't \*about Wikipedia topics\* in any deep sense, no matter the factorization method --- we simply talk about them as distributional meaning representations.2. Regarding the methodology of evaluation 2: is your aim to demonstrate the necessity and sufficiency of affordance knowledge for object feature knowledge? The evaluation demonstrates a rough sort of sufficiency, but not necessity. Demonstrating necessity would require testing alternate representations, I think, which isn't reported. What do you think about this premise/issue?3. Evaluation 1 doesn't seem very meaningful to me. It seems self-evident that representations constructed on the basis of verb--object co-occurrence data will perform well in predicting object--action co-occurrences, and probably better than representations which are not specifically tuned exclusively for that language task. (I agree that it's nontrivial that a corpus-derived representation would suffice here, but I don't find it interesting that it outperforms other more general / less task-specific corpus-derived representations.)  SignificanceThe aim of this paper is not clear to me. It cannot argue for a superior system of word representation, since it does not evaluate these representations on any broad evaluation tests. It also doesn't make a convincing cognitive argument about the content of mental representations, given the conceptual and methodological issues in evaluation 2, discussed above. (A convincing cognitive argument would also need to draw on data from human behavior beyond the sort gathered on AMT, or possibly neural evidence; see Mitchell et al. (2008) as an example.)  OriginalityBecause I haven't closely followed the relevant literature, I can't speak to the originality of the embedding method. That being said, it doesn't seem like a substantial conceptual innovation to me. I would be more motivated to let this slide if the paper were stronger on the experimental / analytic side.  ClarityThe paper is clearly written and the authors provide plenty of supporting supplemental information. Some minor comments on this front:* Figure 1 is not very useful, either for assessing success of the method or for understanding its shortcomings. For the latter purpose, maybe consider showing the \*residuals\* of the regression, so we can understand where affordance information performs relatively better/worse across SPoSE dimensions?* Gibson (2014) citation should be Gibson (1979).  Mitchell, T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A., & Just, M. A. (2008). Predicting human brain activity associated with the meanings of nouns. science, 320(5880), 1191--1195\. *Summary*The authors tackle the problem of zero-shot learning, that is, the recognition of classes and categories for which no visual data are available, but only semantic embedding, providing a description of the classes in terms of auxiliary textual descriptions. To this aim, authors propose a method dubbed  Image-Guided Semantic Classification in which a two-stream network (fed by either visual and semantic embedding) learns a compatibility function whose recognition performance is enhanced by means of calibrated stacking (Chao et al. 2016).  *Pros** The method is simple and easy to understand. *Cons** The computational pipeline is not novel and largely inspired by methods such as the ones proposed in [Yang et al., A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015] or [Liu et al. Generalized Zero-Shot Learning with DeepCalibration Network, NeurIPS 2018] which are not even cited in the paper, unfortunately. Authors seem essential to add a calibration module to this kind of architectures: since the calibration module is inherited from prior works (Chao et al. 2016), I found the method quite incremental.* Within the experimental comparison, GAN-based methods are not reported although being a mainstream class of state-of-the-art methods. I am referring to works such as CLSWGAN [Xian et al. Features Generating Networks for Zero-Shot Learning, CVPR 2018], f-VAEGAN-D2 [Xian et al. A Feature Generating Framework for Any-Shot Learning, CVPR 2019],  CADA-VAE [Schonfeld et al., Generalized zero- and few-shot learning via aligned variational autoencoders, CVPR 2019], DLFZRL [Tong et al., Hierarchical disentanglement of discriminative latent features for zero-shot learning, CVPR 2019] or tf-VAEGAN [Narayan et al. Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification, ECCV 2020]. Authors justified this approach by reducing the methods in comparison within the ones which use unseen semantic embeddings only at test time. However, the knowledge of semantic embeddings also for the unseen classes is something which is necessary in a zero-shot recognition paradigm: the authors themselves take advantage of them during the nearest neighbor search. Thus, at this point, using the semantic embeddings for the unseen classes is therefore legit and I do not see any added value in constraining the experiments.* The reported performance is highly suboptimal with respect to the state-of-the-art: invertible zero-shot recognition flows, recently proposed at ECCV 2020, greatly outperformed the proposed approach by margin: H=49.8 on aPY, H=54.8 on SUN, H=59.4 on CUB and H=68.0 on AWA2.  *Final Evaluation*I regret to register a substantial overlap with prior methods, thus undermining the novelty impact of the present submission. On the experimental side, I found a sharply gapped performance which is highly inferior with respect to the state-of-the-art, caused by reducing the approaches included in the comparison on the basis of which methods exploit unseen semantic embeddings for training (this claim is not convincing in my opinion, unseen semantic embeddings are used in any cases, why not exploiting them for feature generation purposes?). For those reasons I am afraid to discourage the acceptance of the manuscript SummaryThe paper studies reward-free reinforcement learning (RL) methods based on empowerment. The authors propose a technique for empowerment estimation under the assumption that a state after H timesteps can be factorized as a product of the current action and a matrix G(s) that depends on the current state. In contrast to the existing methods that rely on the optimization of variational lower bounds on mutual information for empowerment estimation, the technique allows having an almost closed-form expression for empowerment. The authors qualitatively demonstrate the convergence of their method to the true empowerment function on simple environments such as 2D ball-in-box as well on image-based Pendulum environment.Strengths- Under the proposed factorization, the empowerment estimation requires only calculating SVD of the G(s) matrix and line search for satisfying constraints.- The plots for 2D ball-in-box environment demonstrate that the proposed method estimates empowerment more accurately than other algorithms that have empowerment estimation as a component.Weaknesses- The experimental results could be demonstrated on harder environments such as MuJoCo. For example, DIAYN [1], one of the methods the authors compare with, provides results on Ant and Cheetah environments.- The proposed factorization, which is a key component for empowerment estimation, might be too restrictive and not scalable. Moreover, the reviewer did not find the details on how exactly G(s) is calculated except that it is the output of a neural network.- The positioning of the paper is unclear. If the main contribution is the improvement over existing reward-free RL methods, then it is more appropriate to demonstrate that the agent that uses estimated empowerment achieves high returns in the environment or has an interesting emergent behavior. If the main contribution is the improvement in empowerment estimation, it is more appropriate to compare with methods designed for mutual information estimation.- The overall clarity of the paper could be significantly improved.RecommendationThe reviewer votes for rejection. The methods the authors compare with are not designed for empowerment estimation, they use empowerment only as a proxy reward e.g. for overcoming exploration or learning skills. Moreover, it is unclear whether the method will scale beyond simple environments. Addressing the outlined weaknesses might increase the final score.Notes:1. The connection between equations (1) and (3) which both define empowerment is unclear. 2. It would be helpful to provide more experimental details. For example, the authors state that their method requires only training only two neural networks compared to three networks for variational lower bound methods. However, the benefit of this is unclear without providing the architectures of the neural networks and the total number of parameters.3. The code and additional materials were not available in the Google Drive folder indicated in Supplementary Material at the moment of submitting the review. However, this did not influence the final score.[1] Eysenbach, Benjamin, Abhishek Gupta, Julian Ibarz, and Sergey Levine. "Diversity is all you need: Learning skills without a reward function." arXiv preprint arXiv:1802.06070 (2018). Summary:- This paper proposes a two-stage learning algorithm to address imbalanced datasets. Only data-rich classes is used in the first representation learning stage. In the second stage,  an exemplar memory bank with graph matching is used together with standard classification. Experimental results on benchmarks highlight the effectiveness of the proposed method. Strengths:- The total experiments conducted are thorough and satisfactory.Weaknesses:- The presentation of this paper can be improved. For example, the notation in Figure 1 is very hard to parse. The writing could be improved as well. There's a chance that I didn't understand the paper so I just list all my concerns in questions.- In the implementation detail section, there's only implementation details shared with the baseline. There's no detail about the proposed method, e.g., what's the choice of $\lambda$, s, how does the authors actually do the two-stage training. what is fixed and what is learned in the second stage? Questions:- For Figure 1 (c) and (d), why is the L2 norm of the overall gradient larger than both grad1 and grad2?- What's the rationale behind the design of memory bank? To be more specific, what's the rationale behind the design of Equation (3), (4)?- "In contrast to the aforementioned strategies, we approach the long-tailed recognition problem byanalyzing gradient distortion in long-tailed data"  How does the proposed method get connected with this statement and differ from other two-stage training algorithms?- For equation (6), what's the intuition to use $a_{ji}$ to reweight each norm?- For $L_{intra}$, why do the authors choose hinge loss rather than cross entropy? To the best of my knowledge, Hinge loss does not work well in deep learning, esperically with large amoung of classes as the gradient can be vary sparse. # SummaryThis paper works on long-tailed classification. The authors conducted an analysis and claimed that the difference of gradients computed on the head and tail classes plays an important role in the performance drop. The authors then proposed a two-stage approach to first train on the head classes and then train on the tail classes in an incremental learning fashion. The proposed algorithm achieved better performance than existing methods on benchmark datasets.# Strengths- The analysis of the gradients between the head and tail classes seems to be novel.- The proposed methods achieve good performance on benchmark datasets.  # Weaknesses1. The paper lacks a detailed description of how the gradient analysis is conducted. For example, is the gradient computed at some layers or all the layers? How is the variance computed? Is it possible that the relatively large variance is due to a smaller sample size of tail classes? Why does the gradient computed on both types of classes have a larger norm? I would suggest that the authors provided some equations. Moreover, given just Figure 1 and the difference to balanced training, it is still unclear if such a gradient difference really leads to poor long-tailed performance. An analysis of the relationship between gradients and the classification performance will make the paper stronger. 2. The idea of learning the classifier with two-phase has been proposed (Kaidi Cao et al., 2019; Kang et al., 2020). The most similar ones to the paper are [A, B, C], in which the first stage only considers data-reach classes. The authors, however, failed to identify and discuss them.[A] Wang et al., Frustratingly Simple Few-Shot Object Detection, ICML 2020[B] Zhang et al., A Study on Action Detection in the Wild, arxiv 2019[C] Cui et al., Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning, CVPR 2018Memories are also used in (Liu et al., 2019) but the authors did not discuss it.3. Details and motivations of the proposed methods are not clear, making it very hard to understand the proposed algorithm.- It is unclear why the memory bank is needed. Many existing works, including [A, B, C], have proposed to simply train the second stage with all classes, with the subsampled class-balanced data, and shows promising results. Some of them even "freeze" the features but only train the classifier in the second stage.- The way the authors tackle the second-stage training is similar to incremental learning, but the authors failed to discuss those works. One example is [D].[D] Rebuffi et al., iCaRL: Incremental Classifier and Representation Learning, CVPR 2017.- The design of Eq. (2), (3), (4) are not well-described and justified. Why do we need a new state? Why do we need Delta? The computation of Delta seems wrong: c_j + c_j -z_{k+1} = 2*c_j  z_{k+1}? Moreover, what does the state mean here?- It is unclear why we need graphs. It is unclear what z stands for in Eq. (5), (6), (7). It is unclear how a_{ij} is computed.- The motivations and description of the intra-class loss in Eq. (8) is unclear.- I would suggest that the authors provide a figure for their algorithm architecture and pipelines.4. Experiments and analysis:- Back to my comment in 1., there is no analysis to further justify that gradient distortion is really the cause of the poor longtailed performance, and there is no analysis if the proposed algorithm resolves it. There is no analysis if graphs and memory banks are really needed. - It is unclear what Ours(Plain) refers to. What is the model without asynchronous modeling?# MinorI would suggest that the authors replace "asynchronous" with other terms. The proposed algorithm is just a two-stage algorithm. There is no component of distributed learning and communications that require synchronization or not.# JustificationWhile the proposed algorithm achieves promising results, the algorithm is not written, well-described, and motivated. It is also unclear if the gradient distortion is really the cause of poor long-tailed performance. I thus give a score of 3. This paper studies homogenious networks, which is defined by the paper as networks that reuse building blocks with shared or different weights multiple times during the inference of the network. During the inference, the network iteratively use the same set of blocks to process input feature maps with different resolutions, and each step, the output feature map can be used by the prediction head to generate the output. This paper studies the cost of the network, in terms of MACs, parameters, memory footprints, and the accuracy vs. the number of iterations. The author noted that for the studied network, they need to increase the MACs by 3x in order to match the performance of regular networks. Despite this, this kind of homogeneous networks can be useful for novel hardware architectures with limited memory bandwidth. Strength of the paper: this paper presents an interesting study over a novel type of network and studied its accuracy-cost trade-off. Weaknesses ofthe paper: 1/ The organization of the paper should be improved significantly. For example, the architecture of the homogeneous network is a central part of the paper, yet the introduction to it is only provided in two figures, and not in the method description. 2/ It is not clear what should be counted as the contribution of the paper. I am assuming a few possibilities: a) proposing the new homogeneous networks? However, there are already several previous papers mentioning this type of networks. [cite] b) Better performance achieved by using homogenous networks? However, the experiments of the paper show that the network under study is not as competitive as previous baselines cited in this paper. Also, though the authors suggest this type of network can be useful for future hardware processors, but without explicitly mentioning the processor, it is hard to justify this. c) The analysis of the homogeneous network and their accuracy/cost trade-off. This might be counted as a contribution, but it is not clear if this analysis can, say, help us improve the performance of homogeneous networks. Overall, I think this paper is not complete and has not met the standard of acceptance.  This paper considers a FPS game that can be decomposed into two sub-tasks, navigation and shooting. A hierarchical meta RL method is introduced and the updating rules for sub-policies and meta parameters are provided. Experiments focus on this specific environment and hence the hierarchical structure is also specified as a meta controller over two sub-policies defined for navigation and shooting explicitly.The proposed hierarchical RL method is not novel and indeed a most straightforward way to control sub-policies with a meta controller. The number of sub-policies, each of which is specified to solve an explicit sub-task, is fixed given the environment and the rewarding scheme is also clear that each sub-task has its own reward. The final policy is simply presented as linear combination of sub-policies. Actually, similar studies has been well studied in the literature, like Feudal networks and MAML, which are even more general meta learning methods to automatically find sub-policies. These highly related approaches are ignored from the discussion and not considered in the experiments as a baseline.Another question is about the considered global reward, which is also formulated as linear combination of the sub-tasks' reward using the meta parameters \alpha_i's. Under such a formulation, the rewarding scheme is dynamically varying as the training goes. One concern is that in IMPALA the estimated values in vtrace or advantage might suffer large variance since at early stages the meta parameters are almost from scratch. Another concern is that this rewarding scheme naturally restricts the meta controller to select sub-policy with high immediate R_i at a specific time step, and hence it seems the meta parameters are almost determined by the rewarding scheme.Experimental parts lack comparison of many related works as mentioned above. Moreover, only one specific environment is studied with only two sub-tasks. It is hard to see the generality of the method when scaling to cases where a large number of sub-tasks exist. This paper introduces a new first-person shooting environment consisting of two tasks: navigation and eliminating enemies via shooting. It describes a reinforcement learning architecture to that trains individual policies for each task and automatically balances between those policies by computing weights to mix the action distributions and the rewards.Unfortunately, this paper leaves significant open questions about the environment and experiments. This makes it hard to judge the reported results. Chiefly, it is not clear what kind of reward should be maximized; the environment provides two rewards (one for each subtask), but Figure 4 and 5 compare algorithms for a single one-dimensional measure of performance. There are further questions around this: what kind of reward is used for the baselines (which are supposedly trained with a single reward)? What kind of reward does the value function for MeSH predict? The mixed reward as determined by the MetaNet? Then: The paper mentions that LSTM networks are used, but of what size? There are no details on complex baselines such as FuN, which are not trivial to implement or tune. Are the returns reported on a fixed set of levels? Figure 6 seems to imply that alpha weights sum up to one, but I couldn't find any description of this. Furthermore, details regarding the action space of the environment are missing.The write-up would strongly benefit from some proof-reading to fix grammar and expression. Technical details such as the algorithm for procedural level generation or the observation space could be put in an Appendix. First of all I want to point something out I found quite bothersome:The abstract states " A desirable agent should be capable of balancing between different sub-tasks: navigation to find enemies and shooting to kill them."  and the intro begins with "...it is an urgent need to use DRL methods to solve more complex decision-making problems."  I want to state that I strongly believe we should not be framing our research problems with these types of problems, nor trivializing concepts such as killing 'enemies'.  I'll try to be as unbiased in my scientific evaluation of this paper, but I would request that the language be toned down a bit, and ideally other types of tasks considered down the road.Back to the review:This paper presents a multi-task agent architecture with a final mixture component.  The authors show that this approach can work on a custom-built FPS game better than competing SoTA methods (both multi-task and mono-task agents.  Overall the method uses a bi-level optimization to find the optimal mixture of sub-policies as well as individually optimize each sub-policy.Pros:This seems like a relatively simple architecture and the empirical results are promising, and the analysis of alpha values correlation to sub-tasks is interesting and seems to indicate that the meta-controller does gain some insight into sub-task structure.Cons:There are many confusing points about the paper that made it hard to follow and that I would need clarified to argue for acceptance:1. Doesn't min(clip(Á) · A, Á · A) = clip(Á) A ?2. Doesn't $\grad \alpha L train(É + \epsilon , \alpha)  \grad \alpha Ltrain(É + \epsilon, \alpha) = 0$. Perhaps you meant \grad w on the second term?3. What are L_train and L_val?  I couldn't find a clear definition.4.  Do the sub-agents receive the values of the respective R_i or is only $R = \sum_i \alpha_i R_i$ passed to the agent?5.  I am not familiar with bi-level optimization, it could be worth talking a bit more about this instead of environmental architecture choices which are relatively irrelevant to the core of the paper.6. Could you come up with  a task with more than 2 sub-tasks?  I find that 2 is likely a corner case and going beyond just two policies would make the method more convincing.  Trying out on a smaller more synthetic environment that would more easily allow task factorization would also allow for better empirical evaluation.7. Although I am not super familiar with the FPS environments for RL, are there some already existant environments that have been independently benchmarked?  Any FPS env has a mixture of navigation and fighting, so I wonder what the value of proposing a new environment for this would be.Conclusion:I am not an expert of the multi-task literature but although the proposed idea seems to have merrits, the paper would need to be more clear on the points above for me to consider it clean enough for publication.  As it stands the approach is too opaque to really understand what is going on.   This paper mainly investigates the effect of iteration-to-iteration correlations in online learning. It recapitulates a fairly obvious and pretty well-known result that such iteration-to-iteration correlations will slow learning. I find the motivating question (the effect of temporal correlations on learning) somewhat interesting, but unfortunately, I think the research reported in this paper is really not very well-executed:(1) Unlike what the title and sections 1 and 2 claim, the experiments in this paper do not test the effect of temporal smoothness. There is no actual time dimension in the data used in this paper. It rather tests something else: more accurately described as iteration-to-iteration correlations in online learning. This makes the set-up considerably less general and less interesting in my mind compared to the actual temporal ordering question, which has some practical relevance. Relatedly, the illustration in Figure 1A is misleading. This is not the setup tested in the experiments. (2) The models and datasets used in this paper are extremely toy, there is no reason why more realistic datasets with actual temporal structure could not be used for this research. (3) The paper only studies the online learning scenario (Appendix A5 reports the results of an experiment with minibatch training, but this is very limited, and not nearly rigorous enough). This limits the relevance of this work both for machine learning and for neuroscience/psychology. Most machine learning research does not do online learning. Even animals do not have to do purely online learning, because they have offline replay mechanisms that dont have to respect temporal order strictly. (4) The authors propose two mechanisms to alleviate the learning slow-down caused by iteration-to-iteration correlations in online learning. However, it isnt at all clear why the proposed mechanisms help with correlated data. No explanation is given for how these mechanisms are supposed to help with learning from correlated data in the online setting. Please note claiming that these mechanisms are brain-inspired is not an explanation. Moreover, the set-up in these experiments is also not described clearly. Section 5.1.1 says The learning algorithm, optimization and initialization methods, and the hyperparameters were identical to those used in training and testing feedforward neural networks, but you cant do online learning with leaky neurons anymore. Later on (right at the very end of the paper in the Conclusion section!), we learn that the learning setup is actually not identical: backprop is truncated in these models to prevent gradients from flowing into previous time steps. This important detail is somehow never mentioned in section 5. (5) Is it possible that the effect of leaky memory is just due to reduced gradient variance via some sort of mini-batching mechanism? (note that Appendix A5 doesnt address this question). Since the hidden state contains information about previous examples in this model, the memory may be acting as some sort of implicit mini-batching mechanism that reduces the gradient variance.(6) The results in Figure 4A: the baseline no-memory model is outperforming the other models. This seems to contradict the results earlier in the paper (e.g. Figure 2) showing the benefits of memory+gating. What is the explanation for this discrepancy? (7) The experiments are also in general not done very rigorously. For example, no hyperparameter tuning was done for the smooth case, but maybe the problem is just that the learning rate in this case should be slightly different (i.e. no need for special mechanisms like memory or gating). We can never know this unless the experiments are done more rigorously. In this paper, the authors propose a series of methods to tackle policy learning in POMDPs. At the core of the proposed method sits an augmented memory that the agent is allowed to write on and read from at the time of decision making. This allows the agent to store some of its past to be used for future decision making. The authors raise a few training, stability, and limitation issues in prior work. And argue that their proposed method improves over the prior works.The idea of using augmented memory is exciting and fundamental and definitely worth expanding.However, I found a few issues with the presentation and contribution of this paper that I would be happy to share. 1) Second line of abstract:"Learn- ing memoryless policies is efficient and optimal in fully observable environments." It is not clear what the authors mean here. Is the learning of memoryless policy efficient and optimal? If yes, it would great if the authors could parse it.If they mean memoryless policies are optimal, then I recommend the authors to restate this statement since it is incorrect. 2) The authors state "can solve problems that were unsolvable using LSTMs" since it is an impossibility statement, I would recommend either proving a reference for such a statement or provide proof.3) "Notice that neither q-learning nor 5-step actor-critic were able to understand how to use the B1 memory to consistently solve the gravity domain." I guess the authors mean the agent using q-learning or 5-step actor-critic were not able to learn how to use ... .4) I strongly recommend the authors to use more concrete notation. It seems that they study episodic POMDP or maybe a fixed horizon. They mentioned episodic but did not define it. Also, it is not clear what are state, action, and observation spaces. Are they finite? In the analysis I found in the appendix, it seems the authors approach finite ones. But it would be useful to mention it in the main body since there are high-dimensional exps in the paper. Also, q is not defined. I checked the referenced paper,  Jaakkola et al. 1995, there it was also not clear what is q. They first define it for time step zero. Then later use it for any time step. They seem to not define P_\pi(s|m). It would be great if the authors could define these quantities. The authors state that "PÀ (s|o) is the probability of being in state s given that the observation is o, when following policy À" well, at what time step? Please define these terms. 5) I did not understand the point of a recall task example on page 5. As the authors know, there are examples of POMDPs that constant actions are optimal. I am not sure what would a significant conclusion one can draw from such examples.----------------General evaluation:6) Almost all the theoretical statements are straightforward results of definitions and provided for justification. They are useful in understanding the paper. I appreciate the authors for including them. 7) I again encourage the authors to make their notation a bit more concrete. If they study fixed horizon POMDPs, where the stages (time step in the episode) are encoded?8) While I appreciate the augmented memory type policy, it seems the authors' proposed method is quite fragile. For example, for the OAk setting, if the agent needs to store k pairs of (o_t,a_{t-1}) to achieve a good solution, then the method breaks? 9) Such fragileness mentioned in 8 seems not to be an issue in methods that learn latent states. How would the authors handle that?10) Since the contribution is empirical, I would be happy if the authors provide a study against existing baselines, e.g., those referred to in the related works. 11) Regarding memoryless policies in pomdps, I recommend the authors to take a look at Policy Gradient in Partially Observable Environments: Approximation and Convergence. They seem to have some convergence analysis that might be useful. The authors study the problem of learning the rule for the game of life with a convolutional network. This is indeed an interesting problem that can be a great benchmark for studying the lottery ticket hypothesis and related training issues - kudos to the authors for this great idea. Unluckily, the methods and results of the paper do not stand up to the idea. While the game of life is an interesting problem, the methods employed to study it must be comparable to modern deep networks so that the results can be meaningfully interpreted in the broader context. The authors study convolutional models that are to learn multi-step processes, but they use neither residual connections in their models nor do they employ normalization or an appropriate multi-layer initialization scheme like fixup. There is also no study of the optimizer - Adam is used with the default Keras hyperparameters and no ablations are performed, even though other works find crucial benefits in tuning them - and also the epsilon parameter that's not even mentioned by the authors. Therefore, while we find the idea very promising, we need to recommend rejection of this paper until the methods are improved. The authors propose to use the same formulation of adversarial losses over space and time as done by Xie et. al. The block-wise frequency evaluation is a new contribution of the paper. The paper reads more like a report as opposed to showing unique insights for the niche problem the authors have tackled. There is little to no novelty in the MSE and adversarial loss formulation over space and time. The use of a generator and two discriminators is exactly the same as done in tempogan. I can see two differences:1. Use of MSE as additional loss2. Use of generator output from previous time step as a recursive input. Both these additions are incremental and not novel. The evaluation is contrived in my opinion. It is natural that tempogan will underperform wrt to MSE metric. The frequency based metrics are derived by the authors and the sparse qualitative comparison to tempogan on a single example make it hard to be confident about the improvement over tempogan. It needs to be evaluated both qualitatively and quantitatively on a wider range of applications like done in tempoGan to validate its usefulness. I would like evalaution with more neutral metric which exists in literature beyond the ones in the paper. As the evaluation metric is derived by the authors, it is hard to validate the benefit of the approach and the evaluation protocol. There should be ablation studies that discuss the suggested improvements in architecture over tempogan. Also the authors should compare with simple baselines like nearest neighbor and encoder-decoder networks. As the MSE loss is evaluated over paired samples,  the usefullness of GAN should be validated in the paired case. Some discussion on directly optimzing the frequency metric proposed in the paper using a GAN will be useful to the readers. Overall, the originality of the work is very weak and experiments are insufficient.  The paper studies 1) the convergence rate; and 2) the implicit regularization of (stochastic)GMD, a generalization of (stochastic) mirror descent where mirror maps can be time dependent. The contributions are as follows: 1) Theorem 1: for PL+smooth functions, the paper presents a linear convergence result for GMD; 2) Theorem 2, 3: they provide sufficient conditions on the Jacobian of the mirror map, under which (stochastic)GMD converges linearly; 3) Theorem 4: argues for an approximate implicit regularization effect of GMD, by showing that GMD converges to a point, close to the initialization, where closeness is measured in terms of the \ell_2 distance in the mirrored space.- Correctness: I have a problem understanding the proof of Theorem 3. In the fifth line of the proof of theorem 3, page 15 of the appendix, Im not sure how you upper bound the quantity - < \nabla f(w^(t)), J_\phi^-1 \nabla f_{i_t}(w^(t)) > by the right hand side. Of course, the Jacobian is assumed to be positive definite, but how does this upper bound follow? Besides, I found the claim in Theorem 3 quite interesting, and even surprising. It seems to me that SGD, which is SGMD with mirror maps equal to identity, satisfies the conditions of Theorem 3, simply because the mirror map is the identity. Then, the result claims an exponential convergence rate for SGD to the global optima. Can you clarify this, in light of the minimax lower bounds for SGD in smooth and strongly convex setting, e.g. the recent work: https://papers.nips.cc/paper/8624-tight-dimension-independent-lower-bound-on-the-expected-convergence-rate-for-diminishing-step-sizes-in-sgd.pdf- Novelty/Significance of the results: Theorem 3 is novel and seems very interesting, if the authors clarify the issue above, as well as clarify the generality of the assumption, i.e. the conditions required for the Jacobian of the mirror map. Can you clarify what are the technical challenges in proving Theorem 1, and what are the novelties with respect to several related work, including the work of Karimi et al., 2016?- Clarity: In my humble opinion, Theorem 4 cannot be interpreted as an approximate implicit regularization effect, simply because the distance to the optimum measured in the mirrored space can be very large, and the authors do not provide examples when this can actually be small. In the remarks, you point out Hence provided that R is small (which holds for small f(w(0))), GMD selects an interpolating solution that is close to w*  in \ell_2-norm in the dual space. But why should one expect f(.) to be small at the initialization? Specifically, this theorem is proved under the assumption that f(.) is non-negative, therefore, f(w^(0)) being small simply means that w^(0) is already an approximate global optimum.- Presentation: I think there is room for improving the writeup. Here are some examples:   + The presentation of MD-type updates in Equation (1) is quite abrupt, without providing the motivation/context.   + All of the main theorems guarantee linear convergence without actually characterizing the rate of convergence. I found it very confusing as it hides the dependence on important parameters.   + The presentation of the PL* condition in section 3 is very confusing as none of the main results (Theorems 1-3) requires this condition. What is the motivation? It seems that this condition is not preserved under simple transformations such as translation, i.e. if f(X) satisfy PL* then f(x) + 1 will not. What are some interesting function classes that satisfy this property?   + Do the assumptions of Lemma1 (PL*, smoothness, non-negativity) imply existence of x* such that f(x*) = 0?   + Can the monotonicity condition in Theorem 4 be restated in terms of strong/strict convexity of the potential function associated with the mirror map?   + The sufficient conditions in theorems 2 and 3 are not discussed at all. What are some examples of interesting potential functions / Bregman divergences that satisfy these conditions?For the reasons listed above, I dont think this paper is ready to be published at ICLR. Of course, I will happily reconsider my evaluation and increase my score if the authors clarify the issues raised above. ##########################################################################Summary:This paper shows a generalized mirror descent converges linearly if the objective function is smooth and satisfies the Polyak-Lojaciewicz condition. ##########################################################################Reasons for score:  The novelty seems to be very limited. This paper does not actually show the benefit of replacing the gradient of the Bregman function by a general mapping in mirror descent. The analyses are standard and do not actually get more difficult with the generalization. The theorems require the derivatives of the Bregman function to be bounded, making the result not even applicable to entropic mirror descent, arguably the most notable instance of mirror descent. ##########################################################################Pros:  Pros: 1. None. I have not seen a proof showing mirror descent converges linearly with the Polyak-Lojaciewicz condition in literature, so this could be a novelty. But the analyses in this paper are just standard and very restrictive because they require the Bregman functions to have bounded derivatives.  ##########################################################################Cons: 1. I do not see the benefit of replacing the gradient of the Bregman function by a general mapping \phi. The authors use gradient descent, mirror descent, and AdaGrad as examples of the generalized mirror descent in Section 5. However, it is already known that gradient descent is a special case of mirror descent and AdaGrad is mirror descent with a time-varying Bregman function. All three do not need the notion of a generalized mirror descent.(2. The proposed generalized mirror descent is not actually more general than mirror descent. The assumption on \phi in Theorem 1, for example, coincides with the standard assumption in mirror descent literature that the Bregman divergence is strongly convex. Moreover, the condition that \phi is Lipschitz is restrictive (for example, it does not hold for entropic mirror descent) and does not appear in standard mirror descent literature. The conditions on the derives of \phi are even more restrictive in Theorem 2 and Theorem 3. (3. The theorems in this paper only states that the algorithms converge linearly. Please make an optimal choice of the step sizes and make the dependence of the convergence rate on the problem and algorithm parameters explicit. Also please make sure that the exponent in the linear convergence rate does not depend on the iteration counter, as opposed to those shown in the appendix. (4. The paper starts with a discussion on overparameterization. However, I do not see the connection of this paper with explaining the benefit of overparameterization. Indeed, the analyses in this paper has nothing to do with overparameterization, unless the authors connects the PL condition with overparameterization. (5. Isnt the PL^* condition simply the PL condition with the objective function shifted by -f(x^*)? As long as f(x^*) is finite, without loss of generality, one can always do such a shifting and the sequence of iterates does not change. I do not think PL^* is a new condition.  ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above.  ######################################################################### Briefing:This paper proposes a new Bayesian metric learning method, robust to noisy labels. The paper introduces a variational formulation for incorporating the Bayesian framework to triplet-loss training, with supporting experiments.Strong pointsThe paper proposes fancy variational derivation for the Bayesian frameworks for triplet-loss training.Weak points(1) An essential reference [1] and other triplet ablations [2] seems to be missing.[1] Lin, Xudong, et al. "Deep variational metric learning." Proceedings of the European Conference on Computer Vision (ECCV). 2018.[2] Duan, Yueqi, et al. "Deep adversarial metric learning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.(2) Connected to (1), Experiments do not seem to be enough for supporting the superiority of the proposed method.Comments:(1) The author should clarify the difference between [1]. [1] also uses variational formulation for the triplet loss.(2) Experimental setup seems to be not enough. Along with [1], typical retrieval dataset such as CUB-200- 2011, Stanford Online Products dataset, Cars196 dataset should be concerned.(3) Along with [1] and [2], the author should compare recent triplet-loss-based embeddings with the mentioned typical retrieval datasets.Note:The paper seems to miss the related works that must compare. The paper also does not seem to include typical retrieval tasks to compare with other metric learning methods, which has been standard experiments in this field. Unless a clear explanation for this issue are provided, the reviewer cannot avoid rejecting this paper.  Annotating images for training of segmentation models is time consuming and it can be difficult to annotate enough examples to ensure good performance on the rare difficult examples that often occur when methods are applied to real world data. These cases are referred to as corner-cases. The paper therefore proposes a measure based on the discrepancy of a group of segmentation models to identify more valuable images to annotate and add to the training data in a iterative fashion. The approach is tested on the PASCAL VOC database.The problem is important and relevant. I find the motivation clear, however, I disagree somewhat with the reasoning in parts of the introduction (see detailed comments). The approach is reasonable, and there is some evidence that performance is improved on these specific corner-cases by the addition of similarly identified corner-cases to the training set (Table 1). I am not sure how the approach would generalize, however. As I understand it the test-sets where performance is improved consists entirely of corner-cases identified in a similar manner, so whether the approach will generalize, depends entirely on how representable corner-cases identified in this manner are. It would have been much better to use a independent dataset of corner-cases identified manually.Detailed comments- "First, segmentation benchmarks require pixel-level dense annotation", I do not believe this is necessarily true, and there is little need to state this. One could certainly think of useful benchmarks with hard examples only. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans.- "Second, it is much harder for segmentation data to be class-balanced in the pixel level, making highly skewed class distributions notoriously common for this particular task", "Besides, the universal background class (often set to cover the distracting or uninteresting classes (Everingham et al., 2010)) adds additional complicacy to image segmentation (Mostajabi et al., 2015).", while this may be true for training datasets, I do not see how this is a problem for benchmarks necessarily.- I find the description of the construction of the test dataset used in the different iterations unclear. It is my understanding, but I am not actually sure so it would be good to have the approach clarified, that the test datasets ($T^{(1)}$, $T^{(2)}$, and $T^{(3)}$) of iteration 1, 2, and 3 are hard examples, and are thus biased towards the methods involved. That is, they consist of examples that the proposed segmentation model disagrees with the "competing" models the most on. It is clear from the Table that these images are selected both based on mistakes of the competing models and mistakes of the target model. After one and two iterations we see that the target model now does much better on the next iteration of hard examples, but we really do not know how representative these hard examples are. If the methods tend to disagree on a limit number of typical cases, then these cases will be added to the training set and it is not so surprising that improvements in the target model is seen. To evaluate how this approach generalizes to the real world, an independent dataset would have to be used.- "This also provides direct evidence that existing segmentation models could be particularly weak at certain real-world generalization, which is not surprising because the 1,464 training images are deemed to be extremely sparsely distributed in the space of natural images." I am not sure I agree with the evidence part. The dataset in question is selected to be hard (as far as I understand), so it is not surprising that the methods perform worse on it and does not say much about generalization. Essentially this is just evidence that the selection procedure is working as intended.ClarityWhile I believe I understand most of the manuscript to an acceptable level, it contains quite a lot of sentences that would benefit from some editting. Some examples below.- Introduction is wordy, with long and difficult to understand sentences. Words that mean different things than the authors probably intended are also used, see examples "boosting", "spot", "cover" and wrong words are often used.- "While the performance of segmentation models, as measured by excessively reused test sets (Everingham et al., 2010; Lin et al., 2014), keeps boosting", keeps boosting is a bit of a weird phrase here, "keeps improving" perhaps?- "suggesting their insufficiency to cover hard examples that may be encountered in the real world", maybe replace "insufficiency" and "cover" with "inability" and "handle".- "such test sets may only spot an extremely small subset of possible mistakes that the model will make", "spot" is likely the wrong word to use here, maybe contain? But even so, test sets do not contain mistakes, the methods possibly make mistakes on the test set. Consider rewording.- "The existence of natural adversarial examples (Hendrycks et al., 2019) alsoechos such hidden fragility of the classifiers to unseen examples", while I could guess at what the sentence means, it does not really make sense.- "which possess inherent transferability to falsify different image classifiers with the same type of errors", not sure what you mean by this.- "It is clear that images in S are visually much harder.", something is missing.- "weakly labelling method of filtering", what do you mean by this?- "Specifically, given the target model $f_t$, we let it compete with a group of state-of-the-art segmentation models {g_j}^m_{j=1} by maximizing the discrepancy (Wang et al., 2020) between f_t and g_j on D." They are not really competing are they? The point, as I understand it, is not to select the best model, but to find the most "controversial" image.- I would have prefered legends in each figure, as opposed to having to scroll up and down to find the relevant information.- "indicating that many images in $T^{(1)}$ are able to falsify both the target model $f_t$...", the images are not really falsifying the model.- "This suggests that the target model begins to introspect and learn from its counterexamples", the word introspect appears to be wrongly used here (and elsewhere).- "Moreover, the top-1 model on $T^{(0)}$ does not necessarily perform the best on $T^{(1)}$ , conforming to the results in (Wang et al., 2020).", what results are you talking about specifically? And I guess it should be "confirming".OriginalityI don't find the work very original and it is not clear to me if the work is very novel. A lot of literature is referenced under related work, but I find it to be mostly tangentially related. It would be good if the authors could describe how this specific problem has been addressed before in image analysis and particularly image segmentation. A google search brings a number of works on hard negative mining. But "human-in-the-loop" techniques such as interactive training [1, 2] also enable annotators to focus more time on harder examples. The methodology itself is not groundbreaking. Multiple trained models have been used in combination to assess prediction certainty previously and uncertainty has also been used in active learning setups to focus annotations on difficult regions [3]. I am sure there are even more relevant links, but this is what a couple of minutes googling brought up.SignificanceWhile the problem is relevant and the method possibly useful, because of previously mentioned concerns with respect to novelty and generalizability of the results, I do not think it will have a wide ranging significance.[1] Gonda, Felix, et al. "Icon: An interactive approach to train deep neural networks for segmentation of neuronal structures." 2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). IEEE, 2017.[2] Berg, Stuart, et al. "Ilastik: interactive machine learning for (bio) image analysis." Nature Methods (2019): 1-7.[3] Casanova, Arantxa, et al. "Reinforced active learning for image segmentation." arXiv preprint arXiv:2002.06583 (2020). Strength:I appreciate the experimental results demonstrated on challenging datasets like CIFAR100 and ImageNet.Weakness(1) There are two existing papers emphasizing direct training SNN with extremely low latency [1][2]. Therefore, the authors should comment more on how the proposed method is different or is better compared to these two papers (they use a smaller number of time steps.). In addition, I hope the authors to show the performance comparison with these two references. In the experimental results, the paper compares performance with [1]. However, the network size is significantly different. I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method.(2) To my understanding, the method proposed in this paper has nothing different compared to the existing BPTT method with the surrogate gradient. I hope the authors can claim clearly the novelty of this paper. For example, how is the proposed method different or better from [3] and [4]. As far as I know, the only difference is that the threshold and leaky parameters are also trained in this paper. However, this can also be easily done in other existing methods like [2][3][4]. There's no doubt their performance can also be improved since it introduces more tunable parameters.(3) Tuning threshold and leaky parameters may be unbiologically plausible.(4) What is the weight optimization method? As shown in Table 1, it seems the weight optimization contributes more to the performance than the proposed method.[1] Wu, Y., Deng, L., Li, G., Zhu, J., Xie, Y., & Shi, L. (2019, July). Direct training for spiking neural networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 1311-1318).[2] Zhang, W., & Li, P. (2020). Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. arXiv preprint arXiv:2002.10085.[3] Wu, Y., Deng, L., Li, G., Zhu, J., & Shi, L. (2018). Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12, 331.[4] Shrestha, S. B., & Orchard, G. (2018). Slayer: Spike layer error reassignment in time. In Advances in Neural Information Processing Systems (pp. 1412-1421). This paper solves the constrained clustering from a probabilistic perspective in a deep learning framework. In general, this paper suffers from several major problems. I will illustrate my concerns point-by-point.1. The authors mention that none of the existing work in the deep (constrained) clustering models the data generative process. First, this is not true. For example, Semi-crowdsourced Clustering with Deep Generative Models. Second, the authors should illustrate the benefits of data generative process for constrained clustering. That is the motivation of this paper. Unfortunately, the motivation is not strong and clear.2. If I understand correctly, Eq. (9) and (10) are the core techniques for the proposed algorithm. Such a penalty is straightforward in   constrained clustering. 3. In Section 3.4.2, there is another side information, named partition level constraint. The authors might want to explore this as well. This point is not a drawback. Just a suggestion.4. Some traditional constrained clustering methods with deep VaDE features can be involved for comparisons. 5. It is better to provide some insights on robustness with noisy side information.6. How to set alpha? Is there some normalization to make alpha within a small range?7. It is better to show the performance with different numbers of constraints.8. I am thinking whether two applications in the experimental section are practical in real-world scenarios. I mean how to obtain the pairwise constraints? If I were the project manager in charge of annotation, I will directly label their categories, rather than providing the pairwise constraints. It is a very poorly written paper. Basic idea of finding a way to not have to wait for full forward pass is not new. Multiple research papers have been published from the extreme of using stale weight to some form of sub-network backdrop as a proxy for the full network. This paper proposed no new idea for local update. Prior work have all suffered with one or both of these two limitations: a) poor experimental framework, or b) not being able to meet the accuracy bar set by backprop. This work suffers from both.  Very poorly described experimental basis - and failing to come even close to the backprop accuracy target with any decent speedup claim. Former is my biggest concern. Section 6 starts with 'Here we show that performance gains of local parallelism can be realized on real hardware' - with near-zero description of any 'real' hardware, except a footnote on '1000 IPUs on a chip'. Reference: https://deepai.org/publication/loco-local-contrastive-representation-learning - also cited in this details the same idea of local learning (see, Fig 1 - similar to overlapping in this work). And they do offer same benchmark Resnet-50 with ImageNet - meeting accuracy and memory saving.  Other cited work: arXiv:1901.08164 has shown the same local learning concept deliver accuracy for Resnet-152. Therefore, I was hoping this submission would go significantly beyond these, which it does not. It does add one language model benchmark, however, the model is very small (6M parameters). Most concerning part though is its experimental framework - details are almost completely missing. No standard CPU-GPU details are mentioned where the experiments where conducted.  Only mention of experimental platform is in first part of Sec 6: "We implement the models in TensorFlow (Abadi et al., 2016) and train them across 4 or 8 IPUs" - then there is a footnote on that page describing hardware with 3-4 additional lines: "Intelligence Processing Units (IPUs) are massively parallel machine learning hardware accelerators. Each chip incorporates > 1000 processor cores" - This gives me no reason to believe that the authors experimented on any 'real hardware'. Hence, my overall low rating for this work.    Verdict:Recommendation to REJECT; Please consider for ICLR Special Journal Issue with modifications.(Unique situation, please read fully) ############################################################################# Summary:The paper provides insight into the boundaries and feasibilities of a monolithic formulation of multitask learning by neural networks. The authors show how complex tasks can be modularly formulated thus yielding a joint monolithic learning possibility. They also show that such modularity can be used to interpret simple algorithms thereby also leading to their joint learning.The main contributions of the authors are the following:- showing that "the two layer neural network can jointly learn the task coding scheme and the task specific functions without special engineering of the architecture"- "systematic theoretical investication of the extent of this ability" (ability = single network can successfully be trained to perform a wide variety of tasks) - "...primarily interested in the extent to which different tasks may interfere,..." (in a multitask setting)############################################################################# Reasons for recommendation / score: The paper is composed of a vast amount of very good research work. The research results seem significantly novel and definitely not incremental or based on other similar contemporary works. The rigorous mathematics and the attention to many details is laudable. However, the content in the paper literally and logically coerces the reader to constantly look at the supplementary material. I could also dare to say that the paper, strictly, without the supplementary material almost seems incomplete or as a compilation of claims, making for a choppy read. This is clearly an effect of the authors trying very hard to squeeze a lot of content into the 8-page limit. This must have taken a lot of efforts and I definitely can see the work that has gone into writing this concisely.The paper would read much better with all the appendices and supplementary material introduced in the appropriate places, in proper continuum. I could see this paper be re-written almost as a proper tutorial paper in this topic of research. I would in this context, recommend some extra experiments and discussions (commented below also), to make the work more thorough.For these reasons, I must insist that the venue for submitting this work should be a suitable journal such as JMLR, ML, or specifically be rewritten for the Special Journal Issue @ ICLR 2021. It is not suitable to be accepted as a conference contribution, by the sheer magnitude of work and the style it is presented in.Another possibility I see, but do not recommend, is that the main motivation of the paper be modified to a solely task solving angle rather than the science of comparison and analysis. Then the theoretical rigour could be reduced and the focus can land on experimental results. This, in my opinion, could be suitable and a definite ACCEPT for ICLR conference.############################################################################# Note about the reviewer:My area of research is Bayesian non-parametrics applied on to multitask learning. I am not very familiar with the mathematical support provided for the theorems in this work. I cannot promise a critical verification of the correctness of the proofs.Please also keep this in mind when considering my recommendation above. ############################################################################# Pros:  1. The research quality and quantity are exceeding requirements for acceptance!2. The language is clear and crisp when introducing the research area and placing it in context with its related works. I especially like the delineated "Our Results" section. The authors clarify and discuss the topic with respect to two landmark papers very well.3. The flow of thought is clear and makes the reader comfortable with the presented paper structure.4. All the assumptions are made evident and clarified beyond any doubts, there are no hidden assumptions or simplifications. The scope of the focused research is also well clarified.5. There is thorough mathematical justifications, case studies of monolithic formulations, guarantees on bounds and learnabilities in the supplementary material. (I have tried to not give it attention as it is not a necessary part of the submission)############################################################################# Cons:  1. The title, abstract and some parts of the conclusion suggests a tone of comparison. This makes me expect a more involved discussion about the topic "Modular versus Monolithic Task Formulations". The authors have a lot of insight in this matter, however, when it comes to presentation, they fall short to guide the reader through them.2. The authors have carefully cherry-picked the theorems and balanced the extent to which they explain them so that it reads with completeness on the whole. I would argue that the details are important and without the proofs and mathematical involvement the scientific reader is forced to question "why?" or "how?" quite often.3. I would have liked the authors to place themselves better in the research context. I would have liked to know the findings of the authors in the lists of references (in Sections 1.1 and 1.2) with respect to the title of this work. 4. The writing in many middle sections where details are needed, are overtly compressed. This is an effect of trying to squeeze in too much in too little space. The authors do direct the reader to the supplementary material many times.5. The authors do not answer the grander questions they begin the paper with. They analyse other attributes which are aligned in the same directions as these questions. Eg. Is modular construction better than the monolithic ones? When should we use which construction?6. The authors talk about multitask learning in the same context as [Caruana 1997]; That is, they reduce the scope of their analysis to inputs of the same size for all tasks or even same inputs to different functions (or tasks) to be learnt. Is this the general case of "any" multitask learning scenario? 7. The experiments themselves do not seem statistically thorough. There should be more than 3 trials to draw conclusions, especially when the experimental setup is based on parameters drawn from a uniform distribution.8. The authors need to address the few inconsistencies in the graphs they have shown in Fig.2. I definitely would have liked more authors' insight on the observed statistics.9. In Fig.3 are the numbers significant? They are reported in the third decimal position for the Test R-squared values. Some more explanation is needed.10. Is the monolithic formulation of multitask learning effectively: joint learning of the switching function and the task function? It would be nice to read some more of the authors' explanation of how and if they are doing something different.############################################################################ Suggested Presentation Changes:1. There could be more figures explaining the schematics of the networks, explaining the setup etc. Especially Sec.2.3 and Sec.2.4.2. There could be more figures and clearer captions with simpler explanations. This is where the reader looks first. It would be nice to suggest what to expect from such a graph and then highlight any results.3. The details in Table 1 can be made more readable. It is unclear where the focus lies.4. I feel it is important, the authors highlight that the Simple Programming Cosntructs part of their research derives from their novelty in the formulation of modules in terms of mathematical functions!##########################################################boarman################## Suggested Small Corrections: Generally, try to break down longer sentences into shorter multiple sentences.- Sec. Abstract: remove or replace word "underlying"- Sec. Abstract: "... trees over some task-code attributes." Change 'some' --> 'certain' - Sec. Introduction: "As techniques, such as neural networks, for learning with relatively rick classes have been developed, it is ..."- Please see if you can move the references to the end of sentences than in the middle. It makes for better readability.- Sec 1.1: move NTK references to end of the sentence.- Sec 2.2 theorem 2: for omega(1) far A...A subspaces, if A ...- Sec 2.2 theorem 2: for omega(1) separated c...c prototypes, if ||c ...- Fig.3. caption: 1-(Test R-squared). Parentheses helps understand the subtraction from unity.#########################################################################Updates:  Summary: The paper proposes two benchmarks for continual language modeling: one evaluating character-level multilingual drift between languages which share similar characters and second evaluating word-level drift between English corpora of different domains. The setup is online in the sense of evaluation: they evaluate on the new sentences and then train over them (unlike image datasets), and catastrophic forgetting is hence characterised as having higher error than was in the past when there is a switch between the domains/languages. Hence, the loss functions measuring forgetting quantify the height and length of the rise in error. They compare a mixture of expert baselines with gating by different gating methods on this setup.Primary Concerns:1. There are few  sentences and terms that are hard to understand and to me they seem imprecise. Examples would be:     (1.1) Intro: human children still manage to acquire multiple languages without being explicitly asked to keep them separated -- not sure if I buy this as it is known that if children are exposed to situation where there are many languages, they get confused, sometimes many kids find it hard to learn any of them, and it becomes important to give them guiding signal. Do you have any reference to support this hypothesis?    (1.2) Section 3, second para: preventing data leakage: what do you mean by data leakage?    (1.3) Section 3, third para: hard to follow, notation isnt clear. And it seems there is a typo in S_i = \sum_j T_i.    (1.4) Section 3, fourth para: for a model to be resilient to forgetting, it must adapt quickly: this statement is not correct because if a model adapts quickly to a new distribution, the parameter change would lead to forgetting and thats primary the reason why there are regularization based approaches for continual learning enforcing models to be in the vicinity of old parameters. Too much adaptivity does not ensure less forgetting.    (1.5) Section 3, loss after switch: what do you mean by a switch? How do you know when a switch happens (task label is not given)? In practice the loss curve is not smooth. How do you identify the switch? Fig 1 (a) is too smooth, does not represent the real loss curve. 2. Regarding experiments, is it not possible to design much simpler methods which work for this problem? If it's known there is expected to be a character/word-sequence distribution shift, I believe it's likely they can be detected easily with traditional n-gram models and style distinguishing attributes typically used for author identification [1,2]. Why isn't it possible to use a baseline which consists of experts for one domain/language where the character-sequence decides which expert to use instead of these weaker gating-based methods? Also, English/czech/german/french seem very distinguishable and share little in common in terms of character sequences [3], hence I am doubtful of the finding that combining these models will improve any single language performance.3. Why is it not possible to apply traditional continual methods like Experience replay to this setting-- you simply store intelligently selected past sentences in memory (when say error shoots up) and replay using them. There are many other continual learning approaches that potentially could be applied here. Any particular reason for not using them?[1] Koppel et. al., Computational Methods in Authorship Attribution[2] Sapkota et. al., Not All Character N-grams Are Created Equal: A Study in Authorship Attribution[3] Gerz et. al., On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling (edited)  The value of the optimal objective as a function of the cost vector $c$ can be written as $z^*(c) = c^T u^*(c)$ where the optimal solution $u^*$ also depends on $c$. The function $u^*(c)$ is piecewise constant -- there are finitely (resp. countably) many feasible solutions; candidates for $u^*$ -- and so the function $z^*(c)$ is a piecewise linear function of $c$, with gradient $u^*(c)$, wherever it exists (otherwise there is analogous subgradient). Obviously, all it takes for computing $u^*(c)$ is solving -- anyhow -- the combinatorial problem. This is all trivial and well-known, yet the authors do precisely that.Can it be saved by proposing gradients also of w.r.t. constraints? No. These results are (slightly) less trivial but -- as authors admit -- are known since 1975. Moreover, the gradient with respect to $c$ is the only one used in experiments, as far as I understand.Is there independent value in Theorem 1? I do not see it. It seems to be a bulky wrapper around the classical result. It only introduces some sort of transition from a vector specifying a combinatorial problem to a collection of vectors/matrices specifying an integer program. Also, the central concept of generalized gradient merely provides a formal framework to talk about non-unique gradients at boundary regions -- similarly to subgradient, subdifferential -- for the method itself, it has no specific relevance.The claims of better performance compared to cvxpy are also absolutely non-surprising -- cvxpy currently uses a slightly suboptimal -- and a very expensive -- solver for linear programs. That is all. In this paper the authors introduce an asymptotic-equivalent (ASEQ) reward shaping reinforcement learning algorithm, which, they argue, is a relaxation of potential based reward shaping (PB). The authors present results suggesting that the proposed ASEQ method is significantly better than PB. The authors state that this new method is a superset of PB (or a subclass of reward shaping broader than PB) which allows for a policy to be determined by shaping rewards on all intermediate states in a trajectory. There is unconvincing details on how this is done (eg. the authors state that it is more direct). The authors make claims that this new method is suitable in difficult exploration problems in high-dimensional states spaces. They do not show evidence to support this claim and evaluate their method in a 10-d state space robotic manipulation task. The contributions of this work are stated unconvincingly. For example, claims 1 and 2 are the same. The authors do not compare this method with other learning from demonstration methods (e.g. behavioural cloning or Generative Adversarial Imitation Learning)Definition 4.1 is unclear and \mathcal{G} is undefined. There are assumptions that the authors state about the applicability of this method, such as at the end of section 4.1 regarding the likelihood of an agent leaving a well-defined volume of state space. The authors make claims that their method is asymptotically equivalent, that is in the long run, behaviour in a modified MDP will have the same result in a modified MDP. I would urge the authors to clarify this point. The metic state-space (\mathcal{S}, d) is not well defined in Theorem 4.2. The authors present their method as a modification to DDPG, and show that reward shaping leads to better performance on their tasks than no reward shaping. This is a likely outcome, given that they are providing the method with additional information. I would urge the authors to compare against algorithms which use the same information, and stronger baselines rather than just ablative baselines on their method.  Finally, I do not think that this work is a good fit for ICLR given that it is more related to methods in reward shaping for reinforcement learning as opposed to the topics of more general greater interest to the audience at ICLR.  #### Major Issues:1. The biggest problem is, the author claimed "super-pixels help eliminate the search space of off-manifold adversarialexamples, leading to examples which are truly generalization errors" in the introduction, but never provide convincing proofs for this argument. The analysis in Section 3 is basically based on intuition and is full of holes.2. It is very obvious that the authors do not have enough time to polish this paper. This problem is even more obvious in the later parts of this work. To name a few: (a) in the contribution part of Section1, the sub-title is missing for the third part. (b) in Section 3.3, the author first term bilinear transformation as BiLIN but refer it as BiLN in later parts. (c) Some notations are absent or inconsistend. For example, the $u_i$ vector in Equation 2 is first denoted as $u_j$. BTW, what does $\beta$ represents? (d) The information provided by the images is very unclear. What does the column of "CIFAR10/ImageNet" represents? Standard training? This makes the reviewer can not catch the main idea of the work. Even after reading this paper, I still do not understand what method has the author proposed as they claimed in their third contribution.3. The author claimed in the third contribution of their work that their method can encourage on-manifold examples. How will this help improving robustness? Will this decrease the generalization errors?#### Minor Issues:1. In Sec 3.4, the author wrote: "Thus, we are interested in the tradeoff between queryefficiency and search fidelity. This introduces our central research question: How does searching overreduced-dimension increase efficiency, if the search resolution is decreased as a side-effect?" What is the answer to this question. BTW, the author never mention this "trade-off" part in the introduction, which seems very abrupt to me.2. The Q&A in Sec 4.1 are redundant. It basically rephrase the content in the three contribution.3. Why select FID-64? How about features in other depth? The work proposes a new way to analyse flow structured data using what they call flow neural networks, which supposedly better exploits correlations between different connected nodes at different time-points. The approach is tested on the public dataset NumFabric with better results than the compared to approaches.Although I believe I get the rough outline of what the authors are proposing, there is simply too many unclear aspects of the work for it to be published at ICLR. Prior work, especially the benchmarked methods, also seems to be mostly tangentially related, e.g. covering general methods for time-series or graph analysis and not specific methods for traffic analysis.QualityA single dataset is used for comparison and it is unclear how relevant the baselines are, as there appears to be a lot of prior work on deep learning for traffic analysis, some of this is cited, but not benchmarked against. See also the additional references [1, 2], and the large number of works citing them. It would be good if the authors could explain why their chosen set of baselines are relevant.ClarityThe work is hard to understand. An incomplete list of things follows below.- Crucially I don't clearly understand the STI effect as explained in definition 1.- Why do you specifically mention self-driving Tesla cars, wouldn't this be relevant for other self-driving cars as well?- What do you mean by more than 1 million flows, that is, how do you count flows?- "As the advent of many advanced machine learning (particular deep learning), intelligent flow analysis tools are gaining more momentum to proceed", sentence does not make sense.- I do not understand the paragraph starting with "Additionally, in contrast to the learning analysis of formation-agnostic natural objects, ..." What is a formation-agnostic natural object? How can a network system enjoy rich domain expertise? How can network traffic experience something? Etc.- temproal -> temporal- was originated -> originated- "This essentially discriminates", does not really make sense to me in this context.- Sentence starting with "In the path-constrained propagation process as doing in above IP traffic flows,..", does not make sense to me.- What is APP?- It would be good to use the same number of decimal points for the result of each method in Table 1.- "However, all deep learning based solutions achieve several times to tens times better accuracy.", I do not see this in the figure.- Please use consistent naming schemes, see "uniGRU" and "univarGRU".- In summarizing the results (4.1), I think the word improves is a bit unclear when talking about reduction in loss. E.g. the loss is only xx percent of yy would be more clear.- "This is benefited from the STI effect" -> "This is benefiting from the STI effect"OriginalityThe proposed work may very well be original and novel. I am just not convinced.SignificanceIn its present form, I doubt it will have much influence.[1] Polson, Nicholas G., and Vadim O. Sokolov. "Deep learning for short-term traffic flow prediction." Transportation Research Part C: Emerging Technologies 79 (2017): 1-17.[2] Cui, Zhiyong, et al. "Traffic graph convolutional recurrent neural network: A deep learning framework for network-scale traffic learning and forecasting." IEEE Transactions on Intelligent Transportation Systems (2019). Below constitutes the Official Review for Paper2605.Summary of the paper:This paper explores the problem of constructing invariant representations to certain new environments. Specifically, they constrain the problem to a so-called single-environment graph classification or regression task. Authors define a notion of counterfactual coupling, and consider a notion of invariance different from (though not well-justified) the standard literature. Based on the authors' own definition, they consider a few example tasks on random graphs. Under strong assumptions, authors show a couple generalization error bounds. Their bounds appear to be algorithm agnostic and do not take into account the neural network or optimizer properties. On the modeling side, authors propose a model: the model is to simply replace a one-hot vector with a GNN. Small-scale experiments are conducted on two toy datasets to show the proposed model slightly improves four vanilla baselines, on these two toy datasets.Evaluation:While it is clear that papers like Paper2605 have much to offer, the official recommendation is rejection. Overall, it is unclear what contribution this paper has. The problem setting is contrived, over-complicated, and not well-defined. It is hard for one to find the theorems meaningful: they assume strong assumptions and contrived settings, and are not applicable to real problems. What conclusion one can draw from the theorems are also quite unclear. The technical proofs are unimpressive either as they appear to be maneuvering the already contrived definitions with basic inequalities. The proposed model is unclear, unmotivated, and has logical gaps.  No clear algorithm process or code is given. Reproducibility is impossible. No motivation or theoretical guarantee is given, neither were we given evidence how it may compare to other invariant/causal models such as IRM or domain adaptation techniques.Because the theoretical and modeling contributions were unclear, one would expect to see strong experimental results. Yet, the experiments are particularly unconvincing, in that the proposed one had only been evaluated on two toy datasets,  and compared to four baselines. State-of-the-art models such as IRM, REx, domain invariant models are are missing from the comparison. Putting aside the unconvincing execution,  the experiments do not seem to corroborate the theorems (which are ambiguous and unclear anyways) either, making the theoretical/modeling contribution even weaker and more unclear. Additionally, one must complain about the poor writing. This paper suffers from the lack of logic and mathematical rigor; it is full of jargons that are unexplained and undefined. For example, after reading the entire paper, one still can't find a definition of GNN or GNN+, which constitute the main part of the proposed model. It is impossible to imagine the ICLR community will appreciate this paper. Based on the evaluation, a rejection is recommended. $\textbf{Unclear contribution and contrived/trivial theorems}$ One cannot draw any clear conclusion from the theorems. Problem setting and theorems are contrived, over-complicated and not rigorously defined. The technical proofs are unimpressive either as they appear to be maneuvering the already contrived definitions with basic inequalities. ''Definition 1 (Counterfactual coupling (CFC)).'' This definition is simply confusing and contrived. How can you even evaluate over all permutations? This is NP-hard?  The independence assumption is also strong? How is this different from standard definitions? Never explained?''Proposition 1. Let P(Y |G(obs) N(obs) = G (obs) n(obs)) and P(Y |G(cf) N(cf) = G (cf) n(cf)) be the conditional target distributions ''  Proposition 1 seems to be only stating definitions (generalization error etc), how is this even a proposition? ''Proposition 1. a link function Á(·, ·) such that'' link function p is never defined. ''assume Y  Y is discrete'' What about regression?'E  Z+ that describes the graph-processing environment''.  Graph-processing environment is never defined. What is that?''supervised task over a graph input Gn(n e 2) and its corresponding output Y''. Problem is undefined. What is graph classification or regression?  Is the response variable over graph, edge, vertex? Is input one graph or many graphs?'' Consider a permutation-invariant graph representation  : * n=1& n×n  R d''  How is this even possible?  Permutation-invariant graph representation is such a strong assumption?''Proposition 1 shows that an E-invariant representation will perform no worse on the counterfactualtest data (extrapolation samples from (Y, G (cf) N(cf))) than on a test dataset having the same environment distribution as the training data (samples from (Y, G (obs) N(obs))).''  Well, this is apparently wrong? Evidence? ''Other notions of E-invariant representations are possible (Arjovsky et al., 2019; Scholkopf, 2019), but ours through coupling provides a direct relationship with how we learn graph representations from a single training environment.''  Not convincing? Evidence? Well, Arjovsky et al., 2019; Scholkopf, 2019 can be applied to graphs too? You are also missing a great amount of literature on invariant models and domain adaptation techniques.''Theorem 1. Assume our graph-processing heuristic'' What does graph-processing heuristic even mean?''Theorem1.   the outputs of ge1 and ge2 of Equation (1) can only differ in their attributes  Summary: This paper introduces a new PyTorch library for computing Fisher Information Matrices and Neural Tangent Kernel (NTK) in deep learning; with applications ranging from Frobenius norm regularization, second-order optimization, and generalization analysis. The authors begin by providing a background on Fisher matrices and NTK and then present the main components of their proposed NNGeometry library (consisting of Layer Collection, Generator, and  Concrete Representations modules). A brief experimental study is provided in the last part of the paper. Assessment: While I think that a clean and effective computational library for implementation of Fisher matrices would greatly benefit the DL/ML research community, the current work falls short of the standards of an ICLR publication in numerous ways. Detailed Comments:- Towards the end of the introduction, the authors claim "NNGeometry aims at making use of these appoximations effortless...". I really do not see how NNGeometry is capable of doing this. One of the reasons why natural gradient methods (and its approximations such as K-FAC) are rather difficult to implement is that they are "white-box" optimization methods and very much model-dependent, i.e., different model architectures means that the Fisher matrices (and their approximations) can look very different from each other. Please see [1], [2] for the K-FAC approximations needed for convolutional and recurrent networks respectively. As an aside, I do not believe there is even a good open-source code for K-FAC on RNNs especially given the complexity involved. I did not find anywhere in the paper how NNGeometry addresses approximations for different types of layers.- A lot of spelling mistakes and strange notations throughout the paper. Here are a few I found- "jacobian" is not capitalized throughout, the big "O" notation in Equations 3 and elsewhere in the paper are all small "o"'s- The discussion in Section 1.2 is a bit wordy and not precise mathematically at times. I would suggest cutting down and citing [3]. Also, somewhere in the introduction, I think the authors should make a note of the distinction between the empirical Fisher matrix and the true Fisher matrix (and perhaps cite [4]); and be clear about which one they are working with. - Given that this is a library concerned with computing FIM/NTK; there should be some comparisons with existing open-source libraries such as JAX and Neural Tangents? - Lots of issues in Sections 2.2.1 and 2.2.2. It is not exactly clear where the authors are trying to do here; and there are many imprecise/incorrect mathematical statements throughout. I believe that the purpose of Section 2.2.1 was to describe that the FIM defines a Riemannian metric on the parameter space; and that the FIM is a representation of this metric in coordinate form. This is certainly true- but I cannot see the connection of this to the NNGeometry framework. Another purpose of this subsection was the notion of duality; for example, which objects may be pushed forward/pulled back (to be more precise, which ones live on the tangent and cotangent spaces). I would encourage the authors to look at the publicly-available JAX documentation/tutorial; where it is explained nicely how all of the theory + code fits together; JVP (Jacobian-vector products) / forward-mode autodiff <--> pushforward map of tangent spaces, VJP (vector-Jacobian products) / reverse-mode autodiff <--> pullback of cotangent spaces.- It would be great if the authors were more clear and explained explicitly the tricks in the sentence "NNGeometry's generator incorporate similar tricks in several other places, including in implicit operations". Many of these types of tricks are known to practitioners who have had to implement FIM (and its approximations); so I am curious what is the novelty provided by NNGeometry's generator here.References:[1] Grosse, Roger, and James Martens. "A kronecker-factored approximate fisher matrix for convolution layers." International Conference on Machine Learning. 2016.[2] Martens, James, Jimmy Ba, and Matt Johnson. "Kronecker-factored curvature approximations for recurrent neural networks." International Conference on Learning Representations. 2018.[3] Martens, James. "New insights and perspectives on the natural gradient method." arXiv preprint arXiv:1412.1193 (2014).[4] Kunstner, Frederik, Philipp Hennig, and Lukas Balles. "Limitations of the empirical Fisher approximation for natural gradient descent." Advances in Neural Information Processing Systems. 2019. This work proposes to improve the generalizability of bAbi models through [entity permutations].More specifically the approach assumes domain knowledge of word/entity type equivalences, which helps restricting possible permutations between word POS (e.g., John vs why) or gender (e.g., John vs Mary). Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation. Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models (especially TPR) to handle artificial data sets with large number of entity names. Overall I find the proposed research not very well motivated. Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach. Commonly used strategy is to replace named entities in sentences with their word type tokens . e.g., from [how old is Obama] to [how old is PERSON]https://arxiv.org/abs/1601.01280https://arxiv.org/abs/1611.00020The proposed approach seems to achieve a similar effect, but is a lot more complex. The authors take an attempt at offline RL thanks to a mix between behavioural policy regularization and model based policy optimization. They basically combine two algorithms: AWAX and, depending on the level of safety given an epistemic uncertainty evaluation, MOPO may be additionally used to fine tune the policy.Unfortunately the work suffers from several severe weaknesses:- the writing is not good. See the typo and minor comments section.- the positioning is biased and missing to many accounts. Out of 31 citations, 12 are from the same author. Even more problematic, most, if not all, the references on offline RL are from this author, and therefore lacks diversity. In particular Model-based offline RL [Iyengar2005,Nilim2005], and model-free offline RL [Thomas2015b] have a rich history. More specifically, Equation (3) is identical to that of the Reward-Adjusted MDP found in [Petrik2016]. The Safe Policy Improvement objective has been considered for instance in [Thomas2015b,Petrik2016]. Equation (7) proposes to optimize the policy under a constraint on the policy search (identical to online TRPO, which is evoked later) that is very similar to [Laroche2019], except that the constraint is not state based and therefore probably less efficient.- "If our initial policy does not achieve expert level performance, and we are confident that we can learn an effective model with the available data, then ..." => it is unclear how these decisions are sorted out. Performing those safety tests are an area of research in themselves [Thomas2015a].- even if we assume that the algorithmic novelty is proven, it seems pretty incremental, since it amounts to perform a test to decide between two algorithms.- Finally, the experimental results do not savethe day. We observe that "Ours" is always the max of AWAC and AWAC+MB2PO, which is a little suspicious, since we have no information on how the decision is made. In comparison with CQL, it is not better (but it is a strong baseline). So, it's not improving the state of the art. It would have been informative to show the behavioural performance in each setting.Typo and minor comments:- AWAC is used without citation or explanation first (2.1)- "The most common off-policy model-free algorithms are actor-critic algorithms that alternate between policy evaluation and policy improvement in order to learn an effective policy." => this is not actor-critic but policy iteration.- "Otherwise, we use the fully trained AWAC policy. These results are reported in the column Ours in Table 1." => otherwise what?- Sec. 4.2: effect => affect- Sec. 4.3: degredation => degradation[Iyengar2005] Iyengar, G. N. Robust dynamic programming. Mathematics of Operations Research, 30(2):257280, 2005.[Laroche2019] Laroche, R., Trichelair, P., & Tachet des Combes, R. T. (2019, May). Safe policy improvement with baseline bootstrapping. In International Conference on Machine Learning (pp. 3652-3661).[Nilim2005] Nilim, A. and El Ghaoui, L. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780798, 2005.[Petrik2016] Petrik, M., Ghavamzadeh, M., & Chow, Y. (2016). Safe policy improvement by minimizing robust baseline regret. In Advances in Neural Information Processing Systems (pp. 2298-2306).[Thomas2015a] Thomas, P. S., Theocharous, G., & Ghavamzadeh, M. (2015, February). High-confidence off-policy evaluation. In Twenty-Ninth AAAI Conference on Artificial Intelligence.[Thomas2015b] Thomas, P., Theocharous, G., & Ghavamzadeh, M. (2015, June). High confidence policy improvement. In International Conference on Machine Learning (pp. 2380-2388). ##########################################################################Summary:The paper develops a method to select a radar return region to be sampled at a higher rate based on a previous camera image and radar recording. Furthermore, the paper validates that an end2end transformer model trained on both camera and radar data outperforms an end2end transformer model only trained on camera data and hence, supports the argumentation to add a radar sensor to an automated vehicle.##########################################################################Reasons:Overall, I vote for rejecting the paper. While it is generally a great idea to guide the selection of radar regions to be sampled at a higher rate the paper is very application-focused and lacks novelty in its method. The result that camera and radar data combined will outperform camera data only is expected. Using detections in images to guide the radar reverses its advantage to work well in adverse weather conditions compared to the camera.##########################################################################Pros:* Interesting and relevant topic* Training results support claims##########################################################################Cons:* Lack of algorithmic novelty* Use of Faster R-CNN (slow)* Using camera to select most important radar regions contradicts the stated advantage of radars to perform better in adverse weather conditions ## RecommendationI like the submission's high-level goal of trying to do careful, controlled studies of different factors which can affect a NAS algorithm's ability to ability to rank different candidate architectures within a search space. And the paper seems reasonably well-organized and well-written. However, due to some concerns about the paper's methodology and conclusions (discussed below), I do not feel comfortable recommending the paper for acceptance in its current form.## BackgroundThe goal of Neural Architecture Search (NAS) is to automatically identify network architectures from a human-defined search space which have good accuracy/speed tradeoffs. However, ranking the accuracies of different candidate architectures within a search space can be extremely compute-intensive if done naively, by training each candidate network from scratch and then evaluating it.The submission conducts empirical studies on the NASBench-201 benchmark dataset for two previously proposed classes of techniques which can be used to efficiently rank different network architectures within a search space without training all of them from scratch:1. train a one-shot model -- a single shared set of weights which can be used to evaluate many different candidate architectures.2. train a predictor by (a) sampling a limited number of candidate architectures from the search space, (b) training/evaluating each one, and then (c) using the resulting <architecture, accuracy> pairs to train a regression model which can predict the accuracy of *any* architecture within the search space.## Paper ContributionsThe submission conducts experiments to estimate how changes in training/evaluation setups will affect the ability of these two methods to rank different candidate architectures within a search space. For one-shot models:* Evaluating how the one-shot model's ability to rank the most accurate architectures in the search space vs. ranking random architectures evolves over the course of a search.* Evaluating two tricks for reducing the cost of training a one-shot model: (a) reducing the number of layers in the one-shot model, or (b) reducing the number of filters per layer.* Changing the architecture sampling strategy that's used when training the one-shot model.* "Post de-isomorphism:" The NASBench-201 search space has multiple ways to encode the same architecture which produce different accuracies according to a one-shot model; here, the authors propose taking the average accuracy over all possible encodings of a given architecture.* Some experiments to quantify the predictor's ability to rank large vs. small models.For predictors/regressors:* Comparing the relative ranking ability of predictor models trained using random forests regressors, MLPs, LSTMs, and GATES.## Concerns about methodologyMany of the paper's experiments -- especially in earlier sections -- seemed reasonable. In particular: "the influence of channel vs layer proxy" (Figure 3) and the effects of post-de-isomorphism (Table 3) and sampling (Table 4) on one-shot model / supernet rankings. However, I have concerns that about a number of experiments, especially in later sections of the paper:**Insufficiently tuned hyper-parameters?**In Section 5.1, when describing their experiments comparing different predictors/regressors, the authors write that "For optimizing MLP, LSTM, and GATES, an ADAM optimizer with a learning rate of 1e-3 is used, the batch size is 512, and the training lasts for 200 epochs." This makes me concerned that (i) the authors used a default set of hyper-parameters to optimize their models, and (ii) few attempts were made to tune the hyper-parameters. Because the quality of models trained using gradient descent can be quite sensitive to the training hyper-parameters, this makes me wonder whether the authors would've drawn different conclusions about the performance of these predictors if they'd spent more time on hyper-parameter tuning. For example: experimenting with learning rate schedules, tuning the learning rate and weight decay used during training, tuning the MLP's hidden layer sizes, etc.The authors also claim that "training predictors with regression loss is not stable and sensitive to the choice of training set." If the authors' hyper-parameters were not sufficiently well-tuned, that could explain the instabilities the authors saw. Other standard tricks (e.g., normalizing the labels to have mean 0 and variance 1 before performing a regression) could also drastically affect this conclusion.**Seemingly trivial results presented as non-trivial?**In Section 4.5, the submission argues that ""the average rank diff shows a decreasing trend, which means that the larger the model, the easier it is to be underestimated." And the intro states that "parameter sharing evaluator tends to over-estimate smaller architectures." These findings are presented in a way which suggests that there's bias in the one-shot model itself, but the observation itself seems trivial, and would likely hold even if the one-shot model was unbiased. To take an extreme example: the one-shot model can underestimate but cannot overestimate the rank of the most accurate architecture in the search space. So the expected rank of this top architecture would almost certainly be less than the true rank unless the one-shot model provided a perfect ranking of architectures within the search space.**Unsupported claims related to multi-stage training?**In Section 5.2: the authors claim that "As shown in Fig. 5, P@0.1% increases a lot after multiple stages of training, which indicates that the predictors perform better and better on distinguishing good architectures. This is expected since the training data are more and more concentrated on good architectures."If I understand this figure correctly, the authors are using progressively more training data at each stage. In this case, the improvements could simply be explained by the fact that they're using a larger training dataset, and have nothing to do with the fact that the training data is concentrated on good architectures. Additional baselines would be needed to test the authors' original hypothesis.## Additional notes**Missing description of de-isomorphic sampling?**I had trouble finding details about how de-isomorphic sampling was performed during one-shot model training. (Please let me know if there's a description I overlooked.)**End of Section 4.2:** "with medium or pool performance:" I think this is a typo. Did you mean "poor" rather than "pool"**Section 4.5:** "We inspect the Ranking Diff of the ground truth performance and the one-shot evaluation of an architecture [...] r_i - n_i. [...] Note that a positive Ranking Diff indicates that this architecture is over-estimated, otherwise it is underestimated." I interpreted "diff of the ground truth performance and the one-shot performance" to mean "[ground truth performance] minus [one-shot performance]". But under this interpretation, I think a *negative* ranking diff would mean that the architecture is over-estimated by the one-shot model. Did you mean to write "ground truth diff of the one-shot evaluation and the ground truth evaluation" rather than the other way around? The paper is generally well presented. However, a main issue is that the optimization algorithms for the l0-norm regularized problems (Section 3.1.2 and Section 3.2) are not correctly presented. Specifically, in the algorithm development to solve the "Fix $\boldsymbol{R}$, optimize $\boldsymbol{Y}$" subproblem, it overlooks the coupling/interaction between the variables $y_1, y_2, \dots,y_M$ and mistakenly obtains a closed-form solution. See Comment 1 for details.This paper proposes two L0 regularized sparse coding and dictionary learning models: one in bound form (with L0-norm explicitly bounded as a constraint in the formulated problem) and the other in Lagrangian form (with L0-norm term appearing in the objective function), and uses it as a robust layer for vanilla deep models. Experiments demonstrate that using the proposed model as a plug-in layer improves noise-robustness of both ResNet/DenseNet for classification and deep PPO models for reinforcement learning tasks.Below are specific comments.1. Main comments:1.1 In Section 3.1.2, the "Fix $\boldsymbol{R}$, optimize $\boldsymbol{Y}$" subproblem is not correctly solved.Specifically, in Eq. (10), the $\left\|\boldsymbol{B} (\boldsymbol{h}-\boldsymbol{y})\right\|_F^2$ (written as $\left\|\boldsymbol{B} (\boldsymbol{h}-\boldsymbol{y})\right\|_2^2$ below) should be\[\begin{aligned}\left\|\boldsymbol{B} (\boldsymbol{h}-\boldsymbol{y})\right\|_2^2&= \left\| \sum_{j=1}^M (h_j-y_j) \boldsymbol{B}_j \right\|_2^2 \\&= \sum_{i=1}^M \sum_{j=1}^M (h_i-y_i) (h_j-y_j) \boldsymbol{B}_i^{\rm T} \boldsymbol{B}_j.\end{aligned}\]Since $\boldsymbol{B}$ is a wide matrix (with much more columns than rows: $M=2n \gg d=2m$), its columns are not (all) orthogonal. Actually, as implied by Theorem 2, $|\boldsymbol{B}_i^{\rm T} \boldsymbol{B}_j|$ can be very close to $\|\boldsymbol{B}_j\|_2^2 = \|\boldsymbol{B}_j\|_2^2 = \frac{m}{n}$ when $n$ is comparable to or greater than $m^2$. Therefore, $\left\|\boldsymbol{B} (\boldsymbol{h}-\boldsymbol{y})\right\|_2^2$ can not be expressed as $\sum_{j=1}^M (h_j-y_j)^2 \|\boldsymbol{B}_j\|_2^2$, i.e., as a sum of $M$ functions, each depending on only $y_j$, and each $y_j$ appearing in only one of these functions, thereby each $y_j$ can be independently solved in closed form.Instead, the variables $y_1, y_2, \dots,y_M$ are coupled in the objective and numerical optimization methods need to be used be solve them.1.2 Similarly, in Section 3.2, the algorithm to solve the "Fix $\boldsymbol{R}$, optimize $\boldsymbol{Y}$" subproblem is also not correctly presented, due to the invalid equality in Eq. (18) which ignores the coupling/interaction between the variables $y_1, y_2, \dots,y_M$.2. Minor comments (notation inconsistencies/abuse, typos, etc.):In Section 3.1.1, "a $n \times n$ discrete Fourier matrix": The article `a' should be `an'."Eq.(4)" etc. is missing a space between "Eq." and equation number.In Section 3.1.1 and Section 3.2, to make it clearer, please change the Frobenius norm operator $\|\cdot\|_F$ to the Euclidean norm operator $\|\cdot\|_2$ whenever it is applied to a vector, e.g., in Eq. (8) on the right-hand side of equality, and in Eqs. (9-11).Throughout the paper, symbols denoting scalars are not in bold typeface, vectors are denoted as boldface lowercase letters and matrices as boldface capitals. For clarity and following the convention, these notation rules could also be applied to the lower-order parts, e.g., the $j$-th elements of vectors $\boldsymbol{h},\boldsymbol{y}$ are denoted as $h_j,y_j$ (rather than $\boldsymbol{h}_j,\boldsymbol{y}_j$), and the $i$-th columns of matrices $\boldsymbol{X},\boldsymbol{D},\boldsymbol{Y}$ are denoted as $\boldsymbol{x}_i,\boldsymbol{d}_i,\boldsymbol{y}_i$ (rather than $\boldsymbol{X}_i,\boldsymbol{D}_i,\boldsymbol{Y}_i$).On page 5, the sentence in the first two lines should read as "We can see that Eq. (12) is minimized when $S$ consists of the indices of the $k$ largest (in absolute value) elements of $\boldsymbol{h}$".In Eq. (18), $\|\boldsymbol{y}_j\|_0$ should be written as $1[y_j \neq 0]$, where $1[\cdot]$ is an indicator function which is 1 if its argument is true and 0 otherwise. The paper considers the sequential recommendation problem. The proposed method essentially combines the following two ideas: (i) two-stage learning: using conventional CF to pretrain user/item embeddings, and feed them (fixed, unlearned) into the 2nd stage learning. (ii) two-time-scale: using 2 RNNs to model active users and inactive users respectively.Although the model design makes sense, it seems too ad-hoc and not novel. For example, the idea (i) actually learn the model with pretrained embeddings, which achieves faster training speed. But this has been widely adopted in various domains. The comparing baselines certainly could easily adapt to use the same pretrained embeddings for faster speed. The claimed advantages are not convincing given important related work/baselines are missing. Specifically I have the following concerns:- Encoding long-range sequence: the proposed method doesn't model long sequences, while seeks to capture user history with a simple MF. This idea: (i) ignores sequential patterns in user previous history; (ii) has been widely adopted in the literature[1][2], especially before we have powerful deep sequential models.- Computational efficiency: (i) the efficiency advantage mostly comes from the use of pretrained embeddings, which can be adopted by almost all the baselines for acceleration. (ii) Another trick used is to cut off the sequence (only train the last 10/20 user actions). Such an acceleration, again, could be adopted by other methods.- Baselines: The paper claims SOTA performance on sequential recommendation, but baselines like SASRec(2018), BERT4Rec(2019) are missing. SASRec performed better than GRU4Rec and also showed much faster training speed. The GRU4REC is almost the 2nd best method among all the baselines used in the paper, and it's from 2015.- table 1 may have some typo. The recall of 2TS on lastfm-1K is worse than GRU4Rec, but the improvement shows 2.2%.The proposed mode design, though, makes sense, but lacks novelty as it's a combination of various commonly used tricks. The improved training speed comes from pretrained emb and sequence cut-off, which are not a novel contribution for efficient sequential/recommendation model training. Several strong baselines are missing, which further weaken the paper.[1]Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation, ICDM'16[2]Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding, WSDM'18 This paper proposes a framework that embeds rhetoric knowledge into rhetoric text generation to bring diversity in the output sentences, take both knowledge tuples and their associated sentences as model inputs to emphasize logic relations. Besides, this paper builds the first Chinese rhetoric graph with an innovative mechanism of relationship storage.Pros:1.Attempting to address logicality and diversity issues in rhetoric text generation, the author proposes an architecture that incorporates logical knowledge information of different rhetoric entities through a knowledge graph and uses a pre-trained language model as the text encoder.2.This paper builds the first Chinese rhetoric (metaphor and personification) graph with 35228 tuples and designed a graph storage method to avoid certain illogical problems.Cons:1. The illustration of the framework is ambiguous and confusing.  1.1 How the context vector is calculated by quarrying the keyword in the knowledge graph?  1.2 Will the keywords also act as a part of the input to Bert?2. AC model cannot serve as the only one baseline method, as the AC model does not use a knowledge graph. Its unfair to compare two methods with different inputs. Experiments should also include comparisons with other knowledge-enhanced generation methods.3. The authors claim the strategy yields more diverse syntactic information but not verify it. The paper lacks detailed comparisons on the diversity between the proposed methods and the template-based methods. (For example, a straight-forward template-based method that uses multiple templates can also prompt diversity). 4. The paper hardly presents any technical novelty. The framework is a simple combination of existing methods (pre-trained language models like Bert and GPT2), and the construction of the knowledge graph is not so novel.5. As this paper proposed a new dataset, authors should make it possible to be followed and reproduced. Its necessary to release the dataset or elaborate all details about the data source and dataset construction.Questions:In the detail of constructing KETG, if the bert_vector is the representation of keyword and sentence, why using [mask] to separate them two instead of using [sep], as [mask] is used to represent the unseen token and [sep] is designed to separate two sentences in the original BERT architecture?   This paper studies the problem of source detection in an epidemics when one observes the underlying graph and a snapshot of the population at a given time i.e. who is infected or not infected. For a SIR (or SEIR) model, the authors propose to use GNN for this task. The learning procedure is then the following: given a fixed graph G, the authors create a dataset of snapshots by running a SIR on G.I am not convinced this problem should be solved with a machine learning approach. In most practical cases, we only have access to one snapshot for a given graph and learning is impossible. The authors here solve this issue by simulating many SIR processes but techniques like the one described by Shah and Zaman without any learning seem much more appropriate.The authors should compare their results to the results obtained by Shah and Zaman. An S(E)IR epidemics propagates on a graph, and the goal is to detect its source (P0) only from the observation of the state (S,E,I,R) of every node of the graph at some time $T > 0$. This version of the source detection problem has been studied first by Shah and Zeman (2011) for SI epidemics, as listed in Section 2. The current paper claims to (i) establish new fundamental limits on this problem, showing in particular that after some time the source detection becomes difficult, and (ii) to demonstrate the ability of graph convolutional networks to solve the problem and validate the results on real data.(i) The theoretical part consists in two short theorems, but their proofs are problematic. - Theorem 1 builds on an approximate analysis in Newman (2018), which requires a number of assumptions and approximations (for example, a mixing assumption that allows to replace expectations by ensemble averages in ordinary differential equations describing the evolution of $S$, $I$ and $R$; the limitation of time $t \rightarrow 0$). These assumptions and approximation might not be valid on all graphs (e.g., a line graph has the largest eigenvalue around 2, but it is difficult to imagine that $O(1)$ nodes get infected in $\Theta(log(N))$ time). The proof cannot rely on implicit assumptions and approximations whose error is not rigorously estimated. Maybe the theorem is true for some class of graphs including ErdQs-Rényi graphs, but even in that case a more involved proof is needed, since the current techniques only work for early times $t$ and not for large times as in (5). Some computations could be clarified, for instance if $ \langle \psi^{(1)} \cdot I(0) \rangle $ denotes the average (over the uniform prior of patient zero over the $N$ nodes) of the scalar product between $\psi^{(1)}$ and the one-hot vector $I(0)$, why is it equal to $\lVert \psi^{(1)} \rVert_1 $ instead of$ \langle \psi^{(1)} \cdot I(0) \rangle = \frac{1}{N} \sum_{i=1}^{N} \left(  \psi^{(1)} \cdot I(0) \right)_i = \frac{1}{N} \lVert \psi^{(1)} \rVert_1$ ?- The proof of Theorem 2 is flawed. It starts with the statement that "if P0 is in a triangle, we may miss it $2/3$ of the times." Why would all nodes of a triangle be equally likely to be the source? With the same rationale, why could not we argue that if P0 is in an edge, we may miss it $1/2$ of the times, which would give an upper bound of $1/2$ (as long as $|G_I|>2$) and would contradict the simulations results? The next statement that `"in $G_I$ all nodes have degree $k \approx p|G_I|$" appears incorrect: suppose that the graph is an E-R graph $G(N,p)$, that $R_0$ is very large and that the infection spreads in a snowball way. Then most of the early infected nodes in $G_I$ have node degree $pN$, and not $p|G_I|$. - Now, the fact that P0's detection becomes harder over time is an important message, especially these days, but it is not a surprising result that when the infected set is a constant fraction of the population, then it is hard to detect P0. This difficulty was already reported in the initial paper by Shah and Zaman (2011).- In Section 3, the authors contend that compared to the SI model, the removed state introduces additional uncertainty about the temporal order of infections. Why? Since the state of each node is known, having 3 classes (S,I,R) instead of two (S,I) gives more information, which should ease the task of detecting P0. (ii) The comparison of the GNNs used by the authors over state-of-the-art message passing algorithms is made only with the DMP method of Lokhov at al (2014), but not with the (in general more accurate) belief propagation method of Altarelli et al (2014), also cited in Section 2. There is no comparison with the rumor centrality method developed by Shah and Zaman (2011) either, in terms of accuracy and speed. Also, it would be interesting to see the comparison at times other than $T=30$. - In terms of speed, training and inference should clearly be separated. It may be misleading to report in the abstract that GNNs are 100 times faster than state of the art methods: that applies only for inference. It would be more accurate to report that GNNs are 100 times faster for inference and twice faster for training compared to the DMP method (as well described in Section 5.1).  - It may be unrealistic to assume that all 4 states (S,E,I,R) can be detected for each individual; the exposed state in particular might be very hard to detect. Otherwise the simulations on real data seem well-done. It is unfortunate that with the non-interpretable theoretical results, the simulation results do not give much insight either. For example, it would be interesting to compare the accuracy results to the size of the infected set on the simulations. Maybe it would be more meaningful to normalize the rank by $|G_I|$ instead of $N$. - In Figure 2, how are the theoretic curves computed? Equation (7) in Theorem 2 depends on $|G_I|$, which is not directly linked to $T$ nor to the epidemic parameters. Is $|G_I|$ computed based on the simulation results? If so, why are then the confidence intervals not given for these curves? - In Figures 4 and 5, how is the size of the set $G_I$ evolving over time? - The paper should be proofread, it contains quite a few typos or vague statements, for instance: Theorem 3 in the appendix is actually Theorem 1 in the main paper; Figure 2 caption does not read well (no verb in the sentence : While accuracy drops below...);  Figure 4 caption: cycles significantly reduces accuracy of P0 -> cycles significantly reduce the accuracy of the detection of P0; Bottom of p4: where each edge has independent an probability $p$ -> where each edge has an independent probability $p$; etc **Summary**The paper addresses the problem of learning with noisy labels by transforming the original category classification task into a semantic-similarity prediction task. The new task takes pairs of samples as input and predicts if the two samples are coming from the same category or not. It is theoretically shown that the similarity pairs have lower noise rates compared to the original categorization. It is then argued that this lower noise rate makes the learning more robust in the presence of class label noise. In practice, the idea is applied on top of a standard categorization network (equipped with other approaches to handle class label noise). Specifically, for each pair, a similarity score $\in[0,1]$ is obtained from the category outputs of the two samples as the dot product of their softmax distributions. Then, a similarity transition matrix (that is pretrained) is applied to the prediction. Finally, a two-class cross-entropy loss is optimized. The similarity transition matrix is obtained by using prior techniques for finding a $C\times C$ class transition matrix (with $C$ being the number of classes) and then analytically turning it into a $2\times2$ similarity transition matrix.**Quality**The writing is of a noticeably-low quality and seems to have been overly-rushed. The experiments are done on several datasets of different modalities. The theories are partially informative but not entirely and/or directly relevant.**Clarity**The method section is not clearly written. Section 3.1 and 3.2 were quite hard to follow due to some notational inconsistencies, lacking definitions, and not being self-contained (relies on the knowledge of Hsu et al. 2018). The relevance and implications of the theories are not properly discussed. **Originality**The idea of turning category classification to similarity prediction in order to make the learning robust to class label noise seems quite original to the reviewers knowledge.**Significance**The results show improvements on various benchmarks as well as various underlying category classification methods for learning under noisy labels. The improvements are not always substantial but they seem to be statistically significant and consistently present for large synthetic symmetric noise levels of CIFAR100. The idea being general and the results somewhat significant indicate a potential significance of the method.**Major technical comments***Theory*1. when learning with similarity labels, is it important to consider the total noise levels among similar and dissimilar pairs or also the worst case of noise level in similar pairs and dissimilar pairs separately? To make this more clear, imagine the extreme case that all of the given labels are noisy. Even for this case, when constructing the similarity labels, while all similar labels can be wrong, still the majority of dissimilar labels (and vast majority if the number of classes are high) will be correct. This renders the similarity noise rate to be (arbitrarily) low (depending on the number of classes and samples per classes). Does this low noise rate mean the task is learnable although there is literally zero information on true class labels? I doubt it. That would essentially mean we can take any set of images and apply random similar/dissimilar pairs to them and consider it a low-noise-rate dataset. Thus, formal analysis and/or informal theoretical discussions are required in this regard.2. from algorithm 1 it seems the method requires two independent trainings, shouldnt that at least double the computational complexity? Learning with pairs could potentially take longer to converge due to the quadratic increase in the number of input data. How is the claim in section 3.3 that Class2Simi increases the computation cost *slightly* supported?3. regarding theorem 3, based on the first point above, I have a concern that the relevant risk to be studied here should be the original category classification risk as opposed to the similarity risk. The latter could be dominated by the dissimilar pairs and unless formally analyzed or at least directly discussed its hard to draw any conclusion on how the bound on the similarity prediction empirical risk translates to a bound on the original classification risk which is the objective of interest.*Experiments*1. Regarding the experiments on Clothing1M, while I understand the raised points regarding the dominant class confusion, I believe the method should still be compared on the original dataset along with the proposed Clothing1M*. In fact, this can be a weakness of the proposed method that should be studied further and more thoroughly with designated experiments. Such a noise is possible in the real-world applications due to semantic ambiguity or human error (as demonstrated in Clothing1M).2. Is the same set of hyperparameters used for all the baselines as well as the variants of the proposed approach? How are the hyperparameters optimizations done? In particular, which variant of the method or baselines are the hyperparameters optimized on? In our experience, when it comes to noisy labels, it is quite common that different methods perform better with different sets of hyperparameters, so its important to optimize the parameters per method.3. the improvements for asymmetric noise, and on MNIST and CIFAR seem marginal. A statistical paired significance test could be useful.**Minor technical comments**- the notation of the summands subindices $i,j,i,j$, in theorem 1 is a bit confusing. For instance, when $i=i$ should only $i$ be used?- the similarity label is denoted as $H_{ij}$ in section 3.1but as $S_{i,j}$ in figure 2 and section 3.2.- what is the difference between samples denoted by $X_i$ in section 3.2 and $x_i$ in section 3.1?- what does $\theta$ parametrize? The network or the similarity transition matrix? What is the difference between $f(X_i)$ and $f(X_i;\theta)$ in figure 2?- the algorithm defines a function $g(.)$ which is not referred to in the methods section- where are parameter matrices $W_1,...,W_d$ defined? What is d?- how is the expected risk $R$ defined? - what is $\hat{f}$ - what is $R_n$?**Overall**While the reviewer can guess the meaning of some of the notations based on the literature it renders the paper hardly readable and at times the discrepancies were unresolvable to the reviewer. Furthermore, there are concerns regarding the motivation, relevance of the theories, and the significance of the results. So, overall, while I believe the paper has original contributions of potentially high impact I strongly believe it needs a major revision before its presentable at a conference.   Findings:- Reproduces various existing findings about anisotropy of contextual representations viewed globally.- Contextual representations are highly isotropic within clusters of the representations. - GPT representations follow a Swiss Roll manifold, where the most frequent words appear at the head and less frequent words are gradually appended at the bottom.- The Swiss Roll manifold is taller in deeper layers.- BERT representations fall in a Euclidean space.-  The Local Intrinsic Dimension of contextual embeddings is lower than for unigram embeddings.Pros:- The manifold analysis of word frequency is intriguing and intuitive.- The explanation of experiments was clear in each section.- They produce compelling evidence that the global token-level anisotropy of these representations is largely due to membership of large clusters. This is a valuable contribution because it explains previous findings in Ethayarajh 2019 and reconciles them with theoretical expectations.Cons:- In Section 2.3 and Section 3.1, the paper gives insufficient credit to Ethayarajh 2019. As far as I could tell, every initial result is a reproduction of a result from Ethayarajh 2019, and the methods are very similar.- Though they acknowledge it, the methodology is largely taken from existing unigram embedding analysis.- Once they start to identify very well defined clusters, I was very curious about the distinctions between the islands. It would not be difficult to inspect some of the data by hand, so I don't understand why the authors didn't try.- The authors offer no analysis for the difference in behavior between different models. I felt like I was reading a taxonomy, and the plots were left for the reader to connect. The authors have presumably spent quite a while thinking about these geometric structures and models, so surely they have conjectures about the behavior they observed or hypotheses they can test.Minor / style:- I didn't realize until the conclusion that your main finding was an explanation of existing claims about anisotropy by considering behavior within the clusters, so that needs more emphasis.- The papers you cite do a decent job of explaining why isotropy in the representation space is significant both token- and type-wise. The paper would be a lot more readable if you made a similar effort in explaining background.- There is inconsistent use of "isotropy" vs "isotropicity".- Citations needed for claim "widely believed that the contextual space is so weird"- Needs proofreading for minor grammar and typos (e.g., downstreaming instead of downstream, could resides instead of could reside)- Instead of referring variously to high similarity or cosine, silhouette scores, and other measurements of isotropy, it might be clearer to link each concept to isotropy once, and then each subsequent result simply refer to it as isotropy while mentioning the metric. Then the reader doesn't have to constantly remember which metric indicates high isotropy as they read the results.Questions:- The authors claim to select the clusters that maximize MMS. I read the wording to imply that this optimum is tractable/stable. Is that the case? This paper propose a counterfactual approach to improve the performance in information extraction tasks and in particular on the rarer classes. While this is an important research problem, I think the paper has the following issues:- The overall  approach is best described as a form of a data augmentation based on the model predictions. Steps 1 to 3 is essentially finding out which part of the input had the most impact on the model prediction by comparing the impact of a token being masked at random, and then steps 4 and 5 use this info to improve the prediction. This is quite commonly done in the context of model interpretation, e.g. finding out which words matter the most in a prediction was done in the work on LIME: http://sameersingh.org/files/papers/lime-kdd16.pdf , and by many others that followed this up, e.g. finding out which words are the most important via removal: https://arxiv.org/abs/1804.07781. The latter is actually doing what the first three steps do of the proposed approach. Steps 4 and 5 essentially use this output which presumably captures the contextual information and combine it with the contextual word representation, which is a pretty standard thing to do. However this existing work in NLP has been ignored.- The causal framing in Figure 1 appears flawed. There is no good reason to assume that the NER tag "causes" the trigger representation and not the other way around. However this is a fundamental assumption here. Also, why is not the contextual word representation "causing" the NER tag? Furthermore, there is no causality inferred here; the model output is post-processed, and the causality is not assessed in any way.- Few-shot learning has been investigated in the context of information extraction. Here are some papers: https://www.aclweb.org/anthology/D19-1649/ https://www.aclweb.org/anthology/P19-1589/ . Only token-level tasks are explored, but often in IE we want to infer relations between tokens, which this work doesn't apply to- The paper is hard to follow. What is v_r in equation 3? In section 2.2, it should be explicit that the tokens are not replaced, but removed. Note that replacement with semantically equivalent words has been explored: http://sameersingh.org/files/papers/sears-acl18.pdf- There are some vague statements about the novelty vs previous work by Tang et al, but no explicit statement made in the model description. Eventually it seems to be about considering the syntax (page 7), but this is only used to just select which words to remove as far as I can tell and the GNN over dependency trees which is based on previous work.- While it is stated that the results of previous work are reproduced, then it is stated that they were not really. While this is not necessarily the authors' fault (reproducibility can be difficult sometimes), combined with the uncommon choice of metrics and the lack of any comparison to previously reported results in the literature, means that it is impossible to understand where the presented method stands.  The paper proposes a Hidden Markov Recurrent Neural Network (HMRNN) that mimics the behavior of traditional hidden Markov models (HMM) and shows that the proposed network can be trained to obtain a similar solution as HMM.Some questions: 1. The topic is not significant. The paper proposes an RNN to mimic the behavior of traditional HMM. Although the paper proves that the model can be optimized to obtain similar parameters as HMM, it doesn't include any comparison or experiment for the usefulness of the model.2. The paper lacks the necessary discussions. In particular, the paper claims that the proposed model allows for substantial modularity with other predictive networks. Even though the paper has added an additional network on the Alzheimer dataset, the paper does not show any ablation experiment on the contribution of each component and eventually the effectiveness of the proposed work.3. The paper lacks baseline models for the disease progression modeling. There are several disease progression models based on HMM (e.g., [1] and [2]). The paper doesn't compare to such baseline models to demonstrate the effectiveness of the proposed work.[1] Yu-Ying Liu, Shuang Li, Fuxin Li, Le Song, James M Rehg. Efficientlearningofcontinuous- time hidden markov models for disease progression. In Advances in neural information processing systems, pages 36003608, 2015. [2] Ahmed M Alaa, Scott Hu, and Mihaela van der Schaar. Learning from clinical judgments: Semi-markov-modulated marked hawkes processes for risk prognosis. International Conference on Machine Learning, 2017. 4. The experiment on Alzheimer's disease dataset is not well described (feature dimension, covariances, etc) and no code or pseudo algorithm is provided, making the experimental results and findings hard to be reproduced.5. The methodology is not well explained. In the traditional HMM, the states in the next timestamp are computed based solely on the state on the current timestamp and are independent of the observation. However, based on equation(4)  (6), the proposed RNN is computing the states based on the current observation. In the submitted manuscript, "Empirical Frequentist coverage of deep learning uncertainty quantification procedures", the authors propose to investigate the Frequentist coverage properties of predictive intervals by numerical experiment for a number of machine learning models applied to benchmark datasets.  I can't say that I find this a strong submission because:1. the authors give a confused (mis-)definition of coverage; essentially they seem to have taken Barber et al.'s definition of "marginal distribution free prediction intervals", mangled it and then called it Frequentist coverage citing Wasserman2. the authors claim one of the contributions of this manuscript to be "introduce coverage and width as a natural and interpretable metrics for evaluation predictive uncertainty" but in fact these aspects of predictive intervals from ML models has been studied for many years, as a simple google search will confirm3. the results shown will not generalise in any meaningful sense: for example, GPs are found to have excellent coverage over the set of regression tasks shown, but in fact GPs are themselves a case study in the difficulties of achieving Frequentist style coverage in the domain of Bayesian non-parametrics (e.g. Hadji & Szabo 2019; Neiswanger & Ramdas 2020; Rousseau 2016 ; prior over-smoothing being the root of many problems ). Many thanks to the authors for their submission which aims at combining data-driven approaches with model-based ones for optimising centralised cooling systems (chiller plants). The overarching aim of this paper has been well studied before; given one aims at incorporating physical constraints, one needs a solid understanding of the underlying physical system, hence how one adds such a constraint to the model requires deep investigations. The machine learning part of this paper is also not something new. To match the physical constraints of the problem in hand authors incorporate monotonicity to the Neural Network model. Other than that, the paper does not offer a comprehensive explanation on what the novelty and actual contributions are, nor does it provide a solid experimental set-up. Please consider some of my comments below:Major comments:--No conclusion or discussion has been provided. That makes the paper incomplete--Experimental section is rather inadequate and rushed up; the results are not explained properly; no concrete information on the dataset used has been provided to contextualise the experimental design.--Paper is heavily skewed towards providing too much background information and rather simplistic information on incorporating physical constraints on to the loss function of a neural-net.Minor comments: Quite a few punctuation errors exist. Some sentences are not finished or finish with a full stop where it shouldn't. Please consider proof-reading the manuscript carefully for correcting such typos. Summary:This paper tackles the energy optimization problem of chiller plants, which are the main equipment of a cooling system. This paper aims to learn the non-linear mapping between the control parameters (of a cooling system) and energy consumption. The key idea of this paper is to incorporate domain knowledge into the training of neural networks. Specifically, monotonic constraints are proposed to constrain the learning of the non-linear mapping so that physical laws are satisfied. The authors propose two methods to implement monotonicity: hard-MNN and soft-MNN.Hard-MNN treats monotonicity as hard constraints and uses a Monotonic Neural Network to learn the non-linear mapping. Soft-MNN treats monotonicity as soft constraints and adds a monotonic loss into the training objective to encourage the learned mapping to be monotonic.  Experiments show that (1) both the hard-MNN and soft-MM outperforms multilayer perceptron (MLP), and (2) hard-MNN yields lower energy consumption than MLP.Pros:+ The idea of incorporating domain knowledge into training neural nets is reasonable.  (But this is not a new idea.   E.g.,  KG-GAN: Knowledge-Guided Generative Adversarial Networks.)+ Appendix A.1 does a great job of giving an introduction about cooling systems and chiller plants.Cons:- Some claims are unjustified.Section 2 states that we make a theoretical analysis and & However, I cannot find such analysis. Does the theoretical analysis mean Equation 3, 4, and Table 1?Section 2 states that Compared with the above state of art method, MNN reduces the dependence on the amount of data, provides a more accurate function space, facilitates subsequent optimization steps and improves optimization performance. However, the current experimental results are not strong enough to support these four claims.Specifically, no experiments empirically compare performance in terms of different amounts of training data. There is no attempt to quantify the so-called a more accurate function space. No results show information about the optimization applied to the learned mapping function.- The technical method is not clearly described.Appendix A.3 gives an overview of how the energy optimization problem is decoupled into two sub-problems: identification and optimization.I appreciate the overview but also think there is room for improvement.This part could be important since it gives a concrete high-level view of the problem formulation. In particular, this paper concentrates on the first sub-problem. However, it is not evident to see this point neither from the introduction nor from the beginning of the method section, making it hard for me to understand the problem formulation clearly in my first-pass reading.In Section 4, three type models are introduced. However, their names and notations are not linked together, which makes the reading very difficult. It would be better if the notations are shown together with their corresponding names like cooling/chilled water pump power model (P_{COWP} and P_{CHWP}), cooling tower power model(P_{CT}), and chiller power model (P_{CH}).The connections between math equations are loose, which makes the technical method hard to understand. For example,(1) the connection between y in Equation 2 and y_{CH} in Equation 4 is not clear.(2) The connection between y_{CH} in Equation 4 and x_{CH} in Equation 5 is also not clear enough.(3) (c,f_{chiller}(x)) mentioned in Section 4.3 is somewhat isolated, its relationship with the other equations are not evident.The last five lines of Section 4.3 (below Figure 4.4) are hard to understand. Overall, the technical writing of Section 4 could be further improved.Figure 4.1 and 4.4 lack a detailed explanation and thus are hard to understand. What do f_fan, cow_pump, P3, P1+P2+P3 in the figure mean, respectively? What is the difference between (a) and (b)? Why (a) is called identification hyperplane, while (b) is called optimization hyperplane?Most of the math notations are not explicitly explained in the paragraph. I had difficulty reading the mathematical equations until I found the table of notations in the Appendix (Table 2). However, Table 2 is not referenced throughout the paper.- Insufficient experiments in terms of setting, details, and comparisons.The Experiments section lacks a detailed description of dataset information, training settings, implementation details. The current experiments look impossible to reproduce the results. What is the general introduction of the dataset used in the experiments? Do you split the data into training, validation, and testing set? What is the number of data examples of each subset? How to train the proposed model? What is the detailed network architecture of the MLP and the proposed MNN?In the first experiment (Figure 5.1), what is MAPE, and how to calculate it? What is DC mentioned in Figure 5.1?The second experiment (Figure 5.2) is unclear in its purpose, comparison method, metric, and effectiveness.It is hard to judge the effectiveness of the proposed method based on the experiments.Why do we need to consider experimenting with different wet-bulb temperatures?What is the key difference in terms of the experiment setting compared with the previous experiment? The 1st experiment focuses on internal control, while the 2nd experiment focuses on external control?Can the authors elaborate more on these two experiments (Figure 5.1 and 5.2)?What is PUE, and how to calculate it?What do the dots and lines in Figure 5.2 mean, respectively?What is MLP with local PID? PID is not explained throughout the paper.Is improving the average PUE by 1.5% significant?Why no results of soft-MNN in this experiment?As the related work mentioned, linear regression is the mostly used modeling method in real-world &, a baseline comparison with linear regression looks essential to let readers know the baseline performance. However, the current manuscript compares with MLP only. Is there any reason that prevents from comparing with linear regression?This paper proposes a new activation function (Equation 11) and two extensions of the proposed monotonic neural networks (MNN): partial-MNN and recurrent-MNN. However, no results are shown to demonstrate their effectiveness. This paper focuses on the problem of generating sparse l2-adversarial examples in a white-box and surrogate/transfer setting. The authors consider local attacks  perturbing on a limited number of pixels while achieving high attack success rate. The main contribution of this work is to define the region to perturb using grad-cam based saliency maps to identify regions that have a greater impact on the classification decision. Having identified this region, the author use SGD to find the adversarial perturbations. The experimental results show that a high attack success rate can be achieved with this method. Pros:1)The idea is simple yet affective. Attack success rate seems to be high. 2)Paper is well written and easy to read.3)The experimental setup is reasonable.Cons:1) My main concern with this paper is that it appears incremental to me. The idea builds upon Grad-cam and existing adversarial example generation approaches.  2)Also, the idea is not new implying that there exist several related approaches achieving sparse adversarial examples (see [1,2]). 3)I would encourage authors to perform a more detailed literature survey and compare their results with them in a fair manner. The metric should not just be the attack success rate but also the number of perturbed pixels.  4)The robustness analysis of adversarial examples to image transformations or adversarial example detection methods can be interesting.   [1] Kaidi Xu et al., STRUCTURED ADVERSARIAL ATTACK: TOWARDS GENERAL IMPLEMENTATION AND BETTER INTERPRETABILITY, ICLR 2019[2] Hai Shu et. al., Adversarial Image Generation and Training for Deep Neural Networks In this paper, the authors proposed a new adversarial attack method based on feature contributive regions. The authors firstly utilize an off-the-shelf method to extract FCR of the image and then utilize the FCR as constraint for the perturbation. I tend to reject this paper due to following reasons:1) The presentation of this paper needs significant improvement, the current manuscript is hard to read. Furthermore, the equations and many mathimatical presentations are unprofessional. 2) Although I am not familiar with the adversarial attack literature,  I think the method proposed in this paper is quite straightforward and the contribution of this paper is not significant enough for ICLR.3) The FCR is estimated for the original input, while, introducing perturbations might change the FCR (but I guess the FCR won't change significantly). Instead of adopting a two-stage approach which utilize fixed FCR, have you thought about generating perturbation and update FCR jointly or alternatively?4) I think the authors should analysis the effect of FCR threshold. This paper proposed a simple stochastic normalized gradient methods with momentum (SNGM) for non-convex optimization, the authors argued the main advantage of the method is it allows large-batch training. Theoretical convergence guarantees and  empirical results for neural network training are provided.My main concerns are in the theoretical results: in the paper the authors argues the following: standard MSGD has convergence of O(/1/sqrt(C) + B^2/C), while SNGM, with B=sqrt(C), has convergence rate of O(1/C^(1/4)).  It seems to me it is unfair comparison as one has O(1/C^(1/2)) rate and the other has O(1/C^(1/4)) rate.If we choose B=C^(1/4) in MSGD, then MSGD has convergence rate of O(1/C^(1/2)), which is much faster than the SNGM rate of O(1/C^(1/4)) ? Only if we fix the overall rate of O(1/C^(1/4)), then for MSGD the largest mini-batch size is B=C^(3/8), which is smaller than the SNGM minibatch size. Can SNGM obtain O(1/C^(1/2)) rate ? What is is mini--batch size limit under the fast O(1/C^(1/2)) rate ?Please correct me if I am wrong. This work proposes a new method by combining GCN and GAT to perform node semi-supervised classification task. The new model uses node features to build another graph, uses GCN and GAT on the original graph and the new graph, and also adds a loss term to reduce the similarity of the final node representations.  1. The idea is very heuristic without much insight. The paper keeps arguing traditional GNNs only use one-side information, but they are actually leverage both node features and graph structures by propagating nodes features over the graph structure. So the statement is not correct. The paper claims that different node attributes contribute in different ways that should be sufficiently leveraged, but this is a very confusing argument if not paired with empirical justification. In the proposed model, it seems the authors try to resolve the above confusing issue via using another graph structure built only based on only node attributes. There is very unclear connection showing why this method resolves the problem they proposed. 2. The experiment parts are not in a fair comparison too, as the paper does not use the standard way to perform dataset splitting. For this new splitting way, no hyperparameters are report for both the model here and previous models. Some benchmark datasets for semi-supervise learning are also not used, e.g., cora, pubmed,... 3. There are quite a few errors in grammar. I suggest authors to perform some grammar checking.  # SummaryThis work claims SGD regularizes the model through penalizing the trace of Fisher in the early phase. This claim is supported by the similar generalization behavior of SGD (with optimal learning rate) and Fisher penalization (for SGD with small learning rate). A series experiments are conducted to verify the understanding.I find the paper writing is rather misleading. On the one hand, no mathematical justifications, and the logical chain is weak --- hard to claim which factor is the cause and which one is the effect. On the other hand, though written as "trace of Fisher" in the paper, in experiments they compute a different regularizer --- norm of expected gradient. Detailed questions come in the following.# Questions1. Abstract, "We highlight that in the absence of implicit or explicit regularization...". How do you get rid of the implicit regularization???2. Eq.(2) is super misleading. Trace of Fisher is the expected gradient norm, but in Eq. (2) you compute norm of expected gradient. Statistically the latter has nothing to do with the former: one is the second moment, and the other is the squared first moment. Please justify.3. I have a very simple explanation to your observed phenomenon. In the beginning, gradient is large, thus your version of "trace of fisher" is large, as the gradient decreases, the "trace of fisher" decreases. Now we look at large learning rate and small learning rate. With large learning rate, SGD converges faster, thus its gradients decrease faster, which causes the "trace of fisher" decreases faster. But with small learning rate, SGD converges slower, thus its gradient decrease slower, and the "trace of fisher" appears to be large in the beginning epochs.Therefore I am not at all convinced by your arguments. 4. From the above discussion, at least you need to normalize the "trace of fisher" by gradient norm --- which then becomes a measurement of gradient confusion. See following for references.5. FP/GP. If I understand correctly, you need to compute gradient of gradient, which expands your computation graph at least twice. Can you report the GPU memory consumption? 6. Finally, let us take about the practical role of FP. According to Table 1, the improvement of FP over GP is marginal. I cannot see a potential of FP. Not only in theory, but also in practice this paper is not satisfactory. # Missing RefsTons of theory paper should be discussed.  A few of them come to my brain are listed in below. Please do a more complete literature investigation.Fisher- Liang, Tengyuan, et al. "Fisher-rao metric, geometry, and complexity of neural networks." The 22nd International Conference on Artificial Intelligence and Statistics. 2019.- Karakida, Ryo, Shotaro Akaho, and Shun-ichi Amari. "Universal statistics of Fisher information in deep neural networks: Mean field approach." The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.SGD regularization mechanism- Daneshmand, H., Kohler, J., Lucchi, A., and Hofmann, T. Escaping saddles with stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.- Zhu, Zhanxing, et al. "The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects." arXiv preprint arXiv:1803.00195 (2018).- Wu, Jingfeng, et al. "On the Noisy Gradient Descent that Generalizes as SGD." arXiv preprint arXiv:1906.07405 (2019).Gradient confusion is highly related to the trace Fisher. See this one and its follow-ups.- Sankararaman, Karthik A., et al. "The impact of neural network overparameterization on gradient confusion and stochastic gradient descent." arXiv preprint arXiv:1904.06963 (2019).Adversarial regularization is also related to GP/FP:- Miyato, Takeru, et al. "Virtual adversarial training: a regularization method for supervised and semi-supervised learning." IEEE transactions on pattern analysis and machine intelligence 41.8 (2018): 1979-1993.   This paper proposes a method for producing representations for clustering that take into account global trends in the dataset, rather than considering each pair of instances in isolation. They claim to achieve competitive clustering performance on omniglot, which they attribute to the use of these contextualised embeddings. They also present a theoretical justification for using transformers in metric learning## Strengths* The need to make use of context is well motivated, and the authors show that their method does make use of context effectively.* Both the algorithmic and theoretical work described by the paper appear to be technically correct.## Weaknesses* Important details pertaining to both the proposed architecture and experimental evaluation are missing (see questions below).* The novelty seems limited, given that Lee et al. (2019a, 2019b) have already proposed using a set transformer to compute contextualised embeddings for clustering.* While technically correct, it is unclear to me how the theoretical analysis directly relates to the SAB components used in the architecture. In particular, assuming all within cluster weights are equal to $\alpha$ or $\beta$, while all between cluster weights are equal to $\gamma$ does not seem like a realistic model of an SAB module.* The experimental evaluation could be improved. Currently only one real-world dataset is used in the evaluation, which makes it hard to say whether the findings will generalise to other situations.* The way the experiments on omniglot are set up does not match the motivation for the method. In particular, one would expect context to be useful when elements from different clusters can sometimes look very similar (e.g., letters from different alphabets that resemble each other). However, the omniglot experiments consider each alphabet in isolation, so this impies that context will be useful when individual letters in the same alphabet can sometimes look similar.## Questions* How does $\mathcal{T}$ map from $\mathbb{R}^{n \times d_x}$ to $\mathbb{R}^{n \times d_z}$ when the input and output of each SAB is the same dimensionality? Does this mean $d_x = d_z$?* In the circles experiment, are multiple datasets (i.e., "instances") synthesised in order to train the embedding block?* It is said that the "pairwise" baseline, the embedding block is removed, but also that it is a metric learning method. What metric learning is going on here?* When would one expect additive attention to outperform multiplicative attention, and vice versa?## Other comments* Siamese networks were proposed by Bromley et al. (1994), not by Koch et al. (2015).Jane Bromley, et al. Signature verification using a "siamese" time delay neural network. NeurIPS, 1994. ##########################################################################Summary:The paper aims at achieving learning-based graph representations of MDPs with the goal of improving results in sparse-reward RL problems via better exploration. Experimens are mostly done on synthetic environments proposed by the paper itself (besides MountainCar environment). Baselines are SPTM, SoRB and HER.##########################################################################Reasons for score:Experimental evidence in the paper is insufficient to estimate the value of the approach.##########################################################################Pros:1. Interesting ideas about planning for exploration and learnable graph representations for MDPs.2. Visually the paper looks nice.##########################################################################Cons:1. Test environments/tasks don't look persuasive. Why not pick some more established setups from prior work (e.g., SPTM or SoRB or HER papers) and show advantage on their ground? Creating new setups is only justified if the existing ones are not sufficient with respect to the goals of the paper.  2. The main algorithm idea is not clear. There are many components and the description does not comprehensively show how they fit together and what motivated those design choices. 3. Some statements in the paper would profit from softening/more accurate formulation. To give a few examples:- "SPTM rely on human sampled trajectories to generate the graph, which is infeasible in RL exploration" - collecting human demonstrations/trajectories is indeed costly but it's not infeasible. In fact, it is a common recipe for kick-starting RL which powered the first versions of AlphaGo and AlphaStar.- "Another drawback is that existing methods cannot be used for facilitating exploration which is important in RL." - it is worth keeping in mind a paper "Episodic Curiosity through Reachability" https://arxiv.org/pdf/1810.02274.pdf. While it doesn't plan for exploration, it still keeps some kind of topological map in memory, although without explicitly storing edges.- "In cognitive science society, researchers summarize these discoveries in cognitive map theory (Tolman, 1948), which states that animals can extract and code the structure of environment in a compact and abstract map representation." - that paper came many years earlier than those previously mentioned, so it can't possibly summarize them. In fact, O'Keefe was sceptical about cognitive maps from what I read in those papers. ##########################################################################Questions during rebuttal period: Unfortunately, to make paper meet high ICLR standards would require too large a change in my opinion. I would encourage the authors to fix the cons mentioned above and resubmit.#########################################################################Minor suggestions and typos: (1) "which mark three disjoint regions [0; 1]; (1; 3]; (3;+1)," - how did the magical constant 3 come to be? The authors examine the commonly used paradigm of not discounting in the policy gradient objective. They propose two hypotheses relating to discounting. (1) discounting the critic improves representation learning. (2) undiscounted policy gradient is similar to discounting + an auxiliary loss. These hypotheses are studied through a series of empirical tests in the MuJoCo domain with PPO. Strengths:- I believe this paper is asking the right type of questions about common setups. There are a lot of choices made in deep RL algorithms which don't align with theory and are otherwise unstudied and empirical studies are an important. - Some of the approaches used to answer these questions are quite unique.- Overall, there a lot of experiments both in the paper and the appendix, which is detailed. This is a paper which will benefit from the additional page of content as a lot of key figures can be shifted to the main body.Weaknesses:Given the empirical nature of this study, it is really important to have robust experimentation to really answer the hypotheses the paper raises. I think the paper falls short at this aspect and I wasn't convinced by the arguments made for either hypothesis. Furthermore, the conclusions that could be drawn from the results are generally not that surprising. - I'm not sure PPO is the best algorithm to analyze many of these questions. For example, Engstrom et al., 2019 showed a lot of very minor implementation level details had a large impact on the performance. Consequently, it may be difficult to disentangle the actual causative factors in performance. This is problematic as many of the claims in the paper are supported by empirical tests where the performance is not strikingly different. For example, Figure 1 is meant to justify that for $\gamma_c = 0.99$ additional transitions improved performance, but on several environments increasing $N$ to 2 or 4 seems to hurt performance, going against our intuition about variance reduction. Figure 2 shows that for $N \neq 0$ there is a large performance drop, but all values of $N \neq 0$ achieve a very similar performance rather than trending downwards as $N$ increases. To me this suggests a very brittle algorithm. - For section 3 the bias-variance trade-off is evident from prior work (as referenced by the authors) so the result is of course not novel. I think analyzing it in a deep RL setting is important but because of the problems mentioned prior, I didn't find that these results provide anything solid to add to our understanding. - The results for Figure 3 aren't convincing (1) because they are overfit, by selecting the best possible H for each it seems likely to always arrive at a high performing agent. (2) This more suggests that these environments don't require the full horizon to achieve a high performance. Consider a simple cartpole problem which is optimal using greedy actions but has a horizon of 1 million time steps. Since were in an approximate setting with deep networks, it isn't surprising that the agent can achieve a high performance without considering the full horizon. - The results from the toy MRP experiment and distributional RL do suggest some kind of connection to representation learning, but isn't considering a longer horizon simply a more difficult learning problem? Is the representation necessarily an important aspect here? I didn't find that the authors answered this question. - The conclusion from Section 4 is that $\gamma_A=1$ is an inductive bias that all transitions are equally important seems entirely self-evident from the mathematical definition given it applies equal weight to all transitions. At the same time the main question of hypothesis 2 seems unanswered. Shouldn't AuxPPO $\approx$ PPO, rather than DisPPO if this was true? - A single environment for Figure 9 is not enough to draw any meaningful conclusions. I did not find the discussion in B.1. convincing that the other environments were not suitable. Simply change $t_0$ for the other environments. From personal experience the horizon of Ant is generally large (near 1000) as the terminal condition is hard to achieve meaning the difference between Ant and the fixed length environments should be small.Additional Comments:1. I do wonder if this paper is better off as two separate documents where each hypothesis is provided much more significant attention/experimentation. For example, hypothesis 1 isn't actor-critic specific and is also applicable to Q-learning based methods. These experiments could be simplified by looking at algorithms with significantly fewer components and more settings. 2. For the PPO-TD-Ex experiment I think it's also worth considering extrapolation error (Fujimoto et al., 2019) in TD learning. Since $S^i_{t+1}$ is sampled from a single transition rather than a full trajectory it is not necessarily contained in the batch. As a result, $\hat v$ is not trained on $S^i_{t+1}$ and produces an erroneous TD target. My first impression was that the performance drop for $\gamma_c=1$ was not surprising but the performance gain from $N=1$ for $\gamma_c=0.99$ was, and I think are are unanswered questions here. Another important reference is Bengio et al., 2020 which showed TD(0) generalizes worse than TD($\lambda$) and there is clearly a related result here. 3. Given MuJoCo environments are time-limited to 1000 time steps, 1024 heads for PPO-FHTD seems like a mistake/oversight. 4. Why does PPO-FHTD with H=1024 produce different results for the different parametrizations? 5. Is Figure 6 surprising since the value function needs to consider a large space of solutions as the horizon increases?6. Given distributional RL provides a large performance gain (which to the best of my knowledge, we are still missing a conclusive reason as to why), I'm not sure PPO-C51 > PPO-TD is a significant result. 7. It would be clearer if DisPPO was described before mentioning Figure 15. 8. Figure 15 seems like an important conclusion and should be contained in the main body of the paper. However, the y-axis of Figure 15 also conflicts with the description in the main body so I'm not sure what the correct interpretation is.9. I wonder if the result from Figure 9 is reproducible if the flipping was done in a different way. In the MuJoCo environments is the agent is rewarded mainly for velocity and the behavior of the agent in these cases would be enlightening. Does the agent run forward and then attempt to terminate? Can it move backwards?Conclusion:I think the authors present a lot of interesting ideas and experimental approaches to answer their underlying questions. However, I felt that the experimentation was not sufficiently robust to justify their conclusions and I cannot recommend acceptance.References- Engstrom, Logan, et al. "Implementation Matters in Deep RL: A Case Study on PPO and TRPO." 2019.- Fujimoto, Scott, et al. "Off-policy deep reinforcement learning without exploration." 2019.- Bengio, Emmanuel, et al. "Interference and Generalization in Temporal Difference Learning." 2020. This work introduces a new architecture for predicting the acoustic scattering fields from an object and an incident plane wave. This is done by describing the object using a point cloud mesh whose local geometric properties is encoded in a set of latent space vectors and processed by a deep neural network. The resulting architecture can predict the acoustic scattering fields quickly with high accuracy for a given test set.The problem formulated is an interesting one and the numerical results are very impressive. The description of the method, leaves quite a bit to be desired. The details of the network are described in vague terms and when formulas are introduced, much of the notation is not defined or not well motivated. As a result, it becomes very hard to understand exactly how and why the proposed method works. For this reason, I do not recommend that this work be published as part of the proceedings.As stated above, the numerical results are impressive. The authors state that their network is able to calculate the acoustic scattering field in less than one millisecond for a given object. It is not clear how this compares to the baseline method or the state of the art (PointNet and DGCNN). Section 5.1 mentions it taking twelve days to generate the ground truth data set of 100000 objects, which comes out to about 10 seconds, so this would be a considerable speedup. Still, it is not clear how much faster the proposed method is compared to the state of the art.The main problem is the description of the method, starting in eq. (4), where differential coordinates are defined. These seem to be vectors in R³, but what is their purpose? What do they encode? Then a variant of this is introduced in (5), where a learned latent space is used in the form of the z coordinates. How exactly are these latent space coordinates learned? What is the objective? Then the vector in eq. (6) is introduced. It is sometimes referred to as a discrete Laplacian, but it is not clear how (in the equation, it is simply labeled feature). Sections 4.4 and 4.5 are similarly impenetrable. Furthermore Section 4.5 refers to Appendix B, which defines a theorem, but which has no proof. **Review Summary**I have a question about the correctness of this paper because I do not think the statement and proof of the main theorem are mathematically rigorous nor correct. Regarding empirical evaluation, I think this paper correctly evaluates the proposed diverse sampling enhances the performance of base GNNs in the multi-class and multi-label node classification problems. However, I have several questions about the discussion about the ablation studies.**Summary**This paper proposed DS-GNN, a GNN augmented with plug-in diverse sampling methods. It conducts node sampling to construct multiple subgraphs and apply base GNNs to them. Theoretically, they claimed to show that DS-GNN has high expressive power when the number of sampled subgraphs are sufficiently large. Empirically, they applied DS-GNN to multi-class and multi-label node classification problems and claimed to demonstrate that the proposed sampling method enhances the performance of base GNNs and that DS-GNN is comparable to the state-of-the-art GNN models. Finally, this paper constructs a new dataset, DBMovie, which can be used for multi-label node classification problems.**Claim**If I understand correctly, this paper's claims are as follows:- [1-1] Claim 1: The proposed DS-GNN has theoretically powerful expressive power.- [1-2] Claim 2: The proposed DS-GNN empirically performs well.- [1-3] Claim 3: Practically, DS-GNN applies to any GNNs.**Soundness of the claims**Can theory support the claim?- [2-1] Claim 1: This paper justified this claim by proving that the proposed method is injective in the sense that "any two nodes that have either different features or different neighborhood structures are mapped to different values" (I call this property as Property A in this review). I agree with this paper in that it is one way to prove the expressive power of GNNs. However, I question the correctness of the main theorem for establishing this result (see Correctness section).- [2-2] Claim 2 is not a theoretical one.- [2-3] The model described in Section 3 supports Claim 3.Can empirical evaluation support the claim?- [3-1] Claim 1 is not an empirical one.- [3-2] Claim 2: Experiments in Tables 2 and 3 support this claim for node classification tasks. The ablation studies in Figures 3 and 4 are appropriate to confirm if the proposed sampling strategy works as expected. - [3-3] (This is not a requirement but a suggestion) I think it would be beneficial to claim the proposed method's strength if this paper compares it experimentally with CPNGNN and PNA, which the authors mentioned in the introduction. By doing so, I think this paper could claim that the tricky design (according to this paper) of CPNGNN and PNA is harmful to empirical performance.- [3-4] Claim 3: They applied the proposed diverse sampling to two types of GNNs: GCN and GCT, which empirically support Claim 3.**Significance and novelty**Novelty- [4-1] This paper has constructed a new dataset, DB-Movie, for multi-label node classification problems.Relation to previous work- [5-1] I think this paper discuss the relation with previous work well.   - [5-1-1] The idea of sampling the input graph itself has been employed in existing studies, such as DeepWalk, DeepEdge, and GraphSage. The proposed method differs from them because they are random-walk-based (DeepWalk) or edge-sampling-based (DeepEdge and GraphSage), while DS-GNN is node-sampling-based. I think it is better to mention FastGCN [Chen et al. 2018], another edge-sampling-based GNN.  - [5-1-2] Designing GNNs that have the maximal expressive power has been investigated by previous studies, such as CPNGNN and PNA. The proposed method took a strategy different from these methods.[Chen et al. 2018] Chen, J., Ma, T., & Xiao, C. FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling. ICLR2018.**Correctness**Is the theorem correct?- [6-1] If I understand correctly, the statement nor proof of the main Theorem is not appropriate mathematically.  - [6-1-1] I am not confident I understand what "can be close to 1" correctly. Does it mean that the probability tends to 1 as some parameters (such as K) goes to infinity? I think "can be close to 1" in the statement is not a popular mathematical term, so I would recommend writing what it means mathematically. Also, the term "with high probability" in the proof has several meanings depending on the context. So, it is better to clarify what it means mathematically.  - [6-1-2] Assuming that my understanding of "can be close to 1" is correct (the limit is in terms of $K\to \infty$), I think the proof does not prove it. Specifically, the proof claims that from the assumption that "$u$ and $v$ have different attributes or neighbors, each item in Eq. 16 can be smaller than 1.", it is true that the injectivity holds with high probability. However, it is not true in general that $0\leq a_i < 1$ for all $i$ implies $\prod_{i=0}^\infty a_i = 0$. ~~For example we have $(1-1/n)^ne^{-1} > 0$ as $n\to \infty$.~~  [Nov. 11, 2020 edit]: Since I found this example is not appropriate, I gave another example. See the reply to this comment.  - [6-1-3] $p$ in (13) implicitly depends on the index $i$. Does (13) mean that the probability is the same for all $i$ ? Same is true of (14), in which $q$ implicitly depends on $i$ and $j$.  - [6-1-4] The proof says that "it is reasonable to assume that [...], $q\leq p$". I recommend explictly writing the assumption not in the proof but in the statement.- [6-2] Even if the main Theorem is true, I think we cannot show that DS-GNNs has the Property A. If I understand correctly, the proof assumes that the probability that $h_u^{i, L} = h_v^{i, L}$ is not 1 for some $i$, where $h_\ast^{i, L}$'s are the outputs of the base GNN. In order to prove that DS-GNN has Property A, this paper seems to assume implicitly the following assumption (Assumption B) and apply the main Theorem. However, I think Assumption B is almost equivalent to assuming that the base GNN is sufficiently strong, which is what we want the desired GNN to have.    - [6-2-1] Assumption B: The base GNN can output different representations with non-zero probability if $u$ and $v$ has either different features or different neighborhoods.Is the experimental evaluation correct?- [7-1] Multi-class node classification problems in Section 4.3  - [7-1-1] Table 2: Basically, OK. However, I wonder why only DS-GCN and DS-GAT have standard deviations (Table 4), while others not.  - [7-1-2] Figure 3: This paper discussed that accuracy is stabilized when the injectivity probability is close to 1. However, I have several questions about this statement. First, the accuracy in Figure 3 is not stabilized: the performance peaks at K=5 and drops at K=6. Second, since this experiment does not measure the injectivity probability, we cannot say the injectivity probability is truly close to 1. Finally, we do not know that the injectivity probability is truly correlated with performance accuracy.   - [7-1-3] Figure 4: This paper discussed that "[Furthermore], under diverse sampling, we observe that the best $p_{init}$ are different among datasets." However, we cannot check whether it is correct because the concrete values of $p_{init}$ are not shown. I would recommend this paper to show the values of $p_{init}$ if this paper wants to claim this sentence.- [7-2] Multi-label node classification problem in Section 4.4  - [7-2-1] I did not find any critical problems. I think it was good that this paper compared the proposed method with not only GNNs but also MLP and DeepWalk to show that the task needs both node features and graph topologies.Is the experiment reproducible  - [8-1] Experiment code is not provided. However, hyperparameters and train/validation/test splits are written.  - [8-2] Since the DBMovie is not available, we cannot reproduce the results in Section 4.4.**Clarity**Can I understand the main point of the paper easily?- [9-1] Yes. However, the main theorem and its proof is not clear.Is the organization of paper well?- [10-1] YesAre figures and tables appropriately made?  - [11-1] Yes**Additional feedback**- [12-1] It is better to discuss the computational complexity of the proposed methods. If I understand correctly, the time and space complexity is multiplied by K. Is my understand correct?- [12-2] The new dataset, DBMovie, is interesting. Do you have a plan to publish the dataset?Minor comments- [13-1] Page 1, Abstract: L.10: For a target node, diverse sampling offers it diverse neighborhoods, i.e., [...]  For a target node, diverse samling offers diverse neightborhoods to it., i.e., [...]- [13-2] Page 2, Paragraph 3: For convenience, we denote with DS-GNN ...  we denote by DS-GNN - [13-3] Page 3, L.3: Remove DS-GNN- [13-4] Page 4, paragraph after (4): I could not understand what this paper intended to mean by "a generation of Dropout from dropping features to dropping edges"- [13-5] Page 6, Section 4.1, Note that [...] 10 sampled garphs [...]  graph- [13-6] Page 6, Section 4.1, All GNN models [...] leverage Relu as [...]  ReLU- [13-7] Page 8 Section 4, Last Paragraph: [...] Deep Walk (only using only) [...]:  only using graph structures- [13-8] K is not writen in italic in some places (e.g., Page4 Sampling Strategy, last sentence)- [13-9] Appendix, after (14): $q <= p$$q\leq p$ This paper studies online test for qualitative treatment tests. The authors propose a scalable online algorithm for Type 1 error control. I find the paper under-developed that the writing has to be substantially improved, and the presentation, especially in Section 3.2, is not friendly.1. The authors claim to have an "Online" algorithm. However, I don't see that the algorithm addresses any real online challenge. The challenge of online testing is that we are doing testing at each iteration, and it is difficult to adjust all p-values. The authors didn't address this issue at all, which is a fatal flaw.2. The linear space approximation is very artificial to me. Can the authors give some real motivating applications?3. I highly suggest the authors to move the theorems 3 and 4 to Section 3.2, and move the derivation (which are standard) to the appendix.Overall, I don't see much novelty, and the authors are overclaming the contribution. This is a clear rejection. Summary:The authors present a regularisation term for Variational Auotencoders that forces the distance of mapped points in the embedding space to be similar to the distance of those points in the metagraph of the data derived from relational information about these points. The intention of the regularisation term is to enforce a consistent graph between the original representation and the embedded representation in a manner that is agnostic to the structural choices of the model to be estimated. Strengths:1) The paper provides a good organisation of existing methods for utilising topological information, and, thus, positions its contributions well in relation to existing work.2) The empirical results are presented honestly, even when they do not support the proposed method.Weaknesses:Ordered form less to more specific:1) The intention of the paper is unclear.Is this intended as a review paper of recent research on graph neural networks or to present a new regularisation term? The title and abstract seem to imply that this is intended as a review, but the paper only considers three existing methods and a single modification to VAEs. Unfortunately, the paper is not convincing in either regard, and the idea of analysing topological information to improve classifications and representations is already well discussed in the literature and a very active area of ongoing research.2)  The efficacy of the proposed regularisation is not convincingly supported either theoretically or empirically.The authors state, Notably, GR-VAE is devised to infer topological information solely from a soft constraint, without any architectural requirements such as graph convolutions (line 1, p. 4), but this is not discussed further. The intention of graph convolutions is to explicitly encode assumptions about the relationships present in the data, in this situation why would I prefer a soft constraint to a well motivated, explicit one? If structural constraints were unduly restricting the expressiveness of the models, I would expect to see this borne out in the empirical results, but this is not the case across the chemical reaction and citation network experiments. 3) The paper struggles with clarity at points.Specifically, equation (1), which describes the regularisation term is unclear as written: does the plus sign in the exponent denote absolute value or something else? This notation is non-standard and I would not be able to faithfully recreate the results as written.  Additionally, the method for constructing the meta-graph G should be discussed in more detail. From what is this graph derived? Is it an existing observed graph that describes the observed relationships between the data, ex. the citation network or pixels in an image, or does it describe adjacency of the observations as defined by the observed labels or other meta-information. My concern is that using a soft-constraint which effectively focusses the model on the labels in an easy task such as MNIST classification or the synthetic data task hides the fact that the constraint is too soft to produce a useful regularisation of the model, as evidenced by the failure of GR-VAE relative to DGI or even vanilla VAE in the citation network and chemical reactions tasks. Reasons for score: I vote for rejecting the paper, as while I really do appreciate that the results of the paper are presented honestly, I think there are concerns with the current draft. Questions for the rebuttal period:Please refer to the questions in the weaknesses section.  *Summary*:The authors try to solve a special kind of high-dimensional optimal transport problem. Specifically, they consider the cases when features are grouped and the grouping is known a-priori. The authors formulate the problem into the feature-robust optimal transport (FROT) problem.The authors propose two solving algorithms, one based on the Frank-Wolfe method, and one based on linear programming.*Pros*:The connection to the feature group sounds interesting to me, as it has a natural connection to the structure of deep learning models.The presentation (other than the introduction) is easy to follow.*Cons*:Note that the first point is the main contributing factor for my rating.1. Section 3.1 is very confusing, and it seems to me that the authors fail to establish the correct convergence guarantee.As in page 4, the target is $min_{\pi} max_{\alpha} J(\Pi, \alpha)$ If we fix $\pi$, we can solve for the optimal $\alpha$. Plug this optimal $\alpha$ back in and we obtain $G(\Pi)$.Intuitively one may choose to solve for $\alpha$ and $\pi$ alternatingly.However the convergence of $G(\Pi)$ says nothing more than, in a fixed iteration, one can solve exactly for the optimal $\alpha$ and up to $\epsilon$ accuracy for $\Pi$.We still don't know if the solution of the algorithm indeed minimizes the said loss.I checked the proof of proposition 4. It just invokes the standard FW-convergence analysis from Jaggi 2013, and argue nothing about the alternative part. Note that, even though the two subproblems (for $\alpha$ and for $\pi$) can be solved almost exactly, it could be non-trivial to set up the convergence of the entire alternating algorithm.Alternatively, maybe the authors want to argue that solving $min_{\pi} max_{\alpha} J(\Pi, \alpha)$ is equivalent to solving $\max_{\pi} G(\Pi)$. However, this is also not obviously true for me.2.  What are the other potential applications of FROT? While Semantic Correpondance is an interesting application, I find it hard to convince myself that FROT is better than Liu's 2020-CVPR work (requiring validation dataset is not a big problem - you can always to train-val split). With its similarity to group lasso, FROT might have more interesting applications.3. Presentation of the introduction can be improved. I find it hard to parse the introduction until I almost finished reading the entire paper. Putting figure 1 to page 2 only creates more questions in my head instead of offering intuitions. Also, it would be helpful if the author can list their contributions in a more organized way.4. I didn't quite get the high dimensional part. While 'high-dimensional' appears in the abstract, introduction, and conclusion section, I didn't find the correspondence in the main text.5. I didn't get the robust part, other than the empirical performance in the evaluation section. UserBERT: Self-supervised User Representation Learning  ######################################################################Summary:   The paper provides an extension of BERT to user data for pre-training user representation in a self-supervised manner. In particular, it analogise the user behaviour sequence to words in a sentence and leverages the Masked Language Model (MLM) approach typically used in NLP to train the user embedding. To facilitate such extension, the paper also proposes a discretisation approach and a unified input structuring to include long-term, short-term and demographic information.    ######################################################################Pros:    1. Even though, the idea of extending the self-supervised pre-training for user representations is not new, it is still an interesting area of research  2. The discretisation of user behaviour signals over long-term and short-term to form behavioural words is quite reasonable  ##################################################################### Concerns:   The key concerns about the contributions of the paper are as follows: Overall, the novelty of this work is very limited. To elaborate:The major contribution of this work is two fold:- Discretisation of raw user behaviour sequences (for long-term and short-term) and- Using the discretised aggregated behaviour words as inputs to the BERT architecture as is. The other claimed contributions such as having a unified architecture and experiments to validate the approach do not seem substantial.When it comes to discretisation, though the idea seems appropriate, two crucial questions regarding this step are not validated:1) There is no empirical evidence presented in the paper which shows discretisation improves UserBERTs accuracy. Since it is a major contribution, I request the authors to design and implement an ablation study to address this point.2) Seemingly, the authors have come up with a hand crafted/heuristic-driven  approach for discretisation. Why cant it be data-driven too? Meaning, can clustering of actions be done in a data-driven manner? If so, what is the difference in accuracy between the proposed heuristic-driven and the data-driven alternative.When it comes to the second contribution, it is certainly not novel i.e., the paper does not propose any architectural change to BERT. Though the paper claims that the presented model is a unified model to learn long term, short term and demographics based user profile, the unification is brought upon as a by-product of feeding multimodal inputs to vanilla BERT. Hence, the overall novelty of the paper is very limited.The following comments are my major concerns in each section of the paper:Section 2:While the authors have reviewed some literature in transfer learning and self supervised learning and have cited some relevant work, they have not cited even one reference in section 2.3 which on User Modeling i.e., the main theme of the paper. I request the author to make a thorough survey and cite related work in section 2.3 and also highlight how UserBERT is different from them.Section 3:Overall, this section (and section 4) lack cohesion and can be written clearer with the figures, tables, algorithms and descriptions. This could help the reader better understand the approach. For instance, the following main points in the approach are not explained well:1) The paper states the final loss for one input sequence is the weighted sum of the losses of all masked tokens. There is no detail what the weights are, whether they are assigned based on heuristic or learnt.2) The approach considers ordinal attributes such as expense and age similar to categorical attributes (e.g., each age has a unique embedding). This seems counterintuitive and there is no empirical evidence to show that this counterintuitive design works well.3) It is not clear how to use the hidden representation to predict attributes from the transformed masked tokens. More precisely, it is possible that many attributes belonging to different actions are masked and then converted into one token embedding. So what attributes are to be predicted in the final fully connected layer, in this case?4) Minor concern: what does E stand for in equation 1? Section 4:Overall, in this section, the experimental design is not comprehensive, and the results are not convincing for the following reasons: - Along with the Wide&Deep, LSTM, Transformers as baseline, it would have been better to also include vanilla BERT to the baselines against which the UserBERT can be compared. In fact, Vanilla BERT would be the closest and most appropriate baseline for comparison. Hence, I request the authors to include it.- All the experiments are conducted on custom datasets. Since user profiling is an extremely useful and ubiquitous activity that benefits multiple domains, I request the authors to experiment UserBERT on well-known open source e-commerce (and other user profiling) datasets (ref: [1] and [2]). In fact, the profiles could be tested on downstream tasks like next genre prediction with these datasets. This will help the reader to trust the UserBERT model better.- Input representation, being one of the major contributions of the paper, it would give more insights if an ablation study is made on the user behaviour data (long-term features, short-term features, demographic features) to compare and contrast the contribution and lift by each of the behaviour categories - In the attribute prediction task, within the two attributes experimented, the performance of the proposed model is quite unconvincing. Would benefit if more experiments are performed.- For the next genre prediction, though there are more than 10k genres, each users typically have a very small subset of interest. Therefore, it would be more informative if can compare the models mAP@10 with the user-level modes mAP@10.- The discussions of results are very vague and could be a lot deeper and precise.##################################################################### Questions during rebuttal period:    Please address and clarify the concerns above    ##################################################################### Reference[1] Sun, Fei, et al. "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer." Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 2019.[2] Kang, Wang-Cheng, and Julian McAuley. "Self-attentive sequential recommendation." 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 2018. This paper proposed a cross-modal retrieval augmentation for the multi-modal classification task (VQA). The authors first introduce a transformer-based image caption retrieval architecture that achieves decent performance. Then, the authors proposed to use the retrieval model to retrieve relevant visual and textual information as augmentation. The proposed method experiment on 3 existing method (Visual Bert, ViLBERT, and Movie + MCAN) and show good improvements over the baseline model. My major concern about this paper is the lack of novelty and experiment comparison. The proposed image caption retrieval architecture is not novel at all. Most existing method (ViLBERT, UNITER, VLBERT etc.) has a similar transformer objective, while the image fine-tuning are from Pixel Bert. In the experiment section (Table 2), the author even didn't compare these methods. In terms of the VQA performance, pre-training on the conceptual caption actually hurt the performance of visual Bert and ViLBERT. Could the authors explain why a larger dataset can not help with the model? Is Cross-Modal Retrieval pre-training necessary for VQA?  In terms of the speed, the deep fusion model for image retrieval is super slow, since the model need to calculate the score for each pair. What is the size of pool when computing the retrieved captions? What is the time complexity? 