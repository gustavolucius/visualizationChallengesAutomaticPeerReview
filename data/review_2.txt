 This paper proposes a method for a deep generative model utilizing a trained autoencoder in conjunction with kernel conditional embeddings. The authors validate their model experimentally on CelebA and a brain imaging dataset.Overview : I found this paper very confusing with crucial mathematical definitions and exposition often inconsistent or imprecise. Outside of this the general exposition in many ways very strange and confusing. Finally the lack of experimental code makes this paper a very clear reject in my opinion. The rest of the review will list some concrete examples of the issues I mentioned.Mathematical imprecision:- The Phi operator in this paper is problematic. I'm guessing this is meant to be the kernel feature map, but in Def 1 it is introduced as an arbitrary element of the RKHS. In Def 5 Phi_H and Phi_G are initially introduced via the phrase "where an element is denoted by Phi_G" seeming to imply that is again simply an example element of their respective RKHSs. Later in this definition the well-known (kernel) cross covariance operator "C_YX = E_YX [ Phi_G (y) \otimes Phi_H (x)]" which I am assuming implies that it is the kernel feature map, since otherwise this quantity is simply a scalar. Finally in the description of their main algorithm step 1 the refer to Phi_G^T which is highly problematic since Phi_G is presumably well defined here, meaning it is the kernel feature map which is not a linear operator (the whole point of kernel methods being that this operator is nonlinear), thus its "transpose" isn't something that makes sense. The authors give a vector which is equal to the expression in which Phi_G appears, this however depends on a \bold{1} vector defined as " are the top lambda (a hyper-parameter) latent representations" which is not explained further. Presumably this is referring to approximate inverse kernel feature map (which has seen some research), but this needs to be further explained, or at the very least a citation is needed. General Exposition:-I am not sure why the approach to the paper is being framed in a Markov setting, when it seems like the authors really only need kernel conditional embeddings which have the exact same form as the Perron Frobenius operator the authors use. The authors even cite the kernel conditional embedding paper Song et al. 2009. Things like (1) which only serve to complicate the exposition unnecessarily considering there is no Markov process type of structure in the main algorithm.-p. 2 Rationale: "For instance, it is known that given infinite-dimensional observables of the input measurements, there exists a linear transfer operator that perfectly pushes one probability density to another. Of course, this is not practically beneficial because explicitly constructing such infinite-dimensional observables for the data could be intractable. Nonetheless, results in control theory demonstrate that the idea can still be effective in specific cases, using approximations with either spectral analysis of large but finite number of observable functions " This sort of thing needs to be decompressed, I don't know what is meant by an "observable" I assume it has something to do with control theory or Markov process theory. This seems to niche for me for the deep learning crowd.-The Riemannian geometry in the paper needs to be explained more for the average deep learning researcher, e.g. its not clear why the "geodesic interpolation" is doing at or why its algorithm makes sense.-The main algorithm needs to be explained with more details as to _why_ we are doing the steps. Summary:This paper proposes a new approach for meta-RL. The paper claims that the proposed method reduces variance and bias of the meta-gradient estimation by only a few samples. In addition, this paper claims that their method is more interpretable. My comments:There are lots of unknown, unwarranted claims about this paper in addition to no thorough experiments and comparison with previous works : 1. Paper claimed that their proposed method not only reduces the variance of meta-gradient but also reduces bias. Reading through this method and experiments, I don't see any theoretical or empirical justification why that should be the case.2. The method claims this method has a more interpretable meta-gradient. Again, there is nothing in this paper that verifies this claim.3. Paper proposed to use fewer samples for the adaptation phase. I don't understand why this can help at all.4. Experiments are incomplete and there is no rigorous evaluation to analyze this method.5. The proposed context in this paper has been already proposed in previous work, not sure what is new here.6. There are lots of things that need to defined like trail, few-shot, etc. In summary, this paper is not ready at all and needs lots of works. Hence, I'd recommend this paper to be rejected. *** Key idea justification ***This work shows that contrastive loss (for self-supervised learning) is an upper bound of cross-entropy loss (for supervised learning) and leads to a conclusion that this is the underlying reason why self-supervised learning can help supervised learning in FSL. This reasoning makes little sense with little logic. Concretely, there exist a number of to-be-answered questions before connecting the two things and making theoretical conclusion: 1) Why we need to know the upper bound of supervised learning loss given that we already have label data with the training data? 2) Decreasing SSL loss does not necessarily mean that supervised learning loss is also decreased, as it is just an upper bound. No guarantee there. 3) Assume SSL helps decrease the supervised learning loss, then why is this needed when we can simply use class labels to minimize it? Intuitively, the two are overlapping and SSL should be not useful. Besides, this paper only considers the case of contrastive loss which involves false negative samples. What if applying other SSL loss function, for example rotation? I do see the same analysis applies to that.In conclusion, the proposed theory makes little sense and is also over-claimed. The whole study is neither theoretical nor logical. *** Presentation clarity ***1) In general, the presentation of this paper is poor. One reason is using odd/strange terminologies and equation expressions. For example, contrastive loss (Eq 1) and cross-entropy loss (Eq 3) both are not given in their common expression. Other examples are "Supervised Metric for Representations" and "Self-Supervised Metric (SSM) for Representations", "a metric loss", etc. 2) Quite a few equations are hard to read and understand. First, Eq (1) and (3) are not expressed in a standard way. How are they derived? 3) What is the difference between a class-wise prototype pc and an episodic mean of support samples (At the end of Sec 3).4) What means by "the class distribution Á is uniform" in the proof of Theorem 2?5) What is implied by the last sentence of Sec 4: Theoretically, if given an unsupervised set with infinite classes and data, the performance achieved by SSM can be very close to that by supervised training?*** Grammatical errors ***1) a episodic -> an episodic `The paper argues that the existing way of using Translation Memory (TM) in neural machine translation (NMT) is sub-optimal. Therefore it proposes TMG-NMT, namely Translation Memory Guided NMT, which consists of two parts, a universal memory encoder and a TM guided decoder. Experiments are performed to demonstrate that their method can significantly improve the translation quality and show strong adaptation for a new domain. Pros: NoneCons:1. The main concern of this paper is that, the contributions are quite limited. The authors claimed three contributions: n-gram retrieval, universal encoder, and using the copy mechanism. Basically, none of them is novel.   - Bapna & Firat (2019) and Xu et al. (2020) have used n-gram matching for retrieval.   - In late 2020, what is the novelty of using a multilingual BERT when encoding source sentences and retrieved TM sentences? Very little if any.   - Likewise, using the copy mechanism to tackle rare word problems seems a regular approach. Overall, I don't see any of these so-called contributions are truly technically original. This paper seems a very hurry combination of some existing techniques. I basically learn nothing new from reading this submission. 2. Another one of the key concerns about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Despite the paper stating that there have been earlier work (Gu et al. 2017, Can & Xiong. 2018, Xia et al. 2019) that explore Translation Memory in NMT, the paper does not compare with them and only compare to non-TM-guided baselines, making the improvement less convincing. In addition, what is the language pair evaluated in this paper, which was not even mentioned...3. Considering the limited results, a deeper analysis of the proposed method would have been nice. Is the semantic relationship between the source sentence and TM sentences well learned in TMG-NMT? What kind of translation error can be well addressed with the help of TM? Further analysis of the proposed model would provide greater insight to the community.4. Section 5.4: The results would have been more complete if another setting is considered where the transformer is adapted to the target domain without using the TM mechanism, such as fine-tuning the vanilla transformer on the provided TM parallel sentences. In this way, the adaptation ability of TMG-NMT could be better proven.  5. To be honest, the writing of the current version seems a disaster. Not to mention the impressive amount of grammar errors, many parts of the storytelling are logically incoherent. For example,  - "Although Bapna & Firat (2019) and Xu et al. (2020) **also** use the n-gram method to search, they still need to select the corresponding sentence that maximizes the n-gram similarity with the source sentence. " - The usage of "also" is so weird, when you didn't even mention you are using n-gram method in advance... And by the way, what is the difference between your ngram matching and theirs? You should've made it clear.  - "To obtain sufficient training corpus and train network parameters more fully and effectively, in this paper, we also modify the retrieval algorithm and use a pre-trained language model (PLM) to initialize the encoders parameters. Partially inspired by phrase-based SMT, we dont compute the sentence level similarity score between two sentences in our retrieved method. If two sentences have a common n-gram segment, we assume that they are similar, and the sentence pairs of the TM database can provide a useful segment to help improve the translation quality. Currently, many studies have proven that PLM can offer valuable prior knowledge to enhance the translation performance of NMT (Weng et al., 2020; Song et al., 2019), So we also employ PLM to initialize the parameters of the encoder and give encoder well-trained parameters as a starting point." - What is the logical relationship b/w the first sentence and the second one?   - Please check minors for more details.*****Minors:1) It would have been nice to see that the format of the reference are unified.2) Khandelwal et al, 2020 [1] propose a novel way to incorporate Translation Memory into NMT which may bring you more thoughts towards using TM.Typos: too many. 1. Equation (2): a redundant close paren2. Section 3.1 penultimate paragraph: l-th encoder layer -> L-th encoder layer3. Section 4.2 First paragraph: N->M n->mGrammar errors:Too many. E.g., In the contribution part in the intro, the second and the third items start with "does" and "apply". What are the subj of these two verbs?Please try to properly use Grammarly to check your writing before submission.[1] "Nearest Neighbor Machine Translation. Khandelwal et al. 2020 arXiv."******Reasons for score:The novelty of this paper is basically none. The experimental results are limited and the comparison with prior work is none, which cannot fully demonstrate the effectiveness of the proposed method. __Summary__They authors describe a method to detect structural variation from aligned sequencing reads in a genome browser view. Their model encodes this genome browser view into an RGB image and applies a deep convolutional neural network to classify variant type (or no variant). They make use of curated variant annotations to train and test their model.__Major comments__* The RGB encoding is entirely arbitrary, unnecessary, and confusing. The authors should consider the actual range of the various input data and encode with a simple and interpretable strategy. For example, nucleotides are typically one hot encoded.* The improved accuracy on this task is abstract when only summarized in tables. Depicting an example of a structural variant whose prediction is improved by the authors method would be very valuable. Ideally, both a false positive turned true negative and a false negative turned true positive could be shown.* The requirement that this method be provided candidate structural variation start and end points means that it's actually a module that would need to be plugged into a pipeline that also specified how those candidates are obtained. I would encourage the authors to develop that strategy before publishing their method.* The authors have not clearly described how the predictions of the other methods were used to annotate these curated variants. Doing so involves critical parameters, such as the allowed distance between the method prediction and true specified variant break points. Setting these parameters to strict values would be very unfair given that the authors method is not required to produce such breakpoints de novo. The Zook et al. paper describes this process in detail and discusses several software packages to do it.__Minor comments__* Does the authors method make use of paired end read information?* How does Table 2 combine the accuracies for duplications and indels? The authors in the paper describe a deep learning approach to detect copy number variants (CNVs) from DNA sequencing data, CNV-Net.  It described the approach by transforming the pileups into images and pass them through a CNN. This strategy has been proposed four or five years back to do SNPs and indels calling (DeepVariant).  It is challenging for SVs and CNVs as they can be arbitrarily large. However, there are also several existing models for this task (e.g. DeepSV, RDBKE). In this work, the authors only consider "candidate CNV regions" which are 201-bp small genomic regions that centered at the breakpoints. The paper is only 4.5 pages. First, the authors did not explain or investigate many decisions in their model design. Then the experiments are flawed. I have many questions and concerns. 1) First of all, this should not be called a CNV detector because it is in fact a breakpoint detector2) How did the authors do negative sampling? Were the negative samples randomly drawn from the genome? This creates bias as the sequence features might shift dramatically. 3) Why are the negative samples not balanced with the positive samples?4) Are there differences in performance between deletion and duplications breakpoints? Does the model distinguish these two types? 5) When splitting the training/validation/testing dataset, there will be significant data leakage if this is done randomly. Many SV/CNV regions have characteristic repeats patterns.6) The comparison between CNV-Net and other methods is not fair as the other methods are finding CNVs in the whole genome while CNV-Net is given a pre-defined set of "candidate breakpoint regions" in which positive samples have breakpoints perfectly centered in the middle. Moreover, such candidate regions might suffer from data leakage. 7) Does HG002 really only has 174 duplications while NA12878 has 22,936? 8) GIAB has more than two genomes available. Can we test on more genomes? 9) Minor: RELU -> ReLU  This paper introduces a number of data preprocessing options for numeric features provided by an open source library automunge. The most of the paper focuses on explaining the specific transformations offered under each option, including normalization, binning, and noise injection. For normalization, a new transformation 'retain' is offered in addition to traditional z-score, min-max etc. The paper uses one section between normalization and binning to explain a notion 'family tree primitives' which is used in the composition of multiple transformations. In experiments, the paper uses the higgs dataset. Three settings of the dataset are used: full data, 5% data and 0.25% data. 6 settings of transformations are used: raw data, z-score, retain, retain with bins, retain with noise injection, and retain with partial noise injection. When averaged over the three settings of the dataset, using raw data leads to suboptimal auc score compared with the other five. It is acknowledged that "the metrics for normalized data were slightly better than raw on average, it is not clear if they were sufficiently statistically significant to draw firm conclusions. " The paper concludes that "any consideration around benefits of feature engineering should distinguishfirst by scale of data available for training. When approaching big data scale input with infinitecomputational resources there may be less of a case to be made for much beyond basic normalizedinput. "Pros: The code is open sourced and a larger number of data preprocessing options are offered. The library is released as a python package which is easy to access. In the experiments there are certain cases where the preprocessing helps.Cons: 1. The experiment results do not convincingly demonstrate the utility of the proposed library. First, when using full data of higgs, the auc is highest when using the raw data. It is even better than using z-score or retain to normalize the data. That is contradictory with the suggestion to consider even basic normalization. Second, when small samples of the data are used, the best auc score among all studied transformations is much lower than the auc score on full raw data. That means even with the transformations, the accuracy does not reach on par with simply using full raw data. Then it raises the question whether the setting of using small sample of higgs data matters. Third, even if we do care about those settings, the best performing transformation is z-score only. That is not a contribution of this paper. To conclude, it is not surprising that one can find some scenarios where some transformation improves the accuracy, but the paper has not presented a convincing scenario where the proposed library is useful.2. The explanation of 'family tree primitives' confuses me. Table 3 does not help as it is full of unexplained terms. Without examples and context it is difficult to know what 'parents' 'siblings' etc. refer to.3. It remains a question how a user should select transformations from the numerous offered options. This paper reviews an existing Python feature engineering library, Automunge. It describes Automunge's functions operating on numeric (as opposed to categorical) input data. It describes notions of transformations, operations available to bin, inject noise, process sequential data and integer sets. The Automunge library implements what the data scientist might expect from a library of feature transforms for numerical data. It is not particularly rich or original. The library structure is not made clear in the paper. It is a pity that the paper does not compare to any other feature engineering library, although there are very many available. The experiments section reports two experiments on one dataset; it reports AUC and accuracy metrics under different feature pre-processing: this reporting is hardly relevant and does not demonstrate any interesting property of Automunge. For instance, it would be interesting to see whether Automunge is particularly flexible, clear, or convenient when one wants to implement chains of transformations. It is not clear why, table 8, it is interesting to observe 100%, 5% and 0.25% of data regimes, nor why we should average over these three.Tables 3 and 5, and none of the figures are mentioned in the text, let alone commented upon. The figures are left unexplained and contain undefined abreviations (NArw, DPo3, DPo6). I could not make sense of them. The family tree structure which seems to be the object of Table 3 is not explained, and I could not make sense of it, though it seems to be important. In table 4, I could not understand what the columns refer to; equally, the transformations mentioned there are left undefined: for instance, in case, as I assume, "Number of standard deviations from the mean" is $x \mapsto round(|(x_i - µ) / \sigma) |)$ (or maybe the absolute value needs to be replaced by round brackets? this is left ambiguous), it should be properly defined. Similarly, what is "Powers of ten"? Does it mean $x \mapsto round(\log_{10} x) $ ? The id strings are cryptic. It is not clear why identifiers seem to be all limited to four characters; this makes the entire naming scheme very difficult to follow, as abreviations seem arbitrary; in addition, this is contrary to Python variable naming conventions, particularly naming conventions in popular and successful ML libraries, which prefer explicit and long-form function and argument naming over abreviated and cryptic ones.Table 6 refers to categoric noise injections, but the paper was meant to be about numeric input variables, so I'm not clear what it is doing here.The text is hard to understand. This is in part, but not only, due to long-winded, unclear sentences: for instance the very first sentence is obscure and ill-structured, with several syntax errors, for instance the repeated and often incorrect usage of "such as". Throughout the text, syntax issues make the text hard to understand. Examples of this:- "data transformations ... are to be directed for application to a distinct feature set as input"- such as multiplicative properties at, above, or below.- potentially including custom defined transformation functions with minimal requirements of simple data structures- the last sentence of sec3- the first sentence of sec4 is needlessly intricate and seems to say nothing more than "Automunge transformations are invertible".- the paragraph below figure 2- a given ratio of input entries are flipped to one of the other encodings between which have a uniform probabilityThe very last conclusion paragraph is surprising and seems irrelevant. Similarly, the paper mentions quantum computation for reasons that escape my comprehension, section 2.The paper is so hard to understand that it does not allow one to evaluate possible upsides of Automunge on its own merit. After quite some hesitation, I have to assign this paper a severe rating due to the conjunction of several severe shortcomings. I spot a couple of key issues with the proposed method:1) Misinterpretation of the previous study by Shen et al. (CVPR 2020): As presented in Eq (8) of that CVPR paper, the dataset where the influence loss is applied is NOT necessarily the old training set; It  can be the new training dataset. Also, their results show that using new training set for the influence loss is slightly better than using old training set. This makes the statement that "previous works rely on old training" invalid. So Eq (3) of this submission is not complete and somewhat misleading.2) Shen et al. (CVPR 2020) only use the classifier but not the feature model. Although this proposed work does not use the old classifier, the feature model is instead used to estimate the classifier weights. So, it is not true to claim that "the old model is a black-box" for the proposed method; Just use a different part of the old system. Besides, in deep classification model, the feature vectors and classifier weights are closely related and both reveal similar amount of information.  In particular, if a cosine classifier is used, the two can be exchanged to each other. So, this proposed model is actually leveraging the same amount of information from the old system, as compared to Shen et al. (CVPR 2020).Overall, the two points above would make me to conclude that this work comes with fundamental flaws/drawbacks. I work extensively and publish regularly in autonomous vehicle trajectory prediction and am well-placed to provide a review for this paper.*Summary:* The paper addresses the problem of trajectory prediction, which is to predict the future trajectory of a vehicle given its current trajectory, taking into account the nearby vehicle data. The main contribution of the paper is to investigate the possibility of improving performance of trajectory prediction using negative trajectories, that are generated heuristically.This paper suffers from some major flaws in three key areas -- 1) Insignificance of results, 2) Insufficient Evaluation and 3) Low technical novelty. I will be talking about all of these separately below:**Insignificance of results:** The main contribution of the paper is the uncertainty loss term. Therefore, in order to evaluate the contribution of this loss term, it would need to be evaluated against methods that use the same hyperparameters in a loss equation that excludes the uncertainty loss term. That would be the trajectron++ (using the current paper's hyperparameters) (Tab.2, second last row). The maximum average improvement is 6 cm. To put this in perspective, this improvement is less than a *quarter* of the width of a standard vehicle tire. The advantages and usefulness of improvements on such a small scale are therefore questionable.This insignificance of results is not just contained in Tab.2. In Tables 1 and 3, the maximum average improvement is 9cm. This is likewise not useful. I have worked with several codebases that do trajectory prediction and I can say from experience that improvements on such small scales can easily be achieved simply by tweaking hyperparameters. Therefore, I have serious doubts whether the results are significant with respect to the proposed contribution or are just a consequence of model tuning.*Conclusion*: The current results have not convinced me even slightly if the proposed uncertainty loss makes any difference to the current SOTA in trajectory prediction**Insufficient Evaluation:**  Trajectory prediction using deep neural nets is a very well studied field with dozens of benchmark approaches now that report results using the ADE/FDE metrics. So any contribution would need to be evaluated against multiple methods. In this paper, the proposed contribution has only been evaluated against one method.It should be clearly noted that to specifically measure the value of the contribution, other than trajectron++ ("our hyperparameters"), any of the other baselines are not relevant as the evaluation needs to be performed keeping all the parameters same, which is not the case with any of the other methods. (If the parameters are different, then it cannot be ascertained whether the gains come from the uncertainty loss term, or from somewhere else)*Conclusion:* The validity of the method is questionable since it is compared to only one baseline, when there are more than a dozen SOTA methods available.**Low Technical Novelty:** The novelty of the approach consists of the so-called "Context Checker" which identifies negative trajectories. However, the basis on which these negative trajectories are identified, as well as the definition of negative trajectories, is deeply heuristic and contrived.To begin with, trajectory prediction consists of a distribution of future possibilities. The entire distribution contributes to the uncertainty. So the proposed approach wherein the authors manually select a only a part of this distribution, give it a fancy name like "negative trajectory", and re-train it without a formal explanation seems heuristic and unsound. Another concern that I immediately registered was the number of such negative trajectories, which would be negligible compared to the entire distribution of future trajectories. In fact, this is even explicitly agreed upon by the authors themselves in Section 3.3 ("Design of our Checker"). The low number of such "negative" examples would do little to impact the performance, which corroborates my earlier point about the insignificance of results.Finally, the entire approach is a simple and heuristic extension of trajectron++. The extension involves simply re-labeling a part of the predictions of trajectron++ and re-training them. This is not enough to meet the standards of ICLR.*Conclusion:* The incremental extension over Trajectron++ and heurisitc and unsound nature of the novelty.___**Overall:** My overall recommendation for paper is a **strong reject** since according to my review, it fails on all of the crucial parameters used to judge a paper. The paper is interested with multivariate probabilistic forecasting applied in the context of ride-hailing forecast.The goal is to be able to handle the spatial aspect, causal effects of external covariates (e.g. the causal impact of rain, Christmas on supply/demand) and dependency between supply and demand. To this end, the authors propose a causal attention mechanism        (to not just detect correlation with covariates) and a custom transformer architecture where the quadratic attention cost is avoided. Experiments are performed on public and private datasets against a set of (mostly) univariate baselines.Strong points:+ highly-relevant problem+ novel consideration of handling causality instead of simple covariate correlation in this contextWeak points:- very hard to read, many details missing and notation are not introduced- lack of relevant baselines (e.g. baselines using spatial information)- lack of relevant metrics to illustrate the benefit of the contribution- missing related work discussion for efficient attention computationI recommend a reject for this paper.While the problem presented by the paper is highly relevant for the community, the paper has several issues that makes it not ready for publication. The first issue comes in the clarity of the description and in particular section 3.3 where many terms and notation are not introduced at all making the paper very hard to read (see detailed comments). Experiments are also problematic as many details were unclear (see detailed comments) and would be far from being reproducible. But the most problematic bit is in its design: key aspects introduced in the paper are not asserted in the experiments.1) While the method claims ~5% error improvement on the ride-hailing benchmark, only one baseline have access to spatial information while numerous methods have been proposed to handle spatio-temporal forecast, only one baseline (TFT) is provided with spatial information (with no detail). 2) Since you are interested in the probabilistic forecast of the joint demand/supply targets (collaborative), your experimental setup should consider a joint metric (CRPS-sum for instance [1,2]) rather than the average of R50/R90 metrics of demand/supply targets which is "blind" to correlation between the two demand and supply targets.Finally, I had issue with the Taylor expansion as its description was not clear (see detailed comments) but also I found description of related work missing in this aspect. Attention cost has been decreased to $N\sqrt{N}$, $N\log{N}$ and $N$ respectively in [3, 4, 5], this discussion is clearly missing to relate your contribution.# Additional questions to the author- 3.1 at no point in the paper you say explicitly what is the loss that you are minimizing, this makes it harder to read. I would recommend to specify in 3.1 directly that you minimize NLE with poisson distribution (which is what I understood) and specify also clearly how predictions are made.- 3.3 the beginning of the section mix model description and related work- 3.3 Eq. (3) has two unintroduced notation, what is the \bar? what is "batch"?- 3.3 Eq. (6) is so confusing, you are mixing norms and vectors (unintroduced), $T$ is not explicitly introduced, there is no clear explanation why the approximation holds also (e.g. why a^T W would be small)- 4.2 DeepState is *not* an auto-regressive model, it belongs to the second category.- 4.3: "CausalTrans outperforms all other competing methods primarily due to the use of the causal estimator DML and spatial information". This is very problematic as you dont compare with methods designed to handle spatial information. # Additional feedback (not part of the decision assessment)- 3.1 eq (1) (2) makes an assumption that only demand impacts supply: it would be good to discuss it at least. One could imagine where it breaks.- 3.1 weather features: you should precise whether they are known in advance- 3.3 The section will be easier to read if you indicate variable dimensionsFinally, I would recommend to use a public dataset rather than the private one for your benchmark (e.g. NY taxi dataset or Uber), you also have covariates such as date or weather forecast and this would make your work comparable and reproducible in the future.[1] High-dimensional multivariate forecasting with low-rank Gaussian Copula Processes http://papers.nips.cc/paper/8907-high-dimensional-multivariate-forecasting-with-low-rank-gaussian-copula-processes[2] Multi-variate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows https://arxiv.org/abs/2002.06103[3] Generating Long Sequences with Sparse Transformers https://arxiv.org/pdf/1904.10509.pdf[4] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention https://arxiv.org/abs/2006.16236[5] Reformer: The Efficient Transformer https://arxiv.org/abs/2001.04451 This paper presents a Transformer-based model for aspect-based sentiment analysis, intended to support the unsupervised induction of constituents within the Transformer forward pass. Their evaluations demonstrate that their model can match (and in some cases improve upon) models which depend upon explicit dependency parse information in the input, and reliably exceed parse-free models.  I strongly vote for rejection, largely on grounds of quality, elaborated below.  Pros: The paper begins with an interesting idea and implementation, and the results support the claim that their architecture may replicate some of the contribution of dependency parse information.Cons: The presentation is unclear on the concept of "constituent" and the motivation of the model. The later iterations of the model become rather complicated and don't seem well-motivated. The qualitative evaluations don't strongly support the claims of the paper.  Quality1. The design of the model and the use of the word "constituent" seems conceptually problematic to me. What do you take the word "constituent" to mean for your motivation and model design? It seems to me that it might be sufficient to call the ConsTrans model a "spatial attention smoothing" model. Why isn't this a sufficient description? What does the concept "constituent" add?This question is relevant because the later model iterations are developed on further syntactic ideas (typed/labeled relations, syntactic "distance"). But if the model doesn't have a necessary syntactic framing, it's not clear these are the correct model improvements to consider.2. The qualitative evaluations are far too light, testing only a small amount of the model performance.  a. Grammar induction: In particular, I would appreciate a far more in-depth evaluation of the inferred constituent structures. How do they compare to gold and silver dependency parses of within-domain sentences? The current evaluation checks for model inferences on just one short span of text (the aspect term). This is probably one of the easiest terms for the model to recognize as a constituent, too, since the aspect terms are known to be constituents and have verbatim copies in the input sentence. The current evaluation is also only performed on Twitter17 --- why?  b. Interpreting learnt relation labels: I found this evaluation extremely confusing, involving an ad-hoc dependency parsing algorithm built upon an a posteriori fact discovered in model analysis (that relation embedding L2 norm indicates inverse syntactic distance). The resulting parse in Figure 5 is almost entirely incorrect and commits many basic mistakes (for example, not linking the determiner "the" with its immediately adjacent noun). The claim about linking adjectives and nouns is not particularly interesting to me, since this is far less ambitious than the motivation of the model --- if it were, the model could have been quite a bit simpler, I think.3. Significance results are given (thanks!) but with a strangely high significance threshold (0.15 at one point and 0.2 at another). This is not a reasonable significance threshold in my view.  ClaritySome minor comments:1. The constituent derivation algorithm is not clear. Eqn 3 suggests that constituent probabilities are a function of token pairs, but Algorithm 1 line 11 suggests that they can be indexed by a single token position. Is something missing from the algorithm presentation?2. Eqn 3 first condition should be i <= j, I think.3. Figure 4 is not a complete sentence. There's no verb. There's also no clear sentiment (aspect-based or otherwise) that we could talk about for this sentence. A different sentence could serve as both a motivating example for constituency and as a more revealing example of the model's syntactic knowledge. I previously reviewed a version of this paper and unfortunately the primary issues with it have not been addressed in the slightest. While some parts have changed,  I will draw on relevant portions of my previous review where appropriate. This paper sets out to investigate the respective "inductive biases" of LSTM and Transformer neural networks, two dominant model familiesthat are frequently employed in applied NLP tasks.They also seek to compare the "inductive biases" of CNNs and MLPs.The air quotes are placed here because all generalization and thus any claim concerning the generalization performance of a modelnecessarily concern (whether explicitly or implicitly) inductive biases.However, we do not typically need to invoke the term "inductive bias"in every single sentence in a paper just to discuss the comparative suitabilityof some models for some tasks and the comparatively poor performance.There are times when it's beneficial not just to talk about comparative performanceof models but to talk rigorously about inductive biases. In many settings we canformally characterize the bias of a hypothesis, e.g. through learning-theoretic complexity measures. However, here the term is used excessively with fuzzy claims made about some models having "stronger" or "weaker" inductive biaseswithout invoking any concrete measure of the expressivity of a hypothesis class.So the flaws with this paper are two-fold. First, I do not believe that the contributionis sufficiently interesting to warrant publication. Second, I do not believe that the current exposition is suitable for publication.Throughout the authors confuse what has actually been showing in prior works for what has been speculatively claimed in prior works. For example, the authorsrefer to the better performance of LSTMs vs Transformers on a set of agreement tasksas an inductive bias for learning syntactic structures. The authors show plots that simply depict performance but describe them as characterizing the bias-variance tradeoff (absent any discussion of variance).The authors have a lengthy discussion of calibration that does not make much senseand parrots incorrect claims from previous papers such as the bizarre claim that label smoothingcalibrates classifiers (it's rather easy to see how label smoothing could lower ECE for an otherwise overfit classifier but how in general it does not calibrate and can even decalibrate classifier. The idea that calibration magically falls out of knowledge distillationor that any of these models is "perfectly calibrated" (a claim they actually make)is bizarre and unacceptable in a proper publication. The knowlegde distillation experiments are interesting but the speculative interpretationsgo far beyond what the actual experiments show (that distilling from a better performing teacher modelgives a better performing student model, regardless of the student's architecture).In short, this paper is not suitable for publication and must be substantially rewritten.The authors need both a more compelling result and a more forceful editor.  The authors train LSTMs for some tasks where they have been shown to perform better,often con SUMMARY This work proposes to use an ensemble of very well-known machine learning models (mainly tree-based methods) to calibrate radio interferometric data from the KAT-7 telescope. This provides a more efficient alternative to the traiditional approach, which is based on the astrophysicists workforce.REASONS FOR SCOREIn my humble opinion, this work does not present a machine learning contribution of interest for the ICLR community. It applies standard and well-known approaches to an specific remote sensing problem.  I would point the authors to different venues, such as the IEEE International Geoscience and Remote Sensing Symposium. Moreover, I would recommend the authors to compare their approach to some others methodologies used in the field. Right now, the experimental section only analyzes the results of the proposed approach, with no baselines. This paper presents a linear-time attention model based on importance sampling and locality sensitive hashing. The idea is to use Bernoulli sampling to approximate the self-attention - which is quadratic in complexity. Honestly, this is a very crowded space and already many models exist (https://arxiv.org/abs/2009.06732). The authors are aware of these works, cite them and yet there is no comparison. This method seems to be rooted in LSH and a very natural question is how does compare to Reformers. There is not even a sparse transformer or local attention baseline in the experiments. This raises questions about whether this paper will even make any impact at all. (comparisons with longformer is done only on speed/memory but not qualitatively). why? I also find other flaws with the model. If sampling is used, this essentially makes the model stochastic (correct me if Im wrong here). but there are undesirable properties of this such as having non-deterministic inference.Another flaw is that method potentially introduces a lot of instability in training. I think the authors could comment a little on this. Transformers are already notoriously difficult to train and I figure that this method would probably make it way harder for practitioners to get the hyperparameters correct. I think playing with the scaling and normalization of the self-attention weights is something non-ideal, and unless the authors can show this is reasonable stable i am not convinced. I also find it difficult to understand the choices of tasks. It seems like GLUE benchmark is used, yet most of the tasks (like SST) are relatively shorter sequences. I think the authors need to explore datasets that showcase the model's ability on longer sequences. Artificially raising sequence len during pretraining is not really sufficient to be convincing that the model is doing something useful for longer sequences (since the masked out tokens really depend on local context). My constructive feedback to the authors to improve the paper is to have reasonable baselines for comparison. The datasets are also not appropriate. I would suggest some actually long-range tasks in order to showcase the model's capabilities. At the rate of the number of new models that tackle this problem, I suspect it would be wise to wrap up your sleeves and add actual efficient transformer baselines instead of procrastinating.  This work investigates how to choose the right training set and hyper-parameter for a test set. The authors claim that they introduce a concept of sample robustness based on the Lipschitz constant of the label map. The authors then empirically evaluate the robustness distribution of two datasets and investigate the model performance when using training subsets from different parts of the robustness distribution.Pros:- The idea of robustness distribution for a dataset seems interesting. It might be used to compare different datasets.- The authors provide extensive experiments using different models. It is nice to see the details for each model and the robust samples for each dataset.Cons:- The novelty of this work is somewhat limited. The notion of sample robustness that the authors propose is just a minor modification of Lipschitz constant. Moreover, there is no theoretical or empirical justification regarding why this definition of sample robustness is useful.- Despite that I really appreciate the efforts that the authors have put on the experimental section, I think that the contribution to the community is somewhat weak. This paper just uses the so called sample robustness to select a subset of the training sample and then reports the performance of model trained with that subset on some test data. But the authors do not provide any insights on how it works. In fact, from the results, it is still not clear how sample robustness can help choose the right training set and improve the model performance. - There are some typos. For example, in the nine line of abstract, be regarded as an intrinsic should be be regarded as being intrinsic; in the related work, when citing multiple work, it is better use the command \citep{work1, work2}. -It is known that Assumption 3 equation (10) (bounded variance) with Assumption 2 (strong convexity) leads to a contradiction. Thus having these two assumptions together is strong.There are some recent works that overcome Assumption 3 by a different assumption known as the expected smoothness like in the following work: Gower,  R.  M.,  Richtarik,  P.,  and  Bach,  F.    Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching.arxiv:1805.02632, 2018.The authors may revise their strongly convex results part using this kind of assumption.-In proposition 1 equation (13), it is not clear what the authors mean by the probability of a random variable. I checked the proof but I did not understand it either. -Some reported results are already known in the literature and the paper gives the impression that these results are new, especially that the authors give the proofs.Examples of such results: the unbiasedness of the quantization, its bounded variance, and the first part of theorem 1...-Page 5, concerning the quadratic example: this is a trivial case and the only case where one can hope the lower bound to match the upper bound. In fact, alpha = beta iff L=mu and from Assumptions 1 & 2 we get that F is quadratic with mu=L which implies H = mu I.- Equation (20): for me this one of the main results of the paper. But I did not see its proof anywhere? 1.The authors considered uniform upper bound of the stochastic gradients g_i. The authors may argue that "The classical theoretical analysis of SGD assumes that the stochastic gradients are uniformly bounded". But one can even strongly argue that this bound is actually $\infty$. Moreover, an even stronger argument can be made that the above assumption is in contrast with strong convexity. Please see ["SGD and Hogwild! Convergence Without the Bounded Gradients Assumption" by Nguyen et al.] as one of the instances. Please understand there are relaxed assumptions such as Strong growth condition on a stochastic gradient as in Assumption 4 of [1]. 2.There is work [2] that proposes we propose a flexible framework which adapts the compression level to the true gradient at each iteration, maximizing the improvement in the objective function that is achieved per communicated bit. How is work different from theirs? I suggest the authors mention this work and make connections with their present results. 3.Please check Theorem 3.4. and 3.5 in QSGD paper. There is a count of bits communicating in each round for QSGD which depends on s, the quantization level. Moreover, you also mentioned that quantization level is roughly exponential to the number of quantized bits. In light of your formulation how the results in QSGD paper are relevant? Additionally, in Section 3, equations (3) and (4) are not your contributions. Therefore, please adequately cite their sources. 4.What is the idea behind Proposition 1? 5.What is the point of Theorem 2? On the other hand, where are the derivations/proofs of Equations (18) and (20)? Those are the main results of this paper if I am not wrong. 6.In terms of experimental results, I had a hard time interpreting Table 1 and associated text in the paragraph Test Accuracy vs Compression Ratio. Especially, I do not know how do I understand the last sentence in the abovementioned paragraph. To do proper experiments by using compression techniques, the authors can check a very elaborative work and codebase by [Hang Xu et. al, Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation]. In that case, I would encourage the authors to plot relative data-volume vs. test accuracy similar to Figures 6 and 7 therein. In the present papers, the experiments and their presentations are substandard. Additional comments:1.Recently, with the booming---Please refrain from using these types of words. You are preparing a scientific document, not a sci-fi novel. 2.I have some reservations on this statement: Existing algorithms often quantize parameters into a fixed number of bits, which is shown to be inefficient in balancing the communication-convergence trade-off (Seide et al., 2014; Alistarh et al., 2017; Bernstein et al., 2018).  In my opinion, QSGD is robust and stable, For 1-bit SGD (both Seide and Bernstein), the proposed solution is to use error-feedback. 3.show good performance in some certain tasks---bad sentence.[1] Dutta et al. AAAI 2020, On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning.[2] Khirirat et al., 2020, A flexible framework for communication-efficient machine learning: from HPC to IoT. The authors proposed a unique learning scheme for representation disentanglement. However, unlike infoGAN or ACGAN which explicitly learn disjoint feature representations for describing the attributes of interest (via unsupervised and supervised settings, respectively), the authors chose to address this task in a questionable "weakly supervised setting". More specifically, the authors chose to train AE-like model using pairwise images, which the difference between each pair of the inputs is only associated with one attribute of interest (e.g., angle, position, etc.). For some reasons, the authors expected such a training scheme would result in learning feature representation in which only "one" feature dimension would reflect such attribute differences. This is a very strong assumption, since it is very likely that more than one feature dimensions would correspond to such changes. Moreover, assuming that precisely one feature dimension would be associated with the attribute of interest by feeding in a pair of images with exactly this attribute change would not be practical either. Most real-world images would be complex and contain multiple attributes. Making this assumption would imply that the training images are not realistic. As for the evaluation, there is no comparison to any baseline or SOTA representation disentanglement methods, I found the quantitative metrics selected by the authors not sufficiently informative or supportive either. Most importantly, the authors claimed that the features trained by VAE allowed improved performances (e.g., Figs 3~5). Since VAE are simply trained in a unsupervised way (even the authors called their setting a weakly supervised setting), I see no evidence why the resulting features would be any different from those derived from standard VAEs, and why improved disentanglement results could be achieved.Based on the above observations and remarks, I feel that the authors would not be able to deliver a work which is technically strong with sufficiently complete evaluation. Therefore, I do not think this paper is above the ICLR standard for acceptance. 1.In the introduction, the author separately pointed out the issues of DUQ and DKL. However, these issues are not convincing as no citations or theoretical proof is provided in this paper. The notations in the intro are also not well-defined. X, x, x* are used without difference, which however should be clearly defined as vectors or matrices.2.The technical contribution is very incremental. The proposed vDUQ is simply applying the inducing point GP in the DUQ to mitigate the uncertainty collapse in DKL. The inducing point variational approximation of the GP predictive distribution referred as inducing point GP is not clear for me. What exactly does inducing point GP refer to? Why the so-called inducing point GP can speed up inference in GP model? What does decouple it from dataset size mean? All these important points are not clarified in the introduction.3.The theoretical contributions are also not well-organized. The author fails to prove that the spectral normalization as a regularization scheme can be uses to mitigate uncertainty collapse. Moreover, how the spectral normalization guarantees the effectiveness of inducing point GP in vDUQ?4.I also have some concerns on the experimental results of causal inference. Why the treatment effect estimation has uncertainty settings. The authors should fully explain the uncertainty settings in causal inference, as most of the causal baselines are not proposed for uncertainty settings. The paper describes a way to reduce the dimensionality of a deep ReLU network. In general, the paper is not well written and hard to follow. They keep referencing "unpublished work of the authros" although its use in practice is not very clear.Practically, they prune a deep network by (a) removing dead ReLU neurons; (b) combine neurons for which the ReLU always acts as the identity. None of these two ideas is particularly novel, especially considering the huge amount of literature to be found on network pruning. Experiments are only done on an artificial dataset and on MNIST.Many parts of the paper are poorly described. For example, their "stack network" is simply a residual network with affine projections on the residual link. A "P-FUNCTIONAL" is not defined. The link between Eq. (4) and their algorithm is not clear. The paper suggests a pruning technique specific to ReLU networks by taking advantage of activation patterns and the separating hyperplanes. The technique consists of three steps: : (1) remove neurons that are never active and combine neurons that are always active, (2) remove neurons with little contribution to the output, and (3) use weighted k-means to combine other neurons. The method is evaluated on specific toy problems and the MNIST dataset.The paper proposes a novel technique to reduce neurons in a ReLU network. The method to combine neurons takes the hyperplane arrangements (where activations of neurons change) into account and leads to much smaller networks with equal performance in the experiments.The three major problems of the paper are that it lacks motivation of the proposed technique, it contains an insufficient experimental evaluation and important parts of the paper cannot be reviewed either due to a reference to unspecified, unpublished work or a lack of a derivation.Since the correctness of the paper cannot be evaluated and the technique is insufficiently validated, I recommend to reject. Details on weaknesses:- The correctness of the proposed technique cannot be reviewed. (a) Section 3 cannot be confirmed as it refers to results from unspecified unpublished work, i.e., it is impossible to find and read through the unpublished work to estimate its validity. Moreover it is unclear why these results are important for the given paper and how the results are used. The paper states that one should only learn from this entire section that a finite(!) number of neurons is sufficient (which we always have in a practical setting so the conclusion is void?) In any way, either this section is not necessary for the rest of the paper and should be removed, or it is necessary in which case it cannot be verified.(b) The proposed pruning technique consists of three steps (see above in the summary) two of which are trivial: (1)&(2). The third step (3) uses weighted k-means to combine other neurons. There is no explanation, motivation or derivation of the equations how the clusters are combined. (The workings of the method are also surprising, because a cluster containing a single neuron is reduced to a new single neuron in such a way that the function changes, which is counter-intuitive. It seems therefore likely that the equations contain typos, also since they introduce square roots of square roots and it is not defined what is meant by squaring a vector in the function g.)Therefore, the validity cannot be confirmed and the reader must trust the experimental result section. This is unfortunate as there are opportunities to shorten less relevant parts in favor of a derivaiton of the equations.- The experiments are not sufficient. The experiments only consider specific toy problems and the MNIST dataset, which is too simple to showcase the pruning technique. The method is neither compared to any other pruning technique and it is fairly simple to prune networks on MNIST with a similar loss of accuracy. Finally, the method introduces two hyperparameters which are not tested. To validate the performance, both a more complex dataset and the comparison to other pruning techniques are necessary.- The presentation needs improvement. For example, there is a rather long explanation of a seemingly simple architecture and still details are left unclear (Is there a linear layer with weights that are trained, or is the linear skip connection only introduced when pruning the network?) It would be helpful to add an equation for the stack layer and reduce the explanations. The possibly redundant Section 3 could be removed. Instead, the proposed method could be derived and explained. The experiments could be explained in more detail (Why do the plots show the smoothened second derivative?)Typos:Line 4 Motivation authrosPage 3, footnote, bottleneacks has diomensionPage 4 toward the end of Section 3: the thePage 5: the first sentence is not a sentence The paper focuses on defining a new architecture that allows being reduced without significantly affecting the performance. In short, the paper is not properly written nor well organized; is hard to read with vague contributions and vague positioning with respect to the state of the art. Experiments are not convincing: Toy experiment and minimum experiments in MNIST without a clear comparison to existing neuron pruning algorithms.   # SummaryThis paper investigates the incorporation auditory events into reinforcement learning. Specifically, it proposes a new algorithm that uses event prediction as an intrinsic reward. This algorithm has two phases. In the first phase, an agent is given a small number of episodes to gather diverse auditory data. This phase has two pieces that work in conjunction: 1) As it learns, the agent clusters the sound embeddings it has observed using K-means. 2) To encourage the agent to find diverse auditory data, it is rewarded for reaching states that emit sounds that are far away from its existing cluster centers. In the second  phase (which dominates the first in terms of number of episodes consumed), a new agent is trained to predict the cluster center to which the next auditory event will belong. It is rewarded according to how incorrect its prediction is, thereby encouraging the agent to explore states for which it has difficulty predicting the auditory event.The paper performs experiments in Atari, Habitat and TDW. It compares against strong vision-based intrinsic reward baselines. It also performs experiments comparing its proposed methodology to ablations with the aim of determining 1) whether it suffices to predict sound features instead of auditory events, 2) whether a two-stage exploration strategy is necessary, 3) whether it is necessary to perform active exploration in the first phase, and 4) whether event classification is necessary.# WritingThis submission does not read as that of a paper ready for publication. Its organization, unnecessary use of the passive voice, singular-plural inconsistencies, tense mixing, and muddled descriptions weaken its value. Below is a (non-exhaustive!) list of issues.- AbstractWe first conduct an in-depth There needs to be a transition from the method description for this first to fit here.- IntroductionWhy is Deep Reinforcement Learning all caps?algorithms aim to learn a policy of an agent to maximize its cumulative rewards by interacting with environmentssingular/pluraldomains, such as video gameGame should be pluralWhile these results are remarkable, one of the critical constraints is the prerequisite of carefully engineered dense reward signals, which are not always accessible.Is Go a good example of this? AlphaZero accomplishes the same task without carefully engineered dense reward signals.For example, curiosity-driven intrinsic reward based on prediction error of current (Burda et al., 2018b) or future state (Pathak et al., 2017) on the latent feature spaces have shown promising results.This sentence is ordered awkwardly. As written, have refers to curiosity-driven intrinsic reward, which is singular. on the latent feature spaces doesnt work here.visual state is high-dimensional -> states arespeech or other nonverbal but audible signals -> speech or other audible signals OR speech or nonverbal, audible signalsUsing other here makes it read as if speech is a member of nonverbal but audible signalsHowever, it is just as much in physics.(?A naive strategy would be -> A naive strategy isIn the beginningDoes this mean in the first phase?The state that has the wrong prediction is rewarded and encouraged to be visited more.Passive voice makes this hard to parse. The agent is the one making predictions and receiving rewards.understand our audio-driven exploration works well under what circumstancesSome words are missing herecan encourage interest action that involved physical interactionNeeds fixing- Related WorkBy leveraging audio-visual correspondences in videos, it can help to learn powerful audio and visual representations through self-supervised learningIt can help to learn? -> It can learn?Reinforcement Learning (RL)Acronym has already been introduced.makes use of the bootstraps for deep explorationbootstraps -> bootstrapHere, we mainly focus on the problem of using intrinsic rewards to drive explorations.Why is exploration plural?The most widely used intrinsic motivation could be roughly divided into two families. could -> canThe works discussed in the ensuing sentences are already listed and cited in the previous sentences. If you are going to discuss as if you had not already just mentioned them, it is better to exclude them from the previous sentences.Also, maybe you should add approaches or methods somewhere in this sentence.Burda et al. (2018b) employs the prediction errors of a self-state feature extracted from a fixed and random initialized network and encourage the agent to visit more previous unseen states. previous -> previouslyWhat is a self-state feature? Burda does not use that term and there is no explanation here.The end of the sentence doesnt make sense. What is the subject of encourage?Another one is the curiosity-based approach (Stadie et al., 2015; Pathak et al., 2017; Haber et al., 2018; Burda et al., 2018a), which is formulated as the uncertainty in predicting the consequences of the agents actions.-The paper previously said that there were two families so another one should be The other or something else signifying that this is referring back to the two families comment.-The paper described family one as a set of approaches (plural) but family two as an approach singular.-The end of the sentence needs fixing.The agent is then encouraged to improve its knowledge about the environment dynamics then -> therebyFigure 1 would be more clear if it did not use time indexing for the stage 1 section.There are numerous works to explorethat explore?More recently, Dhiraj et al. (2020) collected a large sound-action-vision dataset using Tilt-bolt and demonstrates sound signals could provide valuable information for find-grained object recognition-Dhiraj et al. is plural so demonstrates should be demonstrate.-could provide -> can provide-find-grained -> fine-grainedMore related to us us -> our work OR this workwhich have shown that the sound signals could provide useful supervisions for imitation learning and reinforcement learning in Atari games. -could -> can-unnecessary passive voiceThroughout the Sounds and Actions paragraph, the paper repeatedly switches between describing papers in past tense and present tense.And then we elaborate on the pipeline of self-supervised exploration through auditory event predictions. And then -> ThenThere is nothing wrong (in general) with starting a sentence with the word And but this is not an appropriate place for it.a standard Markov Decision Process (MDP), defined as (S, A, r, T , µ, ³). S, A and µ(s) : S  [0, 1] denoteas -> byIt is bad form to start a sentence with a symbol.The transition function T (s 0 |s, a) : S × A × S  [0, 1] defines the transition probability to next-step state s if the agent takes action a at current state s.The transition function defines this transition probability whether or not the agent executes a at s.The goal of training reinforcement learning is to learn an optimal policy À  that can maximize the expected rewards under the discount factor ³ asThis is the goal of reinforcement learning not the goal of training reinforcement learning.The sentence doesnt work. It reads The goal is to learn an optimal policy that is an optimal policy. Either modify to The goal is to learn an optimal that is an optimal policy. or The goal is to learn an optimal policy. An optimal policy is & The agent chooses an action a from a policy À(a|s) from -> according toIntrinsic Rewards for Exploration(This paragraph is repeating information that was already writtenDesigning intrinsic rewards for exploration has been widely used to resolve the sparse reward issues in the deep RL communities-Unnecessary passive voice-What deep RL communities? Isnt there just one? If there are more than one, what are they?transits to the next state with visual observation sv,t+1 and sound effect ss,t+1. transits -> transitionsstate with -> state, receivingWe hypothesize that the agents, through this process, could learn the underlying causal structure of the physical world and use that to make predictions about what will happen next, and as well as plan actions to achieve their goals.-and as well as is redundantTo better capture the statistic of the raw auditory dataWhat statistic?For the task of auditory event predictions, perhaps the most straightforward option is to directly regress the sound features ¦(ss,t+1) given the feature embeddings of the image observation sv,t and agents actions atThe paper said this alreadyNevertheless, we find that not very effective. We hypothesize that the main reasons are: 1) the mean squared error (MSE) loss used for regression is satisfied with blurry predictions. This might not capture the full distribution over possible sounds and a categorical distribution over clusters; 2) the MSE loss does not accurately reflect how well an agent understands these auditory events. Therefore, we choose instead to define explicit auditory events categories and formulate this auditory event prediction problem as a classification task, similar to (Owens et al., 2016b).-Nevertheless is not appropriate here.-The paper runs this experiment and discusses the results later in the paper. Its confusing to discuss it both as a choice and as an ablation.Our AEP frameworkWhile the acronym can be deduced from the section header, it is still good practice to write it explicitly.We need toThis is a design choice, not a need.And thenAnd is not appropriate here.takes input as the embedding of visual observation and action and predicts which auditory event will happen next.needs fixingthen utilized -> usedto explore those auditory events with more uncertaintythose is unnecessary hereWe will elaborate on the details of these two phases below.Get rid of willThe agents start to collect audio data by interacting with the environmentThey start to isnt good phrasing here. During this exploration, the number of clusters will growfix tenseAfter the number of the clusters is saturatedThe description of the sound clustering process is muddled. It does not defined saturated until after it has used it in context.To be noted, the number of cluster K is determined automatically in our experiments. In practice, we define K  [5, 30] for clustering at each time step and use the silhouette score to automatically decide the best K, which is a measure of how similar a sound embedding to its own cluster compared to other clusters.Do To be noted or In practice have any added value here?we believe it is saturatedThe paper is defining saturation, it is not a belief.We visualize the corresponding visual states in two games (Frostbite and Assault) that belong to the same sound clusters, and it can be observed that each cluster always contains identical or similar auditory events-Unnecessary passive voice-Always is a pretty strong claim. Can we deduce that from this visualization?Since we have already explicitly defined the auditory event categories, the prediction problem can then be easily formulated as a classification task-Dont need already-Dont need thenIt will reward the agent at that stage and encourage it to visit more such states since it is uncertain about this scenario-Use present tense tense-at that state?In practice, we do find that the agent can learn to avoid dying scenarios in the games since that gives a similar sound effect it has already encountered many times and can predict very well.-we do find -> we find-dying scenarios?- ExperimentsAnd thenAnd is not appropriate hereOur primary goal is to investigate whether we could use auditory event prediction as intrinsic rewards to help RL exploration.This statement is repeated many times in the paper and has no added value to this section.also supports an audio API to provideto provide -> that providesWe use 20 familiar video gamesIs familiar needed here?We follow the standard setting in (Pathak et al., 2017; Burda et al., 2018b), where an agent can use external rewards as an evaluation metric to quantify the performance of the exploration strategy. This is because doing well on extrinsic rewards usually requires having explored thoroughly.-Passive voice is excessive in multiple places here-incorrect use of interrogativesthe the typoTable 1: Categorical resultsThese are not resultscould simulatecan simulatephysic simulation platform physicsFigure 7 ,comma misplacedThe agent is required to execute actions to interact with objects of different materials and shapes.The agent interacts with objects of different materials and shapes.When two objects collide, the environment could generate collision sound based on the physical properties of objectscould generate collision sound?We would like to compareWe comparewe choose PPO algorithmeither choose the PPO algorithm or choose PPOThe PyTorch implementationwhich?the open-source toolbox 1-which?-footnote shouldnt have spaceAs for the auditory prediction networkDont need asour model use 10K interactions fixprevious vision-only intrinsic motivation modulesdont need previous hereWe would like to provide an in-depth understanding of under what circumstances our algorithm works well.You would like to or you do?event-driven sounds which emitted when agents fixaction-driven sounds which emitted when agents fixNone of the category accounts for the majority fixSince the sound is more observable effects of action fixgames dominant with event-driven fixThese are also reasonableNothing for These to refer toSometimes sound events will occur independently of the agents decisions and do not differentiate between different policiesdifferentiate is not the right word hereThe phrase ablated study is used repeatedly. Use ablation study instead?We further carry out additional experimentsfurther unnecessaryOne main contribution of our paper is to use auditory event prediction as an intrinsic reward.Repeated without added valueIn Figure 6, using only 16K exploration steps, our agent has already explored all unique states (211 states) fixThese results demonstrate that our agent explores environments more quickly and fully, showing the potential ability of exploring the real world.Do they? This is a strong claimless than 195 unique states fewerThese results demonstrate that our agent explores environments more quickly and fully, showing the potential ability of exploring the real world.Do they? Again, a strong claim.There is a sub-header Setup of Section 4.4 Experiments on TDW. There is also a TDW sub-header of Section 4.1 Setup.The action space consists of moving to eight directions and stop. An action is repeated 4 times on each frame.fixprevious vision-based modulesdont need previouscould not -> did notthe 3D photo-realistic world, in which a physical event happens.needs fixInstead, our auditory event prediction driven exploration will lead agentsIn constrast, & exploration leads agents(See SHE in Figure 8) seeWe will reward an agent We reward an agentWe also want to understand if it is necessary to use audios audios?is powerful for agents to build a causal model of the physical world fix- Comments on Organization-I think it would make more sense to group the question-based analysis together. IE, the ablation studies and the discussions.-I think whether to use clustering classification or feature prediction should be discussed consistently throughout the paper. As it currently stands, sometimes it is presented as a design choice, sometimes with the claim that the latter is worse (without supporting evidence), and sometimes as a question to be answered by an ablation study.- Other commentsAs compared to visual cues, sounds are often more directly or easily observable causal effects of actions and interactions.Is this obvious?# Causality?The paper makes a number of comments about its method learning causal structure. To me, these seem like big claims. The proposed algorithm has no mechanism that tests counterfactuals or, as far as I can tell, any other mechanism for estimating causation, so I see no reason why it would learn anything beyond correlative relationships. Given this fact, in my opinion, if the paper wants to make claims about the fact that its method is learning causal structure, it should back these claims up with experiments.# Experimental EvaluationThe paper makes very definitive claims (see writing section) about the effectiveness of its method compared to the baselines. In my opinion, there are a number of issues with these claims. First, the paper gives no information (that I could find) about how it tuned the baselines or whether they tuned them at all. Second, for the Atari results, the paper states that it used three random seeds. For the other experiments, it does not say how many seeds it used (or at least I could not find where it said so). Both of these facts make it difficult to know how seriously to take the claims of superior performance.# Related workThe paper cites many references in its related work section. Yet, I feel that it tells us almost nothing about what it is most important for us to know about:"More related to us are the papers from Aytar et al. (2018) and Omidshafiei et al. (2018), which have shown that the sound signals could provide useful supervisions for imitation learning and reinforcement learning in Atari games. Concurrent to our work, Dean et al. (2020) uses novel associations of audio and visual signals as intrinsic rewards to guide RL exploration. Different from them, we use auditory event predictions as intrinsic rewards to drive RL explorations."We are only given a couple of sentences of information about these papers. Additionally, I am not sure that is consistent that the paper claims that Dean is concurrent, but at the same time, designed its experiments to follow Dean Following (Dean et al., 2020), we use the the apartment 0 in Replica scene (Straub et al., 2019) with the Habitat simulator for experiments.# Choice of baselinesThe paper appears to be concerned with claiming superior performance over vision-based intrinsic methods, yet it is not clear to me that having superior performance is necessary for the proposed method to have added value, given that the two methods make use of disjointed information for prediction targets. I do not mean to say that these experiments do not have added valuecertainly it is nice to see the proposed algorithm compared to existing algorithmsbut maybe the adversarial narrative is not the right choice? I understand that the paper is claiming that sounds give the agent better information for intrinsic exploration but, in my opinion, convincing evidence for that claim would require extensive experiments on a wide array of intrinsic methods using both sound and vision and many environments.# Broader Scope QuestionCan a paradigm requiring an initial exploration stage to collect diverse sounds be effective in environments in which some sound events require a large amount learning to discover?# Closing ThoughtsThis seems like promising work, but in my opinion it is not ready for publication. Both the quality of writing and the issues with seeds/tuning independently merit rejection. There are also other issues discussed above. his paper present WaveMesh, a wavelet-based superpixeling algorithm and WavePool, a spatially heterogeneous pooling scheme.Strengths:"A wavelet transform image coding technique with a quadtree structure" (Banham and Sullivan 1992) and "Geometric methods for wavelet-based image compression" (Wakin et al. 2003) indicate that a wavelet-coefficient based quadtree decomposition of images is an efficient image-compression technique. This suggests that the goal of this paper is reasonable and the method can likely be motivated.Weakness:The authors provide little theoretical justification for the use of wavelets as they are used in this paper.Moreover, the pooling method in this paper appears to be the same as that in "Image Segmentation Based on Multiscale Fast Spectral Clustering". In addition, the pooling method seems to be applicable to any quadtree-based method. A superpixel quadtree can be defined in many ways, and quadtrees are the basis of other superpixel methods like the "A Hierarchical Data Structure for Picture Processing" (Tanimoto and Pavlidis 1975), analyzed in "Evaluation Framework of Superpixel Methods with a Global Regularity Measure."There are two primary weaknesses: First, SLIC is a poor baseline and many other superpixel methods have been developed since 2010 - "Superpixel Segmentation using Linear Spectral Clustering" or "Robust superpixels using color and contour features along linear path" or even the recent "Simple and fast image superpixels generation with color and boundary probability".Second, almost all of the gains of this method appear to come from the pooling method: In Tables 2 and 3, WaveMesh alone consistently underperforms SLIC.Third, these results appear to depend on the classification model. While the variance is low, alternative hyperparameters for the SplineCNN (or a different model) may have resulted in different performance. Given that this paper did not use any of the existing superpixel evaluation metrics, it is difficult to compare it to prior work. While of course the two papers cannot be compared directly as they are both currently under ICLR review, "Probabilistic Numeric Convolutional Neural Networks," has similar performance on the 75 Superpixel MNIST dataset, using a third as many superpixels.Due to these weaknesses, this paper has neither the experimental analysis nor the theoretical grounding to be appropriate for ICLR at this time. The authors attempt to propose an alternative explanation for the effect of dropout in a neural network and then present a technique to improve existing activation functions.Section 3.1 presents a experimental proof of higher co-adaptation in presence of dropout, in my opinion this is an incorrect experiment and request authors to double check. In my experience, using dropout results in sparse representations in the hidden layers which is the effect decreased co-adaptions. Also, a single experiment with MNIST data-set cannot be a proof to reject a theory.Section 3.2 Table 2 presents a comparison between average gradient flow through layers during training where flow with dropout is higher. This is not very surprising, in my opinion, given the variance of the activation of a neuron in presence of dropout the network tries to optimize the classification cost while trying to reduce the variance. The experimental details are almost nil.The experiments section 5 presents very weak results. Very little or no improvement and authors randomly introduce BatchNorm into one of the experiment. This paper studies sufficient dimension reduction problem, and proposes an incremental sliced inverse regression algorithm. Numerical experiments are provided to demonstrate the effectiveness of the proposed algorithms.The sliced inverse regression here is nothing but generalized eigenvalue decomposition:Ax=lambda Bx.Note that Multiclass Fisher Linear Discriminant Analysis, Canonical Correlation Analysis, Nonlinear Manifold Embedding and many subspace learning methods can also be formulated as generalized eigenvalue decomposition. All these methods need to compute covariance-like matrices in the additive form, which makes incremental update very convenient.The incremental generalized eigenvalue decomposition has been extensively studied for over decades, especially between 1995 and 2005 in the face recognition community. I am just listing a few here:Ye et al., IDR/QR: An Incremental Dimension Reduction Algorithm via QR Decomposition, 2005Law and Jain, Incremental Nonlinear Dimensionality Reduction by Manifold Learning, 2006Yan et al. Towards incremental and large scale face recognition, 2011Ghassabeh et al. A New Incremental Face Recognition System, 2007Song et al. A Novel Supervised Dimensionality Reduction Algorithm for Online Image Recognition, 2006.Wang et al. Incremental two-dimensional linear discriminant analysis with applications to face recognition, 2010.Salman et al. Efficient update of the covariance matrix inverse in iterated linear discriminant analysis, 2010Park and Park, A comparison of generalized linear discriminant analysis algorithms, 2008Wang, INCREMENTAL AND REGULARIZED LINEAR DISCRIMINANT ANALYSIS, 2012These algorithms become less popular/known now, because (1) they are not scalable and efficient for large p, and (2) these classical dimensionality reduction methods perform poorly in many tasks, compared with the state of art results.This paper only cites a few papers on incremental LDA,  but does not even mention that both LDA and SIR are essentially solving similar optimization problems. Moreover, it does not compare the results with any of the above references, either.This paper even claims applying the ShermanMorrison formula as the contribution. However, such an  update has been used in Salman et al. 2010, Park and Park 2008, Wang 2012.In summary, this paper is far below the bar of ICLR.Minor: There are numerous typos in this paper. The authors even misspell "Morrison" in the ShermanMorrison formula as "Morison". The paper proposes performing Thompson Sampling (TS) using a Bayesian Linear Regressor (BLR) as the action-value function the inputs of which are parameterized as a deterministic neural net. The authors provide a regret bound for the BLR part of their method and provide a comparison against Double Deep Q-Learning (DDQL) on a series of computer games.Strengths:  * The paper presents some strong experimental results.Weaknesses:  * The novelty of the method falls a little short for a full-scale conference paper. After all, it is only a special case of [3] where the random weights are restricted to the top-most layer and the posterior is naturally calculated in closed form. Note that [3] also reports a proof-of-concept experiment on a Thompson Sampling setting.  * Related to the point above, the paper should have definitely provided a comparison against [3]. It is hard to conclude much from the fact that the proposed method outperforms DDQN, which is by design not meant for sample efficiency and effective exploration. A DDQN with Dropout applied on multiple layers and Thompson Sampling followed as the policy would indeed be both a trivial design and a competitive baseline. Now the authors can argue what they provide on top of this design and how impactful it is.  * If the main concern is sample efficiency, another highly relevant vein of research is model-based reinforcement learning. The paper should have provided a clear differentiation from the state of the art in this field as well.  * Key citations to very closely related prior work are missing, for instance [1,2].  * I have hard time to buy the disclaimers provided for Table 2. What is wrong with reporting results on the evaluation phase? Is that not what actually counts?   * The appendix includes some material, such as critical experimental results, that are prerequisite for a reviewer to make a decision about its faith. To my take, this is not the Appendices are meant for. As the reviewers do not have to read the Appendices at all, all material required for a decision has to be in the body text. Therefore I deem all such essential material as invalid and make my decision without investigating them.Minor:  * The paper has excessively many typos and misspellings. This both gives negative signals about its level of maturity and calls for a detailed proofread.[1] R. Dearden et al., Bayesian Q-learning, AAAI, 1998[2] N. Tziortziotis et al., Linear Bayesian Reinforcement Learning, IJCAI, 2013[3] Y. Gal, Z. Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ICML, 2016 The stated contribution of the paper is the development of a model to learn continuous representations of k-mers from RNA sequencing experiments in an annotation-free manner. The paper motivates this model by considering analysis challenges faced in cancer genomics. This introduction serves well to frame the paper towards addressing these challenges. In particular, the authors highlight challenges faced in recognizing and quantifying patient/tumor-specific RNASeq based expression estimates involving structural variants and indels, which have and continue to be a challenge for existing tools. Despite this, we are not enthusiastic about the paper for the following reasons:Narrow and incomplete view of commonly used modern RNA-seq tools/pipelines and their application/use in biomedical research.The proposed computational method is computationally intractable and is unlikely to ever scale to the genome-wide context.Described experiments are without context to the existing literature of tools designed to address the biological challenge and by construction are not annotation free.We further describe these reasons in the following subsections. Overall, we do not believe that the described model/experiments demonstrate utility for either the specific problems in cancer genomics that motivate the paper or the biomedical research field in general.Narrow and incomplete view of RNA-seq tools/pipelines/application:------------------Reducing this rich data to only the detection of annotated genes [...] is not appropriate for analysis. Modern RNA-seq pipelines also perform quantification and differential analysis at a minimum.The description of the standard RNA-seq experiment is problematic:Sequencing is of cDNA after reverse transcription, not RNA.Ignores paired end reads (especially with longer insert sizes for fusion detection)Shredder poor analogy given multiple distinct reads from same sequence, known biases in process[...] multiple sequence alignment is an NP-hard problem. This is true but irrelevant.Chromosomal translocations are indeed hard to detect by RNA-seq, but not impossible. There are strategies implemented in commonly used tools such as STAR, kallisto, and others to detect these and other structural variants. Individual reads do not have to themselves cover the sequence where translocation occurs, instead read pairs can imply that the insert contains a translocation -- in this case, sequence similarity is much higher. Regardless, cheaper orthogonal assays exist that can detect these events.The standard RNA-Seq analysis pipeline has a mean processing times of 28 core hours for mapping with software TopHat, followed by an additional 14 hours of quantification. See Please stop using TopHat (https://twitter.com/lpachter/status/937055346987712512?lang=en) by one of the authors of TopHat and the senior author of the cited paper. Standard pipelines use newer aligners like STAR which are substantially faster.Reference based methods yield a relative abundance measurement of genes, which are by definition, hand crafted features. Relating results back to genes is important to be able to connect sequencing results back to known biology. We see the fact that there is no obvious map from the proposed method back to genetic information as a weakness.Proposed computational method computationally intractable and unlikely to scale to genome-wide context------------------------Plus:Paper does acknowledge that scalability is a limitation.Minus:Lower bound of range of unique kmers per sample without pre-filtering is 10 billion; note that storing counts uncompressed as 32-bit integers corresponds to over 37GB per sample.Experiments are for only four genes, two at a time with a 2-dimensional embedding. Unclear how patterns will hold when considering k-mers from more genes simultaneously or how embedding could scale. Model formulation suggests that k-mers from co-expressed genes will have similar embeddings, which could complicate visual inspection.Abstract states that learned representation both useful for visualization as well as analysis. Unclear what is done with model/embeddings besides visualization -- non-visual analyses are performed with k-mer counts before embedding. Identification of abnormalities are described only by visual inspection, which is unlikely to scale as more k-mers are added and/or if the dimensionality of the embedding increases.Experiments without context to existing literature and are not annotation free----------------------------------------The paper describes that existing methods are limited by their dependence on annotations. The paper does not describe existing methods using annotations designed to address tasks/applications suggested for the new model. The paper does not compare developed model to these existing methods. To make the experiments performed in the paper computationally tractable, RNA-seq reads are aligned to the reference genome (annotations of sequence) and sequences in specific regions, defined with respect to genes of interest (annotations of genes). The experiments are therefore dependent on annotation although they lose their information/interpretability in this context. The paper notes that many kmers are not included in exonic sequences according to exact matches to annotations of coding sequence. Nonetheless, these reads are in this dataset, which means they are also identified by standard tools/annotations despite their differences. In Figure 3, embedding overlap between kmers in the annotated coding sequence of ZFX and ZFY are illustrated. It would be helpful to show the proportions of reads that were used for this analysis that mapped uniquely vs not to genomic intervals in these genes (reads rather than kmers). In this regard, the absence of any note or methods describing how reads were mapped (what method, with which parameters) to reference (which reference genome, which gene annotations) is a serious limitation.Other notes---------------------The heatmap in Figure 5A is ambiguous and could be improved by better annotating what is on rows/columns, perhaps also including numeric information textually in addition to by color.Identification of kmers spanning translocation region (illustrated in embedding space by 5B) was done entirely without kmer2vec (identifying exclusive kmers, assembly of kmers, BLAST alignment to two chromosomes). Thus, claim that, as a consequence, kmer2vec captures real genomic abnormalities and allows to extract directly from the kmer embedding space the abnormal sequence is unsubstantiated. This paper study the model-based approach in deterministic low dimensional continuous control. As far as I am concerned and I understood, the main contribution of this paper is in substituting one-step-ahead prediction model with a multiple-step prediction model, resulting in a more accurate prediction model. I was not able to find points beyond this. I would be happy if the authors could clarify it. The paper studies fatigue monitoring of EEG driving simulator experiments using various EEG analysis algorithms, one also based on ranking. The data used was from a prior experiment. The paper is written in a rather confusing manner, which makes the assessment of originality and significance a hard task for the reviewer. A novel algorithm Bdrank (based on raking is defined) and compared to 2 other algorithms; unclear why with these and not with others. The paper ignores a large portion of the literature, starting with Kohlmorgen et al 2007, Blankertz group, Lee group etc. The results  are only somewhat interesting, no understanding of the underlying physiological processes is given. Overall, I consider the paper somewhat preliminary. This paper is poorly written, and looks like it was not proof-read. Presentation of the problem at hand is presented over so many times that it becomes confusing.Authors ought to better define the image description space of the objects and the haptic space. More interesting would have been a good explanation of the different sensors used in the anthropomorphic hand  and the vector built to represent the different sensed objects.The most expected contribution of this work is barely explained: how the haptic sensors' values/object labels vectors were built and fed to the predictor network, what their values looked like for the various objects, how these vectors clustered for the various objects etc.Among the many evident weaknesses:- Domain specific concepts and procedures of most importance to this work are not explained: "... measure various physical properties of objects using the bio-tac sensor using five different exploration procedures (EP)".  Page 3, Paragraph 1. Bio-tac sensor and most importantly exploration procedures (EP) should be presented more clearly.- Incomplete and out of nowhere sentences are common: "The SHAP procedurewas established for evaluating prosthetic hands and arms. With this idea in mind, prior work (?)built a prosthetic arm which could ..." Page 4, Paragraph 1.- Many references are not well introduced and justified: "We then trained the network usingADAM (Kingma &amp; Ba (2014)) with an initial learning rate set to 1e-4." Page 4, Paragraph 6. In the same paragraph,  authors explain using "The groundtruth predictions were per-channel averaged haptic forces" without having defined those channels (that one can guess but shouldn't). Concepts have to be clearly defined prior to their use. # Unrealistic assumptions and trivial theoryThis papers proposes a method to adjust the learning rate of stochastic gradient methods. The problem is of great importance but the theoretical results and presentation contain many issues that make the paper unfit for publication.The main issue that I see is that the assumption made are unrealistic and make the theory trivial. First, for gradient descent, the authors assume that the gradient is of the form L(x_t) (x_t - x*). Under this assumption, gradient descent converges on a single step with step size 1 / L(x_t). In the stochastic setting, they assume that *each* stochastic gradient is of the form L_i(x_t) (x_t - x*), Eq. (11). Again, SGD in this scenario converges in a single iteration with step size 1 / L_i(x_t).No wonder in this scenario the authors are able to obtain linear convergence of SGD to arbitrary precision (which is known to be impossible even for quadratics).# Other Issues* Motivation of Eq. (9) is not discussed in sufficient detail. It is unclear to me how to obtain (9) from (7) as the authors mention. Regarding notation, L(x_t) is a scalar, hence (9) could be written more simply as \nabla f(x_t) = L(x_t) (x_t - x*). Why the need for the Kronecker product?* The authors should clearly state what are the assumptions in the theorem statement. For theorem 1 these are not clearly stated, and phrases like "Theorem 1 provides a simple condition for the linear convergence of SGD" give the wrong impression that the Theorem is widely applicable.# Minor  * Belo Eq. (10): "where \epsilon_1 is a parameter to prevent ||x_t - x_{t-1}|| going to zero: . I guess what the authors meant is to prevent *the denominator* going to zero, you do want ||x_t - x_{t-1}|| to go to zero as you approach a stationary point This paper considers the finite-sum optimization problem that is typically seen in machine learning, and proposes methods that adaptively adjust the learning rate by estimating the local Lipschitz constant of the gradient. The contributions of the paper seem very limited.  The proposed method which estimates the local Lipschitz constant of the gradient, named local predictive local smoothness (PLS) method in the paper (equation (10)), has been proposed in [1] long ago (see equation (11) in [1]) and is very well-known to the community. It is quite surprising that the authors claim to be the first to propose this while completely ignoring previous works.I also believe that there are major issues with the analysis for the methods. For example, I do not understand how equation (9) could possibly hold for general functions, and how it could be possible to transform their method into the linear system in (11). Therefore I do not think this paper is technically correct. In summary, I believe this paper is limited in its contribution and also has major issues in terms of technical correctness, and is well below the standard for ICLR. Reference: [1] Magoulas, G. D., Vrahatis, M. N., &amp; Androulakis, G. S. (1997). Effective backpropagation training with variable stepsize. Neural networks, 10(1), 69-82. The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models.This is an approach that has been tried[1,2] (even with the addition of entropy regularization) and studied [1-5] extensively. It doesn't scale, and for extremely well understood reasons[2,3]. The bias of the empirical Wasserstein estimate requires an exponential number of samples as the number of dimensions increases to reach a certain amount of error [2-6]. Indeed, it requires an exponential number of samples to even differentiate between two batches of the same Gaussian[4]. On top of these arguments, the results do not suggest any new finding or that these theoretical limitations would not be relevant in practice. If the authors have results and design choices making this method work in a high dimensional problem such as LSUN, I will revise my review.[1]: https://arxiv.org/abs/1706.00292[2]: https://arxiv.org/abs/1708.02511[3]: https://arxiv.org/abs/1712.07822[4]: https://arxiv.org/abs/1703.00573[5]: http://www.gatsby.ucl.ac.uk/~gretton/papers/SriFukGreSchetal12.pdf[6]: https://www.sciencedirect.com/science/article/pii/0377042794900337 The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al. in Causalgan: Learning causal implicit generative models with adversarial training, ICLR'18. Same as here, the authors there also "view image generation as a causal process" and "structure a generator network as a structural causal model (SCM)" and use a conditional gan to generate the image from the labels. The generation used here based on three variables, i.e., shape, texture and background seem to be a special case. Therefore, the authors should definitely cite this work. My general remark is that there is very little causality in the approach. The causal structure that is used in the generation of data is not different than a conditional GAN. This makes the claims in the introduction very disconnected from the actual methodology and the experiments in my opinion." we can intervene on a subset of them and generate counterfactual images "-> What the authors call counterfactual images are actually interventional images from a causal point of view. Please consider changing "counterfactual" to "interventional" throughout the paper. This will help clarify the distinction between interventional and counterfactual layers in Pearl's hierarchy. "From a causal perspective, we maximize the average causal effect (ACE) of one IM on the classifiers decision, while minimizing the ACE of all other IMs."Can you formalize this claim? This does not seem trivial.Could you explain "alpha blending"? This step is not motivated well and seems specific to the used dataset. Even though the ImageNet experiments look impressive, I believe the main factor for success is in the deterministic and manually defined composition mechanism. Furthermore, I believe this composition is doing most of the disentangling during training. Foreground and background segmentation use an existing method U2-Net which is used to create masks, or values for the variables used in the graph. The intuition on comparing with other methods is missing. For example, why do you think training a classifier on interventionally augmented data performs better than IRM? Shouldn't this depend on the number of environments and degree of correlation? These are not reported. "We, therefore, follow an augmentation strategy, i.e., we augment ImageNet with additional counterfactual images."How many samples are added to the original data? I believe the amount of augmentation relative to the original dataset size is important. This paper presents an instruction-following model consisting of two modules: agoal-prediction model that maps commands to goal representations, and anexecution model that maps goal representations to policies. The second module istrained without command supervision via a goal exploration process, while thefirst module is trained supervisedly in a metric learning framework.This paper contains an important core insight---much of what's hard aboutinstruction following is generic planning behavior that doesn't depend on thesemantics of instructions, and pre-learning this behavior makes it possible touse natural language supervision more effectively. However, the paper alsocontains a number of serious evaluation and presentation issues. It is obviouslynot ready to publish (uncaptioned figures, paragraphs interrupted mid-sentence,etc.) and should not have been submitted to ICLR in its present form.SUPERVISION AND COMPARISONSI found comparisons between supervision conditions in this paper difficult tounderstand. It is claimed that the natural language instruction followingapproaches described in the first paragraph "require a large amount of humansupervision" in the form of action sequences. This is not exactly true, as someapproaches (e.g. Artzi 2013), can be trained with only task completion signals.More problematically, all these approaches are contrasted with reinforcement andimitation learning approaches, which are claimed to use "little humansupervision". In fact, most of the approaches listed in this section use exactlythe same supervision---either action sequences (imitation learning) or taskcompletion signals (reinforcement learning). Indeed, the primary distinction isthat the "NLP-style" approaches are typically evaluated on their ability togeneralize to new instructions, while the "RL-style" approaches are evaluated onthe (easier) problem of fitting the complete instruction distribution as quicklyas possible.This confusion carries into the evaluation of the approach proposed in thispaper, which is compared to RL and IL baselines. It's hard to tell from thetext, but it appears that this is an "RL-style" evaluation setting, where weonly care about rapid convergence rather than generalization. But the baselinesare inadequately described, and it's not clear to me that they condition on thecommands at all. More significantly, it's not clear what an evaluation based on"timesteps" means for a behavior-cloning approach---is this the number ofdistinct trajectories observed? The number of gradient steps taken? Withoutthese explanations it is impossible to interpret the experimental results.GENERALITY OF PROPOSED APPROACHDespite the advantages of the high-level two-phase model proposed, the specificimplementation in this paper has two significant shortcomings:- No evidence that it works with real language: despite numerous claims  throughout the paper that the model is designed to interpret "human  instructions", it is revealed on p7 that these instructions consist of one or two  5-way indicator features. This is an extremely impoverished instruction space,  especially compared to the numerous papers cited in the introduction that make  use of large datasets of complex natural-language strings generated by human  annotators. The present experiments do not support the use of the word "human"  anywhere in the paper.- No support for combinatorial action spaces. Even if we set aside the  distinctions between human-generated instructions and synthetic command  languages like used in Hermann Hill &amp; al., the goal -&gt; policy module is  defined by a buffer of cached trajectories and goal representations. While  this works for the simple environments considered in this paper, it cannot  generalize to real-world instruction-following scenarios where the number of  distinct goal configurations is too large to tractably enumerate. Again, this  is a shortcoming that existing approaches do not suffer from (given  appropriate assumptions about the structure of goal space), so the lack of  comparisons is problematic.CLARITYThe whole paper would benefit from copy-editing by an experienced Englishspeaker, but a few sections are particularly problematic:- The first paragraph of 4.1.1 is extremely difficult to understand What does  the fingertip do? What exactly is the action space?- The end of the second paragraph is also difficult to understand; after reading  it I still don't know what the extra "position" targets do.- 4.1.4 is cut off mid-way through a sentence.- last sentence of 4.2The figures are also impossible to interpret: three of the four are captioned"overview of the proposed framework", and none are titled. On the positive side, I think it's a good idea to experiment with various approaches to defend DNNs against adversarial attacks, like the Background Check approach considered in this manuscript (which hasn't gotten a lot of traction in the Machine Learning community so far).However, the manuscript has a number of shortcomings which in my opinion makes it a strong rejection. My main concern is about the experimental evaluation: - The authors should test their approach on Carlini &amp; Wagner's attack which allows for explicit control of logit differences and thus could entirely defeat the Background Check.- Moreover, any paper on this topic should evaluate defenses in a complete white-box setting, i.e. the adversary is aware of the detection method and actively tries to bypass it. - A comparison with other detection methods from the literature is missing, too, and the two-class classifier setting is very limited.Besides that, I find there is a general lack of clarity:- It really becomes clear only towards the end of the paper what the Background Check is applied for, namely, the detection of adversarial samples. This should be clearly articulated from the beginning.- Notation isn't always properly introduced (e.g. in the formula for 3-class average recall on page 6), and the same goes for some acronyms (e.g. what is TPR?).- Where does Table 2 show a "mean reduction in average recall of 11.6", and what does that mean exactly? This work proposes an ensemble method for convolutional neural networks wherein each convolutional layer is replicated m times and the resulting activations are averaged layerwise.There are a few issues that undermine the conclusion that this simple method is an improvement over full-model ensembles:        1. Equation (1) is unclear on the definition of C_layer, a critical detail. In the context, C_layer could be weights, activations before the nonlinearity/pooling/batch-norm, or activations after the nonlinearity/pooling/batch-norm. Averaging only makes sense after some form of non-linearity, otherwise the ensemble is merely a linear operation, so hopefully its the latter.        2. The headings in the results tables could be clarified. To be sure that I am understanding them correctly, Ill propose a new notation here. Please note in the comments if Ive misunderstood! Since m is used to represent the number of convolutional layer replications, lets use k to represent the number of full model replications. So, instead of CNL and IEA (ours) in Table 1 and Ensemble of models using CNL and Ensemble of models using IEA (ours) in Table 2, I would recommend a single table with these headings: (m=1, k=1),  (m=3, k=1),  (m=1, k=3),  and (m=3, k=3), corresponding to the columns in Tables 1 and 2 in order. Likewise for Tables 3-6.        3. Under this interpretation of the tables---again, correct me if Im wrong---the proper comparison would be IEA (ours) versus Ensemble of models using CNL, or  (m=3, k=1) versus (m=1, k=3) in my notation. This pair share a similar amount of computation and a similar number of parameters. (The k=3 model would be slightly larger on account of any fully-connected layers.) In this case, the outer ensemble wins handily in 4 of 5 cases for CIFAR-10.        4. The CNL results, or (k=1,m=1), seem to not be state-of-the-art, adding more uncertainty to the evaluation. See, for instance, https://www.github.com/kuangliu/pytorch-cifar. Apologies that this isnt a published table. A quick scan of the DenseNets paper and another didnt yield a matching set of models. In any case, the lack of data augmentation may account for this disparity, but can easily be remedied.Given the above issues of clarity and that this simple method seems to not make a favorable comparison to the comparable ensemble baseline (significance), I cant recommend acceptance at this time. Other notes:        * The wrong LaTeX citation function is used, yielding the author (year) form (produced by \citet), instead of (author, year) (produced by \citep), which seems to be intended. Its possible that \cite defaults to \citet.        * The acronyms CNL and FCL hurt the readability a bit. Since there is ample space available, spelling out convolutional layer and fully-connected layer would be preferred.        * Other additions to the evaluation could or should include: a plot of test error vs. number of parameters/FLOPS/inference time; additional challenging datasets including CIFAR-100, SVHN, and ImageNet; and consideration of other ways to use additional parameters or computation, such as increased depth or width (perhaps the various depths of ResNet would be useful here). Strength: The proposed approach is architecture-independent: the attack is constructed only from the dataset.Weaknesses: Paper is not sufficiently well written for a venue like ICLR. Attack has very low success rate.To the exception of Figures 4 and 5, many experiments are conducted on MNIST.Feedback: Experimental results show that the attack is able to degrade a classifiers performance by inserting perturbations that are computed on the data only. However, there are no baselines included to compare adversarial evasion rates achieved here to prior efforts. This makes it difficult to justify the fairly low success rate. In your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?Furthermore, strong conclusions are made from the results despite the lack of supporting evidence. For instance, on P10, the attack is said to also explains why adversarial examples can be universal.. However, not only does the attack achieve less success than universal adversarial examples would (so it cannot explain all of them) but also does it not share any characteristics with the way universal adversarial examples are crafted. Drawing such a strong conclusion thus most likely needs a lot more supporting evidence. Several directions would improve the content of the paper: * Complete existing experimental results by being more systematic. For instance, in Section 3.1, measurements are only performed on one pair of MNIST images. Without studying a significant portion of the test set of two datasets, it is very difficult to draw any conclusions from the limited evidence.* Perform a human study to have all perturbed images labeled again. Indeed, because of the ways images are perturbed here, it is unclear how much perturbation can be added without changing the label of the resulting input. * Study how existing adversarial example techniques modify internal representations. This would help support conclusions made (e.g., about universal perturbations---see above). * Rewrite the related work section to scope it better: for instance, Sabour et al. in Adversarial Manipulation of Deep Representations and Wicker et al. in Feature-Guided Black-Box Safety Testing of Deep Neural Networks explore adversaries operating in the feature space. This will also help build better baselines for the evaluation.Additional details: TLDR: typo in the first wordP1: The following definition of adversarial examples is a bit restrictive, because they are not necessarily limited to vision applications (e.g., they could be found for text or speech as well). Adversarial examples are modified samples that preserve original image structures P1: The following statement is a bit vague (what is obvious impact referring to?): Experiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks.P1: References do not typeset properly (the parentheses are missing: perhaps, the \citep{} command was not used?)P2: What is the motivation for including references to prior work in the realm of image segmentation and more generally-speaking multi-camera settings in the related work section? P2: Typo in  linear vibrationP2: It remains difficult to make a conclusion about humans being robust to the perturbations introduced by adversarial examples. For instance, Elsayed et al. at NIPS 2018 found that time-constrained humans were also misled by adversarial examples crafted to evade ML classifiers: see Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.P2: Prior work suggests that the following conclusion is not entirely true: Most of these kinds of examples are generated by carefully designed algorithms and procedures. This complexity to some extent shows that adversarial examples may only occupy a small percentage for total image space we can imagine with pixel representations. For instance, Tramer et al. in ICLR 2018 found that adversarial subspaces were often large: Ensemble Adversarial Training: Attacks and Defenses.P2: Others have looked at internal representations of adversarial examples so the following statement would be best softened: To the best of our knowledge, this paper should be the first one that discusses adversarial examples from the internal knowledge representation point of view.. See for instance, Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning by Papernot and McDaniel.P3: Could you add pointers to support the description of human abstraction and sparsity? It reads a bit speculative as is, and adding some pointers would help relate the arguments made to relevant pointers for readers that are less familiar with this topic. P3: What is the motivation for including the discussion of computations performed by a neural network layer-by-layer in Section 2?P4: Given that saliency maps can be manipulated easily and are only applicable locally, it appears that Figure 1 is too limited to serve as sufficient evidence for the following conclusion: This, in some way, proves the point that the knowledge storage and representation of current neural networks are not exactly sparse prototype based.P5: The error rate reported on MNIST is quite low (45%). Even using the Fast Gradient Method, one should be able to have the error rate be as high as 90% on a standard CNN.P7: Would you be able to provide references to backup the following statement? This is a network structure that is very common.P10: How does the discussion in Section 4.2 relate to the attack described in the submission? The paper presents an approach to infer optimal strategies by learning the payoff function of 2 player games with a neural network Q(s, a), where a are the agent actions and s the context (e.g., action of the other player). The strategy is inferred by considering the inputs as free variables at test time and maximizing the learnt Q over its input variables by backpropagation.The structure of the neural network in itself is not particularly original. The originality of the paper is to show that experimentally, in some 2-player games (and small sequential games, using an RNN), the policy that is inferred is close to a Nash equilibrium. While this is an interesting result in itself, the games used in the experiment are pretty easy to solve with existing algorithms, so the experimental evidence that this approach can work in practice in difficult cases is weak.The motivation and intuition fail to be convincing. There is an excessive usage of analogies with intelligence and life in general that are not particularly enlighting (e.g., "Even though its hardware is damaged, the software inthe cloud (or the eternal soul, arguably) of the mosquito [...]", the value of the analogy between the cloud and the soul is unclear), and in the end there is no clear explanation of why it should work in practice. I think the paper in its current form is not ready for publication. More formal arguments and/or stronger experimental evidence are necessary. First of all, the paper cannot be accepted because it violates the double blind submission policy by including an acknowledgments section.Nonetheless, I will give some brief comments: The paper proposes a probabilistic hierarchical approach to perform zero-shot learning.Instead of directly optimizing the standard cross-entropy loss, the paper considers some soft probability scores that consider some class graph taxonomy. The experimental section of the paper is strong enough although more baselines could have been tested. The paper only compares the usual cross entropy loss with their proposed soft-classification framework. Nonetheless, different architectures of neural networks are tested on ImageNet and validate the fact that the soft probability strategy improves performance on the zero-shot learning task. On the other hand, the theoretical aspect is weak. The proposed method seems to be a straightforward extension of Frome et al., NIPS 2013. The main contribution is that soft probability scores are used to perform classification instead of using only class membership information.Some weighting strategy is proposed in Section 2.2 but the proposed steps seem very ad hoc with no theoretical justification. The first equation on page 8 has the same problem where some random definition is provided. The idea and research direction itself is definitely interesting and worthy of pursuit. However, the execution is really poor. In addition to many improvements to clarity and writing, the proposed method is not at all novel and various variants for reconstructing one sensory modality from others have been proposed in the past:Gu et al. "Improving domain adaptation translation with domain invariant and specific information" .arXiv [Preprint].arXiv:1904.03879, (2019).Murez et al.Image to image translation for domain adaptation, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Salt Lake City, UT),45004509, (2018).Luo et al. (2018). ViTac: feature sharing between vision and tactile sensing for cloth texture recognition, in2018IEEE International Conference on Robotics and Automation (ICRA) (Brisbane,QLD: IEEE), 27222727.Lee et al. (2019). Touching to seeand seeing to feel: Robotic cross-modal sensory data generation for visual-tactile perception, in 2019 International Conference on Robotics and Automation (ICRA) (Montreal, QC), 42764282.Tatiya et al. "A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization." Frontiers in Robotics and AI 7 (2020): 137.These papers should be considered by the authors, discussed in related work, and used as a basis to propose something new in future work as there are still problems in that area that need solving. Finally, the notation is way more complicated than it needs to be for a simple encoder-decoder architecture. My advise to the authors is to simplify it.  This paper proposes an approach for encouraging exploration when planning over learned models of discrete reinforcement learning environment. The proposed method involves using an uncertainty-aware model (e.g., an ensemble of neural networks) to predict state-action transitions, together with a graph-based planner operating on this model. The key idea is to replace (with some probability) the planner's action with the action leading to the highest uncertainty in model prediction. The paper evaluates the proposed technique using two standard search planners (MCTS and BFS). Unfortunately, I think the significance and technical contribution of this work is minimal, an issue that mostly likely starts from a deficient literature review. What the authors refer to as Trust-But-Verify, it's just an ad-hoc instance of the well-known principle of *optimism in the face of uncertainty*, which underlies classic bandit and RL algorithms such as UCB1[1], UCT[2], Thompson Sampling [3, 4]. In the model-free setting, this idea has lead to numerous recent algorithms, many of which also use ensembles for uncertainty quantification [5-8]. In the model-based setting there are also some precedents of work using similar ideas [9-11]. There is also a large body of work treating the problem from the point of view of Bayesian RL (see [12] for a survey).  It is a bad sign that none of this body of previous work was discussed in the paper, which I would argue was the more relevant literature upon which the paper had to be positioned.This could conceivably be excused if the paper technical and experimental contribution was impressive enough, but this is not the case.  In contrast to the literature outlined above (where proposed exploration strategies typically follow from rigorous statistical analysis), this work presents the proposed method as a heuristic rule, providing no insight as to why one should expect the approach to work well in general. Moreover, the experiments are done in relatively simple domain, and compared against simple baselines. Some of the baseline choices do not seem appropriate either. For example, why use $\epsilon$-greedy for exploration, instead of more robust strategies using upper confidence bounds? Finally, the writing on the paper can be improved in many places. For example, the paper refers to using the graph structure of the underlying problem, but what this graph structure refers to is not properly defined anywhere in the paper. I imagine it refers to the graph wherein edges represent non-zero probability transitions between states, but this is not clear from the text. Additionally, some paragraphs add little in terms of content. For example, the first paragraph of Section 3 is devoted to describe the basic problem that all model-based RL methods are trying to solve; this issue is ubiquitous so there is no need for a full example and so much text to describe this. Other sentences are unclear, such as "The optimistic and pessimistic errors are often of the same nature", which I am not sure what is referring to . Additionally, I didn't see a description of the learned model used in the experiments, which is not an obvious choice, since the environment is discrete. Overall, to end on a somewhat constructive note, I think the problem the authors are trying to solve is interesting and the proposed approach is based on the right intuitions. However, this work is still too immature for publication. I suggest to the authors to position their work properly with regards to the relevant literature, refine their technical contribution accordingly, and compare with more appropriate baselines.  ----------------------------------------------------------------------[1] Auer, Peter, Nicolo Cesa-Bianchi, and Paul Fischer. "Finite-time analysis of the multiarmed bandit problem." Machine learning 47.2-3 (2002): 235-256.[2] Kocsis, Levente, and Csaba Szepesvári. "Bandit based monte-carlo planning." European conference on machine learning. Springer, Berlin, Heidelberg, 2006.[3] Thompson, William R. "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples." Biometrika 25.3/4 (1933): 285-294.[4] Russo, Daniel, et al. "A tutorial on thompson sampling." arXiv preprint arXiv:1707.02038 (2017).[5] Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. In Advances in neural information processing systems (pp. 1471-1479).[6] Ostrovski, G., Bellemare, M. G., Oord, A., & Munos, R. (2017, July). Count-Based Exploration with Neural Density Models. In International Conference on Machine Learning (pp. 2721-2730).[7] Osband, I., Blundell, C., Pritzel, A., & Van Roy, B. (2016). Deep exploration via bootstrapped DQN. In Advances in neural information processing systems (pp. 4026-4034).[8] Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., ... & Blundell, C. (2017). Noisy networks for exploration. arXiv preprint arXiv:1706.10295.[9] Sanner, S., Goetschalckx, R., Driessens, K., & Shani, G. (2009). Bayesian real-time dynamic programming. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09) (pp. 1784-1789). IJCAI-INT JOINT CONF ARTIF INTELL.[10] Pathak, Deepak, Dhiraj Gandhi, and Abhinav Gupta. "Self-supervised exploration via disagreement." arXiv preprint arXiv:1906.04161 (2019).[11] Shyam, Pranav, Wojciech Ja[kowski, and Faustino Gomez. "Model-based active exploration." International Conference on Machine Learning. 2019.[12] Ghavamzadeh, M., Mannor, S., Pineau, J., & Tamar, A. (2016). Bayesian reinforcement learning: A survey. arXiv preprint arXiv:1609.04436. This paper tries to propose a kernel-based discrepancy measure called generalised probability kernel that can unify MMD and KSD which is an interesting topic of discussion. The paper applies the new discrepancy to perform two-sample tests. The new kernel proposed, unlike the previous RKHS kernels that only depend on data-points, incorporate the notion of probability. e.g. kernel K_{p,q} depends on density p and q. also a symmetric version on discrete KSD is discussed.Despite the idea is interesting, there are several flaws which can be reviewed.Firstly, I think the paper is not clearly presented, with some confusing notation.--in Definition 1, you defined a kernel, on distributions p and q, that is a k x k matrix; while in definition 2, the notion of K, are on samples and is a scalar output.it is unclear of how \phi is defined in general; only examples are given later for specific cases so that we got an conjuncture.--in Definition 5, why is it different from stein operator of KDSD? or it is supposed to say difference operator?In addition I have several confusions:1. why is MMD_E^2 an unbiased estimator? what happened to k(x_i, x_i)? it is not clear from the Bernstein polynomial introduced in appendix. 2. in abstract, it claims that the kernels are between distributions instead of samples, but in the main text it is still evaluation at p_i=p(x_i) on samples; I m confused of the difference and novelty claimed.3. The above concern brings up the question while applying on two sample test. --When the MMD is used to perform two-sample test, it is assumed that both p and q are unknown. however, to my understanding, we need to know p and q to define k_{prob}; how is this going to be applied to two-sample test? --for KSD setting, when the symmetric KDSD is introduced, it also seems to require p and q to known for two-sample testing. In the Liu2016 setting, where goodness-of-fit test is proposed with KSD, q is known (up to normalization) while p is unknown with samples; that is a key point why KSD is useful for goodness-of-fit test.In addition, is there any argument on why the symmetric-KDSD might be better than KDSD Yang et.al 2018?An additional point is regarding literature review, which is yet throughout  to check; e.g. asChwialkowski, et. al  "A kernel test of goodness of fit." proposed independently as Liu et.al for KSD goodness-of-fit test, that might be useful to cite.In my point of view, ICLR may not be a venue of fit either. More reviews and clarifications may be required, for both kernel construction and application.  Summary.The authors describe a family of kernel functions on discrete probability measures. The kernel generalizes existing discrepancies such as the MMD and the KSD. The authors further provide plugin estimates based on empirical frequencies and some arguments for unbiased-ness.While it is interesting to think about alternative estimators for comparing probability distributions, this paper falls short on the execution. I would recommend a major work-over before considering a submission again. There are many missing points in theory, experiments (there are none), and presentation. See below.It is unclear to me why we would care about the proposed estimators* There is no analysis showing that the presented kernels are useful in any way.* I appreciate that the authors show that existing discrepancies are special cases of the proposed one, but I wonder again what that is useful for?* This is in particular as the authors do not provide any sort of asymptotic analysis of the presented estimators. How can we use them for two-sample testing without that? Answering this question is one of the major parts of the kernel two-sample testing literature.Theory.* The presented theory consists of elementary manipulations that mostly follow existing literature, so there is very little actual innovation. For example of of page 5.Experimental evaluation* There is *no* experimental evaluation of the proposed estimators.* It would have been interesting to compare the variances as a function of dataset characteristics.Presentation* There many grammar glitches, spelling mistages, missing articles, etc, to a point that it is hard to follow.* There is no overview of the series of arguments in the later part of the paper.* There is a lot of re-cited derivations from existing papers. The paper describes a two-stage generative image model: first, a GAN is trained to output low-resolution images, and another model to then perform single-image superresolution on the results of the first model. The claim is that the resulting model is slightly better than BigGAN-512 using half the compute requirements, in terms of FID. Two variants are described: one that generates in the wavelet decomposition domain (-W), and another that operates in pixel space (-P).The idea of applying SISR to a GAN output seems potentially novel and useful, but as it stands, I find the paper convoluted and lacking a clear message. I cannot recommend acceptance at this point.The main issues I see are the following:1. I suspect much of the image processing is performed incorrectly, casting doubt on the validity of the -P model and rendering the comparison to the wavelet case moot.2. Results are only reported in numeric form as FID and IS.3. The argumentation about the properties of the wavelet decomposition seem vague and without clear technical counterparts in the (well-developed) literature on wavelets and image processing.4. Simultaneous lack of details and overall verbosity; I find it difficult to find the big picture from this paper even after hours of trying.1. I believe the bilinear downsampling, the basis of the -P variant, is implemented incorrectly. This is visible as clear aliasing in the supplemental Figure 2, rightmost pixel-space column. To verify, I extracted the dog and sailboat images from the PDF and applied bilinear downsampling in Matlab  which uses proper pre-filtering before downsampling to remove aliases, unlike for instance the bilinear grid_sample operation in PyTorch  and get a significantly different result; one that does not have the signature aliasing artifacts that remain in the images shown. If, on the other hand, I explicitly turn off antialiasing, the result quite closely matches the right-hand column. To be clear, this a rudimentary mistake in image processing (which is surprisingly prevalent in the ML and vision literature).This makes me suspect all results of the -P variants are not to be trusted: teaching a GAN to generate aliased images, and then another model to up-res those aliased images, seems like a task that is fundamentally harder than if the aliasing wasnt there. Hence the worse results are not unexpected.In particular: I find the conclusions drawn from the results in Table 1 all potentially invalid.  On the other hand, using pretrained samplers, the pixel space versions appear to actually do a little *better* (in terms of FID) than the wavelet ones in Table 2. Comparing the results in the appendix does not appear to reveal large differences.2. It is well known that metrics like FID and IS do not capture the notion of the quality of a GAN well. They are useful in drawing a picture of how the model performs. While some metrics that better correlate with result quality are known, a satisfactory one hardly exists, so visual inspection and analysis of the results cannot be skipped. I do not approve of pushing them to the appendix.3. Example: "The functional prior imposed by our deterministic encoder leads to a highly structured representation space made up of low frequency TL patches of images. What does this mean, precisely? The repeated application of the wavelet approximation coefficient filter followed by decimation by 2 is equivalent to a particular linear downsampling operation applied to the original image; a poor one at that, because the kinds of critically sampled wavelets employed here are known for their aliasing issues (which has long ago led to a preference of using overcomplete bases). Similar language about the structuredness of the wavelet representation can be found near Figure 2, where the pixel-space comparison is, I believe, incorrect, as I detail above.4. What precisely is going on with Equation 2? IWT(&) would appear to be a reconstruction operation that combines the approximation and detail coefficients into an image of 2x the size, in pixel space; then addition of f(W^l_1,1) seems to add hallucinated detail on top. Does f do anything random or is it deterministic? And more pressingly, how is this actually different from the pixel-space version..?I do not understand the paragraph 3.1.2 U-Net decoder. Why does [it] not take full advantage of the compression that wavelet space modeling brings about?  Deep neural networks are known to be brittle, and can lead to dangerous consequences if left unverified. Forward reach set computation can be used as a basic primitive to verify properties of deep neural networks used in a robotic setting. There has been a rising interest in verifying larger neural networks used in safety critical setting.  In this paper,  the authors propose a way to compute reachable sets for a neural network in a backward sense. Starting from the outputs of the neural network, and  then work it's way to the inputs. This is an interesting way to look at the problem itself,  but as the authors point out it is an intractable problem.My concern about this paper is I don't see the use of a pre-image computation algorithm as being very useful. A forward reachability tool works pretty well for the size of neural networks considered in the paper. Pre-image computation does not provide any advantage in terms of scalability, as is apparent from the experiments. Moreover, almost any safety constraint that needs to be verified with system dynamics in the loop always should ideally work forward in time. Thus for the neural network controller from the inputs to the outputs. Cartpole example : The authors come up with rules about, which output behaviors are correct for a few of the input regions. Then use this as a specification for the verification algorithm. But the very specifications, comes from reasoning about the forward behavior of the system dynamics itself. The idea of forward reach sets computation would generalize much better to a wide range of examples therefore.  Without the need to come up with such handcrafted rules. The authors do make a convincing case for the ACASXu example. But this example is less interesting given the amount of attention it has received recently.  ##### Impact:The submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization ("because all combinations have positive joint probabilities in training"). However, I disagree that this is the case in prior work; here are some examples of prior works that correctly hold out novel combinations (of underlying components) for test time:- https://openreview.net/forum?id=HJz05o0qK7- http://papers.nips.cc/paper/8825-learning-by-abstraction-the-neural-state-machine- https://arxiv.org/abs/1910.09113- https://arxiv.org/abs/1912.09713- https://arxiv.org/abs/1912.12179There are many such examples; they are too numerous to list here.##### Quality: The algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed.  The evaluation tasks are extremely simple (overlayed MNIST digits and conjoined word token) and are, as such, far from the complexity of existing work on compositionality (which can deal with, for example, naturalistic image data; see the references above for examples of such works).##### Clarity:There are many points of ill clarity / inconsistencies; for example:- "The main approach for compositional generalization is to learn compositional representations" Is this really the "main approach"? Compositional generalization has been studied in many contexts outside of representation learning (e.g., see https://semanticsarchive.net/Archive/jcyZDc1Y/Goldberg.Compositionality.RoutledgeHandbook.pdf)- "We find that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions" Why is it assumed here that there is an extraction network? The "extraction network" is referred to several times in the introduction and methods section prior to its introduction/explanation.- "compositional generalization is a type of out-of-distribution (o.o.d.) transferring or generalization, which is also called domain adaptation" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation. I think the submission could do with a better job of dealing with the distinctions between OOD generalization / domain adaptation / concept drift.- "We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network." At this point, neither the "extractor" nor the "auxiliary network" are defined.- "These networks can be some existing networks for compositionality learning" If so, what are examples of "existing networks for compositionality learning"? The proposed algorithm is not properly put in the context of other semi-supervised learning methods that are closely related. These include label propagation, clustering auto encoders, entropy minimization / margin maximization, prototype networks and semi-supervised clustering. Examples of these lines of research are: Zhu and Ghahramani, Learning From Labeled and Unlabeled Data With Label Propagation,2002Xie, Girschick and Farhadi, Unsupervised Deep Embedding for Clustering Analysis, 2016Grandvalet and Bengio, Semi-supervised learning by entropy minimization, 2005Snell et al, Prototypical Networks for Few-shot Learning, 2017The idea of the paper is to penalize maximize distances between pairs of samples from different clusters as determined by the classifier output as a proxy for unlabeled samples. The paper claims previous works do not consider the spatial information provided by unlabeled examples. This is not correct. Many methods take advantage of such information such as label propagation (uses proximity between examples to propagate labels), entropy minimization (uses density of examples), etc. The novelty of the method over these previous techniques is not clear to meThe experiments section of the paper has some shortcomings as well. The datasets used in Figure 3 are also easily solved by other semi-supervised learning methods. The discussion of these figures does not lend insight into what type of problems that are challenging for previous methods are addressed by the proposed approach. Some of the experimental results do not appear to be compared to the state-of-the-art. For instance, Sajjadi et al, Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning, 2016 reports significantly better results for the MNIST dataset with 100 labeled examples compared to the results included in Table 1. Finally, the datasets used with the exception of MNIST are not standard ones used in recent semi-supervised learning papers. CIFAR-10 and STL-10 would have been better choices. The clarity of the paper and English use is also below average. Some sentences contain informal phrases that might not be suitable for a scientific paper such as People try&, "Thanks for the tool; it helped us done differentiation automatically."  or vague statements such as "in some particular space, samples in the same category should be in the same cluster". The paper goes to lengths describing simple concepts such as cluster centroids and distance between cluster centroids (equations 1-3) yet concepts more crucial to the algorithm such as  This paper introduces a family of attack on confidence thresholding algortihms. Such algorithms are allowed to refuse to make predictions when their confidence is below a certain threshold. There are certainly interesting links between such models and KWIK [1] algorithms (which are also supposed to be able to respond 'null' to queries), however they are not mentioned in this paper, which focuses mainly on evaluation methodologies.The definition of the metric is certainly natural: you would expect some trade-off between performance in the normal versus the adversarial regime. I am not certain why the authors don't simply measure the success rate on both natural and adversarial conditions, so as to have the performance metric uniform. Unfortunately the paper's notationleaves something to be desired, as it fails to concretely define the metric.Let me do so instead, and consider the classification accuracy of a classification rule $P_t$ using a threshold $t$ under a (possibly adaptive) distribution $Q$ to be $U(P,Q)$. Then, we can consider $Q_N, Q_A$ as the normal and adversarial distribution and measure the corresponding accuracies. Even if we do this, however, the authors do not clarify how they propose to select the classification rule. Should they employ something like a convex combination:\[V(P_t) := \alpha U(P_t, Q_N) + (1 - \alpha) U(P_t, Q_A) \]or maybe take a nimimax approach\[V(P_t) := \min \{U(P_t, Q) | Q = Q_A, Q_N\}\]In addition, the authors simply plot curves for various choices of $t$, however it is necessary to take into account the fact that measuring performance in this way and selecting $t$ aftewards amounts to a hyperparameter selection [2]. Thus, the thresholding should be chosen on an independent validation set in order to optimise the chosen performance measure, and then the choice should evaluated on a new test set with respect to the same measure $V$The MaxConfidence attack is not very well described, in my opinion. However, it seems it simply wishes to find to find a single point $x \in \mathbb{S}$ that maximises the probability of misclassification. It is not clear to me why performance against an attack of this type is interesting to measure.The main contribution of the paper seems to be the generalisation of the attack by Goodfellow et al to softmax regression. The proof of this statement is in a rather obscure place in the paper. I am not sure I follow the idea for the proof, or what they are trying to prove. The authors should follow a standard Theorem/Proof organisation, clearing stating assumptions and what the theorem is showing us. It seems that they want to prove that if a solution to (1) exists, then MaxConfidence() finds it. But the only definition of MaxConfidence is (1). Hence I think that their theorem is vacuous. There are quite a few details that are also unclear such as what the authors mean by 'clean example' etc. However the authors do not explain their attack very well, their definition of the performance metric is not sufficiently formal, and their evaluation methodology is weak. Since evaluation methodology is the central point of the paper, this is a serious weaknes. Finally, there doesn't seem to be a lot of connection with the conference's topic.[1] Li, Lihong, Michael L. Littman, and Thomas J. Walsh. "Knows what it knows: a framework for self-aware learning." Proceedings of the 25th international conference on Machine learning. ACM, 2008.[2] Bengio, Samy, Johnny Mariéthoz, and Mikaela Keller. "The expected performance curve." International Conference on Machine Learning, ICML, Workshop on ROC Analysis in Machine Learning. No. EPFL-CONF-83266. 2005. The authors propose a review-style overview of memory systems within neural networks, from simple RNNs to stack-based memory architectures and NTM / MemNet-style architectures. They propose some reductions to imply how one model can be used (or modify) to simulate another. They then make predictions about which type of models should be best on different types of tasks.Unfortunately I did not find the paper particularly well written and the taxonomy was not illuminating for me. I actually felt, in the endeavor of creating a simple taxonomy the authors have created confusing simplifications, e.g."LSTM: state memory and memory of a single external event"to me is mis-leading as we know an LSTM can compress many external events into its hidden units. Furthermore the taxonomy did not provide me with any new insights or display a prediction that was actually clairvoyant. I.e. it was clear from the outset that a memory network (say) will be much better at bAbI than a stack-augmented neural network. It would be more interesting to me, for example, if the paper could thus formalize why NTMs &amp; DNCs (say) do not outperform LSTMs at language modeling, for example. I found the reductions somewhat shady, e.g. the RAM simulation of a stack is possible, however the model could only learn the proposed reduction if the number of write heads was equal to the number of memory slots --- or unless it had O(N) thinking steps per time step, where N is the number of memory slots, so it's not a very realistic reduction. You would never see a memory network, for example, simulating a stack due to the fixed write-one-slot-per-timestep interface. Nit: I'm not sure the authors should be saying they 'developed' four synthetic tasks, when many of these tasks have previously been proposed and published (counting, copy, reverse copy). The reviewer feels that the paper is hard to follow. The abstract is confusing enough and raises a number of questions.  The paper talks about `"local maxima" without defining an optimization problem. What is the optimization problem are we talking about? Is it a maximization problem or minimization problem? If we are dealing with a minimization problem, why do we care about maxima?The first several paragraphs did not make the problem of interest clearer. But at least the fourth paragraph starts talking about training networks (the reviewer guesses this "network" refers to neural network, not other types network (e.g., Bayesian network) arising in machine learning). This paragraph talks about random initialization for minimizing a loss function, does this mean we are considering a minimization problem's local maxima? In addition, random initialization-based neural network training algorithms like back propagation cannot guarantee giving local maxima or local minima of the problem of interest (which is the loss function for training). It is even not clear if a stationary point can be achieved. So if the method in this paper wishes to work with local maxima of an optimization problem, this may not be a proper example.The next paragraph brings out a notion of value function, which is hard to follow what it is. A suggestion is to give a much more concrete example to enlighten the readers.The next two paragraphs seem to be very disconnected. It is not properly defined what is x and how to obtain it. If they are local maxima of a problem, please give us an example: what is the optimization problem, and why this is an interesting setup?Since the problem setup of this paper is very hard to decode, it is also very hard to appreciate why the papers in the "related work" section are really related.The motivation and intuition behind the formulations in (1) and (2) are hard to follow, perhaps because the goal and objective of the paper is unclear.Overall, there is no formal problem definition or statement, and the notions and terminologies in this paper are not properly defined or introduced. This makes evaluating this work very hard. This work addresses the problem of learning a policy-learning-procedure, through meta-learning, that can adapt quickly to new tasks. This work uses MAML for meta-learning, and with this choice, the problem can be broken down into two loops: 1) inner loop: adapting a policy \pi_phi based on unseen rollouts, where initial parameters phi were provided by the meta-trainer in the outer loop 2) outer loop: the meta-trainer tries to learn parameters phi on batches of tasks that provide good initial parameters In prior work on meta-reinforcement learning via MAML, both the outer as well as inner objective attempt to minimize a RL objective, leading to an algorithm that has very high sample-complexity. This work uses imitation learning for the outer loop procedure, to significantly decrease sample-complexity.Technical Contribution:-----------------------The idea of using imitation learning for reinforcement learning is well explored in the literature, and so using this idea in itself is not real contribution. There are several issues with the presentation of this work, that make it incredibly difficult to identify a technical contribution:1. overreaching statements without details to backup: you are writing the paper as if you are learning a "RL algorithm" that can be used to quickly learn new tasks. your manuscript does not really provide a description for this "algorithm". After re-reading several other papers I concluded that what you mean is that you learn an initial set of policy parameters that can quickly adapt to new related tasks and an update rule with which you update these parameters. However, standard MAML uses SGD as an update rule so there is really nothing to be learned here. Unfortunately, your paper provides zero detail on these claims of learning a "RL procedure", so for now I have to assume that you are simply learning a good initial set of policy parameters through meta-learning. If that is the case, then using imitation learning in this setting is really not novel, this has been done by a lot of other people before (you're just using MAML to learn "better" initial parameters).2. you're technical section (section 4) provides some details on the technical challenges of using demonstrations to perform the outer loop optimization step. Unfortunately, you are not putting your work in the context of existing work ([1], [2]), that discuss and address the importance/issue of sampling in meta-rl with MAML. So it's impossible to know whether there is any new insight hereExperimental Evaluation:-------------------------The experimental evaluation is very "thin", other than the original MAML-RL and pure imitation learning no other more recent baselines ([1], [2]) have been compared to. And only 2 relatively simple simulation settings are tested. Summary:-----------Very minor contribution, a manuscript that is lacking important details and does not relate it's technical section to existing work, with very thin evaluation. [1] The Importance of Sampling in Meta-Reinforcement Learning, NIPS 2018[2] CONTINUOUS ADAPTATION VIA META-LEARNING IN NONSTATIONARY AND COMPETITIVE ENVIRONMENTS, ICLR 2018 This paper solves Flappy bird by combining DQN and probabilistic programming. I think this is in general a good avenue to explore.However I found the paper to be poorly written. For example, notation is not properly introduced, there are many mathematical mistakes and typos in the written text and citations. This makes it very hard to understand what is actually going on.It is also not clear what is the probabilistic program and what are we conditioning on? What is the inference algorithm? Maybe it's useful to expand more on how this ties to the "RL as inference" framework (see e.g. Levine, 2018). It seems like we are doing rejection sampling where the condition is "no collision". As a result, I'm not sure whether sampling from prior is a competitive baseline.For the DQN experiment, the learning curve seems very noisy in a way that it's unclear whether a fair conclusion can be drawn only from one run (as it appears to be done).The experiments also feel a bit contrived to make a strong case for probabilistic programming + DQN. Pros:- Provides illustration and math formulation for the problem of generalization beyond the correlation of labels and correlated but irrelevant attributes. Forming the issue as a domain adaptation problem (or specifically, a special kind of probability shift) is a clever idea.Cons:- Lacks comparison to existing work. Making features invariant to attributes to improve generalization is not a new idea, cf. :(1) Xie, Qizhe, et al. "Controllable invariance through adversarial feature learning." Advances in Neural Information Processing Systems. 2017.(2) If you consider the domain difference between various domains to be similar to attribute, then this is also related: Li, Haoliang, et al. "Domain generalization with adversarial feature learning." Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR). 2018.(3) There are other works that, although do not aim at improving generalization, use very similar formulation to decouple attribute from features: e.g. (a) Lample, Guillaume, et al. "Fader networks: Manipulating images by sliding attributes." Advances in Neural Information Processing Systems. 2017.  (b) Mitigating Unwanted Biases with Adversarial Learning (which the authors cite, but do not offer any comparison or differentiation)To improve the paper, these related work should be discussed in related work section, and (if applicable) compared to the proposed method in the experiments, rather than a very brief mention of one of them in Section 3.3 and no comparison.- Use of the term "negative transfer" is problematic. This is a more important shortcoming, but people may disagree with me. As far as I know, this term is used to describe a *source task* being used to help a *different target task* but result in a negative gain in performance (Torrey, Lisa, and Jude Shavlik. "Transfer learning."), which is inherently a multi-task learning setting. However, in this paper it refers to the effect of unrelated features being used in classifier, resulting in a worse generalization. The existence of this issue does not involve a second task at all. If this is not intended, please use another phrase. If the authors think that these are one and the same, I would strongly argue against this proposition.Also, there is no "negative transfer technique" as implied by page 2, end of the first paragraph.- Section 3.2 and 3.3's analysis is somewhat disjoint from the method. The analysis boils down to "given a different correlation between primary and aux tasks, you can compute the distribution of inputs, which will be different from the source, so let's make the aux task unpredictable to get domain invariance." And the method goes on to remove auxiliary task information from the shared feature space. This is disjoint from either eq. (1) picking a target domain closest to source, and Theorem 1 the bound for domain adaptation. One way to improve the paper is to analyze how these analysis are affected by the adversarial training.- One of the selling points is that the method can adapt to trainable features in deep learning. However, in the experiment, fixed extracted features from pre-trained ResNet is used anyway. If so, a way to improve the paper is to compare to the traditional methods cited in page 2 paragraph 1, by applying them on fixed extracted ResNet features. This paper presents a framework that allows the agent to learn from its observations, but never follows through on the motivation of experimentation---taking actions mainly for the purpose of learning an improved dynamics model. All of their experiments merely take actions that are best according to the usual model-based or model-free methods, and show that their consistency constraint allows them to learn a better dynamics model, which is not at all surprising. They do not even allow for the type of experimentation that has been done in reinforcement learning for as long as it has been around, which is to allow exploration by artificially increasing the reward for the first few times that each state is visited. That would be a good baseline against which to compare their method.Overall:Pros:1. Clear writing2. Good motivation description.Cons:1. Failed to connect presented work with the motivation.2. No comparison against known methods for exploration. The manuscript focuses on clinical natural language processing of electronic health records. More precisely, it addresses a text classification task called information extraction or named entity recognition from these clinical records. Its contributions include developing an embedding model to capture clinical prototypes (CPs), via supervised contrastive learning, and presenting experimental evidence of these learnt CPs capturing attribute-specific semantic relationships and being helpful in subsequent clinical natural language processing task of information retrieval and clustering of clustering of physiological signals. I find this text processing methodology interesting, carefully described, and supplementary to other studies.However, unfortunately, the authors demonstrate limited understanding of the related literature. First, the second and third paragraph of the Introduction section have many sentences that require references to be inserted. Second, and more importantly, the Related work section seems to not capture the key papers and trends of the field (I suggest reading some systematic reviews or surveys on clinical natural language processing, information extraction, and information extraction by, for example, Wendy Chapman, Carol Friedman, and Pierre Zweigenbaum), and, for example, as illustrated by the ImageCLEF and CLEFeHealth evaluation labs, computer vision tends to proceed faster than text analytics (see, e.g., https://www.researchprotocols.org/2018/7/e10961/), and not the other way around as claimed by this section.In addition, to feel convinced of the presented experimental evidence, I would have wanted to see statistical significance tests, confidence intervals, effect sizes, or similar presented. I could not find this methodology described in the manuscript or its outcomes, although the narrative repeatedly referred to significant performance gains. Please clarify.As my main minor comment, I would like to see a clearer separation of the materials, methods, and experiments sections from results, as well as including clearer justifications of this study design. For example, the aforementioned significance topic has not been addressed sufficiently. Another illustration of somewhat difficult task for the reader is to understand the experimental design as a whole and be convinced of this study being rigorous is the Experimental Results section including a lot of methodological details as opposed to only obtained outcomes.I also suggest including a conclusion statement as well as embedding more evidence (e.g., evaluation materials and methods plus obtained indicators of performance gains, such as measure values and effect sizes) to convince the reader in the abstract. To continue, please remember to punctuate equations and formulae. Finally, typically the Methods, Experiments, and Experimental Results sections would have been written using a past tense to emphasise a finished (as opposed to an ongoing) study where materials, methods, and experiments have already been chosen, justified, and completed. Most importantly, please avoid having inconsistent tense in these sections (see, e.g., Section 4.5. using a past tense whereas almost all others are having a present tense). # SummaryThe authors introduce a novel VAE-based approach for unsupervised learning of disentangled representations of image data.  The approach trains an ensemble of VAEs along with pair-wise linear transformations between their latent spaces.  The objective includes the ELBO objectives for each VAE as well as two additional pressures:  (i) An L2 similarity objective that pressures samples from each VAE latent space to match under linear transformations samples from the other VAE latent spaces, and (ii) A cross-model decoding objective that encourages decoding accuracy of the linearly transformed latent samples.  The authors provide a theoretical argument that the linear transformations should learn to be orthogonal, and show some experimental results indicating that their model performs well compared to baselines when evaluated with an established disentangling metric.# Pros* The theoretical analysis in section 4.1 is clear and provides good mathematical intuition for the authors results.* The introduction and related work sections are clear and include a thorough set of references.# Cons:* The authors baseline results give unexpectedly low metric scores.  The authors report FactorVAE metric values of 0.665 for beta-VAE and 0.764 for FactorVAE on the dSprites dataset.  However, the values reported in the FactorVAE paper (and corroborated by others) on the same dataset are significantly higher.  This makes me suspicious that something went wrong with the authors training --- perhaps they didnt train those baseline models to completion or something else went wrong.  Having baseline results that are inconsistent with the existing literature makes me uneasy.* The traversals in Figure 8 from the authors model are much less disentangled than other models in the literature.  For example, they are much less disentangled than the traversals shown in the beta-VAE paper and the FactorVAE paper on the same dataset.  Thus from these traversals, it seems that the authors model is performing worse than existing models in the literature (the authors metrics indicate the opposite, but as mentioned above Im uncertain about the validity of those metric results).  Figure 3-A also suggests that the authors model is using too many informative latents, i.e. not disentangling well.* I am not convinced by the authors intuitive justification in lines 216-225 (and appendix C) that the cross-model objective encourages entangled models to align to disentangled models.  Specifically, in that argument the authors seem to assume that orthogonal linear transformations are orthonormal.  However, there is nothing to enforce normality of the transformations in the model, hence the cross-model encoding variance from an entangled to a disentangled model could be quite small.* The purpose of the cross-model reconstructions is not clear, particularly given that Im not convinced by the authors intuitive justification of them.  The L2 regularization between the transformed encodings should pressure the cross-model reconstructions to be good, so I do not see the reason to include them in the model objective.  It would be good if the authors could do an ablation study without the cross-model reconstructions.* The authors do not mention the computational complexity of their model, yet computational complexity seems to be a significant drawback of it.  Ensemble training is very computationally expensive, so the authors should include some discussion about it as well as runtimes and memory requirements for their model.  Furthermore, with the cross-model reconstructions the computational complexity of the authors model scales with the square of the number of ensemble elements, which is quite a steep scaling.* The authors only compare to a couple (relatively old) baselines, betaVAE and FactorVAE, which are no longer state-of-the-art.  However, more recently a number of other VAE models have been published that perform better.  In order to support their claims about state-of-the-art performance, the authors should compare to newer baselines.  Here are a few examples:DIP-VAE  (Variational inference of disentangled latent concepts from unlabeled observations.  Kumar et al., 2017)TCVAE (Isolating sources of disentanglement in variational autoencoders. Chen et al., 2018)Spatial Broadcast VAE  (Spatial Broadcast Decoder: A Simple Architecture for Learning Disentangled Representations in VAEs.  Watters et al., 2019)* The authors also dont include many metrics or datasets.  dSprites and CelebA were used in the original betaVAE paper, but more recently it has become the norm to test on a larger set of datasets and with a number of different metrics to convincingly show disentangling.  By the way, a number of models, datasets, and metrics have been open-sourced in DistLib (https://github.com/google-research/disentanglement_lib), which may be useful for comparing to more models with more metrics on more datasets.# SummaryI do not recommend accepting this paper.  Baseline results are inconsistent with prior work, the model seems to disentangle less well than existing methods, and the authors dont do ablation experiments to justify the high computational complexity of the model. Paper summary:The authors argue that they have proposed a method to train robust models to biases without having prior knowledge of the biases. They argue also to provide analysis on how weak learner capacity impacts the in-domain/out-of-domain performance.Reasons to reject:1) The authors argue they have shown the model with limited capacity capture biases. However, this has been shown already in [1] in 2019 and therefore is not a contribution of the authors.2) The main method proposed in this paper, is exactly the same method proposed in [2]. Please note that [2] was already available in early July 2020, and on top of existing work, the paper does not provide other contributions. 3) About the third argued contribution on showing how the performance of the debiasing method change based on the capacity of weak learners, in [1], the authors included the discussion between the choice of weak learners on their impact. Though the method in [1] is different, the discussion in that paper still would apply here as well.  Please refer to table 1-3 and Figure 1 in [1]. Given the points above, and since the main method in the paper is proposed in [2], the paper does not provide enough contributions to be suitable for the ICLR venue. [1] Robust Natural Language Inference Models with Example Forgetting, Yaghoobzadeh et al, https://arxiv.org/pdf/1911.03861.pdf, 2019 [2] Towards Debiasing NLU Models from Unknown Biases, Utama et al, 13 July 2020, https://openreview.net/forum?id=UHpxm2K-jHE, EMNLP 2020  This paper proposes that, in addition to learning the normal action values, an RL agent should also learn the action values for an alternate inverse problem consisting of the same transitions as the original MDP and the negative of the original rewards. The intuitive argument is that the values for the inverse problem clearly identify what actions should not be taken. Results are presented on OpenAI Gym problems in which the new method performed better than conventional methods. The paper is not yet ready for publication for many reasons. First, the idea is not presented clearly, and it is not clear why it ever could be sensible. The inverse problem has a different solution than the base problem. Its solution would appear to have an arbitrary relationship to the base problems solution. The two optimal policies may choose different actions, as suggested in the text, but this is not necessarily true; in some states the two policies may choose the same action. I dont see how anything can be said in general, and no significant theoretical results are presented. (They do prove a form of convergence on the inverse problem, but this is not a new result; the inverse problem is just another problem and needs no new result.)The new hybrid method is never fully explained (e.g., the reader has to guess at what Q^H is). But by combining the solutions to the base and inverse problems in some way, it seems inevitable that the final optimal policy would be changed. Suppose the function approximation is completely successful and the correct values are exactly found. Then those for the base problem would give the optimal policy. Any alteration of them by the correct action-value function for the inverse problem could only make them worse and could only cause them to produce worse (or the same) behavior. There is no room for improvement and so this technique could only make things worse asymptotically if the function approximation is completely successful. This is the only thing that I see that can be clearly said about the new technique, and of course it is not a good thing.There are results presented, but they are not well done; they provide no significant evidence for any conclusion. The methods are not completely presented, and the results seem to be for a single run, in which case any relative ordering of the methods could be obtained.Generally, the paper is unfortunately poorly written. The grammar is not good. The notation is unnecessarily complex and confusing. The citations are made in an unusual, poor way. This paper tries to draw connections between rate distortion theory and DNNs and use some intuitions from that domain to draw conclusions about robustness and generalization of the DNNs. The paper is mostly written in a storytelling narrative with very little rigor. In my opinion, this lack of rigor is problematic for a conference paper that has to be concise and rigorous. Moreover, the story is not told in a cohesive way. In most parts of the paper, there is not much relationship between the consecutive paragraphs. And even within most of the paragraphs, I was lost in understanding what the authors meant. I wish the paper would have been self-contained and made concrete definitions and statements instead of very high-level ideas that are difficult to judge. In the current state, it is very difficult for me to say what exactly is the contribution of the paper in terms of the story other than some loosely related high-level ideas. I feel like most parts the story that the authors are telling is already told by many other papers in other forms(papers that authors have cited and many other ones). This paper aims to view the computations performed by residual networks through the lens of transient dynamical systems. A rough intuition is provided, followed by experiments in a toy concentric circle example and on MNIST. Finally, a method to determine the appropriate depth of ResNets is proposed, though experiments are only performed in the toy concentric circle experiment. The approach of attempting to interpret feedforward networks through a dynamical systems perspective is an interesting and worthwhile one. However, this paper suffers from a number of flaws, and is clearly not ready for publication. The clarity of this paper can be significantly improved. In general, the text is confusing, and as currently written, it is difficult to understand the central narrative of the manuscript. The review of literature in the introduction is relatively complete, though again, the presentation makes this section difficult to understand. Scientifically, it is clear that further experiments on less toy datasets and settings will be required. While MNIST is useful for prototyping, experiments on datasets such as CIFAR (or ideally ImageNet) will be necessary to evaluate whether the observations made hold in more realistic settings. Moreover, the primary claim: that ResNets sum the residuals across layers is by definition true and by design. The scientific contribution of this statement is therefore questionable.In addition, the case analyzed in the majority of the paper -- weight sharing across all layers -- is an unusual one, and as Figure 3 shows, clearly changes the network dynamics. The use of sigmoid activation functions is also an unusual one given that ResNets are generally trained with ReLU activations. Finally, the proposed method for determining the optimal depth for ResNets is an interesting idea, and worth further examination. However, the paper currently evaluates this only on an an extremely toy dataset of concentric circles. Evaluation on more realistic datasets (comparatively) with appropriate baselines will be required to determine whether this method is in fact helpful. This review will unfortunately be very short because I am afraid there is not much to say about this well written paper, which seems to have been sent to the wrong conference. The scientific problem is interesting, namely the detection of topological artifacts in images showing biological phenomena (which I dont know much about). The relevant literature here is basically literature from this field, which is not machine learning and not even image processing. The contribution of the paper, in terms of machine learning, is to apply a well known neural model (YOLO) to detect bounding boxes of objects in images, which are very specific. The contribution here does not lie in machine learning, I am afraid.This is thus a purely experimental paper on a single application, namely object detection in specific images. Unfortunately the experiments are not convincing. The results are validated against a traditional method, which has never been cited, so we do not know what it is.The performance gain obtained with YOLO seems to be minor, although the difference in time complexity is quite enormous (to the advantage of YOLO).The contribution is thus minor and for me does not justify publication at ICLR.The grant number is mentioned in the acknowledgments, which seems to violate double blind policy. This paper introduces the implicit autoencoder, which purports to be a VAE with an implicit encoding and decoding distribution.My principle problem with the paper and reason for my strong rejection is that there appears to be a complete separation between the discussion and theory of the paper and the actual experiments run.  The paper's discussion and theory all centers around rewriting the ordinary ELBO lower bound on the marginal likelihood in equations (4) through (7) where it is shown that this can be recast in the form of two KL divergences, one between the representational joint q(x,z) = p_data(x) encoder(z|x) and the 'reconstruction joint' r(x,z) = encoder_marginal(z) decoder(x|z), and one between the encoding marginal q(z) and the generative prior p(z).   The entire text of the paper then discusses the similarities between this formulation of the objective and some of the alternatives as well as discussing how this objective might behave in various limits.However, this is not the objective that is actually trained. In the  "Training Process" section is it revealed that an ordinary GAN discriminator is trained.  The ordinary GAN objective does not minimize a KL divergence, it is a minimax formulation of a Jensen Shannon divergence as the original GAN paper notes.  More specifically, you can optimize a KL divergence with a GAN, as shown in the f-GAN paper (1606.00709) but this requires attention be paid to the functional form of the loss and structure of the discriminator.  No such care was taken in this case.  As such the training process does not minimize the objective derived or discussed.  Not to mention that in practice a further hack is employed wherein only the negative example passes gradients to the generator.  While is not specified in the training process section, assuming the ordinary GAN objective (Equation 1) is used, according to their own reference (AVB) the optimal decoder should be:  D = 1/(1 + r(z,x)/q(z,x))  for which we have that what they deem the 'generative loss of the reconstruction GAN' is T = log(1 + r(z,x)/q(z,x))  .   When we take unbiased gradients of the expectation of this quantity, we do not obtain an unbiased gradient of the KL divergence between q(z,x) and r(z,x).Throughout the paper, factorized Gaussian distributions are equated with tractable variational approximations.  While it is common to use a mean field gaussian distribution for the decoder in VAEs this is by no means required.  Many papers have investigated the use of more powerful autoregressive or flow based decoders, as this paper itself cites (van der Oord et al. 2016).  The text further misrepresents the current literature when it claims that the IAE uniquely "generalizes the idea of deterministic reconstruction to stochastic reconstruction by learning a decoder distribution that learns to match to the inverse encoder distribution".  All VAEs have employ stochastic reconstruction, if the authors again here meant to distinguish a powerful implicit decoder from a mean field gaussian one, the choice of language here is wrong.Given that there are three joint distributions in equation (the generative model, the representational joint and the reconstruction joint), the use of Conditional entropy H(x|z) and mutual information I(x, z) are ambiguous.  While the particular joint distribution is implied by context in the equations, please spell it out for the reader.The "Global vs. Local Decomposition of Information in IAEs" section conflates dimensionality with information capacity.  While these are likely correlated for real neural networks, at least fundamentally an arbitrary amount of information could be stored in even a 1 dimensional continuous random variable.  This is not addressed.  The actual experiments look nice, its just that objective used to train the resulting networks is not the one presented in the paper. The paper proposes to combine two ideas from previous publications, Fisher-GAN and Deli-GAN, i.e., use a mixture noise (Deli) with Fisher IPM metric for training GAN.The extension of the previous work is trivial and the combination of the two ideas lack of any motivation. The experimental results are also weak. It is certainly below the bar of acceptance. This paper is a straightforward combination of two previous works, Deli-GAN and Fisher GAN. Deli-GAN has a mixture prior distribution in the latent space, while Fisher GAN uses Fisher IPM instead of JSD as objective.Inception score on CIFAR-10 is used to empirically measure quality.Cons:The exposition of the ideas is lacking. What's wrong with Deli-GAN? What is this paper trying to accomplish by incorporating fisher metric?No theoretical justification while empirical results are sparse and unconvincing.Writing quality could be improved throughout the paper in terms of both structure and language.  In summary, this paper is not of the quality that should be accepted by ICLR. The paper gives theorems concerning "dual learning" - that is, makinguse of round-trip consistency in learning of translation and othertasks.There are some interesting ideas here. Unfortunately, I think thereare issues with clarity/choice of notation and correctness (errorsresulting from problems with the notation - or at least it's veryhard to figure out if things are correct under some intepretation).More specifically, I'm uneasy about the use of x^i and x^j as definedin section 2. In some cases x^j is a deterministic function of x^i, insome cases it's a random variable, these cases are mixed. Section 2becomes tangled up in this issue. It would be much better I think todefine a function f_ij(x) for each (i,j) pair that maps a sentencex \in S^i to its correct translation f_ij(x) \in S^j.A critical problem with the paper is that Eq. 2 is I think incorrect.Clearly,Pr(T_ij(x_i) = f_ij(x_i), T_ji(f_ij(x_i)) = x_i)      [1]=Pr(T_ij(x_i) = f_ij(x_i))                             [2]*Pr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i))    [3]I think [1] is what is meant by the left-hand-side of Eq 2 in the paper -though the use of x^j is ambiguous (this ambiguity is a real issue).It can be verified thatPr(T_ij(x_i) = f_ij(x_i)) = p_ijhoweverPr(T_ji(f_ij(x_i)) = x_i) | T_ij(x_i) = f_ij(x_i)) \neq p^r_jiThe definition of p^r_ji is that of a different quantity.This problem unfortunately permeates the statement of theorem 1, andthe proof of theorem 1. It is probably fixable but without asignificantly revised version of the paper a reader/reviewer isbasically guessing what a corrected version of the paper wouldbe. Unfortunately I think publishing the paper with errors such asthis would be a problem.Some other points:[1] The theorem really does have to assume that there is a uniquecorrect translation f_ij(x^i) for each sentence x^i. Having multiplepossible translations breaks things. The authors say in section 2 "Inpractice, we may select a threshold BLEU (Papineni et al., 2002a)score, above which the translation is considered correct": this seemsto imply that the results apply when multiple translations (abovea certain BLEU score) are possible. But my understanding is that thiswill completely break the results (or at least require a significantmodification of the theory).[2] A further problem with ambiguity/notation is that T^d is neverexplicitly defined. Presumably we always have T_ij^s(x^i) = T_ij(x^i) ifT_ji(T_ij(x^i)) = x^i? That needs to be explicitly stated.[3] There may be something interesting in theorem 1 - putting aside point[1] above - but I am just really uneasy with this theorem and its proofgiven that it uses p^r_ji, and the issue with Eq. 2.[4] Another issue with the definition of p^r_ij: the notationP_{X^(j, r) ~ \mu}(...) where the expression ... does not referto X^{j, r} (instead it refers to x^j) is just really odd,and confusing. Review:  the writing is not sufficiently clear and a lot of the ideas are hard to follow (the sections 3.2 and 3.3 which should cover proposed methods are only a paragraph long each, have no loss functions and no architecture descriptions/diagrams) the ideas presented are only derivative and are not sufficiently novel for the venue the experimental section is incomplete having results on one dataset and not enough state-of-the art baselines. the uplfits look small and there is no discussion on statistical significance The idea of learning user embeddings for downstream tasks in recommender systems is a good one.However, this paper proposes no significant methodological developments (e.g., user2vec is an extension of item2vec obtained by transposing the observation matrix). Further, it does not present a thorough study with interesting empirical results (doc2vec does not improve performance, a single dataset is used, baselines are not state of the art).Overall, this short paper (3 pages + refs) seems a bit preliminary and, in its current state, does not make a significant enough contribution to be accepted at this venue.I would suggest that a more thorough analysis of similarity methods for NN models could be interesting to a recsys workshop or perhaps a conference focussed on recsys (e.g., ACM recsys). First off, the paper presents a relatively straight-forward extension to video from the work done in image compression. The work uses 3D volumes instead of 2D images, and exploits this structure by adding a secondary network to both the encoder/decoder.The work is therefore *marginally* novel, but it is one of the first to propose neural methods for compressing video.My biggest complaint about this paper, however, is about evaluation. I don't think it's possible to take this paper seriously as is, due to the fact that the metrics use in the evaluation are absolutely skipped.Given that this is such a crucial detail, I don't think we can accept this paper as is. The metrics need to be described in detail, and they should follow some previously used protocols (see below). For example, in libvpx and libaom (which is the current best performing method for video compression - AV1), there are two versions of PSNR: Global and Average PSNR respectively, and this is what gets reported in publications/standards meetings.Global PSNR: Compute MSE for the entire sequence combining Y, Cb, Cr components, and then compute PSNR based on the combined MSE.Average PSNR: Compute MSE for each frame combining Y, Cb, Cr, components; then compute PSNR for the frame based on the combined MSE and cap it to a max of 100. Then average the PSNR over all the frames.MPEG uses something like computing Average PSNR for each component (similar to what I mentioned above, but for each component) and then combine the Y-, Cb- and Cr- PSNRs using a weighted average. For 420 that will be equivalent to [4*MSE(y) + MSE(Cb) + MSE(Cr)/6. For 422 that will be equivalent to [2*MSE(y) + MSE(Cb) + MSE(Cr)/4. For 444 that will be equivalent to [MSE(y) + MSE(Cb) + MSE(Cr)/3.  Additionally, when using YCbCr, the authors also need to refer to which version of the color standard is employed, since there are multiple ITU recommendations, all of which differ in how to compute the color space transforms.Please note that video codecs DO NOT OPTIMIZE FOR RGB reconstruction (humans are much more sensitive to brightness details than they are to subtle color changes), so comparing against them in that color space puts them at a distinct disadvantage. In the video compression literature NOBODY reports RGB reconstruction metrics.Please note that I computed the PSNR (RGB) for H.264, on the resized MCL-V dataset (640x360) as the authors proposed and I observed that the metric has been ***MISREPRESENTED*** by up to 5dB. This is absolutely not OK because it makes the results presented not be trustworthy at all.Here is the bpp/RGB PSNR that I obtained for H.264 (for completeness, this was computed as follows: used version 3.4.2 of ffmpeg, and the command line is "ffmpeg -i /tmp/test.y4m -c:v h264 -crf 51 -preset veryslow", tried many settings for crf  to be able to get roughly the same bpp per video, then compute RGB PSNR for each frame per video, aggregate over each video, then average cross videos):BPP, Average PSNR RGB (again, not a metric I would like to see used, but for comparison's sake, I computed nonetheless -- also, note that these numbers should not be too far off from computing the average across all frames, since the video length is more or less the same)):0.00719, 23.460.01321, 26.380.02033, 28.920.03285, 31.140.05455, 33.43Similar comments go for MS-SSIM. Lastly, it is unfair to compare against H263/4/5 unless the authors specify what profiles were used an what kind of bitrate targeting methods were used. Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task  as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. Novelty: This reviewer is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. This work addresses the problem of online adapting dynamics models in the context of model-based RL. Learning globally accurate dynamics model is impossible if we consider that environments are dynamic and we can't observe every possible environment state at initial training time. Thus learning dynamics models that can be adapted online fast, to deal with unexpected und never seen before events is an important research problem.This paper proposes to use meta-learning to train an update policy that can update the dynamics model at test time in a sample efficient manner. Two methods are proposed- GrBAL: this method uses MAML for meta-learning- ReBAL: this method trains a recurrent network during meta-training such that it can update the dynamics effectively at test time when the dynamics  changeBoth methods are evaluated on several simulation environments, which show that GrBAL outperforms ReBAL (on average). GrBAL is then evaluated on a real system. The strengths of this paper are:- this work addresses an important problem and is well motivated- experiments on both simulated and on a real system are performedThe weaknesses:- the related work section is biased towards the ML community. There is a ton of work on adapting (inverse) dynamics models in the robotics community. This line of work is almost entirely ignored in this paper. Furthermore some important recent references for model-based RL are not provided in the related work section (PETS [3] and MPPI [2]), although MPPI is the controller that is used in this work as a framework for model-based RL. Additionally, existing work on model-based RL with meta-learning [1] has not been cited. This is unacceptable. - There is no significant technical contribution - the "contribution" is that existing meta-learning methods have been applied to the model-based RL setting. Even if no-one has had that idea before - it would be a minor contribution, but given that there is prior work on meta-learning in the context of model-based RL, this idea itself is not novel anymore.- Two methods are provided, without much analysis. Often authors refer to "our approach" - but it's actually not clear what they mean by our approach. The authors can't claim "model-based meta RL" as their approach. - While I commend the authors for performing both simulation and real-world experiments, I find the that experiments lack a principled evaluation. More details below.Feedback on experiments:Section 6.2 (sample efficiency)You compare apples to oranges here. I have no idea whether your improvements in terms of sample-efficiency are due to using a model-based RL approach or because your deploying meta-learning. It is well known that model-based RL is more sample efficient, but often cannot achieve the same asymptotic performance as model-free RL. Since MPPI is your choice of model-based RL framework, you would have to include an evaluation that shows results on MPPI with model bootstrapping (as presented in [2]) to give us an idea of how much more sample-efficient your approach is.Section 6.3 (fast adaptation and generalization)While in theory one can choose the meta-learning approach independently from the choice of model-based controller, in practice the choice of the MPC method is very important. MPPI can handle model inaccuracies very well - almost to the point where sometimes adaptation is not necessary. You CANNOT evaluate MPPI with online adaptation to another MPC approach with another model-learning approach. This does not give me any information of how your meta-learning improves model-adaptation. In essence these comparisons are meaningless. To make your results more meaningful you need to use the same controller setup (let's say MPPI) and then compare the following:1. MPPI with your meta-trained online adaptation2. MPPI results with a fixed learned dynamics model - this shows us whether online adaptation helps3. results of MPPI with the initial dynamics model (trained in the meta-training phase) -without online adaptation. This will tell us whether the meta-training phase provides a dynamics model that generalizes better (even without online adaptation)4. MPPI with model bootstrapping (as presented in [2]). This will show whether your meta-trained online adaptation actually outperforms simple online model bootstrapping in terms of sample-efficiencyThe key here is that you need to use the same model-based control setup (whether its MPPI or some other method). Otherwise you cannot detangle the effect of controller choice from your meta-learned online adaptation.6.4 Real-world: same comments as above, comparisons are not meaningful[1] Meta Reinforcement Learning with Latent Variable Gaussian Processes, UAI 2018[2] MPPI with model-bootstrapping: Information Theoretic MPC for Model-Based Reinforcement Learning , ICRA 2017[3] Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models, NIPS 2018 The authors propose to estimate individualized treatment effects in a setting they describe as continual learning, batches of data becoming available over time. The technical proposal is to store a feature representation of the presently available data to be updated with new data in the future -- a technique that, together with other regularization methods for selection bias and variable selection, is shown to perform well on synthetic experiments. Causal inference is increasingly relevant as machine learning assists decision-making, and an important aspect of this process is to handle information streams over time and update accordingly, just as humans would be expected to. In the context of causality though, I believe this requires a more detailed motivation. Causal effects by their very definition exhibit invariance to interventions on observed variables, and if properly estimated, these should not vary between well-behaved environments. With enough data a priori there should not be large changes in estimation in new environments. This may certainly change if the underlying causal mechanisms change and is a problem that has been studied as data fusion, yet this line of research is not mentioned in the paper.Writing needs to improve substantially. The use of the words extensibility, adaptability, and accessibility is grammatically incorrect, and the context in which they are used does not clarify the meaning of these ideas either. As I understand it, adaptability refers to domain adaptation yet how domains differ, how to merge them depending on their differences, or even an investigation of these problems is not discussed. Accessibility refers to the amounts of data one needs to deal with. This does not strike me as a problem in causal inference where datasets are typically small. Perhaps examples of applications where each of these aspects is a concern would be helpful.More importantly, the notation is counter-intuitive and inconsistent. For instance, \mathcal X is defined as the set of all observed variables, but then a specific realization x is written to be an element of \mathcal X (as if \mathcal X was a measurable space of possible realizations of a random variable). Both interpretations cannot be correct. The notation \mathcal X_d is similarly ambiguous, does this mean each domain has different sets of variables or that the spaces in which they are defined differ? The experiments are underwhelming and certainly do not back up the claims made in the introduction. Most benchmarks for individualized treatment effects are semi-synthetic (it would be advisable to stick to these data generating processes while modifying them to highlight specific features of your problem). Many more competing algorithms could be evaluated. There has been a recent surge of research interest in explaining the double descent phenomenon, which contradicts the classical bias-variance tradeoff and may contribute to the success of DNNs and other overparametrized models. This paper proposes to mitigate double descent by artificially augmenting the dataset with concatenated inputs, and presents some empirical results for linear regression and neural networks.Although the paper is on a timely and interesting topic, the idea of using data augmentation to mitigate overfitting is certainly not new and has been understood in many areas of machine learning. In fact, augmenting the dataset with duplicates or artificially constructed data is equivalent to incorporating certain prior structure or imposing some form of regularization, which in turn should have an impact on the double descent curve. On the theoretical side, the paper does not provide any rigorous theoretical analysis; on the methodological side, the data augmentation strategy is defined in a restrictive manner and seems to be of little practical value.More comments:1) Figures 2-4: It is difficult to decipher these figures without legends. I cant find anywhere in the text or captions mentioning what the orange and blue curves mean. Also, how are the loss and error defined?2) The paper argues that the sample size, and hence overparametrization and underparametrization, in the double descent phenomenon is ambiguous and proposes to increase the sample size by artificially augmenting the dataset. In fact, the notion of sample size usually comes with the i.i.d. assumption, i.e., the samples are drawn independently from a common distribution. In this sense, augmenting the dataset with concatenated samples does not directly increase the sample size. Different ways of constructing the concatenated samples lead to different effective sample sizes, which should be properly defined in order to study their impact on the double descent curve.3) The paper aims to mitigate the double descent curve, which, however, is not necessarily harmful. A more critical question is how the lowest test error in the overparametrized regime compares to the optimal test error in the traditional underparametrized regime. If double descent helps to generalize well, why should we mitigate it? --Summary--This paper introduces a method for matching an ML pipeline to a dataset using the text features of the pipeline of and the dataset along with some additional metadata about the features of the data. They evaluate and train their model on ~20 datasets and ~5 pipelines.--Strengths--The authors introduce an approach to pick a training pipeline for a dataset. I like the fact that the authors provided useful illustrations and clear notation.--Weaknesses--This paper suffers from several issues:**Low impact**The claim of real-time AutoML is exaggerated:a) The proposed approach does not work for general AutoML tasks and search spaces such as architecture search, swapping pipeline pieces in and out. Instead it matches datasets to a handful of pre-existing pipelines (~5).b) Training the model can severely dominate the suggestion time for the proposed algorithm.  This approach focuses on the speed of the first suggestion (zero-shot setting). c) How do you recover if you make a bad decision about a dataset/algo? The proposed algorithm is not iterative.d) Are text features sufficient? What about datasets and pipelines which sound similar but are totally different.**Low novelty**a) This is a method for matching a dataset to a training pipeline largely using text features from their descriptions/documentation. This is a common information retrieval task.b) Its unclear why a GNN and a transformer is needed for a simple feature matching task. Standard retrieval and ranking algorithms such as vector dot product, TF-IDF or LSA could have sufficed.c) Edges in the GNN are chosen using a kNN threshold applied to the node representations. No structure learning is performed.**Weak results**a )The number of datasets (around 20) and pipelines (around 5)  that this approach is evaluated on is woefully inadequate. The results could easily suffer from overfitting.b) For some datasets test performance is already at 100% e.g. Cardi, Wallrobot, Mofn. These seem like easy tasks. More complex and difficult datasets are needed.c) No comparison with other methods. e.g. simple information retreival method such as vector dot product, tf-idf or LSA.**Technical issues:**a) Description of training setup is lacking. Is it training on the same set of datasets? If so theres leak of training -> test data.b) What is the total training time? If it dominates the suggestion. Then this method is not useful since compute has already been spent on fully training the datasets.---What can make this paper better?---- The core approach fundamentally needs improvement. A simple pipeline to dataset matching is unlikely to have a significant contribution to science or application.-  The authors need much higher quality resultsa) Many more datasetsb) Difficult datasets: Not ones that achieve close to 100% accuracy.c) Many more pipelines or complexity of pipeline components.- To claim Real-time AutoML the authors need to demonstrate that the method worksa) For general AutoML tasks and search spaces such as architecture search, swapping pipeline pieces in and out.b) The method is iterative in nature. Otherwise it is impossible to recover from poor choices of pipelines.c) The real-time claim implies an online setting, wherein results are fed back and suggestions are offered in real time. The proposed approach does not have the ability to learn on the fly. The current GNN and Transformer representations will be costly to re-train. This paper studies the problem of providing calibrated predictions for out-of-distribution data. They propose algorithms for both calibrating predictions given a single image from the unknown distribution as well as given multiple images from the unknown distribution. They propose an algorithm that estimates which calibration distribution the novel image came from, and then use calibrated predictions for this distribution. They evaluate their approach on a standard image datasets including CIFAR-10 and ImageNet, and show that their approach outperforms existing work.Pros- Important problemCons- The approach claims to work on out-of-distribution data, but assumes the possible novel distributions are known- Missing related workThe approach proposed by the authors is fundamentally flawed: while they do not directly assume to know which unknown distribution the novel image is from, they assume it is from one of a small set of possibilities. This information is not assumed in existing work, and fundamentally alters the problem, making it simple to address and uninteresting.The proposed approach is also very simplistic, which is not a flaw in and of itself but is a consequence of the extra knowledge they assume. In particular, given this extra information, the authors simply predict which shifted distribution the novel example is from, and then use the calibrated prediction for that distribution.In practice, this problem is important for handling unanticipated distribution shifts in production. If the distribution shift is known and anticipated, then a much more natural approach would be to simply use data augmentation to generate data from the shifted distribution and train the model on this extra data.In addition, there recent work in this area that the authors do not cite, for instance:Park et al., Calibrated Prediction with Covariate Shift via Unsupervised Domain Adaptation. In AISTATS 2020.Wang et al., Transferable Calibration with Lower Bias and Variance in Domain Adaptation. In NeurIPS 2020. This paper introduces "Intervention GAN (IVGAN)", an algorithm proposed to stabilize the training process os GANs as well as alleviating the mode collapse problem. In IVGAN the authors feed O(x) to the generator with O different possible transformations and train the discriminator to also differentiate which transformation originated the sample.The approach is extremely poorly motivated. Why transformations like this have anything to do with mode colapse or training stability is not at all at the level of this conference. This paper is literally written as "We came up with this method and we tried out whether it works or not, the conclusion is it doesn't really hurt in our limited experiments, sometimes it helps by an epsilon". This is really not up to the standards of ICLR, I would expect at least a tiny step towards solving a fundamental problem with a clear motivation and research path, or at the very least better experimental results.There is no quantification of training stability, and the only qualitative experiment in that regard is figure 5, which doesn't really provide a strong argument for the model in my opinion. If they showed they could train faster difficult models (such as bigger ones like SGAN) or they could train models for new tasks where previous methods fail that would be better, but this is really quite underwhelming.The only quantification for mode dropping is on mnist, a dataset that's widely accepted as useless in the current state of generative modelling. Furthermore, important baselines like wgangp are missing.Furthermore, the language and the overclaiming of the paper is very strong: "our approach is able to stabilize GAN's training and improve the quality and diversity of generated samples as well". Do the authors really think that their approach was finaly able to stabilize GAN training? The way these sentences are written make it seem like IVGAN is as easy to train in imagenet as training a resnet with cross-entropy for supervised learning. It is simply unacceptable given the limited experiments (tiny datasets, small improvements, far from the current state of generative modelling).Honestly, I feel very dissapointed about how this paper presents itself. Poorly motivated ideas with an epsilon improvement are not meant for top tier conferences, and the degree to what this paper overclaims is not acceptable either. This paper tries to take a GAN-based approach to monophonic music composition. They introduce a concept of "skill labeling" which they use to condition their GAN so that the generated music has certain bar-by-bar characteristics. They evaluate the generated music by comparing it with human and computer baselines in a survey.Before getting into specific issues, there has been a lot of research into symbolic music generation over the past 60 years, and the issues encountered are usually related to generating structurally coherent music. A sophisticated hand-engineered markov chain can incorporate domain knowledge and generate music which is locally coherent, but even contemporary methods like MIDINet which model longer-term structure struggle to create globally coherent music. The model presented here doesn't address this limitation in the existing literature and, arguably, doesn't generate locally coherent music either.More details on your survey methodology would help strengthen your case. There is a lot of evidence that surveys are a poor way to evaluate creativity in music composition (e.g. http://ccg.doc.gold.ac.uk/ccg_old/events/ecai06/proceedings/Moffat.pdf) because it is easy to unintentionally mislead or bias your participants. Your method also only scores the best based on novelty, but fails on your other questions. That's not very strong evidence in favor of it over MIDINet.Specifically:2.1 This section is both unnecessary and unusual. You use the term "polyphonic rhythm" when you may mean "homophonic texture." You shouldn't introduce new terms for concepts which are already well understood in music theory.2.2 Why do you use a gaussian filter over your binary matrix? Can you provide a citation or experimental evidence for that decision?2.4 You cite Mirza and Osindero, no need to restate these equations.2.5 Empty section?3.1 If you're using a dataset from MIREX, why don't you provide results versus their benchmarks? The melody continuation task has clear evaluation measures, which are more reliable than a survey.4 The music you show here is rhythmically novel, but looks like it was sampled from a Bernoulli distribution. It would be difficult to perform and harder to remember.Generally, there are also a lot of spelling and grammatical errors. Please spend more time editing your writing before submitting.Despite these issues, the idea of conditioning your GAN on descriptors is good! If there was some way to describe musical structure at multiple levels, you may be able to generate something really musical. Summary:In this paper the authors propose a method for generating music using a GAN. They claim to have improved performance by incorporating domain knowledge. Unfortunately, the evidence of this, even by their own metrics, is somewhat lacking. Despite this, their contribution could have been notable if they had detailed their feature extraction and evaluated the impact/benefit of each feature upon performance, but this was also not done. Overall, even by the metrics provided in the paper, the contribution is very unclear. Without clarification of this, and addressing the above comments, I would recommend this paper is rejected.The good things:* Code is released with this paper which helped me to understand the feature generation and modelling process* Figure 2: explains the modelling process relatively well on a high level* The authors make use of a publicly available dataset* The authors provide examples for comparison with another methodThings for improvement:* Descriptions could be much clearer: for instance - I have interpreted the meaning of "musical skill"/"skill labels" to mean "bar level feature". There are other terms used which do not match well with the english meanings.* Details of these "skill labels" should be provided: this seem to be the main contribution of the work, but there is no explicit discussion of the features derived from the bars of the music.* The data being used for modelling needs clarifying: In 3.1 Dataset, the authors state they are using the monophonic data from PPDD, but modelling is done for monophonic and monophonic + chords. In the final para of 3.3 it is implied that these chords are added to each bar randomly but the process is very unclear. If this is a random process, a justification is required as to why the learned representations are interesting.* The evaluation metrics are not well grounded: the authors pick 4 terms by which participants should give a score of max 5 (is 1 the minimum or 0?). There is no link to the literature that these are good/accepted terms to use. At minimum, there should be some discussion as to why these specific terms were selected and a comment made about. Also, on page 7, the authors begin to refer to "stability" is such a way that it makes me think it was a previously evaluated but now omitted term.* Additional justification is required for some claims: for instance, in the abstract the authors state "However, almost in [sic] these studies, handling domain knowledge of music was omitted or considered a difficult task." but there is no justification of this in the text (one approach from the literature is given in the introduction, but there have been many other approaches).* Figure/table captions should be more descriptive: for example, Table 1' caption reads "Human evaluation result". I would propose something more like: "Human survey comparison of model performance against true data: 15 participants were surveyed and asked to score examples on a scale of 0(?)-5 for the qualities listed. The rows listed as human music are real examples taken from the Lakh midi dataset." (I would also: add the standard deviations to the results, since they are calculable and would help establish significance, and round all values to a consistent and appropriate level) This paper purports to be about Hybrid vehicle fuel optimization using RL. In reality, most of the content seems to be a long derivation of a continuous-space generic RL based controller for a trajectory optimization problem. There does not seem to be much that is  novel in this entire presentation and if there is, the authors have not explicated it in the introduction or other sections. In the last section a very brief simulation-based experiment is described supporting the hybrid vehicle part. The bulk of the paper derives a controller using what seems to me to be standard ideas or minor variations thereof. Authors or other reviewers can correct me if I'm wrong but if so, the sheer density of  sec 2, esp 2.2 esp. made it hard to assess this (and I lacked motivation given what I perceived about the overall paper structure), so I would still argue for reject on clarity grounds.The introduction has a comprehensive literature review of methods that have been used for open loop trajectory control, fuel optimization and so on.  This should be appreciated.Coming to the claims in the introduction: First we solve an open loop trajectory optimization problem, then design an RL controller to track the nominal trajectory. There is nothing new in this idea. The second point brings up the use of something called Concurrent Learning as a contribution which is mentioned exactly once again in sec 2.1 in passing and never defined or referred to again. Perhaps 2.2 onwards describes a use of it but I can't tell.  Contribution #3 seems to be that using H-\infinity as a performance measure for an RL control algorithm is proposed for the first time, but again no description or justification is given.Sec 2: up to 2.1 seems pretty straightforward. 2.2: Assumption 2: is g^+ the usual psuedo-inverse? It is usually defined with an extra A^T at the end.  is there a typo in assumption 3?  from that condition it seems that gg^+ should be identity, i dont think this is intended. Where is novelty in the rest of this section? Using HJB etc is standard. The description of various approximation schemes is ok but doesnt lead anywhere, unless 2.2.1-2 represents Concurrent learning.  sec 2.2.1 just seems to say we can approximate the dynamics using an NN. sec 2.2.2 approximates the value function using a NN as well. The universal function approximation theorem etc is invoked, but what new thing is really being said about VF approximation? I couldn't tell. What is the CL-based update law?2.2.4: I dont remember enough about lyapunov conditions etc to dive deeply into this.  I know that lyapunov conditions are used to show stability of controllers. Is this a straightforward application of this method then?sec 3: Since the main thrust of the paper is fuel optimization, absolutely no explanation or citation is given for why these particular system dynamics are relevant. Indeed, they seem far simpler than I would expect such a complex system to have, but I am definitely no expert. The results seem fine but not enough to justify this paper for ICLR as a deep and original contribution on learning representations. The paper proposes a data augmentation technique where source sentences are perturbed by replacing (or mixing) source words with their aligned counterparts from the target language (while the target sentences remain as is). Alignments can be either obtained from an unsupervised aligner like fast-align or from the attention distribution of an NMT model. Perturbations are aimed to be semantically invariant to preserve the meaning of the source sentence. In addition to simply replacing the source word with the aligned word, authors also try out inputting a weighted combination of both the source word and the target word and refer to this method as mixing. Empirical observations suggest that simply replacing the source word with the aligned target yields better results.I believe the paper in the current form has a lot of scope for improvement. The experimental section needs to be strengthened and more thought needs to go into improving the proposed method.I have the following suggestions for improving the paper:More experiments: The experiments are based on a reasonably large translation dataset, while the paper "claims" to improve the performance for low-resource NMT in section 4.5> Specially, we find that our method works better on a low resource settings Summary of claims:The paper studies the estimation of the calibration error of models, which is a problem of increasing relevance due to the rich settings that modern ML systems are being employed in. In particular, the focus is on mitigating the _bias_ in the commonly used binned estimators. The authors make the case that the biases of the binned estimators vary with the number of bins and the number of samples in a nontrivial manner, which is detrimental to point estimation. As a form of autotuning the number of bins that should be used, the  authors use the fact that the true calibration curve should be monotone increasing to propose choosing the number such that the resulting estimates of the average values of the $y$s in the resulting bins are monotonic. The bias of this estimator is then demonstrated on a number of simulations for which the laws of the classifier outputs and the calibration curves are obtained by fitting to models trained on standard datasets.----Strengths:The problem being studied is certainly relevant, and the work is well contexualised. I found the idea of using the monotonicity of the $\{\\overline{y}_k\}$ as a criterion to choose an appropriate binning scale to be clever. I also like the problem of testing if the true calibration error is large or not. Finally, the method of section 5.1 is a nice way of generating realistic laws and calibration curves, and adds meaningful depth to the simulations.----Weaknesses: 1\) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).2\) I don't think the baseline comparisons made in the experiments are appropriate. The proposal is a method to choose the appropriate number of bins in the estimate, and should be compared to other methods to do so instead of to an arbitrary choice of number of bins as is done in section 5.2. Without this comparison, I have no way to judge if this is a good autotuning method or not. Reasonable comparisons could be, e.g., choosing $b$ by cross validation, or, in equal mass binning, choosing $b$ so that each bin has a reasonable number of samples for the error $\overline{y}_k$ to not be too large.3\) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of  ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.4\) Choice of law in simulation in section 3, which are used to illustrate the dependence of bias on the number of bins, not aligned with the laws/curves in figure 3. Taking the latter as representative of the sort of laws and calibration curves that arise in practice, there are two issues:4a\) The pdfs of $f$ tend to be a lot more peaked near the end than the one explored in section 3 - this is borne out by the values of $\alpha, \beta$ in the fits in Table 1. Beta(1.1,1) is remarkably flat compared to the curves in Fig 3.4b\) There seem to be a few different qualitative properties of the calibration curves - monotone but with a large intercept at 0; those with an inflection point in the middle; and those with the bulk lying below the $y=x$ line. In particular, all of them tend to have at least some region above the $y=x$ line. The choice of curve $c^2$ in section 3 doesn't completely align with any of these cases, but even if we make the case that it aligns with the third type, this leaves two qualitative behaviours unexplored. In fact, the choice of laws is such that the error of the hard classifier that thresholds $f$ at $1/2$ is $26\\%$. I don't think we're usually interested in the calibration of a predictor as poor as this in practice.All of this makes me question the relevance of this simulation. Is the dependence of the bias on the number of bins as strong for the estimated laws as it is for these? Seeing the the equivalents of figs 7 and 8 for the laws from section 5 would go a long way in sorting this out.5\) Experiments: As I previously mentioned, I don't think the correct baselines are compared to. Instead of posing the method against other autotuning schemes, just one choice of the number of bins is taken. This already makes it near impossible to judge the efficacy of this method.Despite this, even the data presented does not make a clear case for ECE_sweep. In Fig. 4 we see that the bias of EW_sweep is even worse than EW. This already means that the sweep estimate doesn't fix the issues of ECE_bin in all contexts. It _is_ the case that EM_sweep has better bias than EM, but again, for samples large enough for the variances to be in control, it seems like these numbers are both converging to the same, so I don't see any distinct advantage when it comes to estimation. (of course, this is moot because this isn't the right comparison anyway)Also, Fig. 5 is flawed because it compares EW and EM_sweep. It should either compare EM and EM_sweep, or EW and EW_sweep, I don't see why EW and EM_sweep are directly comparable.Minor issues:a\) Algorithm (1) and the formula for ECE_sweep in section 4 don't compute the same thing. In algorithm (1), you find the largest $b$ such that the resulting $\{\overline{y}_k\}$ is a monotone sequence, and return the ECE_bin for this number of bins. In the formula, you maximise the ECE_bin for all b that yield a monotone $\{\overline{y}_k\}$. From the preceding text, I assumed that the quantity in Algorithm (1) is intended. b\) Why is the $L_p$ norm definition of the ECEs introduced at all? In the paper only $p = 2$ is used throughout. I feel like the $p$ just complicates things without adding much - even if you only present the $L_2$ definition, the fact that a generic $p$ can be used instead should be obvious to the audience.c\) Design considerations for ECE_sweep - it is worth noting that accuracy is not all that we want in an estimate of calibration error. For instance, one might reasonably want to add this as a regulariser when training a model in order to obtain better calibrated solutions. One issue with ECE_sweep is that it introduces a problem in that how the number of bins in the ECE_sweep estimate changes with a small change in model parameters seems very difficult to handle, which makes this a nondifferentiable loss. Broader issues of this form, and a discussion of how they may be mitigated, could lead to a more well rounded paper.----Comments:a\) Exact monotonicity in the ECE_sweep proposal - I find the argument stemming from the monotonicity of the true calibration curve, and the idea to use this to nail down a maximum binning size interesting. However, why should we demand exact monotonicity in the bin heights? Each $\\overline{y}_k$ will have noise at the scale of  roughly $\sqrt{b/n},$ (for equal mass binning with $b$ bins), and in my opinion, violation of monotonicity at this scale should not be penalised. Also, what if a few $\overline{y}_k$s decrease but most are increasing (i.e., the sequence has a few falling regions, but the bulk is increasing)? Perhaps instead of dealing with this crudely, the error of a shape constrained estimator may serve as a better proxy.b\) Isn't the procedure for parametrically fitting the pdf of $f$, and $\\mathbb{E}[Y|f(X)],$ and then integrating the bias a completely different estimator for TCE of a model? In fact, if the laws are a good fit, as is claimed in section 5.1, then this plug in estimator might do well simply because the integration is exact. In fact, since the fit is parametric, this can further be automatically differentiated (if, say, $f$ were a DNN), and thus used to train.c\) It would be interesting to see what number of bins are ultimately adopted in the ECE_sweep computations that are performed. ----Overall opinion: The lack of comparison to appropriate baselines makes it near impossible for me to judge the validity of the proposed estimator. I feel like this is a deep methodological flaw when it comes to evaluating the main proposal of the paper. This is a real pity because I quite like some of the ideas in the paper.Due to the inability to evaluate the main contribution of the paper, i am rating it a strong reject. I'd be completely open to re-rating it if appropriate comparisons are performed, and the case for the method is properly made. This submission studies how the choice of activation function impacts the reproducibility of experiments involving deep networks. It proposes a new activation function with the goal of designing a smoothed ReLU, and provide experiments comparing it against other activations in terms of irreproducibility (measured via PD) and performance.The problem of understanding how model design choices can have negative impacts on experimental reproducibility is interesting and timely, but I believe the paper does not provide a strong enough case for their approach and contributions.First, the adopted metric to measure irreproducibility, 'Prediction Difference (PD)', is never evaluated in terms of how sensible of a metric it is to capture reproducibility -- this also seems to be lacking in [1]. Actually , one can argue that it is not a sensible metric at all (except for its Hamming form), as it is not invariant to how the models are calibrated, as discussed below.For example, take any binary classifier and consider two copies of it with different calibrations (i.e. scaling the output layer weights by positive scalars, one for each model): even though the models always agree on their predicted labels regardless of their calibrations, the PD can be made arbitrarily close to 0.5 by calibrating the models appropriately. Even more worrying is that the same can be done by taking a binary classifier and a copy of it with flipped predictions: the PD between the two can be made arbitrarily close to 0 by scaling their weights down. Note that this problem also happens with the relative PD.To see how this is connected to the choice of activation functions (especially ReLU x SmeLU), note that for normally-distributed inputs (centered around the origin), gradient's variance of ReLU is 1/4 while for SmeLU it is approximately sigma^2 / (4 beta^2) for large enough beta (sigma^2 being the variance of the input distribution): this discrepancy can have a non-trivial impact on the model's calibration and cause differences in PD to be artifacts.Since this doesn't happen with the Hamming form of PD I believe Figure 15 in the Appendix to be the most informative one. However, it seems that different activations result in less than 1% prediction discrepancy across models, which is fairly insignificant and hence it is hard to argue that activations actually matter for reproducibility (at least from the presented experiments).Lastly, it is hard to draw any conclusions from the presented experiments: the CTR results are based on a private dataset while the MNIST ones are extremely small-scale, with both the dataset and the model being arguably toy problems. There are numerous tasks where reproducibility is a prominent issue e.g. deep reinforcement learning, generative modelling (especially GANs), making training a 2-layer network on MNIST a poor choice to evaluate reproducibility problems.As an additional note, the authors seem to rely heavily on the work of Shamir & Coviello '20 [1] which introduced the PD metric, even though the paper was only made publicly available on arXiv a week --after-- the reviewing period for this submission started. When citing papers which are yet to be made available it would be helpful to introduce and discuss the relevant content in a self-contained way -- while the authors avoided much of my confusion by presenting the full definition of the PD metric, the referred paper has useful information which was not discussed (such as which summand is normalized in its relative form and how the different variants compare).Since I have major concerns with the paper -- particularly on the reliability of PD as a metric and the unconvincing empirical results -- I am voting for rejection.[1] Shamir & Coviello, Anti-Distillation: Improving reproducibility of deep networks, In this paper, the authors present an exploration of learning reward signals and how various reward signals can be adaptively used to provide a single reward feedback channel for a reinforcement learning process. First, they provide a solid introduction and related work research on multi-objective reinforcement learning. I would urge the authors not to present statements about work that they will continue to work on or work towards in the future. These forward-looking statements do not serve the purpose of the current work. A great deal of real estate in this paper is committed to background information on related work reinforcement learning and reward shaping which does not serve this paper. Indeed it is not until section 4.2 Methods: Selection when the author's finally present details on new methods and experimental design.The results and figures presented are unclear and I would urge the authors to reconsider the information that they would present in terms of mean and measure of central tendency. It is hard to compare the significance of their results against their baseline methods. This paper feels incomplete to me, and I would like to see this research continued toward a more full submission. Finally, I do not feel as though this paper is suitable for the general audience at ICLR. Perhaps there would be audiences more interested in the specific application and innovations in reward shaping at other workshops. Summary: The ensemble method MIMO is proposed in this paper to reduce the inference delay and keep the prediction diversity. Only one model with sufficient capacity exists in this method while multiple implicit subnets are embedded in the parent model. Each subnet has individual I/O, so only one forward pass of the parent model is needed to process all subnets and make the ensemble.Quality: Medium to high. Pros: 1) The dissections of the subnets are impressive. They design the experiments to survey the loss plane, the parameter/activation projection of different subnets. 2) With the proposed training paradigm and proper test setting, the accuracy improvement can be seen and also uncertainty estimation. 3) They report the SOTA results of accuracy, uncertainty, and robustness on various datasets and their OOD variants when considering the inference latency. Cons: 1) They only report the inference time of one sample, but the total computation costs (e.g. MACs or FLOPS) are omitted. 2) The multiple branch networks are not only used in ensemble and broader usages exist. I know at least three works which involves multi-branch architecture (output-wise or layer-wise) in robustness or knowledge distillation and some of them are also using the ensemble of multiple predictions. The lack of citations in the related work seriously declines the quality of this work.Clarity: High. Pros: 1) The method framework has a brief and clear explanation (Figure 1). 2) The training methods are conveyed in every detail, including some techniques like input repetition. 3) Some critical plots reporting accuracy using error bars or box plots to display the performance variance. Cons: Some format mistakes of symbols are existing. For example, the authors mix the usage of the normal and italic font of x when referring to samples in different expressions; the chaotic usage can even happen in the same equation for the first one of Section 3.3.Originality: Low to medium. Pros: This method successfully uses the multi-branch architecture to reduce the inference delay in ensemble, which is a rather novel idea. Cons: As mentioned above, many similar multi-branch architectures have been used in different but related topics. Consider all of these works, the originality of this work has to be downgraded.Significance: Low to medium. Pros: The shining points of this work are making ensemble for free. They do decrease the inference delay significantly. Cons: 1) The authors attempt to distract our attention on the computational costs of this MIMO architecture and try to make an illusion that its convenient in computing. The latency is decreased at the cost of batch size. Other ensemble methods have a longer delay, but they can process a batch of images. This method fills the batch with the same image which implicitly decreases the batch size. Furthermore, no measurement is used on computation costs or other related aspects. I think its deceptive and tricky. 2) As shown above, the originality of this work is not as solid as their experiments. [ Detailed comments]1. What are the structural considerations for the related work of Section 5 not to be explained in Section 2?2. In Figure 6, the # marked on the abscissa of (b) is redundant Summary-------------This paper defines encoding and decoding procedures which use transformations inspired by Gaussian mixture models (GMMs). The decoding procedure further involves "sharpening" steps. A heuristic for training the parameters shared by the encoder and decoder is proposed which optimizes the likelihoods of GMMs defined on various outputs of the encoder. The decoder is evaluated in terms of clustering performance, sample quality, and outlier detection.Quality (1/5)-----------------It seems like a stretch to call the proposed model a "deep Gaussian mixture model" or a probabilistic model at all. For a probabilistic model we should be able to assign a probability to any (measurable) set of the input space. (For some models such as GANs this probability is intractable but it is still easily defined.) However, it is not clear from the model's description (Section 3) what this probability should be. This lack of a well defined density (or measure) is surprising given the authors' emphasis on "density estimation" as a "main objective" of probabilistic image modeling and a potential application of their model.One could view the entire decoding process as a complicated generative model which involves an iterative sharpening procedure, but this is not how the model is presented. In particular, the training procedure does not seem to be optimizing any divergence of this model and it is not clear how the encoder relates to the posterior of this model.The output of the GMM layer ("responsibilities") live on a simplex (Eq. 2). If we stack two GMM layers, doesn't the likelihood of the second GMM explode (since the differential entropy of the inputs is negative infinity)?  Is suspect the reason that the training loss doesn't explode may be an artefact of SGD and/or the pooling layers.Clarity (3/5)----------------I appreciated that the encoding and decoding procedure as well as the training objective were clearly described.On the other hand, already in the abstract and the first two paragraphs the authors make confusing claims such as the following:(1) The authors claim in the abstract that "DCGMMs can be trained end-to-end by SGD" and that this "sets them apart from vanilla GMMs which are trained by EM, requiring a prior k-means initialization". But vanilla GMMs may very well be trained with SGD as the authors note themselves in the related work section. While k-means may speed up training, it is not "required" by GMMs.(2) The authors claim that since "images usually do not precisely follow a GMM distribution [...] clustering is not a main objective [of image modeling]." This seems like a non-sequitur.(3) "An issue with GANs is that their probabilistic interpretation remains unclear. This is outlined by the fact that there is no easy-to-compute probabilistic measure of the current fit-to-data that is optimized by GAN training." I would argue that the divergence(s) (approximately) optimized by GANs as well as their probabilistic interpretation are much better understood than the proposed model, as discussed above.The authors point out that "training GMMs by SGD is challenging" because the covariance matrices are constraint to be positive definite. Isn't reparametrization relatively easy (C = AA')? And don't you have the same issue in your model (Eq. 2)? How do you enforce positive definiteness in your model?Originality (2/5)---------------------I would have expected a more thorough comparison with deep GMMs (van den Oord & Schrauwen, 2014) which appears to be the most closely related model.Another line of research not being discussed are autoregressive Gaussian mixture models (e.g., Domke, 2008; Hosseini et al., 2011; Theis et al., 2012). These models generalize Gaussian mixture models and are able to efficiently model images of arbitrary size by (like convolutions) making a stationarity assumption. Deep extensions exist as well (Theis & Bethge, 2015).Significance (1/5)-----------------------A lack of conceptual insights or a principled motivation would be fine if the empirical results made up for it. Unfortunately the empirical evaluation seems rather limited as well. No comparisons were made to previously published baselines. Instead, all results are only compared to other DCGMM results provided by the authors.The chosen tasks and datasets (MNIST, FashionMNIST) are rather limited as well. In particular, unconditional image generation is not a well defined task (and certainly not a "main objective" of image modeling) but rather a (poor) proxy for evaluating generative models (see Theis et al., 2016). ## Summary of paperThis paper introduces a novel regularized mean-squared projected bellman objective and corresponding GTD2-like algorithm which minimizes the objective. The paper analytically investigates the convergence rate of the proposed algorithm, then empirically investigates the performance of the algorithm across several problems with linear function approximation.## Summary of reviewThis paper is a clear reject for me. There appear to be significant issues in both the analytical section and the empirical section; which in total bring into question the utility of the proposed algorithm. The literature review also appears to be lacking and there appear to be several minor incorrect statements throughout the paper. I feel quite confident in my evaluation of the empirical section and literature review. I feel confident that there is a bug/typo/incorrect result in the analytical section. I did not attempt to debug the proof to determine which of the three (bug, typo, or incorrect result) was true. Details follow roughly in order of greatest concern -> least concern.### Proof of convergence rateI follow the proof up to equation (12). This is a standard result from (e.g) Maei's 2011 thesis. After equation (12) I follow the transformation that solved for $\rho_{n+1} - \rho_n$ resulting in a matrix inverse. However, after distributing the matrix inverse, I remain confused why $\sqrt{\xi} g_{n+1}$ is not multiplied by the inverted matrix? This, however, may not change the resulting eigenvalues so I do not believe it will change the result. Continuing on to equation (14), I did not check for correctness solving the polynomial for the eigenvalues, so I will rely on the given result in the paper. The conclusion of which states: $\lambda = - \frac{1}{2} (\alpha^{-1} + \lambda_G) \pm \frac{1}{2} \sqrt{\text{thing}} < -\lambda_G$. Ignoring a few details in the middle, the result ultimately states $\lambda < -\lambda_G$. Here lies the fundamental problem. Because GTD2 is known to be a convergent linear system, then we know $\lambda_G$ are all strictly negative (proof of this in Maei's thesis) under some assumptions (notably the invertibility of $X X^\top$). For such a linear system to be convergent, we **must** have the real part of the eigenvalues be strictly negative. The proof under equation (14) states $\lambda < -\lambda_G$, which because $\lambda_G$ is strictly negative, means that $\lambda < \text{some positive number}$. This (a) tells us nothing of the comparative convergence rates since $\lambda$ could be larger than $\lambda_G$, and (b) also suggests that there could be cases where the proposed algorithm does not even converge in the first place because $\lambda$ could be greater than or equal to 0. This could be a typo and the negative sign on the right shouldn't exist, but because there are few details from equation (14) to the end, I was unable to debug and decide if this is typo/bug/incorrect conclusion.There a few other issues with the proof that concern me.* Do we know that the upper-bound on $\kappa$ is reasonable? The quantity $\alpha (\lambda_G - \alpha^{-1})^2 / 4$ is difficult to interpret, but the reliance on $\alpha^{-1}$ seems to imply a preference toward much smaller stepsizes; which seems counter-intuitive towards the goal of improving convergence rates.* Likewise, the first part of the solution for $\lambda$ we have $-\frac{1}{2} (\alpha^{-1} - \lambda_G)$ which likewise implies that smaller stepsizes significantly improve convergence rate (small $\alpha$ implies highly negative $\lambda$). Considering the remainder of the solution for $\lambda$ is under a square-root, this first factor appears to be dominant (though I could be wrong on this, more insight would be appreciated!). A stepsize approaching 0 would yield the fastest convergence rate (apparently) by having the smallest eigenvalue approach negative infinity. This simply does not pass the "smell test".* Further building on my previous point, for large enough values of $\alpha$ we will be in a situation where $\alpha^{-1} < |\lambda_G|$ meaning that the first term will become positive. Considering that the second term is a plus-or-minus, then we could be adding a positive term to a positive term yielding a positive eigenvalue. This means that for large enough $\alpha$ we don't have convergence any longer. I wonder how feasible the upper-bound on $\alpha$ is to guarantee convergence. I would also like to see these assumptions explicitly stated in the proof.* A lot of little details were left out of the proof. Where are the assumptions on boundedness of the features and the rewards? Can one show that the noise sequence of the modified algorithm is actually a Martingale Difference Sequence and thus the result from Borkar and Meyn 2000 holds? Need there be an assumption of independent samples or are these samples coming from Markovian sampling?### Empirical section* The choice to set the initial value function $V(S) = 0.5 \forall S$ for the random walk was odd. I suppose that because the left reward = 0 and the right reward = 1, and the policy is 50% chance to go left or right, and $\gamma=1$, then the optimal value function $v_\pi$ linearly interpolates from (0, 1) with the center-most state having value $V(n // 2) = 0.5$. This choice seems likely to disproportionately favor the proposed GDD algorithm; which encourages the value function estimate to change slowly. Because the initial estimates are so close to correct, only small changes will be necessary and the regularizer term will remain small. What happens if the value function is initialized to 0 everywhere, or even to -1 everywhere?* The exclusion of TDC from the stepsize sensitivity investigation makes little sense to me. The first experiment chose an aggressively large stepsize $\alpha = 0.5$ for which TDC performed poorly. Then did not investigate the sensitivity of TDC to stepsize in later plots because of this choice. If you check [Giassian et al. 2020], they report that TDC in fact out-performs GTD2 on all of the same domains tested here for appropriately chosen $\alpha$ and $\beta$.* The choice of $\beta$ is never discussed. How did you set $\beta$?* How many runs? What is the variance? Are any results statistically significant?* The primary motivation of the paper was around off-policy learning, yet only one of the tested domains was off-policy (Baird's Counterexample star MDP). It would have been nice to see the random walks made into off-policy domains.### Literature review* This paper modifies the MSPBE by adding a regularizer term. There are a few other papers in the literature that do this and derive the corresponding GTD2/TDC algorithms. Liu et al. 2012 and Ghiassian et al. 2020 immediately come to mind. These should both be cited and discussed.* Are there any papers that add such a constraint as $\| w_n - w_{n - 1} \|^2$ to any known objective function? This seems like an odd choice of regularizer (penalizes making changes to the weights), so any prior literature from any field (supervised learning, online learning, optimization, etc.) would go a long way in convincing the reader that this is a good idea.* The paper mentions several times that GTD methods converge more slowly than TD. I know of a single proof that shows this in Maei's thesis for the GTD algorithm. I do not know of any such proof for TDC or GTD2. There exists empirical evidence of this in Ghiassian et al. 2020 or White and White 2016, but neither of these papers are cited.* GTD methods and importance sampling are not mutually exclusive methods for off-policy learning. In fact, GTD methods canonically use IS for their off-policy variants. Further importance sampling definitely does not decrease the variance of parameter updates (mentioned in the second paragraph of Section 1).* Sutton et al. 2009 is not really a breakthrough in the study of convergence properties of MDP systems. In fact, the proofs of Sutton et al. 2009 do not even assume samples are drawn from a distribution induced by an MDP. Perhaps Borkar and Meyn 2000 is a better reference as it fundamentally builds the proof structure used by Sutton et al. 2009?### Other minutiae* Eye-balling the modified objective function leads me to believe the objective shares the same fixed-point as the MSPBE (thus the new GTD2 algorithm converges to the same fixed-point as TD), but it would be nice to show this formally in the analytical section.* Is it possible to extend this objective to the non-linear setting?* The paper mentions that the proposed regularizer avoids large biases in the updating process. Does it not *add* bias to the updating process? Perhaps it was meant that the regularizer avoids high variance? Either way, a careful analytical discussion of the bias-variance properties would go a long way towards improving this paper.* In section 5.3 what is $\eta$? I believe this is supposed to be $\beta$ (i.e. the stepsize for the secondary weights) since $\eta$ is your secondary weight vector?* Would it make more sense to consider $\kappa$ to be a regularizer parameter instead of a stepsize and having it absorb $\alpha$? It seems in the experiment section you split these anyways, so perhaps it makes the analytical section much more clear if the algorithm was instead $\alpha\kappa (x^\top w_n - x^\top w_{n-1})$. * The paper repeatedly defines off-policy learning as learning the optimal policy using an exploratory policy. This is a bit of a restrictive setting and is certainly not the setting that Sutton et al. 2009 considered (the work that this paper builds upon).* Why assume that the target policy is deterministic (mentioned in Section 2.1)? This is a strange choice that is not used in either the empirical or analytical section as far as I can tell.* It is mentioned that TD methods should seek to minimize the MSPBE (or perhaps the MSBE it isn't clear which is meant), but shouldn't instead the goal be to minimize the MSVE (e.g. $\| \hat{V}_w - v_\pi \|^2$)?### Papers mentioned in this reviewMaei, Hamid Reza. Gradient Temporal-Difference Learning Algorithms. University of Alberta, 2011.Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White. Gradient Temporal-Difference Learning with Regularized Corrections. International Conference on Machine Learning, 2020. http://arxiv.org/abs/2007.00611.Adam White, and Martha White. Investigating Practical Linear Temporal Difference Learning. International Conference on Autonomous Agents and Multi-Agent Systems, 2016. http://arxiv.org/abs/1602.08771.Borkar, V. S., & Meyn, S. P. (2000). The O.D.E. Method for Convergence of Stochastic Approximation and Reinforcement Learning. SIAM Journal on Control and Optimization, 38(2), 447469. https://doi.org/10.1137/S0363012997331639Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, C., & Wiewiora, E. (2009). Fast gradient-descent methods for temporal-difference learning with linear function approximation. Proceedings of the 26th Annual International Conference on Machine Learning - ICML 09, 18. https://doi.org/10.1145/1553374.1553501Liu, B., Mahadevan, S., & Liu, J. (2012). Regularized Off-Policy TD-Learning. Advances in Neural Information Processing Systems, 9. **Summary**This paper describes the definition of an intrinsic reward designed to encourage an agent to behave in such a way that they can distinguish between MDPs that have different values of some hidden parameter that affects the way the MDP behaves. Proof-of-concept experiments are included to allow discussion of how providing this classification of MDPs to an agent that needs to learn how to perform some externally-designed task allows it to learn to complete the task faster and discussion of how the agent's observations affect its learning and behaviour.**Strengths and Weaknesses**The primary problem with this paper is that the description of the proposed method is too imprecise to allow me to review the quality of the research completed. I feel that the language and structure are not at an acceptable standard for ICLR. Some acute problems are that the protocol for training as well as some key algorithmic choices are never specified (i.e., the clustering algorithm and the planning algorithm for selecting action trajectories) and the notation and language around causal factors are not clear. When I say that the protocol for training is not specified, I am referring to many missing elements, like how the training dataset is composed and what information the agent is given about each environment it is placed in (e.g., is the agent able to distinguish one environment from another?) The explanation of how the clustering tree is formed lacks substance: Do the human experimenters change the composition of the training data once one causal factor has been learned sufficiently well (and how?), or is the agent able to distinguish which node of the tree a given environment belongs to? For example, what does it mean to "perform an intervention on the embedding of the causal factor isolated by the initial optimization of causal curiosity"? This is a very important aspect of the project, so it is critical to explain in plain language what you did.For the sake of reproducibility and understanding, it is necessary to know what algorithm was used for clustering and what algorithm was used to adapt the agent's behaviour to increase the amount of intrinsic reward received.One strength of the paper is how the authors have included intuitive examples throughout to help explain the intentions behind their design (e.g., "For example, if a body in an environment loses contact with the ground &"), though in some cases the language applied to the examples makes it unclear how they should be understood. For example, when you say "examples of the parameter $H$ include gravity, coefficients of friction&," it sounds like each $H$ could be any one of those examples, but from Definition 1, I suspect $H$ must be a set whose elements could include all or some of the examples. This should be clarified.**Recommendation**I am recommending that this paper be rejected because it is not written with sufficient clarity to understand the research project being reported.**Specific Examples of Lack of Clarity**Definition 1 seems critical to the paper, and as such, I think it needs to be written very carefully and clearly. The relationship between H and its associated sequences of actions is very unclear to me. Does each $h_i$ require a different sequence of actions from the sequence associated with $h_j \neq h_i$? (The definition doesn't seem to have that requirement.) An explanation of what it means for a sequence of actions to cluster state trajectories also seems to be needed. Later in the paper, Definition 1 is described using the phrase "when intervened on over a set of values" and I'm really not sure what this means practically.As a reader, I need help with understanding your choice of notation. For example, I intuitively suspect that you as authors consider $H$ (pp. 1-2) and $H^{(i)}$ (p. 3) to be related by virtue of their very similar notation, but I don't know how I am supposed to connect them in my head, other than I know that both are made up of causal factors but one is a set and the other is a vector.The connection to Rousseeuw's (1987) work is confusing. Is the reward to the agent actually given by Equation (3), which appears to be a variation of silhouette width as defined by Rousseeuw? The presentation makes it sound like Equation (3) and Silhouette Score are essentially unrelated. I suggest rephrasing the introduction of notation on page 1 to reduce ambiguity. I think that $z$ is a latent representation of the environment (my guess based on notation conventions), but I initially assumed that $z$ symbolically represented an environment based on the text.It is unclear whether you are using the term "causal factor" in the same way as you have noted that previous studies do (p. 1), or whether it has some other meaning in this paper.Based on notation, I think that a hyper-parameter $H \in \mathcal H$ (also referred to as simply a parameter) is equivalent to a hidden parameter (or causal factor) in the noted literature. This use of the term hyper-parameter is confusing and if I understand correctly, sticking with the term hidden parameter would be clearer (unless there is a reason I am missing for using hyper-parameter). **Additional Feedback (Here to help, not necessarily part of decision assessment)**It was unclear what the set of trajectories during a rollout would look like. In other papers, trajectory sometimes refers to any observed sequence of state-action-rewards, while in others it might refer specifically to such a sequence that begins and ends at the same time as an episode. In definition 1, the phrase "Let T be the length of the trajetcories during each rollout and $s_{0:T} \in S^T$ denotes a trajectory" (p. 2) makes me think there might be multiple trajectories per rollout, so I would like to be clear on how each is defined.Should $r_{0:T}$ be defined explicitly on page 1 too?I encourage you to be careful with your language when you define mathematical objects to share the name of a naive concept like causal factor. The way the definition is presented already suggests that this definition doesn't capture everything you would like it to capture because of simplifying to binary clusters. While tying it to the naive concept can help provide some intuition about your intentions, I could see this name causing some confusion when this idea develops further."trajetcories" (p. 2)"simplicitly" (p. 2)I was surprised by the curly braces used for the observations (p. 3), which I think are ordered sequences rather than sets? (Typo?) There seems to be a typo in Equation (3): I think there should be "max" on the second and third terms.If you need more space to provide a summary of the algorithm followed by your system, I don't think that the connection to model selection on page 4 is clearly necessary to the paper.You'll probably want to go through the bibliography to check for capitalization errors like "Soft-dtw," "beta-vae," etc.   The paper claims to introduce a new quantum machine learning framework called GenQu. However, the description of the framework very vague (using classical computers to optimize the parameters of a fixed quantum circuit), and hardly novel. In fact, the same basic ideas are so well-known in the community that they are described in detail as usage examples for popular quantum computing platforms such as Qiskit and IBM Q.The only remotely nontrivial part of the paper is contained in Section 4.2 about Quantum Deep Learning, where the authors consider the MNIST data set. Upon closer inspection it turns out that they use PCA to reduce the dataset to 4 dimensions, which is in turn used to train a "quantum neural network" to perform binary classification (i.e. to discriminate between '0'-instances and '5'-instances). The authors claim that such a quantum classifier provides an advantage versus a convolutional neural network in terms of 1. the number of training epochs (while ignoring the time needed to perform PCA), and 2. the number of parameters (while ignoring the parameters needed to describe the principal components).Additionally, no confidence intervals are visible on Fig. 7, which suggests that the data might have been obtained from a single experimental run. Finally, there are several instances of sloppy writing, such as the inconsistent usage of math mode for variables, the statement P(|\phi>) = |0>, the typo "iWs" instead of is, etc. This paper proposes the use of Deep Learning, namely a multi-layer perceptron, for approximating the Whittle index in restless bandits.The introduction and the related works are well written. The problem and the background on restless bandits are clearly exposed.However, there are a lot of issues in the presentation of the proposed algorithm, NeurWin, in the analysis of the proposed algorithm, and in the experimentations.The statement of Corollary 1 is not correct. In the general case, the Whittle index is not optimal, it is a heuristic. So if the neural controller produces the Whittle index, it does not produce the optimal discounted reward. As the authors suggest the statement of Corollary 1 should be rewritten as a necessary condition for a neural network to be Whittle-accurate.The proof of Theorem 2 is wrong. The statement of Theorem 2 is for any states s_0,s_1, for any \lambda \in [f_\theta(s_0)-\delta, f_\theta(s_0) + \delta ] & then the neural network is \gamma-Whittle-accurate.In the proof the authors only show that the neural network is \gamma-Whittle-accurate, when s_0=s_1 and when \lambda =,f_\theta(s_0) + \delta. So they cannot conclude that Theorem 2 holds for any states s_0,s_1, and for any \lambda \in [f_\theta(s_0)-\delta, f_\theta(s_0) + \delta ].In the section 4.2 where the training procedure is described the authors write that the choice of s_1 depends on the MAB problems, and hence it has be chosen knowing the MAB problem. The hearth of MAB problem is precisely that you do not know if some states or some arms have to be less-frequently-visited that others. This is the exploration / exploitation dilemma. So if for tuning the proposed algorithm you need to know the MAB problem, the proposed algorithm does not work at all.In the experimental section the experiment on recovering bandits is not convincing. The authors write that the algorithmic complexity of the algorithms proposed in recovering bandits is (\binom(N,M))^d. It is not correct. In recovering bandits the arms are the recovering functions. In your experiment it should be N, and d is the number of times the arms are sampled in a round, so in your experiment it should be M. The algorithmic complexity is N^M, that it is still very large. However in the cited paper the authors propose the use of optimistic planning with a given budget B. So the algorithmic complexity of their algorithms is B, which is a parameter. So the choice of d=1 in the experiment is unfair.Moreover, in the experiment the choice of only four different recovering functions for 100 arms is a little bit strange. In recovering bandits the arms are the recovering functions.Finally, the choice of training offline the neural network is not realistic in a bandit setting, where the environment is not known at the beginning. To make a fair comparison with other bandit algorithms the authors should train the neural network online and compare the accumulated rewards taking into account the convergence time to a good solution.Approximating the Whittle index by a multi-layer perceptron is a good idea, but the submitted paper is not ready for publication.  This paper presents a model for image segmentation from referring expressions which integrates linguistic representations of the referring expressions both at low-level and high-level stages of visual processing. They argue that this model is both more cognitively plausible and more successful than models which only use linguistic representations to modulate attention over high-level visual features.  I vote for rejection, mainly on grounds of significance and quality, expanded below. To change my vote I would require a substantial improvement on one or both of the quality/significance issues listed below, either presenting the model with a clear conceptual motivation, or doing satisfactory model analysis to understand the contribution of the paper.  Pros: The presented model shows some moderate quantitative improvement over other recent work.Cons: Poor conceptual motivation and error analysis, with little evident understanding of the actual effect of the linguistic representations within the model.  Quality/Significance1. The paper does not provide a clear motivation for their model. Here are some arguments I looked for but did not find:  1. Cognitively: There are some references to relevant cognitive science papers, but there seems to be little concrete inspiration taken from this or other cognitive work in the particular model design.  2. A priori based on the task: What would we expect to gain from using language in early-stage visual representations? What sort of correlations might exist between particular types of linguistic input and low-level visual representations? This might be another way to motivate the model, but I can't find any such discussion in the introduction or anywhere else.2. The evaluations don't convince me that this paper has made a significant conceptual contribution.  1. The quantitative results don't seem to constitute an enormous improvement over past work. The variability in table 2 across evaluation sets makes me doubt the statistical significance of the claims. No statistical significance tests (across resamples of test data or across random training restarts) are provided.  2. There is no satisfactory analysis of the actual cause of the model's success. What are the contents of the linguistic representations, and how exactly do they modulate low-level visual features? For reference, Hu et al. (2020, Figure 4) [1] and Hui et al. (2020, Figure 5) [2] both do some of what I'm looking for here, showing the influence of language on the behavior of the model. While the more complex representations used in this model make it more difficult to provide e.g. an easy heatmap, we absolutely need to see an error analysis that helps us believe your claim that language ought to play a role in low-level visual processing.  OriginalityI don't closely follow the relevant literature and can't speak confidently on the originality of the model. I did have trouble understanding the innovation over Step-ConvRNN, however -- these models seemed within tweaking-distance of one another based on the presentation in this paper.  [1]: https://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Bi-Directional_Relationship_Inferring_Network_for_Referring_Image_Segmentation_CVPR_2020_paper.pdf#page=5[2]: https://arxiv.org/pdf/2010.00515.pdf#page=14 -The idea of this paper is just to crop the detections and then forwarded to a second stage for more accurate predictions. This idea can be traced back to the original R-CNN paper, which is not even referred and discussed. There are also many papers having a second stage to refine the detection predictions, e.g. Cascade R-CNN, RefineDet, Revisiting RCNN, but none of them are discussed in this paper.-The writing is terrible.-Tons of literature is missing.-To be honest, this paper should be desk rejected. ##########################################################################Summary: This paper proposes BBRefinement, which is a post-processing for object detection to refine the predicted bounding boxes. BBRefinement takes cropped images from predicted bounding boxes as input and refine the bounding box with a separate network that is only targeted in predicting box offsets.##########################################################################Pros:The proposed method is simple and experiment shows the effectiveness of proposed method.##########################################################################Cons:1. Missing literature review. This paper is not the first one to study how to refine bounding boxes. There are a lot of works on refining bounding boxes [A, B, and many more], but this paper fails to discuss related works and explain the connection as well as the differences with them.2. Missing ablation studies. Section 2.2 discusses principals of BBRefinement, including the importance of using mixture data. However, there is no experiment supporting this claim. Also the expanding ratio of bounding boxes is an important parameter, but there is also no experiment on this parameter.3. This paper only applies the proposed method to very simple baselines like Faster R-CNN and RetinaNet. These methods (Faster R-CNN, RetinaNet) are known to predict not tight bounding boxes. I wonder if BBRefinement is still necessary when a method already predicts tight bounding boxes like Cascade R-CNN [C].4. It is not clear to me how the model is trained, especially how boxes are sampled during training. Is it an image-centric sampling or an instance-centric sampling? Does sampling strategy matter? There could also be false positives in the prediction, does these boxes harm the training (e.g. existence of background box)?5. Table 1 needs more explanation, do you need to train separate BBRefinement for each model? If not, what boxes do you use to train the BBRefinement.6. The timing of BBRefinement, is "23ms for B1 and 44ms for B3" a single box? What is the average end-to-end runtime on the whole dataset? The timing is also much faster than EfficientNet speed in [D], [D] reports 52 ms for B1 and 114 ms for B3. Can you explain why your timing is 2x faster?7. The dataset split on COCO also has problem, COCO train 2014 and part of val 2014 is exactly the same as train 2017. And the 5000 minival 2014 is exactly the same as val 2017.##########################################################################Reasons for score:Although this paper presents a simple and effective solution, the overall quality of the paper is poor. First, this paper does not have discussion on related works **AT ALL**. Second, it misses important implementation details and important ablation studies. Third, this paper only applies the method to weak detectors and fails to apply it to methods that give tight bounding boxes like Cascade R-CNN. Finally, the authors put a [link to the code](https://gitlab.com/irafm-ai/bb-refinement) which leaks the authorship (one author's name and institute); this is a violation of the double-blind review policy. Considering all this facts, this paper is a clear reject to me.##########################################################################References:[A] Object detection via a multi-region & semantic segmentation-aware CNN model, ICCV 2015  [B] A MultiPath Network for Object Detection, 2016  [C] Cascade R-CNN: Delving into High Quality Object Detection, CVPR 2018  [D] Designing Network Design Spaces, CVPR 2020   ##########################################################################Summary:In short, the main contribution of this paper is to jointly combine the instance-balanced sampler and the class-reversed sampling in the training, where the latter is only applied to the last few epochs. It also provides a viewpoint of the long-tailed problems from the memorization-generalization mechanism.##########################################################################Pros:+ The proposed method is indeed very simple.##########################################################################Cons:- From my point of view, this paper is just a special case and a simple version of BBN [BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition, CVPR, 2020]. The main contribution of this method is jointly combining the instance-balanced sampler and the class-reversed sampling in training, which is exactly how the bilateral-branch network works.- Since BBN and Decoupling have already thoroughly discussed the trade-off between instance-balanced sampler and class-reversed sampling, I don't think this paper provides any new ideas for the later researchers.##########################################################################Questions during the rebuttal period:One piece of advice: try to make the introduction more concise in your future work. The exact same sentence "We only switch from instance-balanced sampler to class-reversed sampler for the last several epochs of training." shows three times in the introduction, which makes the audience question the limited contribution of this paper.##########################################################################Reasons for scores:Since the novelty is limited, and the points made by this paper have already been well discussed by previous work BBN CVPR 2020 and Decoupling ICLR 2020, I decide to reject this submission.########################################################################## ## Summary / WeaknessesI have to confess that I found the paper to be extremely confusing. It is clear that the submission is still several edits away from a version that can be published.As far as I can see, the paper lacks a clear, formal description of the problem, and as a result, it is difficult to see precisely where the existing methods fall short (or even which ones are relevant), and how the proposed procedure addresses these shortcomings. Again, as far as I can see, there is little to no theoretical justification for the proposed method in the paper; the proposed method appears to be a heuristic based on approximations that may be reasonable, but requires some justification.---## RecommendationI recommend a reject.---## QuestionsPlease give a formal description of the problem the proposed method is trying to address. Also, please give brief but clear descriptions of the existing approaches and their shortcomings.I am particularly baffled by the sentence "[the QR models] cannot be be applied to the constrained black-box scenario given that they does (sic) not link their predicted quantiles with a pointwise forecasting system in a constrained way." The way I interpret this sentence is that the existing methods do not model the quantiles (or some other functional) of the _output_ of some pointwise forecasting system, but rather they model functionals of Y | X. If this is the correct interpretation, then I do not see why it would be of interest to model the predictions of a black-box forecasting system rather than Y | X.I am also confused by the experimental setup. Why do N and LP make sense as comparison methods?Also, there are several instances of errors of grammar / syntax.---## DisclaimerI did not have the time to look at the supplement. The authors state that their goal with this paper is manifold:They want to learn a prior over neural networks for multiple tasks. The posterior should go beyond mean field inference and yield good results.  The authors claim in their paper that they learn an 'expressive transferable prior over the weights of a network' for multi-task settings, which they denote with the unfortunate term 'deep prior'.In sec. 2.1 the authors introduce the idea of a hierarchical probabilistic model of weights for a neural network p(W|a) conditioned on task latent variables p(a). They realize that one might want to generate those weights with a function which conditions on variable "z" and has parameters "a". They continue their argument in Sec 2.2 that since the weight scoring can be canceled out in the ELBO, the score of the model does not depend on weights "w" explicitly anymore.This, of course, is wrong, since the likelihood term in the ELBO still is an expectation over the posterior of q(w|z)q(z). However, the authors also realize this and continue their argumentation as follows:In this case -according to the authors- one may drop the entire idea about learning distributions over weights entirely.The math says: p(y|x ; a) = int_z p(z) int_w p(w|z ; a) p(y|x, w)dw dz.So the authors claim that a model p(y|x, z) which only conditions on 'z' is the same as the full Bayesian Model with marginalized weights. They then suggest to just use any neural network with parameters "a" to model this p(y|x, z ;a) directly with z being used as an auxiliary input variable to the network with parameters "a" and claim this is doing the same. This is of course utterly misleading, as the parameter "a" in the original model indicated a model mapping a low dimensional latent variable to weights, but now a maps to a neural network mapping a latent variable and an input vector x to an output vector y. As such, these quantities are different and the argument does not hold. Also a point estimate of said mapping will not be comparable to the marginalized p(y|x).What is more concerning is that the authors claim this procedure is equivalent to learning a distribution over weights and call the whole thing a deep prior, while this paper contains no work on trying to perform the hard task of successfully parametrizing a high-dimensional conditional distribution over weights p(w|z) (apart from a trivial experiment generating all of them at once  from a neural network for a single layer in a failed experiment) but claims to succeed in doing so by circumventing it entirely. In their experiments, the authors also do not actually successfully try to really learn a full distribution over the weights of a neural network. This alone suffices to realize that the paper appears to be purposefully positioned in a highly misleading way and makes claims about weight priors that are superficially discussed in various sections but never actually executed on properly in the paper.This is a disservice to the hard work many recent and older papers are doing in actually trying to derive structured hierarchical weight distributions for deep networks, which this paper claims is a problem they find to be 'high dimensional and noisy', which is exactly why it is a valid research avenue to begin with that should not be trivially subsumed by work such as this.When reducing this paper to the actual components it provides, it is a simple object: A deterministic neural network with an auxiliary, task-dependent latent variable which provides extra inputs to model conditional densities.Such ideas have been around for a while and the authors do not do a good job of surveying the landscape of such networks with additional stochastic input variables.One example is "Learning Stochastic Feedforward Neural Networks" by Tang and Salakhutdinov, NIPS 2013, a more recent one is "Uncertainty Decomposition in Bayesian Neural Networks with Latent Variables" by Depeweg et al 2017.An obvious recent example of multi-task/meta/continual learning comparators would be "VARIATIONAL CONTINUAL LEARNING" by Nguyen et al. and other work from the Cambridge group that deals with multi-task and meta-learning and priors for neural networks.Another weakness of the paper is that the main driver of success in the paper's experiment regarding classification is the prototypical network idea, rather than anything else regarding weight uncertainty which seems entirely disentangled from the core theoretical statements of the paper.All in all, I find this paper unacceptably phrased with promises it simply does not even attempt to keep and a misleading technical section that would distort the machine learning literature without actually contributing to a solution to the technical problems it claims to tackle (in relation to modeling weight uncertainty/priors on NN). Paired with the apparent disinterest of the authors to cite recent and older literature executing strongly related underlying ideas combining neural networks with auxiliary latent variables, I can only recommend that the authors significantly change the writing and the attribution of ideas in this paper for a potential next submission focusing on multi-task learning and clarify and align the core ideas in the theory sections and the experiment sections. Unfortunately, the work does not introduce new contributions, with the point of the paper provided in the introduction:In our experiments, we show that best performing approaches currently available for object detectionon natural images can be used with success at OCR tasks.The work is applying established object detection algorithms to OCR. While the work provides a thorough experimental section exploring trade offs in network hyper-parameters, the application of object detection to the OCR domain does not provide enough novelty to warrant publication. General comment==============The authors describe an attention mechanism for training with images of different sizes. The paper is hard to understand due to major grammatical errors and unclear descriptions. Methods for training with images of different sizes have been proposed before, e.g. spatial pyramid networks. I also have concerns about their evaluation. Overall, I believe that the paper is not ready to be submitted to a conference or journal.Major comments=============1. Methods for training with images already exists, e.g. spatial pyramid pooling (http://arxiv.org/abs/1406.4729) or fully-convolutional networks (https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf). These are not cited in the paper and not included as baselines in their evaluation.2. The attention mechanisms looks similar to classificat soft-attention (https://arxiv.org/abs/1502.), which is not cited in the paper.3. The paper contains major spelling and grammatical errors, making it hard to understand important aspects.4. I can not see a clear improvement of their method over ResNet and DenseNet when the same number of model parameters is about the same. Without making sure that the number of model parameters is about the same, it is unclear if the performance gain is due the increased number of model parameters or the methodology. The paper describes a question about discretize continuous features or group discrete features in the preprocessing step, which they call feature quantification. It considers that a joint training of feature quantification and a discriminative model can lead to a better performance than treating feature quantification as a preprocessing step. This paper has many typos, grammar mistakes and question marks, which make it hard to follow. The question proposed is simple and easy to understand. However, I don't convinced by the solution in this paper. Since it is a hard optimization question, the authors proposed a relaxation approach in section 3.1. I do not think that exp(a+bx) is able to approach step functions since exp(a+bx) is monotone. I think Figure 1 is misleading. For grouping discrete features, the author propose to use exp(\alpha_{x_j, j}^h) and hoping that some \alpha parameters can be optimized to be equal, which is too simple. The exponential transformation here does not have an effect. It is more interesting to consider how to add some constraints. For example, if the discrete feature is ordinal, how one can assure that the grouped discrete feature is still ordinal. The relaxation in this paper is too much without handling any interesting constraints and the proposed exp(a+bx) can not approach step functions. The authors do not provide a good way to select number of cut points, which I think is a hard but interesting question.The work also lacks value in literature review, optimization and experiments. 1. This paper contains many typos, grammar mistakes, and format problems, which make it very hard to read. For most equations, there is no ending period. The citation format seems wrong.2. The equations (1) and (2) are for 1-dimensional random variables. They may not be suitable for high dimensional random variables. The $N$ quantile values defined in this paper are for which random variable?3. For the neural network for $D_\tau$, does it mean the input is $(\tau, X)$? So for different $\tau$, they share the same weights. What is the meaning of this output?4. The "$+\infty$" and "$-\infty$" notation is confusing. Does this mean we choose a very big value or a very small value in practice? But this is very subjective now. Did you perform some sensitive analysis on the choices of $a$ and $b$?5. The authors claim that the WGAN training is slow. For WGAN-GP, I don't see why the training is much slower than the training of QRGAN. Did you perform some analysis on the training time?6. The arguments for QRGAN  to overcome mode collapse is quite vague. Many GANs with the encoder structure can solve the mode collapse well. It may benefit to compare  QRGAN with these methods.7. What is the implication of Table 1? Does that mean WGAN-GP is better than QRGAN? The generative images are not demonstrated. Other dataset such as CelebA can be applied to check the performance. Image interpolation can also be demonstrated for mode collapse situation.8. The Appendix B is completely not necessary. It contains only well known results.  Summary  The paper proposes a new GAN method that applies the quantile regression of reinforcement learning into GAN and aims to show this helps to estimate the 1-Wasserstein distance better without gradient regularization. The idea of quantile regression presented in the paper is a way to match two distributions like WGAN-GP yet at a more grained level and need no regularization like WGAN-GP. The experiments are conducted on 2D-toy examples (Ring-8, Grid-25) as qualitative results and three other image datasets (CIFAR-10, LSUN-Bedroom, Cats) with FID scores. The proposed method is compared with some GANs baselines: SNGAN, LSGAN, and WGAN-GP.  Strength S1 - The paper proposes a new idea to apply quantile regression into GANs. Weakness W1 - The paper is not well-written, and the paper representation is not good.W2 - The performance of the  proposed method does not look outperforming the WGAN-GP even though the paper strongly claims the robustness of this method. As shown in Fig. 4, 5, the proposed method converges faster, but is not necessarily better than WGAN-GP at the end. It looks WGAN-GP converges much more stable than the proposed method.W3 - It does not make sense why WGAN-GP is so bad on Cats dataset as shown in Fig. 6. It could be just the problem of parameters-tuning?W4 - It's unclear why Fig. 2 misses the WGAN-GP?W5  The paper does not convince me why the proposed method is better than WGAN-GP in either theoretical and empirical results. In addition, the paper does not provide sufficient theoretical content to show the 1-Wasserstein distance is the same as minimizing quantile values as claimed.W6 - Many mathematical notions are not explained, e.g., What is $\rho_{\hat{\tau}}$ in Eq. 4? How  do the authors implement with $a = \infty$ and $b = -\infty$?. How is $o_{i, \tau}$ computed?W7 - FID scores alone may be biased, the combination with IS is required in the experiments.W8  The experimental results are not sufficient, e.g., the results are with only standard DCGAN architecture, and the paper would need more ablation studies on some selected hyper-parameters, e.g., $a, b, N, k$ ... in the method.Overall, I think the paper is far to meet the conference's standard, e.g., at paper presentation, strong empirical or theoretical evidence to justify the claims. It also would need substantial revision to improve in writing. I tend to reject the paper. This paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:§1 ¶1  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vagueIm not convinced this is meaningful.§1 ¶2-3 These paragraphs also vague.§1 ¶5 Why is this approach naive? Is this a well-known method? There are no citations.Fig.1 Very confusing: it looks like the target sentence, structural tags and coding model form a loop! This example is also confusing because the structural tags are non-sensical& they have no relation to this example sentence! I cant tell if this is because they were made up without relation to the input sentence, or worse, that theyre an actual example from the data, in which case there is something very wrong with the tagger used in the naive experiments.Sec. 2.1 What is the motivation behind the heuristics for the two-step process that simplifies the POS tags?Sec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these codes" (in the form of simplified POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called softplus in Eq. 2) are never explained.Sec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isnt needed.Table 1. Im not sure what the code accuracy tells us. Its also unclear to me what is means to reconstruct the original tag sequence from the codes, esp. given the description in Sec 2.1.Table 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading& this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.Table 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; Im not convinced that the sequences mean anything per se, but its a bit like adding some random noise to the decoder state before generating the word sequence.5.1 Instead of letting the beam search decide the best & we use beam search to obtain three code sequences with highest scores. Im confused: what is the difference? The authors propose a modified ReLU, the GaLU, where the nonlinearity gating role is decoupled from the linear weights. Similar ideas have been previously proposed. For example Tsai et al: http://papers.nips.cc/paper/6516-tensor-switching-networks: "The TS network decouples a hidden units decision to activate (as encoded by the activation weights) from the analysis performed on the input when the unit is active (as encoded by the analysis weights)" and Veness et al: Online learning with gated linear networks, https://arxiv.org/abs/1712.01897. In short, the paper proposes a tweak to the nonlinearity in neural nets. Since many tweaks have been previously investigated, for such a paper to be worthy of publication, in 2018, the experimental results need to be extremely impressive. The results in this paper, on MNIST and fashion-MNIST are nowhere near sufficient. The fundamental idea proposed in this paper is a sensible one:  design the functional form of a policy so that there is an initial parameterized stage that operates on perceptual input and outputs some "symbolic" (I'd be happier if we could just call them "discrete") characterization of the input, and then an arbitrary program that operates on the symbolic output of the first stage.My fundamental problem is with equation 3.  If you want to talk about the factoring of the probability distribution p(a | s) that's fine, but, to do it in fine detail, it should be:P(a | s) = \sum_sigma P(a, sigma | s) = \sum_sigma P(a | sigma, s) * P(sigma | s)And then by conditional independence of a from s given sigma = \sum_sigma P(a | sigma) * P(sigma | s)But, critically, there needs to be a sum over sigma!  Now, it could be that I am misunderstanding your notation and you mean for p(a | sigma) to stand for a whole factor and for the operation in (3) to be factor multiplication, but I don't think that's what is going on.Then, I think, you go on to assume, that p(a | sigma) is a delta distribution.  That's fine.But then equation 5 in Theorem 1 again seems to mention delta without summing over it, which still seems incorrect to me.And, ultimately, I think the theorem doesn't make sense because the transformation that the program performs on its input is not included in the gradient computation.  Consider the case where the program always outputs action 0 no matter what its symbolic input is.   Then the gradient of the log prob of a trajectory with respect to theta should be 0, but instead you end up with the gradient of the log prob of the symbol trajectory with respect to theta.I got so hung up here that I didn't feel I could evaluate the rest of the paper.  One other point is that there is a lot of work that is closely related to this at the high level, including papers about Value Iteration Networks, QMDP Networks, Particle Filter Networks, etc.  They all combine a fixed program with a parametric part and differentiate the whole transformation to do gradient updates.  It would be important in any revision of this paper to connect with that literature. The authors propose a neural network architecture for lane detection in on-road driving. The architecture consists of multiple encoder-decoder stages. This is motivated by a need to overcome limitations of traditional CNNs with respect to considering high-level context while providing pixel-level accuracy. The paper additionally claims contributions in the analysis of the performance of various instantiations of this architecture as well as in considering limitations of the popular IoU metric. Experiments are reported on the publicly available CULane dataset.The paper addresses a topical problem in autonomous driving and ADAS but is currently let down by a severe lack of accessibility. Much of the evidence corroborating the principal claims of the submission appears to be missing. While a number of CNN architectures do struggle to provide pixel-level segmentation accuracy particularly for objects of certain geometries there exists a whole host of literature regarding attempts to remedy this. The SegNet family of works as well as many works leveraging the now established skip-connection u-net architecture is missing entirely. The related works  section does list a number of recent relevant work but does not succeed in putting this into context given the approach proposed here. This is also not remedied in the experimental evaluation as almost no benchmarking to the established state of the art is performed. The evaluation itself is mainly qualitative and does not serve to convince the reader that the approach offered here is beneficial. For example, how does one choose between the different solutions offered in Fig. 8? When it comes to the quantitative evaluation some important detail appears to be missing. For example, what is a probmap and how is accuracy on these computed to arrive at Fig 7? Much of the experimental detail is also left unclear. The submission also requires extensive spell and grammar checking. This would significantly improve accessibility, though much remains to be done to make the science case more convincing. Overall this makes the originality and significance of the work difficult to judge. As it stands I can not recommend publication. # SummaryThis work deals with a computer vision task specific to autonomous vehicles, namely detection of lane markings on the road. The authors propose an encoder-decoder CNN architecture  (typically used for semantic segmentation) for which a parts of the encoder and the encoder are instanced several times (with different weights) in order to better capture the semantic and spacial information from intermediate feature maps. The method is tested on the recent Lane Detection dataset (Pan et al.) and report qualitative results.# Paper strengths- the paper deals with a topic of interest for the autonomous driving community- the authors identify a flaw in the IoU accuracy evaluation metric for lane detection# Paper weakness- The paper could be written better. It seems unfinished and some additional proof-reading is necessary to correct the multiple typos across the paper- There are no quantitative results and no comparisons with baselines and related works, making it difficult to evaluate the performances of this work.- The authors mention that the current IoU accuracy evaluation metric is flawed on some cases and then propose scanning manually results from 500 prediction maps and report results on that. There is no baseline or other related method considered for this evaluation. Also the original dataset from Pan et al. has ~88k train images, ~9.5k validation images and ~35k test images. It is not clear from which set where the 500 images taken and how representative they are for the entire dataset- The structure of the proposed architecture is not clear, in particular the merging of the feature maps across multiple encoders. From the diagram in Figure 2 b) it seems that the feature maps are transferred to the neighbour encoder/decoder branch similarly with RNNs. Is this right or only the features maps from the first branch are transferred? Furthermore, the merging is done by element-wise addition or concatenation?Given the application domain, an important aspect is the computational complexity of the proposed architectures. I would welcome such an analysis in the current work as well.- I enumerate a few other unclear aspects and improvable points in the paper:  + why standard encoder-decoder architecture are limited to small receptive field (cf. Section 2), while this approach is not and can use large kernels? Do the authors use kernels of different sizes in the branch? This problem is usually addressed with dilated convolutions or parallel sub-networks of different kernel sizes like in Inception.   + there is no description of the task and of the meaning of the 4 lane markings.   + in Section 1 the authors mention that "CNNs show the robust capacity to capture object localization on image classification ... and object detection, but less explored on semantic image segmentation due to strong prior information is needed.". This is not true as semantic segmentation is addressed by means of CNNs since several years already by most of community.  + why the label of lane pixels is set to 0.5? Such problems are typically addressed with binary-cross entropy and the output of the sigmoid is used as classification score at test time.# RecommendationThis paper deals with an interesting task, but it's still in a rather draft state. There are several shortcomings of the work that I've mentioned above. I recommend the submission for rejection. Summary: This paper proposes a method to get feedback from humans in an autonomous vehicle (AV). Labels are collected such that the human actually moves a steering wheel and depending on the steering wheel angle disagreement with the direction the vehicle is actually moving a feedback value is collected which is used to weight the scalar loss function used to learn from these demonstrations. Experiments on a simple driving simulator is presented. Comments: I think this paper is attempting to address an important problem in imitation learning that is encountered quite often in DAgger, AggreVate and variants where the expert feedback is provided on the state distribution induced by the learnt policy via a mixture policy. In DAgger (where the corrections are one-step as opposed to AggreVate where the expert takes over and shows the full demonstration to get Q(s,a)) it is difficult to actually provide good feedback especially when the expert demonstrations are not getting executed on the vehicle and hence hard for humans to ascertain what would be the actual effect of the actions they are recommending. In fact there is always a tendency to overcorrect which leads to instability in DAgger iterations. The paper proposes using a modified feedback algorithm on page 6 whose magnitude and sign is based on how much the correction signal is in agreement or disagreement with the current policy being executed on the vehicle. Unfortunately this paper is very confusingly written at the moment. I had to take multiple passes and still can't figure out many claims and discussions: - "To the best of our knowledge, no research so far has focused on using any kind of human feedback in the context of AV control with learning from demonstration" - This is not true. See: "Learning Monocular Reactive UAV Control in Cluttered Natural Environments, Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert" who used DAgger for autonomous driving of a drone with human pilot feedback. - Lots of terms are introduced without definition or forward references. Example: \theta and \hat{\theta} are provided early-on are refered to on page 3 in the middle of the page but only defined at the end of the page in 3.1. - Lots of confusing statements have been made without clear discussion like "...we could also view our problem as a contextual bandit, since the feedback for every action falls in the same range..." This was a baffling statement since contextual bandit is a one-step RL problem where there is no credit assignment problem unlike sequential decision-making settings as being dealt with in this paper. Perhaps something deeper was meant but it was not clear at all from text. - The paper is strewn with typos, is really verbose and seems to be written in a rush. For example, "Since we are off-policy the neural network cannot not influence the probability of seeing an example again, and this leads can lead to problems."- The experiments are very simple and it is not clear whether the images in figure 2 are the actual camera images used (which would be weird since they are from an overhead view which is not what human safety drivers would actually see) or hand-drawn illustrations. This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation:  d  p_data; d(y|x)  p_d(y|x); m(y|x)  p_theta(y|x)max E_x~d E_y~d(y|x) [ log m (y|x) ]                                (1) max E_x~d { E_y~d(y|x) ) [ log d(y|x) ]}  -  E_y~d(y|x) [ log m (y|x) ]}        (2)max { E_y~d [  log  (y) ]  -  E_y~d  log E_x~d(x|y) [ m (y|x) ]}                        (3)First error is that the max in (2) and (3) should be min. I will assume this minor error is corrected in the following.The equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term  in (2) does not depend on model.  The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following: min { E_y~d [  log d (y) ]  -  E_y~d  E_x~d(x|y) [ log m (y|x) ]}                     (3)Notice the swap of E_x and log. By Jensens nequality, we have  log E_x~d(x|y)  m (y|x) ]  &gt; E_x~d(x|y) [ log m (y|x) -  E_y~d  log E_x~d(x|y)  [ m (y|x) ]    &lt; -  E_y~d  E_x~d(x|y) [ log m (y|x) ]                    So, minimizing (3) amounts to minimizing a lower bound of the correct objective (3). It does not make sense at all. This paper proposes a hierarchical RNN, where the first layer is note-level and the second level is measure-level. In an experiment on the Nottingham MIDI dataset, they show slight improvements in log-likelihood.Overall:This is an interesting application of hierarchical RNNs. However, hierarchical RNNs are known to improve performance. This is an application of existing work (for example, Alexander Graves' thesis also uses hierarchical RNNs and shows improved performance). For an applications-oriented paper, I would hope to see many more experiments than just one on a tiny dataset, and improvements in log-likelihood that are more than the marginal improvements reported here. The human evaluation is neat but is inconclusivein a glaring act of omission, the authors do not link to samples generated by their model, while they include samples generated by the competition. For a fair review, one would hope to compare the models side by side to qualitatively judge the reliability of the MTurk experiments.Minor nits:I appreciate the human evaluation experiments on MTurk but they are very difficult to understand with the figure 5. Please label the y-axis. Think of a different way to present the results. Do not include the numbers on the bars. The acronym HierArchical PolyPhonic musIc gEnerative RNN is destructive; it devalues useful acronyms. Please do not use it.The paper has many grammatical and spelling errors. Please hyphenate compound adjectives. The paper tries to describe SGD from the point of view of the distribution p(y',y) where y is (a possibly corrupted) true class-label and y' a model prediction. Assuming TV metric of probabilities, a trajectory is defined which fits to general learning behaviour of distributions.The issue is that the paper abstracts the actual algorithm, model and data away and the only thing that remains are marginal distributions p(y) and conditional p(y'|y). At this point one can already argue that the result is either not describing real behavior, or is trivial. The proposed trajectory starts with a model that only predicts one-class (low entropy H(y') and high conditional entropy) and ends with the optimal model. the trajectory is linear in distribution space, therefore one obtains initially a stage where H(y') and H(y'|y) increase a lot followed by a stage where H(y'|y) decrease.This is known to happen, because almost all models include a bias on the output, thus the easiest way to initially decrease the error is to obtain the correct marginal distribution by tuning the bias. Learning the actual class-label, depending on the observed image is much harder and thus takes longer. Therefore no matter what algorithm is used, one would expect this kind of trajectory with a model that has a bias.It also means that the interesting part of an analysis only begins after the marginal distribution is learned sufficiently well. and here the experimental results deviate a lot from the theoretical prediction. while showing some parabola like shape, there are big differences in how the shapes are looking like.I don't see how this paper is improving the state of the art, most of the theoretical contributions are well known or easy to derive. There is no actual connection to SGD left, therefore it is even hard to argue that the predicted shape will be observed, independent of dataset or model(one could think about a model which can not model a bias and the inputs are mean-free thus it is hard to learn the marginal distribution, which might change the trajectory) Therefore, I vote for a strong reject. The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.Major concerns:An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance. This paper proposes a way to speed up initial training a model.  The key ideais to:1. Train an autoencoder on the full dataset and select a subset of training examples.  The subset is the union of examples that maximally activate each ofthe dimensions of the autoencoder's low-dimensional embedding.2. Then a target classifier is trained on the subset,3. followed by final fine-tuning on the full dataset.The paper is understandably written, although some crucial experimental detailsneed a bit of guesswork.Their proposal is evaluated on only one dataset, CIFAR10, using an autoencoder  and classifier of roughly similar design from the initial convolutional layers. They mention a baseline [classifier training, I presume] classifier training over ~200 epochs in 736 s (~12 min) to get 83% accuracy.  This skips steps 1. and 2.  Since this is already fast, CIFAR10 is perhaps too small a datasetto spur readers to use their proposed method (which does require them toadditionally train an autoencoder) when tackling more ambitious problems.They do not report the time taken to train their autoencoder for 800 epochs(step 1.).  For larger networks and images, it might also be important toinvestigate whether an autoencoder considerably simpler than the classifiermodel can suffice for subset selection; for example, if I want to train aResnet-152 classifier can I use a poorer quality autoencoder?  Sinceusing a randomly selected subset 20% of the original size works aboutas well as step 1 for CIFAR10, I cannot judge whether the time taken toset up and train an autoencoder makes it worthwhile to further reducethe training subset from 20% to ~8% of the original size.They do not consider alternative subset selection (1.) methods.  For example,one might use a pretrained network to select examplar images by a clusteringmethod (ex. [2]), possibly providing representative images per class.  Other selection criteria are also possible -- for example, [1] evaluates subset selection based on "representativeness" vs "diversity" criteria.They do not compare with many existing approaches to training set compression.Instead, they dismiss (Sec. 3 "Related Work") most previous work on selecting a small subset of training examples.  However, googling will quickly find manypapers on subset selection (exactly what they do) as well as related datasetoptimization techniques (such loss-based revisiting of training examplars, ortraining example weighting etc.).  For example, review-type article [3] provides a good introduction to existing subset selection techniques, as wellas references to earlier papers.It is unclear whether the autoencoder training time is included in theirexperiments that fix the total training time to 7 minutes and compare resultswith different numbers of fast epochs (step 2.).No guidelines are given for how to select the dimensionality of the autoencoderembedding, and how the selection procedure should be done in cases with largenumbers of classes, although they mention the possibility of using combinationsof activations for subset selection.  I do not understand how in problems withlarger numbers of classes I can guarantee that the training subset will containat least one representative from each class.  Some alternative subset selectionmethods can provide such guarantees, which might be important for trainingdatasets with class imbalance.Given that they do not use a very large dataset, where their technique wouldreally be needed, and that they provide no comparison with other possiblyfaster and better ways to select a subset of training examples, I cannot arguefor acceptance of this paper.[1] "Learning From Less Data: Diversified Subset Selection andActive Learning in Image Classification Tasks", Kaushal et al.https://arxiv.org/abs/1805.11191[2] Li, D., &amp; Simske, S. (2011). Training set compression by incrementalclustering. Journal of pattern recognition research, 1, 56-64.[3] Borovicka, T., Jirina Jr, M., Kordik, P., &amp; Jirina, M. (2012). Selectingrepresentative data sets. In Advances in data mining knowledge discovery andapplications. InTech. In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true. As the authors state, their notion of "state-of-the-art" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1. The question is, why one would exlude the mixture-of-softmax approach here? This is clearly misleading.The authors introduce the idea of past decoding for the purpose of regularization. It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank. Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets. The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications. Pros:-- Clustering sequence vectors is a practical and useful problem. Some of the business use-cases described in the paper are indeed useful and relevant for analytics in healthcare and retail.Cons:-- The paper is poorly written. There are numerous typos and grammatical errors throughout the paper. -- The ideas are not presented coherently. The writing needs to improve quite a bit to get accepted at a conference like ICLR.-- Description of related literature is done very poorly.  -- The generative model described clearly lacks justification. The model is not described concretely either. There is no clear description of the inference techniques used.-- Empirical results are weak. The problem formulation at the bottom of page 3 correspond to what a bag of words preprocessing of a document would provide and in this the clustering would be a much simpler solution that just doing LDA.The paper has zero interest. The paper discusses clustering sparse sequences using some mixture model. It discusses results about clustering data obtained from a restaurant loyalty program.It is not clear to me what the research contribution of the paper is. What I see is that some known techniques were used to cluster the loyalty program data and some properties of the experiments conducted noted down. No comparisons are made. I am not sure what to evaluate in this paper. In this submission, the authors present a variational smoothing interpretation of the data noising approach presented in (Xie et al., 2017). Although the theoretical coverage of the problem gives interesting insights. However, a comparison to related work w.r.t. alternative regularization approaches is missing. Similarly, the perplexity values reported in the experimental results on Penn Treebank are far away from state-of-the-art results published by many competitors on this task, e.g. see the current state-of-the-art results on Penn Treebank by (Yang et al., 2017, https://arxiv.org/pdf/1703.02573.pdf and references therein). It is bad practice to ignore existing work completely like this. The interesting question here would be, inhowfar the presented smoothing/regularization methods are complementary to existing approaches, and if the presented methods do provide improvements on top of these.Finally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications. This paper modifies the GraphSAGE on unsupervised inductive node embedding.The authors propose to use the bi-attention architecture to sampleinteresting nodes (instead of the uniform sampler in GraphSAGE), and to use aglobal embedding bias matrix in the local aggregating functions. The methodshowed improvements over GraphSAGE and other baselines on unsupervisedgraph embeddings.The proposition makes sense and the performance improvements are expected.A major comment, however, is that that the proposed method is useful in veryrestricted settings, and it is not clear how to generalize toother applications which GraphSAGE can be applied on.The overall technical contribution is incremental andmay not have enough novelty to be published in ICLR.The technical representation is very poor, unorganized and not self-contained.The paper cannot pass the threshold merely based on the way it is presented.In the algorithms, please give the output besides the input. After thealgorithms, please remark on the computational and memory cost.In algorithm 1, what is this function BIATT()? After algorithm 1, please describe this function as well as SAGE(). In the beginning of section 3, please describe the meaning of theglobal bias matrix. In algorithm 1, if B is zero-initialized, whydoes one need it as input?Some of the equations are poor formatted (e.g. reduce_sum in page 5).Please try to use rigorous mathematical formulations instead of "pseudo equations".For example, re-write "One_hot(i)". In section 3.2, explain A_{gg}, etc.use $\langle \rangle$ instead of $\alpha$.There are many typos. The main context for this paper is two recent publications: Giusti et al.s "A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots (2016) and Smolyanskiy et al.s "Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness (2017). Giusti introduced a dataset of trail images (later called the IDSIA dataset) acquired by having a hiker wear three head-mounted cameras. The forward facing image is associated with a label go straight, whereas the two side images are associated with labels for go left and go right. Giusti then trained a convolutional neural network to predict these labels and used the network to guide a "quadrotor micro aerial vehicle. Smolyanskiy improves on Giustis work by (1) gathering additional trail image data using three cameras mounted to face forward but with lateral offsets and (2) using this additional data to train a 6 output neural network (Trailnet) which predicts both view orientation and lateral offset. In addition, they also combined predicted pose relative to the trail with predictions of localized objects and a depth map for potential obstacles. They compared several neural network architectures for predicting the view angle on the IDSIA data as well as the closed-loop performance of each network in avoiding collisions while operating within a UAV on a previously unseen trail. Though Trailnet did not achieve the highest accuracy (84% vs. the max 92% achieved by ResNet-18), it was the only network that achieved 100% collision avoidance on their UAV test course. This paper, "A CASE STUDY ON OPTIMAL DEEP LEARNING MODEL FOR UAVS, attempts to evaluate two potentially better convolutional neural networks for UAV trail guidance. They fine tune pertained Inception-Resnet and MobileNet models to predict the IDSIA dataset. These then both achieve better accuracy on the IDSIA test set and were analyzed for inference time and power consumption. These two models are then run through a single simulated path, where both seem to perform adequately across 2 turns in the path. This paper has a variety of essential flaws.1. A large portion of the text is devoted to their hardware and UAV control but they were not able to actually run models on a physical UAV "due to a hardware bug we were facing with the FCU. 2. The paper claims to "introduce to the best of our knowledge, a very first comparative study of three algorithms in order to find a better motion control of a drone for detecting a trail. This is a confusing claim since a comparison of neural network architectures is a central part of the evaluation in the Smolyanskiy paper. 3. The higher accuracy on view orientation does not seem relevant since it was also achieved by Smolyanskiy et al. with networks that they then showed performed worse when combined with object detection, obstacle depth inference and combined controller.4. The sentence "An important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying an autonomous drone appears to have been plagiarized from Learning to Fly by Crashing (2017) which contains "A important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying in an indoor environment. This work studies the adaptive proximal gradient descent method, and specifically studies the group sparsity. To encourage the group sparsity, a regularizer which is a combination of $\ell_1$ norm, block $\ell_1$ norm and $\ell_2$ norm square is used. This paper gives the update rule of the proximal gradient with the specific regularizer. After proposing the update rule, the paper analyzes the convergence and regret guarantee of the algorithm.However, I'm not sure if the contribution is enough for the conference, as it is known that the block $\ell_1$ norm can encourage the block sparsity, and the computation of proximal gradient is fairly standard and straightforward. The convergence of proximal gradient method is not too different from gradient method as well.I think it can be more interesting if the work can focus on the statistical property of the regularizer $\Psi$. As suggested in Oymak et al, summing up a few regularization terms might not actually benefit with getting the structure. Analyzing whether the solution of the objective is group sparse, and whether one can find the group sparse parameter with less data than solving with unregularized least squares, is more interesting and less exploited.   1). The claim of the variable number of node representations as inputs and producing fixed-sized graph representations is weak. This can be easily addressed by simply using average pooling or sum pooling.2).. The writing needs to be improved. The quality is very low. For example, even in the introduction, where is the ending of the second paragraph? some  full stop mark is needed. Neural Pooling Method 1 and 2, it is better to give them specific names for better reference. After this H0 is again passed through, simply not readable..3). Neural Pooling Method 2 lacks of intuition. Could you please provide more intuitions on the solution during the rebuttal?4). The results reported seems much worse than results reported in other paper. For example, in the paper below. We can easily see the best result in PTC, best result is 80.41±6.92 while the submission gives only 76.2 ± 4.2  From that perspective, I did not see any advantage in the submission.Structural Landmarking and Interaction Modelling: on Resolution Dilemmas in Graph Classification,https://arxiv.org/pdf/2006.15763.pdf5). The paper is lack of parameter sensertivity analysis, without which the robustness of the proposed algorithms is unknown to me. I would suggest the authors add additional section to discuss that.6). The draft includes an github link to share the code, however that link indicates the author affiliation information. Clearly I could not access the code on the github as I would have risked infringing anonymity. This paper introduces DYnamic multi-Agent Relational Inference (DYRI), which is an extension of Neural Relational Inference (NRI) for dynamic relations.  This extension shows improved performance on the real-world basketball trajectory dataset.The article is fairly well structured, apart from the literature review, which is somehow missed very important and very related works. The paper is clearly motivated and seems to be technically correct. My main concern with this paper is a novelty. The main argument is to increase the expressive power of static NRI for dynamic relations, which I think is a very valid motivation for this paper; however, this idea has been already published as Dynamic Neural Relational Inference (dNRI; Graber & Schwing, CVPR 2020). Based on my understanding, the model and inference of DYRI are the same as dNRI.  The authors have to clearly discuss their difference with dNRI if they still think those are different. Also, I would have expected to see it as a baseline.As I mentioned above, one of the main drawbacks of the paper is its literature review and baseline methods. Besides dNRI, this problem has been tackled in other papers with different structures. For example, Graph-VRNN (Chen Sun, ICLR 2019) combined variational RNNs and NRI for multi-agent interactions and applied it in the same basketball dataset. Also, Dynamic Neural Relational Inference for Forecasting Trajectories has been proposed in CVPR 2020.  - The paper argues that neural models perform significantly worse than GBDT models on some learning to rank benchmarks. It first conducts a set of experiments to show that GBDT outperforms some neural rankers. Then, it presents a few tweaks related to feature transformation and data augmentation to improve the performance of neural models. The resulting neural models perform on par with the state-of-the-art GBDT models.- I think this paper establishes an unfair comparison between GBDT and neural-based models. As known,  neural models are good at learning great representations from the raw inputs, such as audio, images, and texts, while GBDT models are good at dealing with sparse features. A more fair comparison could be having the neural models to learn feature representations, which will be concatenated with the normalized sparse features.- Finally, the technical contribution of the paper is also quite limited. ******************************************************************************The paper is not properly anonymized. The intro refers to Our previous research and says As we defined in Engineering AI Systems: A Research Agenda, (Bosch et al., 2020), & . As such it violates the anonymity policy. ******************************************************************************This paper describes end-to-end implementation of Federated Learning (FL) on a use case of steering wheel prediction in autonomous driving. It provides empirical evaluation on real-world autonomous driving datasets and shows improved performance compared to centralized learning methods. Pros:Is it interesting to see an implementation of FL on a real-world use-case. The paper also does well in comparing different factors such as training time and bandwidth cost for FL and centralized training.Cons:The paper doesnt have enough technical depth to be accepted at ICLR and reads more like a report than a paper. It mainly describes the implementation of FL for a real-world application, which, although important, does not contribute to the field in terms of developing better algorithms or better understanding the current ones.  A large part of the experiment section describes the hardware features, network structure and training method in great details, which seems redundant or unnecessary for an ICLR submission. For example, section 4 reads The weights of the CNN are adjusted using back propagation to enforce the model output as close as possible to the desired output., which is obvious to most readers.  There are also some statements in paper that are not quite scientific or concrete. For example, the intro reads due to the characteristics of Federated Learning, on-device training becomes possible. This is not true as on-device training is not becoming possible due to FL, though FL certainly requires it.  The paper proposes ClusterFormer to address the problem of quadratic compute requirements of the attention mechanism in a Transformer model. To this end this paper proposes to combine local attention to promote local consistency and proposes KMeans clustering to gather global information for every token. The paper establishes strong results on the long form question answering task of Natural Questions in an extractive setup, with it getting the leaderboard position ahead of ETC-large. While the idea in the paper is natural and the results on NQ are strong, unfortunately the idea in the paper is not new and has already been introduced in the work "Efficient Content-based Sparse Attention with Routing Transformers" [1, 2] which the authors fail to cite or credit. Therefore, I recommend rejection.References:[1] https://openreview.net/forum?id=B1gjs6EtDr[2] https://arxiv.org/abs/2003.05997 # OverallThis paper presents some interesting ideas. In particular, this is the first time I have seen genetic algorithms used in the context of pruning, and I encourage the authors to explore this concept further. However, the paper has some severe weaknesses:(1) The paper doesn't appear to be complete. The latex is a mess, with broken references, missing formulas, citations that aren't in parentheticals, math notation that is broken, etc. Key aspects of the methodology (as mentioned in the notes below) are missing. This paper was not in submittable condition, and the authors should probably have continued to work on the paper and submitted it to another conference.(2) The two methods studied in the paper have little to do with each other. It's unclear why they're both included in the same paper, and they should be studied separately. The dissipated gradients method has little justification itself, and it is simply declared.(3) The paper only studies MNIST and FashionMNIST, which are known to allow for many findings that do not scale to larger networks. I encourage the authors to extend their method to a larger-scale setting (e.g., ResNet-20 on CIFAR-10) to show that the results also work there.(4) The paper does not include any baselines, including other early pruning methods, making it impossible to evaluate whether these results are trivial or SOTA in any way. The authors cannot plead ignorance on this matter: they cite a paper (Frankle et al., 2020) in their abstract that explicitly surveys methods for pruning at initialization. In fact, one of the primary findings of Frankle et al. 2020 is that it's important to include baselines.I cannot recommend this paper for acceptance, and I suggest that the authors re-submit the paper at a time when it is in more complete shape.# Notes## AbstractThe citations in the abstract are garbled.The abstract mentions two approaches but three are given.MNIST and FashionMNIST are not sufficient datasets to justify any pruning results. Many pruning results (e.g., lottery tickets) work on MNIST but not on larger-scale settings. The authors will need to include a larger-scale setting (e.g., CIFAR-10 and ResNet-20) to substantiate any results in this paper.## IntroThe authors should use the "\citep" macro to ensure that citations are parenthetical rather than appearing in the middle of the text.Pruning was generally claimed as a way to reduce overfitting in the 1980s and early 1990s, but it generally has not been used in this way in almost 30 years. Today, it is used solely to reduce costs (e.g., storage, running time, FLOPs, etc.).Pruning is used on all sorts of networks, not just fully-connected networks.The citations and sentence structure are garbled throughout this section.## Section 2I have no idea what Equation 2 means, but I assume you're keeping population members that lead to the highest accuracy before any training occurs?I assume that, once an individual makes it to the next generation, its weights are perturbed in some way to create new members of that generation? This doesn't seem to be explained in the text.What is the justification for dissipated gradients? Why this design and not other designs.Dissipated gradients and the genetic algorithm (GA) approach are completely unrelated. Why are they in the same paper? Perhaps each should receive consideration in a separate paper.Why combine these two approaches and not any two other approaches?## Section 5The paper frequently uses the word "Dropout" when it means "Pruning." Dropout is something different entirely.## Section 6:What does the magnitude heuristic mean? Why don't these fitness heuristics appear in Section 2? This paper proposes to learn patient-specific representation using patient physiological signals.  The authors design a PCP representation for each patient, which is learned to agree with signals from the same patients and disagrees with the remaining patients. In the supervised part, the classifier is generated from patient-specific parameters by meta-learning. The model was evaluated on three large ECG datasets: PhysioNet 2020 ECG, Chapman ECG, PTB-XL ECG.Strength: This is an important problemThe paper is easy to follow, and the experiment settings are well elaborated ___________________________________________________________________________Weakness:The authors are ignorant to several lines of existing literature in deep learning for healthcare, particularly deep phenotyping and deep patient subtyping. They also seem not aware of the existing works on explaining Black-box models.   For deep phenotyping algorithms, the patient representations generated from existing deep phenotyping algorithms are ALL patient specific.  To list a few, the authors may take a look at [1-4]. For more related works on deep phenotyping, you may refer to [5]. For more related works on deep learning for ECG data, you may refer to [6]. To learn disease subtypes, patient prototypes, there are also a series of works. Below are several the authors could refer to [7-10]. In addition, there are several model agnostic algorithms designed for explaining deep learning models that could be used here to add more explanability. For example [11-13]. In addition, there are many existing works around contrastive learning on ECG signals, see [14-16]. The authors are either not aware of these lines of works or have wrong understanding on them, which cause several major issues as listed below.(a) Lack of Novelty. Given the existing works listed below. The novelty of the paper is not enough for the conference. The methodology of the paper consists of a standard contrastive learning method and a standard meta-learning setting. All have been done in one or more existing works.(b) Lack of Baselines. The paper does not include any baseline. It is unclear how it compares with existing works.  (c) The experiment results are mostly unconvincing. For example,- In Experiment 5.1, the paper states that PCPs exhibit tighter clusters than those found with training representations in Figure 2. However, it is hard to tell which result is more separable from my point of view. A possible suggestion is that the authors could use quantitative clustering metrics to demonstrate, such as Adjusted Rand index. Also, the resolution of figure 2 could be improved.- In Experiment 5.2,  the detail of meta-learning network, hypernetwork, is not mentioned in the paper. Is it just a matrix transform? Or a more complicated neural network? The detailed implementation of hypernetwork could be discussed.- In Experiment 5.3, The result is not surprising, since the model have used contrastive loss to maximize similarity of PCP to Same Training Patient. Therefore, it should have smaller distances, naturally. Also, we encourage the paper to discuss why using the Euclidean distance not the cosine similarity as mentioned in Sec. 3.1.- In Experiment 5.4, the paper shows two examples of similar patients, where the performance is hard to evaluate. The reviewer will suggest two ways: (i) using demographics, physiology, or treatment features to match two similar patients, and then quantitatively compare with the matching results given by the paper; (ii) setting distance threshold or cosine similarity threshold to decide whether two patients are similar, and then justify the threshold.- In Experiment 5.5. This experiment is very problematic. First, the paper claims and show that the PCP representation could provide the same classification performance as training on the whole training set. It is also not surprising, because when obtaining the PCP representations, the model already uses all the training data, then of course PCP would give the same performance; Second, we encourage the paper to compare with using different proportions of the training set. Often the time, the whole training set could be redundant in ECG setting.- The authors are encouraged to analyze the computational complexity of the proposed method.- In appendix, figure 9 (bottom) has exactly the same channel signals, which is a significant mistake. Figure 10 has a wrong title description.- For dataset PhysioNet 2020 ECG, the paper states that Each recording can be associated with multiple labels. It is unclear how to use the dataset for evaluation.- The motivation of the paper could be improved. Also, some related works on contrastive learning are missing, for example [17-18].References[1] RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism. NeurIPS 2016[2] MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare. NeurIPS 2017[3] MINA: Multilevel Knowledge-Guided Attention for Modeling Electrocardiography Signals. IJCAI 2019[4] RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data. KDD 2018[5] Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review, Journal of the American Medical Informatics Association 2019[6] Opportunities and Challenges in Deep Learning Methods on Electrocardiogram Data: A Systematic Review, Computers in Biology and Medicine 2020[7] Patient subtyping via time-aware LSTM networks, KDD 2017[8] DDL: Deep Dictionary Learning for Predictive Phenotyping, IJCAI 2019[9] PEARL: Prototype Learning via Rule Learning, ACM BCB 2019[10] Identifying Sepsis Subphenotypes via Time-Aware Multi-Modal Auto-Encoder, KDD 2020[11] Why should I trust you?: Explaining the predictions of any classifier. KDD 2016[12] A Unified Approach to Interpreting Model Prediction. NeurIPS 2017[13] Anchors: High-Precision ModelAgnostic Explanation AAAI 2018[14] CLOCS: Contrastive learning of cardiac signals. arXiv preprint arXiv:2005.13249, 2020. [15] A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. [16] Subject-aware contrastive learning for biosignals. arXiv preprint arXiv:2007.04871, 2020.[17] Momentum contrast for unsupervised visual representation learning. CVPR 2020[18] Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. This work considers online variational Bayesian approaches to continual learning. The authors propose a beta-ELBO objective which they claim interpolates between Gaussian variational inference (beta = 1) and Laplaces approximation (beta = 0).Furthermore, the authors propose task-specific, non-probabilistic (point estimation) FiLM layers that apply an element-wise transformation to the activations.Theory / Contribution:The two contributions seem quite orthogonal to each other and each of them is rather minor in novelty. It is obvious that using beta=0 leads to MAP estimates from which Laplaces approximation can be computed. (However, I am quite confused what exactly the authors do here and there could be a major mistake:From the paper, I am not sure if the authors a) compute Laplaces approximation in the end, at the resulting mean of q, for any beta value? As far as I understand, the authors instead b) only optimise the variance through the beta-ELBO. However, in this case, the resulting approximation would *not* identical to Laplaces approximation!I am confused what the authors are doing here and could not find out out from the paper. (Consider the case of beta=0, the covariance will be the dirac distribution as the authors note in Sec. 2.2 or the supplementary material. The authors then go on and write the optimal covariance matrix for which the derivative of the beta-ELBO is zero.You have first postulated that the covariance is zero, in order to be able to pull out the expectation, and then you again allow for a non-zero beta-elbo-minimizing covariance. This would be a contraction. (This makes me guess you do compute Laplaces approximation instead. But then it is not discussed how you deal with beta>0. Related work:The related work section is rather short mentioning only very few related approaches. More effort is required here.Experiments:The experimental evaluation is thorough and seems promising. Although I am wondering why e.g. Fig. 2 does not include VCL and EWC.(Figure 8 in the supplementary material probably has some legends mixed up, or the explanations that small beta values cause locally measured locally are wrong? For Fig. a), the largest beta=10 seems to be a good approximation and also the most local. In case of Fig. B) and C) it is unclear / subjective (from visually inspecting the likelihood function) which is the best approximation. In A), beta=0.1 is the least local approximation, in B) beta=10 and in C) beta=1. I cannot follow the intuition provided here.Summary:(I am sceptical about the correctness regarding the equivalence between VI and Laplaces approximation; the exact approach proposed in the paper is unclear and may be based on a contradiction. (In case I have a misunderstanding here, I hope the authors will point this out and update the manuscript.  This paper aims to measure sample efficiency of available RL  methods instead of algorithmic improvement.  It draws a conclusion about continuing improved RL sample efficiency in the past few years.  This conclusion is rather thin to fill a complete paper. And the evaluation procedure is rather limited: it directly uses data reported from the original papers without re-producing them at the same environment. These different papers came across a span of at least four years;  although Mujoco is a standard benchmark,  it also has evolved multiple times, basically, there are so many uncontrolled variables in the plot reported by authors. I consider these results not eligible for interpretation. And the algorithms considered here for sample efficiency are not on the same ground, some are off-policy, some are on-policy, off-policy algorithms are naturally far more sample efficient than on-policy, they can't be compared in this way.  Also, authors only report results for score 400 and 2000, which is also inadequate.    In the pixel experiments, it mentions SLAC and dreaming, they are not purely pixel-based RL, rather a latent space is trained at first; so it's so a valid comparison between them and methods like CuRL. In the state experiments, SLAC is not purely state-based, it trains VAE first for a low-dim latent space. In all, the material presented in this paper does not fill a complete paper, and the evaluation protocol and conclusions are not valid.   The authors propose a reinforcement learning approach for splitting a DNN for faster initialization. More specifically, the inference time can be split to two parts: initializing the prediction model (loading from disk, uploading to memory, initializing for inference) and the actual inference time. This paper focuses on reducing the time of the former operation.The strength of the manuscript is that reducing the loading time has not been studied earlier. As the authors claim, most research on computational load of modern machine learning models is focused on reducing the inference time, and the loading time is dismissed.However, the authors do not justify thoroughly why the loading time should be a concern. The startup time of the executable depends on many factors, not only the initialization of the neural network. Moreover, the loading time is a one-time event, so I fail to see the impact of whether the DNN is initialized in 0.74 seconds (proposed) versus 2.18 seconds (conventional). There might be use cases where this difference is indeed significant, but they are not presented in the paper.The literary style of the paper is not suitable for a scientific publication. There are many errors in spelling and the clarity of presentation is poor. As a detailed note, there are no paragraph breaks, which makes the reading hard. Summary:The authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.#################. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \beta-VAE?. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  . Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \beta-VAE.  Summary:This paper studies Lyapunov chaos in learning algorithms for matrix games. It appears to extend earlier work by Cheung and Piliouras to more general-sum settings with the conclusion that in these more common settings the learning algorithms considered exhibit chaos. The paper also presents an interesting notion of matrix domination which is a necessary and sufficient condition for chaos, and also a linear programming approach for the purpose of identifying chaotic games.Strengths:* very nice example before section 3.2* thm 7 is very interesting* (8) is also very cleverWeaknesses (in rough order of appearance):* constantly referring to "AI/ML" - just pick one* "measurement errors in real economies" - why "economies" all of a sudden?* "Nash equilibrium is not achievable in general." - this is patently false. There is a wealth of recent literature on learning algorithms which stably approach Nash equilibria, see, e.g., https://arxiv.org/pdf/1901.00838* the whole motivation with roundoff errors reads very naive... these issues have been studied for decades and are taught in standard courses on numerical methods. A good reference which covers these issues with rigor would be the classic book by James Demmel (who is also, coincidentally, known for his work on LAPACK). It is also worth pointing out the extensive work behind (and Turing award for) the development of the IEEE floating point standard. My point being that problems of algorithmic stability arising from finite-precision computation have been (and continue to be) studied extensively, and much theory absolutely considers the presence of computation errors.* I do not follow footnote 1* repeated references to "dual space" - what is this? It does not appear to be defined* (1) is confusing. Some prose description would be good.* The definitions following "convex regularizer function" are unclear.* I found (4) onward (through the end of pg. 5) very confusing and didn't really follow* Notation of "u_i" in Def. 3 is not introduced before* In 3.2.2 I don't understand why the "iff" property above doesn't apply* The end at pg. 8 is extremely abrupt. Please offer further interpretation of the results (e.g., what should the reader take away from Thm 11?) and a proper conclusion for the paper.* [throughout] there are numerous syntax/semantic errors that should be corrected before publication. I suggest employing a copy editor.* [high level comment] matrix games are a nice theoretical starting point for game theory, but I have yet to see them used in realistic settings. I'm sure that theoretical results may be very interesting, but practically speaking I have difficulty seeing the motivation. I would suggest providing stronger motivation for the reader early on.* [high level comment] there are *no* experiments at all. Please show at least some numerical examples, if for no other reason than to convince skeptics of the theoretical results.* [high level comment] numerous times, the paper referred to something like a definition that did not appear until much later in the paper. this is a little awkward for the readerOverall:While this paper does offer some interesting ideas, I cannot recommend publication at this time. I have pointed out a number of directions in which the manuscript could be improved; I hope that the authors are able to clarify these points in later revision. The paper proposes an approach to develop synthetic datasets aimed at training DeepCoder-alike models. They claim that current approaches to generate synthetic datasets do not help coding algorithms to obtain a good generalization. The reviewed literature and the experiments presented back such claim. The approach consists in an _adversarial_ Evolutionary Computation (EC) methodology.Pros:- The paper is, in general, well written. The language is clear, and overall structure of the paper is OK.- The method is sounding, as it departs from proven concepts such as using evolutionary methods as adversarial sample generators for deep models.Cons:-  Sec. 3 could be improved. Authors rely too much on formal notation to express their main contribution, with almost no explanation in simple words, or the intuition behind it.- Although the paper may be solid enough, from a scientific point of view, I consider the scope of the paper too narrow. Authors are proposing a tool to help another tool without too much use other than competitions on synthetic datasets. As the authors state themselves, there are no enough real-life cases for these types of models to perform, and therefore everything is reduced to benchmark tests on toy, artificially generated, datasets.- i.e., their contribution is minimal from a wider point of view. This is evidenced in their own list of stated key contributions, where they expand a unique contribution (the proposed approach) into three redundant achievements (experimental evaluation of the approach in two different settings).Other comments:A major problem with the entire field of Program synthesis within the field of deep learning, is that sometimes overlaps alarmingly with the field of Genetic Programming (GP), and it becomes difficult to really put in perspective if a contribution is original enough, or just a re-painted version of an older idea. This paper is a clear example of such problem. The method the authors are proposing, bears some resemblance to the (competitive) Co-Evolutionary approach of subset sampling used in GP [1,2,3]. But these methods are never mentioned in the related work area, becoming impossible to perform a side by side comparison of the proposed approach against what already exists in other areas; in other words, the paper fails by being too focused on the area of deep learning, while being oblivious to very similar developments in parallel areas.1. Doucette, J. A., Mcintyre, A. R., Lichodzijewski, P., & Heywood, M. I. (2012). Symbiotic coevolutionary genetic programming: a benchmarking study under large attribute spaces. Genetic Programming and Evolvable Machines, 13(1), 71-101.2. Chong, S. Y., Tino, P., & Yao, X. (2008). Measuring generalization performance in coevolutionary learning. IEEE Transactions on Evolutionary Computation, 12(4), 479-505.3. Pagie, L., & Hogeweg, P. (1997). Evolutionary consequences of coevolving targets. Evolutionary computation, 5(4), 401-418. ### Summary The paper claims to exhibit a connection between differential privacy and neural network pruning. The main result is that, for a single layer network with Gaussian weights that depend on the privacy parameters, there exists a differentially private function which approximates the pruned network in $L_2$ distance. This is proved by bounding the sensitivity of the network and adding Laplace noise so that the resulting function satisfies differential privacy and approximates the function defined by the pruned network. According to the authors, these results mean that  network pruning can be an effective method to achieve differential privacy### EvaluationThere are multiple serious conceptual issues with this work. Let me list them in order of severity:* The fact that a differentially private function h approximates another function g has no implications about the extent to which g preserves privacy. Differential (or any) privacy is not a notion thats in any sense continuous with $L_2$ distance. For example, suppose we have numerical data consisting of n reals $x_1, \ldots, x_n$ in $[0,1]$, and $g = (x_1 + \ldots + x_n)/n$ is their average, and let $h(x)= g(x) + e$ where e is $\mathrm{Lap}(0,1/\varepsilon)$ noise. Then g and h are very close and, in the authors terminology, the average has a similar effect to adding differentially private noise. Nevertheless, averages are far from being privacy preserving: for example the exact value can be encoded in the lower order bits of the $x_i$ in a way that is preserved by the average, so that the entire data set $x_1, \ldots, x_n$ can be read off from $g(x)$. Note that this example is *exactly* analogous to what the authors prove, just simpler. More concisely, the authors have gotten things backwards: finding a differentially private approximation h to a function g does not mean that g is in any way privacy preserving. * The privacy model used here is peculiar, and it is not clear to me when it makes sense to use it. The authors show how to release a *single prediction* of the network on a *single data point* with differential privacy, where differential privacy is defined with respect to two data points being neighboring if they differ in a single coordinate (in the appendix) or have $\ell_1$ norm at most 1 (in the body of the paper). So what is the data of an individual here? For example, if $x$ is an image, are we preserving pixel privacy? And why is it interesting to release a single prediction rather than an entire network, as is more common?* The theorem is proved for random Gaussian weights, and the variance of the weights depend linearly on the privacy parameters. In particular, the variance of the weights depends linearly on $\delta_{dp}$, which is a parameter thats often assumed to be cryptographically small. In that case, it looks like the networks output would contain essentially no signal about its input.* The paper is badly written. There are problems with phrasing (e.g. algorithms are called differential privacy rather than differentially private), with undefined notation (what is $\circ$ in the proof sketch of the main theorem?), and with inconsistent definitions (neighborhood definitions are different in the body and the appendix), and unexplained assumptions and model choices (the Gaussian weights, the semantics of the privacy model). This work proposed a novel learning strategy for unsupervisedly anomaly detection. Particularly, authors propose to use an iterative mask generation process based on image impainting and reduction of a structural similarity metric (SSMI) between the input image and its reconstructed version. For evaluation purposes, authors resort to the public MVTec benchmark, showing better results than the baselines. Please find me comments below:Strengths-The paper is generally well written and easy to follow.-Results show that the proposed method outperforms the baselines.Weaknesses.-The methodological contribution is marginal/incremental. Similarly to several methods in weakly supervised segmentation (see [1] for example) authors use iterative steps to refine the initial segmentation mask. The only difference is that instead of mine regions based on classification activation maps and classification scores the authors use a structural similarity pixel-wise metric. Beyond that there is nothing novel in the proposed methodology.-Some ideas/motivations are unclear. For example, I dont really understand why the mask initialization is needed at test time. Further, authors also mention in the Appendix. D that to cover whole images, they are split into X by X patches and then aggregated into four masks. Please provide more details since this is unclear. -Literature is poorly conducted with many relevant recent papers missing (furthermore, the literature comes late in the paper). In addition to the previous paper in weakly supervised segmentation, the works in [2-6] are recent works in anomaly detection, just to name a few. Authors should include all this papers, discuss their limitations and show how the proposed work can overcome these drawbacks. Authors mention that auto-encoders produce blurry images, which is true. Nevertheless, works including an adversarial discriminator have somehow addressed the issue of blurred reconstructed images. Given all this, authors should better motivate their work. -Among previous missing papers, there is the work in [2], which also employs an impainting strategy coupled with an adversarial model. Which are the differences with respect to this work?-Experiments need to be significantly extended. First, authors merely include two methods in their evaluation, while there exist more than those used in the comparisons. This is particularly important since some recent methods which have been omitted in the literature review made by the authors (e.g., [4]) significantly outperform the proposed method (0.863 vs 0.90 in AUC). Second, authors report results in terms of AUC, while I strongly suggest that they use the accuracy for individual classes, and the AUC as average of the classes. The reason for this is to better compare to related work (See for example Table 6 in [4]).References:[1] Wang et al. Weakly-supervised semantic segmentation by iteratively mining common object features. CVPR 2018.[2] Sabokrou et al. Avid: Adversarial visual irregularity detection. ACCV 2018 [3] Perera et al. Ocgan: One-class novelty detection using gans with constrained latent representations. CVPR 2019.[4] Venkataramananet al. Attention Guided Anomaly Localization in Images. ECCV 2020.[5] Deecke et al. Image anomaly detection with generative adversarial networks. In Joint european conference on machine learning and knowledge discovery in databases 2018.[6] Li et al. Exploring deep anomaly detection methods based on capsule net. In Canadian Conference on Artificial Intelligence 2020Minor: The paper in David Dehaene, Oriel Frigo, S´ebastien Combrexelle, and Pierre Eline. Iterative energy-based projection on a normal data manifold for anomaly localization. arXiv preprint arXiv:2002.03734 is an ICML 2020 published paper, Summary:This paper proposes two variations on the CFR algorithm for solving extensive form games. The first (Temperature Regret Matching) changes Regret Matching, the method for computing a policy from accumulated regret values.  The second ("New Regret Updating Method") changes the regret update by putting extra emphasis on adjusting to the most recent opponent strategy.  By combining both changes, the authors show a slight advantage in convergence speed over baseline CFR variants.----------Positives:- The two techniques proposed are each small, straightforward, and independent changes to the CFR family (TRM more so, "New Update Method" less so). The CFR family of algorithms is an active research area, and an advance could have impact across many papers in combination with other ongoing research.----------Negatives:- The paper has clarity issues throughout the work.  There are frequent minor issues (typos, poor word choices, grammatical errors) but there are also key sections that I cannot follow at all, despite being very familiar with the CFR technique and literature being extended in this paper.  I'll flag both the minor and major issues in 'Issues and Suggestions' below.- Incorrect statements. I'll flag these in 'Issues and Suggestions' below.  There are three statements in the paper that I flagged as being simply false, and are described correctly in papers that the authors cite.- Poorly motivated changes.  For both of the proposed changes, the text doesn't motivate why the proposed change should actually help (beyond a vague statement such as "This setting can help agents have a certain chance of producing more effective strategies."), and a proof sketch that the technique should still converge.  The experiments then show particular hyperparameter settings that show a slight advantage.- Incremental technique and results.  While acknowledging that an ICLR paper does not have to beat baselines to be accepted, and this paper does present results under some hyperparameters that exceed baseline techniques, the advantage is *very* slight, only occurs with specific hyperparameter values, and are not averaged over repeated independent experiments.  The techniques themselves are not motivated and described well enough to be of sufficient interest on their own.- Insufficient experiments.  The technique proposes two independent changes, and while TRM has experiments on its own (Figure 2, 3), the second ("New Regret Updating Method") is only examined in combination with TRM (in Figure 4).  These techniques needed to be examined independently as well as in combination, for us to properly attribute where an advantage might be coming from.----------Recommendation and Justification:I suggest rejection.  I feel that the clarity issues, lack of motivation and intuition for the changes, and insufficient empirical analysis would each independently be reason enough to reject the paper; in combination, it is just not ready.  I've flag the issues I noted in 'Issues and Suggestions' below, which is pretty extensive, and gives concrete advice.This is unfortunate - the authors are experimenting with changes to CFR that could have broad applicability across many CFR variants (which they did present in their combination with Vanilla CFR, CFR+, LCFR, and Deep CFR), so if they have indeed found a useful set of changes and can demonstrate it through better experiments, I can imagine a future resubmission of this paper being accepted and having impact.----------Questions to clarify recommendation:I don't have a question for the authors that would clarify my review, but I've given some pretty extensive comments in the section below, with concrete and actionable ways to improve the presentation and content of the paper.  If the authors feel I've misunderstood the paper from my comments below, then I would be very open to hearing any clarifications they would like to offer.----------Issues and Suggestions:I flagged a mix of major and minor issues in my review.  I'm going to list them as they occurred in the paper.- Pg1, Intro.  Incorrect statement.  The statement that CFR+ was used to solve heads-up limit hold'em (Bowling, 2015) is true.  But the sentence continues to say "...and heads-up no-limit Texas hold'em (Moravcik 2017, Brown and Sandholm, 2018), and this is *definitely* false.  Solving a game means computing a Nash equilibrium, or in the case of Bowling 2015, approximating it to such a tight tolerance that the game is "essentially solved".  Heads-up no-limit is, unquestionably, not solved.  Not only does neither of those papers claim to have solved the game, but as far as I know, no group has claimed to have done so for no-limit poker.  Further, even if a strategy was claimed to be a solution to the game, there is no known, practical technique for computing a best response in no-limit poker, which would be required to verify how close it was to an equilibrium.  Instead, the claims made in Moravcik 2017 and Brown and Sandholm 2018 is that their respective programs had defeated human pros and top human pros, respectively.  While that is an accomplishment, it is far from solving the game, and the authors have made no claim about how close they are to Nash equilibria.- Pg1, Intro.  Awkward phrasing.  "It helps agents defeat poker professionals."  Maybe "CFR is the technique these algorithms used to defeat poker professionals."?- Pg1, Intro.  Clarity issue, incorrect citation.  "We prove that any strategies which are inversely positively to the external regret can help the CFR algorithm converge to an e-equilibrium."  First, the clarity issue: I cannot parse this sentence, and I can't even imagine what it's referring to.  What does "inversely positively to the external regret" mean?  Further, the sentence cites Nesterov 2005 for this statement, but the first CFR paper did not come out until 2007, so it is impossible that Nesterov has made any statement about CFR.  This whole paragraph is actually unclear; the notion of a "combinatorial regret matching method" needs to be explained, and the last sentence describes weighting external regret, but isn't it counterfactual regret that is weighted in this technique?- Pg1, Intro.  Typo.  "...which the last iteration" --> "...where the last iteration"- Pg1, Intro.  Typo.  "convergence , we" --> "convergence, we"- Pg1, Intro.  Typo.  "are listed bellow" --> "are listed below"- Pg2, bullet 1.  Clarity.  "inversely positively to the external regret" again.  This sentence doesn't make sense.- Pg2, bullet 3.  Clarity.  "opponent's strategy of the last moment".  Does this mean "last iteration"?  Can the statement about "improves the learning efficiency" be made more precise?- Pg2.  Typo.  "in the vanilla CFR" --> "in vanilla CFR"- Pg2.  Typo.  "...standard benchmarks Leduc Hold'em" --> "standard benchmark".  Although, I flagged this later - the version of Leduc Hold'em used in this paper is quite different from the standard one!- Pg2, Section 2 title.  Typo.  "Perliminaries" --> "Preliminaries"- Pg2. Typo.  "a finitset" --> "a finite set"?- Pg2.  Typo.  "extension form game" --> "extensive form game"- Pg2. Typo.  "...where c is the chance." --> "...where c is the chance player."?- Pg2. Clarity.  "u_1 + u_2 = 0" --> "u_1(z) + u_2(z) = 0 for all z \in Z"?- Pg2, Notations.  Clarity.  The sentence "The probability of a particular action a..." describes a behavioral form strategy, which is standard in the CFR literature (after all, we use regrets to compute action probabilities at infosets).  The following sentence "We define \sigma_i to be a probability vector for player i over all available strategies in the game" describes a normal form (e.g., mixed) strategy, defined at the root as a probability distribution over pure strategies.  While these notions are equivalent in perfect recall games (you can convert from one to the other) it's confusing to define both back-to-back (especially without differentiating them!) and the latter definition doesn't appear to be needed in the rest of the paper.- Pg2, Notations.  Clarity.  "Hence, \pi^\sigma_i(h) is the probability..." is an unclear sentence.  Maybe "...is the probability that h is reached if player i plays according to \sigma, and chance and the other players choose actions leading to h."- Pg3, before equations 2 and 3.  Typo.  "strategy profile \sigma satisfies" --> "...that satisfies"- Pg3, Exploitability.  Clarity.  In other CFR and poker work, it's standard to divide the two best response values by 2, giving an average exploitability across the game positions.  For example, see papers like "Accelerating Best Response Calculation", IJCAI 2011, which give the exploitability of Always Fold as 0.75 and not 1.5 (exploitable for 1.0 in one position, 0.5 in the other).  Without the /2, a player can be exploitable for more than the max amount that can be lost in a single game, which while technically correct (it's only different by a constant) is quite non-intuitive.- Pg3, Section 2.3.  Clarity.  The sentence "Let \Sigma_a be the set of valid actions."  The previous page described \Sigma_i as the set of whole-game strategies for player i, and A(h) as the set of valid actions at history h.  So what is \Sigma_a?  The previous sentence said "consider repeatedly playing an extensive game", which suggests we really are picking actions, and not picking strategies from \Sigma_i as we would in a normal form game.- Pg3, Section 2.3  Clarity.  "At each iteration, the player selects an action a_t and get a reward CFR makes frequent use..."  Is there a missing period after 'reward'?  And 'gets a reward'.- Pg3, above equation 7.  Typo.  "infoaet" --> "infoset"- Pg3, above equation 8.  "The whole regret R^T".  By "whole regret" do you mean "accumulated regret"?  That's the name I'm most familiar with from the CFR literature, and I haven't seen "whole regret" used in previous works.- Pg3, below equation 9.  Typo.  "interation" --> "iteration".  I believe I saw this typo in several places in the paper.- Pg3, describing CFR. Maybe move the Zinkevich 2007 cite one sentence later, since both statements (regret bound, average strategy) are citing that work and not just the first.- Pg3, describing CFR+.  Correction.  This states CFR+ has two small changes from CFR, but it's actually three: the two mentioned here, plus the use of alternating updates, where on each iteration we update P1 and then P2 (using P1's updated strategy), instead of updating P1 and P2 simultaneously or precomputing the P1 and P2 current strategies before computing the regret changes.  This makes a surprisingly large difference in convergence speed; CFR+ performs much worse without it.- Pg3, describing CFR+.  Clarity.  "CFR+ uses strategy on iteration T according to Regret Matching +".  Two things.  First, the previous sentence should use the name "RM+" where "regret-like value" is currently, so that the reader knows what RM+ is in this sentence.  Second, this is pretty unclear...  Maybe "CFR+ uses RM+ instead of RM to compute its strategy on iteration T.- Pg3, describing CFR and CFR+.  Clarity.  CFR has two strategies for each player: the strategy computed from the regrets (often called the Current strategy in the CFR literature) and the average strategy (the part that converges to Nash).  Throughout this paper, a collection of other names are used, like "output strategy", and it's unclear which of these two strategies those names are referring to.  The authors should choose standard names (ideally 'current' and 'average') and use them throughout the paper, and also define them here, so that the reader knows what they are.- Pg3, CFR.  "Optimistic CFR counts the regret..." --> "...accumulates the regret..."?- Pg3, CFR.  "It means the iteration t regret has a weight 2t/(T^2+T)..." This sentence is unclear, and seems to disagree with the previous sentence which said the regret is multiplied by t/(t+1).  Please clarify this.- Pg3, CFR.  In the 3rd last sentence, the phrase 'external regret' is used.  This term was used in the intro, but was never defined or described.  Further, throughout the paper, the authors seem to use it interchangeably with "counterfactual regret", but where counterfactual regret is the correct term.  The CFR theory uses counterfactual regret to bound external regret, but the regrets that are accumulated, weighted, etc, are counterfactual regrets.  Further, the next page mentiones "swap regret" without definining it.  Both of these terms, "external regret" and "swap regret" should be defined and described here in the Notations section.- Pg4, Swap Regret.  This section seems to be connecting the TRM technique to Swap Regret (in Fig 1, for example) but I cannot follow most of this section because of clarity issues, and I can't see a place in the text that actually makes this connection.  Please clarify this entire section and make the connection to swap regret clear.- Pg4, first sentence of TRM.  "...efficient RM method that adopts a different strategy..."  The word 'strategy' already has a meaning in this sentence (an agent's policy).  Maybe use a different term like 'a different approach' to make this easier for the reader. - Pg4, TRM section.  "output strategy".  I mentioned this term above, but this is the first use.  What is an 'output strategy'?  The current strategy, the average strategy, or something else?  The term suggests the average strategy (since that is the output of running CFR - the current strategy isn't used to play actual games and doesn't converge!), but Regret Matching is used to produce the current strategy (which is never "output" by the program).  Please use consistent terms, and define them in the Notations section.  And ideally, use the standard terms in the CFR literature. - Pg4, TRM section.  "perliminary" --> "hyper-parameter"?- Pg4, TRM section.  Incorrect statement.  "In all past variants of CFR, each iteration strategy is given by regret matching using external regret".  This is incorrect.  Prior work has used Hedge instead of RM (see "Dynamic Thresholding and Pruning for Regret Minimization" by Brown et al, or "Solving Imperfect Information Games via Discounted Regret Matching" by Brown & Sandholm, or "Lazy-CFR" by Zhou et al, which describes CFR as using RM or AdaHedge).  And should "external regret" be "counterfactual regret" here?- Pg4, TRM section.  There are grammatical and clarity issues throughout this section.  - Pg4, TRM section.  Clarity.  What does it mean for an *action* to have its own set of algorithms?  An algorithm like RM takes regrets and produces a probability distribution over actions; I can imagine using several algorithms to produce different probability distributions for one infoset, but I don't understand how an action can itself have a set of algorithms.  This needs to be presented much more clearly.- Pg4, TRM section.  Clarity.  What is a "counterfactual vector"?  A vector of counterfactual values?  Is this a vector over actions (i.e., at one infoset), a vector over private states (as in vector-form CFR, as used in CFR+), or something else?  This term was used without defining it.- Pg5, below equation 16.  "It means we can first as m_1 algorithm for a recommended strategy, then use the regret R^T ask m_2 algorithm for a recommended strategy".  Grammar issues aside, the 'm' algorithms were previously described as indexed using two values (m11, m12, ..., m1k, m21, m22, etc), below Equation 10.  I didn't understand what these algorithms were from the previous section, but how can we index with one value here?- Pg5, below equation 16.  "When an action with a higher regret" --> "action has a higher regret"- Pg5, equation 17.  Note the Beta term here is 'alpha plus gamma t'.  Later, in Figure 3, Beta is described instead as 'x + t(y-x)/T', and the text describes x=alpha and y=gamma.  Why aren't the equation and the later use stated in the same way?- Pg5, end of section 3.  The phrase "This setting can help agents have a certain chance of producing more effective strategies".  I didn't follow much of the section, but it appeared to be describing a convergence proof for using B terms in TRM.  But I didn't see any intuition described for why this might converge faster - just that it should converge.  What is the motivation behind this sentence, saying that it should have a chance of producing more effective strategies?  Is there a *proof* of faster convergence?  If not, what is the intuition behind why this might help?  As it stands, this seems like an unsupported claim.  Even saying "In our later experiments, we will show that some choices of alpha, gamma appear to converge faster than CFR" would be better than this claim.  Further - what is the intuition for alpha and gamma?  Gamma is described as affecting noise...  How about alpha?  Is there some intuition for how we should tune them, other than through a parameter sweep to try many values and see what works?- Pg5, "A New Regret Updating Method".  This section describes a new approach for updating regret, which seems most similar to Optimistic CFR.  It looks like it can be used independently of TRM.  However, this section does not appear to give it a name!  This makes it difficult to describe later in the paper.  How would we describe using it without TRM?  What should we call it?- Pg5, "New Regret Updating Method".  A few grammar issues: "leads to faster converge", "Linear CFR calculate the t iteration regret has a weight".- Pg5, above equation 18.  "redefine two regret" --> "define two regret"?  Nothing is being redefined.- Pg6, first line.  "The regret \sigma^t_i r_i^...".  Is that leading "\sigma^t_i" a typo that should be removed?  It doesn't appear in equation 19, which just defines r_i^...- Pg6, "New Regret Updating".  More confusing uses of "output strategy".  It seems like "current strategy" from context, but it's really important to be clear about this!- Pg6, "New Regret Updating".  "Second, when the CFR+ / Linear CFR algorithm iterates many times, according to experience, each iteration of its output strategy is close to the Nash equilibrium".  At least for CFR+, while this has been found in practice for specific  games like heads-up limit hold'em, I am not aware of any theoretical proof that the current strategy converges.  Further, it's clearly not true for most of the computation (or else we would stop, since our goal is to get close to Nash equilibrium!).- Pg6, "New Regret Updating".  Similar to the TRM section, this section ends with an unsupported claim that this will help the algorithm converge faster.  This appears to be based on intuition, and not from proof or prior experiments, so it should be described as a hypothesis, instead of a fact.- Pg6, Experiments.  "To verify ... TRM algorithm and regret matching method" --> "regret updating method"?  TRM was changing regret matching, and "new regret updating" was changing the update.- Pg6, Experiments.  Incorrect statement.  Leduc Hold'em is described here, and Southey 2005 is (correctly) cited.  Leduc Hold'em is indeed a standard benchmark game.  However, what the authors describe here is not the standard version of Leduc Hold'em!  Leduc Hold'em is a limit poker game, where bets must be of a specific size (See Southey 2005, pg2, under "Leduc Hold'em").  This limit-betting game is the standard version used in other CFR work.  What the authors describe here, in the sentence "In Leduc Hold'em, the player may wager any amount of chips up to a maximum of that player's remaining stack", is a no-limit poker game.  This is a significant difference, and means that the results in Figures 3 and 4 cannot be compared against Leduc Hold'em results in other papers!  Further, the authors don't describe the stack size they use for Leduc or Big Leduc, so we can't even tell how big the game is, or replicate the experiments.  Ideally, the authors would use the standard limit game, so that the results can be compared against other work.  If they want to stay with no-limit, they should describe it as no-limit in the text, clarify that it is NOT the standard benchmark game, and also state the stack size.- Pg6, Experiments.  Frequent grammatical errors throughout this section.- Pg6, Experiments.  Describing Leduc poker.  "If a player's private card is paired with the community card, that player wins the game; otherwise, the player with the highest private card wins the game."  This is not true!  The player with the highest-ranked hand wins a *showdown*, if one occurs.  But if the higher-ranked player folds later in the game, they will still lose.- Pg7, Figure 2.  Why use 'distance to Nash' on the y-axis here, instead of 'exploitability' for consistency with later results?  That would be clearer for the reader.  Also, why use a log-linear plot, when a log-log would likely be easier to read, and also consistent with the remaining figures?  Also, typo: 'equilibriumin' --> 'equilibrium'- Pg7, Ablation Studies.  This is not an ablation study - it is a hyperparameter sweep.  An ablation study is when components of a technique are removed, to show whether the technique's success comes from one particular component, or a set of components in combination.  This section is just sweeping over B and stochastic B values, and is not an ablation study in any sense.  However, note that this paper really does need an ablation study: examining the performance of CFR, CFR+TRM, CFR+"New Update Method", and CFR+TRM+"New Update".  But CFR+"New Update", without TRM, is not examined in any of the figures.  That would have been a very important ablation study to have in this paper.- Pg7, end of Ablation Studies.  "These results valudate that RM converges to..."  Should RM be TRM?- Pg7, start of Poker Games.  "Since we do not know what Nash equilibrium strategies are..."  First, a clarity issue - the text means that we don't know what they are for these games...  Except that we do: we can compute them to an extremely tight tolerance (far closer than CFR gets) using a Linear Program, and Leduc Hold'em is small enough to do that.  The *real* problem, however, is that there can be multiple equilibria in these games, so measuring "Distance to Nash" is not straightforward: distance to which Nash?  Distance to any Nash?  Do we have algorithms for computing all Nash for a game?  And so on.  That is why exploitability is the better measurement than distance here.- Pg7, typo.  "iteartions" --> "iterations"- Pg7, poker games.  "This is because in the experiments of CFR on Leduc Hold'em, there are a large portion of histories with n average."  This sentence makes no sense to me - please clarify.- Pg8, Figure 3.  Several issues here.  What is the unit on the y-axis - exploitability in chips/game?  Why use log10 on the y-axis and log2 on the x?  This would be clearer with log10 on both axes.  And clarify - is this the exploitability of the average strategy?  Even without the "output strategy" confusion described earlier, it should still be stated to make this clear.- Pg8, Fig 3.  As noted earlier, the definition of B here differs from the one given in Equation 17 - why?  Also, in Figure 2, an 'a' value like 'a0.1' meant randomly sampling a Beta value in (0, 2a).  Is that random sampling of 'a' values in Figure 3, like 'a1.005-0.995' also true, or are these deterministic B values as given by the equation in the Figure 3 caption?  If deterministic, maybe avoid using 'a' in the name to avoid this confusion.- Pg8, Figure 4.  "The exploitability (y-axis) are reported in T iterations".  What does this mean?  The y-axis measures exploitability (after small t iterations), and the x-axis measures iterations.  Also, 'itearations' --> 'iterations'.  Also, what does "RTRM" stand for?  I know it's the new regret updating method, and TRM is TRM...  so what's the R?  Once again: the "new regret updating method" really needs a name that we can refer to it by.  And this figure needs to demonstrate that technique without TRM.- Pg8, under Figure 4.  "We use the network in deep CFR to save the startegies..." First, typo: 'startegies' --> 'strategies'.  But further - wow, Deep CFR is a lot of complication to add to this!  The memory cost of CFR usually involves two equal-sized arrays: one for regrets per infoset, the other for the accumulated average strategy.  The "New Update" method would require another array (so a 50% increase in memory) to store the previous iteration's current strategy.  That's a bit annoying, but not bad: if we can afford the memory to solve Big Leduc, then a 50% increase should not be an issue.  However - Deep CFR adds A LOT of complexity, and approximation error!  Leduc and Big Leduc are toy games (*tiny* compared to the full-scale games used in CFR research like Limit and No-Limit Hold'em) and this paper is proposing and justifying the new techniques.  That is best done with a clear experiment focusing on your contributions, without adding in extra complications like Deep CFR in order to "avoid consuming too many storage resources".   Authors propose to apply information bottleneck to network structured data which is represented by graphs whose nodes are assigned features. The idea seems promising but the authors need to improve their manuscript considerably. In particular, the probabilistic model underlying the IB framework needs to be made clear right from the start. Which random graphs do you consider ? - "... GCN outputs the node embeddings X from the following process:... " what does that mean ? - "...the GIB seeks for the most informative yet compressed representation Z by optimizing the following objective .. " what is the domain of the optimization problem here ? and what do you mean precisely by "compresse representation"  *Summary: This paper tries to study unsupervised domain adaptation problem via minimized joint error. The authors propose an upper bound including joint error term, and design algorithm on the basis of the theoretical results. Good experimental results show the effectiveness of the proposed method.*Strengths:+This paper studies unsupervised domain adaptation in the perspective of joint error, which I think is one of the most important issues in the deep regime. I really like the topic of the paper.+The proposed method works well in several benchmarks.+This paper conveys some interesting conclusions by showing some interesting examples in figures. Hence this paper is very interesting and easy to follow.*Weaknesses:I like the topic of the paper, nevertheless, I'm not convinced by the theoretical results, and some claims are not well supported. This is my main scoring basis. Also, I think the practicality of the method needs further verification. The specific comments are as follows:-Unreliable theoretical analysis. I don't think introducing true labeling functions is a good choice for minimized joint error. We can see that $C_{S, T}(f_S, f_T, h)$ is actually an upper bound of joint error in inequality (2). Nevertheless, I don't think it makes any sense and it is just an upper bound. On the one hand, the bound is not tighter; on the other hand, without assumptions about $f_S, f_T$, the bound is pointless. For example, we can set $f_1$ and $f_2$ predict opposite results and then $C_{S, T}(f_1, f_2, h)$ is larger than $M$ for any $h$. In this point of view, the theory actually relies on assumptions about the $f_S$ and $f_T$.-$f_S, f_T$ are contained in hypothesis space is a very strong assumption. In some real scenarios with high-dimensional input spaces, the true labeling function may be very complex and can not be contained in the hypothesis space. Additionally, in order to ensure that the bound is not trivial, authors even need to restrict $f_S, f_T$ in subsets of hypothesis. -The assumption that $f_T$ can be found by source risk minimization with a smaller weight may not be reliable. By the result of [i], neural networks could memorize any labels. So, weights may not impact the memorization of source data, and the claim $\gamma$  is accuary and used as weighted source error is very confusing. Do you mean a small learning rate for source loss will lead to a low accuary, and their relationship is approximately linear? I think this claim is clearly refuted by [i].-The assumption that $f_T$ could be achieved by source loss minimization and pseudo label training needs theoretical guarantees. I think the method of using pseudo label is difficult to guarantee performance. When the two domains are distant, pseudo label may be unreliable. Using pseudo label may be okay for relatively simple benchmark. Nevertheless, I doubt whether it is helpful for solving complex problems, especially many practical problems. I am very interested in the performance of the proposed method on domainnet[ii], which is a difficult benchmark for domain adaptation.-The experimental results are very sensitive to hyperparameters and may not be applicable in real problem. The proposed method achieves best results on different datasets with different choices of $\eta$ and $\gamma$. However, how to select a proper hyperparameter is not well stated.-The motivation of the paper is similar to [iii], which shows some interesting empirical results. This paper can give the value of joint error empirically, similar to [iii].[i] Zhang et al. Understanding deep learning requires rethinking generalization.[ii] Peng et al. Moment matching for multi-source domain adaptation.[iii] Chen et al. Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation. The paper proposes a defense that works by adding multiple targeted adversarial perturbations (with random classes) on the input sample before classifying it. There is little theoretical reasoning for why this is a sensible defense. More importantly though, the defense is only evaluated in an oblivious threat model where the attacker is unaware of the defense mechanism. As has been argued again and again in the literature and in community guidelines such as [1, 2], the oblivious threat model is trivial and yields absolutely no insights into the effectiveness of a defense (e.g. you can just manipulate the backpropagated gradient in random ways to prevent any gradient-based attack from finding adversarial perturbations). The problem with oblivious attacks is clearly visible in the results section where more PGD iterations are less effective than fewer iterations - a clear red flag that the evaluation is ineffective. The paper also fails to point out that Pang et al. 2020, one of the methods they combine their method with, has been shown to be ineffective [2].This work clearly violates the guidelines and best practices that the adversarial robustness community has developed and the claims are not substantiated by the evaluation or the results. Fixing this problem will require developing adaptive attacks that are effective against the proposed defense. [2] is a good starting point, in particular the attack against Pang et al. 2020. This will require time, care and effort to get right, and I don't see a way how reliable results from a new adaptive evaluation can be generated during the relatively brief rebuttal period.[1] On Evaluating Adversarial Robustness, https://arxiv.org/abs/1902.06705[2] On adaptive attacks to adversarial example defenses, https://arxiv.org/abs/2002.08347 #################################### Summary ######################################The paper presents an inference-time defense which generates single-step attacks towards other classes, and adds a sum of all these perturbations projected to an $\ell_\infty$ ball to the original image before inference. This combined with other existing methods achieves 69.3% robust accuracy on CIFAR-10 dataset. The authors claim to achieve a 11.4%  boost in robustness without the use of adversarial training. ##################################### Details #######################################  -  The threat model chosen is not justified. The paper considers a white-box threat model where only the network model (architecture and weights) is accessible to the attacker. Even in the experiments section, only oblivious attacks which are unaware of the defense mechanism are considered. As explained by Carlini et al. [1], for white-box or black-box defenses, it is not reasonable to assume that the defense algorithm can be held secret. The only secret can be the randomness chosen during test time. In this case, the authors select a random subset of classes for the single-step attack during inference, and it can be assumed that the attacker is not aware of which classes are chosen. Apart from this, it must be assumed that all other details related to the defense are known to the attacker.  This is because an attacker is assumed to be infinitely thorough, without any computational restrictions, towards finding an adversarial perturbation within the threat model if it exists. The attacker could even use a naive attack which considers infinitely many random samples in the constraint set to fool the model. This random sampling based attack would almost surely succeed if an adversarial perturbation exists within the constraint set, and it also does not require knowledge of the defense. However, for practical evaluation, where we are limited by the computation available, we need to make use of the knowledge of the defense to find the strongest possible attack.   -  While Carlini & Wagner define the Zero-Knowledge threat model in their earlier paper [2], they clarify in their subsequent work [1] that there is no justification for such a threat model, and it was defined only to highlight that some defenses were not even robust against such a weak threat model.   -  Given that the defense must be known to the attacker, it is possible to construct an adaptive attack which includes the defense (addition of single-step attacks to multiple random classes) in each attack step. Different random classes can be chosen in each step of the attack. This is similar to the attack constructed by Tramer et al. [3] to break the defense by Pang et al. [4]. Using this attack, the accuracy can potentially be brought down to the baseline method (IAT). Another possible adaptive attack is EOT [5], since the proposed defense involves randomization.   -  All the baselines and methods that the proposed defense is combined with, are broken in prior work. The work by Guo et al. [6] and Xie et al. [7] are broken using EOT and BPDA attacks by Athalye et al. [5]. The defense by Pang et al. [4] is broken by Tramer et al. [3]. Hence, they are not justified as baselines, and their contribution in the proposed combined defense can also be nullified using the same attacks.   -  Robustness evaluation has only been done using PGD-200. Robust accuracy needs to be reported on more recent attacks such as AutoAttack [8] and Multi Targeted attack [9]. As mentioned above, even for these evaluations, the defense needs to be considered while generating the attack. [1] Carlini et al., On evaluating Adversarial Robustness, https://arxiv.org/abs/1902.06705[2] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods, ACM Workshop, 2017[3] Tramer et al., On Adaptive Attacks to Adversarial Example Defenses, NeurIPS 2020[4] Pang et al., Mixup inference: Better exploiting mixup to defend adversarial attacks, ICLR 2020[5] Athalye et al., Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, ICML 2018[6] Guo et al., Countering adversarial images using input transformations, ICLR 2018[7] Xie et al., Mitigating adversarial effects through randomization. ICLR 2018.[8] Croce et al., Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks, ICML 2020[9] Gowal et al., An Alternative Surrogate Loss for PGD-based Adversarial Testing, https://arxiv.org/pdf/1910.09338.pdf################################# Reasons for the score #################################The paper rests on a threat model that considers the attacker to be oblivious to the defense used. Prior works [1, 3] consider such a threat model to be unjustified. An adaptive adversary cognizant of the defense can potentially break the proposed defense by incorporating the defense during attack generation, and by using EOT attacks. Hence I strongly vote to reject the paper.  ##########################################################################Summary:This paper proposes "overinterpretation" which describes the phenomenon that CNNs could achieve high test accuracy while replying on features that lack semantic meaning. To demonstrate overinterpretation on CIFAR-10 and ImageNet, the authors use Batched Gradient SIS to select a small subset of pixels for each image and trained CNNs on the modified images. While humans can not make accurate predictions on those modified images, CNNs can still achieve high test accuracy. Lastly, the authors propose to use ensembling and input dropout to address overinterpretation. ##########################################################################Reasons for score:Overall, I vote for rejecting. The main reason is that this paper lacks novelty. While the authors pointed out a realistic issue of CNNs that they could rely on irrelevant features for predictions, this drawback has been largely explored in previous works. Furthermore, the solutions (simple ensembling & input dropout ) proposed by the authors have also been thoroughly studied in previous works.##########################################################################Pros:This paper points out a realistic pathology that is shared by mainstream CNN architectures.The paper is well-organized.##########################################################################Cons:This paper lacks novelty. Both the phenomenon of "overinterpretation" and the proposed solutions have been thoroughly studied in previous works.While the authors show that CNNs could rely on irrelevant features, they did not investigate the cause of their behavior. Specifically, the paper does not investigate whether such pathological behavior is caused by the properties of the datasets or it's originated from the model architecture.Predictions generated by neural networks are uncalibrated. It's questionable to directly measure the confidence of the models by their raw prediction values as in Section 4.######################################################################### The paper proposes to use a set of input examples, x1 to xn, having a common label y, and use them together for better classification. And use these shared label examples as additional information during model pruning.I have multiple challenges to understand this research paper:1. Clarity of Writing:The paper is very tough to read and understand. The authors jump through multiple levels of motivations, starting with information retrieval and then to approximation of database queries. But then the rest of the paper talks about loss averaging and results are shown using a CNN model on CIFAR10, and TinyImagenet. Either the whole motivation on IR aspects can be removed or relevant experiments and approach be proposed. At multiple places, I am either lost or confused on what is the problem that the paper is trying to solve. Example, after reading 2 pages of the paper, the authors state, "We depart from the original motivating information retrieval scenario, and henceforth consider a simpler, toy problem which we call the shared-label prediction problem."2. Misleading Title and Takeaway in the paper:The paper title, abstract, and motivation says to use "More Side Information". While shared-labels is not side information or additional information. If you have 10 instances to classify, instead of classifying them independently, the paper is trying to classify them together. So, there is no side information used in the approach. Also, this is not a "structure" present in X.3. Incorrect or Insufficient assumptions:The paper makes a lot of strong assumptions, which are not practical:a. What if multiple instance, x1 to xn of the same class label is not available ? In few shot learning or one shot learning scenario. b. "We assume the network and the pruning methods are state-of-the-art, and it is not our goal here to improve them" -  I do not understand the need for introduction model pruning for shared label classification. If the goal is not to improve pruning methods, then why do network pruning, at all?c. "For all methods we study, fixing the information size n, our experiments suggest that there exists a sweet spot phenomenon, or a "compression threshold" in the sense that RI, as a function of Á, has a global maximum Á" - There is no proven approach that such a threshold should exist for any dataset/model combination. I do not agree to this assumption.4. Lack of Novelty:Most of the approaches explained in Section 4 are just averaging the loss of the tuple of samples. When we average the samples, it is automatically considered that the tuple of samples are drawn from i.i.d. And in the proposed CNN-LSTM approach, the tuple of samples are "sequentially" classified. That raises more questions than answers - why sequence, and in what sequence? While the paper does not discuss any of these important questions.5. Appendix has more information than the paper itself:Most of the detailed and important information about the paper, including primary details about the approach, the model architecture, and many experimental results are in the appendix. At many instances, the paper reads like an index to the appendix. Example, section 4.3, "We investigate two kinds of architecture, see Appendix B for further information". Throughout the paper we do not have information the architectures. Even the proposed approach in 4.4, "An illustration of this architecture is shown in Appendix A.1"6. Lack of experiments:The proposed approach of CNN-LSTM is comapred with baseline methods : loss averaging, graph based averaging of loss. And the paper claims that the CNN-LSTM approach is better than the baseline methods. This is insufficient. The paper fails to compare with other relevant techniques in literature and place the paper empirically among the others papers in the literature.  In this paper, it is proposed to have a new problem setting of "unsupervised feature learning", which as claimed, is supposed to be different from standard continual learning settings. In this proposed setting, a data point can be seen only once by a model for training. I think, this is too strict a condition, as it is reasonable for a model to keep a data point in memory for a short while so as to use it for training across multiple epochs. In the experimental setup, neural baselines are trained only for a "single" epoch, in consideration of the proposed problem setting, which doesn't make sense for all practical purposes as neural models need a decent number of epochs for training. Furthermore, the final classifier used after learning the unsupervised representations is k-NN which is again unrealistic in the context of continual learning literature, especially in the context of neural baselines. All of this is justified for accommodating the  proposed problem setting which is itself not well motivated. The proposed model for the introduced problem setting is more of an engineering approach, relying upon some basic techniques such as clustering, novelty detection. There is no clear motivation for the learning algorithm, it is not clear how the model is optimized wholistically. It is more of a heuristic driven approach. The model is claimed to be brain-inspired; for instance there is a component in the proposed model which has a "hierarchy of increasing receptive fields", which is nothing but CNN-like neural net.  For experiments, I have the following suggestions.(1) In Fig. 3, x-label should be changed to something more appropriate, number of new classes introduced.(2) Results are good for MNIST dataset primarily.(3) Any other evaluation metrics besides accuracy? Accuracy can be misleading for multi-class scenario. (4) Report accuracies separately for new classes introduced and the old classes.(5) It needs to be clarified exactly what "class boundary information" the baseline methods have access to.(6) Only 10 new classes introduced from the 5 phases, what about the datasets with a large number of classes? Summary:The paper proposes a variation of the conditioned Transformer-based language model. The authors use POS labels (for English and Chinese) and participial construction labels (only for Chinese) to control the output of the decoder and show the results are better than an unconditioned generation with GPT-2 in terms of several metrics.Cons and questions:- The paper describes a generic conditioned language model approach, I can see no novelty of methods or results here.- No definition of "structures" given. Instead, the authors use fuzzy formulations like "multiple types of multi-granularity structure information" or "the auxiliary model can be any credible model or tool that can extract soundable structure information from the template." In fact, the paper provides the results of experiments with POS labels and participial construction labels (only for Chinese).- No comparisons with other known syntax-aware language models are done (consider [arXiv:1909.02273], DOI:10.18653/v1/N19-1118, DOI:10.1109/IALP48816.2019.9037672, etc).- No exact details on the structure encoder or decoder pre-training or finetuning process are given, although they can represent some practical interest.- No clear description of the resulting architecture is provided ("We only modified the input representation and few parameters of transformer").- The reason for using a Transformer encoder is ambiguous ("both the transformer encoder and decoder are adopted here is that we want each token in sequence to aware its local and global structure information"), the logic of the number of Encoder layers selection is hard to check without details of the training process.- The Structure Matching evaluation was done through human assessment. However, no clear rules of the process are given.Minor comments:The paper has a lot of typos and grammar issues. Here are some samples:(1) "importan-t" -- an incorrect word-wrap.(2) "quality ( like Fluency" -- a space after the open parenthesis(3) "extracted structure information are regarded"(4) "word can aware the global structures"(5) "which can be any precede words for continue generation"(6)"reagrded"There are many more, I suggest at least to use some automatic spellchecking tools. SUMMARYThis paper presents a text generation model conditioned on desired structures. The proposed method is essentially a translation model from structure information (represented with multiple sequences of tokens) to a text. This study converts a text into structure information such as part of speech (POS) and participial construction (PC). Then, this paper proposes Structure Aware Transformer (SAT), which is essentially the same as the Transformer architecture. The experiments use datasets of Chinese lyrics and English Penn Treebank. This paper reports that giving structure information improved the performance in PPL and BLEU compared with GPT-2.PROSIt was nice to confirm that we can control language generation from POS and PC.CONSThe proposed method presented in Section 3.3 is identical to Transformer except that:+ An input consists of multiple sequences of structure information (e.g., pos and pc)+ Input embeddings are sums of structure embeddings (Equation 7)For this reason, I do not think Structure Aware Transformer (SAT) is a novel proposal. If this explanation is sufficient, I think that the descriptions in Section 3.1 and 3.3 are redundant.Because structure sequences (POS and PC) are obtained from sentences in the test set, it is not surprising to see performance improvements of language models. In other words, predicting word sequences with some hints (POS and PC) is much easier than doing without any hint (GPT-2). For this reason, the findings in this paper are not convincing.QUESTIONSCould you explain the major difference between the proposed method and Transformer (excluding minor differences in how input embedings are computed and hyper-parameters such as the number of layers)? The paper attempts to formulate the task of robust OOD detection by expanding the usual definition of OODs, and then presents a theoretical analysis in the simple setting of a Gaussian data model by porting results from adversarial defense. An experimental comparison is also presented. Pro:+ The paper addresses a very important problem which is growing in importance. A number of approaches have been recently proposed to address this problem that attempt statistical, explanation-based, and relational/semantic techniques. The paper takes a simple approach of class augmentation and then picking hard OODs to solve this problem. Cons:- Why is it surprising that the "majority of auxiliary OOD examples may not provide useful information to improve the decision boundary of OOD detector"? This seems to be rather obvious both for the toy case of Gaussian model or in general. It is well known that hard examples help train. The triplet loss function and other methods have been used for years based on this insight. The paper appears to present this obvious observation as the central finding of the paper.  For e.g. in the context of embedding, see how finding hard examples has been found to be useful in https://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf - The paper appears to be mixing a lot of different problems to formulate a very generic notion of OOD. Typically, OODs have been used for out of distribution datasets (SVNH for CIFAR10), new classes on which model was not trained (leave some classes out of CIFAR10), or transforms such as rotations or other transformations of inputs. The paper mixes adversarial examples to it and then considers even further compositions. While there is significant effort to break OOD by building a useful taxonomy of it to refine the sources of uncertainty and address these in a principled way, the paper creates a monolithic problem of detecting when inputs don't belong to the training distribution for any reason. This creates a rather ill-defined mathematical problem because modeling the exact distribution of the training data is quite infeasible. The theoretical treatment of the experiments in the paper fail to convince the reviewer that there is an advantage in merging these separate problems into a single problem.- The theoretical treatment are directly borrowed from http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness.pdf where the treatment was for adversarial examples - since, the paper's definition of OOD includes these, the previous results are directly applicable. It would help to make this obvious in the main text instead of having this reference in the appendix. The connection is very strong. Also, this reviewer currently does not see the challenge in lifting the result from the reference to the theoretical analysis in this paper (please see question below). - The experimental evaluation is limited and does not meet the usual set of experiments reported for OOD detection which makes it difficult to understand the empirical benefit of the presented approach. - While there are other methods that also use "exposure to outlier" in OOD detection, it is debatable such an approach would be useful in practice, particularly, if one also included adversarial examples. But given past literature on this topic, the reviewer is less concerned about this aspect. In summary, the idea of using hard examples is not new. The definition of OOD to include adversarial examples is not well motivated. The theoretical results are borrowed without significant extension from literature on adversarial defenses. The experimental results do not meet the expectation of a paper on this topic to judge its empirical value against existing approaches. Questions for the author:- Why is it helpful to merge different problems of adversarial example detection, detection of data away from training distribution due to new classes, detection of corrupted inputs into a single problem? Does a uniform treatment yield some new insight or enable some particular approach? Does it beat the state of the art in any of these sub-problems? - Can you elaborate on the nature of "natural OODs" in the experiments so that it is easier to understand comparison with ODIN, Mahalanobis, etc methods? They perform differently when OODs are because of new datasets or because of held-out classes. This "natural OOD" is better split into at least these two different types before evaluating the methods. - How is the theoretical analysis presented here not a trivial application (from adversarial setting to a superset definition of OOD) of results in http://papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness.pdf  and https://arxiv.org/pdf/1804.11285.pdf ? Could you help identify the challenge and the novelty in this extension? The paper currently appears to be building on the premise that if we think of adversarial examples as a subclass of OODs, theoretical results from adversarial examples are applicable to OODs. A simple supervised approach using hard examples is then presented to solve this problem. Its empirical evaluation appears to behind the state-of-the-art in OOD detection as well as adversarial defense, but mixing both does not make this any stronger a paper. In its current form, the paper appears to need better motivation and identification of novelty.  # OverallThis paper is inscrutable. The research question is unclear and the hypothesis is vague and imprecise. The metric being examined ("utility imbalance") is never justified in name or in definition, and broad, sweeping, unsubstantiated claims are made that it has to do with how the network uses its weights or the sharpness of the minima (among other aspects of deep learning). Section 3 seems completely unrelated to the previous analysis, and I don't understand how Sections 2 and 3 relate to form a cohesive story. I can't make sense of this paper - its question, its claims, or its method - and I'm an expert reader on topics of pruning and lottery tickets. I can't imagine what a non-expert reader would be able to glean from this paper.# ScoreI therefore strongly recommend rejection (score of 2).# To Improve My ScoreTo Improve my score, the authors need to:* Explain what the research question is, precisely.* Explain what the hypothesis is, precisely.* Explain (in detail) what the justification is for "utility imbalance" and why it corresponds to meaningful behaviors of a neural network. The authors will need to include evidence that this corresponds to the claimed attributes of a neural network, which will include a precise definition of what it means for a network to "utilize" a weight and how this metric measures that.* Explain why this is called "utility imbalance"* Explain jhow the results in sections 2 and 3 related to each other and the larger narrative/takeaway that they provide.* Explain why the network under study changes mid-way through the paper. Is there a reason for this? When I see this in papers, it's usually because the authors tried this on both networks and are only showing the positive results and are hiding bad results; to assuage this concern, the authors should show all results for both networks.From there, I'll need to re-read the experiments to see if I can make sense of them. In its current state, I do not believe that this paper makes a contribution to the scientific literature.# Notes## AbstractWhat do the authors mean by "the pruning mechanism"?## IntroWhat prior work has claimed that "the reason for training the large network [is] to obtain a good minimum"? I don't recall having previously seen this specific claim, nor the implied negation of this claim: that training a pruned network finds a "bad minimum." What makes a minimum "good" or "bad" and how can we measure this?"Cannot achieve similar performance when trained from scratch" - this only occurs with a new random initialization, as Frankle & Carbin 2018 show (at least for the small-scale networks they examine).What do you mean by a "utility imbalance"? Which networks aren't "utilizing all their weights," the unpruned networks or the pruned networks?After reading the introduction, I really have no idea what the paper is about. I don't understand the research question, the main hypothesis, the methodology, or the findings. the authors should make efforts to clarify this story. I'm confused, and that means I'm going to have a hard time making sense of the rest of the paper.I also can't make sense of Figure 1. I don't know which network this refers to, and I still don't know what a "utility imbalance" is.## Section 2Why is there an assumption that a large network "does not utilize all its weights and thus can easily be compressed?" There are many possible reasons that one can prune a network, and we often find in the literature that pruning reduces accuracy. The entire point of retraining is to recover this accuracy. But since accuracy went down, it seems that these weights did serve some purpose.Is N_small the same pruned architecture, or is it a different pruned architecture, a smaller dense network, etc.?What does it mean for a network to "utilize all its weights"?In Definition 1, does the pruned network include retraining?Why is this measure of utility imbalance meaningful or useful? Networks whose outputs have very different distributions may get similar accuracy, so KL divergence doesn't seem to be esxpecially meaningful here. I also don't see what this has to do with utility.#### Section 2.1.1I don't follow this logic, and I'm an exceedingly well-informed reader when it comes to lottery tickets. What is the purpose of this section? Is utility being used in a formal way or an informal way? If formal, I don't see a proof or derivation to support the claims here. This seems like an argument without any evidence.#### Section 2.1.2It is unacceptable that the name and details of the network used for this paper are buried in an appendix. The network in question, a small convolutional network, appears to be similar to the small convolutional networks used by Frankle & Carbin. These networks are not representative of larger-scale settings for lottery ticket behavior see ("Linear Mode Connectivity and the Lottery Ticket Hypothesis" (Frankle et al., 2020)), and the results in this paper should not be taken to be general.The first paragraph of 2.1.2 suggests that the "utility imbalance" metric has anything to do with the specified claim made by Frankle & Carbin, and I see no reason to believe that.I don't know how to interpret the "utility imbalance." It has a fancy name and it has a fancy definition, but I have no idea what it means. Graphs of the utility imbalance therefore aren't very enlightening.Why does the utility have anything to do with the network "utilizing the weights more effectively"?What does the utility imbalance have to do with the sharpness of the loss landscape?Why are the weights and SGD being anthropomorphized? e.g., "The weights is struggling to be utilized more, rather than SGD purposely differ the utility among the weights?" I'm completely lost in this section, as the above comments hopefully convey. I have no idea what is being measured, why it is being measured, or why the results have anything to do with the claims. These experiments are also being conducted on a single, non-standard network, so I don't understand why these results should generally be true.## Section 2.2What does it mean to "utilize a weight"?Why were the weights rescaled? I don't understand what "This was to control the number of feature maps and thus avoid any architectural bias." means.Why did the experiment suddenly switch networks from what was being used in 2.1?I made a good-faith effort to read the rest of the section from here, but I wasn't able to make much sense of why the evidence substantiated the claims. Please see my general comment at the beginning of the paper.## Section 3"Empirically, the generalization gap is known to be closely related to the geometry of the loss basin." This is hotly contested in the literature. I'm not sure you can say this so strongly.Figure 6 needs a legend to explain what the networks are. The text never explains what W*, Wp, and Wr are.I have no idea how this section relates to the rest of the paper. The paper is poorly written. This is especially the case for theproposed algorithm (the core contribution). This section is verydifficult to understand, and notations are awkward. Everything is abit messy. However, the experimental results are quite well presented(to be compared with the beginning of the paper). 1. Summary of paper:a. The paper contributes a deep RL approach to learning instructional sequencing. The approach called STEP starts by simulating tutor and student models. The tutor simulation is based on the RoboTutor ITS and the student simulations are fit to historical data of children interacting with RoboTutor using the HOT-DINA approach. The instruction sequencing model is then trained using PPO and evaluated using novel measures of sequencing decision impact on local and global learning gains (estimated from running student simulation). The paper contributes one set of experimental comparisons between four variations of the sequencing agent and the RoboTutor baseline in 8 runs of the two systems.2. Strengthsa. At a high level, the proposed approach is well motivated, using RL to optimise parameters in the existing tutoring system or more granularly make sequential decisions about what activity to provide to the student.b. The local and global reward design is a strong contribution that uses historical data and the student simulation (knowledge tracing) to estimate credit for policy decisions in a counterfactual manner.c. The future work discusses the limitation of not using actual children's scores to evaluate this learning model.3. Weaknessesa. Novelty/impact and context within related literature: i. It is difficult to judge the novelty of this work since the related work section is too brief and does not actually describe several works compared against. Additionally, table 4 is not descriptive enough, has potentially relevant work undescribed (Whitehall & Movellan's 2017 POMDP/policy gradient approach), and has work referenced previously missing from it (Yudelson et al. 2013, Pardos & Heffernan 2011).b. Experimental rigour:i. Two strong claims are made in the related work section that do not have sufficient experimental evidence. 1) A direct comparison of BKT to HOT-DINA for modelling student knowledge gains is required to show the impact of this central claimed contribution. 2) A direct comparison against Shen et al. 2018 is required to show that PPO is indeed more effective in this domain. This is also important since it is a central claimed contribution.c. Clarity: The clarity of the paper needs significant effort to improve. Instances below.i. The structure of the paper reads like a technical report rather than an empirical investigation. The contents would be far easier to understand with a different structure emphasising a research question, background to understand/motivate it, methodology to answer it, results, and discussion.ii. Several sections are far clearer in the supplementary data document. The reproducibility of the paper is boosted by this. Given the extra space available, several sections could stand to be transferred to the main paper. My original review did not include supplementary data and points around describing data and examples are boosted by adding them to the main paper.iii. Until significant rereads, it isn't clear what the relationship is between RoboTutor and STEP/this work. With my current understanding, RoboTutor is an external system that has been used to collect data on children learning various skills using it. This data was then used to evaluate STEP in terms of estimated learning gains.iv. The tutor simulator section describes how RoboTutor functions. The student simulator describes how the knowledge tracing model works. Example differences between activities, skills, steps, etc. would make the content clearer. v. In the tutor simulation section it states that the child can select activities, so this part is replaced by RL decision-making in agent type 3 and 4, right? This relates to my question about RoboTutor and this work. I am understanding that the current work simulates the exact decision-making process of RoboTutor but can vary/change that process according to the different agent types. Is that correct?vi. Bayesian Knowledge Tracing needs a citation and a brief explanation in a background section.vii. HOT-DINA needs a citation and a brief explanation in a background section.viii. Item Response Theory needs a citation and a brief explanation in a background section.ix. What is the difference in computational cost between a BKT approach and HOT-DINA?x. A clearer highlighting of what data was used would make it easier to read the article. What was the contents of the data collected to enable knowledge tracing in the student simulator? This also ties in to the comment about examples. Adding a running example of an activity, skill, step for the tutoring task would enable a description of what data is collected to measure student knowledge in the data set.xi. What do the guess, slip, learn, etc. Parameters measure?xii. "We use MCMC sampling for Bayesian inference with PyStan rather than the OpenBUGS Gibbs sampling package used in the original HOT-DINA work because PyStan is faster and handles larger datasets." This line needs references for all software used, but most importantly, a reference to the original HOT-DINA work is necessary.xiii. The sole paragraph on page 3 is difficult to parse and seems important to understand how the student simulator works. The paragraph is dense and conversational in style referencing a sequence of steps without making them clearer and referencing equations that are on the next page. It would be much clearer to have the exposition and equations interspersed and use pseudo-code or a flowchart to explain the steps performed to simulate the student (at least a list).xiv. "We can train different types of RL agents depending on their state space and range of actions, which depend on how far they depart from RoboTutors current decision policy." This would make sense as the start of a new section on the experimentation.xv. A stronger partitioning of content would also be achieved by calling a potential new section at this point methodology, experiments, or evaluation. This would also help organise the next section of state, action, and agent types into a concrete experiment for which the paper is describing the state and action spaces.xvi. In section 3.2, it is confusing to have states, then actions, then agent types described. It seems like one experiment with 4 experimental variants (agent types) and a baseline (RoboTutor).xvii. Table 2 does not convey much more information than the explanation before it.xviii. Figure 4 is very difficult to parse. Instead of showing 36 subplots (with 4 empty subplots) for 8 students and 4 agents comparing against the RoboTutor baseline, it would be far easier to compare performance against students or against agent types, by combining all 8 student type runs for each variant into a single variance-shaded run in a single graph (e.g. using https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.fill_between.html). That would allow for much higher information density and an at a glance comparison between the 5 runs being compared combined across all 8 students. At the very least this should be done by combining all 5 runs into 8 separate graphs, though the previous approach is preferable.xix. Related work is far too brief and does not make clear what is being compared for many cited works. E.g. It isn't clear why the works that specify reward a certain way in Doroudi et al. (2019) show a disadvantage to the current approach, the works in table 4 don't specify why the current approach is an advance over their contributions.d. Reproducibility: i. Section 3 (and the paper in general) contains far too little information about the policy representation, learning hyperparameters, network architecture, etc. to understand the contribution.4. Recommendation: a. Per the weaknesses in the review above, I recommend the paper for rejection. I don't think the weaknesses in experimental rigour can be fixed in time, content space, or degree to support acceptance. The significant editing required to fix the other issues also seem unrealistic in time and space.5. Minor Issuesa. "(Previous methods used reward=0 or 1 based on correct attempt or something else. Useful to mention this?)" There is a comment remaining in the paper that should have been removed. The paper intends to contribute a novel framework for optimizing instructional sequencing in ain intelligent tutoring system. More specifically, this framework uses deep reinforcement learning and evaluates learned policies on historical data.A major strength of this paper is working with real human data from an application with obvious positive human impact. That working with this rich data comes necessarily with only working with a small amount of data is understandable, and it is not a weakness of the paper.The most significant weakness of the paper is that it does not articulate a contribution that centers on representation learning -- the focus of this conference. A representation of student knowledge over time is learned (the 118-parameter HOT-DINA model), but this representation was already contributed by past researchers and does not seem to match the intended contribution of the paper. An action policy for controlling tutor behavior as a function of student knowledge state is also learned, but policy (or its internal details) are not examined from a representation learning standpoint. (For example, does some aspect of the different learned student-specific policies appear to recover/mimic another known aspect of those student identities? If so, would that be a good or bad result?)The next most significant weakness is that when the paper makes novel choices, those choices are neither evaluated nor strongly motivated based on the past literature. Why PPO over DQN? (PPO can work in certain settings that DQN cannot, but past work already demonstrated DQN for a very similar setting.) Why HOT-DINA over BKT? (HOT-DINA is a much more expressive model, but the small amount of valuable historical data for this setting may limit the effectiveness of expressive models due to overfitting.)Additional weaknesses are noted in the additional section-by-section feedback below.This reviewer recommends (2) strong rejection. This is not inherently a paper about representation learning, and, even as a generic applied machine learning paper, it does not sufficiently motivate or evaluate the intended contribution of a novel framework for using machine learning in a specific application.Questions for the authors:* Are there structural reasons why Q learning (e.g. DQN) cannot be applied in this setting?* Is there a way to verify that HOT-DINA is not overfitting in a way that makes evaluating the system on the same historical information used to train it unsound?Section-by-section notes:Title- Focus on instructional sequence policies (for ITS, using RL), was hoping for something that sounded immediately relevant to representation learning.Abstract- It sounds like the method might not be as important as the application.Hopefully well hear more about the representation learned because this is an ICLR submission.Introduction- The introduction doesnt state the intended conclusion or motivate the novel parts of the work for the reader.- Theres a learned policy in here, but what is the learned representation you want to highlight for this specific venue on representation learning (ICLR)?Simulating- This is how you fit a pre-existing representation to new data, it doesnt seem like this is the contribution of the paper.Training- Missing use of domain knowledge:  * Is the the 100 timesteps considered in PPO training based on 100 being a typical number of interaction steps with the ITS among the Tanzanian students?  * Even with a finite horizon, the rate at which students decide to exit the activity could be used to motivate a discount factor < 1. Presumably you have this information as well.- Notes like Useful to mention this? suggest the paper was submitted in an incomplete state.- Its interesting to list the design alternatives for representing actions, but each should be contextualized with references to past research that used something similar.Evaluating- The evaluation in terms of local and global impact is unfamiliar to this reviewer (who knows other RL+ITS work). Not enough information is given to pin down exactly what local impact measures.- What is the source and meaning of the historical baseline number?- Figure 3 is too busy to interpret. Consider presenting it as a chart that aggregates across students using a single line (plus error bars) to represent the mean (plus stddev) state over time for the two policies.- The way student-specific models are evaluated against historical data on those specific students suggests we are just testing on the training data. Why is this a valid methodology for this application? Cite past work on evaluating RL-based ITS systems to motivate your methods.Relationship- Consider covering work by others much much earlier in the paper so that the reader can understand why you made the choice you did (and that you can convince them that you know RL has been applied to ITS across many decades previously).- Near STEP uses a more powerful deep RL method -- it is true that policy gradient can be applied in certain applications where Q-learning cannot, but if we arent given a note as to whether this is the case in the current application. Based on previous work, it seems like Q learning was applicable. Thus, using policy gradient methods (including PPO) would seem to add needless complication.- This section indeed state how the current work is different from past work, but it does not motivate the differences. If others were successful with different methods, why change them for this paper?Conclusion- This paper contributes a novel framework -- what is novel is the combination of the HOT-DINA student knowledge model with the PPO reinforcement learning approach (within a larger framework shared by many other papers). I hardly agree with the assumption made in the graphical model c) and d) that each cluster of data has a different generating mechanism from others. First, it is named the ICM-conditional mechanism in the paper, which is very confusing since it contradicts the definition in the literature of ICM [1]. The ICM, in the literature, originally describes the independent autonomous for generating a sequence of variables in the causal graph, e.g., p(v_1,...,v_k) = \Pi_k p(v_k | Pa(k)), based on the assumption that the exogenous variables are independent with each other. Besides, the difference in terms of variation for digits 1 and 2, in the example made for supporting this assumption, should be encoded in the latent variables z, rather than the generating process. Since the latent variable z is defined to describe the high-level abstractions or concepts, including but not limited to the thickness, width, length in the example of MNIST. Therefore, the graphical model should be y -> z -> x, which is aligned with existing literature in nonlinear ICA.  Besides, even if the assumption is correct, the posterior distribution of p(z_Mk | x_Mk) should be varied across k, which hence cannot be learned by the same encoder E. Although the experimental results yield good results, it can not be interpreted in this way. The high-level spirit of this paper is based on an invalid assumption, which makes the promising results in experimental parts non-important. [1] Schölkopf, Bernhard. "Causality for machine learning." arXiv preprint arXiv:1911.10500 (2019). My review is going to be short because I truly believe that this paper, in the current form, does not reach the acceptance bar for ICLR.Indeed, the paper tries to show that SGD carries an implicit regularization along the procedure. The list of the contributions is summarized on page 2: the authors proposed a decomposition of the noise into two terms (i) a noise due to the subsampling of the gradient (ii) a noise due to the noisy labels. Neither this decomposition nor the conclusion from it is new. Worst, there are no proofs or experiments backing up there fuzzy conclusions.Despite, the fact the story behind SGD seems understood by the authors, I see no contributions in this paper. The paper presents a method for improving the accuracy when training CNNs from scratch on a small dataset. The method is as following: instead of training the whole network on the full images, one first creates a shallow model and trains it on the small crops from the dataset. The labels for the patches are set the same, as for the whole image. Then one adds more layers, freezes the previously trained and train on bigger patches and so on. The method is evaluated on 2 datasets: image quality assesment on LIVE dataset and scene classification on MIT Indoor 67. ******While the idea might be good, the paper itself is a low quality.Problems:1. Two things are mixed together: (a) progressive patch cropping, starting from small and ending up with big image and (b) progressive adding more and more layers, while freezeing earliest. That is not clear and justified to use both together.The cropping can be seen is very aggressize data augmentation and if fact is commonly used for training ImageNet CNNs, e.g. see [2]. The freezing and training is a variant of Net2Net[1], which is not cited. 2. Datasets useBlind image quality assessment is a bad task for testing the idea. Why? Because when image is of a bad quality, it can often easily be told from a small patch. That is why aggressize patch cropping and  keeping label the same is justified. That is not true for other tasks, like classification, object detection, metric learning and so on. Yet the method is claimed to be quite general. 3. There is no baseline on any of the standard dataset, not strong supervised baseline on the datasets paper proposed. By strong I mean, where hyperparameters are reasonably tuned and standard data augmentation is used. E.g. for the MIT Indoor67 the paper reports 42.95% accuracy. In the same time, course report from 2015 [3] has 43.8% accuracy. I don't see improvement from the proposed method.4. The model is not even specified. Is it custom CNN? VGG-style? ResNet-style?***I would recommend to start from the strong baseline and proper evaluation. If the method can improve on them and then  well presented, then it can be published.[1]. Net2Net: Accelerating Learning via Knowledge Transfer. Chen et.al, ICLR 2016 https://arxiv.org/abs/1511.05641[2]. https://github.com/NVIDIA/DALI/blob/1e9196702d991d3342ad7a5a7d57c2893abad832/docs/examples/use_cases/pytorch/resnet50/main.py#L116[3]. http://cs231n.stanford.edu/reports/2015/pdfs/ondieki_final_paper.pdf This paper addresses estimation of a certain type of OT distance, substance robust wasserstein distances, by Patty and Cuturi, 2019I don't think this paper is suited for publication since it lacks enough substance. 1)There is gross error in the abstract: the curse of dimensionality doesn't refer any cubic scaling, but on an exponential dependence in dimension.2)the first 4 pages are spent on elementary definitions. This appears as an unnecessary padding. I suggest authors put that kind of definitions in the appendix and/or cite relevant literature, e.g. the paper by Patty and Cuturi.3)The overall idea, although sensible, appears unjustified. Why would the community be interested in this problem? In the current papers, authors claim they are generalizing the results of Patty et al. Nonetheless it is unclear whether there is reasons for wanting to create such generalized framework. It would be helpful if the authors had a concrete application to showcase their results.4)Experimental results are weak, and comparisons with other methods are lacking, so it is hard to judge what are the actual gains. this work posits that invariant mechanisms exist in a dataset. a machine learning algorithm that is trained using gradient descent usually averages gradients across examples. the thesis is that by averaging gradients, information is lost. the method posits that in a gradient descent algorithm, instead of an arithmetic average, a geometric (or karcher) mean can be used to preserve information about invariant mechanisms - while ignoring confounders. there are difficulties in a straightforward application of the geometric mean, so a simple heuristic algorithm is developed, involving masking gradients depending on whether the sign of the gradient agrees across a batch of examples (or, whether some agreement threshold is reached). this algorithm is tested on a synthetic dataset, a semi-synthetic task on CIFAR-10, and coinbase, an RL algorithm. recommend major revisions and baselines. it is interesting ideas and it would do the ideas justice to put a lot of effort into a rewrite so the ideas are properly understood by the ICLR community. clarity needs to be significantly improved in the introduction and conceptual framing (mainly in intro/methods sections)the rest of the paper is largely well-written, the experiments are well-documented in the appendix, relationship to invariant risk minimization and causality is documented in appendix.baselines: sum vs geometric mean is a well-known effect in products of experts vs sums of experts work. i think this should be the right baseline to compare to. train separate classifiers and use them as a product of experts, and you should get the same performance as in this paper. definitely need to discuss this related work and how learning consistency /geometric mean measures differ and warrant comparison.the writing is more difficult to read than it ought to be. more work is needed to re-use existing concepts. for example, consistency is overloaded here (it might confuse some readers from a statistics background). the paper's title has the concepts of varying and explanations (variance and explanations do not appear in the method). small things like this abound. trying to find specific examples where i have a strong opinion to help:* instead of consistency, it may be worth spending some time to think of another word that is less confusing. congruence? concordance? 'learning invariant mechanisms with geometric mean gradient descent' is an example of a title that would be easier to understand: (1) mechanisms relates to prior work such as the pearl work that is cited (indeed, in the appendix the authors state 'causality... is a key element informing our exposition'). a causal mechanism is a well-known concept to the ICLR community. (2) concordance is still overloaded (e.g. this is a major concept in biostatistics) but not to the degree that consistency is. similarly, if you choose congruence, this could have the nice geometric interpretation in the hessian example that is helpful for understanding the work in figure 3.* the david deutsch quote distracts and confuses, remove it (i also know some researchers that might be immediately skeptical of a david deutsch citation in an ICLR paper). see above for ideas on title changes.* the first example is helpful to understand the goals of the paper. however, figure 1 --  the symbol with the plus sign and arrow is confusing, all the axes tick labels need to be much larger* the 'an example' chess example in the gray box on page 2 is unnecessary and distracting. instead, the discussion of the relationship to arjovsky et al could be inserted (with grass and cows), which is much more interesting and useful to a reader. * equation 1 engenders a double negative throughout the conceptual framing of the paper. i highly suggest considering renaming this to a 'consistency' score to reflect the name of the method; this will make it easier for readers to understand. * 'patchwork' may be another jargon word, try to find another one that is easier to understand. on page 3: 'low consistency of a classic patchwork solution' is loaded with jargon. first, a patchwork solution is not defined here or in previous work. second, a 'classic patchwork' solution is more confusing. removing adjectives such as 'classic' or even 'patchwork' throughout the paper may help readability and clarity. * i found the synthetic memorization dataset very difficult to understand. never start a sentence with math like p(y | x_{d_M}), it looked like it was connected to the previous with a \cdot. ideally write out the functional form of the mechanism. the axes in figure 5 are not labeled, so i have a hard time understanding what we are looking at: are the x and y axes d_S or d_M? how is environment A vs environment B defined? * re: wording -- ideally delete shortcuts. do not introduce more unnecessary lingo, because it leads to sentences like 'the shortcuts are not shared across environments, but provide a simple way to classify the data, even when pooling all the environments together'. this is asking a reader to do a lot of work: (1) what is pooling? this has a standard usage. (2) what are environments? this is used in RL (3) what are shortcuts, and how do they relate to environments? trying to rewrite this, i would want something with fewer novel concepts, that focuses on mechanisms that are invariant across datapoints or groups of datapoints (groups of data arising from causal graphs, e.g. in causal inference terminology).* extremely minor, a few typos and inconsistent capitalization (section 3.1 is not capitalized, figure titles not capitalized, paragraph headings, figure legends not capitalized , good to pick one and stick to it) The paper is about an interesting setting, where observations correspond to lots of individuals evolving over time. The twist is that individual identifiers are not available.  Thus from one time point to the next, the observer does not know which individual is which. The problem is to learn a time series model from this, and models considered here are in the form of an SDE.In my first reading of the paper, I thought the topic was very interesting but I struggled to follow some parts of the manuscript, which I thought was overly unclear. Some of the notation is undefined or ill-defined, the general reasoning is hard to follow, the notation does not help identifying the objects that are observed from the objects that are simulated, there is a lack of acknowledgement of limitations of the proposed method, the synthetic examples are not well-motivated, and various other fairly standard concerns.However, during my second reading I looked at the reference Wang et al, "Learning Deep Hidden Nonlinear Dynamics from Aggregate Data", published in UAI in 2018.  From this I got more serious concerns.The authors' abstract starts with "Learning nonlinear dynamics from aggregate data is a challenging problem since the full trajectory of each individual is not available, namely, the individual observed at one time point may not be observed at next time point, or the identity of individual is unavailable due to technical limitations, experimental cost and/or privacy issues." Wang et al's abstract mentions: "However, in most of the practical applications, these requirements are unrealistic: the evolving dynamics may be too complex to be modeled directly on observations, and individual-level trajectories may not be available due to technical limitations, experimental costs and/or privacy issues."The authors write "In the work of Hashimoto et al. (2016), a stochastic differential equation(SDE) was adopted to capture the dynamics of particles directly on observations, in particular the drift coefficient is parameterized by a recurrent neural network."Wang et al. write: "Modeling the dynamics on aggregate observations have been investigated recently in (Hashimoto et al., 2016), where a stochastic differential equation (SDE) has been used to capture the transition directly on observations Yt."The authors write: "There are many existing models to learn dynamics of full-trajectory data. Popular ones include Hidden Markov Model (HMM)(Alshamaa et al., 2019; Eddy, 1996), Kalman Filter (KF)(Farahi & Yazdi, 2020; Harvey, 1990; Kalman, 1960) and Particle Filter (PF) (Santos et al., 2019; Djuric et al., 2003). These models and their variants (Deriche et al., 2020; Fang et al., 2019; Hefny et al., 2015; Langford et al., 2009) require full trajectories of each individual, which may not be directly applicable to the aggregate data as we mentioned earlier. "Wang et al. write: "Existing models such as Hidden Markov Model (HMM) (Eddy, 1996), Kalman Filter (KF) (Harvey, 1990) and Particle Filter (PF) (Djuric et al., 2003) are popular methods with hidden variables. However, these models and their variants (Langford et al., 2009; Hefny et al., 2015) require individual-level trajectories, which may not be available, as was mentioned earlier."--Based on these strong similarities I do not think that the proposed manuscript is suitable for publication. The authors introduce a log-barrier extension loss term enforcing soft constraints on the range of values to enable fully end-to-end quantization-aware training. Strengths of the paper: - The paper addresses an important topic, because there are increasing concerns in performing fully end-to-end low precision training to deploy on low-precision hardware. - The method has a practical goal and could be interesting for practitioners Weaknesses of the paper: - Lack of positioning with respect to the SOTA quantization-aware training(QAT) and post-training quantization(PTQ) schemes, there are plenty of missing related literatures on both quantization schemes. Some statements in the background and related work could be wrong. For example, QAT also focuses on efficient inference as well as PTQ. The levels of practical applicability of a variety of quantization solutions have been introduced in DFQ(Nagel etal., 2019). Survey on the related work is not sufficient. - Having a benchmark would be interesting if it will include some SOTA methods and evaluates with them. The comparison targets are mostly out-of-date. It is lack of convincing evaluation results to support the proposed scheme.- Organizing the whole contents is ok but not good enough for the readers to easily follow and understand. Detailed comments: (1) The terminology on swamping might not be familiar with the ML community. Explaining the criticality of swamping problem is not good enough in the intro. You should provide how critical the problem is on the low-precision hardware with the other SOTA quantization schemes. For example, the probability of occuring swamping without applying the proposed scheme, etc.(2) Evaluation results provided in the paper are just for comparing accuracy. Accuracy loss is intrinsic in fully end-to-end low-precision training. The benefits of employing the proposed scheme would be beyond accuracy, say memory or energy-saving constraints for on-device training. Experimental evaluations to support the necessity and merits of the proposed scheme should be provided. (3) The quantization range is fixed in the proposed scheme. Is it a merit that the proposed scheme does not need to adjust the range and precision either per-layer or per-channel during training as in other SOTA methods?(4) Writing on the constrained optimization formulation is a bit verbose and not properly formulated. (5) Inducing the tail bound of distribution to demonstrate that the probability of swamping can be controlled, several assumptions and approximations have been applied for the worst-case upper bound. Are the assumptions reasonable to work in practice? For example, assuming that the weight distribution is Gaussian is too strong to be practical. (6) The paper has a conceptual overlap with other quantization approaches and some of the proposed scheme is not entirely novel resulting in a weak contribution. (7) In Table 2, the MobilNet has more severe degradation of accuracy than the ResNet on the low-precision(8-bit) setting. Could you explain why this happens?Minors: - Several typos: There was been in p.2, to soft threshold the range of in p.3, theta-i in eq.(3), some more in p.7 This paper is concerned with the problem of cooperative multi-agent reinforcement learning for CTDE scenario which is well studied in recent literature. The authors propose a factorisation method based on soft value functions. I found that the paper is extremely poorly written which makes it very difficult to understand the overall method. The presentation is also quite arbitrary with discussion around results that seem unnecessary. There is little novelty as most of the paper borrows from SAC paper by Harnooja et al, albeit with gross errors in copying. Here are some of the  major issues:1. The authors discuss the IGO decentralisability, however what is the relation between IGO optimal policy and soft policy when the former is not representable by latter?2. How does the local soft policy iteration guarantee joint policy improvement? 3. Why is the * being arbitrarily switched in sec 3.2? What does eq 12 even imply? isn't the KL minimiser $\pi_i^*$ itself? Where is $\Pi$ defined?4. How did eq 18 come about?5. The paper is full of unbacked blanket statements like: " Although energy based distribution is very general which has the representation ability of most tasks," "Our method are a member of them but out of the deterministic policy" etc.6. There are many unintelligible sentences like: "we need to extend the function class into any distributions", "IGO is more generality than IGM", "The individual value network is trained by minimize", "relative overgeneralization, where finding asuboptimal Nash Equilibrium, which is a well-known game-theoretic pathology" etc.7. In proof for Theorem 2, $\epsilon$-greedy eq 26 cannot be matched by a soft policy in general, thus the rest of the proof cant follow without corrections. The paper proposes a Q-factorization method by assuming an energy-based policies model. Q-functions are formulated as soft value functions with the energy parameters, and this adoption renders the function factorization more flexible compared to existing ones. The proposed solution applies to continuous-action tasks, a feat left unconquered by some of the existing methods. Authors exhibit that FSV outperforms others in various environments characterized by local optima.Strengths:+ The formulation of Q-functions as soft functions, despite appearing simple, shows some effectiveness in a number of MARL tasks.+ The network architecture is intuitive.Major Concerns:- Neither energy-based policies nor soft value functions is an original contribution of this work. True, the authors do not claim so. But the reviewer is left unsure as to what then the primary contribution of the paper would be.- The method generalizes IGM to IGO but in doing so, foregoes the simplicity of the IGM condition. The reviewer would then expect to be met with a somewhat strong guarantee, but is instead presented with approximations on \lambda_i. It is not clear from the paper how much insightful value the method has, when its criticism of a previous work (QTRAN) was based on intractability but the FSV method itself still relies on approximations. It would seem as though QTRAN and FSV each chose different paths to approximate different components of an MARL training scheme - the former takes may stronger assumption on the value functions while the latter takes assumptions on the nature of value functions being parametrized by approximated weights.- The effectiveness of the proposed method is not yet well-accounted for. Issues are raised, but little explanation (or any attempt thereof) is provided. For example, the reviewer would have very much liked to gain an understanding of the relevance between IGO and its ability to alleviate relative overgeneralization. How does taking on greedy policies (which makes IGO collapse into IGM) make MARL agents more prone to overgeneralize with respect to each other? What kinds of findings would the authors present? What evidence could support those findings? The evaluation, while illustrating great performance gaps, needs a careful redesign so as to construct solid grounds for the soft value function factorization under IGO to be "explainably" better than existing works.- The paper could be better positioned. The Related Works section could be put to better use to clearly distinguish two very different lines of research: value function factorizing MARL works and maximum entropy principle.- There needs to be some justification about multi-head attention being used to "enable efficient learning" in Section 3.3. The reviewer is left hanging as to why and how such a choice was made.Minor Concerns:* A few parts of the paper were difficult to follow. For example, there is an unfinished sentence in Related Works. In Section 2.1, there is an incomplete clause beginning with "the reward function [...] shared by all agents". Under Theorem 1, "any distributions" --> "any distribution". Also, what is meant by "correct architecture" in that same paragraph? We find he paper inappropriate for ICLR for many reasons. We give below some elements to help the authors in increasing the relevance of their work and paper.The title is inappropriate, as Legendre transformation is not used in this work. And the word Legendre is never used in the paper. The introduction is too long, describing many elements that are not necessary for the paper. For example, the paragraph describing the PCA and some of its variants, as well as the orthogonality constraint. In the paper, the PCA is not considered, neither the orthogonality constraint.The authors focus on the fact that other methods are sensitive to initialization and they may converge to local optimum. In practice, the proposed method may have similar issues, as can be seen from algorithm 1 because it requires the initialization of \theta_s. It is not clear what is the used initial values, but it seems that it is 0 and not random because it makes the obtained results constant after 49 runs. This doesnt mean that it is independent of the initialization. It is easy to demonstrate that the proposed method cannot jointly verify the independence on initialization and the convexity/convergence to global optimum. To show this, we can initialize with the results obtained from FastICA. As FastICA sometimes outperforms all the other methods, this means that the proposed method should provide better results that FastICA throughout iterations, because the optimization problem is supposed to be convex and it converges to the global optimum; However, this means that the method depends on the initialization. If the proposed method is supposed to be invariant to initialization, this means that it will yield the same values given in the paper, thus not outperforming FastICA is several cases. This means that it converges to an optimum that is not a globale one, as FastICA converges to better optimum solution.Experiments are not convincing, because comparing only to too old methods, such as FastICA from 2000 and dictionary learning from 1997. The authors need to compare to more recent methods from the state of the art.There are many spelling and grammatical errors, such as disctionary. **Justification for Score**The paper is missing a thorough literature review and there are many missing citations and comparisons. Overall, unfortunately there are multiple misleading (and sometimes false) claims throughout the paper. The evaluation performance is also a little unfair as they use ECE as an objective and then only show better performance on ECE. Overall, the paper is well written but the content does not have good enough quality as there seem to be many unsupported claims.## Review### SummaryThis paper proposes a post-hoc calibration method which aims to preserve the accuracy of the classifier as well as improve its uncertainty calibration. The core of the method lies in formulating a general form of the commonly used scaling method Temperature Scaling (TS) and more recent variation of it, Local TS (Local TS). The proposed method Neural Rank-Preserving Transforms (NRPT) maintains the accuracy but also shows better calibration performance compared to TS and Local TS.### Strengths* It is important that a post-hoc calibration does not decrease the accuracy. I like the justification of the authors about this: " a calibrator that does not maintain the accuracy may attempt to improve the accuracy at the cost of hurting (or not improving) the calibration". Though, I have to point out that a method which can change the rank performance also has advantages compared rank-preserving methods (i.e. TS or NRPT): The literature has shown that accuracy and calibration performance can jointly be improved - the authors don't really acknowledge this point (see below for references and examples of such methods). * A rank preserving method has the advantage of optimizing loss functions which do not have to "care" about accuracy. In this case, the authors propose to use the ECE loss. A method which does not preserve the rank would completely decay the accuracy in an attempt to optimize the ECE. That being said, I still think that optimizing on the ECE and only showing improvements on the ECE metric is not surprising (see below for more discussion on this point). So it does have the advantage of optimizing ECE, but the paper does not really show the benefit here as it only shows that optimizing the ECE performs better on ECE (this is expected) but performs worse on the other metrics (see Tab. 1). Also, I would like to point out that I include DECE when I talk about ECE in this review.* Better calibration performance compared to the Baseline, TS and LTS. The authors also compared against deep ensembles which have been shown to improve calibration, though they only perform better on ECE for the case where NRPT is optimized using the ECE (NRPT-E). NRPT (without -E) performs significantly worse than ensembles larger than 4 (at least more than half better). * Even though NRPT does not have overall better performance compared to deep ensembles (i.e. only is better at ECE metric and only when ECE is specifically optimized during training), it does have the advantage of being computationally faster. ### Weaknesses1. The major weakness of this paper is its view of the current post-hoc calibration literature seems to be outdated. A re-occurring theme throughout the paper is that post-hoc methods are very simple and always talks about Temperature Scaling (TS) as an example. The post-hoc literature has come far beyond simple methods such as TS. Even though it is true that many recent variants of TS have shown good improvements, there are many other non-"simple" methods which have much (1) larger capacity, (2) can improve accuracy (i.e. they do not have accuracy loss) and (3) greatly improve calibration performance. 2. The related work paragraph also only talks about three post-hoc calibration methods. Even though, the literature grows very fast and it can be hard to always keep track of all new papers, there have definitely been much more than the three variants cited in this paper (see below for references). Later on, the paper does cite the work [1] and claims that they show that "overfitting cannot be easily fixed by applying common regularizations such as L2 on the calibrator". Despite, this statement not being entirely true (see below), it seems that the authors are aware of this paper. [1] has also presented a calibration method (i.e. Dirichlet calibration - mentioned in the title of [1]), so why has this method not been compared or cited for its calibration method? [1] has thoroughly compared against multiple other calibrators (in addition to TS and Matrix scaling), so why were none of these other calibrators cited, used and compared against?3. This is why I find that there are many misleading statements throughout the paper which imply that the (large) family of post-hoc calibration methods can be summarized as "simple" and limited to TS or Matrix Scaling. One example of such a statement can be found in the appendix: "Existing post-calibration methods such as temperature scaling recalibrate a trained model using rather simple calibrators with one or few parameters, which can have a rather limited capacity." The reason I find them misleading is that for a reader/reviewer who is not familiar with post-hoc calibration, it might seem that TS (and its variants) are the only and best methods for post-hoc calibration. Recent years have shown multiple other solutions which are not limited to such simple methods.4. Here is a list of some recent and relevant approaches: GP [3], Histogram Binning [5], I-Max histogram binning [4], Beta Calibration [2], Dirichlet Calibration [1], Matrix Scaling with ODIR regularization [1], BTS [6], Isotonic Regression [9] and MnM [8]. These post-hoc calibration methods range from relatively old methods to much newer methods. Even though I certainly don't expect to see all of them cited and compared against, the authors should do a much more thorough search of the literature or not make statements which imply that the post-hoc literature is only limited to the methods which they compare against.5. Even though the authors compared against deep ensembles, the performance is only better in one case: the ECE metric and only when NRPT optimizes the ECE loss (i.e. NRPT-E). This is no surprise and cannot be used to claim that the method is better than deep ensembles. A method which optimizes the metric A directly will perform better at this metric A. Of course, there are some exceptions such as NLL training without regularization. But overall this is not really an impressive result, especially given that the ECE has lately been criticized (see above).6. State-of-the-art-performance: "Local Temperature Scaling (LTS, see (4)), a generalization of temperature scaling that is observed to achieve state-of-the-art performance on a variety of computer vision tasks (Ding et al., 2020)" This paper claims that LTS obtains state-of-the-art performance and by performing better than LTS implying that they have beaten state-of-the-art in post-hoc calibration. The authors of this current paper do not claim that they obtain state-of-the-art but they imply this by mentioning that they perform better than a method which is previously state-of-the-art. I looked into the LTS paper and they also did not make the claim of obtaining state-of-the-art performance. So where is this claim coming from? It is hard to claim that LTS obtains state-of-the-art performance. After a brief look in the LTS paper, it seems that they too ONLY compare against TS variants (please correct me if I am wrong). As I have listed above, there are multiple other methods which have shown to improve calibration and have a thorough comparison against multiple other post-hoc calibration methods (and not only limited to TS and some of its variants).  Again, I find this a little misleading that the authors of this paper seem to imply many statements to make their method seem good.7. Fig. 1 and Matrix Scaling: The authors seem to want to show the benefit of NRPT by showing superior performance compared to TS ("simple") and Matrix Scaling ("complex calibrators can often overfit"). Despite, the fact that these are poor baselines to compare against (see list of calibrators above), there has been a new proposal to fix the overfitting issues of Matrix Scaling. [1] proposed to use ODIR regularization ("simple L2 regularization") to improve the performance of Matrix Scaling. Firstly, the authors cite this work which means they are aware of this paper and this technique to reduce the overfitting. So then why has it not been used to in Fig. 1? And why was it chosen to compare against Matrix Scaling (without regularization) if there does exists a better solution for matrix scaling? Secondly, the authors instead make a misleading statement: "It is further observed that the overfitting cannot be easily fixed by applying common regularizations such as L 2 on the calibrator (Kull et al., 2019)" This statement is not true. This paper shows in Table 3 (of [1]) and Table 4 (of [1]) that Matrix Scaling with ODIR (i.e. L2 regularization) is sometimes the best performing method. Even for the rest of the cases where it does not perform best, it performs similarly. This shows that "simple regularization" can help address the over-fitting issue. This regularization has also been used in [4] with Matrix Scaling for the ImageNet classifiers and has shown to be the best performing method in terms of accuracy (showing that it helps with reducing the overfitting) Again, this is a mis-leading statement and the literature does not support this.8. Based on the previous (possibly) false claim, the next claim that "This empirical evidence seems to suggest that complex calibrator with a a large number of parameters are perhaps not recommended in designing post-calibration methods" is also not entirely true. As mentioned above, highly expressive methods can improve calibration.9. The authors mention that the small size of the validation set can cause highly expressive methods to overfit, thought [3,4] have both shown to greatly improve calibration performance with as little as 1000 samples for ImageNet classifier calibration and even shows better performance than methods using significantly more samples. Therefore, it is not true that a limited dataset will lead to overfitting and these results show that regularization and other techniques can handle a low data regime for calibration.10. The authors also claim that "matrix scaling is not guaranteed to maintain the accuracy". This claim is true, but the authors fail to mention that Matrix Scaling (and its regularized variants) can also improve the classification accuracy performance. The high capacity allows it to improve accuracy as well. After using L2 regularization the accuracy performance can be greatly improved. Alternatively, [5], also presented Vector Scaling (which has less capacity than Matrix Scaling but more than TS) and they also show that it too can change the accuracy performance and actually improve the accuracy in most cases. So even though I do see the benefit of having method which preserves the rank, it should clearly be pointed out that it has the disadvantage of not being able to improve the accuracy. For example, compared to [3] which improves the accuracy performance, NRPT has the disadvantage with respect to accuracy. So despite this above statement being true, the authors use this statement to justify why methods like TS which preserve the accuracy are better, though TS actually has a disadvantage compared to the many cases where these rank-changing methods improve the accuracy.11. ECE: The authors cite [10] to use their presented debiased estimator. Though, they completely ignored the main message of that paper. [10] has shown that continuous output scaling methods (e.g. TS) have trouble estimating the ECE and instead propose to use binning or quantized output methods instead. [1] also shows how the ECE is underestimated when using too few bins during evaluation. They show that the ECE of scaling methods are non-verifiable. So even though the authors are aware of this paper, they do not at all comment on the use of ECE for continuous output scaling methods such as the one presented. Other recent works [3,4] have also discussed this and proposed solutions such as increasing the number of bins when estimating the ECE of scaling methods. How many bins were used to estimate the ECE? As the authors are aware of this work, they should discuss this weakness of ECE in their paper and more importantly use approaches used in other recent works in the literature to address these issues to some extend. Again, it seems that the authors need to do a more thorough literature search on post-hoc calibration.12. The method also uses ECE during training: Even though this might be an advantage of rank preserving methods, it is not surprising that the method using ECE as a loss performs the best at ECE. Table 1 shows that "NRPT+E" is only better at ECE and no other metric. As discussed earlier, the ECE metric has major flaws and problems, so using this as a loss also comes with its problems. So I do not see this variant of NRPT as very useful. It would be interesting to how "NRPT-E" performs on other metrics and tasks which are not directly optimized during training, as using ECE as the objective and the evaluation is unfair.13. Tab.1 : Why are the uncalibrated ECEs so high? These are unusually high ECE even for uncalibrated networks? There is no surprise then that even simple methods such as TS can show great improvement when the networks are so miscalibrated. Many other works using similar networks report much lower uncalibrated ECE. Maybe the ECE was evaluated differently?14. ECE as objective: As you use ECE during the learning, you should be providing all details about ECE (e.g. the number of bins, the bin widths or bin edges ).### Minor comments* Some references as using arxiv versions of the paper (e.g. Guo2017 is a ICML paper from 2017, so shouldn't be using an arxiv reference)### References (all can be found on arxiv)[1] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach.  "Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration"[2] Meelis Kull, Telmo Silva Filho, and Peter Flach. "Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers".[3] Jonathan Wenger, Hedvig Kjellström, and Rudolph Triebel. "Non-parametric calibration for classification"[4] Kanil Patel, William Beluch, Bin Yang, Michael Pfeiffer, Dan Zhang. "Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning"[5] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. "On calibration of modern neural networks"[6] B. Ji, H. Jung, J. Yoon, K. Kim, and y. Shin. Bin-wise temperature scaling (BTS): "Improvement in confidence calibration performance through simple scaling techniques"[7] Jize Zhang, Bhavya Kailkhura, and T Han. "Mix-n-Match: Ensemble and compositional methods for uncertainty calibration in deep learning."[8] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. "Obtaining well-calibrated probabilities using Bayesian binning."[9]  Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates.[10] Ananya Kumar, Percy S Liang, and Tengyu Ma. "Verified uncertainty calibration" This paper proposes a multi-task multicriteria hyperparameter optimization method. Experiments fail to demonstrate the performance of the described method. Strong points:- The multi-objective selection procedure appears sensible- Experiments run on state of the art modelsWeak points: - Experiments don't say what dataset they used, don't make me look at the citation when you have about 4 pages left of space- Real values for objectives never provided- No baseline / comparison - No justification for method choiceJustification: The paper makes little sense to me. A lot of things are poorly explained, although somewhere in there is the core of a good idea. The whole paper needs to be rewritten with an emphasis on properly defining terms, objectives and research questions.I strongly recommend that this paper be rejected. Some suggestions:  - I am not sure if hyperparameter optimization is the right term to describe this method. One cannot speak of hyperparameter optimization, as there is no hyperparameter optimization process taking place. In fact, this is more akin to a selection procedure. - Equation 9 is introduced as the vector of the optimal solution, whereas it is one of many optimal solutions. It is incorrect to say that the optimal solution is putting an equal weight on all objectives, as far as multi-objective optimization goes.  - Add some figures explaining the whole procedure (from hyperparameter evaluation to multi-objective selection) - Evaluate against baselines - Provide some justification for the steps of the method (i.e. in 3.2) - Have someone not familiar with the work revise the paper before submitting. SummaryThe paper addresses the proposes a solution for multi-task multicriteria hyperparameter optimization. The solution hinges around Pareto optimality. The technique appears to do an exhaustive enumeration in the space of hyperparameters (Equation 4). Experimental results are provided for two scenarios with two and three hyperparameters. ProsThe paper addresses an important problem.Cons1.The paper appears to use exhaustive enumeration before finding Pateto frontier. Exhaustive enumeration is computationally expensive.2.The experimental results are sketchy. The final performance of the models as a result of hyper-parameter selection is not provided.3.Comparison with baselines is missing.Clarifications neededThe paper can be improved by adding the following information.1.Section 4.1: Provide details of the tasks.2.Section 4.1: Neural networks trained on ten TPUs v2, which took several days.: How many days?3.Section 4.2: sample mean/variance of the epoch number at which convergence is achieved in the test sample.: What is meant by convergence on test samples? Summary--------The paper describes a method for inverse reinforcement learning---called stochastic IRL---that learns a distribution over reward functions. In that sense, the method is similar to Bayesian approaches, however, the learned distribution doesn't seem to approximate the posterior for any given prior. It is hard to summarize the algorithm because it is highly unclear what the method actually does, but I will try my best by stating my most likely hypothesis.The algorithm starts with an initial set of $N_0$ parameter vectors, each corresponding to the weights for a linear reward function. Each of these weights are improved by using a fixed number of gradient steps using MaxEnt-IRL (Ziebart et al. 2010), where different (overlapping) subsets of the demonstrations are used for the different weights. Subsequently, a GMM over weights is fitted to the $N_0$ improved weights using maximum likelihood (EM). This procedure is iterated, where the weights at each iteration are drawn from the current GMM. The number of weight vectors is doubled at every iteration t, i.e., $N_{t} = 2 N_{t-1}$.I'm not at all confident that I got this right. For example, I solely inferred the use of MaxEnt-IRL from a sentence in the introduction ("In this paper, under the framework of the MaxEnt approach, [...]").The algorithm is evaluated on a 10x10 gridworld and compared with MaxEnt-IRL and DeepMaxEnt-IRL. The experiments compare two versions of the algorithm called SIRL and DSIRL. The paper doesn't explain how they differ, but a neural network parameterization is mentioned for DSIRL, so I guess D stands for deep and uses DeepMaxEnt-IRL instead of MaxEnt-IRL.Strong points-------------Learning a distribution over reward functions is a laudable goal for inverse reinforcement learning as it can be a principled way to deal with the uncertainty in modeling the expert.Concerns---------__Clarity__ The paper is really hard to follow, even though I'm very familiar with related work, and the algorithm and the equations seem to be quite simple. There are several reasons for this lack of clarity.1) Lack of details. Actually not only details, some of the most fundamental aspects of the algorithm are not mentioned anywhere. For example, the likelihood of the demonstrations given the weights, $g(\mathcal{O}|\mathcal{W},\Omega)$ is nowhere defined. $\theta$ is also not defined. 2) Some of the equations are very confusing. For example, the second to last equations at page 4 seems to overload the subscript "i" which seems to index both the sampled weight as well as the gradient step, depending on the variable. Even when disentangling these different meanings the equation looks weird to me. Based on the surrounding text, I guess it should mean m gradient steps on the likelihood are performed, but the equation says different. It is also strange that the weights W are updated based on the gradient w.r.t. $\Omega_1$ and that $\Omega_1$ is set to the updated weights in the last equation at page 4. It seems like Omega_1 is superfluous. Also, the mixture model $\mathcal{D}(\mathcal{W}|\zeta, \Omega_2)$ seems to be conditional independent of $\zeta$ given $\Omega_2$. What's the difference between $\mathcal{D}(\mathcal{W}|\zeta, \Omega_2)$ and $h(\mathcal{W}|\Omega_2)$?3) Some algorithmic choices are not motivated at all (for example, doubling the samples at every iteration), others are only (insufficiently) motivated in hindsight. For example, the use of "representative trajectory classes" is motivated after introducing them and based on the example of different drivers with different driving styles. However, the paper never mentioned a multi-expert scenario and also doesn't try to cluster the demonstrations but only randomly removes few demonstrations; thus, it is not even clear how the trajectory classes would tackle such problem. 4) Bad structure. While it is in general a legitimate approach to introduce the algorithm step by step, I think you should always also provide a rough sketch of the overall algorithm already early on, so that the reader has some context when you introduce the details. The way of introducing the trajectory classes is also a good example for the bad structure of the paper. Before introducing them, there is no mentioning of them, there is no motivation for them, and for all I can tell the paper doesn't even discuss the problem that they are supposed to tackle. And then in Section 2, they are introduced by stating "Suppose that we have trajectory classes [paraphrased]" and moving on as if nothing happened. 5) I must say that the writing is also quite bad. Some sentence are hard to follow due to grammar mistakes, especially if the reader doesn't have enough background to infer their meaning. For example "[...] suffer from the problem that the true reward shaped the changing environment." should probably mean something like "suffer from the problem that the learned reward function is shaped---that is, it is entangled with the dynamics---and, thus, does not transfer to different environments". As another example "However, GAIL is in a lack of an explanation of experts behavior and a portable representation for the knowledge transfer which are the merits of the class of the MaxEnt approach, because the MaxEnt approach is equipped with the "transferable" regular structures over reward functions."__Soundness__ It doesn't make sense to talk a lot about the soundness of the approach before clarifying what it actually does. However, it seems to me that the learned distribution is only heuristic and does not approximate the posterior for any given prior. Also, for the linear reward function, the maximum likelihood objective is convex, so I think that the different weights should even converge to the same solution. __Evaluation__I don't demand MuJoCo experiments and the like, and in some cases evaluations on gridworlds can be sufficient. But I must say, that the excuse for not performing continuous control problems because the reward functions can not be visualized by 2d-heatmaps lured out a smile when reading the article. If I understand the algorithm correctly, it is significantly more expensive than standard MaxEnt-IRL which already doesn't scale to such problem settings as it requires iteratively solving the reinforcement learning problem. The results on the objectworld are also not convincing. The expected value difference is for 80 demonstrations larger than for 40, and for 160 demonstrations larger than for 80. Similary, the performance initially degrades when increasing trajectory lengths and only improves again when taking trajectories of length 32 or more. The sentence "A notable point in Figure 3 is that very few expert demonstrations (less than 200) for our approach also yields a small EVDs, which manifests the merit of Monte Carlo mechanism in our approach." is highly misleading. 200 trajectiories are not "few" for a 10x10 gridworld and an EVD over 20 is not small. When introducing this environment, Levine et al. (2011) achieved an EVD of around 1 based on 8 demonstrations (using 8 steps instead of 5).The robustness experiment seems to subsample the weights based on the EVD (which requires knowledge of the true reward) before evaluation. Questions--------------To better understand the proposed algorithm, I have the following questions.1) How does the algorithm relate to MCEM? Is it some sort of EM within MCEM, where the first stage corresponds to the Monte-Carlo E-Step and the EM in the second stage corresponds to M-step of MCEM?2) How are the densities g and h defined?3) How does $\Omega_1$ affect the optimziation. How does it differ from $\mathcal{W}$?4) How exactly are the m-step update steps performed? Do they indeed correspond to m gradient steps on the MaxEnt objective?5) Section 3.3.: Should the inequality hold for $\lim_{t \to \infty}$?6) What's the difference between DSIRL and SIRL?7) How do the algorithm compare in terms of computational cost?Assessment----------Unfortunately, I don't see a chance for acceptance here. Even if the algorithm was sound and sufficiently novel, the paper would need to be almost completely rewritten in order to address severe problems of the current presentation. ### SummaryThe authors proposed inverse reinforcement learning (IRL) algorithm based on Monte Carlo expectation-maximization (MCEM) that maximizes the predictive distribution of trajectories given the reward distribution parameter (eq (1)). In my understanding, the knowledge of the environment dynamics is assumed. The authors tried to validate the proposed idea on objectworld (Levine et al., 2011)### QualityThe quality needs to be improved in the sense that a clear theoretical link between the target problem and MC-EM cannot be found in the submission. For example, the main objective (3) is optimized through (4) and (5), but the relation is unclear. There are lots of such things in the submission. ### ClarityThe readability of the submission is poor and needs to be improved. Lots of terms are unclear to me (e.g., succinctness, robustness, transferability of rewards). At some part of derivation, I couldnt understand the motivation. Experiment settings are unclear, and the results are not confident and seem irreproducible with given information. ### OriginalityExploiting the distribution of reward is considered in Bayesian IRL. I think the probabilistic view was originated from Bayesian IRL (e.g., uniform prior on rewards may cover the idea of this work). The submission only sets MaxEntIRL as its baseline, but I think Bayes IRL should have been considered. ### SignificanceThere seems to be a minor contribution ### Detailed comments(p.1, `Abstract`) `expert demonstrations may be optimal for many policies`- I feel this statement is weird since we havent defined the optimality of expert demonstrations.(p.1, `Abstract`) `we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions.`- SIRL tries to solve the inherent issue of IRL problem, not **generalize** IRL. Also, since Bayesian IRL also recovers the reward distribution, I couldnt get the major advantage of the SIRL from this statement.(p.1, `Abstract`) `The solution is succinct, robust, and transferable`- Definitions of these expressions seem ambiguous to me. (p.1, `Abstract`) `a global viewpoint`- Again, ambiguous. (p.1, `Introduction`) - It would be better to write it in a more abstract way and separately write down the `Related Work` section.- References should be much clearer: LaTeX commands like `\citet{}` and `\citep{}` should both be used. (p.1, `Introduction`) `if the model dynamics are known`- Recent works on IRL such as adversarial IRL (Fu et al, 2017) didnt require the knowledge of model dynamics.(p.1, `Introduction`) `The recovered reward function provides a succinct, robust, and transferable definition of the learning task`- `succinct, robust, and transferable`: Ambiguous(p.1, `Introduction`) First paragraph- Lots of words from `Abstract` seem to be repeated. (p.1, `Introduction`) `In a real-world scenario, experts always act sub-optimally or inconsistently, which is another challenge.`- The sentence seems abrupt. The terms like `sub-optimal` and `inconsistent` here are awkward. (p.1, `Introduction`) `imposes regular structures of reward functions in a combination of hand-selected features`- GAIL (Ho et al, 2016) doesnt require a hand-crafted feature. (p.1, `Introduction`) `hand-selected by experts`- A word `experts` here seems to imply a reward designer, not an expert on target tasks. Id rather use a different word here. (p.1, `Introduction`) `based on demonstrations respectively`- `respectively` seems inappropriate. (p.1, `Introduction`) `Influenced by the work of Finn et al. (2016a;b)`- How these references affected AIRL needs to be mentioned. (p.2, `Introduction`) `because the MaxEnt approach is equipped with the "transferable" regular structures over reward functions.`- In Ziebart et al., 2008, transferability wasnt mentioned. - I believe the statement -- MaxEnt itself gives transferable reward feature -- is wrong but you should share the correct reference if this is true. (p.2, `Introduction`) `The solution of SIRL is succinct and robust for the learning task in the meaning that it can generate more than one weight over feature basis functions which compose alternative solutions to the IRL problem`- This explanation seems insufficient to understand the meanings of succinctness and robustness. (p.2, `Introduction`) `Benefits of the class of the MaxEnt method,`- Thanks to the benefits of the class of the MaxEnt method?(p.2, `Introduction`) `Since of the intractable integration in our formulation,`- Due to the intractable integral in our formulation? - I think the intractability of the mathematical derivation didnt need to be mentioned in `Introduction`. (p.2, `Introduction`) `in a model-based environment`- when model dynamics is known? (p.2, `Introduction`) `In general, the solutions to the IRL problem are not always best-fitting in the previous approaches because a highly nonlinear inverse problem with the limited information is very likely to get trapped in a secondary maximum in the recovery.`- I couldnt understand what the authors wanted to emphasize. - It seems like they intended to emphasize the problem of local optima, but I dont know if such a problem is exactly whats happening in IRL. (p.2, `Introduction`) `global exhaustive search`- What does `global` imply? Knowledge of dynamics?(p.2, `Introduction`) `theoretically convergent demonstrated by pieces of literature`- is theoretically convergent?- How the theorem in the references (Caffo et al., 2005, Chan and Ledolter, 1995)  is applicable to the proposed idea should be much clearer since this is one main advantage that the authors argue. For example, what kind of assumptions are required to acquire global optimality? What is the algorithmic assumption of MC-EM for optimality? How are those assumptions linked with IRL setting?(p.2, `Introduction`) `is also quickly convergent`- converges quickly?- How can we guarantee the convergence speed? Empirically or theoretically?(p.2, `Introduction`) `the preset simple geometric configuration over weight space in which we approximate it with a Gaussian Mixture Model (GMM)`- preset -> predefined?- approximate it -> approximate(p.2, `Introduction`) `We generalize the IRL problem`- It seems the objective is not a generalization. (p.2, `Preliminary`) $\mathcal{T}:=\mathbb{P}(s_{t+1}=s|s_t=s, a_t=a)$- $\mathcal{T}(s|s, a):=\mathbb{P}(s_{t+1}=s|s_t=s, a_t=a)$(p.2, `Preliminary`)  `a sequential of state-action pairs`- a sequence of state-action pairs?(p.2, `Preliminary`) `The estimated complete MDP yields an optimal policy that acts as closely as the expert demonstrations.`- The discount factor should be considered as well. (p.3, `Regular Structure of Reward Functions`) $\mathcal{N}$- Id rather use a different letter since $\mathcal{N}$ is used to indicate Gaussian distribution in Section `Second Stage`. (p.3, `Regular Structure of Reward Functions`) $\{\phi_i(s, a)\}_{i=1}$- $\{\phi_i(s, a)\}_{i=1}^M$?(p.3, `Problem Statement`) $\mathrm{MDP}\backslash R:=(\mathcal{S}, \mathcal{A}, \mathcal{T}, \gamma)$- The definition doesnt match with one without the discount factor $\gamma$ in `Preliminary`.(p.3, `Problem Statement`) $\{\phi_i(s)\}_{i=1}^M$- $\{\phi_i(s, a)\}_{i=1}^M$?(p.3, `Problem Statement`) weights $\mathcal{W}$- The definition should be provided. - Either $\mathcal{W}=(\alpha_1, &, \alpha_M)$ (for linear model) or the weights of neural network (for non-linear model)?(p.3, `Problem Statement`) `more likely generates weights to compose reward functions as the ones derived from expert demonstrations`- Is this only a special case of Bayesian IRL?(p.3, `Problem Statement`) `Suppose a representative trajectory class ~`- The explanation should be clarified. In my understanding, $\mathcal{C}_\epsilon^E$ is a class of sets of trajectories.- Why do we need to care such a class with $\epslion$ threshold?(p.3, `Problem Statement`) Integrate out unobserved weights $\mathcal{W}$- What does *unobserved* weights mean?- Integrate out -> Marginalizing out?(p.3, `Problem Statement`) trajectory element set $\mathcal{O}$ assumes to be uniformly distributed for the sake of simplicity in this study- I dont fully understand whats the advantage of considering a representative trajectory class and why it is required. - The section `Note:` tries to explain it, but more explanation or theorems seems to be needed. How can we theoretically guarantee that using a representative trajectory class doesnt affect our estimation? It seems to me that we cannot guarantee the optimality with this class is the same as the original optimality.  (p.3, `Problem Statement`) $f_{\mathcal{M}}$- How this quantity is related to reward weights is unclear to me. The relationship between weights and $f_\mathcal{M}$ for both linear and non-linear models should be specified.(p.3, `Note:) - Instead of using a separate section, Id rather put these statements in the middle of `Problem Statement` for a clearer explanation. (p.4, `Two-stage Hierarchical Method`)- Why do we need to use two-stage method instead of single-stage method (joint optimization over $\Theta_1$ and $\Theta_2$)? The advantage of two-stage methods should be briefly mentioned when it first appears for readability. - How does the iterative update rule (4), (5) guarantee the optimization of (3)? Its unclear to me due to the expectation in (4) and (5). My guess is that direct optimization of RHS of (3) is not possible and (4) and (5) might be either lower or upper bound of (3) due to Jensens inequality. (p.4, `Initialization`) `~in each learning task`- Do we care about multi-task learning or multiple reward weights only? I believe the latter case.(p.6, `Experiments`) `since almost only objectworld provides a tool that allows analysis and display the evolution procedure of the SIRL problem in a 2D heat map, we skip the typical invisible physics-based control tasks for the evaluation of our approach, i.e. cartpole Barto et al. (1983), mountain car Moore (1990), MuJoCo Todorov et al. (2012), and etc.`- I think this makes the contribution weaker. At least a few classic control tasks should have been considered. One way of evaluating the quality of rewards is retraining the agent with acquired reward, which is already widely used in the literature. (p.6, `Objectworld`) - One figure for illustration will enhance readability. (p.7, `Evaluation Procedure and Analysis`) `DSIRL`- DSIRL abbreviates Deep SIRL but wasnt mentioned. (p.7, `Recovery Experiments`)- How many runs were used? Hows the mean and confidence interval of the empirical result? (p.8, `Robustness Experiments`)- I couldnt understand how the robustness of reward is related to the proposed experiments. How the robustness is defined and its relation to the experiment should be clarified.  (p.8, `Hyperparameter Experiments`)- How is the range of hyperparameter search for all methods? Currently, only the results for SIRL and DSIRL are given. (p.8, `Conclusion`)- It seems like both succinctness and transferability were not discussed in the main part of the submission. ### References- Levine et al., 2011, Nonlinear inverse reinforcement learning with gaussian processes- Fu et al., 2017, Learning robust rewards with adversarial inverse reinforcement learning- Ho et al., 2016, Generative adversarial imitation learning- Ziebart et al., 2008, Maximum Entropy Inverse Reinforcement Learning- Caffo et al., 2005, Ascent-based Monte Carlo expectation-maximization- Chan and Ledolter, 1995, Monte Carlo em estimation for time series models involving counts # SummaryThis paper proposes a new method called Disentangled Exploration Auto-Encoder (DEAE). This new method is based on (Ge at al.) Zero-shot synthesis with group-supervised learning to which a modified cyclic loss term is added. This method is trained on datasets with label supervision.# Pros1. Compared to (Ge at al.) Zero-shot synthesis with group-supervised learning the results seem to be more visually pleasing.# Cons1. The method is not clearly presented. In particular the loss terms are not mathematically expressed as equations. I dont know if were expected to read the paper this method is based on to have detailed equations. In any case, the added cyclic loss is expressed as an equation either, so its really hard to say what the method really does. Its also unspecified how the latent space is allocated to attributes.2. The scope of the method is not clearly presented either: I was under the impression that the proposed method was comparable to other unsupervised auto-encoders while in fact it requires label supervision3. It seems the paper claims to be more than it actually is: if I understand correctly, the sole contribution of this paper is a cyclic loss term, compared the (Ge et al) paper which can be seen as a regularizer.4. Experiments are mostly focused on visual inspection of images and do not appear impressive to me (except for the toy dataset of colored letters, but then I dont know what the SotA is like). Very few numerical results (save for dataset bias elimination) are presented. Results are not compared to other SotA methods, except the (Ge at al) paper which the method is based upon. Experimental setup is very incomplete.# Questions and nits1. It would have benefited my comprehension to mention in the abstract that the proposed method requires attribute/label supervision. In its current form, it seems to claim to solve disentanglement for unsupervised auto-encoders and I find it misleading.2. The mention of the generative ability without GAN based training in the abstract is also confusing: the proposed method seems orthogonal to GANs and could be combined with them.3. Several times I see the term perfect disentanglement to describe the improved disentanglement that this method offers compared to (Ge et al). What I dont understand is what makes it perfect, cant it be further improved?4. <mention unsupervised auto-encoders>. We propose a different solution to empower precise attribute controllable synthesis ability on autoencoders: DEAE. To be fair, its also a solution to a different problem scope. The proposed method uses attributes while the cited methods dont. So its really a solution to a different problem altogether.5. Typo whic h => which6. Fig. 4 (d) shows that we can combine the UDVs to dicover new attribute values. Aside from the typo on "discover", its unclear how you discover new attributes values (which I assume are centroid like the example you mentioned before for the blue color). Here instead, my understanding is that UDV just provides a vector along which values of interest may lie. It seems the eventual decision to make a value an attribute is manually decided by a human after inspecting the effects along an UDV axe.7. Downstream task performance. It is a toy dataset and its hard to really tell the real power of the proposed method. A lot of information is missing, what are the sizes the $D_S$ and $D_L$? The only reported numbers are $D_S$ vs $D_{S+DEAE}$. What is the unreported accuracy gap between $D_{S+DEAE}$ and $D_{S+GSL-AE}$ that leads to the later conclusion that DEAE performs better? What is the accuracy for $D_L$? No information is known about the classifier network architecture(s), parameter sizes, tuning and whether or not they overfit or what other causes could be responsible for the observed results. This paper proposes to use orthogonal weight constraints for autoencoders. The authors demonstrate that under orthogonal weights (hence invertible), more features could be extracted. The theory is conducted under linear cases while the authors claim it can be applied to more complicated scenarios such as higher dimension and with nonlinearity. The experiments demonstrate the performance of proposed model on classification tasks and generative tasks. Several baselines are compared.The paper is poorly written. It is full of inconsistent and irrelevant claims. The method is not clarified. All experiments are in low quality. +ves: + This paper discusses its connection to several of topics such as mutual information, greedy learning, SVD etc. Concerns: - No novelty. Using orthogonal weight regularization has been widely studied.- The theory does not apply to higher dimensional cases or nonlinear cases. The discussion seems trivial. There are no connection of corresponding theory and the model in the experiments.- Throughout the paper, the "pretraining" process is not clarified. - No experimental detail is given, e.g. dataset, model architecture, training procedure, evaluation metric etc. - The authors claims applying their method to GAN but I don't see how they combine their model with it. - The classification and generation experiment results are not convincing. In Figure 6, 7, the difference are in range of error bar. In Figure 8, 9, there is no advantage from proposed method. This paper designs a multilayer connection structure for neural networks, such that the connection architecture supports implementation of a hierarchical classification scheme within these layers.  It applies this design to the task of hierarchical classification on ImageNet.  Experiments compare results with those of Deng et al. (2012), as well as baseline flat classification models.The paper motivates the proposed approach via broad claims about what networks understand, but does not provide sufficient analysis or experimental evidence to justify these claims.  For example:"In particular, when an existing CNN correctly identifies an image of an English Setter, the network itself does not learn that it is an instance of a dog, or more precisely, a hunting dog which is also a domestic animal and above all, a living thing"Assuming it is trained on example images of all of these categories, how do we know that the CNN does not learn shared representations that implicitly reflect such organization of the concept space?  The paper does not employ any techniques to probe the learned representations of CNNs; without such analysis, the sweeping statements about what CNNs do or do not learn are mere speculation.On a technical level, the design of the proposed dense classification layers appears to be quite ad-hoc.  It is not clear why a special design intermixing concept prediction nodes with hidden nodes is desirable or necessary.  How does this compare, both conceptually and experimentally, to a branching hierarchy of subnetworks?  The scheme of HD-CNN (Yan et al., 2015) is similar to the latter, but is not represented in experimental comparisons.In fact, experiments appear to lack comparison to any recent published methods on hierarchical classification.  Deng et al. (2012) is the only prior publication that serves as a reference point.  This is far from a sufficient baseline as surely there has been other work on hierarchical classification in the past 8 years.  For example, a highly relevant work that this paper fails to even cite or discuss is:M. Nickel and D. Kiela. Poincare Embeddings for Learning Hierarchical Representations. NeurIPS, 2017.Together, the unsubstantiated motivating claims, ad-hoc design of questionable merit, limited experimental comparison, and missing citations to highly relevant recent work suggest that this paper is not of sufficient quality for publication. This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks, including autonomous robotics and driving domains. All domains also come with simulation environments.While the dataset provided by the authors may be a useful resource for researchers in offline-RL, this paper does not introduce any new or novel ideas. Overall the main contribution of this work is the gathering of offline data in one place, reducing the time needed for other researchers to do so. It does not seem as if the authors did any non-trivial work other than annotating the data. Furthermore, there have been many previous work which have already collected offline datasets as part of their work.I do not underestimate the importance the authors' hard work, nor the importance of the provided datasets to the RL community. Nevertheless, I do not believe this work should be published in a high-end conference without presenting any ideas that are not trivial or known to other researches. The authors propose to use the following design factors in their offline datasets: narrow distributions, multi-task data, sparse rewards, suboptimal data, and partially observable policies. While these factors may indeed be good for testing offline-RL algorithms, they do not provide a complete picture of real world datasets. Real datasets present many more real-world problems, including:1. Non-stationary data. Many real datasets are non-stationary. The non-stationary behavior could be mimicked or simulated from real behavior.2. Real policies. Real datasets don't involve policies that were trained by RL agents. The authors could create datasets that are constructed by real human beings (for example, humans playing atari games, with mixed or different expertise). In a controlled setup, the datasets could be constructed so that policies are categorized (e.g., "level of non Markovianess`"). 3. Causal structures. Real policies may act according to some causal structure in the background that is not necessarily known.4. Reduction from real datasets. One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality (e.g., removing certain trajectories). If the initial data is of high quality, the corrupted data could be controlled well.5. Changing and/or very large action sets. Real world datasets have changing and large datasets. As an example, consider ad placement, or text based tasks.6. Non-robotic environments, including games but also real world problems.If the authors bring together datasets that generate original ideas that have never been previously explored, then I believe it's more likely that this paper could be accepted in future venues. 1.The main problem I have with this paper is that this paper idolizes the paper [distributed EF-SGD by Zheng et al. NeuRIPS 2019]. However, the main result, that is, Theorem 1 in dist-EF-SGD is mathematically problematic or simply wrong. The proof of Theorem 1 as given in [distributed EF-SGD by Zheng et al. NeuRIPS 2019] does not hold good when the learning rate sequence $\eta_t>0$ is decreasing. Therefore, the authors claim in the Abstract and several parts of the present paper We strengthen their analysis to show that the rate of convergence of two-way compression with error- feedback asymptotically is the same as that of SGD eventually invalid. I suggest the authors please read the distributed-EF-SGD paper carefully, understand it better, write the proofs on their own before making these types of strong claims. Please study [1] and work on your proofs. 2.Page 2: However, Karimireddy et al. (2019) theoretically show that SGD with compression does not converge in general. This is a very strong statement which I do not agree with. Karimireddy et al. (2019) showed how error feedback can fix the convergence issue of sign-based quantization as in signSGD. Moreover, they showed how any compressor (biased/unbiased) can be converted to a $\delta$-compressor. But that does not mean the authors statement in this manuscript is correct. As authors claimed error feedback-based algorithms circumvent the convergence issues for SGD with compression is not right. Error feedback is known to work well for sparsification, where a subset of gradient components are sent or for extreme quantization (such as sign-based compressions). However, regular random dithering-based quantization techniques such as QSGD, natural compression, etc. converge just fine without error feedback. Actually, error feedback may degrade their performance. I would like to request the authors to first understand these works in detail before writing these types of strong statements on their paper.  3.Another vague statement is: Therefore, these two classes can be respectively thought of as approaches that reduce the quantity versus the quality of the gradient. Based on what you claimed this? 4.Sign-based compression schemes such as Scaled-sign, SignSGD and Signum (Bernstein et al., 2018)& SIGNUM is not a sign-based compressor. It is the momentum version of signSGD, nothing novel. 5.You may want to talk about the most relevant and recent work on compression known as SketchML [J.  Jiang,  F.  Fu,  T.  Yang,  and  B.  Cui,  SketchML:  Accelerating Distributed Machine Learning with Data Sketches, SIGMOD, 2018] while talking about delta encoding first paragraph in Section 3. This recent paper on gradient compression uses delta encoding. 6.I failed to understand the benefit of Generalized dist-EF-SGD algorithm in Section 3? What are the main differences telling me? 7.In our case, while the length of the parameter vector d is well over a million for models of practical interest, the entries of b are not necessarily i.i.d.. Can you make an assumption of the independence of the gradient components? If you make that assumption you may elevate the issue. Also, the assumption is not a strong assumption and generally made for stochastic gradients. Please See [Huffman Coding Based Techniques for Fast Distributed Deep Learning, Gajjala et al., CoNext DistML workshop, 2020]8.Uniform upper bound of the stochastic gradients g_i is an obsolete concept. The authors may argue that "The classical theoretical analysis of SGD assumes that the stochastic gradients are uniformly bounded". But one can even strongly argue that this bound is actually $\infty$. Moreover, an even a stronger argument can be made that the above assumption is in contrast with strong convexity. Please see ["SGD and Hogwild! Convergence Without the Bounded Gradients Assumption" by Nguyen et al.] as one of the instances. Please understand there are relaxed assumptions such as Strong growth condition on a stochastic gradient as in Assumption 4 of [2]. 9.You said: Since the communications component in Horovod is designed for a master-less setup, we simulate a master-worker environment in our implementation. But I was wondering why do you need this? In any case, if you use all-reduce collective for aggregation even for P2P architecture the aggregation will be similar. Please correct me if I am wrong. 10.While ImageNet accuracy is similar to why test accuracies of the models on CIFAR-100 and ImageNet-32 are below 60%?11.In terms of experimental results, instead of Figure 1, the authors may use relative data-volume vs. test accuracy. Especially, when the accuracy figures are really cluttered. To do proper experiments by using compression techniques, the authors can check a very elaborative work and codebase by [Hang Xu et. al, Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation.] In that case, I would encourage the authors to plot relative data-volume vs. test accuracy similar to Figures 6 and 7 therein. I am sorry but in the present papers, the experiments and their presentations are substandard. 12. Why did not you compare with sign-sgd algorithm? Also, sign-based algorithms are notorious in their convergence. SignSGD uses a special stepsize. So I was wondering what is your step-size schedule? You never mentioned this in your paper. You may did it in the Appendix and I did not check the Appendix. So, please indicate if you did. Minor Comments:1.These two sentences in my understanding are claiming the same thing? We strengthen their analysis to show that the rate of convergence of two-way compression with error feedback asymptotically is the same as that of SGD. As a corollary, we prove that two-way SignXOR compression with error-feedback achieves the same asymptotic rate of convergence as SGD.2.Novel approaches such as federated learning& Why is it novel? Unnecessary hyperbole is not part of technical writing. 3.In this section, we prove that the combination of Algorithm 1 and Algorithm 2 converges, and the convergence rate is asymptotically the same as that of SGD. Why Algorithm 2 when Algorithm 1 implicitly implies the inclusion of Algorithm 2?4.Please correct the typos and please write as it is done in a technical paper, not in a sci-fi novel. [1] Communication-Efficient Distributed SGD with Error-Feedback, Revisited, Tran Thi Phuong, Le Trieu Phong, 2020. [2] Dutta et al. AAAI 2020, On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning Short summary---------------------The authors propose a framework combining a GAN and linear encoder and decoders to reconstruct perceived face stimuli from fMRI data. They compare their framework to two baselines in the field and display a higher similarity (in different spaces) between the reconstructed images of their method and the stimuli, compared to the baselines.Strengths--------------The framework is simple and allows for using different models to generate the stimuli, encode brain activity, decode brain activity and decode the stimuli. Appropriate baselines are selected and the authors quantify the similarity between generated and reconstructed images in different spaces.Weaknesses-------------------I am uncertain of the impact of the proposed approach, as it does not propose techniques to investigate brain functioning, nor does it provide a means of communication with disabled patients (as claimed by the authors). I see Ethical concerns with this type of model, which, in my opinion, are not counterbalanced by its usefulness.Novelty-------------The approach combines well established models and techniques into a novel framework. While I found some creativity in the setup, the novelty is overall low and I am not convinced that some of the techniques used are not acting as bottlenecks (see detailed comments).Clarity----------- The paper is relatively clear. I appreciated the presence of multiple figures and of various examples. I believe that the methods could be clearer (see detailed comments).Significance----------------To me this was a major concern with this work as I found some claims bold and not substantiated by the experimental setup or the results. For instance, the authors claim to decode naturalistic stimuli. However, they can decode GAN generated images, which is substantially different, especially given the approach to average the fMRI signals over 14 repetitions in the test set to increase SNR.Rigor--------Overall, I found that the work was relatively well performed, although not excellent. I wished that the comparison with the VAE approach was fair and would suggest that the authors work towards achieving SOTA in their baselines for a fair comparison.Detailed comments---------------------------- Faces generated by a GAN cannot be deemed naturalistic- Isnt the setup circular? How would that model help understand face processing in naturalistic settings?- Ethics concerns: how is the application helping people with disabilities or understanding brain function? The authors mention this as a potential communication means for locked-in patients. However, (1) the results are not strong enough to suggest a potential communication tool, (2) the reliance on fMRI signals makes this impractical and expensive. There are no conclusions regarding brain function that this technique brings, especially given the voxel selection based on a linear regression model.- Novelty and technical sophistication is rather low: combining existing techniques in a novel system.- A better test would have been to reconstruct stimuli that were not generated by the GAN- Importantly, we only took the centers of the activation maps to exclude surrounding background noise. I believe the authors refer to activation maps as the fMRI z-score maps. This can however be confusing for an ML reader. Please clarify the language.- It is unclear to me what the goal of the five additional loss functions is, or how they are formulated.- why is the test set not sampled the same way as the training set? While taking the average of 14 repetitions increases signal-to-noise ratio, this setting further departs from any naturalistic decoding.- An fMRI study with 2 subjects is not representative, given inter-subject variability. This is further compounded by the low number of test images (36 per subject after averaging).- There is a clear imbalance in the generated stimuli in terms of age, race or whether they wear glasses. This limits the impact of the scores for the 5 different attributes. It is also a reason why neuroscience experimental stimuli are thoroughly controlled for. While this is touched upon in the discussion, this could be an indication that the proposed approach would fail on naturalistic stimuli.- Some reconstructed stimuli are highly similar, despite different generated images (e.g. 23 and 24). What could explain this phenomenon? Could there be some type of mode collapse in the reconstruction? It would be interesting to compute the pair-wise similarity between reconstructed images compared to the pairwise similarity of the generated images (in the different spaces mentioned, i.e. latent, feature, attributes).- Isnt the linear regression model a bottleneck here? Why not use the raw BOLD signal instead of z-scores? This reflects an assumption that the encoding between stimulus and activation map is linear. Couldnt there be a non-linear mapping between stimulus perception and the latent space? Overall, the proposed framework relies on established techniques without questioning or reflecting on their assumption.Minor-------- Given the linear model used, why limit to 4096 voxels? This seems like an arbitrary number. Is it related to the z-scores or a specific p-value threshold on the activation map?- The relationship between the trained models and the ResNet mentioned in section 2.4 is unclear. Is it used for evaluation as another way to estimate the similarity between the reconstruction and the GAN generated images? Is there a justification/reference for this technique?- Figure 5C, could the authors use the same y-scale? The legend mentions We found high correlations for gender, pose, and age, but no significant correlation for the smile attribute. Pose however had the lowest correlation values. How about eyeglasses?- [&] permutation test), indicating the probability that a random latent vector or image would be more similar to the original stimulus. This sentence is unclear to me: is the permutation test assessing whether HYPER has significantly higher similarity between the reconstructed image and the generated image than if using a random latent vector? Please provide more details on the hypothesis tested by the permutation test in each space, as well as how these tests compare the different techniques. Couldnt the permutation tests be applied to the baselines techniques as well? While overall the writing quality of the paper is high, the paper itself is a strong rejection.  I believe the analysis of the paper is at points flawed, and the experiments are minimal.  This work attempts to study the degree to which a layer by layer information bottleneck inspired objective can improve performance, as well as generally attempt to clarify some of the discussion surrounding Shwartz-Ziv &amp; Tishby 2017.  Here, the authors study a deterministic neural network, for which the mutual information estimation is difficult (I(X,L)) and error prone.  To combat this they use the noise-regularized mutual information estimator (I(X; L+eps)).  To actually estimate the mutual information the authors use the MINE estimator of Belghazi (2018).  Here they suggest using the neural network itself as a structural element in the form of the discriminator to take advantage of the specific circumstances in this case.  Doing this ensured that their estimator diverged in the zero noise limit as expected.  From here they show some experimental results of the effect of their objective on an MNIST / CIFAR10 classification task.This paper fits into what is an increasingly large discussion in the literature, surrounding Information Bottleneck.  The paper itself does a very good job of citing recent relevant work.  Technically however I take issue with the framing of previous work in the last paragraph of the "Deep neural nets" subsection of Section 2.  Technically Achille &amp; Soatto explicitly formed a variational approximation to the posterior over the weights of the neural network and so was not a "single bottleneck layer" as stated in the paper.  More generally at the end of that paragraph it is implied that the single bottleneck layer scheme "deviates from the original theory".  This is a misleading characterization of the original information bottleneck (Tishby et al 1999) in which there was a single random variable, a representation of the data (Z) satisfying the Markov conditions Z &lt;- X -&gt; Y.   I believe the authors instead meant to say that the cited works deviate from the information bottleneck theory of learning suggested in (Shwartz-Ziv &amp; Tishby 2017).  In general the paper does a poor job of distinguishing between the Shwartz-Ziv &amp; Tishby paper and the rest, but this is a distinction that should be maintained.  The original information bottleneck may and has demonstrated utility regardless of whether the information bottleneck generally can help explain why ordinary deterministic feed forward networks trained with cross entropy and sgd generalize well.  This also raises one of the main problems with the current work. The title, abstract and especially the conclusion ("This provides, for the first time, strong and direct emperical evidence for the validity of the IB theory of deep learning") seem to present the paper as somehow offering some clarity and further support for the assertions of the Shwartz-Ziv &amp; Tishby 2017 paper, but that paper hoped to establish that information bottleneck can explain the workings of ordinary networks.  Here the authors modify the ordinary cross entropy objective, and so their networks are necessarily not ordinary and so they cannot claim they have helped clarify our understanding of the vast majority of neural networks currently being trained.  Again, this is distinct and should be kept distinct from the utility of their proposed objective, itself inspired by the information bottleneck.  Here too the paper falls flat.  If instead of attempting to comment on networks as they are designed today they aim to proposed a new information bottleneck inspired objective they really ought to directly compare other attempts along those lines (such as the ones they themselves cite  Alemi et al. 2018, Kolchinsky et al. 2017, Chalk et al. 2016, Achile &amp; Soatto 2018, Belghazi et al. 2018) but there are no comparative studies.The experiments are extremely lacking, not only are any of their cited alternatives compared, they don't compare to what would be an equivalent network to their but where they did utilize the noise at every layer and actually made the network stochastic.  Their reported numbers are not very impressive with their top MNIST number at 98.09 and their baseline at 97.73. These numbers are worse than many of the papers they themselves cite.  Only a single comparative results for both a limited training set run and the full one are shown, as well as only a single choice of beta.  The CIFAR10 numbers are not very good either.  There is some discussion of the text suggesting they believe their method acts like an approximate weight decay, but there are no results showing the effect of weight decay just on the baseline classification accuracies they compare against.Technically a deterministic function need not have infinite mutual information, if it is non-invertible, i.e. the sign function, or just floating point discretization. Their own results in Figure 2 and the main body of the text highlight that the authors believe the true mutual information between the activations of the intermediate layers and the input is infinite.  If the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?Just plugging in the Discriminator for the objective (equation (7)) is flawed.  The discriminator, if optimal would learn to approximate the density ratio 1 + log p(x,y)/(p(x) p(y)) .   ( see f-GAN, Norowin et al. 2016).  How does this justify using the individual elements of the discriminator in the functional form of the IB objective?  At the bottom of page 6 they rightfully say that mutual information is invariant to reparameterizations, but their noise regularized mutual information estimator is not (by their own reference (Saxe et al 2017).The discussion at the center of page 8 is confusing.   They claim that Figure 5 (a) is more 'quantized' than (b) and "has reduced entropy".  I think it should be the other way.  More clusters should translate to a higher KL divergence, or higher entropy.  If you need only identify which cluster an activation is in, that should require log K nats where K is the number of clusters.  (a) shows more clusters and so seems like it should cost more and have a higher entropy not a lower one.Despite a recurring focus of the text that this paper applies and information theoretic objective at each layer of the network, and hence is novel, the final sentence of the paper suggests it might not actually be needed and single layer IB objectives can work as well. Unfortunately I don't understand what this paper is about. Please assign to another reviewer. The paper investigates different machine learning approaches to model andpredict the return on property investments, in particular with respect to eagerand lazy learning techniques. The authors evaluate those different techniques ona dataset of properties in Virginia. They conclude that lazy techniques providebetter performance than eager ones.The paper is a purely empirical study that does not introduce any novelmachine learning or evaluation techniques. The authors use the off-the-shelfWEKA toolbox. The results are not clear, given that only a single data set wasused to evaluate the different approaches, and general recommendations cannot bemade.The paper is not well written and the descriptions do not convey what theauthors have done very well. An example of this is Figure 3, which purports toshow the average rent (or rent distribution?) for different housing types. Thereare multiple categories in there that are not valid housing types ("Make MeMove") and the rents shown are incorrect (e.g. more than a million for a townhouse). It is also unclear why the average rent for a single family house isapproximately 4 times as much as for a town house.The problems with Figure 3 are exemplary of the paper; the other issues are toonumerous to list.In summary, this paper should be rejected. This paper addresses long-text generation, with a specific task of being given a prefix of a review and needing to add the next five sentences coherently.  The paper proposes adding two discriminators, one trained to maximize a cosine similarity between source sentences and target sentences (D_{coherence}) and one trained to maximize a cosine similarity between two consecutive sentences.  On some automatic metrics like BLEU and perplexity, an MLE model with these discriminators performs a little bit better than without.This paper does not include any manual evaluation, which is critical for evaluating the quality of generated output, especially for evaluating coherence and cohesion.  This paper uses the task setup and dataset from "Learning to Write with Cooperative Discriminators", Holtzman et al., ACL 2018.  That paper also includes many specified aspects to improve the coherence (from the abstract of that paper "Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.").  But this paper:--Does not compare against the method described in Holtzman et al., or any other prior work--Does not include any human evaluations, even though they were the main measure of evaluation in prior work.This paper states that "To the best of our knowledge, this paper is the first attempt to explicitly capture cross-sentence linguistic properties, i.e., coherence and cohesion, for long text generation."  There is much past work in the NLP community on these.  For example, see: "Modeling local coherence: An entity-based approach" by Barzilay and Lapata, 2005 (which has 500+ citations). It has been widely studied in the area of summarization, for example, "Using Cohesion and Coherence Models for Text Summarization", Mani et al., AAAI 1998, and follow-up work.And in more recent work, the "Learning to Write" paper that the dataset and task follow from addresses several linguistically informed cross-sentence issues like repetition and entailment.  The cosine similarity metric in the model is not very well suited to the tasks of coherence and cohesion, as it is symmetric, while natural language isn't.  The pair:"John went to the store to buy some milk.""When he got there, they were all out."and "When he got there, they were all out.""John went to the store to buy some milk."would have identical scores according to a cosine similarity metric, while the first ordering is much more coherent than the second.The conclusion says "we showed a significant improvement": how was significance determined here? The paper proposes a method for improving the quality of text generation by optimizing for coherence and cohesion. The authors develop two discriminators--a "coherence discriminator" which takes as input all of the sentence embeddings (i.e. averaged word embeddings) of the document and assigns a score, and a "cohesion discriminator" which takes as input the word embeddings of two consecutive sentences and assigns a score. In the former, the score is the cosine similarity between the encodings of the first and second half of the document. In the latter, the score is the cosine similarity between the encodings of the two sentences. Both discriminators use CNNs to encode the inputs. The discriminators are trained to rank true text over randomly drawn negative samples, which consist of randomly permuted sentence orderings and/or random combinations of first/second half of documents. This discriminators are then used to train a text generation model. The output of the text generation model is scored by various automatic metrics, including NLL, PPL, BLEU, and number of unique ngrams in the outputs. The improvements over a generically-trained generation model are very small.Overall, I did not find this paper to be convincing. The initial motivation is good--we need to find a way to capture richer linguistic properties of text and to encourage NLG to produce such properties. However, the discriminators presented do not actually capture the nuances that they purport to capture. As I understand it, these models are just being trained to incentivize high cosine similarity between the words in the first/second half of a document (or sentence/following sentence). That is not reflective of the definitions of coherence and cohesion, which should reflect deeper discourse and even syntactic structure. Rather, these are just models which capture topical similarity, and naively at that. Moreover, training this model to discriminate real text from randomly perturbed text seems problematic since 1) randomly shuffled text should be trivially easy to distinguish from real text in terms of topical similarity and 2) these negative samples are not (I don't think) at all reflective of the types of texts that the discriminators actually need to discriminate, i.e. automatically generated texts. Thus, even ignoring the fact that I disagree with the authors on exactly what the discriminators are/should be doing, it is still not clear to me that the discriminators are well trained to do the thing the authors want them to do. I have various other concerns about the claims, the approach, and the evaluation. A list of more specific questions/comments for the authors is below.- There are a *lot* of unsubstantiated claims and speculation about the linguistic properties that these discriminators capture, and no motivation of analysis as to how they are capturing it. Claims like the following definitely need to be removed: "learn to inspect the higher-level role of T, such as but not limited to, whether it supports the intent of S, transitions smoothly against S, or avoids redundancy", "such as grammar of each of the sentences and the logical flow between arbitrary two consecutive sentences"- You only use automated metrics, despite acknowledging that there is no good way to evaluate generation. Why not use human eval? This is not difficult to carry out, and when you are arguing about such subtle properties of language, human eval is essential. There is no reason that BLEU, for example, would be sensitive to coherence or cohesion, so why would this be a good way to evaluate a model aimed to capture exactly those things?- Also related to human eval, there should be an intrinsic evaluation of the discriminators. Do they correlate with human judgments of coherence and cohesion? You cannot take it for granted that they capture these things (I very much believe they do not), so present some evidence that the models do what you claim they do.- The reported improvements are minuscule, to the extent that I would read them as "no difference". The only metric where there is a real difference is on number of unique ngrams generated cross inputs, which is presumably because its just learning (being encouraged to) spit out words that were in the input. I'd like to see the baseline of just copying the input as the output.- You mention several times that these models will pick up on redundancy. It is not clear to me how they could do that. Aren't they simply using a cosine similarity between feature vectors? Perhaps I am missing something, but I don't see how this could learn to disincentivize redundancy but simultaneously encourage topical similarity. Could you explain this claim? Unfortunately, while this is interesting work, the authors emails are listed on the first page and the acknowledgments are very revealing. I am a big fan of Google, UCL, and the Royal Society, and this strongly biases my view of the work. My biased review:- the paper is interesting, and should go to another venue. I do not think the authors will get benefit from presenting this work at ICLR (there is a tiny quantum focus).- how is the cost function justified? I'd be curious to see how the authors derived it. Right now above Eq 2.4 it seems like it is heuristic to balance successful/erroneous/inconclusive rates. If it is a heuristic, the paper should clearly state this. - using simple examples of quantum data and quantum states would go a long way towards helping me understand the problem setup (Eq 2.1). It took me a while to grok this.- The acronym POVM is never defined. This paper suggests a source of slowness when training a two-layer neural networks: improperly trained output layer (classifier) may hamper learning of the hidden layer (feature). The authors call this inverse internal covariate shift (as opposed to the usual one where the feature distribution shifts and trips the classifier). They identify hard samples, those with large loss, as being the impediment. They then propose a curriculum, where such hard samples are identified at early epochs, their loss attenuated and replaced with a requirement that their features be close to neighboring (in feature space) samples that are similarly classified, but with a more comfortable margin (thus easy.) The authors claim that this allows those samples to contribute through their features at first, without slowing the training down, then in later epochs fully contribute. Some experiments are offered as evidence that this indeed helps speedup.The paper is extremely unclear and was hard to read. The narrative is too casual, a lot of handwaving is made. The notation is very informal and inconsistent. I had to second guess multiple times until deciphering what could have possibly been said. Based on this only, I do not deem this work ready for sharing. Furthermore, there are some general issues with the concepts. Here are some specific remarks.-The intuition of the inverse internal covariate shift is perhaps the main merit of the paper, but Im not sure if this was not mostly appreciated already.-The paper offers some experimental poking and probing to find the source of the issue. But that part of the paper (section 3) is disconnected from what follows, mainly because hardness there is not a single points notion, but rather that of regions of space with a heterogeneous presence of classes. This is quite intuitive in fact. Later, in section 4, hard simply means high loss. This isnt quite the same, since the former notion means rather being near the decision boundary, which is not captured by just having high loss. (Also, the loss is not specified.)-Some issues with Section 3: the notions of task needs a more formal definition, and then subtasks, and union of tasks, priors on tasks, etc. its all too vague. The term non-computable has very specific meaning, best to avoid. Figure 2 is very badly explained (I believe the green curve is the number of classes represented by one element or more, while the red curve is the number of classes represented by 5 elements or more, but I had to figure it out on my own). The whole paragraph preceding Figure 3 is hard to follow. I sort of can make up what is going, especially with the hindsight of Section 4, since its basically a variant of the proposed schedule (easy to hard making sure all clusters, as proxy to classes, are represented) without the feature loss, but it needs a rewriting.-It is important to emphasize that the notion of easy and hard can change along the training, because they are relative to what the weights are at the hidden layer. Features of some samples may be not very separable at some stage, but they may become very separable later. The suggested algorithm does this reevaluation, but this is not made clear early on.-In Section 4, the sentence where S_t(x) is mentioned is unclear. I assume surpass means achieving a better loss. Also later M_t (a margin) is used, when I think what is meant is S_t (a set). The whole notation (e.g. topk, indexing that is not subscripted, non-math mode math) is bad.-If L_t is indeed a loss (and not a performance like its sometimes referred to, as in minus loss), then I assume larger losses means that the weight on the feature loss in equation (3) should be larger. So I think a minus sign is missing in the exponent of equation (2), and also in the algorithm.-Im not sure if the experiments actually show a speedup, in the sense of what the authors started out motivating. A speedup, for me, would look like the training progress curves are basically compressed: everything happens sooner, in terms of epochs. Instead, what we have is basically the same shape curve but with a slight boost in performance (Figure 4.) Its totally disingenuous to say this is a great boost in speed (end of Section 5.2) by saying it took 30 epochs for the non-curriculum version to get to its performance, when within 4 epochs (just like the curriculum version) it was at its final performance basically.-So the real conclusion here is that this curriculum may not have sped up the training in the way we expect it at all. However, the gradual introduction of badly classified samples in later epochs, while essentially replacing their features with similarly classified samples for earlier epochs, has somehow regularized the training. The authors do not discuss this at all, and I think draw the wrong conclusion from the results. This work proposes to replace the regular deterministic activation functions used in artificial neural nets with stochastic variants. In particular, the author(s) considered the q-derivatives of standard activation functions. The author(s) claimed that ``By Proposition 2, the p-derivative of the q-activation g_q(x) agrees with the original activation function f.'' I have trouble understanding this. I assume by original activation function the author(s) meant f(x), then how can Eqn (4) agree with f(x)?At the bottom of pp. 3, the author(s) wrote: ``q-neuron ... combines stochasticity and some second-order information in an easy-to-compute way.'' I definitely can not agree with this point. Basically, q-neuron is the ``derivative'' of the original activation function, so there is no surprise that its derivative links to the second derivative of f(x). I can always use the high order derivative of some function as activation and claim now we are combining even higher order information into the neural network, but does that help? I don't think so. It really annoys me to see that four out of the eight pages are occupied by gigantic figures, which should be placed in supplementary material in my opinion. A simple table could do the job equally well in the text. We are not interested in nitty-gritty details on how the training evolves. Let alone the datasets tested are all small-scale image classification tasks. At least the author(s) should diversify their test beds (e.g., NLP tasks and ImageNet scale experiments) and model architectures (e.g., RNN, ResNet). What's also missing from their experiments is a fair comparison with the real counterparts. I do not see comparisons with dropouts, and to more direct activation function randomization schemes (additive noise to regular activation functions). To summarize, I can not approve this paper as it falls well below the acceptance level of an ICLR. In its current form, it's more like a sketchy note rather than a serious academic paper. I would encourage the authors to significantly enrich the content of this writing before considering resubmitting to another venue. Summary of the paper:The paper proposes an algorithm to find solution to the maximum likelihood problem that could generalize well. The paper argues from the point of view that purely optimizing over the likelihood could result in solution that corresponds to poor local minimum which does not generalize well. By introducing a certain prior on weight, there exists a solution that could generalize. The solution arrived by introducing the prior makes it stable under perturbations of the training data. Recurrent update rules are derived for computing the integrals and hence the solution could be calculated. The authors discuss about the convexity of the effective loss when the variance is large. The paper itself is very bad in its presentation. In terms of technical presentation, it is missing a lot of details, which makes reading and understanding the paper very hard.1.It does not come with any proper literature review and introduction to the formulation of the problem. 2.The presentation of the methodology is also missing a lot of the explanation for many of the details used in the method. For example, in section 2, I do not quite understand the reasoning behind setting the probability P(y|x,w) = (1+1/T lnP(y|x,w))^T. Also, why R_t(w)/Q_(t+1)(w) could be approximated by 1. 3.The theoretical results come in plain words without proper mathematical presentation and the proofs for the statements are not well organized. The correspondence between the proofs and statements are not clear.4.There seems to be no experiments conducted to support the practical use of the method proposed in the paper.Overall, I feel the paper is not ready for publication as a conference paper. The lack of details especially for the technical presentation part make it very hard to read. And the presentation of the results seem to be short of clarity and organization. Further, no experiments showing the practicality of the method are included in the paper. Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions. The authors propose a variant to convLSTMs with convolutional peephole units (as opposed to the original Hadamard product) and tied gates. The writing is poor, the paper seems to indicate a lack of understanding of the SOTA, basing on which the authors propose a variant of the convLSTM with serious flaws.The analysis of the SOTA in the first two sections is mostly incorrect, e.g., the update gate in GRU does not combine input gate, forget gate and memory unit of LSTMS - if anything, the reset gate in GRUs is similar to the forget gate in LSTMs. Also, the term vanilla is normally used as a synonym to original. Vanilla LSTM refers then to the original 1997 implementation, not to Greff et Al.s work in 2017 as suggested by the authors. It is also obviously false that removing an LSTM gate does not incur in a reduction of trainable parameters, etc, ..The model description is difficult to follow, but from what I can gather the model depiction is flawed. In particular, the proposed model bases on the idea to use tied gates to reduce the number of parameters. f^{(t)}, i.e., the value that governs the input, forger and output gates, is derived via a sigmoid. When the sigmoid is zero, the network will hence set all the gates to zero, ignoring the input, forgetting the memory and suppressing the output, which is hardly a behaviour one could ever want. Unfortunately, almost half of the sigmoid spectrum will lead to such behavior. This seems to be a pretty poor modeling decision. The nodes in Figure 1 seem to suggest that two consecutive non-linearities are applied in a row, if W_fc is clamped to zero, as reported by the authors, there is no reason to specify it, the corresponding part of the equation can be removed and the narrative simplified. In sec3, the meaning of net input image and network gate image value is unclear. Also the description of the equation is hard to follow, e.g., the square bracket notation is eventually explained only after 8 lines of text.At the beginning of Sec4 err_1 and err_2 are defined as the difference between the predicted frame and the target frame, and vice versa. This error is then fed to the rgcLSTM input arranger unit and to the next higher layer. By my understanding the error of one layer is fed in the next as an input. I wonder if such error is provided also at inference time, giving a clear guidance to the network to produce the correct output exploiting privileged information. This could also explain why the training process was completed in only one epoch.- Typos:*Intro: More important  more importantly* page5: ReL -&gt; ReLu The paper considers the problem of automatic tuning of hyperparameters in machine learning models. To address this problem the authors propose to use the so called cross-validation gradients, which optimize a validation objective with respect to the hyperparameters of a model. This approach and the investigated setting falls into a class of optimization problems known as bilevel optimization. The main characteristic of this class of optimization problems is the nested structure, with an outer and inner optimization objectives/problems. The outer problem corresponds to the validation objective and it is defined via an optimal solution to the inner problem which corresponds to a training objective. The paper, however, fails to make a reference to a rather rich literature on bilevel optimization (e.g., see [3-5] and references therein).The approach, presented as Algorithm 1, does not seem different from [1] and [2] where hyperparameter optimization was considered for (kernel) support vector machines and (kernel) ridge regression. The data is initially split into k-folds (not necessarily of identical size) and each fold is used exactly once to define a validation objective whereas the complementary folds act as training data. The validation gradient is obtained by averaging the gradients of the k validation folds. Essentially, the same algorithm with k-fold cross-validation was considered in [1]. Thus, for me there does not seem to be any novelty in this approach and the paper itself.The experiments involve synthetic regression and classification datasets but there are no novel insights that advance what is already known about the hyperparameter optimization (e.g., see [3]). For example, there is no intuition on the geometry of the optimization problem and the optimality of the outer optimization problem which is non-convex (e.g., see [7]), or dependence of the outer solution on the accuracy of the inner solution.References:[1] S. Keerthi, V. Sindhwani, and O. Chapelle. An Efficient Method for Gradient-Based Adaptation of Hyperparameters. NIPS 2007.[2] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing Multiple Parameters for Support Vector Machines. Machine Learning, 2002.[-3] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel Programming for Hyperparameter Optimization and Meta-Learning. ICML 2018.[-4] G. Kunapuli, K.P. Bennet, J. Hu, and J-S. Pang. Bilevel Model Selection for SVMs. American Mathematical Society, 2008.[-5] E.S.H. Neto and A.R. de Pierro. On Perturbed Steepest Descent Methods with Inexact Line Search for Bilevel Convex Optimization. Journal of Mathematical Programming and Operations Research, 2011.[-6] B. Colson, P. Marcotte, and G. Savard. A Trust-Region Method for Nonlinear Bilevel Programming: Algorithms and Computational Experience. Computational Optimization and Applications, 2005.[-7] M. Janzamin, H. Sedghi, and A. Anandkumar. Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Method. arXiv preprint arXiv:1506.08473v3, 2016. The authors propose a model that learns to play the China Competitive Poker game. The model uses CNN to predict the actions, and is trained from actual human game records. The model is shown to beat the current best AI and human amateur players.The performance is certainly strong (if it were true). But given the double-blinded policy, there is literally no way to verify the correctness of the performance---in other words, the paper is currently not reproducible at all. So the following comments are based on the trust-worthiness of the paper.(1) immature writing: The writing lacks formality and looks like a final project report. For instance, the super-short Section 2 is rather unprofessional---it is hard to believe that the related works can be described within two paragraphs anyway. Even as someone who understands the game of China Competitive Poker, I find it very hard to follow Section 3. There is a big room for improving the English writing.(2) ill-illustrated specialty of the model: In particular, it is not clear why the model should be superior than other modeling choices. For instance, what role does the neighboring connections of CNNs play? What are the cons and pros of choosing CNNs? Are there strong motivations to design the model this way? (3) many unanswered mysteries: why does the model trained with human records readily super-human? Note that this is controversial to common imitation learning where the typical performance is bound by the human-level performance. Even though authors claimed in the response that there are "many professional records"---but how many is many? Did the authors analyze the records and separate the professional versus amateur ones? The paper argues that in many RL settings the expect discounted reward criterion common in RL is less appropriate than undiscounted success rate maximization, which this paper claims to introduce. The paper argues that the two result in different solutions, points out that success rate maximization can result in instability of existing RL approaches due to introducing "uniformity" of state values within state loops, and proposes modified losses for PPO, Monte-Carlo, and Q-learning algorithms that allows them to optimize the success rate more reliably than they are able to using their standard losses.The paper's motivation is sound: the discounted-reward criterion is indeed conceptually less appropriate than success rate maximization for goal-directed decision-making problems. Unfortunately, however, the paper's claimed contributions in addressing this issue are not novel, and largely flawed:1) Contrary to the paper's claimed contribution, success rate maximization in MDPs isn't new. Paper [1] introduced this criterion, calling it "MAXPROB", and analyzed it mathematically. Without an official name this criterion was known even before that (see, e.g., [1a]). The analysis in [1] focuses on MDP planning, i.e., assumes known model, but the mathematical properties pointed out there that affect value iteration convergence on these MDPs hold in the RL setting just the same.2) The submission's claims about the success rate/MAXPROB criterion causing the instability of value iteration based approaches are partly imprecise and partly outright mistaken. In the intro, the paper states, "this expression belongs to undiscounted problems and the convergence of value iterationoften cannot be guaranteed (Xu et al., 2018)". First of all, I couldn't find any such claims in that paper. Second, that paper doesn't deal with *finite-horizon* MDPs, whereas this submission does, and the problem is that in finite-horizon MDPs succcess rate maximization poses no issues for value iteration at all. The reason is that value iteration in finite-horizon MDPs such as those in Section 3.1 essentially operates on time-augmented state space, needs a single backward pass from states with 0 steps-to-go to states with T steps-to-go in order to compute the optimal value function, and its convergence doesn't depend on the properties of the reward function.Note that Section 3.2.C and Figure 3 that it refers to doesn't talk about value iteration's convergence difficulties during success rate maximization in finite-horizon MDPs from Section 3.1 anymore. It just states in a hand-wavy way that there are some difficulties with this criterion, but doesn't explain exactly what they are.3) The concept of uniformity, as flawed as it is for finite-horizon MDPs defined in Section 3.1, is actually not novel and is subsumed by previously published analysis, again from paper [1]. [1] analyzes the success rate/MAXPROB criterion in *infinite-horizon* undiscounted MDPs with absorbing non-goal states, where vanilla value iteration truly has difficulties with this criterion, and shows that these difficulties are indeed caused by state values being uniform in loops/strongly connected components of the MDP's transition graph. In particular, as shown there, this uniformity introduces additional fixed points of the Bellman backup operator that value iteration relies on, and value iteration can converge to any of these fixed points as a result.4) Several of the submission's other theoretical claims are quite sloppy as well. For instance, in the intro there is a statement "We believe that success rate is different from expected discounted return". I think this notion of difference should be made sufficiently precise so as to take faith out of the equation. Same goes for the loop penalty surrogate criterion in Section 4.1. Does optimizing it, at least in tabular MDPs using vanilla value iteration, result in obtaining a policy that maximizes success rate?5) By itself, the loop penalty criterion looks new. However, a) as mentioned above, it's not clear whether it is a heuristic or has actual optimality guarantees w.r.t. success rate optimization and b) more importantly, papers [1] and [2] suggest at least two alternatives to fixing value iteration's convergence for success rate maximization (in infinite-horizon undiscounted MDPs):   (a) As shown there, for value iteration/Q-learning-like methods, initializing state values *inadmissibly* (i.e., pessimistically in the face of uncertainty) and amending the Bellman backup operator to deal with "value uniformity" yields an optimal algorithm for this criterion.   (b) Turn the success rate maximization MDP into a stochastic shortest path MDP (see [3] or almost any textbook by Bertsekas and Tsitsiklis) by assigning a positive cost to every action, introducing a "cap" D on the highest possible state cost, and minizing the undiscounted expected cost (see stochastic shortest path MDPs with finite dead-end penalty in [2]). D is a hyperparameter, and for *any* such cost assignment there exists a D s.t. the optimal policy for this surrogate MDP will that maximizes success rate in the original MDP. Results from [4] may even help prove something about convergence rate of this approach, although empirically convergence speed and resulting policy (note that there are generally many success rate-maximizing policies) will depend on the specific cost function choice.At least method (b) is conceptually simpler than the loop disorientation penalty, and may even be theoretically similar to the latter, and provides a natural baseline for the proposed approach.6) Last but not least, the empirical evaluation is too limited to be able to assess the merits of the proposed approach. While there are certainly problems where optimizing for success rate directly is preferable to optimizing the discounted reward, the use of discount factor in RL is important in many problems for mitigate the effects of estimation errors -- see, e.g., Xu et al. 2018 and [5]. Therefore, to get a better picture of whether success rate optimization is worth it in practice, one would need a more extensive evaluation on benchmarks such as goal-directed Atari or Procgen games and/or more complex robotics scenarios.Thus, despite studying an interesting topic, I think this work needs to be significantly revised and extended before publication.[1] Kolobov, Mausam, Weld, Geffner. "Heuristic search for generalized stochastic shortest path MDPs" ICAPS-2011[1a] Little, Thiebaux "Probabilistic Planning vs Replanning" An ICAPS-2007  workshop [2] Kolobov, Mausam, Weld. "A Theory of Goal-Oriented MDPs with Dead Ends" UAI-2012[3] Bertsekas, Tsitsiklis. "An Analysis of Stochastic Shortest Path Problems" Mathematics of Operations Research, 1991[4] Yu, Bertsekas. "On Boundedness of Q-Learning Iterates for Stochastic Shortest Path Problems" Mathematics of Operations Research, 2013[5] Jiang, Kulesza, Singh, Lewis. "The Dependence of Effective Planning Horizon on Model Accuracy" AAMAS-2015 This paper attempts to provide a convergence analysis for nonconvex continual learning with episodic memories, and try to theoretically show the degradation of backward transfer caused by  overfitting to memorized samples.  It further proposes an algorithm for learning rate scheduling in  nonconvex continual learning based on these results.The reason of the score of the paper is that the theoretical proof is wrong in my understanding and cannot support the main contribution claimed in this paper,  the main problems are as below.  The proof of the main theorems is questionable regarding the nonconvex assumption, which is the most important contribution claimed in this paper.  Regarding the inequality in  Eq.(5),  in my understanding it is hold to be true only when f is a convex function [1].  And the theorems are based on this inequality which cannot be hold  for nonconvex case if this inequality is not true for nonconvex functions.  If I'm wrong, authors please provide proof of how to get Eq.(5) by L-smooth nonconvex functions.  Moreover, in the proof of Theorem 1, Eq.19  (Appendix A), the inequality of the last step cannot be hold unless the inner product of gradients < \Delta f,  \Delat g > is always positive, which cannot be guaranteed. Otherwise, there is no reason to develop gradient-based approaches in continual learning, such as  GEM [2] or AGEM [3].  So even if Eq.(5) can hold for nonconvex case, the theorem is still questionable. Therefore, the main claim of this paper is highly suspicious to me.  If authors cannot clarify these issues, this paper would be considered as with significant flaws. Despite the questions on the main theorem, the assumption of the initial state of the model is quite strong as it assumes the initial values of parameters are close to the optimal values, which is not very practical unless a pre-trained model is applied.  So the  significance of this paper is further limited.  As the theoretical part is incorrect, I haven't reviewed the experiments part of this paper. If the authors can clarify all above main concerns, I'm willing to make another round of review. [1] Nesterov, Yurii. "Introductory lectures on convex programming volume i: Basic course." Lecture notes 3.4 (1998): 5.[2] Lopez-Paz, David, and Marc'Aurelio Ranzato. "Gradient episodic memory for continual learning." Advances in neural information processing systems. 2017.[3] Chaudhry, Arslan, et al. "Efficient lifelong learning with a-gem." arXiv preprint arXiv:1812.00420 (2018). This paper wants to discuss a new objective function, which the authors dub "Conditional Entropy Bottleneck" (CEB), motivated by learning better latent representations. However, as far as I can tell, the objective functions already exists in the one-parameter family of Information Bottleneck (IB) of Tishby, Pereira, and Bialek. The author seems to realize this in Appendix B, but calls it "a somewhat surprising theoretical result". However, if we express IB as max I(Z;Y) - beta I(Z;X), see (19), and then flip signs and take the max to the min, we get min beta I(Z;X) - I(Z;Y). Taking beta = 1/2, multiplying through by 2, and writing I(X;Z) - I(Y Z) = I(X;Z|Y), we find CIB. Unfortunately, I fail to see how this is surprising or different.A difference only arises when using a variational approximation to IB. The authors compare to the Variational Information Bottleneck (VIB) of Alemi, Fischer, Dillon, and Murphy (arXiv:1612.00410), which requires a classifier, an encoder, and a marginal posterior over the latents. Here, instead of the marginal posterior, they learn a backwards encoder from labels to latents. This difference arises because the IB objective has two terms of opposite sign, and we can group them into positive definite terms in different ways, creating different bounds.Perhaps this grouping leads to a better variational bound? If so, that's only a point about the variational method employed by Alemi et al., and not a separate objective. As this seems to be the main contribution of the paper, this point needs to be explained more carefully and in more detail. For instance, it seems worth pointing out, in the discrete case, that the marginal posterior |Z| values to estimate, and the backwards encoder has |Z| x |Y| -- suggesting this is a possibly a much harder learning problem. If so, there should be a compelling benefit for using this approximation and not the other one.In summary, the authors are not really clear about what they are doing and how it relates to IB. Furthermore, the need for this specific choice in IB parameter space is not made clear, nor do the experimental results giving a compelling need. (The experimental results are also not at all clearly presented or explained.) Therefore, I don't think this paper satisfies the quality, clarity, originality, or significance criteria for ICLR. SUMMARY:This paper is about potential problems of the information bottleneck principle in cases where the output variable Y is a deterministic function of the inputs X. Such a deterministic relationship between outputs and inputs induces the problem that the the IB "information curve" (i.e. I(T;Y) as a function of I(X;T)) is piece-wise linear and, thus, no longer strictly concave, which is crucial for non-degenerate ("interesting") solutions. The authors argue that most real classification problems indeed show such a deterministic relation between the class labels and the inputs X, and they explore several issues that result from such pathologies.EVALUATION:In my opinion, the whole story could be summarized as follows: if  Y isa deterministic function of p-dimensional inputs X, then the joint distribution P(X,Y) is degenerate in that its support lies in a space of dimension p (an not p+1 as it would be in the non-degenerate situation), and this is the source of all pathologies observed. As a consequence, only the cumulative distribution is defined, but there is no density with respect to the Lebesgue measure of R^{p+1}. Thus, one has to be careful when defining the mutual information I(X,Y), which explains the problems with the IB information curve (which should asymptotically converge to I(X;Y) as I(X;T) gets large. Another consequence of this degeneracy concerns the latent variable interpretation of the IB: if T is treated as a latent variable (as, for instance, in the "deep" IB models) then we have the conditional independence relation "Y independent of X given T", which simply makes no sense if Y is deterministic in X (there is, of course, a deeper underlying problem here: the IB problem is difficult in that it is difficult to define a geneative model with a faithful DAG...).Analyzing situations in which Y = f(X) (with f being a deterministic function) is certainly interesting from a theoretic point of view, but I am not convinced that this analysis is truly relevant for practical problems. In particular, I strongly disagree with the statement that "in most classification problems, the labels Y are a deterministic function of X". I would rather argue that the opposite is the case, because I don't think that there are too many such problems with zero Bayes error rate.  In particular, I would argue that digit recognition problems like MNIST so not have deterministic labels, since there will always be images of handwritten characters that will give room for interpretation... This paper proposes a definition for interpretability which is indeed the same as model simplicity using the MDL principle. It has several issues:1) Interpretability is not the same as simplicity or number of model parameters. For example, an MLP is thought to be more interpretable than an RNN with the same number of parameters.2) The definition of explainability in Eq. (5) is flawed. It should not have the second term L(M(X)|M^o, X) which is the goodness of M^o's fit. You should estimate M^o using that equation and then report L(M^o|M^p) as the complexity of the best estimate of the model (subject to e.g. linear class). Mixing accuracy of estimation of a model and its simplicity does not give you a valid explainability score. 3) In Section 2.4.2, the softmax operator will shrink the large negative coefficients to almost zero (reduces the degrees of freedom of a vector by 1). Thus, using softmax will result in loss of information. In the linear observer case, I am not sure why the authors cannot come up with a simple solution without any transformation.4) Several references in the text are missing which hinders understanding of the paper. This paper proposes a method for automatically generating accompaniments using Mel-spectrograms as inputs to a CycleGAN. Overall I think the paper requires significant revision and additional work before it can be accepted as a conference publication. Title: -The title is misleading. The title claims that the proposed model is for "Automatic Music Production". However the actual task considered is more restrictive. The authors propose a model for automatic accompaniment. Music Production involves many other tasks like mixing, mastering and so on, none of which are a part  of this study. The title should therefore be updated to be more specific. Abstract: -"Despite consistent demands from producers and artists...": I think this sentence should be rephrased to motivate the need for automatic accompaniment from a different angle. If not, the authors should present some justification for the demand for this technology from artists and producers. -"Automatic music arrangement from raw audio in the frequency domain": why not simply say automatic music arrangement/accompaniment in the Mel-frequency domain? I find the raw audio part of the description unnecessary and confusing. -The authors claim that the they are the first to treat music audio as images and then apply techniques from computer vision. However, treating spectrograms as images is the current standard for many MIR tasks like music transcription, chord recognition and so on e.g. "An end-to-end Neural Network for Automatic Music Transcription": https://ieeexplore.ieee.org/abstract/document/7416164/. There are hundreds of other publications that are similar to this approach. Introduction: -The authors claim that automatic accompaniment in the waveform/frequency domain has many advantages. However they fail to motivate the short-comings of this approach. Namely the lack of source separated training data and the extreme difficulty in source separation for music recordings. It  would also be useful to cite a review paper or some of the many publications on automatic accompaniment generation in the symbolic domain so that the reader can find references to this problem which has an extensive literature already. -The authors mention that they use the Demucs algorithm for source separation. However they do not provide any details whatsoever about this approach, especially the downsides. A quick scan of the paper reveals that the algorithm introduces severe artefacts under various conditions. -The authors mention the low-computational cost of their proposed method, however they do not satisfactorily quantify this claim. Firstly, is computational cost an issue? Does this algorithm have to run on a mobile device? Will it be run in a streaming setting? These questions are not answered in the paper. Related Works:-The authors cite many papers on music generation in the waveform domain however they do not cite any of the extensive literature on music generation in the symbolic domain. This literature is extremely relevant to the work presented in this paper. -"Nevertheless, only raw audio representation can produce, at least in the long run, appealing results in view of music production for artistic and commercial purposes." Why is this the case? Why is generating music in the symbolic domain and then using state-of-the-art synthesisers not an appealing direction? This point isn't made clear in the paper. Method:-There are no details provided about the Demucs algorithm used to separate the source training data into various channels like vocal, bass, drums etc. How big was the model? Did the authors train the model themselves? Did they use a pre-trained model? Were there any artefacts present in the source separated tracks? Are there any downsides to this algorithm? Are there any alternatives to this algorithm? Do the artefacts not interfere with the  downstream task? -A reference/citation about the Mel scale would be useful. -There are no details about the CycleGAN used in the paper. How big is the model? What is the architecture? How was it trained? What flavour of gradient descent was used for training? What are the hyper-parameters? Was the model trained on a single GPU? Experiments:-How was the subset of pop music selected? How was the metadata filtered to obtain the 10000 tracks used for training? If the filtering algorithm cannot be outlined, then it would be useful to provide a list of the 10000 tracks used for training, for the purpose of reproducibility. -How did the authors arrive on the 4 attributes quality, euphony, coherence and intelligibility? Is there some theory that suggests that these 4 attributes would be useful in determining whether the accompaniment is somehow good? These attributes have been presented without justifications and citations. -The features (STOI, FID) used to compare the automatically generated accompaniment have also been presented without much justification. Why is it that these features  are an adequate representation of the generated audio? -I found the description of the grades and the subsequent comparison in Figure 3 difficult to follow. I think the description needs to be significantly more rigorous.  * First of all, the submission is not following the style.* The paper only has one page.* The proposed method is not novel, and it is not described clearly. * Lack of experiments and comparisons. Similarly, a one-page paper will have a very hard time to deliver enough story. There is only one table in the whole paper.* BRP or BPR?* I also found that it is on a YouTube video (https://www.youtube.com/watch?v=Rj5E724O4nA&ab_channel=anwithaparuchuri) mentioning it is a submission to another venue (WeCNLP). It looks the results (the only table) are totally identical, so it probably should be considered as dual submission. Also from the video, probably it also violates the double-blind policy. The submissions deals with the problem of assigning tickets to agents in a Customer Relationship Management (CRM) system. The motivation is that the traditional way of assigning tickets does not take into account the interactions between agents and the categories of the tickets. Three algorithms (Table 1) are compared on the AUC measure, however the details of the experiment are not clear. It is also not clear how the experiments were implemented. Figure 1 is hard to read and the caption on Table 1 is incomplete. Several features are used for agents and tickets but their details are not provided, e.g., how are the word2vec features described in Section "Model Architecture" obtained? The results are hard to understand too. Which model are being compared exactly, and what are their parameters? The "Model Architecture" section talks about a LightFM based model, while the table describes a model named "logistic", another named "BRP" (is it supposed to be the well known BPR-MF algorithm?) and the third one named "WARP." None of these are described. The conclusion refers to a "proposed recommender system", but no such system is proposed in the paper. This submission cannot be accepted to the conference. Thanks for submitting your paper. It takes a lot of effort and courage to put your ideas out into the world. Sometimes the hardest work for researchers is conveying their thoughts to others in a manner in which those ideas can be understood.With that in mind, I had an extremely difficult time following your arguments. I noticed several things: - There are numerous places in the text that lack proper citation, or are cited improperly. - Why was there not a related methods section? I find it hard to believe that all of your ideas have no precursor. - When there are citations, there is usually only one text and it is quite old. For example, all of your neuroscience citations reference a work that is almost 40 years old. There have been quite a few improvements in our biological understanding as well as theoretical understanding since then. ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology. )- There is a claim regarding how this can be used in fintech. This statement doesn't belong in this work. - There are many different equations given throughout the text. Some of these equations come from areas like physics or information theory, and others seem to be of your own design. Regarding the latter, there is no justification or explanation for the origin of the equations. Regarding the former, if you are using equations from lots of different fields, or even field you think part of your audience might not be familiar with, you should, at the very least, include a some description of the algorithm or intuition as to why it is being leveraged. - It wasn't clear from your diagrams or your descriptions what the difference between a synapse and a neuron was in your architecture. It seemed like the name was used interchangeably in some areas, but then had a strict definition in others. - I was also not able to understand how the excitatory and the inhibitory connections that were to enter each neuron were connected to the previous layer of the network. Is a link between neurons in Figure 2 actually two links? If this is the case, then it is a direct violation of Dale's law. Again, I only mention this because most arguments seem to be of the form "this is correct because it is how it is done biologically". - There were a few claims made in the paper that were completely unsubstantiated. A good example of this was in the conclusion section part ii) where it was stated that "using a large number of synapses and neurons SynaNN can solve the complex problems in the real world."- Also, the last sentence of the conclusion was not discussed anywhere in the rest of the paper. Nor was the statement itself supported except with a single citation and no description.Regarding the empirical testing of your algorithm, I was very dissapointed to see that the only dataset it was tested against was MNIST. Furthermore there was absolutely no benchmarking against other comparative algorithms. At the very least I would have expected a comparison to the perceptron algorithm that you use as inspiration, but that would also still not have been enough.This paper needs heavy amounts of work to make it understandable. Once it is understandable an attempt to evaluate the merit of the scientific contribution would then be possible. The authors present a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells.  Unfortunately, the paper is structured incoherently, making it nearly impossible to appreciate the authors' contribution.  The introduction references neuroscience alongside ResNets, FinTech, surprisal spaces, Bose-Einstein statistics, and topological conjugacy without adequately motivating or defining any of the above.  The fundamental definition of the model synapse as a conditional probability (Eq. 1) is not guaranteed to be non-negative, casting serious doubt on any of the subsequent conclusions.   Figure 1 conveys no further information about the proposed model.  There is no explicit related work or background section.  The single experiment offers no comparison to alternative methods.   I suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologically-inspired learning models.  This is indeed a subfield of machine learning worthy of more investigation. Quality - poorThe highly complicated work is evaluated only on the simplest of benchmarks with no significant results. Clarity - poorThe paper seems to amount to gobbledygook, many disparate terminology strung together. OriginalityNo idea. Significance None. cons: the paper to me seems a hashing of citations to the main works in neuroscience and deep learning for which only the simplest network is demonstrated (single hidden layer MLP on MNIST) with results that do not exceed that of a standard MLP.pros: the only pro I can think of for this work is that synaptic computing imo deserves more consideration, as real synapses are very complicated beasts, the functioning of which relatively little is known about. The paper introduces HR-TD, a variation of the TD(0) algorithm. The variant is meant to ameliorate a problem of over-generalization with conventional TD. This problem is briefly characterized, but primarily it is presumed to be established by prior work. The algorithm is simple and a series of experiments are presented with it applied to Mountain Car, Acrobot, and Atari Pong, with both linear function approximation and neural networks (DDQN). It is claimed that the results establish HR-TD as an improvement over TD. However, I found the results unconvincing because they were statistically insufficient, methodologically flawed, and too poorly presented for me to be confident of the meaning of numbers reported. In addition, it is not hard to imagine very simple problems where the HR-TD technique would be counterproductive, and these cases were not included in the experimental testbeds.The first weakness of the paper is with its characterization of the problem that it seeks to solve: over-generalization. This problem is never really characterized in this paper. It instead refers instead to two other papers, one published only in a symposium and the other with no publication venue identified.The second weakness of the paper is the claim that it has done a theoretical analysis in Section 4.4. I dont see how this section establishes anything of importance about the new method.The problem with the main results, the empirical results, is that they do not come close to being persuasive. There are many problems, beginning with there simply not being clear. I read and reread the paragraphs in Section 5.1, but I cannot see a clear statement of what these numbers are. Whatever they are, to assess differences between them would require a statistical statement, and there is none given. Moreover to give such a statistical statement would require saying something about the spread of the results, such as the empirical variance, but none is given. And to say something about the variance one would need substantially more than 10 runs per algorithm. Finally, there is the essential issue of parameter settings. With just one number given for each algorithm, there are no results or no statement about what happens as the parameters are varied. Any one of these problems could render the results meaningless; together they surely are.These problems become even greater in the larger problems.A nice property of HR-TD is that it is simple. Based on that simplicity we can understand it as being similar to a bias toward small weights. Such a bias could be helpful on some problems, possibly on all of those considered here. In general it is not clear that such a bias is a good idea, and regular TD does not have it. Further, HR-TD does not do exactly a bias to small weights, but something more complicated. All of these things need to be teased apart in careful experiments. I recommend small simple ones. How about a simple chain of states that are passed through reliably in sequence leading to a terminal state with a reward of 1000 (and all the other rewards 0). Suppose all the states have the same feature representation. If gamma=1, then all states have value 1000, and TD will easily learn and stick at this value even for large alpha, but HR-TD will have a large bias toward 0, and the values will converge to something significantly less than the true value of 1000. That would be an interesting experiment to do. Also good would be to compare HR-TD to a standard bias toward small weights to see if that is sufficient to explain the performance differences. This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. First, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldnt make sense: in RL you get to choose the policy so it doesnt make sense to assume that the choice of action is confounded while youre doing RL. To get around this, the authors assume that theyre working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they dont explain why you would want to do RL at all when you could just do causal inference directly. If you cant experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). # MethodThe authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to  believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. Quibbles: - Page 3: the authors claim the model is without loss of generality but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc.  - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldnt all be factorized normal distributions. Real data isnt made up of factorized normals.# ExperimentsThe authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so Ill only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. Confounding is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isnt affected directly by the latent variable. Because of this, there isnt actually a confounding problem - the confounder simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). The RL evaluations arent described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions.# SummaryThis work studies a setting in which the correct baselines would be causal inference algorithms (but they arent considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. In this paper, the authors proposed to learn a stacked classifier on top of the outputs of well-known transfer learning models for transfer learning. The authors claimed that their proposed solution can avoid negative transfer.Technically, there are no contributions. The proposed solution is a straight-forward A+B, where both A and B are well-known. Specifically, in the proposed solution, different well-known transfer learning models are used as the 1st level classifiers to generate intermediate outputs, then a stacked classifier is trained with the intermediate outputs as its inputs. Stacking techniques are also well-known in ensemble learning. Therefore, I do not see any new technical ideas. Moreover, the proposed solution cannot really avoid negative transfer. If two domains are indeed very different, the performance of the basic transfer learning models would be very bad, e.g., worse than random guess. In this case building a stacked classifier cannot help to boost the final performance. The datasets used to conduct experiments are all of toy sizes.There are a lot of typos and grammar errors. The format of citations in the main text are incorrect. In summary, the quality of this paper is far below the standard of top conferences. My recommendation would be to reject this paper for reasons which I will outline below.Pros:The problem of learning good features for RL is an important one to tackle and has application in many important tasks with complex input spaces like vision. The authors explain why this is important and I encourage them to continue to explore this line of research albeit more thoroughly as detailed below. Cons:One big issue with the paper is in its lack of experimental rigour. This can be improved by considering more than just the single Breakout task (the Atari suite itself contains many example tasks ripe for research) and also by comparing against more competitive baselines.For instance the authors mention the AC-VAE (Yang et al. [1]) but do not compare against their method since they do not report rewards achieved on Breakout. To me this is not a good enough reason to not have a comparison. Instead I encourage the authors to reimplement the alternative approach and compare it on the tasks they consider.While the paper lists some early work on representation learning and learning features for RL they do not mention or compare against many other works (e.g. Jaderberg et. al. [2] and Achiam et al [3] to name a few). A thorough literature survey helps place the work in the broader context of the field and allows the work to be evaluated more comprehensively. Finally there are a number of minor points in the presentation which did not affect my final decision but would be good to fix.  There is a typo where 'Introduction' is spelled 'Introdction'. Reference capitalisation is inconsistent - in some places the paper refers to 'Figure 1' but elsewhere there is 'figure 3' and 'figure 4' (on page 7). If references are to be capitalised, 'table 1' would also have to be replaced with 'Table 1'.References: [1] John Yang, Gyuejeong Lee, Simyung Chang, and Nojun Kwak. Towards governing agents efficacy: Action-conditional ²-vae for deep transparent reinforcement learning. volume 101 of Proceedings of Machine Learning Research, pp. 3247, Nagoya, Japan, 1719 Nov 2019. PMLR. URL http: //proceedings.mlr.press/v101/yang19a.html.[2]. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.[3] Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299, 2018. This paper proposes to use multiple levels of language units, including characters, subwords, and words, for Chinese language modeling. Three versions of multi-vocabulary pretraining methods are also studied. Experiments show that using the optimized seg_tok units could improve the model performance in various downstream tasks, and the MVP strategies boost the seg_toks results on sequence labeling tasks.Strengths:1.Empirical studies on building vocabularies with three different granularities of language units for Chinese language model pre-training.2.Evaluations on three multi-vocab pre-training strategies.My most concerns are:1.The idea of modeling different levels of language units has been widely studied before. Although they have not been comprehensively evaluated on pre-trained LMs, the findings are quite similar with previous studies, without much new highlights. The claim of the contribution, the combination of CWS and subword tokenization is novel., is quite weak. Missing References:Zhang, X., & Li, H. (2020). AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. arXiv preprint arXiv:2008.11869.Zhang et al, (2019). "Effective subword segmentation for text comprehension." IEEE/ACM Transactions on Audio, Speech, and Language Processing 27.11 (2019): 1664-1674.Liu, Z., Xu, Y., Winata, G. I., & Fung, P. (2019). Incorporating word and subword units in unsupervised machine translation using language model rescoring. arXiv preprint arXiv:1908.05925.2. In seg vocab development, the choice of 99.95% is interesting but can you please tell what is the lowest word frequency in this vocab? how many words with a frequency of less than 40 are present? if criteria is changed from 99.95% coverage to the selection of only frequent words (maybe >40), would it had any effect on speed? or performance?3.This paper is poorly written. Thorough proofreading is required. There are too many typos and grammar errors. e.g.,In the abstract, performances -> performance; remain -> remains; does not only improves -> does not only improve;In the second paragraph of page 2, incorporate -> incorporates; combine -> combineslIn Footnote 5, avalable -> available.4.There is no comparison on public tasks or datasets.Other comments:1.For the multi-vocabulary pre-training, the vocabulary size, i.e., the size of the seg_toks, would be an important influence factor for the downstream task performance. 2.Only ALBERT small model is evaluated. It is not quite sure if the method can further enhance the state-of-the-art language models.3.The citation format is in chaos, check the usage of \citep{} and \citet{}. The citation of ALBERT is missed in the last paragraph of page 2.4.Check the second line in page 5.  This paper studies the problem of certified robustness in adversarial learning. In a nutshell, they apply the randomized smoothing technique to the distributional robustness certificate proposed by Sinha et al. (2018), thereby relaxing the smoothness assumption required therein so that the ReLU network can be applied. Based on this new formulation, they derive the upper bound on the worst-case population loss and develop an algorithm with convergence guarantees. The results on tested on MNIST, CIFAR-10 and Tiny ImageNet.The topic is definitely important and the authors did a good job of explaining their framework. Nevertheless, unfortunately, I think the proposed methodology seems rather straightforward and does not provide many new insights into this area. Besides, there are numerous careless flaws in the paper.1. Theorem 1 appears to be a standard result in distributional robust optimization and it is unfortunate that the authors did not recognize it. See, for example, Kuhn et al. (2019) (https://arxiv.org/abs/1908.08729).2. Theorem 2 and Corollary 1 appear to be a standard result in randomized smoothing. Besides, the proof is not rigorous -- it takes for granted that the $\hat\ell$ is twice differentiable.3. The statement of Theorem 3 contains an obvious typo. The left side and the right side are no different from each other -- the only difference is the integration variable: one is $\hat{x}$ and the other is $x'$.That is to say, all three major theoretical results are either straightforward corollary from existing results or contain flaws. 4. The algorithm proposed in Section 3.2 is problematic. In particular, in equation (6) and Alg.1 line 3-6, when $\cal X$ is a normed vector space, the value of the inner supremum does not depend on $z_{ij}$ simply via a change of variable from $x+z_{ij}$ to $x$. I think the correct version should be $$  \frac{1}{n} \sum_{i=1}^n \sup_{x\in\cal X} [ \frac{1}{s}\sum_{j=1}^s \ell(\theta;x+z_{ij}) - \gamma c(x,x_0^i) ].$$5. As for the numerical experiment, it is unclear to me how $\gamma$ is chosen, and in particular, does such choice make the inner supremum of (6) a strongly concave problem?Given these many issues that are not easy to fix, I would encourage the authors to carefully revise their manuscript and resubmit to another conference. This work proposes to augment object focused image to improve image classification. Essentially, this work tries to remove background from original image using an existing algorithm and human editor, and train an image classification model using different combinations of original images and background-removed image. Five simple models are trained and their performance is compared to show that using background-removed images together with original images can help improve accuracy on validation set on a simple dataset. Overall, this paper has serious flaws and is far from the level of ICLR. It is more like a course project report than a scientific paper. It lacks rigorous experiment design and result analysis, and the conclusion is not surprising or insightful. A clear rejection.Detailed comments:Technical:- Although it makes sense to augment. input images to improve model performance, the design of the proposed work is less convincing. When talking about data augmentation, there usually won't be human editor/annotator involved and all the operations should be performed on-the-fly algorithmically. However, in this work, the background removal for original image is done either by a human editor or a third-party tool, which is not scalable at all. - The technical part is also significantly limited. The background removal assumes that there is only one dominant object related to the label in an image. This simplifies the real problem too much. As the paper shows in Figure 3, this poses a serious problem when there are more than one object and the object of interest is not dominant. I do not think the proposed method can be applied to more complex scenarios at all. - The paper states "When that baby sees only a cat in the image, s/he would not get confused which object is the cat." However, this may not be true. Actually we haven't really understood how exactly human's perception/neural system works.- The performance of the model trained on images using the third-party background removal tool heavily depends on the quality of the tool. Since there is no evaluation on the tool, we do not know how the quality of background removal contributes to model performance improvement/degradation. Experiments:- The proposed model is only evaluated on one simple dataset containing only cats and dogs. It is more a sanity check/toy example than a realistic evaluation setting since there is just binary classification. Larger-scale datasets with multiple classes should be used.- The model evaluated is a simple convolutional network, and not a standard model; i.e., ResNet, VGG or other widely used model, so it is difficult to say whether the conclusion drawn from this model can by applied to other models and datasets. - Why not mix human edited images and images modified by the tool and train a model on all the images to see how it works?  Summary:This paper proposes a method for causal inference given observational data. The authors formulate this task as a domain adaptation problem and propose a self-training algorithm to address it. The algorithm imputes the counterfactual outcomes (to simulate a randomized controlled trial) in an iterative process that supposedly gets better and better at estimating treatment outcomes. I have many concerns about the paper; I touch on some of them in the following:- There seems to be a misunderstanding in the literature review. There are two main approaches for *Policy Optimization*: Direct Method (DM) and Utility Maximization (UM). Counterfactual Risk Minimization (CRM) is a method in the UM category. It is the *Causal Inference* task that this paper addresses; not *Policy Optimization*.- It is unclear why the proposed algorithm, namely Counterfactual Self-Training, should perform well; i.e., why a model trained on a new dataset with randomly imputed counterfactuals should do any better & - It seems that the second term in the objective function in Eq. (1) is always zero; since, $\hat{r_{i, p}}$ is equal to $f_theta(x_i, p)$ according to the line just above Eq. (1). Therefore, the objective function reduces to an unweighted factual loss, which we know would not result in a good model, since it does not account for selection bias.- Algorithm 1, on the other hand, says that $\hat{r_{i, p}}$ is sampled from $P_{\theta}( r | x_i, p )$. The authors should comment on whether this is equivalent to $f_theta(x_i, p)$...?- The iterative process of imputing counterfactuals and then training a new model on the new RCT-like dataset is said to continue until the optimization converges. However, the authors do not discuss what the convergence criteria are.- Definition for $M$ in Theorem 1 is vague: which values for $P$ and $P_{\theta}$ are used? Shouldnt $M$ be a constant?Minor comment(s):- There are many typos in the paper, some of which are in the equations.  The authors present an analysis of previously proposed Dirichlet based models for adversarial robustness and empirically evaluate exiting methods on two image datasets, MNIST and CIFAR10, as well as 2 tabular datasets.While in principle adversarial robustness is an interesting topic, the scope of the paper is extremely narrow and I would have liked to see a broader set of Bayesian models being included. In addition, I find the set of experiments very limiting. MNIST is not a representative dataset at all and for a meaningful comparison analysing a large-scale dataset (such as Imagenet) is crucial. Furthermore, I would have liked to see a broader type of data - how about a text dataset (such as 20 newsgroup) or a different sequential dataset with a recurrent architecture? I feel with the limited  experiments is not a very useful resource for practitioners. Also, performance for other attacks (deepfool, black-box attack) would have been interesting.Even with the very limited set of experiments, the authors basically report negative results, showing that neither of the approaches could detect adversarial attacks, OOD samples or highly perturbed dataThe authors also propose a robust training strategy, but concede that this increases performance either for either ID data or OOD data, but not both.While in principle also such negative results can be valuable for the community, I feel in this case the scope of the paper is too narrow and a broader class of Bayesian and ideally non-Bayesian but uncertainty-aware methods should have been analysed. ## SummaryThis paper presents a graph attention architecture that captures long-range interactions. The novelties in the architectures are (1) vector-based parameterization of edge type in modeling message, (2) slight modification of graph attention (Section 3.2), and (3) GRU-based node update function. The experiments are primarily on synthetic tasks. However, it is unclear if modeling such long-range interaction is useful in real tasks. The paper fails to demonstrate convincing results on the real tasks of entity classification in knowledge graphs.## Pros1. Detailed architecture explanation.2. Good performance on synthetic tasks.3. Careful design of synthetic tasks.## Cons:1. The novelty of architecture is limited as detailed below.- Vector-based parameterization of edge type in the relational graph has been commonly adopted in GNNs for molecular graphs (e.g., Eq (1) in https://arxiv.org/pdf/1709.04555.pdf), and not novel. - The modifications of graph attention architecture are rather minor (removing a single linear transformation, adding edge type embedding in the key of the attention mechanism). - GRU-based node update function is not empirically shown to be beneficial although being highly complicated.2. Experiments are largely synthetic, and no convincing results are provided for the real datasets on entity classification in knowledge graphs. It is unclear if modeling long-range interaction is useful in practice. One domain long-range interaction could be useful is molecule classification, where you can treat molecular graphs as multi-relational graphs and those graphs tend to have large graph diameters. Many datasets are readily available [here](https://ogb.stanford.edu/docs/graphprop/).3. Details of the real knowledge graph datasets (AIFB and AM) are not provided in the main texts. The paper proposes to combine deep kernel learning (DKL) with PDE/ODE prior knowledge for learning spatiotemporal systems. The idea is sound and sensible, although it represents an incremental combination of two existing techniques. The differential assumption clearly improves in spatiotemporal experiments, which also motivates the idea. The paper has unfortunately three major mathematical errors. First, the very first equation of the method (eq 4) is already incorrect: g(x) is not a function of input x but instead its a function of solution f. The paper itself acknowledges this by giving series of examples which all are functions of f. The following GP prior for g(x) is then placed on incorrect inputs and seems misguided or at least insufficient to model the differential. Intuitively this is also easy to see: the differential of the solution is obviously not just a function of space and time (but of solution as well). Second, the eq 6 states that a non-linear differential of a GP is some other Gaussian process. This is incorrect: non-linear transforms of a Gaussian process does not retain Gaussianity. Since this is a central definition of the paper, the whole method is likely incorrect.Third, the paper introduces a bizarre concept of constant zero vectors as random variables, ie. p(0|g). This is clearly wrong, and the probabilistic model is then wrong as well. While this could be easily fixed, the model is an example of GP-matching (which has known pathologies), and this has been extensively studied by many authors (eg. Wenk19, Wenk18, Gorbach17, MacDonalds15).Given these obvious errors, the paper needs a major revision.Technical comments:o Eq 4 should be g(f,x), since later the \varphi is a function of f and its x-derivatives. Currently none of the examples (eg. burger) follow eq 4. Also eq6 is also wrong with same argumento Please define (mathematically) the tackled problem and the problem domain. The paper starts by discussing classification, but at sec 3 the context suddenly changes into PDEs without the reader being informed about this.o The paragraph to incorporate.. is difficult to follow since its technical but does not open up the math yet. It would help to write this in more conceptual wayo It would greatly help the reader understand the method to include the sup-fig1 in the main papero It seems that both f and g are assumed to be separate GPs. This is unclear from text, please clarifyo eq7 is strange, since it defines a dirac between a random variable g and a sample of h. This is nonsensical.o The 0-sampling is obviously wrong and unnecessary. Double priors are perfectly fine in Bayesian modelling, and eq9 could be defined without the 0-stuff. The double priors are called product of expert priors, see eg. MacDonalds'15 or Wenk19. They do have their own pathologies, but these papers discuss them extensively.    Summary: - The paper claims to discover a universal spatio-temporal induction (STI) effect in network traffic flows, and developed a model FlowNN to learn representations of flow-structure data. However, the STI effect was not clearly explained and the problem is not well formulated, making it hard for readers to understand the value of this work.Strong points: - Modeling flow-structure data and internet traffic flows is an interesting problem.- The paper motivates the problem well.Weak points: - The writing of this paper is very confusing and unclear. - The problem was not clearly formulated or articulated.- The descriptions of the model and STI effect are confusing.Recommendation: - I strongly recommend a reject. While the topic is interesting, the paper is hardly understandable in its current form. The contributions cannot be well understood and evaluated. Improving clarity will make it a much stronger submission in the future.Comments & questions:- What exactly is the task? It was never clearly stated in the paper. Only mentioning Learning the representations or regression tasks is unclear and confusing.- What is the training and test data split? How does the approach generalize to unseen networks or flows?- The definition of STI is unclear. In Definition 1, the paper writes Spatio-Temporal Induction defines the synchronized map process to produce the future/spatial evolution between neighboring nodes. This does not define the STI effect. What exactly is the synchronized map process? Different network flows process packets at different paces, so how are they synchronized?- How can the representation (Fig 3a) scale with the number of nodes in the network? You would need one such 3D matrix for a flow. The paper evaluates with 28 nodes in total (Sec. 4), but the real internet is orders of magnitude larger (billions and millions of flows).- The meaning of Fig 4. is unclear. a spike at a upstream node in Fig. 4a spurts the flow at downstream nodes in certain subsequent time windows. However, its very hard to see that from the figure. It seems like all flows are fluctuating at different time steps.- Many claims seem to be wrong or not supported. A few examples:  - a encoding net with shared weights is learnt to extract the common patterns shared in all received inputs. Share weights imply extracting similar patterns across flows, how can the design extract common patterns?  - This reconstructs the evolution patterns of the APP source flows that drive the evolution processes at all nodes along the path. -> How exactly?- The meaning of many sentences are unclear. A few examples:  - the flow rates be- tween two neighbouring or any pair of nodes in the routing path will become larger interchangeably than the other so that the flux is conserved among nodes.  - This not only experimentally confirms the S-shaped spatio-temporal correlation present in the flow-structured data, but also further confines the correlation with the explicit (partial) flow conservation.  - Induction is operated on & by the paired induction operator.- Many grammar issues. A few examples:  - Data can be organized compact and complete  - propagation process as doing in above IP traffic flows  - While any encoder and decoder can be used so long as we can backpropagate through it.Suggestions- Writing-wise, it will be helpful to simplify the wording and be direct and specific. - Itll be helpful to ask people not familiar with the project to read the paper and get feedback. **1. Presentation and clarity**I believe the paper is very poorly structured, does not introduce related work properly, contains many unclear points in the presentation, making it almost impossible for the reader to grasp key ideas without reading at least three related works on which this paper is heavily based. Until one consults (Schonfeld et al., 2020) it is not clear what the per-pixel feedback means, it's also not clear what is inpainting regularizations. If we look at section 3.3 we might get confused what is the disciminator encoder and decoder, and how the losses are used to train them. These details are fleshed out in the appendix and in the prior work. I believe the flaw in presentation is due to weak originality of the paper. It borrows heavily from (Schonfeld et al., 2020) and (Siarohin et al., 2019b) and hence does not have much of original content. **2. Originality**The paper replaces the discriminator of (Siarohin et al., 2019b) with the one presented in (Schonfeld et al., 2020)  supervising it with the images generated with the priority-cut scheme. The latter uses the occlusion mask of (Siarohin et al., 2019b) to generate a new image by combining the generated image with the driving image, attempting to show the discriminator which pixels require further attention. In my opinion, these ideas are quite marginal and do not contain any interest for community. The paper re-uses already existing and well working techniques, such as the whole framework of (Siarohin et al., 2019b).**3. Results**By looking at the results I cannot see significant differences compared with (Siarohin et al., 2019b). The artifacts present in the first order motion model are present here, so no noticeable improvement overall. The reader will be able to notice something only if they zoom into the figures and compare very small details. To facilitate such behavior, the authors increased the regions in which such details might be noticeable. Even with such visualization, I have to admit, the differences are insignificant. This observation is supported by the numerical results as well. The improvement over FOMM is 0.0012 in terms of L1 on VoxCeleb and 0.002 on Tai-Chi-HD and 0.0016 on BAIR. **4. Rating**I believe, poor presentation alone is a sufficient reason to recommend rejection. If we set aside it for a moment, we notice that the presented ideas are simple adaptations from previously published papers and results do not show necessary improvement. 1. SummaryThis paper proposes an improved offline RL (batch RL) algorithm combining the state-of-the-art behavior-regularization actor-critic method (Nair et al., 2020) with a model-based RL technique. N-trained probabilistic dynamics models generate fictitious trajectories with uncertainty-penalized rewards after pretraining the policy with the behavior-regularization solely on the offline data. Both these generated data and the original offline data are used for the further behavior-regularized actor-critic training. Numerical results showed that the proposed method outperformed recent offline model-free and model-based RL algorithms.2. Pros- This paper efficiently combines the state-of-the-art method with a model-based RL technique.- Challenges in offline RL and the related works are well-described.- Ablation study investigated five different kinds of data types for in-depth investigation.3. Cons(1) Experimental Support- The ablation study is unfair. The results of 'ours' in Table 1 are the maximum combination of AWAC and AWAC+MB2PO, which includes five best results from AWAC out of nine best results from 'Ours.' Although related explanations are written at the end of Section 5, AWAC+MB2PO should be considered as 'ours.' This is the reason why I had no choice but to give a harsh rating.- Besides, model-based learning and generating fictitious samples suffer from compound errors, so the learning can be inaccurate, especially when the length of the trajectories is long. However, the proposed algorithm uses 95% of fictitious samples in the model-based fine-tuning phase, causing degradation of overall performance due to the compound error issues.(2) NoveltyThis work is not novel since this paper's algorithm heavily depends on state-of-the-art AWAC (Nair et al., 2020), and the uncertainty-penalized reward generalization is from other previous work (Yu et al., 2020).(3) ReproducibilityThe paper's experiments do not guarantee reproducibility since the authors did not provide the implementation code.4. Minor concerns- The notion of the behavioral policy should be unified, either pi_b or pi_beta.-  The notion of the expectation should be unified either E or mathbb{E}. The paper is very poorly written. This is in part from English as a second language I suspect, but even assuming some mentorship on the writing, there are too many flaws to consider accepting this paper. The modelling work is so poorly described I can't really comment on it. The majority of my review will just attempt to point out some areas I found lacked clarity.Some examples of lack of clarity and poor writing:Introduction: * "commonsense knowledge graph"; not clear what "commonsense means here. Related work:* "fine tuning models are raised". I can only guess at what was meant Knowledge enhanced LM section: * there are several other works looking at incorporating knowledge graph (KG) information into text generation processes, including using popular LM as part of it. What you really mean with this work is something like uncontrolled/unconstrained story generation, although it is never properly defined what your are looking at. Rhetorical text generation section: *The negative example "Flakes of snow are flying like snow" -- it isn't certain that such an error is due to not incorporating KG information into the NLG process. This is just a possible hypothesis.Our KETG framework section:* "innovative". This is non-scientific. Don't use adjectives to describe your own work.* "just like the external device to computer". Again, can only really guess at what is meant.* This whole section is lacking any form of clarity. There are no equations grounding what is precisely done. It isn't even made clear what is assumed by your proposed framework. Where the keywords come from a sentence is not clear. I think this is likely a strong requirement of the approach, but again am guessing given details are lacking. The exact relation to the Guan et al. 2020 paper, which appears to be important, isn't properly described.* Figure 1: This is very unhelpful. There are so many ways that this figure could be instantiated as an actual model, and no detail is given for how you do so.* Mentioning [cls] and [mask] without any definition. It can be guessed what you are talking about by readers familiar with BERT and some recent NLP standards, but again the description lacks greatly in clarity. Section 4, also called "Rhetorical Text Generation":* I'm not sure what it means to say a poem is a metaphor. A poem can contain several linguistic traits. How is it determined that a poem is a metaphor?Constructing Rhetoric Graph section:* where did the 3422 negative examples come from?* who manually marked the rhetorical sentences to check accuracy?* the accuracies of the classifiers are not very good, and yet they are used to bootstrap the rest of the dataset. I assume the data contains a large amount of errors as a result.* not convinced that "noumenon" is the correct word, but I could be wrong.Generating with Rhetoric Graph section:* "sentence to text generation model" ... again, I'm just left guessing at what was actually precisely done.* "the Top-K", no citation and top-k sampling isn't accurately described.Remaining sections:In full disclosure I have only skim read the remaining sections. I was immediately confused by not seeing any results against the proposed KETG name. summary:This paper proposes a framework, KETG, to introduce knowledge during language generation, aiming to enhance the logicality and diversity of generated texts. It is able to utilize both the source sentence and its associated knowledge graph tuples for training.The authors validate their methods on rhetorical text generation. They first construct a rhetoric graph, then extract context vectors with rhetorical information from it to guide text generation.pros:1.Rhetoric is a stylistic aspect of literature. It is interesting to consider this attribute in natural language generation.cons:1.The writing part of this paper has severe drawbacks.      a.The contents are not well-organized at all, the workflow of KETG is still not clear after reading the paper. The overall structure should be modified to present the main ideas clearly.      b.Some technical details are missing. In Sec.5, Transformer model and GPT-2 are used as the backbone models and the constructed knowledge graph is embedded into them. But there are not enough details for readers to reproduce the experimental results, e.g. number of epochs and other hyperparameters used during training.     c.There are too many typos. Take paragraph 2 in Sec.3 for example.     line 3: input them together with associated sentence to the ---> input them together with the associated sentence to the;       line 5: forcing the model pay more attention to them ---> forcing the model to pay more attention to them;     line6: which then serve as input to the trained model ---> which then serves as input to the trained model 2. This paper aims to introduce knowledge to guide text generation and improve its logicality, but it only evaluates the proposed framework on rhetorical text generation. Actually, there are more representative text generation tasks like dialogue generation, which is also discussed in the related work. More experiments should be included in this work.Question:Could you give more details about knowledge integration in Fig.1? This paper proposes to use a rhetoric knowledge graph for rhetorical text generation. One of its key contributions is to construct a rhetoric knowledge graph by leveraging SOTA NER and relation classification models.  To generate a rhetorical text, the new method starts with sending a keyword to the knowledge graph to retrieve the neighborhood of the keywords as its context words. Both the context words and the original query word are fed into a language model to generate the final word sequence. Pros:+ The idea of using a knowledge graph to generate rhetorical text is interesting.+ There is both qualitative and quantitative analysis of the model performance.Cons:- Overall, the paper is poorly written. A significant amount of details are missing such that I do not believe this work is reproducible.- It is unclear if any datasets are labeled to train NER and relation classification models for knowledge graph construction. What kind of entity types and relations are considered? Which model is actually applied for rhetoric graph construction?- This work is not compared with the existing SOTA models for poetry generation. It would be more convincing if this work is compared with (Liu et al., 2019).Questions:1.  Which transformer model is used as the language model?2. Which top-K generation method is used for text generation?3. How do the authors show that the proposed model can eliminate logical inconsistency?4. Which templates are used for the results in Table 4?5. What do authors make sure the evaluation of artistic aesthetics is consistent and meaningful? This paper presents how to utilize the domain knowledge which might be useful for solving problems in developing a deep neural network with the forms of variable constraints or objective functions. And the proposed method, called 'Monotonic Neural Network', was applied to the application of chiller plant optimization. Compared to the method (MLP), the proposed method showed better performance in the accuracy measure. Though this article shows how the machine learning model can be guided with prior knowledge in the real-world application, it is hard to say that this paper is above the accept line as there are lots of rooms for improvement.Completeness- No conclusion: To make the research more consolidate, authors need to consider putting conclusions of their works including summary, reemphasizing what they found, some key points, or future works. By just finishing with 'Experiments' in this article, readers may wonder what are the main points of arguments from authors.- Explanations for abbreviation: As the paper deals with the application of  'Chiller plant', using technical terms is inevitable(e.g PUE, We-Bulb Temperature, DC, or CHWP). However, authors need to bring the meaning of the terms from the appendix into the main article so that readers can glimpse easily when the unknown terms are shown. Applicability- Its applicability is limited only to the same problem from the same facilities. Authors may consider making their proposed method more general so that practitioners from other industries who want to combine their domain knowledge into their problem can get ways or insights from this study.- No specific information about the proposed model structure found. How many layers(k) are used and why? What is the size of the layer weights? How are hyperparameters set from how many candidates? Superiority- Authors may compare their works to other state-of-the-art approaches to present the superiority of them. Results showing better than MLP may not appeal to readers. Typo/correction- Authors need to check the correctness of sentences. Especially, the first paragraph of Section 2 has several misuses of capital letters. And the fifth line(Although Some... ) is not a complete sentence.- In section 3, Consider -> Considering- In section 2(Third paragraph), state of art -> state of the art This paper presents a dual complementary network framework for graph representation learning. Two graphs representing topology and features respectively are first constructed. Then, two branches leveraging the two graphs are proposed to explore different aspects of the original graph. Finally, a diversity loss is presented to capture the rich information of node features.To me, the overall presentation is barely satisfactory, with many proposals not well-motivated. Also, the novelty is limited given the large body of existing work on exploring dual aspects of graphs. Moreover, the experiments are not convincing.Detailed comments:* The reason why two branches of GConv nets are used is not clear. I am especially not clear how the proposed DGCN differs from dual-channeled GAT, why the diversity loss is not employed on attention heads, and how does the embedding learnt by GCN supplement the information of that by GAT. More elaborations are needed.* Experiments are not convincing; the result analysis of this paper is rather superficial.  * Since DGCN uses two branches of GConv nets, large-scale datasets are necessary to evaluate the performance and efficiency.  * Inconsistency between GCN and kNN-GCN. It seems that on UAI2010, BlogCatelog, and Flickr kNN-GCN is significantly better than GCN, but the opposite holds for the other two datasets. It should be noted why the two methods show such different performance on different datasets.* Given the large amount of existing literature regarding dual networks, many related methods are missing. The authors should especially pay attention to network embedding techniques, e.g., [1].Minor:* Mathematical expressions are in chaotic forms, which makes the readability poor.* Page 5: CDAN -> DGCN?[1]Z. Meng, S. Liang, H. Bao, and X. Zhang, Co-Embedding Attributed Networks, in WSDM, 2019, pp. 393401. The findings of this work are that for contrastive learning, most of the negatives deemed easily separable are unnecessary, the most important negatives are somewhere in the top 5% closest to the positive sample, and that some of the exceedingly hard examples are detrimental.-In general, I felt the main findings of this work to be roughly in line with what we already know about contrastive learning. We can easily look at this work's findings with respect to the soft SVM margin, in that only the examples close to the decision boundary should matter (max margin), but some difficult examples  (the aforementioned exceedingly difficult ones) make the data inseparable, so we allow some violation (slack terms).  While I'm not suggesting that slapping a soft SVM here would solve the problem, there is a large body of SVM-based detection/classification literature that precedes the findings of this work.-Validity of WordNet as a measure of semantic similarity: Section 4 uses WordNet distances to estimate the semantic similarities between classes by finding their shared subtree root. The deeper the subtree, the more semantically similar. While I do not dispute the claim of the hardest negatives being from semantically similar classes. Different parts of the WordNet synset tree have semantic hierarchies of varying levels of coarseness. A 2 hop distance in one subtree could easily be more of a semantic jump than a 3 hop distance in another. -The exist prior works dealing with the neglected semantic hierarchies in ImageNet by setting up hierarchical classifiers. An example is [1].-I would further argue that there's some nuance in the correlation between semantic similarity and example hardness, in that it really depends on your choice of feature representation. Visual features will naturally correlate with closer semantic levels in visually-defined categories. However, this will not necessarily hold for semantic categories defined by function, in that two visually distinct items may fall under close semantic labels. -The related works section claims object detection works have not "explicitly involved negative examples as in CID." I have to imagine this statement is poorly phrased, as [2] (also cited in this paragraph) very explicitly mines for  face-like non-face patterns. There is a very long list of hard-negative mining works in object detection.Overall, I value the empirical impact of this work, in that the rather detailed analysis may lead to improvements to future versions of the contrastive feature learning task. However, I do not find the findings of this work to be sufficiently novel for this conference, and therefore cannot recommend this work for acceptance in its current state.[1] Yan et al. HD-CNN: Hierarchical Deep Convolutional Neural Networksfor Large Scale Visual Recognition. ICCV 2015[2] Sung and Poggio. Example-Based Learning for View-Based Human Face Detection. TPAMI 1998 This paper presents a modification to existing information theoretic feature selection algorithms which adds a strong relevance term estimated using a k-nn MI estimator. It's a slightly expanded copy of a paper published at IEEE International Symposium on Information Theory 2020, referenced as "Exploring unique relevance for mutual information based feature selection" Liu & Motani 2020. This paper contains the same experimental results, same plots, same theoretical description and is in most ways a direct copy of the ISIT paper, violating ICLR's dual submission policy. I think the only new material is the experimental results on the CLF algorithms.The authors should note that GSA-BUR has already been published as the SURI technique (referenced as Liu et al 2018), even considering this paper is a version of the ISIT 2020 paper.The notion of "redundancy rate" is ill-defined, and the experiments which measure it are not discussed. If it's measuring the joint mutual information then it's a measure of the approximation used, and also a factor of the greedy search algorithm (which is used by all the criteria considered in the paper).The proof of proposition 1 follows from the definition and has been known since the 1990s when strong relevance was introduced.Given the CLF variants use a classifier to estimate the probabilities then the authors should validate that the features are still widely useful (by transfering the features found using the SVM to the RF) or compare performance against a wrapper like RFE as it's similarly expensive. This submission just describes some phenomenons in a strange setting but not proposes any valuable questions.  The authors claim that "anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time." However, this paper creates the so-called trigger from the poisoned calssifier. Such a choice is unsual in the adversarial machine larening problem. In most cases, this model is not occupied by the adversary. The authors did not provide some convicing reasons to verify the rationality of this setting. Moreover, the proposed method constructs the alternative triggers by first generating adversarial examples for a smoothed version of the poisoned classifier and then extracting colors or cropped portions of adversarial images. But the motivations of building the  adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper.