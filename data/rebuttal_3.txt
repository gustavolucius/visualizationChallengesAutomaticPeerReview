 ============Post Rebuttal====================After reading the feedback from authors, I still have my concerns. The novelty of this paper is too limited for ICLR. I really do not think a combination of CutMix with existing saliency detection method is a novel method. Moreover, the improvement over CutMix is diminishing. These main concerns are not addressed by the authors. So, my final recommendation is still rejection. ----------- Post-Rebuttal Comments -----------------Thanks to the authors for the response and for updating the draft. Some of my queries were clarified. However, I still think the paper lacks the mathematical rigor required for a theoretical security/privacy paper. For instance, in the updated proof of Theorem 2, the authors consider the output of a pseudo-random generator (PRG) as uniformly random and claim information-theoretic (perfect) security. However, PRG output is not uniformly random, and one needs to consider computational security, which is standard in cryptography/security literature. Moreover, the comparison with (Bonawitz et al., 2017) and (Bell et al., 2020) seems a bit unfair since the threat model in the paper is weaker. As an example, for the same threat model in the paper, it is not clear if (Bell et al., 2020) would need the strong assumption on dropouts. For these reasons, I retain my original score.  Update: Thanks for the authors' response. After reading the response and the other reviewers' comments, I think the paper needs to be further improved, and thus I will keep my score. # Update after discussionThe discussion has reinforced my concerns. I cannot fully follow the logic of the paper and am not convinced the controls are useful. Therefore  I stand by my original assessment. [Post rebuttal]I've done a complete re-read of the updated manuscript. I am not convinced with the efficacy of the proposed method in solving mode drops beyond what has already been achieved in the literature. In particular, there is no guarantee that matching covariances within the feature space of a NN prevents mode dropping, as the NN (discriminator) itself must abstract away visual information to perform discrimination. The quality of writing has not improved significantly either. I am now more confident with my original assessment.  ------------UPDATE AFTER AUTHORS RESPONSE: The authors did not address my concerns. The author response and discussion in the paper for the whitening is wrong: this paper use the whitening to mean $\bf{X}\bf{X}^T=\bf{I}_n$ where n is the data size, but the whitening in machine learning means $\bf{X}^T \bf{X}=\bf{I}_d$ where d is input dimension. This is completely different: the former makes the problem trivial whereas the latter does not. The explanation with the mini-batch is also wrong: optimization problem and the optimal solution are defined for the full dataset, and not for the mini-batch. For the memorization, I am not talking about label memorization, but the fact that we can set the weight matrix to have the direction of Y at the first layer, which is done in this paper. In Figure 4, for MNIST, it has only 60% test accuracy. For CIFAR10, it has only 18% test accuracy. This demonstrates my points above. We know closed-form solutions easily for deep neural networks in this paper's setting (as explained above), but this should not work as it is using very strange models so that we can easily have closed-form solutions. Closed-form solutions have no value with 18% test accuracy for simple datasets. It is memorizing the direction of the training data and over-fitting a lot. Linear models work better. 6.  I noticed that there is a recently published NeurIPS paper (Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis), which further reduces the merit of the paper on the perspective of adapting SVR techniques to nonstandard stochastic optimization algorithms such as TDC/Greedy-GQ.  Post-discussion update: I thank the authors for their participation in the discussion. Unfortunately I find it mostly has not cleared the numerous question marks I have regarding the paper. I recommend putting more effort into clarifying the mathematical derivations and into positioning the paper correctly w.r.t. prior work on the topic. UPDATE:In the rebuttal, the authors emphasize two facts:1. There exist such IoT scenarios and real-time systems that employ multi-exit architectures (including those that employ cloud computation.).2. The slowdown attack is effective in these scenarios.However, to prove this method works in practice, it is not a simple "1 then 2", you need to show us "1 and 2". That is, you do actually deploy any system described in [1,2,3,4,5,6,7,8], and provide a feasible approach to attack with your method, and report the actual damage caused by your method, and convince the readers the damage is significantly severe compared to the efforts spent for causing the damage. Otherwise, it is only an application of PGD with a different loss function. After rebuttal === I thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score.   ==== After rebuttal === I thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score. ==================== ======== after discussion phase =======The main drawbacks of the paper remain after the discussion. Mainly, the analysis is too simplistic which basically ignores the new aspects of the algorithm and its comparison with well-known methods is unclear. Therefore, I keep my score. # Post-discussion update The authors have significantly updated the paper during the discussion period. I have seen the changes, but they unfortunately do not substantiate the claim that the proposed methods are learning anything meaningful. In the new results in table 1, the train accuracy now is better than random, but the test accuracy on the unseen environments is worse than a model that makes uniformly random predictions. This implies that the proposed method is not learning to ignore the spurious features at all. It might seem from table 1 that C-VIRMv1 is performing well --- it has 46% accuracy on the test environment after all. However, C-VIRMv1 also has the worse performance on the train set (50% accuracy). In-fact, the test or train performance alone does not mean anything in this benchmark. The goal of the benchmark is to do well on the train set while also generalizing to an unseen environment. Doing well on test set by doing poorly on the training set is not progress. The new results in table 3 are equally troubling. The authors claim that the results in table 3 show that C-VIRM with EIIL is performing better than IRM with EIIL, but the data does not support the claim. Both IRM and C-VIRM are performing similarly; all the results are with-in error margins of each others, and the table can not be used to make any claims. I would encourage the authors to do a more systematic study of the proposed method. Investigate if the proposed methods are learning to ignore the spurious correlation at all (Table 1 suggests they are not) and only make claims that are supported by the data. In its current form, I cannot recommend this paper for acceptance.  -----------------------Edit after rebuttal: Thank you for the rebuttal and clarifying some of my questions. I have decided to keep the original score. ---## Post rebuttalDuring the rebuttal, the authors failed to handle the issues that I raised. Especially, the authors did not respond to my criticisms about the strange experiments. Therefore, I stick to my initial rating. ----------------------------------------------------------------After rebuttal: the paper seems interesting but, as I already mentioned and as other reviewers pointed out, the main concerns about this paper are novelty and relevance to the ML community. I have read the author's comments and I stand with my previous rating. I believe the paper addresses an interesting problem but lacks sufficient analysis due to the realizability assumption A2 which doesn't apply for the given problem and the other reviewers feel the same way. Unlike in online learning and MAB, the memory used by the proposed Neural-linear bandit significantly deviates from the A2 assumption. These should have been analyzed empirically or theoretically to understand the impact of the past history on the regret.  _**Update after author response:** I have read the author response, but do not find that the answers really address my concerns.  I have also not really seen any improvements in the paper itself._  ### Addendum ###After an in-depth discussion with the authors (see below), my opinion on the paper has not changed. All of my major criticisms remain: (1) There are far easier ways of achieving f(x) ~ x than propositions 4/5/7, i.e. we simply have to choose \phi(x) approximately linear. (2) The experiments are too narrow, and learning rates are badly chosen. (3) The authors do not discuss the fact that as f(x) gets too close to x, performance actually degrades as \phi(x) gets too close to a linear function. (Many other criticisms also remain.)The one criticism that the authors disputed until the end of the discussion is criticism (1). Their argument seems to hinge on the fact that their paper provides a path to construct activation function that avoid "structural vanishing gradients", which they claim 'tanh' suffers from. While they acknowledge that tanh does not necessarily suffer from "regular" vanishing gradients (as shown by "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice" and "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks"), they claim it suffers from structural vanishing gradients. I do not believe that there is such a thing as structural vanishing gradients. However, even if such a concept did exist, it falls on the the authors to provide a clear definition / explanation, which they neither do in the paper nor the rebuttal. *EDIT: I thank the authors for a careful point-by-point comparison of our disagreements on this paper so that we may continue the discussion. However, none of the points I identified were addressed, and so I maintain my original score and urge against publication. In their rebuttal, the authors have defended errors and misrepresentations in the original submission, and so I provide a detailed response to each of the numbered issues below:(1) I acknowledge that it is common to set c=1 in experiments. This is not the same as the misstatements I cited, verbatim, in the paper that suggest this is required for variance reduction. My aim in identifying these mistakes is not to shame the authors (they appear to simply be typos) but simply to ensure that future work in this area begins with a correct understanding of the theory. I request again that the authors revise the cited lines that incorrectly state the reliance of a control variate on positive correlation. It is not enough to state that "everyone knows" what is meant when the actual claim is misleading.(2) Without more empirical investigation, the authors' new claim that a strictly state-value-function baseline is a strength rather than a weakness cannot be evaluated. This may be the case, and I would welcome some set of experiments that establish this empirical claim by comparing against state-action-dependent baselines. The authors appear to believe that state-action-dependent baselines are never effective in reducing variance, and this is perhaps the central error in the paper that should be addressed. See response (3). Were the authors to fix this, they would necessarily compare against state-action-dependent baselines, which would be of great value for the community at large in settling this open issue.(3) Action-dependent baselines have not been shown to be ineffective. I wish to strongly emphasize that this is not the conclusion of the Mirage paper, and the claim repeated in the authors' response (3) has not been validated empirically or analytically, and does not represent the state of variance reduction in reinforcement learning as of this note. I repeat a few key arguments from the Mirage paper in an attempt to dispel the authors' repeated misinterpretation of the paper.The variance of the policy gradient estimator, subject to a baseline "phi," is decomposed using the Law of Total Variance in Eq (3) of the Mirage paper. This decomposition identifies a non-zero contribution from "phi(a,s)", the (adaptive or non-adaptive) baseline. The Mirage paper analyzes under what conditions such a contribution is expected to be non-negligible. Quoting from the paper:"We expect this to be the case when single actions have a large effect on the overall discountedreturn (e.g., in a Cliffworld domain, where a single action could cause the agent to fall of the cliff and suffer a large negative reward)."Please see Sec. 3, "Policy Gradient Variance Decomposition" of the Mirage paper for further details.The Mirage paper does indeed cast reasonable doubt on subsets of a few papers' experiments, and shows that the strong claim, mistakenly made by these papers, that state-action-dependence is always required for an adaptive control variate to reduce variance over state dependence, is not true. It should be clear from the discussion of the paper to this point that this does _not_ imply the even stronger claim in "A Better Second Order Baseline" that action dependence is never effective and should no longer be considered as a means to reduce variance from a practitioner's point of view. Such a misinterpretation should not be legitimized through publication, as it will muddy the waters in future research. I again urge the authors to remove this mistake from the paper.(4) I acknowledge the efforts of the authors to ensure that adequate background is provided for readers. This is a thorny issue, and it is difficult to balance in any work. Since this material represents a sizeable chunk of the paper and is nearly identical to existing published work, it leads me to lower the score for novelty of contribution simply by that fact. Perhaps the authors could have considered placing the extensive background materials in the appendix and instead summarizing them briefly in the body of the paper, leaving more room for discussion and experimental validation beyond the synthetic cases already studied in the DiCE paper.(5), (6) In my review I provided specific, objective criteria by which I have assessed the novelty of this paper: the lack of original written material, and the nearly identical experiments to the DiCE paper. As I noted in response (4) above, this reduces space for further analysis and experimentation. Update: I still feel that the paper should have either strong theory, strong experiments, or some of each to be accepted, but that both are lacking. The revisions required would be too great for acceptance at this time.  +++ updates after authors' feedback +++I  thank the reviewers for their detailed response. I still feel that more work (experimental and theoretical, as outlined in my review) is needed. I also would like to make sure that my point is not misunderstood when I said that the sparse model requires twice as many parameters to be stored (the value and the index),  compared to the dense model. Using compression algorithms, the number of bits to store the model can obviously be reduced (below the factor of two). Anyways, the deeper point that I wanted to make  was the connection between minimum description length  (number of bits to store the model) in information theory and the model complexity /capacity in statistics/machine learning: see BIC in https://en.wikipedia.org/wiki/Minimum_description_lengthHence, the bits to store a model are directly related to the model-complexity/capacity in machine-learning/statistics. Update: From the perspective of a "broader ML" audience, I cannot recommend acceptance of this paper. The paper does not provide even a clear and concrete problem statement due to which it is difficult for me to appreciate the results. This is the only paper out of all ICLR2019 papers that I have reviewed / read which has such an issue. Of course for the conference, the area chair / program chairs can choose how to weigh the acceptance decisions between interest to the broader ML audience and the audience in the area of the paper. ----------------------------------------------------------------------------------------------------------------------------------  ---I appreciate the authors' effort in responding to my comments. But the arguments in their response appear in conflict. Overall, I am still not convinced by their arguments. So I stay with my original review.  EDIT: Reviewer has considered the response by the authors. Key details of the baseline regressor are missing, such as the exact network structure used. As a result: 1) Reviewer is unable to determine if the baseline is a proper fair comparison. 2) Authors have confirmed the methods reliance on strong shape prior, but this caveat is not clearly mentioned in the paper as a requirement for the method to work. Furthermore, authors did not quantify what affect this reliance has by adding experiments on datasets with weak shape priors mentioned by reviewer. As a result, reviewer is lowering score. Reviewer encourages authors to continue this line of research, but carefully consider the feedback given to make the work stronger before publication.  ------In light of the author's response I am changing my review from 2 to 3.  I still feel as though the paper should be rejected.  While I appreciate that there is a clear history of using GANs to target otherwise intractable objectives, I still feel like those papers are all very explicit about the fact that they are modifying the objective when they do so.  I find this paper confusing and at times erroneous.  The added appendix on the bits back argument for instance I believe is flawed."It first transmits z, which ideally would only require H(z) bits; however, since the code is designedunder p(z), the sender has to pay the penalty of KL(q(z)kp(z)) extra bits"False.  The sender is not trying to send an unconditional latent code, they are trying to send the code for a given image, z \sim q(z|x).  Under usual communication schemes this would be sent via an entropic code designed for the shared prior at the cost of the cross entropy \int q(z|x) \log p(z) and the excess bits would be KL(q(z|x) | p(z)), not Kl(q(z)|p(z)).  The appendix ends with "IAE only minimizes the extra number of bits required for transmitting x, while the VAE minimizes the total number of bits required for the transmission" but the IAE = VAE by Equation (4-6).  They are equivalent, how can one minimizing something the other doesn't?  In general the paper to me reads at times as VAE=IAE but IAE is better.  While it very well might be true that the objective trained in the paper (a joint GAN objective attempting to minimize the Jensen Shannon divergence between both  (1) the joint data density q(z,x) and the aggregated reconstruction density r(z,x) and (2) the aggregated posterior q(z) and the prior p(z)) is better than a VAE (as the experiments themselves suggest), the rhetoric of the paper suggests that the IAE referred to throughout is Equation (6).  Equation 6 is equivalent to a VAE.I think the paper would greatly benefit from a rewriting of the central story.  The paper has a good idea in it, I just feel as though it is not well presented in its current form and worry that if accepted in this form might cause more confusion than clarity.  Combined with what I view as some technical flaws especially in the appendices I still must vote for a rejection. Updates:Author(s) acknowledged that they cannot get a robust analysis. Furthermore, the optimality test also requires a robust analysis. Therefore, I believe the current version is still incomplete so I changed my score. I encourage author(s) to add the robust analysis and submit to the next top machine learning conference.------------------------------------------- ## AFTER REBUTTALFirst of all, I would like to thank the authors for their detailed response: I realize that the authors spent a lot of effort to address most of my comments and questions. I have read other reviews and the responses by authors. However, I still have questions about the paper, preserving me from increasing the score.1. **Assumption 1.** Even in the updated form presented in the rebuttal, Assumption 1 is not mathematically rigorous. How can $\mathbb{E}[||g(x,k)||^2]$ be independent of $x$? Can you provide any non-trivial example? Actually, it is highly relevant to my third concern (**Misleading results**) from the weaknesses part: when we use stepsizes dependent on $m_k$ from the future, we implicitly assume that we can use any stepsizes without changing the sequence {$m_k$} (otherwise $m_k$ should depend on $x_k$). Then, if we consider SGD with arbitrary small stepsize, we will get the method that generates arbitrary close points. In these settings, for the majority of problems, the sequence {$m_k$} should be stationary (since we can take arbitrary small stepsize). But it is not stationary, at least in the experiments presented in the paper. **It is a crucial contradiction that significantly decreases the value of the results given in the paper.** In other words, this assumption **never** holds. I understand that the authors simplified the assumption to handle the non-stationarity of the noise. However, the settings considered in the paper are too simplified: they do not cover any problem.2. **"We could present the paper in terms of $\sigma_k$..."** The paper would significantly benefit from this. Moreover, without resolving the issue mentioned above, it is better to remove the whole part based on the independence of {$m_k$} on $x$.3. **"Moreover, allowing the step size choice to depend on noise level is a standard approach in almost all SGD analysis."** I agree with the authors. Still, the key difference is that typically it is assumed that **the upper bound** for the noise level is known. It significantly differs from the case when {$m_k$} is known.4. **"We have compared SGD and Adam in Figure 4 of the appendix (see Appendix A)."** Unfortunately, I have to disagree: the authors presented the behavior of $m_k$ and $\sigma_k$ for SGD and Adam in separate figures -- Figures 1 and 4. There are 10 pages between them, so there is no transparent comparison. It would be much better to see one figure for both algorithms --- this is what I meant by comparison in my review.5. I still find the proof of Remark 9 to be incomplete: in the proof of the analog of (8) for the case of Remark 9 the authors assumed that $\hat m_{k+1}^2 = \beta \hat m_k^2 + (1-\beta)||g_k||^2$ (in the proof of the last but one inequality), while in this remark it should be $\hat m_{k+1}^p = \beta \hat m_k^p + (1-\beta)||g_k||^p$. Next, what are the assumptions on $p$? Can it be any positive number? If yes, then what is the best choice of $p$? This part requires further discussion and development.6. **About my 19-th question from the list.** I meant that $D^2$ and $M^2$ could be unrelated in general. It would be interesting to see the discussion on how it influences the convergence rate. The current analysis says that the smaller $D$ is, the better the algorithm converges. It should be discussed how it relates to the empirical findings.7. The proofs of Corollaries 5, 6, and 7 were given nor in the rebuttal, neither in the paper. Actually, the authors have not updated the paper at all, although many corrections should be applied (at least grammatical errors and misprints noticed above).To conclude, the most important questions and comments from my review have not been properly addressed by the authors. Therefore, I want to keep my initial score unchanged. **Update after author response:**I went over the author response and have had a chance to carefully evaluate the updated draft. I appreciate that the authors have taken the time to address two of my comments but I believe major concerns still remain unaddressed, so my evaluation remains unchanged.1. I appreciate the time taken to conduct the fastText experiments, would be helpful to show this on BERT (which is arguably your strongest model) as well.2. Per my reading it appears that you've shown how models behave if the replacements are sampled at random. I appreciate the time taken to show that. I believe this could be made better with a concrete discussion of the same.Cons 3, 4, 5, and Additional comments 1,2: I do not think these have been convincingly addressed. You point to Dynabench while suggesting that this would eventually lead to better evaluation sets but as it is shown in [1] and [2], "difficult" evaluation sets that are tied to one model are not necessarily difficult for other models. If the idea is to identify model's vulnerabilities and not use these as common evaluation sets, then this is more or less the same as generating adversarial examples (but called "difficult" examples, in which case this would be better presented as an application/extension of Zhao et al to identify adversarial examples in NLP). If the idea is to only use these examples for evaluation purposes, then the comparison to counterfactually augmented data (which you show does not include "difficult" examples per your definition) does not make sense since that is meant solely for augmenting training sets. Discussion in Section 4 and onwards is shallow and often unclear. That is primarily where I believe the exposition can be improved significantly. For instance, Section 4 says FIM values capture resilience to linguistic perturbations but that has not been discussed in any of the paragraphs that follow. Section 5.1 ends in a sentence that says, "By repeating this process multiple times, more robust classifiers can be created", and that is not discussed any further as to why you think that would be the case. It is also not supported by any theory or empirical results presented in your prior or updated draft.[1] Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering." In TACL.[2] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. "Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension." In TACL.--------------------------------------------------------- ---- Update ----I thank the authors for clarifications. I trust that the suggestions of all reviewers, taken together, provide substantial avenues for improving the work. However, at this point I must keep my score and encourage the authors to continue the work with the valuable honest feedback provided here. Thanks for your response.I have taken a quick look at some sections of the updated paper. Some problematic paragraphs---e.g. the one that I long-quoted in my first review---are verbatim identical to how they appeared in the first version (other than s/"In addition"/"Also"). The few highly relevant references I mentioned have not been added yet, etc. As this is a work in progress, that is no problem; I understand that one would not be able to fix everything in one rebuttal period!My understanding is also that you plan to keep working on all aspects of the project: the writing, the evaluation, and the model itself. Best of luck with all this! Response to authors: After reading the authors' response, I have decided to maintain my original rating. The authors have not adequately addressed my main concerns.Novelty: The work here, as indicated by the authors, is largely an incremental improvement over an existing work MINE. The authors' response did not alleviate this concern and in fact reinforced it.Citations and comparisons to other work: The authors did not agree to even include citations to important literature in this area. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions.Theoretical work: The authors simply pointed to the theoretical work for MINE. However, the theoretical work in MINE is also very weak and only focuses on estimation consistency and not convergence rates (i.e. the statistical bias and variance of the estimator). More theoretical work is needed in this area to justify the use of these estimators over others.Some responses to other comments that may help the authors with further revisions:(1) The presentation of MINE should occur in the main paper as this is crucial for understanding the paper.(2) This was not clear. Perhaps the authors could include similar pointers in the paper with each of these issues.(3) The bias I'm referring to here is the actual statistical bias of the estimator. From Theorem 6, it seems that the drift problem does seem to create some bias but it would be useful to quantify that, which could then lead to a bias correction approach.(7) The way this is currently worded, it sounds like you are saying that training with a larger batch size is bad. This part should be clarified to avoid this.  ** UPDATE after reading other reviews, author responses, and revised paper **I'm unconvinced by the arguments related to the resizing of feature maps between ResNet branches. The added experiment in A.1 does not address the question of misaligning the features between skip branch and convolution branch. Figure 8 suggests that the alignment with input is well preserved with resizing (i.e., Fig. 8(a, d) look similar). The illustration is misleading because it uses a kernel with constant weights and *extremely* oversaturates the colors. In other circumstances, we would see that each corner in Fig. 8(c, d) is different. This is because Fig. 8(b) will contain four identical blobs after the convolution, and Fig. 8(c) is by definition a central crop of Fig. 8(b). Thus, the bits we see in Fig. 8(c) are different quadrants of this blob, and they are in fact all different. This leads to the misalignment between skip and convolution branches, so that subsequent processing will see different data at each of the four corners. The figure makes it look like there are only small differences by oversaturating the colors to the point that only the footprint of the convolution can be seen. Using a non-symmetrical convolution kernel would have also highlighted the differences.The experiment also misses the larger point: The different architectures may cause differences in the results that exceed the effects of the padding itself. The paper claims to measure the latter, but I believe both AR2 and I are concerned that the paper may be measuring the architectural effects instead. As this was my main concern about the experimental design, my confidence in the paper's results is not increased, and my rating is not changed. If anything, I'd be inclined to lower my score because of how misleading the added Fig. 8 is. .Update after Rebuttal:I do appreciate the argument given in appendix A.1 about the relative position that changes with cropping and that boundary information may get lost. However, it is not clear to me if that change in relative position matters because the network could take this into account. However, the issue that the feature maps between the shortcut and residual connection do not align with the bilinear interpolation seems to be much harder take into account for the network. To me it seems an actual example on the ImageNet classification (i.e. Table 2) that shows that the degradation in performance is not due to the resampling/misalignment of the feature maps in the ResNet would be very important. Moreover, a related problem is that it seems plausible that a network could extract position information from the spatially varying misalignment of the feature maps (in the image center there is no misalignment and on the border there is 1px (for 3x3 conv). The amount would reveal the position). Therefore an experiment that shows that this does not happen in practice would also be important. This is the same major concern I share with AR4 even after the rebuttal ------------------------------------------------------------------------------------------------------------------------------------------------Post rebuttal: While I appreciate the authors' comments, they do not fundamentally address my concerns that the paper is too unclear in terms of the meaning of its technical results to merit acceptance. As a concrete example, in their clarification, the authors indicate that they obtain "probabilistic safety guarantees" by checking the Lyapunov condition (5) using sampling. However, at best, sampling can ensure that the function is "approximately" Lypaunov (e.g., using PAC guarantees) -- i.e., satisfies (5) on all but 1-\epsilon of the state space.Unfortunately, an "approximately" Lyapunov function (i.e., satisfies the Lyapunov condition (5) on 1-\epsilon of the state space) provides *zero* safety guarantees (not even probabilistic safety at any confidence level). Intuitively, at each step, the system has a 1-\epsilon chance of exiting a given level set of the Lyapunov function. These errors compound as time progresses; after time horizon T, only 1 - T * \epsilon of the state space is guaranteed to remain in the level set, so eventually the safety guarantee is entirely void.One way to remedy this is if the Lyapunov function is Lipschitz continuous. However, then, the number of samples required would still be exponential in the dimension of the state space. At this point, existing formal methods tools for verifying Lyapunov functions would perform just as well if not better, e.g., see:Soonho Kong, Sicun Gao, Wei Chen, and Edmund Clarke. dReach: ´-Reachability Analysis for Hybrid Systems. 2015.This approach was recently applied to synthesizing NN Lyapunov functions (Chang et al. 2019). My point isn't that the authors' approach is invalid, but that given the current writing it is impossible for me to understand the theoretical properties of their approach.Overall, I think the paper may have some interesting ideas, but I cannot support publishing it in its current state Update: Thanks for the authors' response. After reading the response and the other reviewers' comments, I think the paper needs to be further improved, and thus I will keep my score. --------Updates after rebuttal-----------Since the author did not propose an updated paper and new experiments. I keep my original score.--------------------------------------------------- Post Rebuttal Comments:I thank the authors for their feedback. I have no modification to make to my original review. -------The reviewer appreciates very much, and has read the response letter, which is mostly about clarity though it's not the main concern of the reviewer.  # After discussion and editsI acknowledge that I have read the other reviews and resulting discussions and I have read the relevant changes in the edited text. I have raised my score from 2->3 to reflect that several concerns were alleviated through the edits, but several new concerns (and old concerns) remain. I will summarize below.After the author edits, the issue with convergence and convergence rates appear to have been resolved. I additionally appreciate the much greater clarity in the analytical section. However, I still find the contribution to be borderline at best in terms of novelty of approach and I find that the evidence of applicability is still considerably lacking. The introduction of a regularizer to accelerate GTD methods is itself not novel. The form of the proposed regularizer is novel, however, I find its form to be unintuitive as it punishes making changes to the weights. There are some prior works that motivate this well (i.e. TRPO and other trust-region optimization techniques), but this paper does not appeal to prior works to motivate their regularizer. Instead, I must rely on the empirical study which does not investigate the learning speed of the proposed algorithm compared to baselines. In many cases, the proposed algorithm does not clearly outperform baselines. ---------------------------------------------------------------**UPDATE after author rebuttal**The authors have made substantial improvements to the paper over the rebuttal period, but there is still work to be done to make this paper communicate as clearly as it should for publication. I share the concerns of Reviewer 1 and feel that this paper would benefit from more thought into how to present the ideas and another round of reviews.**Additional Feedback for future revision**The preamble to Section 2 is now quite helpful, but it would be even more helpful if placed prior to Definition 1! Helping the reader to think about the elements of $H$ as random variables and to think of causal factors as determining which environment you end up in would aid understanding of Definition 1.Depending on who you want your audience to be, I suggest considering adding an explanation of the "do" notation. While the sub-community focused on causality may be aware of the notation, the community interested in intrinsic motivation more broadly would probably be interested in this paper but not know the notation.I think that the following sentence could be quite helpful if written in a different way: " In our definition, $h_j$ are causal factors such friction with some particular coefficient of friction, or gravity with acceleration constant $g$ or other" (p. 14). It would be helpful to have the variable (e.g. amount of friction) separated from its value (which might represent the coefficient of friction). I'd like the concept that causal factors are variables that could take on any of a set of values made more explicit. "the outcome of running" (p. 14) is slightly ambiguous (at first I read it that running was the outcome, rather than the experiment. A phrase like "the outcome of the attempt to run" or the outcome of the running experiment" might be clearer.Consider using the singular they in your human example (p. 14). A decent primer: https://apastyle.apa.org/style-grammar-guidelines/grammar/singular-theyAdditional typos found:Let $o_{0:T} \in \mathcal O^T$ denotes  denote (p. 2).$k$ is both the length of the set $H$ and the comparator with $h_j$ (p. 2) Maybe don't use $k$ and $K$ either, since it still gives me the vague sense that they might be related, but I'm pretty sure they're not. Neither CEM nor MDL is written out in full (p. 4 is first use)."as compared to the vanilla CEM planner (Figure??)." (p. 7)"are causal factors such friction"  such as friction (p. 14) -------- after rebuttal --------Thanks to the authors for the response and the updated manuscript. My assessment stays the same, and below are my additional comments.1. About Appendix EThanks for the clarification about the initialization scaling. However, this raises more concern about the significance of the result. In Appendex E, it is shown that the scaling considered in the paper and the NTK scaling lead to the **same** gradient flow dynamics. This suggests that we are actually still in the kernel regime, in contrary to the main motivation and the claims in the paper. (As for the time rescaling issue, it doesn't matter in gradient flow since the difference can be absorbed by rescaling the learning rates.)Appendix E also mentions that several previous papers used a small multiplier $\kappa$ to make the initial network small. The authors claim that this makes the convergence rate slower. I don't think this affects the convergence rate, but it only affects the width requirement (see e.g. [1]). In fact, in stead of using this multiplier, there is another way to make the output zero without changing the NTK and without requiring a larger width, that is to use an anti-symmetric initialization -- see e.g. [2][3][4][5].[1] Arora et al. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks[2] Chizat et al. On lazy training in differentiable programming[3] Hu et al.  Simple and effective regularization methods for training on noisily labeled data with generalization guarantee[4] Bai and Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks[5] Zhang et al. A type of generalization error induced by initialization in deep neural networks2. About the motivating questionsThis paper proposes to answer two questions in the introduction. The first question is "Is the kernel regime, which requires impractical bounds on the network width, necessary to achieve good generalization?" First, I don't think this paper answers this question since the considered regime is still basically the same as the kernel regime. Second, even if it does, this question itself is not valid, since there are numerous previous theoretical works that study generalization outside the kernel regime, in more interesting settings, e.g. [6][7][8][9] and many more (none of which are mentioned in the paper).[6] Allen-Zhu and Li. Backward Feature Correction: How Deep Learning Performs Deep Learning[7] Allen-Zhu and Li. What Can ResNet Learn Efficiently, Going Beyond Kernels?[8] Wei et al. Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel[9] Woodworth et al. Kernel and Rich Regimes in Overparametrized Models.The second main question in the introduction is "Does generalization depends explicitly on acceleration? Or is acceleration required only due to the choosing an initialization outside the good generalization manifold?" I genuinely cannot understand this question.3. In the updated manuscript the authors state "To the best of our knowledge, this is the first non-asymptotic bound regarding the generalization property of wide linear networks under random initialization in the global sense." This is still false (and insignificant) since the stated result is a direct consequence of previous NTK work.4. I certainly understand that understanding deep learning is very challenging so it's a natural step to start with simple models. However I think this paper in its current form has limited significance and has major issues in how it discusses previous work, main motivations and contributions, etc., for reasons described in the review. # After discussion period:I have read all other reviews, resulting conversation, and have read the edits to the paper. After extended conversation with the authors and a deeper investigation into the empirical components of the paper, I find I have further concerns than originally realized in my original review and that many of my original concerns remain. I am lowering my recommendation from a 5 -> 3 to reflect the new concerns; namely the validity of the ablation study as detailed in-depth below. I agree that promoting experiments for real quantum hardware is important. But I don't think the team has yet to create a working platform that could be accepted to ICLR. If the open-sourced platform is the main contribution (rather than a new understanding of how quantum computers could be useful for machine learning problems), then the authors should submit the manuscript after having the open-sourced software available. Furthermore, a lot of the wording should be changed (the current version sounds like they are proposing a new quantum machine learning framework, while they are creating an open-sourced platform). **Update**: Thank you to the authors for addressing the comments and updating the paper. I decrease my rating from 4 to 3 as the original claims of the paper were disproved by the experiments with black-box attacks on CIFAR10 which showed that Neural ODEs offer little advantage over Resnets. I believe that more experiments are needed to demonstrate that Neural ODEs offer robustness advantages. For example, when increasing the computational budget for one step FGSM attack, the accuracy stays the same, which might indicate that Neural ODEs obfuscates the gradient. Experiments with gradient-free attacks also indicate that Neural ODEs obfuscate the gradient (PGD has higher robust accuracy compared to gradient-free attacks). The authors should do an extensive evaluation with stronger attacks (PGD with DLR loss or CW loss with multiple random restarts up to 100-1000, AutoPGD); the study of the gradient obfuscation (confirm that when $\epsilon=1.0$, the attacks can always succeed). Additional ReviewThis paper did NOT handle the non-differentiability and non-linearity very well. We can see this from the following three perspectives:1. Proof idea: the proof of this paper is noisy version of the convergence analysis of  a simple convex problem --it treats the contribution of the non-linearity and non-differentiability as bounded noise.2. The network size is of order n^6.3. Network size requirement is dependent on \lambda_0. 1.Proof idea: The proof is essentially a noisy version of the convergence analysis of a linear regression problem provided in Appendix (at the end of this updated review). The only difference between linear regression and the problem in this paper is the changing patterns due to the non-linearity of ReLU. However, this paper views the changing patterns as noises compared to those unchanging patterns (e.g., S_i v.s. S_i^\perpendicular). The key trick is that if the actual trajectory radius (i.e.,the largest deviation from the initial point) R is much smaller than the desired trajectory radius R (given by a formula), then along the trajectory, the contribution of non-linearity is just O(n^2 R), which is small compared to the contribution of linearity, i.e., -\lambda_0 (shown in proof on page 9). Following the above analysis, if the experiment shows that R is really small compared to R, then the approach of treating non-linearity as noise is fine. However, it is not the case for the problem studied in the experiments (Sec 5, Fig 1). In figure 1, we can easily see that the maximum distance R is O(1), which is far larger than R = c*\lambda_0/n^2 =10^-6 when n=1k. Therefore, the proof idea used in this paper is fundamentally not able to explain the phenomenon shown in the experiment. In fact, to address this issue, authors need to consider significant contribution of non-linearity, instead of just viewing them as noises. 2. The network size is too large. This paper requires O(n^6) neurons, that is 10^18 neurons for n=1000 samples used in the experiment. The theoretical trick to make R&lt; R is to note that R can be bounded by O(1/sqrt{m}) while R is independent of m, thus picking a sufficiently large m can make R very small. In a word, the reason that this paper requires so many neurons because of the inability of properly addressing non-linearity. 3. I found the dependence of the network size on the least eigenvalue funny, although the authors claim this tool is elegant. After authors add Thm 3.1 in the revision, I realize that the dependence on \lambda_0 might come from the fact that authors do NOT handle the issue of non-differentiability. Let us see a simple example. Assume I have a dataset with \lambda_0 = 1. Now I am adding one more data point (x=0_d, y=1) to the dataset. After adding this sample, \lambda_0 clearly becomes 0. It seems I am just adding a constant 1 to the loss function and the gradient descent can also converge to the global min with a linear convergence rate since the constant does NOT contribution to the gradient. However, it seems the proof does NOT work. This is due to the fact that the gradient of the non-differentiable points are NOT well defined. Here is a simple example: h(w)=(y-ReLU(w*x))^2, where x= 0, y =1. By the definition provided in this paper (Eq.4), we can easily see that dh/dw = 1 for any w, even if h(w) = 1 for any w. This means that the constant can provide fake gradient information and make  the maximum distance become infinity, (R=\inf). Therefore, the whole proof collapses. In fact, changing the gradient definition from I{z&gt;=0} to I{z&gt;0} does not address the issue and we can see this from this example w=g(w)=Relu(w)-Relu(-w) has a zero gradient at w=0. In summary, the problem considered in this paper where the size m=O(n^6), maximum distance R= O(1/n^2) is too easy compared to most problems in practice where m=\Theta(n), R=O(1). To address the latter problem, we need a better definition of subgradient and need to analyze the significant contribution of non-linearity and non-differentiability, instead of just viewing them as noises. =================================Appendix===============================The proof basically follows from the convergence analysis of the following linear regression problem (note that u_j is fixed):           \min_{w_1,...,w_m}\sum_{i=1}^{n}(f(x_i;w_1,...,w_m)-y_i)^2 = L(w_1,...,w_m)where   f(x;w_1,...,w_m)=1/\sqrt{m}\sum_{j=1}^{m} a_j*(w_j^T x)*1{u_j^T x&gt;=0}Gradient Descent Algorithm:-Initialization:-For each j=1,...,m: a_j ~ U({-1,1}), u_j~N(0, I)-Fix a_1,...,a_m, u_1,...,u_m-Update:-For t = 1,...,T    w_j(t+1)  = w_j(t) - \eta* \nabla_{w_j}L(w_1,...,w_m) for j=1,..., m.In this problem, since a_j and u_j are fixed, then model f is just a linear model w.r.t. w_js and the above problem is just a simple linear regression problem. Therefore, it is not difficult to prove the linear convergence rate for the gradient descent for the above problem under some mild assumptions.  Note that in this paper, u_j(t)= w_j(t) and are not fixed in iterations, i.e., patterns can change. ========================= [UPDATE] Authors' response to my questions did not change my opinion about the overall quality of the paper. Both theory and writing need a major revision. UPDATE:I have read the authors response and the other reviews.  While the authors have made some improvements, my core criticism remains  the paper does not produce concrete theoretical or empirical results that definitively address the problems described.  In addition, there are many confusing statements throughout the paper.  For instance, the discussion of positive and negative rewards in the introduction does not conform with the rest of the literature on exploration in RL.The authors also seemed to have missed the point of the Kearns &amp; Singh reference.  The authors are right that the older paper is a model-based approach, but the idea is that they too were solving infinite-horizon MDPs with finite trajectories and not introducing a bias.   ** EDIT **The authors added many experiments, tables, figures, sections, and an appendix. The changes are too substantial and the paper looks like a completely new one. The purpose of author rebuttals is to address issues like a reviewers uncertainty about a point, an incorrect assumption, a misconception, or a misunderstanding of a part of the paper, not to revamp the paper completely.The paper was clearly incomplete at the time of its submission, and I still vote for its rejection.   Besides many technical concepts are not really accurate on how they are presented. It needs further attention and improvements.The paper reads more like a review papers than a new research article. I remain to my initial decision. === After rebuttal ===Thank you for your answers. I'm keeping the rating at 3.1)+2). I'm still not convinced that it's fair to claim an improvement of X% for a curriculum that, relying on final weights of a "vanilla" SGD-trainedmodel, converges in _additional_ X% to the 100% of "vanilla" time.3). Fair enough, but the revised draft still reads like examples are sampled from it.I double checked the context of citing (Graves et al, 2017) and believe it's still imprecise as in the original draft. -------------------------------Post-rebuttal-------------------------------Thank you for revising the submission and the clarification in the rebuttal. After reading the rebuttal and other reviews, my main concerns about the novelty and computation cost are still unsolved. Therefore, I will keep my original score.    xxxxxxxxxxxxxxWhile I appreciate the authors' rebuttal and revisions, I still do not see sufficient contribution here worthy of a regular ICLR paper. I have read author response and thank authors for their response. I have decided to keep my initial rating. I am not convinced by the response that the proposed method has a clear advantage over Fixmatch as some of their reported numbers for Fixmatch are very different (worse) from the ones from the original paper. ------After rebuttal: Thank the authors for the response. I think my original concern largely stands and would like to keep my original evaluation of the paper. ===================================================I have read the author response, and I have to downgrade my score to 3 because of the validity of experiments. A minor issue is that the theoretical result is not impressive since the technique is basically the same as ICML'20 paper, except that expectation is taken here which makes things easier.My major concern is about the experimental results. The additionally provided Table 3 makes me question the implementation of the experiments. For example, a=0.005 should be very close to the true label (by Definition 3), but the accuracy is only around 70%. For LeNet on SVHN dataset, the accuracy of training with true label y should approach 90%, e.g. see the report in [Coverage Testing of Deep Learning Models using DatasetCharacterization, 2019]. Indeed, currently best model would give <5% test error. Such a big gap is questionable. The only reason I can think of is that this is a typo and larger 'a' should actually mean more weight on y. Then, the results say that a=1 (corresponds to the true label without smoothing) gives best performance (around 90% which makes sense). However, this actually means that the proposed method is ineffective. In any case, for me the validity of experiments is poor.I'm fairly confident in my evaluation. However, if any reviewer or area chair points out that I made a mistake, please kindly correct me and I will re-evaluate the work. Currently, since the experimental results are questionable for me, I would suggest a rejection of this paper.  ------------After author's response----------------My major concern is about the connection between the universal approximation theorem and the proposed architecture. In the paper, the authors mentioned that " the following universal approximation theorem for DKL implies that this effect can be compensated by adding parallel learners". However, the fact that a multi-output learner is not equivalent to M different single-output learners makes it hard to justify the proposed architecture theoretically from the universal approximation theorem. I think this is something that is crucial to be justified, otherwise, the theory does not really match with the proposed method. ================================= Update ===================================I slightly raise my rating, as everything is correct and well organized. However I'm still not sure if this is enough contribution or just incremental compared to the existing computation of proximal gd. I'd leave it to other reviewers. I insist (update): band-limited is a very very inappropriate terminology for convnets. I am waiting for author's' answers (need for clarifications): the paper needs to be revised. **Post-RebuttalI appreciate the authors taking the time to respond. Unfortunately, my belief that this paper is not strong enough from the deep RL perspective to warrant acceptance has not changed. If other combinations of tools do not work, then the authors should improve this justification, with ablation studies or stronger theoretical motivation. Ill add that my score is not influenced by my concern that this paper may not be a good fit for ICLR (I leave that choice to the AC). ********************Edit: responses after rebuttal:> The class of PVFs are developed to be applied and evaluated in the online learning setting.This is not true. See for instance off-line and zero-shot learning experiments with PVFs that can learn new policies which do not interact with the environment and still show generalization.> We think this may also be a reason to the insignificant improvement compared with their baselines in their experiments (their Figure 2), even the inferior results in simple tasks like InvertedPendulum and CartPole. In contrast, our proposed policy representations enable PeVFA to be compatible with normal-scale policy networks and we then demonstrate the superiority of PPO-PeVFA against PPO in standard continuous control task of OpenAI GymI would strongly discourage you from claiming this. PPO-PeVFA is built on top of PPO, so it is very hard for your algorithm to perform worse than the baseline. On the other hand, PVFs and PENs are novel algorithms that rely COMPLETELY on the prediction of the value function. It is expected that they might outperform baselines in some environments and be comparable or worse in others. The PSSVF proposed in the work on PVFs is outperforming the baseline ARS (Mania et al. 2018) in all environments but Reacher, even when the policy is a neural net with 2 layers and 64 neurons per layer.About Q2:1. If the authors are using the $L_{\infty}$ norm, then the results practically apply only to finite state MDPs. Indeed, this is the case for the papers [2,3,4] cited by the authors in the comment above. However, the experiments proposed in the paper deal with continuous state spaces, so the theoretical results are disconnected.It is still not clear what the assumption in Corollary 2 means and if that assumption is met in practice. If the authors cannot justify the assumption, the Corollary should be removed. The novelty of Theorem 1 is not obvious, since it seems to be a quite trivial result from Mathematical Analysis.> The results show that PeVFA consistently shows lower losses (i.e., closer to approximation target) across all tasks than convention VFA before and after policy evaluation along policy improvement path, which demonstrates Corollary 2Again, is the assumption of Corollary 2 met here? If you have a logical rule (or corollary) of the form: assumption -> consequence, and you show that the consequence holds, this does not imply that he assumption is true!One main argument in favor of PVFs is that even with a much bigger policy, PVFs are able to outperform the results of PENs. This suggests that Raw Policy Representation (RPR) can be a strong baseline and any work trying to introduce a different policy representation should at least compare to RPR. Without such a baseline it is difficult to assess the benefits of the proposed policy representations. Note that PeVFAs are PVFs! The PVF formalism already accounts for all kinds of policy representations.Of course, all of these relations should also be clarified in title and abstract.(Mania et al. 2018): Horia Mania, Aurelia Guy, and Benjamin Recht (2018).  Simple random search of static linear policies is competitive for reinforcement learning.  In Advances in Neural Information Processing Systems,pp. 1800-1809, 2018. .**Update**: After reading the rebuttal, I am maintaining my previous score. I believe that, given the related work reviewers have mentioned, the paper most likely requires some experimental comparisons with these other methods. The rebuttal arguments are good, but not convincing enough that I can believe in this method's superiority without seeing a comparison with, e.g., CaCE After rebuttal: no rebuttal, so I will keep my score.----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ############################################AFTER THE REBUTTALI have decided to keep my score unchanged. The authors include a new "road navigation experiment" in Section 5.2. However, I have several concerns about it:* This is a synthetic experiment that the authors have created themselves. Therefore, it does not address my concern on seeing actual real-world problems that can be solved with this approach.* The results of GP-NC (the proposed approach) and GP are very similar. I don't really see that the predictive mean of GP-NC avoids the "negative samples" more than the standard GP.* In fact, the design of this experiment is not clear to me. They say at the beginning that they use the present location ($x,y$) to predict the next location ($\hat x, \hat y)$. How do they model these two outputs? How do they obtain the plotted error bars for this model? This is not sufficiently explained. My impression is that they are just fitting a standard 1-dimensional GP being the input the x-dimension and the output the y-dimension.  During the rebuttal, I concluded that this submission is highly confusing, rather misleading. I was led to believe the authors are in fact talking 'pure 16b MAC' - meaning 16b FP multiplies and 16b accumulate. After reading their responses to R4, I now learnt that they in fact are using 32b accumulates as is already standard practice in BF16 - If so, they have little or nothing new to offer. Rounding discussions the paper focuses on become highly secondary. BF16 is already well-understood and accepted. Their writeup was highly misleading to say the least. I change my rating based on this. They use a representative set of benchmarks which include, CNN-based Resnet, recommendation proxy DLRM, and NLP proxy BERT. Novelty is limited, but critical: They reiterate prior observations that accurate weight updates are more critical for maintaining accuracy than forward-backward gradient updates. Former is what suffers when round-to-nearest cancels out small updates. Stochastic rounding has also been published before and shown to still miss the accuracy mark with >0.,1% accuracy gap for some important benchmarks. Novel part of this work is suggesting a Kahan summation based software fix on top of stochastic rounding to overcome the accuracy loss.Primary issue: This is a very confusing and misleading writeup. Authors should have clearly spelt out that they are in fact talking about BF16 - which is well-publicized for years now as 16b mule and 32b accumulate - and not labelled it as 'pure 16b MAC' - which is already known to exist as well, and proven to be not sufficient for DL training. In light of this, this work has very little value-add. I change my rating now to 'clear reject' for their misleading writing style. --- Update after author response period ---Thank you for the clarifications! After reading the other reviews and the paper updates, I still feel that the paper requires an additional, empirical evaluation to prove the value and contribution of the approach. Otherwise I find it hard to tell to what extent / in which situations / for which user group the tool is useful. While I appreciate that the authors made changes to their manuscript based on the reviewers' comments, I thus keep my recommendation for rejection for the current version of the paper. ### Recommendation after Author Response ###I have read the authors response and appreciate the clarification by the authors. I was aware that other attacks (FFGSM, RFGSM, RPGD), were in the supplementary material. However, I don't see any reason for focusing on the particularly weak FGSM in the main paper. The main evaluation should always be based on the strongest not the weakest attack. Moreover, also RPGD was used as a transfer attack and as such is quite weak. In summary, I keep my recommendation of rejecting the paper. Update: I have read the author's response and decided to keep my review, confidence, and score. ---- **Post rebuttal**I thank the authors and other reviewers for their comments and discussion. While the direction the authors pursue is of unquestionable merit, I remain unconvinced that the work as it stands is sufficiently impactful for this venue.  **Update**: After reading the rebuttal and revised paper, I am keeping my score the same. There is a lot in this paper I really like -- an important prediction target and race analysis among them -- but still have concerns about clinical utility. In standard photoplethysmography, the causal graph is PaO2-->light absorbance, which leads to a clear story of what a fingertip SpO2 sensor is doing (modeling an imperfect but mostly unidirectional relationship). The causal graph in this paper is less clear but probably has some kind of cycle like ventilation-->SpO2-->ventilation, which is  a much more challenging story. I think there are two good routes this paper can take: (1) focus on how to clinically justify a model with such a potentially complicated causal graph; is the model learning the vent->O2 relationship, O2->vent, or some combination of the two? How can we be confident we know that? To which scenarios will and won't a model that has learned such relationships generalize? (The rebuttal has analysis that is a good start, but I think not enough to fully answer these questions) Or (2) acknowledge that the scope will necessary be limited when causal structure is so complicated and poorly understood, and actually limit the tasks the model is capable performing to those on which we can be confident it will do well. I think either (1) or (2) would involve major changes to the structure and goals of the paper, and probably require new experiments.  [After rebuttal]  Thanks for the clarification, the aim of the authors becomes clearer. However I still think the paper requires more work before publication. Your definition of graph mapping is still unclear. From your definition, it looks like the only dependence of $\Psi$ with respect to the graphs $G_t$ and $G_{t+1}$ are through their number of vertices N and M: " bijective mappingÈ:{1,...,M+N}  {1,...,M+N}with  the  additional  restriction  that  for  the  setInsÈ:={jdN|È1(j)> M}we  obtainÈ1(InsÈ) ={M+ 1,...,M+|InsÈ|}"How can this mapping be related to the edit distance of $G_t$ and $G_{t+1}$.After reading the other reviews, I think the authors should clarify if their gaph mapping is related to the standard graph matching pb see https://en.wikipedia.org/wiki/Graph_matching. ------Thank the authors for the responses. While I agree with some of the points made by the authors (e.g. understanding the base case is interesting), I am still concerned about the significance of the result. Therefore I would like to keep my original evaluation. ----------### Feedback after discussionI would like to clarify my thought here. Recall that using the weight decay is equivalent to adding the L2 regularization to the training objective. An important observation here is that the addition of the L2 regularization to the proposed objective will make the global optima non-trivial (apart from the trivial ones I raised), and there might be a hope that the new global optima has some useful properties. What this observation indicates is that the use of the L2 regularization (or weight decay) is an essential factor for the proposed method to output something meaningful. This fact also implies that the analysis of the objective function alone (without the regularization, in Section3) is no longer meaningful. Moreover, because the L2 regularization (or weight decay) is an essential factor, the tuning of its weight should have a major impact to the resulting model. I therefore think it will be important to investigate the effect of such a weight in the experiments, instead of just using a standard weight. Update after rebuttal period================ The connection between the contrastive learning objective and discriminative learning is made via "resemblance". And the author claims the "resemblance" as a theoretical contribution, which the first reason I vote for a clear rejection. This issue has not been addressed by the authors. The second reason for my rejection of the paper is the paper requires an effort to make it self-contained, especially for the experimental section. I remain my score of clear rejection.  ==================== Post rebuttal ====================I thank the authors for their response and additional experiments. Please see my comments below.Relation to [1] The first version of [1] appeared on arxiv in February. I am not sure if it has been published since then or not, but regardless it is sufficiently ahead of the ICLR submission deadline to consider it a prior work. [1] provides a Mapper optimization algorithm in Figure 4. There are some minor differences with your algorithm, but I don't see how they make your algorithm more communication efficient. Generalization analysis in [1] is for the mixing parameter learned from data (i.e. adaptive), that is why there is no dependency on it. Your analysis is for a fixed mixing parameter, but since it is not possible to know it in advance, learning it from data seems to be more reasonable. So I am not sure what is the advantage of a theorem with explicit dependence on it. I agree that the convergence analysis is new, but [1] also has two more algorithms.Experiments Despite that your paper has more experiments, EMNIST experiment in [1] is better suited for studying personalization in my opinion. EMNIST results in Appendix B in the submission are for digits only (and also use fewer clients, but that is less important). One of the reasons that a centrally trained model on EMNIST performs worse than personalized models is the shift in the distribution of characters and digits across clients. This is not the case for MNIST/CIFAR10: if I train a neural net using all of MNIST train data it will easily achieve 99+ average test accuracy (without any personalization). How the test data is split across clients does not matter because accuracy on each digit is roughly the same. That is what I meant by a "single global model with good performance (i.e. trained on the full dataset)".Having worked on FL with personalization myself, I've noticed that it is quite hard to achieve a meaningful improvement over the vanilla FedAvg + fine-tuning. This is also evident from Table 2 of [1], where none of their algorithms offer a convincing improvement. This paper claims that an algorithm very similar to Mapper [1] indeed outperforms FedAvg + fine-tuning. I'd be happy if it is so, but I do not find the provided experimental evidence sufficient. I recommend reproducing the EMNIST experiment of [1] (please don't discard characters, use the same number of clients per communication, etc.). If your algorithm can achieve 91+ accuracy, I'd consider it an improvement. In that case, a more detailed discussion of the differences with Mapper [1] that enabled the improvement would also be great.Regarding the number of parameters, based on eq. (1), personalized model is a convex combination of two models. So if I understood correctly, the number of parameters is increased both during training and testing. Post-review comments:After reading the reply I decided I will keep my low score though it hurts to do so for a paper into which the authors definitely invested a lot of energy. Here is the reason why:I think there are usually two ways in which a paper can make an important contribution: Through a new insight or through a new piece of modelling that will be widely used afterwards. I think in it's current form the paper presents neither.The potential insight I see is proposal neglect. However even with the added experiments in Table 3 I find the results neither sufficient to prove the effect exists. The GT boxes are already added so overlooking objects while fine-tuning should not be a big issue. If it was it should have a bigger impact (improvements in Table 3 and on general performance are minimal).So what about a widely usable piece of modelling? Despite the anecdotal motivation the presented method improves performance. So the question becomes: Will this be widely used? My prediction is that it won't. The improvement is rather incremental but the effort to use it is high. While the model is simple it comes with three additional hyperparameters which the authors tune individually for each experiment (Section 4, Hyperarameters VOC & COCO). This is the biggest issue I have with the method: It requires tuning a bunch of hyperparameters to achieve a marginal improvement.Compare this to TFA [1], the method the paper builds upon. TFA is built upon a very simple insight (fine-tuning the heads works better than comparing representations) and because the insight and model are easy to use they will be the basis for a number of follow-up works (e.g. this paper). I cannot see the same happen with the presented method as long as hyperparameters have to be tuned for each dataset and split. I am sorry but in my eyes this is bad practice!.To me this means the method will likely be of no lasting value. While it is SotA in the one-shot case for now I think the insights and methods used for achieving this performance cannot be used by other groups to improve performance even more. So what can I recommend the authors to do with the paper? I think there are two ways to increase the papers contribution: 1. Study different effects and problems of RPNs in few-shot object detection. A better understanding will for sure help moving the problem forward. Even if the end result is: The RPN works surprisingly well. That would be a great insight as well in my eyes as it would free resources to address the other problems.2. Improve the method so it provides a significant gain without any additional hyperparameter tuning. At only 3% more computing time using it would probably be a no-brainer if it was not for the hyperparameter tuning.[1] Wanget. al ICML 2020, "Frustratingly simple few-shot object detection"------------------------------- **Update**My assessment remains unchanged. See the comments for details.The proposed approach is circular---it tries to sample points at a "sufficient distance" from the data, then tries to learn this "sufficient distance" by predicting the points. This circular dependency should be broken somehow. The current paper set this distance arbitrarily by sampling from a Gaussian with covariance $I$. **POST-DISCUSSION SUMMARY**I want to thank the authors for answering my questions and correcting misunderstandings and updating the paper. I still recommend rejection of this work given that the novelty is not yet fully clear. While the updated list of contributions is indeed a better match for this work, claiming that correct inductive biases improve generalization does not seem to be a new insight. As mentioned in my initial review, I still believe that the general line of work has a lot of potential and want to encourage the authors to fully address the general concerns raised in the reviews and resubmit the work. %% post-rebuttal %%Though I appreciate the efforts of the authors to clarify their methodology and assumptions in their answer, these clarifications (which I still don't fully grasp or agree with) have not been reflected in the revised version. This work still needs a significant revision and, in my opinion, cannot be accepted in its current form.%%%%%%%%%%%% ==Post Rebuttal Response:I read the rebuttal, and unfortunately it didn't address most of my pressing concerns. I appreciate the authors' efforts to add new experiments on other datasets. However, in my opinion these new datasets are not very relevant for the action recognition community, i.e. they are small, and they are rarely used to compare the effectiveness of a particular model. In my initial review, I listed a few datasets that are most commonly used for action recognition comparisons. In my view, without comparisons on these more popular datasets it is very difficult to tell the real value of the proposed approach. If the authors could demonstrate close to state-of-the-art performance on those datasets I would be more convinced that the proposed approach is effective. Currently, most of the comparison are done w.r.t baselines that are implemented by the authors which is insufficient in my opinion. Therefore, I stand by my original recommendation of rejecting the paper. Update:While some of my points have been addressed and the quality of the paper has been improved, my main concern, that the experiments do not support the central claim.  The authors argue that because their method has a smoother loss landscape then similar methods, it performs better.  In the updated paper, the evidence that the method performs better than similar methods has been made clearer, but the improvement is is still marginal.  The more pressing matter however is that the conclusion that the cause of the improvement is from a change in the loss landscape smoothness is based only on a qualitative comparison of only five methods (as the benefit is not consistent between methods in any category).  This is enough to at best demonstrate a weak correlation, but not enough to demonstrate causation.  >> commented after the rebuttals:Thanks for the answers. However, I think the paper still suffers from too many basic issues (still standing from the review). Main one is that it is not clear what the current definition is and what it achieves that previous definitions miss. There are informal texts, but they are not coherent formal and verifiable. ---### After rebuttal:After reading the rebuttal, I feel my main concern is still not addressed by the response. The authors agree that they may provide a different kind of guarantee for the certificates as in (Cohen et al., Salman et al.). An empirical comparison between the output of the proposed model and the mean from sampling is not sufficient. We hope the authors can improve on this point and provide formal robustness guarantees like those in (Cohen et al., Salman et al.). =====POST-REBUTTAL COMMENTS======== I thank the authors for the response and the efforts in the updated draft. Some of my queries were clarified, particularly concerning missing experimental details. However, unfortunately, I still think more needs to be clarified in the actual paper write up, notably on the points of non-adversarial as well as the method description. ------------ UpdateThank you for the clear reply. Unfortunately, I remain concerned about the significance of experiments. I mentioned above that 4 or 5 runs is typically not enough, and because the standard errors are overlapping, the differences could be due to chance. The addition of a result with 10 runs is a good step. But, as part of the reply, the authors state: "Figure 3(a) shows the learning curves of all methods on the SlimHumanoid-ET environment over 10 random seeds. First, one can not that SUNRISE with random weights (red curve) is worse than SUNRISE with the proposed weighted Bellman backups (blue curve). Additionally, even without UCB exploration, SUNRISE with the proposed weighted Bellman backups (purple curve) outperforms all baselines. This implies that the proposed weighted Bellman backups can handle the error propagation effectively even though there is a large noise in reward function." However, if you look at this figure, the error bars all still overlap. 10 random seeds is still not enough. I am also not confident that the issue will be remedied, as the authors additionally state in the rebuttal: "we believe that SUNRISE is evaluated in a broad collection of domains in the RL literature and the performance gap is also noticeable." An insignificant gap across many domains does not tell us anything. Actually, if you take the runs and tried to do significance tests by pooling all the runs across environments, then maybe the result might actually be significant. But, of course, there will be higher variance due to differences in the environments, so it is not obvious this would be true. Nonetheless, this could be a natural next step. ------ After rebuttal: I am lowering my score for the paper. I am not convinced by the responses to several of my questions, in particular to what I felt was an exaggerated insistence on the paper being about  Variational Transformers, the rather artificial connection to ELBO (noted by several reviewers), and the lack of self-contained description in the paper of the actual technique used, MC-Dropout, which might be explained in simple and sufficient terms on its own. Also, I am disappointed that the authors did not update in any way their submission to reflect the reviewers comments (contrarily to misleading expressions in the rebuttals). It is therefore impossible to know whether such unconvincing claims as that made in the conclusion With the new tools above & would be maintained in the final version. ---## Post rebuttalIn response to my doubts about a unified framework, the authors claimed that their *theory* could *predict* an algorithm or hyperparameter's performance.Since there was no description of the theory, I assumed that the theory is:> If an algorithm is different from AF, its performance is expected to be poor.And the authors refuted my interpretation:> We claim that SI and MAS work because they are similar to AF.The authors claim that the theory somehow applies to SI and MAS but not others. However, I could not find any description of why the applicability is restricted and to what extent it is applicable. I think these are vital parts of a proper theory. Without them, a theory is useless since we cannot decide whether it applies to a new CL algorithm until we actually run some tests.Also, I want to emphasize that association is not causation. The authors should have claimed, "SI and MAS work, **and** they are similar to AF," instead of "SI and MAS work **because** they are similar to AF."Even after the discussion with other reviewers, my concerns are not resolved.Therefore, I retain my initial rating. ====== POST-RESPONSE UPDATE ======I appreciate the author's response and the additional illustrations provided. At the same time, my concerns remain:- I still disagree with the claim that PCA directions are "semantic and meaningful." Yes, some of them might correspond to image changes that are intuitive, e.g., Figure 4, but it is still impossible to draw any such conclusions without manually inspecting individual directions. In other words, what do I learn about my model by reading Table 1?- I understand the process of counting the number of images. However, I still think that it is a fundamentally flawed metric. Based on this definition of "distinct", if I change the value of a single pixel by 1/255 I get a distinct image. This is clearly not an intuitive behavior. As a model designer, what do I understand about my model by look at these astronomical numbers.Overall, while I still find the broad direction interesting, I believe that the paper has fundamental issues and is hence unsuitable for publication. Edit:### RebuttalI did read the authors' rebuttal, and the main issue, i.e. the significance, has not been addressed. I cannot take into account the new experiments, since they are not in the paper. Anyway, an experiment with a 2-layer network would be a significant modification of the present paper, which would be not acceptable during the rebuttal phase. ---------------------Post rebuttal: I thank the authors for their rebuttal. Let me focus my reply on a few important points. I first thank the authors for clarifying the meaning of Condition 2. In this sense, Condition 1 is the key main contribution; however the current proof does not look correct to me, and the revised argument is far from being sufficient. In particular:- Point 5 of the rebuttal: I think the revised argument here is incomplete. The given argument concerns trivial facts and does not imply the claim. For example, what if the distribution of the terms is symmetric, the expected sum is zero, and hence the quantity might be of order smaller than d? Note that this is an example problem; there are multiple problems with the proof of Lemma 1.4. For example, the paper claims this for all k; but if k is something like d^100, would things hold? What would stop the magnitude of the weights to grow with time?- Point 6 of the rebuttal: The CLT, when applied w.r.t. the randomness of the weights, says that for a fixed $x$, $\sum_{r=1}^d \hat{a}_r \phi (\hat{w}_r x) / \sqrt{d} \sim N(0,v_x)$ approximately. That is, there is a non-zero probability (w.r.t. the randomness of weights) that the claim in the paper for a fixed $x$ fails. As such, to reason the claim for many $x$, one requires doing probabilistic arguments very carefully.The paper should execute the proof very carefully. It is not just a matter of technicality; I suspect some of the claims are actually wrong.More importantly, Condition 1 alone is insufficient to claim dynamical stability at any time k. What should qualify for dynamical stability is rather the existence of a well-defined limiting dynamics exists (which is argued heuristically in Appendix C), and its proof. In the current writing, its unclear how Condition 2 is crucial; while it studies interesting properties, it is very restrictive.As said in my last review, one thing that has been missing is really whether the insight here differs qualitatively from the known NTK and MF limits. Further looking at the limiting dynamics in Appendix C, one sees that they are qualitatively either NTK or MF. There are possible degeneracies due to scalings and the use of logistic loss, but these do not lead to much deviation from NTK or MF behaviors. If one is to use a squared loss for instance, what one would obtain in Figure 1 is just the line connecting NTK and MF; all other points in the band outside this line are degeneracies due to logistic loss. The behavior on this line, again, is qualitatively either NTK or MF, and this is shown (somewhat implicitly) already by a number of past works.I would imagine a rigorous derivation of the limiting dynamics for each point in the band revolves around the renormalized dynamics in Appendix C. When translating from the renormalized dynamics to the original one, the extra scaling factors will complicate the proof (for instance, they can blow up Lipschitz constants). Again this has to be done very carefully. I have read the authors' detailed rebuttal. Thanks. Post-revision update----------------------------Thanks to the authors for their revision. Unfortunately, I still feel the contribution and value of the work is not well communicated. Many of my previous points still stand. It is not a sufficient response to just say that the two propositions are the main contribution and the critical summary of findings and their relevance. ** Update after the authors' response **I have read the authors' response and other reviews. I still believe that my evaluation is correct at the moment. At the same time, I believe that the research direction is very and promising, and I hope to see the updated version of the manuscript published in the future! ############################feedback to authors' response#############################I'm aware of the non-convex setting is valid, but since the corrected proof of the theorem is not uploaded, I will raise my score to 3.  --------------------------------------------------------------Post rebuttal: Table 3:the results indicates that only Albert-xxlarge achieve very high performance (222M). however, comparable models to other approaches, such as Roberta-base or albert-xlarge achieved around ~57% performance which is within margin of previous arts. for example, SimpleTOD with gpt2-base (124M) achieved 55.7% and ConvBert achieved 58%.Therefor, it is unclear why Albert-xxlarge get so much higher performance compared to other encoders, since same tokenization and domain-slot relation is used. Based on results in Table 3, there is inconsistency in which domain-slot relation does not always results in better performance,  and it depends on the choice of encoder too. Overall, the proposed architecture is very similar to TRADE model, in terms of using an encoder for dialogue history, slot-gate and slot-value classifier. The only difference is in using a much powerful pretrained encoder.  Revision: Although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:- Missing experimental evidence for the efficiency of the NN search algorithm.- Experiments are using Parzen window for estimating likelihood which  are known to be unreliable in high dimensions.  - None of the suggested experiments were considered. In my opinion these experiments could improve the quality of this work. - Moreover, as mentioned by reviewer 1, Grover et al., 2017 provides evidence contrary to what the authors claim but this was never addressed so far in the paper.- Theorem 1 makes rather strong assumptions: as pointed out by reviewer 1, assumption 3 is unlikely to hold for the distributions used in practiceFor these reasons I recommend a clear reject.  Update: I have read the author's response and decided to keep my review, confidence, and score.--- ***UPDATE: The authors have addressed some of my comments. I appreciate their efforts for making the paper clearer. That being said, I would still keep my original score, because my major concern (evaluation) has not been addressed. Also, the paper would be much better if its writing and organization can be improved. # Post-Rebuttal CommentsI'm afraid I'm maintaining my score as a 3. The authors' response did not do enough to address my concerns:### Potential for Attacks on Saliency ModelsUnless I'm misunderstanding, the perturbed images presented in Fig. 1 are the results of adding perturbations intended to cause a change in the behavior of the **classifier**, not the **saliency model**.### Evaluation of attack strength on robust classifiersI still don't understand why the success of the dual-perturbation attack against models that were not trained to be robust to such attacks is of interest.### Suspiciousness of adversarial examplesThe images identified in Appendix L are visually suspicious due to the sharp boundaries between the "foregorund" and "background", not because of a noticeable change to the background. The fact that these images are considered unsuspicious by the metric defined in the paper suggests that the metric needs to be modified. --- Update after reading the rebuttal and other reviewsThough some concerns have been addressed, a critical issue remains unresolved, which is that the experiments use only 3 runs. As an additional point, it is useful that you've identified MAPPO as a good multi-agent algorithm. However, for clarity, it is better to focus the paper around MAPPO rather than strictly calling it a benchmarking paper. Further, it would be good to clearly state in the paper that the results now include reward normalization.  After author response: My main concerns about this paper are technical novelty and weak experimental results. The authors did not make efforts to address the two aspects properly. For example, I have listed several issues in my comment 3 about the specific experimental results, but the authors did not try to address them at all. Their responses to Reviewer 4 about the transductive ZSL are not relevant to the weak experimental results of concern.  As most of my concerns are not resolved in the response, I see no evidence to upgrade my rating.I appreciate that the authors have revised their responses. However, the added response still does not address the two specific questions (the reasons why AREN is not included in Table 3, and AREN+CS not in Table 4) in my comment 3. My final evaluation about the paper is on the negative side in that its technical novelty is moderate and the experimental results are not convincing.  *Post-Rebuttal Evaluation [FINAL]*I would like to thank the authors for their response and for the updated version of the manuscript, I appreciate their efforts. Unfortunately, I still believe that the paper lacking about original contribution and I am not fully convinced by the authors' comments on the relationship with [Yang et al., A Unified Perspective on Multi-Domain and Multi-Task Learning, ICLR 2015] and [Liu et al. Generalized Zero-Shot Learning with Deep Calibration Network, NeurIPS 2018] which I still judge highly overlapping with the current methodology.Furthermore, although authors clarified on the avoidance of using semantic embeddings for the training methodology, I do not see a sharp point in pursuing this approach given the high gap in performance with prior art (GAN-based). For all these reasons, I regret to confirm my initial rejection score. -------------------------- Post-rebuttal ----------------------------I read the authors' rebuttal and I appreciate their efforts. I would suggest that the authors incorporate those clarifications into their manuscript. I would also suggest that the authors re-motivate their paper and modify their approach section.In terms of the discussions to related work, I do think [A, B, C] is about long-tailed recognition/detection, not few-shot learning. For instance, [A] works on LVIS, a long-tailed object detection dataset; [C] works on iNaturalist, which is clearly long-tailed. [B]'s Fig 1 clearly shows that the problem is long-tailed. I'm surprised that the authors simply said that [A, B, C,] works on different problems but did not intend to discuss the similarity in methodologies.There are some very important questions not addressed yet, specifically, my comments 3 and 4: There is no analysis if the proposed algorithm resolves gradient distortion. There is no analysis if graphs and memory banks are really needed. I also read other reviewers' comments and I agree with R4 that the current version still lacks critical insights and some justifications are questionable.Given these, I would keep my initial score unchanged. Post rebuttal comments: Thank you for the rebuttal. The broad concerns of insufficient novelty remain and I am sticking to my initial paper rating.  After reading the rebuttal:I keep the score.In my view, the Taylor-series analysis is a very straightforward approach such that existing frameworks apply to the setup considered in this paper. Such a straightforward approach renders the results restrictive and hence I do not consider it a significant novelty.The proof of Theorem 1 just rewrites the existing proof with minor modifications for the considered setup and hence do not require any additional restriction. Emphasizing Theorem 1 is not very appropriate for the rebuttal.The "generalization" I mentioned is for the \phi function. There are already several works on mirror descent/FTRL with a time-varying regularizer (Orabona et al. (2015) is for FTRL, as clarified in Orabona's lecture notes on online learning). Without an example where the \phi function is not the gradient of a potential function, I do not see the necessity of this generalization.For the same reason above, emphasizing Theorem 1 is not very appropriate for the rebuttal.My point is to clarify the dependence of the convergence rate on the problem parameters. I do not think the proofs in the appendices provide explicit characterizations of such dependence. To make such explicit characterizations, I think specifying a step size instead of a range of step sizes is perhaps necessary.I suggest the authors rewrite the first two paragraphs to make the motivation of the problem setup clearer there. Section 2 is OK. _**POST-REBUTTAL**Sadly, I'm decreasing my score a notch because the authors lack a deep understanding of MIR [1] that would make it evident that GMED is *much* closer to the three methods proposed in MIR. Specifically, the authors have responded to my concern about the lack of novelty:    In GEN-MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The generator is indeed optimized for maximizing the forgetting, but the forgetting here is measured for the generator - i.e., the generator retrieves most forgettable examples for the generator itself. The approach does not learn a generator that can generate examples that are more forgettable for the classifier; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.This is simply not true. If you take a look at Equation 2 in MIR, you will see that the generator is generating forgettable examples **for the classifier**. It also uses it for itself, see Equation 3. GEN-MIR is thus a gradient-based memory editing for CL.        AE-MIR (...) It is a hybrid approach of example compression and ER-MIR. There is no online optimization towards more forgettable examples for the classifier like GMEDAgain, just like GEN-MIR, AE-MIR uses gradient-based memory editing for CL for the classifier.In GMED lies somewhere between ER-MIR and [GEN-MIR, AE-MIR]. My guess is that the MIR's author didn't propose the GMED method because it doesn't make a lot of sense to update edit a sample **and not edit its label**. Here is an intuition on the behavior of each method: let's say your models sequentially learning to visually classify objects. The model is now learning about zebras and it's causing some interference on the horse's representation.- ER-MIR will search in its buffer and retrieve a horse for the classifier to do replay on.- GEN-MIR will search inside the latent space of a generative model to find generated horses for the classifier to do replay on.- AE-MIR will search the latent space of an autoencoder to retrieve past horses that appeared in the data stream for the classifier to do replay on.- GMED randomly samples some data in the buffer, e.g. a car, and takes one gradient update on the car such that it resembles more a horse. Then the classifier is fed that modified image (i.e. x) as well as the unchanged horse label (i.e. y)The empirical section shows us that one needs to add MIR to GMED to obtain the best results. This comes as no surprise. Combining methods with each other and increasing computing needs and/or replay will give you a better performance on forgetting._________ ## Post-Rebuttal UpdateSince many of the concerns from my original review would require a major revision to address, I've tentatively left my original review score (3) unchanged. The biggest reason is that I'm still concerned that insufficient hyper-parameter tuning could lead to wrong conclusions; this would need more work to address. I've included some additional notes below.In their response to my review and those of the other reviewers, the authors promise to address a number of issues with the current draft. While I hope these updates will ultimately improve the paper, the authors' current responses don't provide me with enough information merit increasing my ICLR review score. For example:* "We will try to auto-tune the hyper-parameters of different predictors in the future."* "We think we can improve Figure 3c" by measuring mean/variance across multiple runs.On a positive note: I'd like to thank the authors for clearing up my confusion about ranking diff under "additional notes". I also think the authors' explanation of their isomorphic sampling procedure in the response to AnonReviewer2 addresses one of my questions. brief update after author response============================== I decided to maintain my rating due to several key claims in the response are unverified. For example, in Q1 "We propose that NOT ALL users (but only active users) should use transductive embeddings to memorize their history, which is different from existing methods that treat all users in the same way.", however, comparison against them is missing, especially for methods like [1] and [3] that uses both inductive and sequential embeddings; in Q2 "Trivially cutting off the sequence could result in worse accuracy", however, this is not verified: not clear what's the effect of cutting off on baseline models. I believe these are the core research questions that need to be verified in the paper, which are unfortunately missing. [No rebuttal given by the authors] Score unchanged. -----------------------------Post Rebuttal ModificationRegarding A1: I agree with R2 that the theory has major concerns and the authors were not able to fix it during rebuttal. I think we need to be clear that whether the method can work empirically and whether the provided theory can explain it are two problems. Now it seems to me that it is clear that the theory is wrong, and the problem is that the authors did not take into account the difference of the class-wise labels and pairwise labels. I suggest the authors to change the theory completely or remove the theory before next submission.Regarding A2: I don't chase SOTAs and I could certainly appreciate works that give nice theoretical insight but limited improvement. Now that the theory is wrong I have to be critical about the experiments. Since the performance is much worse than STOA, it is no longer clear whether the proposed algorithm works or it's just because the baselines are too bad.I adjusted my rating from 5 to 1.-----------------------------Regarding the authors' 2rd and 3rd responsesFirst, please allow me to clarify that my wording "the theory is wrong" means the theoretical justification on why the proposed algorithm can benefit from the transformation and achieve better performances is wrong, as the authors wrote This theoretically justifies why the proposed method works well in their submission. The major flaw/concern has been raised by R1(Q1) and myself(Q1), and the authors responses on these two questions are not convincing. This is the concern that I have been asking, so I assume it is not vague. I dont see any potential way to fix this major concern in the current theoretical justification sketch, so I think this submission needs a major revision. I want to give my apology if my wording "the theory is wrong" leads to misunderstanding to the authors or other reviewers.Second, I would like to see ACs or PCs to step in and let me know if I could rate the submission as 1 in this case. I have temporarily increased my rating from 1 to 3 as it has been questioned by the authors, especially the author who have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR.Whats more, I would also like to request apologies from the authors. The wording angry is unpleasant and misleading. As the author asked, what are you angry for?, Im not angry at all. I simply adjusted my post-rebuttal rating with my expertise after reading the authors responses and other reviewers comments.Finally, I would like to remind the author who have served as a reviewer 100+ times and as an area chair 10+ times for top conferences like NeurIPS/ICML/ICLR, one of the main rules of academic writing is to avoid using second person. I hope this will be helpful.Best **Post Rebuttal**I had concerns regarding the clarity, theory, and experiments. During the rebuttal phase, the authors actively discussed various points raised by the reviewers and the AC. Despite its length and breadth, I do not find them addressing the core of the raised concerns. That is except my 2nd point of the major theory concerns, regarding the time complexity and convergence time which is at least partially addressed.  The time complexity is addressed since the length of the first round of training for transition matrix is negligible compared to the main training. The convergence time would also be addressed if the number of epochs for the proposed method is the same as the baselines. The reviewer is not entirely sure that is the case though. Given the outstanding majority of the concerns my final feedback is as follows:The paper provides an original idea for learning with label noise which is positive, to rate the demonstration of the relevance of the idea, I can either consider the paper from an empirical study lens or a theoretical one. From the latter perspective, the concerns above effectively affect the whole theoretical arguments of the paper. The main claim of the paper, even in the latest revision, is based on the noise rate of similarity labels being lower than the noise rate of the corresponding class labels and that this is what can bring improvement in the final performance. This reads absolutely unfounded to the reviewer. As discussed, the similarity rate is mostly influenced by dissimilar labels (in the balanced c>2-classification scenario) and can be made arbitrarily low. Furthermore, the discussion still does not make a clear formal connection between the error bound on the noisy similarity learning and the noisy classification for the general case. Nevertheless, such a connection would require a major revision/addition to the paper that would need a proper round of review.When it comes to the empirical view, the experiments are inconclusive and not thorough-enough for an empirical paper due to 1) the drastic change in the learning setup (which is implemented inhouse including the base transition-matrix methods) in tandem with the fact that hyperparameter optimization is not done per method (e.g., the hyperparameters are taken from the papers for baselines while they are optimized for the proposed method's training). This is especially important since when learning with noisy labels, the choice of hyperparameters are extremely influential in the final results. 2) the improvements are mostly marginal except for CIFAR100. 3) Results are not provided for the original Clothing1M dataset. The shortcoming that led to changing Clothing1M needs to be thoroughly studied for an empirical work since it directly affects the applicability of the paper.On top of these, the final version of the pdf is still lacking on clarity several instances of which were listed in the original review. Thus, considering all the points above leads me to confidently keep the original rating as "clear reject". ====== Post discussion comments ======After reading all reviews, responses, and discussions, I still do not see evidence that the model has learned a "high-level plan", which is the main claim of the paper.I agree that most deep learning models are not interpretable, but most papers do provide some qualitative/anecdotal/generality/strong empirical evidence to support their claims. In this paper, I do not see such strong evidence.My main concerns:1. Is it possible that the 2-stage search simply increases the diversity of solutions? If there is an empirical improvement, I would like to understand the simplest explanation ("Occam's razor"). If the main contribution is diversity, I would expect the authors to spell it out clearly.Further, if the main contribution is diversity, maybe there are much simpler and general ways to achieve diversity (e.g., diversity inducing versions of beam search), that can be applied to different architectures (i.e., not coupled with VQ-VAE).I feel that my question "Can we have an additional baseline that improves diversity in RobustFill, to reject the hypothesis that the VQ-VAE simply increases the diversity of solutions?" was not answered by the authors.2. The 2-stage search is a general approach, but the paper did not convince me that it is useful to settings beyond the FlashFill task, and for models other than the textual approach where programs are generated as text.If the authors claim that their approach allows "high-level planning", I would expect to see that it works across different models / tasks / datasets / settings.Minor: I do not agree with the authors that this is self-supervised. I think that the paper uses the term "self-supervised" incorrectly.