 The authors introduce DreamerV2, a modification of the influential Dreamer RL agent (hereafter refered to as DreamerV1). The primary changes from DreamerV1 are a discrete latent space and a modified loss function (and with it, a modified optimization scheme). As in DreamerV1, the agent trains a world model with environment experience, and the policy is learned by "imagining" within the learned latent space using the world model to simulate transitions and rewards. They demonstrate superior performance over a variety of successful benchmarks that use similar compute (1 GPU, 1 environment -- e.g. MuZero, which requires vastly more, is not considered) on Atari. Further, they analyze several ways of aggregating Atari scores, and (while their algorithm performs best of those tried in each aggregation), they recommend one aggregation method (along with several other choices made for benchmarking) going forward.This is impressive work. The modification over DreamerV1 is simple (simple enough that they can describe important optimization details within the main body of the paper -- great!) and the results are a convincing demonstration of its utility. The methods are detailed and well-described. Further, I think the effort spent on exactly how to benchmark (and in particular, to report scores) is very welcome and useful to communicate (though I'll be interested to hear if others object to portions of the recommendations!).The main weakness: I wish we gained a better understanding of why the discrete latent space works well. Interpretability might be difficult (and I don't necessarily consider that bad -- the results speak for themselves) but we are left with a mix of several plausible hypotheses. Perhaps some visualization might be useful. Regardless, I strongly advocate for acceptance. What they propose is relatively simple (and the paper describes it well) and appears to work well in this setting. I'll be recommending students try it in other settings. Further, the benchmarking discussion is very useful for the community.On the use/interpretation of the discrete latent, a question for the authors: do we have a good sense of what sorts of environments DreamerV2 does poorly on, relative to the continuous latent ablation? DreamerV1 does well on a variety of continuous control tasks -- does DreamerV2 do poorly in continuous control? (Apologies if I missed this described somewhere.) Having some delineation of where it does well vs poorly could help me get a better sense of use of the discrete latent.Appendix A is most welcome. In this paper, the authors introduce a novel procedure to predict or acquire insights from the structure of a macromolecule (such as a protein, RNA, or DNA), represented as a set of positions associated with atoms or groups of atoms in 3D Euclidean space. Their approach, called GVP-GNN, can be applied to any problem where the input domain is a structure of a single macromolecule or molecules bound to one another. Their approach is divided into two steps: model quality assessment and computational protein design. The paper addresses a notable problem in computational biology: learning on 3D structures of large biomolecules.  I have noticed various industry-based studies struggling to find the right solution. The paper is powerfully written, and the approach is novel. The steps are described explicitly, which helps the reader to understand them. I have a few suggestions for the Authors. Please grammar proof the paper thoroughly to avoid small grammatical mistakes such as articles, or typos. In the results, please refer to the suggested approach as GVP-GNN. Sometimes, you refer to your method as "ours". Please be uniform. The availability of a dataset, especially the synthetic dataset, will allow the paper to be reproducible by others.  The paper raises many important questions about unrolled iterative optimization algorithms, and answers many questions for the case of iterative soft thresholding algorithm (ISTA, and learned variant LISTA). The authors demonstrate that a major simplification is available for the learned network: instead of learning a matrix for each layer, or even a single (potentially large) matrix, one may obtain the matrix analytically and learn only a series of scalars. These simplifications are not only practically useful but allow for theoretical analysis in the context of optimization theory. On top of this seminal contribution, the results are extended to the convolutional-LISTA setting. Finally, yet another fascinating result is presented, namely that the analytic weights can  be determined from a Gaussian-perturbed version of the dictionary. Experimental validation of all results is presented.My only constructive criticism of this paper are a few grammatical typos, but specifically the 2nd to  last sentence before Sec 2.1 states the wrong thing "In this way, the LISTA model could be further significantly simplified, without little performance loss"...it should be "with little". This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes. The key components of the approach are :- increasing the batch size by a factor 8- augmenting the width of the networks by 50% These first two elements result in an Inception score (IS) boost from 52 to 93.  - the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments. The comments that more data helps (internal dataset experiments) is also informative. Very nice to have included negative results and detailed parameter sweeps.This is a very nice work with impressive results, a great progress achievement in the field of image generation. Very well written.Suggestions/questions: - it would be nice to also propose unconditioned experiments. It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve. - I understand that no data augmentation was used during training?    - clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.- It would be nice to display more Nearest neighbors for the dog image.- It would be nice to add a figure of random generations.- make the bib uniform: remove unnecessary doi - url - cvpr page numbers This manuscript describes an extension of the knockoff framework, which is designed to carry out feature selection while controlling the FDR among selected features, to settings in which the generative distribution of the features is not Gaussian. Specifically, the authors employ a GAN (with several modifications and additions) in which the generator produces knockoffs and the discriminator attempts to identify which features have been swapped between the original and the knockoff.The method works as follows: (1) A conditional generator takes random noise and the real features as input, and outputs knockoff features. (2) A modified discriminator is used in such a way that the generator learns to generate knockoffs satisfying the necessary swap condition, so as to control the FDR of the knockoff procedure. (3) A power network uses Mutual Information Neural Estimation (MINE) to estimate the mutual information between each feature and its knockoff counterpart, so as to maximize the power of the knockoff procedure.Results are provided on synthetic and real data. As for the synthetic data, when the underlying feature distribution is Gaussian, the proposed method, KnockoffGAN, performs almost as well as the original knockoff and outperforms the BHq method; when the underlying feature distribution is non-Gaussian, KnockoffGAN dominates both the original knockoff and BHq methods. As for the real data, the authors claim to identify nine relevant features for cardiovascular disease and eight relevant features for diabetes, whereas the original knockoff procedure identifies zero features from the same data.General comments: This is an extremely impressive piece of work. The manuscript itself is a pleasure to read, and the results clearly demonstrate that the proposed KnockoffGAN both controls FDR and achieves power comparable to the original knockoff procedure in the Gaussian setting and much better than the original knockoff when the underlying distribution is not Gaussian.Strengths: The combination of GANs and knockoff filter is a very promising and intriguing idea.The use of the modified discriminator to ensure that the generated knockoffs satisfy the necessary swap condition is novel and intuitively sound.The use of MINE to maximize power by maximizing the mutual information between each feature and its knockoff counterpart is also interesting.The paper is well written, reads smoothly and the ideas are well exposed.The illustrative figure is straightforward.Weaknesses:Intuitively, the modified discriminator and the power network should conflict with each other. I expect it was tricky to achieve a good tradeoff between two, but the authors failed to elaborate on these details.The authors do not provide the design details of the neural networks. How dependent on the specific parametrization of the network architecture is the performance? How does the training order of four networks matter to the performance? The manuscript should cite [[ Jaime Roquero Gimenez, Amirata Ghorbani, and James Zou. "Knockoffs   for the mass: new feature importance statistics with false discovery   guarantees." arXiv:1807.06214, 2018. ]] which proposes a way to generate knockoffs for a Gaussian mixture model, and this method should be included in the relevant supplementary figure.In Section 5.1.4, I would like to know, for a fixed data set, how the regularization affects the final values of the other loss terms.The analysis of real data in Section 5.2 is unsatisfying in several respects.First, there is an unfortunate oversight in Table 1: the text refers to three features that are "trivial," but only one of these is marked with an asterisk.  This leaves open the question of whether there are other trivial features beyond the three mentioned in the text. In addition, it is not clear exactly what it means for a feature to be "trivial" in this context.This point gets to a deeper problem with the evaluation, which is that we are told, with no evidence, that these features are supported by literature in PubMed.  I would like to see two things here.  First, it seems obvious to me that if you are going to say that there is support in PubMed, you are obliged to actually report the citations that supposedly give this support. This could be done in the appendix. Equally importantly, there is a potential here for ascertainment bias which should be combatted in some fashion.  Presumably, some human expert had to do the PubMed searches to make this assessment.  I would like to know how "permissive" this assessor is.  To assess this, one could give the assessor a collection of terms, some of which were selected by KnockoffGAN and some at random, and then report the results. Obviously, some features that are significant may not be in the list of selected terms (because KnockoffGAN does not achieve 100% power) and so may appear as false negatives. But without some assessment like this, I have trouble believing this assessment.A related point is that it seems quite unfortunate that the authors chose a data set that cannot be described at all due to the anonymity constraint.  At the very least, it seems that we should be told the dimensionality of the data set. The Knockoff literature contains real data sets that could have been used here.Minor comments:On the first page, the sentence beginning "On the other hand," should clarify that this is only in expectation.p. 3: Missing right paren after [7].p. 5: Write out Gaussian process.p. 5: "as little" -&gt; "as little as possible"p. 6: "to show that in" -&gt; "to show, in"In Figures 2-5, add a horizontal line at 10% FDR for reference.p. 10: "features ones" -&gt; "features"Note to program committee:I did not review the technical details of the proof in the appendix. The paper presents a simple but remarkably efficient exploration strategy obtaining state-of-art results in a well known hard-exploration problem (Montezuma's Revenge). The idea consists of several parts:1. The authors suggested distilling a fixed randomly initialized network into another randomly initialized trained network in order to use prediction errors as pseudo-rewards. The authors claim that distillation error is a proxy for visit-counts and experimentally demonstrate this idea on MNIST dataset.2. The authors suggested using two separate value heads to evaluate expected rewards and expected pseudo-rewards with different time horizons (discount factors) under the same policy.The paper is overall well written and easy to read. As far as I can tell, the use of a distillation error as an exploration reward is novel. Relative efficiency of the method compared to its simplicity should interest most people working in RL.The main problem which I see is the presentation of learning curves as a function of training steps rather than acting steps. While I acknowledge that the achievement of state-of-art asymptotic performance is valuable on its own, presenting results as a function of acting steps (rather than parameter update steps) may better show data and exploration efficiency. This would also facilitate comparisons with other RL algorithms which may have different architectures (for example, multiple networks updated at different frequencies).I liked the idea to use two value heads to evaluate intrinsic and extrinsic values with different discounts. Still, as both heads share a common 'trunk' network, they will inevitably affect each other. For example, scaling the pseudo-rewards by 10 and scaling the pseudo-reward value function by 0.1 to produce the same summed value function may lead to a different training dynamics due to the influence of intrinsic value head onto the extrinsic one. Are the results sensitive to this effect? Also, how are the results sensitive to the scale of pseudo-rewards? What would happen if they were simply multiplied or divided by 10?Also, what was the distributed training setting that you used to train your agent? Were the actors running on a single machine or on multiple machines? Was a single trainer running on a single machine training network on batched observations or was training distributed in some way? The reason why I am asking this is that as a distillation error fundamentally depends on its training dynamics, I would not be surprised if the results could be affected by the training setting. For example, if the network was trained in a distributed setting, asynchronous updates could introduce implicit momentum and thus may cause a pseudo reward to oscillate. While I do not think that is a fundamental problem with the work either way, it would be nice to know a few more details for future reproducibility.Other minor comments:Figure 2. It would be nice to see if both x and y axes was plotted in log scale in order to visualize any power-law (if one exists) between samples and MSE.Figure 3. I would prefer 'x' axis to be in the number of steps.Figure 4. Again, performance between different actor-configurations would be easier to see if x axis was a total number of steps, as it would be easier to see if the curves overlap and the method scales linearly with the number of actors. Strengths:- even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to MT is novel- the presentation is very clear- the choice of methods is well argued and justified- the experiments are well executed and analysed- thorough and varied analysis of the experimental findings I recommend this paper for the best paper award. The paper "Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow" tackles the problem of discriminator over-fitting in adversarial learning. Balancing the generator and the discriminator is difficult in generative adversarial techniques, as a too good discriminator prevents the generator to converge toward effective distributions. The idea is to introduce an information constraint on a intermediate layer, called information bottleneck, which limits the content of this layer to the most discriminative features of the input. Based on this limited representation of the input, the disciminator is constrained to longer tailed-distributions, maintaining some uncertainty on simulated data distributions. Results show that the proposal outperforms previous researches on discriminator over-fitting, such as noise adding in the discriminator inputs. While the use of information bottleneck is not novel, its application in adversarial learning looks inovative and the results are impressive in a broad range of applications. The paper is well-written and easy to follow, though I find that it would be nice to give more insights on the intuition about information bottleneck in the preliminary section to make the paper self-contained (I had to read the previous work from Alemi et al (2016) to realize what information bottleneck can bring). My only question is about the setting of the constaint Ic: wouldn't it be possible to consider an adaptative version which could consider the amount of zeros gradients returned to the generator ? Summary:In one-shot differentiable NAS, a supergraph is usually trained (via bilevel optimization as in DARTS, or other approximations to bilevel such as gumbel softmax, etc). After supergraph training, a final architecture is obtained by taking the operator at each edge which has the highest architecture weight magnitude. This step is usually termed as the 'finalization' step. (In DARTS the finalization step actually orders incoming edges by the max of the architecture weight magnitudes at each edge and selects the top two edges and the corresponding maximum architecture weight in them as the final operators.). This paper examines this quite ad hoc step very closely. It finds that the magnitude of architecture weights (alphas commonly in this niche literature) are misleading. It shows by careful ablation experiments that alpha magnitudes are very much not useful in selecting good operators. By taking inspiration from the "unrolled estimation" viewpoint of ResNet prior work it shows that DARTS converging to degenerate architectures where alphas over parameters operators like skipconnect is actually to be expected when finalization step relies on the magnitude of alpha. The paper proposes a much more intuitive finalization step which just picks the operator at each edge which if removed from the supergraph results in the largest drop in validation accuracy. To bring back the supergraph to convergence a few epochs of further training is carried out between operator selection.  Experiments show that just by carefully thinking about the finalization step in differentiable one-shot NAS, one can obtain much better performance. In fact, one does not even need architecture weights at all! Don't worry about complicated bilevel optimization, gumbel softmax approximation, etc. Just train a supergraph and pick operators progressively. Comments:- The paper is wonderfully written! Thanks!- As I read a paper I try to think without looking at the experiments, what set of experiments I would try to run to prove/disprove the hypotheses proposed. Afterwards I go through the experiments and see if those experiments were actually run (or if they differed why). In this case, every experiment and more were already run. Particularly towards the end I was thinking what if we just got rid of all the alphas and just trained a supergraph as usual and did the PT finalization as proposed. And lo and behold, it actually works better!- This paper is actually throwing a big wrench in one-shot differentiable NAS literature. Many papers are being written which try to improve/fix DARTS and DARTS-like methods. If I were to believe the experiments, I don't actually need to do any of that. I have some questions I hope to discuss with the authors:1. Is all the complicated bilevel optimization (often popular as 'metalearning' currently) not useful in the case of NAS? (This is not really the authors' burden to answer but I am just hoping to see if they have any insights.)2. Can we view the PT finalization step as a progressive pruning step?  So if I were to turn this into a method which produces a pareto-frontier of models (e.g. accuracy vs. memory/flops/latency etc), we first train a big supergraph and then progressively prune out operators one at a time as proposed here and take a snapshot of the supergraph and plot it on the x-y plot (where say x is latency and y is accuracy) and pick the ones clearly on the pareto-frontier and train them from scratch? (Again not really authors' burden but curious if they have any insights)3. Figure 4 suggests that training the supergraph anymore than 20 epochs only hurts performance (no matter which finalization procedure is used, of course PT has far less of a drop). Does bilevel optimization actually hurt with weight sharing? ##### SummaryThis work is concerned with learning mappings between pairs of domains that may differ in representation (imagery vs. configuration state), physical parameters, morphology, or combinations thereof, relying only on unpaired training data from both domains.  The key idea is to leverage the (existing) concept of cycle consistency, incorporating dynamics to formulate a cycle loss that spans vastly different domains.  The cycle is closed by learning a domain translator $G$ that maps states (the paper calls them observations - this is unimportant as the system makes no such distinction, as either domain might represent a "state" or an "observation") from domain $x$ to domain $y$, and a forward model $F$ that operates in the $y$ domain.  In addition, functions $H$ and $P$ are learned that map actions $a$ and $u$ between their respective domains $x$ and $y$, in either direction.  The difference between $G(x_{t+1})$ and $F(G(x_t), u_t)$ is then the part of the overall loss function that closes the loop. $G$, $H$ and $P$ are trained adversarially using cycle consistency losses; $F$ can be trained by regression, exploiting training sequences in the $y$ domain.The system is evaluated on four different tasks from OpenAI Gym, and is shown to be effective in diverse settings.##### Strengths* This work addresses an important, fundamental problem that arises in many applications, including state estimation and sim2real transfer.* The paper makes a significant, nontrivial contribution to this problem, allowing hard cross-domain transfer problems to be solved dramatically better than before.* The work is well related to prior literature.* The claims are supported by the empirical results.* The paper is well organized, readable and clear.* The appendix provides a lot of additional detail important for reimplementing this method and replicating the results.##### Weaknesses* The implementation details, in particular the neural-network architectures, appear to be very specifically tailored to the scenarios of evaluation. Instead it would be helpful to provide insight into how a user of this method should go about designing the networks, given their application domains.* For cross-modality alignment, paired training data are typically available.  How can the method take advantage of this? How would this impact the results, e.g. those of Table 2?  Some related results are given in Fig. 3(c) but it is unclear what exactly was done and what exactly the graph shows.* An obvious and explicitly-stated application of this work is sim2real transfer, but no such experimental results were provided (I realize that it is not easy to set up such an experiment from which strong conclusions can be drawn).##### RecommendationUnless I am missing important similar work, the paper makes a strong contribution in an important area.  It should be accepted.  While I wrote more text under Weaknesses than under Strengths, the weaknesses are minor compared to the strengths.##### Questions for the Rebuttal* The results on cross-physics alignment (Table 1) seem to leave a lot of room for further improvement.  What is the limiting factor?* Please comment on the listed weaknesses.##### Details* "RL score" = return?* There are many little typos. This article proposes a novel approach integrating language throughout the visual pathway for segmenting objects according to referring expressions. The article is well written, and poses an important question about how best to integrate linguistic and visual information. The limitations of the currently dominant top-down approach are well argued. The answer proposed by the authors is to integrate linguistic information throughout the visual hierarchy. The task of segmenting by referring expression is important and well chosen. The proposed model is sound, and well described in the article, and the experimental results demonstrate that the model outperforms clearly the state-of-the art in all metrics. The qualitative examples provided are quite impressive and demonstrate the success of the approach. In sum, I feel this is a well written paper addressing a very timely and important problem in computer vision and AI research and should be of broad interest in the community.  The paper demonstrates a mathematical equivalence between Hopfield nets and RBMs, and it shows how this connection can be leveraged for better training of RBMs.What a great paper - well written, an enlightening mathematical connection between two well-known models that to my knowledge was not previously known.  Hopfield nets and RBM's have been around for decades, and I don't think we've been aware of this connection, so it seems like a pretty important finding.  The paper explores the utility of this connection by applying to an MNIST task.  Interestingly, the connection yields important insights in both directions: stochastic sampling in an RBM is faster than Hopfield due to a smaller matrix and parallel layer wise updates, whereas initializing an RBM with the projection rule from Hopfield allows it to find a better solution faster.I really enjoyed reading the paper, I learned something new, and I think others will too!  It is an important advance in our understanding of Hopfield nets and RBMs. This paper proposes a family of cost functions and a framework for modeling a continuous multimodal (CMM) space.The proposed model converges more stably and faster than conventional methods and shows high-quality results in several tasks. Also, this method can generate diverse outputs at inference with a single model. It was partially possible with many GAN methods, but there is a significant improvement in diversity.- Clear motivation and well-defined method\The problem of modeling the CMM space is well defined, and the limitation of the previous methods are described in detail. And the intuition to resolve this problem is also highly convincing.- Various and extensive experiments\The experimental settings and results support the effectiveness of the proposed method. The toy example in Figure 3 clearly shows that the proposed method can model CMM space properly.Each experiment for downstream tasks also has a detailed explanation, showing good qualitative and quantitative performance.There are no major weaknesses in the overall content. However, it is necessary to correct that table 5 and table 6 are overlapped, and many figures and tables are mixed in a somewhat disorderly manner.It is somewhat similar to [1,2] in that a separate variable is defined for generating multiple outputs in a deterministic function. It's good to discuss this.It would be great if there is a follow-up study to see if it could be extended to generate an image from random variables, like Conditional GAN. And some naming is required to represent the method.[1] Auxiliary deep generative models, L. Maal√∏e, ,et al.[2] Sym-parameterized Dynamic Inference for Mixed-Domain Image Translation, S. Chang, et al. This work proposes that the transferability of adversarial attacks has a negative correlation with the interaction within an input perturbation. By defining the interaction of perturbations with the Sharpley value, it can quantify the interactions and demonstrate the negative correlation with the transferability. Furthermore, this work shows that prior work on adversarial attacks (e.g., VR attack and MI attack) can be explained by the (expected) interaction scores. This work further demonstrates that the way of enhancing transferability by minimizing the interaction within input perturbations, with the experiments on the image classification task. Overall, I think this work provides a new perspective of understanding transferability and presents solid analysis/experiments to verify the hypothesis. Only one comment about the definition of interaction scores. In some literature [Lundberg et al., 2019], it is called the Shapley interaction index, which uses the definition in equation (13) of Appendix D. Shapley interaction index has mainly been used in the machine learning literature recently for explaining feature interactions within models. E.g., 1. Lundberg et al. Consistent Individualized Feature Attribution for Tree Ensembles. 20192. Chen and Ji. Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers. 2020 Reasons for Score:The work proposed in this paper is novel and very well presented. The claims are supported by experimental results on different neural network models and applications, showing a good trade-off between accuracy and neural network compression.Strengths:1.The Quant-Noise idea for quantizing the weights and activations of neural networks is innovative, and it can be easily applied to various quantization methods for compressing the neural networks and saving energy. 2.The results are very good in terms of compression rate and accuracy, and the experiments are obtained on complex state-of-the-art neural network models and applications.3.The combination of int8 and Product Quantization results in hardware-deployable compressed models.4.The background and the methods used are clearly explained, also from the mathematical and formal point of view. 5.The paper is overall clear and well written. Minor Comments:1.Which is the training time overhead of having quantization noise during training?2.The percentage (%) should be indicated in the vertical axes of Figure 2. The paper proposes a generic strategy to accelerate time integration of physical systems, including elasticity, fluid simulation, and cloth simulation. The results are impressive not only for  the generality, but because this is the first paper that I read that shows a fair speedup combined with stability for such complex physical systems. I wish the authors attached the source code so that I could analyze it more closely to make sure that no shortcuts were taken in the evaluation.Assuming that the evaluation is correct, this is a seminal paper that will allow to use data-driven methods in scientific computing applications.Comments:- the entire discussion about adaptive refinement seems out of place. Would the method also work without and is this an extra feature? Or is it necessary to use this feature to make it work? My understanding is that this is optional, and only helps, in particular for cloth. Can you confirm that this is the case?- I do not understand the sentence: Each output feature is then processed by a forward-Euler integrator with t = 1. It only becomes clear after reading section 4. Please clarify earlier- Why is the model stable for hundreds of steps? Is this due to training with noise? Please do experiment to try to identify which feature makes it stable. Will it eventually break if you let the system run for thousands of frames- while the learning baseline are clearly described, I do not understand how the timings are measured for the GT in table 1. Is every row computed with a different software, as listed in A.1? If so, are all of these using a single core or multiple? This should be clarified, especially since the machine learning results are run on a gpu, making the comparison not fair. Could you report also the timing for doing the prediction of the NN using a single CPU core, or alternative run the iterative solvers of the classical methods on the same GPU?- Is there a case where you could notice that the integration is not stable? Maybe by reducing the noise in the training or by reducing the training data size or by simulating scenes far from the training data? It would be interesting to know what the limitations of the method are and where there is room for improvement.- Could you add a simple nearest neighbor baseline?- It would be interesting to add a comparison of running time *for a given accuracy* instead of simply showing what is the error in the table. This could be achieved running many simulations with analytic methods until they match the error of the data-driven model wrt a finely discretized ground truth.Overall, my impression is that the results look amazingly good: it is the first paper I read that shows that data-driven method can beat standard scicomp approaches on complex scenes. The reason for the performance of  the methods likely lies in the combination of using mesh and space variables, plus making the integration step explicit and enriching the training with controlled noise. I wish the authors attached the source code of  their method and of baselines (and I encourage them to do it after publication), I look forward to experimenting with this method and test it on my applications.  Summary: The authors propose an algorithm that separates a network into two distinct regions where one is trained with SGD while the other is trained with a local Hebbian learning rule. A weak supervision rule is proposed that improves localized learning. The authors demonstrate close to baseline performance on CIFAR-100 and ImageNet for diverse networks while speeding up training.Strong points:- Very creative use of multiple learning rules during training.- People could never get localized learning to work well enough to compete with SGD. This is the first work that uses some localized learning and manages to come close. This is a big success.- These findings have broad applicability, from brain-like learning algorithms, efficient training on one GPU, and efficient learning algorithms for model parallelism for massive networks where communication is the bottleneck.Weak points:- Missing ablation for the weak supervision algorithm.- This work is very impactful in many different ways, but it only mentions the computational efficiency perspective. The related work needs to be expanded to make readers aware of the impact of this work.- The heuristic for learning mode selection is complicated. Initial experiments suggested that a simple learning-rate-schedule-like way to select the learning mode would be possible.- Some algorithmic details in the weak supervision algorithm not clear.Recommendation (short):This is very important work with results that will significantly impact many fields (efficient training, parallelization, brain-like algorithms). It is a creative solution to a significant problem in localized learning. I strongly recommend accepting this work. If this work is rejected, I will no longer review for future ICLR conferences. I recommend this work to be accepted as an oral presentation. Selecting this work for a best paper award would be appropriate.Recommendation (long):This work is impactful for multiple reasons:- Localized learning is difficult. There has not been a work that uses localized learning and makes it work close to SGD performance on large datasets/models.- The brain does not use SGD, and it is difficult to think about algorithms that work in the brain and yield good performance. It might be that initial learning in the brain is done differently until local learning rules are used. This view is mostly ignored, but this paper yields evidence that such learning might be possible and efficient.- Layers updated with localized rules can be updated independently of other layers (if the weakly supervised rule is not used). This enables fully asynchronous training for early layers. There will be a synchronization point at the SGD layers, but through pipeline parallelism, the communication overhead can be hidden in Hebbian layers. This enables the training of massive neural networks with trillions of parameters. With current parallelism tools becoming more and more limited, this is a crucial innovation since previous asynchronous parallelism procedures always decrease predictive performance. This is the first work that shows a way to do asynchronous parallel training without performance degradation.Beyond this, the paper also yields some speedups for tasks while decreasing predictive performance only slightly. This is also an impressive feat, but the overall broad insights this paper yields make it much more impactful than this result. As such, I do not view the experimental results as the main contribution, but overall, the papers' main contribution is that it shows a way to include (gradual) localized learning in a neural network while not impacting performance.Comments for authors:This is excellent work  well done! I think the main weakness is currently a missing ablation on the effect of the weak supervision rule. You note that improves performance but by how much would be an important detail. If you do not include these ablations, I would still accept the paper, but I might rescind my oral presentation recommendation.Another issue is that your work is relevant in many different domains, but you keep it confined to the idea that your method is only useful for faster training. I think making the reader aware that local training has many advantages across many domains could be very valuable. You do not need to elaborate on this, but I would like to see some of these connections in the paper because not everyone has the background to see these connections. I think you can do this mostly by mentioning it in the conclusion since you already mention a little bit of work in parallelism, and you mention previous results about local learning that failed to obtain good performance. Another line of work that I would mention in the related work section is that of neuroscientific faithful learning rules. The most relevant line of research is the work on various forms of feedback alignment and other algorithms. For a summary of past research and results on large datasets, see Bartunov et al., 2018[1]. Beyond this, you might want to add "sparse training" to the related work on efficient deep learning. Sparse training differentiates from pruning during training by initializing the neural network sparsely during initialization (not densely and then prune to sparse). See work on a mixture of experts[2,3] and sparse dynamic training[4,5,6]. I do not require you to include these references, but they might improve the related work section. On the algorithmic and experimental side, it seems that a simple learning-rule-like schedule might be sufficient for selecting the learning mode. While I do not require you to add these experiments, it would make the algorithm simpler and more appealing if you can show that a simple learning rule works a la "warmup with SGD for 5 epochs, then shift by 1 layer (block) every 5 epochs" etc.[1] Sartunov et al., 2018. Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures[2] Shazeer et al., 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer[3] Lepikhin et al., 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding[4] Mostafa & Wang et al., 2019. Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization.[5] Dettmers & Zettlemoyer et al., 2019. Sparse Networks from Scratch: Faster Training without Losing Performance.[6] Evci et al., 2019. Rigging the Lottery: Making All Tickets Winners.